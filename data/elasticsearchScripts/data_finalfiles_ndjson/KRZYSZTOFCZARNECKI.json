{"title": "Generative programming\n", "abstract": " This paper is about a paradigm shift from the current practice of manually searching for and adapting components and their manual assembly to Generative Programming, which is the automatic selection and assembly of components on demand. First, we argue that the current OO technology does not support reuse and configurability in an effective way. Then we show how a system family approach can aid in defining reusable components. Finally, we describe how to automate the assembly of components based on configuration knowledge. We compare this paradigm shift to the introduction of interchangeable parts and automated assembly lines in the automobile industry. We also illustrate the steps necessary to develop a product line using a simple example of a car product line. We present the feature model of the product line, develop a layered architecture for it, and automate the assembly of the components using a generator. We also discuss some design issues, applicability of the approach, and future development. 1 From Handcrafting to an Automated Assembly Line This paper is about a paradigm shift from the current practice of manually searching for and adapting components and their manual assembly to Generative", "num_citations": "4994\n", "authors": ["34"]}
{"title": "Model-driven software development: technology, engineering, management\n", "abstract": " Because of its great potential, model-driven software development (MDSD) has received a lot of attention in the last few years [1], including: the Object Management Group\u0393\u00c7\u00d6s model-driven architecture [2], Microsoft\u0393\u00c7\u00d6s software factories [3], or plain code generation [4]. In their book, Stahl and Voelter advocate for a more pragmatic MDSD approach, which they call architecture-centric MDSD (AC-MDSD for short). Traditional software development presents some important problems related to individual productivity, software portability, product interoperability, and system maintenance [5]. The Object Management Group (OMG) MDSD initiative, model-driven architecture (MDA), resorts to platform-independent models that separate application functionality from the technology-specific code that implements it. MDA-related standards such as meta-object facility (MOF), which is OMG\u0393\u00c7\u00d6s foundation specification for modeling\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1525\n", "authors": ["34"]}
{"title": "Feature-based survey of model transformation approaches\n", "abstract": " Model transformations are touted to play a key role in Model Driven Development\u0393\u00e4\u00f3. Although well-established standards for creating metamodels such as the Meta-Object Facility exist, there is currently no mature foundation for specifying transformations among models. We propose a framework for the classification of several existing and proposed model transformation approaches. The classification framework is given as a feature model that makes explicit the different design choices for model transformations. Based on our analysis of model transformation approaches, we propose a few major categories in which most approaches fit.", "num_citations": "1524\n", "authors": ["34"]}
{"title": "Classification of model transformation approaches\n", "abstract": " The Model-Driven Architecture is an initiative by the Object Management Group to automate the generation of platform-specific models from platformindependent models. While there exist some well-established standards for modeling platform models, there is currently no matured foundation for specifying transformations between such models. In this paper, we propose a possible taxonomy for the classification of several existing and proposed model transformation approaches. The taxonomy is described with a feature model that makes the different design choices for model transformations explicit. Based on our analysis, we propose a few major categories in which most model transformation approaches fit.", "num_citations": "1423\n", "authors": ["34"]}
{"title": "Formalizing cardinality\u0393\u00c7\u00c9based feature models and their specialization\n", "abstract": " Feature modeling is an important approach to capture the commonalities and variabilities in system families and product lines. Cardinality\u0393\u00c7\u00c9based feature modeling integrates a number of existing extensions of the original feature\u0393\u00c7\u00c9modeling notation from Feature\u0393\u00c7\u00c9Oriented Domain Analysis. Staged configuration is a process that allows the incremental configuration of cardinality\u0393\u00c7\u00c9based feature models. It can be achieved by performing a step\u0393\u00c7\u00c9wise specialization of the feature model. In this article, we argue that cardinality\u0393\u00c7\u00c9based feature models can be interpreted as a special class of context\u0393\u00c7\u00c9free grammars. We make this precise by specifying a translation from a feature model into a context\u0393\u00c7\u00c9free grammar. Consequently, we provide a semantic interpretation for cardinality\u0393\u00c7\u00c9based feature models by assigning an appropriate semantics to the language recognized by the corresponding grammar. Finally, we give an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "870\n", "authors": ["34"]}
{"title": "Mapping features to models: A template approach based on superimposed variants\n", "abstract": " Although a feature model can represent commonalities and variabilities in a very concise taxonomic form, features in a feature model are merely symbols. Mapping features to other models, such as behavioral or data specifications, gives them semantics. In this paper, we propose a general template-based approach for mapping feature models to concise representations of variability in different kinds of other models. We show how the approach can be applied to UML 2.0 activity and class models and describe a prototype implementation.", "num_citations": "763\n", "authors": ["34"]}
{"title": "Staged configuration using feature models\n", "abstract": " Feature modeling is an important approach to capturing commonalities and variabilities in system families and product lines. In this paper, we propose a cardinality-based notation for feature modeling, which integrates a number of existing extensions of previous approaches. We then introduce and motivate the novel concept of staged configuration. Staged configuration can be achieved by the stepwise specialization of feature models. This is important because in a realistic development process, different groups and different people eliminate product variability in different stages. We also indicate how cardinality-based feature models and their specialization can be given a precise formal semantics.", "num_citations": "709\n", "authors": ["34"]}
{"title": "Staged configuration through specialization and multilevel configuration of feature models\n", "abstract": " Feature modeling is a key technique for capturing commonalities and variabilities in system families and product lines. In this article, we propose a cardinality\u0393\u00c7\u00c9based notation for feature modeling, which integrates a number of existing extensions of previous approaches. We then introduce and motivate the novel concept of staged configuration. Staged configuration can be achieved by the stepwise specialization of feature models or by multilevel configuration, where the configuration choices available in each stage are defined by separate feature models. Staged configuration is important because, in a realistic development process, different groups and different people make product configuration choices in different stages. Finally, we also discuss how multilevel configuration avoids a breakdown between the different abstraction levels of individual features. This problem, sometimes referred to as 'analysis\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "647\n", "authors": ["34"]}
{"title": "Feature diagrams and logics: There and back again\n", "abstract": " Feature modeling is a notation and an approach for modeling commonality and variability in product families. In their basic form, feature models contain mandatory/optional features, feature groups, and implies and excludes relationships. It is known that such feature models can be translated into propositional formulas, which enables the analysis and configuration using existing logic- based tools. In this paper, we consider the opposite translation problem, that is, the extraction of feature models from propositional formulas. We give an automatic and efficient procedure for computing a feature model from a formula. As a side effect we characterize a class of logical formulas equivalent to feature models and identify logical structures corresponding to their syntactic elements. While many different feature models can be extracted from a single formula, the computed model strives to expose graphically the maximum of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "450\n", "authors": ["34"]}
{"title": "FeaturePlugin: Feature modeling plug-in for Eclipse\n", "abstract": " Feature modeling is a key technique used in product-line development to model commonalities and variabilities of product-line members. In this paper, we present FeaturePlugin, a feature modeling plug-in for Eclipse. The tool supports cardinality-based feature modeling, specialization of feature diagrams, and configuration based on feature diagrams.", "num_citations": "376\n", "authors": ["34"]}
{"title": "Cardinality-based feature modeling and constraints: A progress report\n", "abstract": " Software factories have been proposed as a comprehensive and integrative approach to generative software development. Feature modeling has several applications in generative software development, including domain analysis, product-line scoping, and feature-based product specification. This paper reports on our recent progress in cardinalitybased feature modeling and its support for expressing additional constraints. We show that the Object-Constraint Language (OCL) can adequately capture such constraints. Furthermore, we identify a set of facilities based on constraint satisfaction that can be provided by feature modeling and feature-based configuration tools and present a prototype implementing some of these facilities. We report on our experience with the prototype and give directions for future work.", "num_citations": "370\n", "authors": ["34"]}
{"title": "Cool features and tough decisions: a comparison of variability modeling approaches\n", "abstract": " Variability modeling is essential for defining and managing the commonalities and variabilities in software product lines. Numerous variability modeling approaches exist today to support domain and application engineering activities. Most are based on feature modeling (FM) or decision modeling (DM), but so far no systematic comparison exists between these two classes of approaches. Over the last two decades many new features have been added to both FM and DM and it is tough to decide which approach to use for what purpose. This paper clarifies the relation between FM and DM. We aim to systematize the research field of variability modeling and to explore potential synergies. We compare multiple aspects of FM and DM ranging from historical origins and rationale, through syntactic and semantic richness, to tool support, identifying commonalities and differences. We hope that this effort will improve the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "353\n", "authors": ["34"]}
{"title": "Bidirectional transformations: A cross-discipline perspective\n", "abstract": " The GRACE International Meeting on Bidirectional Transformations was held in December 2008 near Tokyo, Japan. The meeting brought together researchers and practitioners from a variety of sub-disciplines of computer science to share research efforts and help create a new community. In this report, we survey the state of the art and summarize the technical presentations delivered at the meeting. We also describe some insights gathered from our discussions and introduce a new effort to establish a benchmark for bidirectional transformations.", "num_citations": "341\n", "authors": ["34"]}
{"title": "Verifying feature-based model templates against well-formedness OCL constraints\n", "abstract": " Feature-based model templates have been recently proposed as a approach for modeling software product lines. Unfortunately, templates are notoriously prone to errors that may go unnoticed for long time. This is because such an error is usually exhibited for some configurations only, and testing all configurations is typically not feasible in practice. In this paper, we present an automated verification procedure for ensuring that no ill-structured template instance will be generated from a correct configuration. We present the formal underpinnings of our proposed approach, analyze its complexity, and demonstrate its practical feasibility through a prototype implementation.", "num_citations": "314\n", "authors": ["34"]}
{"title": "Overview of generative software development\n", "abstract": " System family engineering seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In system family engineering, new system variants can be rapidly created based on a set of reusable assets (such as a common architecture, components, models, etc.). Generative software development aims at modeling and implementing system families in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages. This paper gives an overview of the basic concepts and ideas of generative software development including DSLs, domain and application engineering, generative domain models, networks of domains, and technology projections. The paper also discusses the relationship of generative software development to other emerging\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "301\n", "authors": ["34"]}
{"title": "Generative programming and active libraries\n", "abstract": " We describe generative programming, an approach to generating customized programming components or systems, and active libraries, which are based on this approach. In contrast to conventional libraries, active libraries may contain metaprograms that implement domain-specific code generation, optimizations, debugging, profiling and testing. Several working examples (Blitz++, GMCL, Xroma) are presented to illustrate the potential of active libraries. We discuss relevant implementation technologies.", "num_citations": "295\n", "authors": ["34"]}
{"title": "An exploratory study of cloning in industrial software product lines\n", "abstract": " Many companies develop software product lines-collections of similar products-by cloning and adapting artifacts of existing product variants. Transforming such cloned product variants into a \"single-copy\" software product line representation is considered an important software re-engineering activity, as reflected in numerous tools and methodologies available. However, development practices of companies that use cloning to implement product lines have not been systematically studied. This lack of empirical knowledge threatens the validity and applicability of approaches supporting the transformation, and impedes adoption of advanced solutions for systematic software reuse. It also hinders the attempts to improve the solutions themselves. We address this gap with an empirical study conducted to investigate the cloning culture in six industrial software product lines realized via code cloning. Our study\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "247\n", "authors": ["34"]}
{"title": "Generative programming for embedded software: An industrial experience report\n", "abstract": " Physical products come in many variants, and so does the software embedded in them. The software embedded in a product variant usually has to be optimized to fit its limited memory and computing power. Generative programming is well suited for developing embedded software since it allows us to automatically produce variants of embedded software optimized for specific products. This paper reports on our experience in applying generative programming in the embedded domain. We propose an extended feature modeling notation, discuss tool support for feature modeling, describe a domain-independent system configuration editor, and comment on the applicability of static configuration in the area of embedded systems.", "num_citations": "232\n", "authors": ["34"]}
{"title": "Feature models are views on ontologies\n", "abstract": " Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches", "num_citations": "220\n", "authors": ["34"]}
{"title": "Evolution of the Linux kernel variability model\n", "abstract": " Understanding the challenges faced by real projects in evolving variability models, is a prerequisite for providing adequate support for such undertakings. We study the evolution of a model describing features and configurations in a large product line\u0393\u00c7\u00f6the Linux kernel variability model. We analyze this evolution quantitatively and qualitatively.               Our primary finding is that the Linux kernel model appears to evolve surprisingly smoothly. In the analyzed period, the number of features had doubled, and still the structural complexity of the model remained roughly the same. Furthermore, we provide an in-depth look at the effect of the kernel\u0393\u00c7\u00d6s development methodologies on the evolution of its model. We also include evidence about edit operations applied in practice, evidence of challenges in maintaining large models, and a range of recommendations (and open problems) for builders of modeling tools.", "num_citations": "202\n", "authors": ["34"]}
{"title": "Components and generative programming\n", "abstract": " This paper is about a paradigm shift from the current practice of manually searching for and adapting components and their manual assembly to Generative Programming, which is the automatic selection and assembly of components on demand. First, we argue that the current OO technology does not support reuse and configurability in an effective way. Then we show how a system family approach can aid in defining reusable components. Finally, we describe how automate the assembly of components based on configuration knowledge. We compare this paradigm shift to the introduction of interchangeable parts and automated assembly lines in the automobile industry.             We also illustrate the steps necessary to develop a product line using a simple example of a car product line. We present the feature model of the product line, develop a layered architecture for it, and automate the assembly of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "197\n", "authors": ["34"]}
{"title": "From state-to delta-based bidirectional model transformations: The symmetric case\n", "abstract": " A bidirectional transformation (BX) keeps a pair of interrelated models synchronized. Symmetric BXs are those for which neither model in the pair fully determines the other. We build two algebraic frameworks for symmetric BXs, with one correctly implementing the other, and both being delta-based generalizations of known state-based frameworks. We identify two new algebraic laws-weak undoability and weak invertibility, which capture important semantics of BX and are useful for both state- and delta-based settings. Our approach also provides a flexible tool architecture adaptable to different user\u0393\u00c7\u00d6s needs.", "num_citations": "189\n", "authors": ["34"]}
{"title": "Learning rate based branching heuristic for SAT solvers\n", "abstract": " In this paper, we propose a framework for viewing solver branching heuristics as optimization algorithms where the objective is to maximize the learning rate, defined as the propensity for variables to generate learnt clauses. By viewing online variable selection in SAT solvers as an optimization problem, we can leverage a wide variety of optimization algorithms, especially from machine learning, to design effective branching heuristics. In particular, we model the variable selection optimization problem as an online multi-armed bandit, a special-case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized. We develop a branching heuristic that we call learning rate branching or LRB, based on a well-known multi-armed bandit algorithm called exponential recency weighted average and implement it as part of MiniSat and CryptoMiniSat. We upgrade the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "182\n", "authors": ["34"]}
{"title": "A study of variability models and languages in the systems software domain\n", "abstract": " Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "175\n", "authors": ["34"]}
{"title": "Feature and meta-models in Clafer: mixed, specialized, and coupled\n", "abstract": " We present Clafer, a meta-modeling language with first-class support for feature modeling. We designed Clafer as a concise notation for meta-models, feature models, mixtures of meta- and feature models (such as components with options), and models that couple feature models and meta-models via constraints (such as mapping feature configurations to component configurations or model templates). Clafer also allows arranging models into multiple specialization and extension layers via constraints and inheritance. We identify four key mechanisms allowing a meta-modeling language to express feature models concisely and show that Clafer meets its design objectives using a sample product line. We evaluated Clafer and how it lends itself to analysis on sample feature models, meta-models, and model templates of an E-Commerce platform.", "num_citations": "163\n", "authors": ["34"]}
{"title": "Variability-aware performance prediction: A statistical learning approach\n", "abstract": " Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["34"]}
{"title": "Mining configuration constraints: Static analyses and empirical results\n", "abstract": " Highly-configurable systems allow users to tailor the software to their specific needs. Not all combinations of configuration options are valid though, and constraints arise for technical or non-technical reasons. Explicitly describing these constraints in a variability model allows reasoning about the supported configurations. To automate creating variability models, we need to identify the origin of such configuration constraints. We propose an approach which uses build-time errors and a novel feature-effect heuristic to automatically extract configuration constraints from C code. We conduct an empirical study on four highly-configurable open-source systems with existing variability models having three objectives in mind: evaluate the accuracy of our approach, determine the recoverability of existing variability-model constraints using our analysis, and classify the sources of variability-model constraints. We find that both\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "147\n", "authors": ["34"]}
{"title": "DSL implementation in MetaOCaml, Template Haskell, and C++\n", "abstract": " A wide range of domain-specific languages (DSLs) has been implemented successfully by embedding them in general purpose languages. This paper reviews embedding, and summarizes how two alternative techniques \u0393\u00c7\u00f4 staged interpreters and templates \u0393\u00c7\u00f4 can be used to overcome the limitations of embedding. Both techniques involve a form of generative programming. The paper reviews and compares three programming languages that have special support for generative programming. Two of these languages (MetaOCaml and Template Haskell) are research languages, while the third (C++) is already in wide industrial use. The paper identifies several dimensions that can serve as a basis for comparing generative languages.", "num_citations": "142\n", "authors": ["34"]}
{"title": "Cost-efficient sampling for performance prediction of configurable systems (t)\n", "abstract": " A key challenge of the development and maintenanceof configurable systems is to predict the performance ofindividual system variants based on the features selected. It isusually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predictperformance based on small samples of measured variants, butit is still open how to dynamically determine an ideal samplethat balances prediction accuracy and measurement effort. Inthis paper, we adapt two widely-used sampling strategies forperformance prediction to the domain of configurable systemsand evaluate them in terms of sampling cost, which considersprediction accuracy and measurement effort simultaneously. Togenerate an initial sample, we introduce a new heuristic based onfeature frequencies and compare it to a traditional method basedon t-way feature coverage. We conduct experiments on six\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "141\n", "authors": ["34"]}
{"title": "Urban Driving with Multi-Objective Deep Reinforcement Learning\n", "abstract": " Autonomous driving is a challenging domain that entails multiple aspects: a vehicle should be able to drive to its destination as fast as possible while avoiding collision, obeying traffic rules and ensuring the comfort of passengers. In this paper, we present a deep learning variant of thresholded lexicographic Q-learning for the task of urban driving. Our multi-objective DQN agent learns to drive on multi-lane roads and intersections, yielding and changing lanes according to traffic rules. We also propose an extension for factored Markov Decision Processes to the DQN architecture that provides auxiliary features for the Q function. This is shown to significantly improve data efficiency. We then show that the learned policy is able to zero-shot transfer to a ring road without sacrificing performance.", "num_citations": "125\n", "authors": ["34"]}
{"title": "Recommending refactorings to reverse software architecture erosion\n", "abstract": " Architectural erosion is a recurrent problem faced by software architects. Despite this fact, the process is usually tackled in ad hoc way, without adequate tool support at the architecture level. To address this issue, we describe the preliminary design of a recommendation system whose main purpose is to provide refactoring guidelines for developers and maintainers during the task of reversing an architectural erosion process. The paper formally describes first recommendations proposed in our current research and results of their application in a web-based application.", "num_citations": "125\n", "authors": ["34"]}
{"title": "Framework-specific modeling languages with round-trip engineering\n", "abstract": " We propose Framework-Specific Modeling Languages (FSMLs) as a special category of Domain-Specific Modeling Languages that are defined on top of an object-oriented application framework. They are used to express models showing how framework-provided abstractions are used in framework-based application code. Such models may be connected with the application code through a forward and a reverse mapping enabling round-trip engineering. We also propose a lightweight and iterative approach to round-trip engineering. Furthermore, we present a proof-of-concept FSML for modeling the interaction of workbench parts within Eclipse. Finally, we identify a number of challenges, opportunities, and directions for future research on FSMLs.", "num_citations": "121\n", "authors": ["34"]}
{"title": "Generating range fixes for software configuration\n", "abstract": " To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix, failing to cover the solution that the user wants; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from five open source projects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["34"]}
{"title": "Modelling the \u0393\u00c7\u00ffhurried\u0393\u00c7\u00d6bug report reading process to summarize bug reports\n", "abstract": " Although bug reports are frequently consulted project assets, they are communication logs, by-products of bug resolution, and not artifacts created with the intent of being easy to follow. To facilitate bug report digestion, we propose a new, unsupervised, bug report summarization approach that estimates the attention a user would hypothetically give to different sentences in a bug report, when pressed with time. We pose three hypotheses on what makes a sentence relevant: discussing frequently discussed topics, being evaluated or assessed by other sentences, and keeping focused on the bug report\u0393\u00c7\u00d6s title and description. Our results suggest that our hypotheses are valid, since the summaries have as much as 12 % improvement in standard summarization evaluation metrics compared to the previous approach. Our evaluation also asks developers to assess the quality and usefulness of the summaries\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "116\n", "authors": ["34"]}
{"title": "Clafer: unifying class and feature modeling\n", "abstract": " We present Clafer (class, feature, reference), a class modeling language with first-class support for feature modeling. We designed Clafer as a concise notation for meta-models, feature models, mixtures of meta- and feature models (such as components with options), and models that couple feature models and meta-models via constraints (such as mapping feature configurations to component configurations or model templates). Clafer allows arranging models into multiple specialization and extension layers via constraints and inheritance. We identify several key mechanisms allowing a meta-modeling language to express feature models concisely. Clafer unifies basic modeling constructs, such as class, association, and property, into a single construct, called clafer. We provide the language with a formal semantics built in a structurally explicit way. The resulting semantics explains the meaning of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["34"]}
{"title": "An analysis of ISO 26262: Using machine learning safely in automotive software\n", "abstract": " Machine learning (ML) plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation; however, its adequacy from the perspective of safety certification remains controversial. In this paper, we analyze the impacts that the use of ML as an implementation approach has on ISO 26262 safety lifecycle and ask what could be done to address them. We then provide a set of recommendations on how to adapt the standard to accommodate ML.", "num_citations": "113\n", "authors": ["34"]}
{"title": "Generative programming\n", "abstract": " This report describes the results of a one-day workshop on Generative Programming (GP) at ECOOP\u0393\u00c7\u00d602. The goal of the workshop was to discuss the state-of-the-art of generative techniques, share experience, consolidate successful techniques, and identify open issues for future work. This report gives a summary of the workshop contributions, debates, and the identified future directions.", "num_citations": "110\n", "authors": ["34"]}
{"title": "Model-driven software product lines\n", "abstract": " Model-driven software product lines combine the abstraction capability of Model Driven Software Development (MDSD) and the variability management capability of Software Product Line Engineering (SPLE). This short contribution motivates the idea of model-driven software product lines and briefly explains the concepts underlying feature-based model templates, which is a particular technique for modeling software product lines.", "num_citations": "97\n", "authors": ["34"]}
{"title": "Design space of heterogeneous synchronization\n", "abstract": " This tutorial explores the design space of heterogeneous synchronization, which is concerned with establishing consistency among artifacts that conform to different schemas or are expressed in different languages. Our main application scenario is synchronization of software artifacts, such as code, models, and configuration files. We classify heterogeneous synchronizers according to the cardinality of the relation that they enforce between artifacts, their directionality, their incrementality, and whether they support reconciliation of concurrent updates. We then provide a framework of artifact operators that describes different ways of building heterogeneous synchronizers, such as synchronizers based on artifact or update translation. The design decisions within the framework are described using feature models. We present 16 concrete instances of the framework, discuss tradeoffs among them, and identify\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["34"]}
{"title": "Where do configuration constraints stem from? an extraction approach and an empirical study\n", "abstract": " Highly configurable systems allow users to tailor software to specific needs. Valid combinations of configuration options are often restricted by intricate constraints. Describing options and constraints in a variability model allows reasoning about the supported configurations. To automate creating and verifying such models, we need to identify the origin of such constraints. We propose a static analysis approach, based on two rules, to extract configuration constraints from code. We apply it on four highly configurable systems to evaluate the accuracy of our approach and to determine which constraints are recoverable from the code. We find that our approach is highly accurate (93% and 77% respectively) and that we can recover 28% of existing constraints. We complement our approach with a qualitative study to identify constraint sources, triangulating results from our automatic extraction, manual inspections, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "91\n", "authors": ["34"]}
{"title": "Variability mechanisms in software ecosystems\n", "abstract": " ContextSoftware ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms.ObjectiveVariability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems.MethodWe analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["34"]}
{"title": "Correctness of model synchronization based on triple graph grammars\n", "abstract": " Triple graph grammars (TGGs) have been used successfully to analyze correctness and completeness of bidirectional model transformations, but a corresponding formal approach to model synchronization has been missing. This paper closes this gap by providing a formal synchronization framework with bidirectional update propagation operations. They are generated from a TGG, which specifies the language of all consistently integrated source and target models.               As a main result, we show that the generated synchronization framework is correct and complete, provided that forward and backward propagation operations are deterministic. Correctness essentially means that the propagation operations preserve consistency. Moreover, we analyze the conditions under which the operations are inverse to each other. All constructions and results are motivated and explained by a small running\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["34"]}
{"title": "Comparison of exact and approximate multi-objective optimization for software product lines\n", "abstract": " Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (eg., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["34"]}
{"title": "Clafer tools for product line engineering\n", "abstract": " Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.", "num_citations": "78\n", "authors": ["34"]}
{"title": "A user survey of configuration challenges in Linux and eCos\n", "abstract": " Operating systems expose sophisticated configurability to handle variability in hardware platforms like mobile devices, desktops, and servers. The variability model of an operating system kernel like Linux contains thousands of options guarded by hundreds of complex constraints. To guide users throughout the configuration and ensure the validity of their decisions, specialized tools known as configurators have been developed. Despite these tools, configuration still remains a difficult and challenging process. To better understand the challenges faced by users during configuration, we conducted two surveys, one among Linux users and another among eCos users. This paper presents the results of the surveys along three dimensions: configuration practice; user guidance; and language expressiveness. We hope that these results will help researchers and tool builders focus their efforts to improve tool support for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["34"]}
{"title": "Exponential recency weighted average branching heuristic for SAT solvers\n", "abstract": " Modern conflict-driven clause-learning SAT solvers routinely solve large real-world instances with millions of clauses and variables in them. Their success crucially depends on effective branching heuristics. In this paper, we propose a new branching heuristic inspired by the exponential recency weighted average algorithm used to solve the bandit problem. The branching heuristic, we call CHB, learns online which variables to branch on by leveraging the feedback received from conflict analysis. We evaluated CHB on 1200 instances from the SAT Competition 2013 and 2014 instances, and showed that CHB solves significantly more instances than VSIDS, currently the most effective branching heuristic in widespread use. More precisely, we implemented CHB as part of the MiniSat and Glucose solvers, and performed an apple-to-apple comparison with their VSIDS-based variants. CHB-based MiniSat (resp. CHB-based Glucose) solved approximately 16.1%(resp. 5.6%) more instances than their VSIDS-based variants. Additionally, CHB-based solvers are much more efficient at constructing first preimage attacks on step-reduced SHA-1 and MD5 cryptographic hash functions, than their VSIDS-based counterparts. To the best of our knowledge, CHB is the first branching heuristic to solve significantly more instances than VSIDS on a large, diverse benchmark of real-world instances.", "num_citations": "74\n", "authors": ["34"]}
{"title": "Engineering of framework-specific modeling languages\n", "abstract": " Framework-specific modeling languages (FSMLs) help developers build applications based on object-oriented frameworks. FSMLs model abstractions and rules of application programming interfaces (APIs) exposed by frameworks and can express models of how applications use APIs. Such models aid developers in understanding, creating, and evolving application code. We present four exemplar FSMLs and a method for engineering new FSMLs. The method was created postmortem by generalizing the experience of building the exemplars and by specializing existing approaches to domain analysis, software development, and quality evaluation of models and languages. The method is driven by the use cases that the FSML under development should support and the evaluation of the constructed FSML is guided by two existing quality frameworks. The method description provides concrete examples for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["34"]}
{"title": "Synthesizing objects\n", "abstract": " This paper argues that the current OO technology does not support reuse and configurability in an effective way. This problem can be addressed by augmenting OO analysis and design with feature modeling and by applying generative implementation techniques. Feature modeling allows capturing the variability of domain concepts. Concrete concept instances can then be synthesized from abstract specifications.               Using a simple example of a configurable list component, we demonstrate the application of feature modeling and how to implement a feature model as a generator. We introduce the concepts of configuration repositories and configuration generators and show how to implement them using object-oriented, generic, and generative language mechanisms. The configuration generator utilizes C++ template metaprogramming, which enables its execution at compile-time.", "num_citations": "73\n", "authors": ["34"]}
{"title": "Performance prediction of configurable software systems by fourier learning (t)\n", "abstract": " Understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant. Given a software system with n optional features, measuring all its 2 n  possible configurations to determine their performances is usually infeasible. Thus, various techniques have been proposed to predict software performances based on a small sample of measured configurations. We propose a novel algorithm based on Fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user, while using minimum number of samples up to a constant factor. Empirical results on the case studies constructed from real-world configurable systems demonstrate the effectiveness of our algorithm.", "num_citations": "72\n", "authors": ["34"]}
{"title": "Visualization and exploration of optimal variants in product line engineering\n", "abstract": " The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (ie., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["34"]}
{"title": "Data-efficient performance learning for configurable systems\n", "abstract": " Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["34"]}
{"title": "Modelling and multi-objective optimization of quality attributes in variability-rich software\n", "abstract": " Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["34"]}
{"title": "Efficient synthesis of feature models\n", "abstract": " ContextVariability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, creating feature models from large, legacy systems is a long and arduous task. We describe three synthesis scenarios that can benefit from the algorithms in this paper.ObjectiveThis paper addresses the problem of automatic synthesis of feature models from propositional constraints. We show that the decision version of the problem is NP-hard. We designed two efficient algorithms for synthesis of feature models from CNF and DNF formulas respectively.MethodWe performed an experimental evaluation of the algorithms against a binary decision diagram (BDD)-based approach and a formal concept analysis (FCA)-based approach using models derived from realistic models.ResultsOur evaluation shows a 10 to 1,000-fold\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["34"]}
{"title": "Synchronizing cardinality-based feature models and their specializations\n", "abstract": " A software product line comprises a set of products implementing different configurations of features. The set of valid feature configurations within a product line can be described by a feature model. In some practical situations, a feature configuration needs to be derived in stages by creating a series of successive specializations of the initial feature model. In this paper, we consider the scenario where changes to the feature model due to, for example, the evolution of the product line, need to be propagated to its existing specializations and configurations. After discussing general dimensions of model synchronization, a solution to synchronizing cardinality-based feature models and their specializations and configurations is presented.", "num_citations": "65\n", "authors": ["34"]}
{"title": "Maintaining feature traceability with embedded annotations\n", "abstract": " Features are commonly used to describe functional and nonfunctional aspects of software. To effectively evolve and reuse features, their location in software assets has to be known. However, locating features is often difficult given their crosscutting nature. Once implemented, the knowledge about a feature's location quickly deteriorates, requiring expensive recovering of these locations. Manually recording and maintaining traceability information is generally considered expensive and error-prone. In this paper, we argue to the contrary and hypothesize that such information can be effectively embedded into software assets, and that arising costs will be amortized by the benefits of this information later during development. We test this hypothesis in a study where we simulate the development of a product line of cloned/forked projects using a lightweight code annotation approach. We identify annotation evolution\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["34"]}
{"title": "Three cases of feature-based variability modeling in industry\n", "abstract": " Large software product lines need to manage complex variability. A common approach is variability modeling\u0393\u00c7\u00f6creating and maintaining models that abstract over the variabilities inherent in such systems. While many variability modeling techniques and notations have been proposed, little is known about industrial practices and how industry values or criticizes this class of modeling. We attempt to address this gap with an exploratory case study of three companies that apply variability modeling. Among others, our study shows that variability models are valued for their capability to organize knowledge and to achieve an overview understanding of codebases. We observe centralized model governance, pragmatic versioning, and surprisingly little constraint modeling, indicating that the effort of declaring and maintaining constraints does not always pay off.", "num_citations": "62\n", "authors": ["34"]}
{"title": "Generative programming: Methods, techniques, and applications tutorial abstract\n", "abstract": " Today\u0393\u00c7\u00d6s software engineering practices are aimed at developing single systems. There are attempts to achieve reuse through object- and component-based technologies with two specific goals: to cut development costs, and time-to-market and to improve quality. But current research and practical experience suggest that only moving from the single system engineering to the system-family engineering approach can bring significant progress with respect to these goals [4, 8, 10].", "num_citations": "62\n", "authors": ["34"]}
{"title": "Model synchronization based on triple graph grammars: correctness, completeness and invertibility\n", "abstract": " Triple graph grammars (TGGs) have been used successfully to analyze correctness and completeness of bidirectional model transformations, but a corresponding formal approach to model synchronization has been missing. This paper closes this gap by providing a formal synchronization framework with bidirectional update propagation operations. They are generated from a given TGG, which specifies the language of all consistently integrated source and target models. As our main result, we show that the generated synchronization framework is correct and complete, provided that forward and backward propagation operations are deterministic. Correctness essentially means that the propagation operations preserve and establish consistency while completeness ensures that the operations are defined for all possible inputs. Moreover, we analyze the conditions under which the operations are inverse to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["34"]}
{"title": "A model for structuring user documentation of object-oriented frameworks using patterns and hypertext\n", "abstract": " Adequate documentation of an object-oriented framework is the prerequisite to its success as a reusable component. The overall design of a framework and its intended method of reuse are not obvious from the source code and thus have to be addressed in the documentation. Most importantly, the documentation of a framework has to be structured in such a way that it guarantees the adequate support of three major audiences: users selecting a framework, users learning to develop typical applications based on the selected framework, and users intending to modify its architecture.             This paper presents a model for structuring the documentation of an objectoriented framework. The model integrates existing approaches such as patterns, hypertext, program-understanding tools, and formal approaches into a single structure that is geared towards supporting the three audiences. The model will be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["34"]}
{"title": "Swing to SWT and back: Patterns for API migration by wrapping\n", "abstract": " Evolving requirements may necessitate API migration-re-engineering an application to replace its dependence on one API with the dependence on another API for the same domain. One approach to API migration is to replace the original API by a wrapper-based re-implementation that makes reuse of the other API. Wrapper-based migration is attractive because application code is left untouched and wrappers can be reused across applications. The design of such wrappers is challenging though if the two involved APIs were developed independently, in which case the APIs tend to differ significantly. We identify the challenges faced by developers when designing wrappers for object-oriented APIs, and we recover the solutions used in practice. To this end, we analyze two large, open-source GUI wrappers and compile a set of issues pervasive in their designs. We subsequently extract design patterns from the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["34"]}
{"title": "Coevolution of variability models and related software artifacts\n", "abstract": " Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["34"]}
{"title": "Matching business process workflows across abstraction levels\n", "abstract": " In Business Process Modeling, several models are defined for the same system, supporting the transition from business requirements to IT implementations. Each of these models targets a different abstraction level and stakeholder perspective. In order to maintain consistency among these models, which has become a major challenge not only in this field, the correspondence between them has to be identified. A correspondence between process models establishes which activities in one model correspond to which activities in another model. This paper presents an algorithm for determining such correspondences. The algorithm is based on an empirical study of process models at a large company in the banking sector, which revealed frequent correspondence patterns between models spanning multiple abstraction levels. The algorithm has two phases, first establishing correspondences based on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["34"]}
{"title": "A solution to the constructor-problem of mixin-based programming in C++\n", "abstract": " Mixin-Based Programming in C++ is a powerful programming style based on the parameterized inheritance idiom and the composition of C++ templates. Type expressions describing specific inheritance hierarchies can be composed either automatically using generative programming idioms in C++ or manually. Unfortunately, the mixin-based C++ programming techniques published to date do not adequately support optional and alternative mixin classes with constructors expecting varying numbers of arguments, which are common in practice. This is because the varying base class constructors do not provide a uniform interface on which the constructors of the derived classes could rely. This paper discusses several partial solutions to this problem that were proposed to date and presents a new, complete solution. The new solution uses generative programming techniques to automatically generate the appropriate constructors, and this way it avoids the overhead and clumsiness of instantiating composed mixin classes in the client code using the partial solutions. In fact, the new solution allows users to instantiate automatically composed mixin classes with the simplicity of instantiating concrete classes from traditional class hierarchies. Finally, the new solution does not suffer from the scalability problems of the partial solutions.", "num_citations": "49\n", "authors": ["34"]}
{"title": "SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines\n", "abstract": " A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.", "num_citations": "48\n", "authors": ["34"]}
{"title": "Fantrack: 3d multi-object tracking with feature association network\n", "abstract": " We propose a data-driven approach to online multi-object tracking (MOT) that uses a convolutional neural network (CNN) for data association in a tracking-by-detection framework. The problem of multi-target tracking aims to assign noisy detections to a-priori unknown and time-varying number of tracked objects across a sequence of frames. A majority of the existing solutions focus on either tediously designing cost functions or formulating the task of data association as a complex optimization problem that can be solved effectively. Instead, we exploit the power of deep learning to formulate the data association problem as inference in a CNN. To this end, we propose to learn a similarity function that combines cues from both image and spatial features of objects. Our solution learns to perform global assignments in 3D purely from data, handles noisy detections and varying number of targets, and is easy to train. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["34"]}
{"title": "Transferring performance prediction models across different hardware platforms\n", "abstract": " Many software systems provide configuration options relevant to users, which are often called features. Features influence functional properties of software systems as well as non-functional ones, such as performance and memory consumption. Researchers have successfully demonstrated the correlation between feature selection and performance. However, the generality of these performance models across different hardware platforms has not yet been evaluated.", "num_citations": "47\n", "authors": ["34"]}
{"title": "Sat-based analysis of large real-world feature models is easy\n", "abstract": " Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, ie, the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["34"]}
{"title": "A recommendation system for repairing violations detected by static architecture conformance checking\n", "abstract": " This paper describes a recommendation system that provides refactoring guidelines for maintainers when tackling architectural erosion. The paper formalizes 32\u0393\u00c7\u00ebrefactoring recommendations to repair violations raised by static architecture conformance checking approaches; it describes a tool\u0393\u00c7\u00f6called ArchFix\u0393\u00c7\u00f6that triggers the proposed recommendations; and it evaluates the application of this tool in two industrial\u0393\u00c7\u00c9strength systems. For the first system\u0393\u00c7\u00f6a 21 KLOC open\u0393\u00c7\u00c9source strategic management system\u0393\u00c7\u00f6our approach has indicated correct refactoring recommendations for 31 out of 41 violations detected as the result of an architecture conformance process. For the second system\u0393\u00c7\u00f6a 728 KLOC customer care system used by a major telecommunication company\u0393\u00c7\u00f6our approach has triggered correct recommendations for 624 out of 787 violations, as asserted by the system's architect. Moreover, the architects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "45\n", "authors": ["34"]}
{"title": "Intermodeling, queries, and kleisli categories\n", "abstract": " Specification and maintenance of relationships between models are vital for MDE. We show that a wide class of such relationships can be specified in a compact and precise manner, if intermodel mappings are allowed to link derived model elements computed by corresponding queries. Composition of such mappings is not straightforward and requires specialized algebraic machinery. We present a formal framework, in which such machinery can be defined generically for a wide class of metamodel definitions. This enables algebraic specification of practical intermodeling scenarios, e.g., model merge.", "num_citations": "45\n", "authors": ["34"]}
{"title": "Understanding VSIDS branching heuristics in conflict-driven clause-learning SAT solvers\n", "abstract": " Conflict-Driven Clause-Learning (CDCL) SAT solvers crucially depend on the Variable State Independent Decaying Sum (VSIDS) branching heuristic for their performance. Although VSIDS was proposed nearly fifteen years ago, and many other branching heuristics for SAT solving have since been proposed, VSIDS remains one of the most effective branching heuristics. Despite its widespread use and repeated attempts to understand it, this additive bumping and multiplicative decay branching heuristic has remained an enigma.                 In this paper, we advance our understanding of VSIDS by answering the following key questions. The first question we pose is \u0393\u00c7\u00a3what is special about the class of variables that VSIDS chooses to additively bump?\u0393\u00c7\u00a5 In answering this question we showed that VSIDS overwhelmingly picks, bumps, and learns bridge variables, defined as the variables that connect distinct\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["34"]}
{"title": "Towards improving bug tracking systems with game mechanisms\n", "abstract": " Low bug report quality and human conflicts pose challenges to keep bug tracking systems productive. This work proposes to address these issues by applying game mechanisms to bug tracking systems. We investigate the use of game mechanisms in Stack Overflow, an online community organized to resolve computer programming related problems, for which the improvements we seek for bug tracking systems also turn out to be relevant. The results of our Stack Overflow investigation show that its game mechanisms could be used to address these issues by motivating contributors to increase contribution frequency and quality, by filtering useful contributions, and by creating an agile and dependable moderation system. We proceed by mapping these mechanisms to open-source bug tracking systems, and find that most benefits are applicable. Additionally, our results motivate tailoring a reward and reputation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["34"]}
{"title": "fmp and fmp2rsm: eclipse plug-ins for modeling features using model templates\n", "abstract": " Feature-based model templates have been proposed as a technique for modeling software product lines. We describe a set of tools supporting the technique, namely a feature model editor and feature configurator, and a model-template editor, processor, and verifier.", "num_citations": "43\n", "authors": ["34"]}
{"title": "Multi-level customization in application engineering\n", "abstract": " Developing mechanisms for mapping features to analysis models.", "num_citations": "40\n", "authors": ["34"]}
{"title": "A study of feature scattering in the linux kernel\n", "abstract": " Feature code is often scattered across a software system. Scattering is not necessarily bad if used with care, as witnessed by systems with highly scattered features that evolved successfully. Feature scattering, often realized with a pre-processor, circumvents limitations of programming languages and software architectures. Unfortunately, little is known about the principles governing scattering in large and long-living software systems. We present a longitudinal study of feature scattering in the Linux kernel, complemented by a survey with 74, and interviews with nine Linux kernel developers. We analyzed almost eight years of the kernel's history, focusing on its largest subsystem: device drivers. We learned that the ratio of scattered features remained nearly constant and that most features were introduced without scattering. Yet, scattering easily crosses subsystem boundaries, and highly scattered outliers exist\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["34"]}
{"title": "The shape of feature code: an analysis of twenty C-preprocessor-based systems\n", "abstract": " Feature annotations (e.g., code fragments guarded by #ifdef C-preprocessor directives) control code extensions related to features. Feature annotations have long been said to be undesirable. When maintaining features that control many annotations, there is a high risk of ripple effects. Also, excessive use of feature annotations leads to code clutter, hinder program comprehension and harden maintenance. To prevent such problems, developers should monitor the use of feature annotations, for example, by setting acceptable thresholds. Interestingly, little is known about how to extract thresholds in practice, and which values are representative for feature-related metrics. To address this issue, we analyze the statistical distribution of three feature-related metrics collected from a corpus of 20 well-known and long-lived C-preprocessor-based systems from different domains. We consider three metrics\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["34"]}
{"title": "Feature scattering in the large: a longitudinal study of Linux kernel device drivers\n", "abstract": " Feature code is often scattered across wide parts of the code base. But, scattering is not necessarily bad if used with care\u0393\u00c7\u00f6in fact, systems with highly scattered features have evolved successfully over years. Among others, feature scattering allows developers to circumvent limitations in programming languages and system design. Still, little is known about the characteristics governing scattering, which factors influence it, and practical limits in the evolution of large and long-lived systems. We address this issue with a longitudinal case study of feature scattering in the Linux kernel. We quantitatively and qualitatively analyze almost eight years of its development history, focusing on scattering of device-driver features. Among others, we show that, while scattered features are regularly added, their proportion is lower than non-scattered ones, indicating that the kernel architecture allows most features to be integrated in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["34"]}
{"title": "An empirical study of branching heuristics through the lens of global learning rate\n", "abstract": " In this paper, we analyze a suite of 7 well-known branching heuristics proposed by the SAT community and show that the better heuristics tend to generate more learnt clauses per decision, a metric we define as the global learning rate (GLR). Like our previous work on the LRB branching heuristic, we once again view these heuristics as techniques to solve the learning rate optimization problem. First, we show that there is a strong positive correlation between GLR and solver efficiency for a variety of branching heuristics. Second, we test our hypothesis further by developing a new branching heuristic that maximizes GLR greedily. We show empirically that this heuristic achieves very high GLR and interestingly very low literal block distance (LBD) over the learnt clauses. In our experiments this greedy branching heuristic enables the solver to solve instances faster than VSIDS, when the branching time is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["34"]}
{"title": "Beyond objects: Generative programming\n", "abstract": " The object-oriented paradigm constitutes a major advance in software engineering. Some of its main contributions include\u0393\u00c7\u00f3 improvement of understandability and better management of complexity: According to some theories in cognitive science, humans organize knowledge into sets of interrelated concepts (which are modeled as semantic nets or frames [Min75]). Since concepts can be modeled as objects quite adequately, the object-oriented paradigm allows us to build\" natural\" and better understandable models. Furthermore, encapsulation results in hiding unnecessary detail (ie it supports abstraction). Last but not least, objects can be organized into inheritance and containment hierarchies which capture the relationships between the corresponding concepts (see [Col95] for a detailed discussion of the interrelations between objects and cognitive psychology).\u0393\u00c7\u00f3 extensibility, adaptability, and reusability: Inheritance and overriding allow us to extend and modify objects. Polymorphism enables providing alternative implementations of components.\u0393\u00c7\u00f3 maintainability: Encapsulation helps to reduce coupling and polymorphism allows us to avoid errorprone type switches. A recent shift in the object-oriented community is the focus on developing reusable solutions for families of applications instead of developing single applications from scratch. Class libraries providing sets of classes to be reused in a larger application are being evolved into frameworks which allow us to reuse their components, flow of control, and the overall application structure. Additionally, frameworks provide sets of alternative components in order to allow for axes of variation (ie\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["34"]}
{"title": "Towards a framework to manage perceptual uncertainty for safe automated driving\n", "abstract": " Perception is a safety-critical function of autonomous vehicles and machine learning (ML) plays a key role in its implementation. This position paper identifies (1) perceptual uncertainty as a performance measure used to define safety requirements and (2) its influence factors when using supervised ML. This work is a first step towards a framework for measuring and controling the effects of these factors and supplying evidence to support claims about perceptual uncertainty.", "num_citations": "36\n", "authors": ["34"]}
{"title": "Range fixes: Interactive error resolution for software configuration\n", "abstract": " To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix or a very long fix list, difficult for the user to identify the desirable fix; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from nine\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["34"]}
{"title": "Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance\n", "abstract": " There is an increasingly apparent need for validating the classifications made by deep learning systems in safety-critical applications like autonomous vehicle systems. A number of recent papers have proposed methods for detecting anomalous image data that appear different from known inlier data samples, including reconstruction-based autoencoders. Autoencoders optimize the compression of input data to a latent space of a dimensionality smaller than the original input and attempt to accurately reconstruct the input using that compressed representation. Since the latent vector is optimized to capture the salient features from the inlier class only, it is commonly assumed that images of objects from outside of the training class cannot effectively be compressed and reconstructed. Some thus consider reconstruction error as a kind of novelty measure. Here we suggest that reconstruction-based approaches fail to capture particular anomalies that lie far from known inlier samples in latent space but near the latent dimension manifold defined by the parameters of the model. We propose incorporating the Mahalanobis distance in latent space to better capture these out-of-distribution samples and our results show that this method often improves performance over the baseline approach.", "num_citations": "34\n", "authors": ["34"]}
{"title": "A case study on consistency management of business and IT process models in banking\n", "abstract": " Organizations that adopt process modeling often maintain several co-existing models of the same business process. These models target different abstraction levels and stakeholder perspectives. Maintaining consistency among these models has become a major challenge for such organizations. Although several academic works have discussed this challenge, little empirical investigation exists on how people perform process model consistency management in practice. This paper aims to address this lack by presenting an in-depth empirical study of a business-driven engineering process deployed at a large company in the banking sector. We analyzed more than 70 business process models developed by the company, including their change history, with over 1,000 change requests. We also interviewed 9 business and IT practitioners and surveyed 23 such practitioners to understand concrete\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["34"]}
{"title": "Study of an API migration for two XML APIs\n", "abstract": " API migration refers to adapting an application such that its dependence on a given API (the source API) is eliminated in favor of depending on an alternative API (the target API) with the source and target APIs serving the same domain. One may attempt to automate API migration by code transformation or wrapping of some sort. API migration is relatively well understood for the special case where source and target APIs are essentially different versions of the same API. API migration is much less understood for the general case where the two APIs have been developed more or less independently of each other. The present paper exercises a simple instance of the general case and develops engineering techniques towards the mastery of API migration. That is, we study wrapper-based migration between two prominent XML APIs for the Java platform. The migration follows an iterative and test-driven\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["34"]}
{"title": "Automated model-based configuration of enterprise java applications\n", "abstract": " The decentralized process of configuring enterprise applications is complex and error-prone, involving multiple participants/roles and numerous configuration changes across multiple files, application server settings, and database decisions. This paper describes an approach to automated enterprise application configuration that uses a feature model, executes a series of probes to verify configuration properties, formalizes feature selection as a constraint satisfaction problem, and applies constraint logic programming techniques to derive a correct application configuration. To validate the approach, we developed a configuration engine, called Fresh, for enterprise Java applications and conducted experiments to measure how effectively Fresh can configure the canonical Java Pet Store application. Our results show that Fresh reduces the number of lines of hand written XML code by up to 92% and the total number\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["34"]}
{"title": "Canadian adverse driving conditions dataset\n", "abstract": " The Canadian Adverse Driving Conditions (CADC) dataset was collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ. The dataset, collected during winter within the Region of Waterloo, Canada, is the first autonomous driving dataset that focuses on adverse driving conditions specifically. It contains 7,000 frames of annotated data from 8 cameras (Ximea MQ013CG-E2), lidar (VLP-32C), and a GNSS+INS system (Novatel OEM638), collected through a variety of winter weather conditions. The sensors are time synchronized and calibrated with the intrinsic and extrinsic calibrations included in the dataset. Lidar frame annotations that represent ground truth for 3D object detection and tracking have been provided by Scale AI.", "num_citations": "33\n", "authors": ["34"]}
{"title": "Precise synthetic image and lidar (presil) dataset for autonomous vehicle perception\n", "abstract": " We introduce the Precise Synthetic Image and LiDAR (PreSIL) dataset for autonomous vehicle perception. Grand Theft Auto V (GTA V), a commercial video game, has a large detailed world with realistic graphics, which provides a diverse data collection environment. Existing works creating synthetic LiDAR data for autonomous driving with GTA V have not released their datasets, rely on an in-game raycasting function which represents people as cylinders, and can fail to capture vehicles past 30 metres. Our work creates a precise LiDAR simulator within GTA V which collides with detailed models for all entities no matter the type or position. The PreSIL dataset consists of over 50,000 frames and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), and detailed annotations for all vehicles and people. Collecting additional data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["34"]}
{"title": "An automated vehicle safety concept based on runtime restriction of the operational design domain\n", "abstract": " Automated vehicles need to operate safely in a wide range of environments and hazards. The complex systems that make up an automated vehicle must also ensure safety in the event of system failures. This paper proposes an approach and architectural design for achieving maximum functionality in the case of system failures. The Operational Design Domain (ODD) defines the domain over which the automated vehicle can operate safely. We propose modifying a runtime representation of the ODD based on current system capabilities. This enables the system to react with context-appropriate responses depending on the remaining degraded functionality. In addition to proposing an architectural design, we have implemented the approach to prove its viability. The proof of concept has shown promising directions for future work and moved our automated vehicle research platform closer to achieving level 4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["34"]}
{"title": "Using machine learning safely in automotive software: An assessment and adaption of software process requirements in ISO 26262\n", "abstract": " The use of machine learning (ML) is on the rise in many sectors of software development, and automotive software development is no different. In particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS) are two areas where ML plays a significant role. In automotive development, safety is a critical objective, and the emergence of standards such as ISO 26262 has helped focus industry practices to address safety in a systematic and consistent way. Unfortunately, these standards were not designed to accommodate technologies such as ML or the type of functionality that is provided by an ADS and this has created a conflict between the need to innovate and the need to improve safety. In this report, we take steps to address this conflict by doing a detailed assessment and adaption of ISO 26262 for ML, specifically in the context of supervised learning. First we analyze the key factors that are the source of the conflict. Then we assess each software development process requirement (Part 6 of ISO 26262) for applicability to ML. Where there are gaps, we propose new requirements to address the gaps. Finally we discuss the application of this adapted and extended variant of Part 6 to ML development scenarios.", "num_citations": "32\n", "authors": ["34"]}
{"title": "Empirical comparison of regression methods for variability-aware performance prediction\n", "abstract": " Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.", "num_citations": "32\n", "authors": ["34"]}
{"title": "A three-dimensional taxonomy for bidirectional model synchronization\n", "abstract": " Early model-driven engineering (MDE) assumed simple pipeline-like scenarios specified by the Model-Driven Architecture approach: platform-independent models that describe a software system at a high-level of abstraction are transformed stepwise to platform-dependent models from which executable source code is generated. Modern applications require a shift toward networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools, and a solid semantic foundation to understand and classify these features. We address the problem by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to a specific synchronization semantics with an underlying algebraic model and the respective requirements for the change propagation operations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["34"]}
{"title": "Maple-COMSPS, MapleCOMSPS LRB, MapleCOMSPS CHB\n", "abstract": " This document describes the SAT solvers Maple-COMSPS, MapleCOMSPS LRB, and MapleCOMSPS CHB, three solvers implementing our machine learning branching heuristics called the learning rate branching heuristic (LRB) and conflict history-based branching heuristic (CHB).", "num_citations": "30\n", "authors": ["34"]}
{"title": "Automatic extraction of framework-specific models from framework-based application code\n", "abstract": " Framework-specific models represent the design of pplicationcode from the framework viewpoint by showing how framework-provided concepts are implemented in the code. In this paper, we describe an experimental study of the static analyses necessary to automatically retrieve such models from application code. We reverse engineer a number of applications based on three open-source frameworks and evaluate the quality of the retrieved models. The models are expressed using framework-specific modeling languages (FSMLs), each designed for one of the open-source frameworks. For reverse engineering, we use prototype implementations of the three FSMLs. Our results show that for the considered frameworks rather simple code analysesare sufficient for automatically retrieving framework-specific models form a large body of application code with high precision and recall", "num_citations": "30\n", "authors": ["34"]}
{"title": "Towards a rational taxonomy for increasingly symmetric model synchronization\n", "abstract": " A pipeline of unidirectional model transformations is a wellunderstood architecture for model driven engineering tasks such as model compilation or view extraction. However, modern applications require a shift towards networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools and a solid semantic foundation. We address the latter by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to its set of synchronization requirements and a corresponding algebraic structure modeling the intended semantics. The space aims to help with identifying and communicating the right tool and theory for the synchronization problem at hand. It also intends to guide future theoretical and tool research.", "num_citations": "29\n", "authors": ["34"]}
{"title": "Requirements determination is unstoppable: an experience report\n", "abstract": " The paper describes the quotations gathered during interviews and focus groups during a consulting engagement to help the client improve its requirements engineering (RE) process. The paper describes also a model of the software lifecycle derived from a Michael Jackson quotation, a model that explains about 95% of the quotations that we gathered. In particular, it explains why basic requirements determination is unstoppable and how management attempts to stop RE lead to the phenomena that are described by the quotations and less than optimal requirements specifications.", "num_citations": "29\n", "authors": ["34"]}
{"title": "Combining SAT solvers with computer algebra systems to verify combinatorial conjectures\n", "abstract": " We present a method and an associated system, called MathCheck, that embeds the functionality of a computer algebra system (CAS) within the inner loop of a conflict-driven clause-learning SAT solver. SAT+CAS systems, \u251c\u00e1 la MathCheck, can be used as an assistant by mathematicians to either find counterexamples or finitely verify open universal conjectures on any mathematical topic (e.g., graph and number theory, algebra, geometry, etc.) supported by the underlying CAS. Such a SAT+CAS system combines the efficient search routines of modern SAT solvers, with the expressive power of CAS, thus complementing both. The key insight behind the power of the SAT+CAS combination is that the CAS system can help cut down the search-space of the SAT solver, by providing learned clauses that encode theory-specific lemmas, as it searches for a counterexample to the input conjecture (just like the T in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["34"]}
{"title": "Two studies of framework-usage templates extracted from dynamic traces\n", "abstract": " Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["34"]}
{"title": "Trajectory prediction of traffic agents at urban intersections through learned interactions\n", "abstract": " To navigate a complex urban environment, it is essential for autonomous vehicles to make educated assumptions and accurate predictions of the movement of other traffic agents. Beyond single object tracking, this task involves understanding behavior of other participants and predicting their trajectories. In this paper, we present a data-driven approach to learn the behavior of traffic agents at an intersection by observing several episodes of real-life scenarios captured through a static camera. We develop a feed-forward artificial neural network called the influence-network, which can simultaneously reason over the influence that agents and the environment have on each other. We compare it to an extension of popularly used Dynamic Bayesian Network. Based on data captured at a busy city intersection, we show that our model can predict trajectories of different classes of traffic agents with improved accuracy, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["34"]}
{"title": "Logical structure extraction from software requirements documents\n", "abstract": " Software requirements documents (SRDs) are often authored in general-purpose rich-text editors, such as MS Word. SRDs contain instances of logical structures, such as use case, business rule, and functional requirement. Automated recognition and extraction of these instances enables advanced requirements management features, such as automated traceability, template conformance checking, guided editing, and interoperability with requirements management tools such as RequisitePro. The variability in content and physical representation of these instances poses challenges to their accurate recognition and extraction. To address these challenges, we present a framework allowing 1) the specification of logical structures in terms of their content, textual rendering, and variability and 2) the extraction of instances of such structures from rich-text documents. Our evaluation involves 36 different logical structures\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["34"]}
{"title": "Domain engineering\n", "abstract": " Domain engineering (DE) is the systematic process of collecting, organizing, and storing past experience in building systems in a particular domain. This experience is captured in the form of reusable assets (i.e., reusable work products), such as documents, patterns, reusable models, components, generators, and domain\u0393\u00c7\u00c9specific languages. An additional goal of DE is to provide an infrastructure for reusing these assets (e.g., retrieval, qualification, dissemination, adaptation, and assembly) during application engineering, i.e., the process of building new systems. By definition, DE focuses on system families rather than single systems. This focus sets it apart from the more traditional software development models, which are geared toward developing one\u0393\u00c7\u00c9of\u0393\u00c7\u00c9a\u0393\u00c7\u00c9kind systems. As a result, DE is the approach of choice for developing product lines and system families, component libraries, frameworks, domain\u0393\u00c7\u00c9specific\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["34"]}
{"title": "Adaptive restart and CEGAR-based solver for inverting cryptographic hash functions\n", "abstract": " SAT solvers are increasingly being used for cryptanalysis of hash functions and symmetric encryption schemes. Inspired by this trend, we present MapleCrypt which is a SAT solver-based cryptanalysis tool for inverting hash functions. We reduce the hash function inversion problem for fixed targets into the satisfiability problem for Boolean logic, and use MapleCrypt to construct preimages for these targets. MapleCrypt has two key features, namely, a multi-armed bandit based adaptive restart (MABR) policy and a counterexample-guided abstraction refinement (CEGAR) technique. The MABR technique uses reinforcement learning to adaptively choose between different restart policies during the run of the solver. The CEGAR technique abstracts away certain steps of the input hash function, replacing them with the identity function, and verifies whether the solution constructed by MapleCrypt indeed hashes to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["34"]}
{"title": "Smartfixer: Fixing software configurations based on dynamic priorities\n", "abstract": " Large modern software systems are often organized as product lines, requiring specialists to configure variability models before delivering a product. Variability models capture both the commonality and variability of different products, and help detect the configurations errors. Existing approaches can recommend fixes for the errors automatically. However, the recommended fixes are sometimes large and complex, and existing approaches lack guidance to help users identify a desirable fix. This paper proposes an approach to provide such guidance using dynamic priorities. The basic idea is to first generate one fix, and then gradually reach the desirable fix based on user feedback. To this end, our approach (1) automatically translates user feedback into a set of implicit priority levels on configuration variables, using five priority assignment and adjustment strategies and (2) efficiently generates potential desirable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["34"]}
{"title": "Out-of-distribution detection in classifiers via generation\n", "abstract": " By design, discriminatively trained neural network classifiers produce reliable predictions only for in-distribution samples. For their real-world deployments, detecting out-of-distribution (OOD) samples is essential. Assuming OOD to be outside the closed boundary of in-distribution, typical neural classifiers do not contain the knowledge of this boundary for OOD detection during inference. There have been recent approaches to instill this knowledge in classifiers by explicitly training the classifier with OOD samples close to the in-distribution boundary. However, these generated samples fail to cover the entire in-distribution boundary effectively, thereby resulting in a sub-optimal OOD detector. In this paper, we analyze the feasibility of such approaches by investigating the complexity of producing such \"effective\" OOD samples. We also propose a novel algorithm to generate such samples using a manifold learning network (e.g., variational autoencoder) and then train an n+1 classifier for OOD detection, where the  class represents the OOD samples. We compare our approach against several recent classifier-based OOD detectors on MNIST and Fashion-MNIST datasets. Overall the proposed approach consistently performs better than the others.", "num_citations": "19\n", "authors": ["34"]}
{"title": "Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems\n", "abstract": " In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today\u0393\u00c7\u00d6s engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["34"]}
{"title": "MathCheck: A math assistant via a combination of computer algebra systems and SAT solvers\n", "abstract": " We present a method and an associated system, called MathCheck, that embeds the functionality of a computer algebra system (CAS) within the inner loop of a conflict-driven clause-learning SAT solver. SAT+ CAS systems, a la MathCheck, can be used as an assistant by mathematicians to either counterexample or finitely verify open universal conjectures on any mathematical topic (eg, graph and number theory, algebra, geometry, etc.) supported by the underlying CAS system. Such a SAT+ CAS system combines the efficient search routines of modern SAT solvers, with the expressive power of CAS, thus complementing both. The key insight behind the power of the SAT+ CAS combination is that the CAS system can help cut down the search-space of the SAT solver, by providing learned clauses that encode theory-specific lemmas, as it searches for a counterexample to the input conjecture (just like the T in DPLL\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["34"]}
{"title": "Formalizing cardinality-based feature models and their staged configuration\n", "abstract": " Feature modeling is an important approach to capture the commonalities and variabilities in system families and product lines. Cardinality-based feature modeling integrates a number of existing extensions of previous approaches. Staged configuration is a process that allows the incremental configuration of cardinality-based feature models. It is achieved by performing a step-wise specialization of the feature model.In this paper, we argue that cardinality-based feature models can be interpreted as a special class of context-free grammars. We make this precise by specifying a translation from a feature model into a contextfree grammar. Consequently, we provide a semantic interpretation for cardinality-based feature models by assigning an appropriate semantics to the language recognized by the corresponding grammar. Finally, we give an account on how feature model specialization can be formalized as transformations on the grammar equivalent of feature models.", "num_citations": "18\n", "authors": ["34"]}
{"title": "Keep calm and ride along: Passenger comfort and anxiety as physiological responses to autonomous driving styles\n", "abstract": " Autonomous vehicles have been rapidly progressing towards full autonomy using fixed driving styles, which may differ from individual passenger preferences. Violating these preferences may lead to passenger discomfort or anxiety. We studied passenger responses to different driving style parameters in a physical autonomous vehicle. We collected galvanic skin response, heart rate, and eye-movement patterns from 20 participants, along with self-reported comfort and anxiety scores. Our results show that the presence and proximity of a lead vehicle not only raised the level of all measured physiological responses, but also exaggerated the existing effect of the longitudinal acceleration and jerk parameters. Skin response was also found to be a significant predictor of passenger comfort and anxiety. By using multiple independent events to isolate different driving style parameters, we demonstrate a method to control\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["34"]}
{"title": "Supporting different process views through a shared process model\n", "abstract": " Different stakeholders in the business process management (BPM) life cycle benefit from having different views onto a particular process model. Each view can show, and offer to change, the details relevant to the particular stakeholder, leaving out the irrelevant ones. However, introducing different views on a process model entails the problem to synchronize changes in case that one view evolves. This problem is especially relevant and challenging for views at different abstraction levels. In this paper, we propose a Shared Process Model that provides different stakeholder views at different abstraction levels and synchronizes changes made to any view. We present detailed requirements and a solution design for the Shared Process Model. We also present an overview of our prototypical implementation to demonstrate the feasibility of the approach. Finally, we report on a comprehensive evaluation of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["34"]}
{"title": "Partial instances via subclassing\n", "abstract": " The traditional notion of instantiation in Object-Oriented Modeling (OOM) requires objects to be complete, i.e., be fully certain about their existence and attributes. This paper explores the notion of partial instantiation of class diagrams, which allows the modeler to omit some details of objects depending on modeler\u0393\u00c7\u00d6s intention. Partial instantiation allows modelers to express optional existence of some objects and slots (links) as well as uncertainty of values in some slots. We show that partial instantiation is useful and natural in domain modeling and requirements engineering. It is equally useful in architecture modeling with uncertainty (for design exploration) and with variability (for modeling software product lines).               Partial object diagrams can be (partially) completed by resolving (some of) optional objects and replacing (some of) unknown values with actual ones. Under the Closed World Assumption\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["34"]}
{"title": "Maple-comsps lrb vsids and maplecomsps chb vsids\n", "abstract": " This document describes the SAT solvers Maple-COMSPS LRB VSIDS and MapleCOMSPS CHB VSIDS that implement our machine learning branching heuristics called the learning rate branching heuristic (LRB) and the conflict historybased branching heuristic (CHB).", "num_citations": "16\n", "authors": ["34"]}
{"title": "MathCheck2: A SAT+CAS Verifier for Combinatorial Conjectures\n", "abstract": " In this paper we present MathCheck2, a tool which combines sophisticated search procedures of current SAT solvers with domain specific knowledge provided by algorithms implemented in computer algebra systems (CAS). MathCheck2 is aimed to finitely verify or to find counterexamples to mathematical conjectures, building on our previous work on the MathCheck system. Using MathCheck2 we validated the Hadamard conjecture from design theory for matrices up\u252c\u00e1to rank 136 and a few additional ranks up\u252c\u00e1to 156. Also, we provide an independent verification of the claim that Williamson matrices of order 35 do not exist, and demonstrate for the first time that 35 is the smallest number with this property. Finally, we provided more than 160 matrices to the Magma Hadamard database that are not equivalent to any matrices previously included in that database.", "num_citations": "16\n", "authors": ["34"]}
{"title": "Modeling aerospace systems product lines in SysML\n", "abstract": " As the complexity of avionic systems increases, the aerospace industry is turning to product-line engineering and model-based development to better manage complexity and reduce cost. This paper describes a method and a pattern catalog for modeling avionics product lines in SysML, a standard systems modeling language. The method is designed to satisfy aerospace systems and software development standards, and the patterns provide guidance for expressing variability in SysML. The paper also reports on the experience in applying the method and the patterns to model families of propeller controllers and fuel controllers for turbo engines.", "num_citations": "16\n", "authors": ["34"]}
{"title": "Automated decomposition and allocation of automotive safety integrity levels using exact solvers\n", "abstract": " The number of software-intensive and complex electronic automotive systems is continuously increasing. Many of these systems are safety-critical and pose growing safety-related concerns. ISO 26262 is the automotive functional safety standard developed for the passenger car industry. It provides guidelines to reduce and control the risk associated with safety-critical systems that include electric and (programmable) electronic parts. The standard uses the concept of Automotive Safety Integrity Levels (ASILs) to decompose and allocate safety requirements of different stringencies to the elements of a system architecture in a top-down manner: ASILs are assigned to system-level hazards, and then they are iteratively decomposed and allocated to relevant subsystems and components. ASIL decomposition rules may give rise to multiple alternative allocations, leading to an optimization problem of finding the cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["34"]}
{"title": "Effects of using examples on structural model comprehension: a controlled experiment\n", "abstract": " We present a controlled experiment for the empirical evaluation of Example-Driven Modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate and undergraduate students from electrical and computer engineering (ECE), computer science (CS), and software engineering (SE) programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples in model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model abstractions only, with an improvement of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["34"]}
{"title": "Unlimited road-scene synthetic annotation (URSA) dataset\n", "abstract": " In training deep neural networks for semantic segmentation, the main limiting factor is the low amount of ground truth annotation data that is available in currently existing datasets. The limited availability of such data is due to the time cost and human effort required to accurately and consistently label real images on a pixel level. Modern sandbox video game engines provide open world environments where traffic and pedestrians behave in a pseudo-realistic manner. This caters well to the collection of a believable road-scene dataset. Utilizing open-source tools and resources found in single-player modding communities, we provide a method for persistent, ground truth, asset annotation of a game world. By collecting a synthetic dataset containing upwards of 1, 000, 000 images, we demonstrate realtime, on-demand, ground truth data annotation capability of our method. Supplementing this synthetic data to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["34"]}
{"title": "Operational world model ontology for automated driving systems\u0393\u00c7\u00f4part 1: Road structure\n", "abstract": " This document is Part 1 of an ontology definition for specifying operational world models for an Automated Driving System (ADS). Part 1 defines a road structure ontology, which covers road types, road surface, road geometry, cross-\u0393\u00c7\u00c9section design, traffic control devices, pedestrian and cycling facilities, junctions, railroad level crossings, bridges and tunnels, driveways, and temporary road structure. The ontology can be used to define the Operational Design Domain (ODD) for an ADS. Other uses of the ontology are ADS behavior specification, test development and coverage assessment, labeling and indexing of datasets, and development of environment models used in an ADS.", "num_citations": "15\n", "authors": ["34"]}
{"title": "Variability in software: State of the art and future directions\n", "abstract": " Variability is a fundamental aspect of software. It is the ability to create system variants for different market segments or contexts of use. Variability has been most extensively studied in software product lines [10], but is also relevant in other areas, including software ecosystems [4] and context-aware software [15]. Virtually any successful software faces eventually the need to exist in multiple variants.", "num_citations": "15\n", "authors": ["34"]}
{"title": "Model-versioning-in-the-large: Algebraic foundations and the tile notation\n", "abstract": " Model-versioning-in-the-large is concerned with complex scenarios involving multiple updates and multiple replicas of a model. The paper introduces tile systems as rephrasing of double categories in model versioning terms, and shows that the tile language enables a very general formalization of versioning concepts. The formalization makes the concepts amenable to algebraic analysis and provides a convenient notation for version system designers. It also allows one to formulate algebraic laws that a correct versioning system must or may want to satisfy.", "num_citations": "15\n", "authors": ["34"]}
{"title": "A dataset of feature additions and feature removals from the linux kernel\n", "abstract": " This paper describes a dataset of feature additions and removals in the Linux kernel evolution history, spanning over seven years of kernel development. Features, in this context, denote configurable system options that users select when creating customized kernel images. The provided dataset is the largest corpus we are aware of capturing feature additions and removals, allowing researchers to assess the kernel evolution from a feature-oriented point-of-view. Furthermore, the dataset can be used to better understand how features evolve over time, and how different artifacts change as a result. One particular use of the dataset is to provide a real-world case to assess existing support for feature traceability and evolution. In this paper, we detail the dataset extraction process, the underlying database schema, and example queries. The dataset is directly available at our Bitbucket repository: https://bitbucket. org\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["34"]}
{"title": "ProcSy: Procedural Synthetic Dataset Generation Towards Influence Factor Studies Of Semantic Segmentation Networks.\n", "abstract": " Real-world, large-scale semantic segmentation datasets are expensive and time-consuming to create. Thus, the research community has explored the use of video game worlds and simulator environments to produce large-scale synthetic datasets, mainly to supplement the real-world ones for training deep neural networks. Another use of synthetic datasets is to enable highly controlled and repeatable experiments, thanks to the ability to manipulate the content and rendering of synthesized imagery. To this end, we outline a method to generate an arbitrarily large, semantic segmentation dataset reflecting real-world features, while minimizing required cost and man-hours. We demonstrate its use by generating ProcSy, a synthetic dataset for semantic segmentation, which is modeled on a real-world urban environment and features a range of variable influence factors, such as weather and lighting. Our experiments investigate impact of the factors on performance of a state-of-the-art deep network. Among others, we show that including as little as 3% of rainy images in the training set, improved the mIoU of the network on rainy images by about 10%, while training with more than 15% rainy images has diminishing returns. We provide ProcSy dataset, along with generated 3D assets and code, as supplementary material 1.", "num_citations": "13\n", "authors": ["34"]}
{"title": "A mathematical model of performance-relevant feature interactions\n", "abstract": " Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.", "num_citations": "13\n", "authors": ["34"]}
{"title": "SATGraf: Visualizing the evolution of SAT formula structure in solvers\n", "abstract": " In this paper, we present SATGraf, a tool for visualizing the evolution of the structure of a Boolean SAT formula in real time as it is being processed by a conflict-driven clause-learning (CDCL) solver. The tool is parametric, allowing the user to define the structure to be visualized. In particular, the tool can visualize the community structure of real-world Boolean satisfiability (SAT) instances and their evolution during solving. Such visualizations have been the inspiration for several hypotheses about the connection between community structure and the running time of CDCL SAT solvers, some which we have already empirically verified. SATGraf has enabled us in making the following empirical observations regarding CDCL solvers: First, we observe that the Variable State Independent Decaying Sum (VSIDS) branching heuristic consistently chooses variables with a high number of inter-community edges, i.e\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["34"]}
{"title": "Detecting out-of-distribution inputs in deep neural networks using an early-layer output\n", "abstract": " Deep neural networks achieve superior performance in challenging tasks such as image classification. However, deep classifiers tend to incorrectly classify out-of-distribution (OOD) inputs, which are inputs that do not belong to the classifier training distribution. Several approaches have been proposed to detect OOD inputs, but the detection task is still an ongoing challenge. In this paper, we propose a new OOD detection approach that can be easily applied to an existing classifier and does not need to have access to OOD samples. The detector is a one-class classifier trained on the output of an early layer of the original classifier fed with its original training set. We apply our approach to several low- and high-dimensional datasets and compare it to the state-of-the-art detection approaches. Our approach achieves substantially better results over multiple metrics.", "num_citations": "12\n", "authors": ["34"]}
{"title": "Incorporating Uncertainty into Bidirectional Model Transformations and their Delta-Lens Formalization.\n", "abstract": " In Model-Driven Engineering, bidirectional transformations are key to managing consistency and synchronization of related models. Deltalenses are a flexible algebraic framework designed for specifying deltabased synchronization operations. Since model consistency is usually not a one-to-one correspondence, the synchronization process is inherently ambiguous, and consistency restoration can be achieved in many different ways. This can be seen as an uncertainty reducing process: the unknown uncertainty at design-time is translated into known uncertainty at run-time by generating multiple choices. However, many current tools only focus on a specific strategy (an update policy) to select only one amongst many possible alternatives, providing developers with little control over how models are synchronized. In this paper, we propose to extend the delta-lenses framework to cover incomplete transformations producing a multitude of possible solutions to consistency restoration. This multitude is managed in an intentional manner via models with built-in uncertainty.", "num_citations": "12\n", "authors": ["34"]}
{"title": "Fast extraction of high-quality framework-specific models from application code\n", "abstract": " Framework-specific models represent the design of application code from the framework viewpoint by showing how framework-provided concepts are instantiated in the code. Retrieving such models quickly and precisely is necessary for practical model-supported software engineering, in which developers use design models for development tasks such as code understanding, verifying framework usage rules, and round-trip engineering. Also, comparing models extracted at different times of the software lifecycle supports software evolution tasks.               We describe an experimental study of the static analyses necessary to automatically retrieve framework-specific models from application code. We reverse engineer a number of applications based on three open-source frameworks and evaluate the quality of the retrieved models. The models are expressed using framework-specific modeling\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["34"]}
{"title": "Analysis of confident-classifiers for out-of-distribution detection\n", "abstract": " Discriminatively trained neural classifiers can be trusted, only when the input data comes from the training distribution (in-distribution). Therefore, detecting out-of-distribution (OOD) samples is very important to avoid classification errors. In the context of OOD detection for image classification, one of the recent approaches proposes training a classifier called \"confident-classifier\" by minimizing the standard cross-entropy loss on in-distribution samples and minimizing the KL divergence between the predictive distribution of OOD samples in the low-density regions of in-distribution and the uniform distribution (maximizing the entropy of the outputs). Thus, the samples could be detected as OOD if they have low confidence or high entropy. In this paper, we analyze this setting both theoretically and experimentally. We conclude that the resulting confident-classifier still yields arbitrarily high confidence for OOD samples far away from the in-distribution. We instead suggest training a classifier by adding an explicit \"reject\" class for OOD samples.", "num_citations": "11\n", "authors": ["34"]}
{"title": "Software reuse and evolution with generative techniques\n", "abstract": " Generative software development aims at modeling and implementing product lines in such a way that all or a substantial part of the desired system can be automatically generated from a specification written in one or more domain-specific languages (DSLs). The tutorial will explore several techniques of generative software development and show how they can help address software evolution and reuse challenges.", "num_citations": "11\n", "authors": ["34"]}
{"title": "Separating the configuration aspect to support architecture evolution\n", "abstract": " Suppose that you are developing a piece of embedded software, where you need to count different kinds of events. For example, you may need to count the number of times some button is pressed, the total time a given function is activated, or to periodically collect the value of some variable signal. Let us assume that for the first version of the application, you need to implement a simple increment counter, that is, one which always gets incremented by one, and a counter with variable increment, which can be incremented by some value other than one. To simplify our example, we assume that the initial counter value is always zero. Fig. 1 shows a simple implementation of both counters as C++ classes. class SimpleIncrCounter {public:", "num_citations": "11\n", "authors": ["34"]}
{"title": "Calibrating uncertainties in object localization task\n", "abstract": " In many safety-critical applications such as autonomous driving and surgical robots, it is desirable to obtain prediction uncertainties from object detection modules to help support safe decision-making. Specifically, such modules need to estimate the probability of each predicted object in a given region and the confidence interval for its bounding box. While recent Bayesian deep learning methods provide a principled way to estimate this uncertainty, the estimates for the bounding boxes obtained using these methods are uncalibrated. In this paper, we address this problem for the single-object localization task by adapting an existing technique for calibrating regression models. We show, experimentally, that the resulting calibrated model obtains more reliable uncertainty estimates.", "num_citations": "10\n", "authors": ["34"]}
{"title": "Operational world model ontology for automated driving systems\u0393\u00c7\u00f4part 2: Road users, animals, other obstacles, and environmental conditions,\u0393\u00c7\u00a5\n", "abstract": " This document is Part 2 of an ontology definition for specifying operational world models for an Automated Driving System(ADS). Part 2 covers road users, including vehicles and pedestrians and their behavior models; animals; other obstacles, including objects placed by forces of nature, lost cargo, and construction equipment; and environmental conditions, including atmospheric, lighting, and road surface conditions. The ontology can be used to define the Operational Design Domain (ODD) for an ADS. Other uses of the ontology are ADS behavior specification, test development and coverage, labeling and indexing of datasets, and development of environment models as part of an ADS.", "num_citations": "10\n", "authors": ["34"]}
{"title": "Does feature scattering follow power-law distributions? an investigation of five pre-processor-based systems\n", "abstract": " Feature scattering is long said to be an undesirable characteristic in source code. Since scattered features introduce extensions across the code base, their maintenance requires analyzing and changing different locations in code, possibly causing ripple effects. Despite this fact, scattering often occurs in practice, either due to limitations in existing programming languages (eg, imposition of a dominant decomposition) or time-pressure issues. In the latter case, scattering provides a simple way to support new capabilities, avoiding the upfront investment of creating modules and interfaces (when possible). Hence, we argue that scattering is not necessarily bad, provided it is kept within certain limits, or thresholds. Extracting thresholds, however, is not a trivial task. For instance, research shows that some source-code-metric distributions are heavy-tailed, usually following power-law models. In the face of heavy-tailed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["34"]}
{"title": "Towards a generic infrastructure for framework-specific integrated development environment extensions\n", "abstract": " Object-oriented frameworks are often difficult to use. Framework- specific extensions to integrated development environments (IDEs) aim to mitigate the difficulty by offering tools that leverage the knowledge about framework's application programming interfaces (APIs). These tools commonly offer support for code visualization, automatic and interactive code generation, and code validation. Current practices, however, require such extensions to be custom-built manually for each supported framework. In this paper, we propose an approach to building framework-specific IDE extensions based on framework-specific modeling languages (FSMLs). We show how the definitions of different FSMLs can be interpreted in these extensions to provide advanced tool support for different framework APIs that the FSMLs are designed for.", "num_citations": "10\n", "authors": ["34"]}
{"title": "Deformable PV-RCNN: Improving 3D object detection with learned deformations\n", "abstract": " We present Deformable PV-RCNN, a high-performing point-cloud based 3D object detector. Currently, the proposal refinement methods used by the state-of-the-art two-stage detectors cannot adequately accommodate differing object scales, varying point-cloud density, part-deformation and clutter. We present a proposal refinement module inspired by 2D deformable convolution networks that can adaptively gather instance-specific features from locations where informative content exists. We also propose a simple context gating mechanism which allows the keypoints to select relevant context information for the refinement stage. We show state-of-the-art results on the KITTI dataset.", "num_citations": "9\n", "authors": ["34"]}
{"title": "Domain-Driven Development\n", "abstract": " Object-orientation is recognized as an important advance in software technology, particularly in modeling complex phenomena more easily than its predecessors. But the progress in reusability, maintainability, reliability, and even expressiveness has fallen short of expectations. As units of reuse, objects have proven too small. Frameworks seem too large, and their development remains an art. Components offer reuse, but the more functional the component, the larger and less reusable it becomes. And patterns, while intrinsically reusable, are not an implementation medium. The time has come for a new one.\u0393\u00c7\u00a3Domain-Driven Development\u0393\u00c7\u00a5 covers a range of emerging technologies, including Model-Driven Architecture, Product-Line Engineering, Aspect-Oriented Modeling, Generative Programming, and Intentional Programming. All of these technologies focus on aligning code and problem domain more closely\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["34"]}
{"title": "Generative techniques for product lines\n", "abstract": " A software product line leverages the knowledge of one or more domains in order to achieve short time-to-market, cost savings, and high quality software. The highest level of reuse comes by using domain-specific languages or visual builders to describe a member of the product line, and to generate the member from the description. Generative techniques can help us to capture the configuration knowledge for a product line and use it to generate concrete family members. This workshop focuses on technical issues of product lines, rather than economic issues.", "num_citations": "9\n", "authors": ["34"]}
{"title": "A behavior driven approach for sampling rare event situations for autonomous vehicles\n", "abstract": " Performance evaluation of urban autonomous vehicles (AVs) requires a realistic model of the behavior of other road users in the environment. Learning such models from data involves collecting naturalistic data of real-world human behavior. In many cases, acquisition of this data can be prohibitively expensive or intrusive. Additionally, the available data often contain only typical behaviors and exclude behaviors that are classified as rare events. To evaluate the performance of AVs in such situations, we develop a model of traffic behavior based on the theory of bounded rationality. Based on the experiments performed on a large naturalistic driving data, we show that the developed model can be applied to estimate probability of rare events, as well as to generate new traffic situations for testing.", "num_citations": "8\n", "authors": ["34"]}
{"title": "Generative software development\n", "abstract": " System family engineering seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In system family engineering, new system variants can be created rapidly based on a set of reusable assets (such as a common architecture, components, models, etc.) [1]. Generative software development aims at modeling and implementing system families in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages [2\u0393\u00c7\u00f48]. In this tutorial, participants will learn how to perform domain analysis (i.e., capturing the commonalities and variabilities within a system family in a software schema using feature modeling), domain design (i.e., developing a common architecture for a system family), and implementing software generators using multiple\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["34"]}
{"title": "Leveraging reuse through domain-specific software architectures\n", "abstract": " Object-orientated frameworks, software architectures, generators, and domain analysis are all technologies that claim to make reuse possible. In this position paper, we argue that in order to achieve a higher level of reuse, all these technologies should be combined into a single development framework using the concept of domain-speci c software architectures. We will examine the elements of a domain-speci c architecture and show how they incorporate the above-mentioned technologies.", "num_citations": "8\n", "authors": ["34"]}
{"title": "ClassExpert: A knowledge-based assistant to support reuse by specialization and modification in Smalltalk\n", "abstract": " Smalltalk-80 is an object-oriented system promoting \"programming by reuse\". However, the complexity of the Smalltalk class library makes it difficult for the non-expert user to find the problem-solving class. This paper describes ClassExpert, a tool that helps to retrieve classes matching the functional specification provided by the user. ClassExpert deploys an attribute-value classification scheme with taxonomies. This paper also shows how this scheme can be used to support reuse by specialization and modification.", "num_citations": "8\n", "authors": ["34"]}
{"title": "MLOD: A multi-view 3D object detection based on robust feature fusion method\n", "abstract": " This paper presents Multi-view Labelling Object Detector (MLOD). The detector takes an RGB image and a LIDAR point cloud as input and follows the two-stage object detection framework [1] [2]. A Region Proposal Network (RPN) generates 3D proposals in a Bird's Eye View (BEV) projection of the point cloud. The second stage projects the 3D proposal bounding boxes to the image and BEV feature maps and sends the corresponding map crops to a detection header for classification and bounding-box regression. Unlike other multi-view based methods, the cropped image features are not directly fed to the detection header, but masked by the depth information to filter out parts outside 3D bounding boxes. The fusion of image and BEV features is challenging, as they are derived from different perspectives. We introduce a novel detection header, which provides detection results not just from fusion layer, but also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["34"]}
{"title": "Bayesian uncertainty quantification with synthetic data\n", "abstract": " Image semantic segmentation systems based on deep learning are prone to making erroneous predictions for images affected by uncertainty influence factors such as occlusions or inclement weather. Bayesian deep learning applies the Bayesian framework to deep models and allows estimating so-called epistemic and aleatoric uncertainties as part of the prediction. Such estimates can indicate the likelihood of prediction errors due to the influence factors. However, because of lack of data, the effectiveness of Bayesian uncertainty estimation when segmenting images with varying levels of influence factors has not yet been systematically studied. In this paper, we propose using a synthetic dataset to address this gap. We conduct two sets of experiments to investigate the influence of distance, occlusion, clouds, rain, and puddles on the estimated uncertainty in the segmentation of road scenes. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["34"]}
{"title": "Improving ml safety with partial specifications\n", "abstract": " Advanced autonomy features of vehicles are typically difficult or impossible to specify precisely and this has led to the rise of machine learning (ML) from examples as an alternative implementation approach to traditional programming. Developing software without specifications sacrifices the ability to effectively verify the software yet this is a key component of safety assurance. In this paper, we suggest that while complete specifications may not be possible, partial specifications typically are and these could be used with ML to strengthen safety assurance. We review the types of partial specifications that are applicable for these problems and discuss the places in the ML development workflow that they could be used to improve the safety of ML-based components.", "num_citations": "7\n", "authors": ["34"]}
{"title": "The effect of structural measures and merges on SAT solver performance\n", "abstract": " Over the years complexity theorists have proposed many structural parameters to explain the surprising efficiency of conflict-driven clause-learning (CDCL) SAT solvers on a wide variety of large industrial Boolean instances. While some of these parameters have been studied empirically, until now there has not been a unified comparative study of their explanatory power on a comprehensive benchmark. We correct this state of affairs by conducting a large-scale empirical evaluation of CDCL SAT solver performance on nearly 7000 industrial and crafted formulas against several structural parameters such as treewidth, community structure parameters, and a measure of the number of \u0393\u00c7\u00a3mergeable\u0393\u00c7\u00a5 pairs of input clauses. We first show that while most of these parameters correlate well with certain sub-categories of formulas, only the merge-based parameters seem to correlate well across most classes of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["34"]}
{"title": "Requirements engineering in the age of societal-scale cyber-physical systems: The case of automated driving\n", "abstract": " Societal-scale cyber-physical systems, such as smart grids, interconnected medical devices, and driverless transportation systems, are at the cusp of transforming how we live. Using automated driving as an example, this paper argues that requirements engineering for such systems will need to be data-driven, continuous, and values-based.", "num_citations": "7\n", "authors": ["34"]}
{"title": "English Translation of the German Road Traffic Act Amendment Regulating the Use of \u0393\u00c7\u00a3Motor Vehicles with Highly or Fully Automated Driving Function\u0393\u00c7\u00a5 from July 17, 2017\n", "abstract": " This document provides a translation of the amendment to the German Road Traffic Act\u0393\u00c7\u00f6Stra\u251c\u0192enverkehrsgesetz (StVG)\u0393\u00c7\u00f6that introduced regulation of \u0393\u00c7\u00a3motor vehicles with highly or fully automated driving functions\u0393\u00c7\u00a5 on German roads. The amendment took effect on July 17, 2017. Second part of the document comments on (1) the relationship between the driving automation function defined in this amendment and the levels of automation as defined in SAE J3016: Sept 2016 \u0393\u00c7\u00a3International Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-\u0393\u00c7\u00c9Road Motor Vehicles\u0393\u00c7\u00a5 and (2) the liability of vehicles operated by this function in Germany.", "num_citations": "7\n", "authors": ["34"]}
{"title": "Improving bug report comprehension\n", "abstract": " Developers need to reason about data in bug reports to diagnose problems and compare proposed solutions. Current bug tracking systems\u0393\u00c7\u00d6 interfaces, however, collect and present data as a conversation. Previous work shows that this hinders the ability of users to consult, reason, and analyze information important to resolve bugs. This work implements BugBot, a bug tracking system that eliminates comments as the main component of bug reports, promoting information instead of conversation. An evaluation for 6 months on a group of 9 users finds that BugBot facilitates reasoning about bug report information and improves its archival value.", "num_citations": "7\n", "authors": ["34"]}
{"title": "An Empirical Study on Consistency Management of Business and IT Process Models\n", "abstract": " Process models support the transition from business requirements to IT implementations. Organizations that adopt process modeling often maintain several co-existing models of the same business process. These models target different abstraction levels and stakeholder perspectives. Maintaining consistency among these models has become a major challenge for such organizations. Although several academic works have discussed this challenge, little empirical investigation exists on how people perform process model consistency management in practice. This paper aims to address this lack by presenting an in-depth empirical study of a business-driven engineering process deployed at a large company in the banking sector. We analyzed more than 70 business process models developed by the company, including their change history, with over 1000 change requests. We also interviewed 9 business and IT practitioners and surveyed 23 such practitioners to understand concrete difficulties in consistency management, the rationales for the specification-to-implementation refinements found in the models, strategies that the practitioners use to detect and fix inconsistencies, and how tools could help with these tasks. Our contributions are 1) an account of how business process models co-evolve and how their consistency is maintained in a concrete industrial setting; 2) a set of recurrent patterns used to refine business-level process specifications into IT-level models, and 3) a set of findings that confirm or contradict conventional wisdom on process model consistency management found in the literature.", "num_citations": "7\n", "authors": ["34"]}
{"title": "Correctness of model synchronization based on triple graph grammars-extended version\n", "abstract": " [en] Success and efficiency of software and system design fundamentally relies on its models. The more they are based on formal methods the more they can be automatically transformed to execution models and finally to implementation code. This paper presents model transformation and model integration as specific problem within bidirectional model transformation, which has shown to support various purposes, such as analysis, optimization, and code generation. The main purpose of model integration is to establish correspondence between various models, especially between source and target models. From the analysis point of view, model integration supports correctness checks of syntactical dependencies between different views and models. The overall concept is based on the algebraic approach to triple graph grammars, which are widely used for model transformation. The main result shows the close relationship between model transformation and model integration. For each model transformation sequence there is a unique model integration sequence and vice versa. This is demonstrated by a quasi-standard example for model transformation between class models and relational data base models.", "num_citations": "7\n", "authors": ["34"]}
{"title": "Towards requirements specification for machine-learned perception based on human performance\n", "abstract": " The application of machine learning (ML) based perception algorithms in safety-critical systems such as autonomous vehicles have raised major safety concerns due to the apparent risks to human lives. Yet assuring the safety of such systems is a challenging task, in a large part because ML components (MLCs) rarely have clearly specified requirements. Instead, they learn their intended tasks from the training data. One of the most well-studied properties that ensure the safety of MLCs is the robustness against small changes in images. But the range of changes considered small has not been systematically defined. In this paper, we propose an approach for specifying and testing requirements for robustness based on human perception. With this approach, the MLCs are required to be robust to changes that fall within the range defined based on human perception performance studies. We demonstrate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["34"]}
{"title": "PURSS: Towards Perceptual Uncertainty Aware Responsibility Sensitive Safety with ML.\n", "abstract": " Automated driving is an ML-intensive problem and its safety depends on the integrity of perception as well as planning and control. Responsibility Sensitive Safety (RSS) is a recent approach to promote safe planning and control that relies on perfect perception; however, perceptual uncertainty is always present, and this causes the possibility of misperceptions that can lead an autonomous vehicle to allow unsafe actions. In this position paper, we sketch a novel proposal for a formal model of perception coupled with RSS to help mitigate the impact of misperception by using information about perceptual uncertainty. The approach expresses uncertainty as imprecise perceptions that are consumed by RSS and cause it to limit actions to those that support safe behaviour given the perceptual uncertainty. We illustrate our approach using examples and discuss its implications and limitations.", "num_citations": "6\n", "authors": ["34"]}
{"title": "Design space of behaviour planning for autonomous driving\n", "abstract": " We explore the complex design space of behaviour planning for autonomous driving. Design choices that successfully address one aspect of behaviour planning can critically constrain others. To aid the design process, in this work we decompose the design space with respect to important choices arising from the current state of the art approaches, and describe the resulting trade-offs. In doing this, we also identify interesting directions of future work.", "num_citations": "6\n", "authors": ["34"]}
{"title": "Learning a lattice planner control set for autonomous vehicles\n", "abstract": " This paper introduces a method to compute a sparse lattice planner control set that is suited to a particular task by learning from a representative dataset of vehicle paths. To do this, we use a scoring measure similar to the Fr\u251c\u2310chet distance and propose an algorithm for evaluating a given control set according to the scoring measure. Control actions are then selected from a dense control set according to an objective function that rewards improvements in matching the dataset while also encouraging sparsity. This method is evaluated across several experiments involving real and synthetic datasets, and it is shown to generate smaller control sets when compared to the previous state-of-the-art lattice control set computation technique, with these smaller control sets maintaining a high degree of manoeuvrability in the required task. This results in a planning time speedup of up to 4.31x when using the learned control\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["34"]}
{"title": "Wisemove: A framework for safe deep reinforcement learning for autonomous driving\n", "abstract": " Machine learning can provide efficient solutions to the complex problems encountered in autonomous driving, but ensuring their safety remains a challenge. A number of authors have attempted to address this issue, but there are few publicly-available tools to adequately explore the trade-offs between functionality, scalability, and safety. We thus present WiseMove, a software framework to investigate safe deep reinforcement learning in the context of motion planning for autonomous driving. WiseMove adopts a modular learning architecture that suits our current research questions and can be adapted to new technologies and new questions. We present the details of WiseMove, demonstrate its use on a common traffic scenario, and describe how we use it in our ongoing safe learning research.", "num_citations": "6\n", "authors": ["34"]}
{"title": "Modeling and optimizing automotive electric/electronic (e/e) architectures: Towards making clafer accessible to practitioners\n", "abstract": " Modern automotive electric/electronic (E/E) architectures are growing to the point where architects can no longer manually predict the effects of their design decisions. Thus, in addition to applying an architecture reference model to decompose their architectures, they also require tools for synthesizing and evaluating candidate architectures during the design process. Clafer is a modeling language, which has been used to model variable multi-layer, multi-perspective automotive system architectures according to an architecture reference model. Clafer tools allow architects to synthesize optimal candidates and evaluate effects of their design decisions. However, since Clafer is a general-purpose structural modeling language, it does not help the architects in building models conforming to the given architecture reference model. In this paper, we present an E/E architecture domain-specific language (DSL\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["34"]}
{"title": "A model management imperative: Being graphical is not sufficient, you have to be categorical\n", "abstract": " Graph-based modeling is both common in and fundamental for Model Driven Engineering (MDE). The paper argues that several important model management (MMt) scenarios require an essential extension of graphical models. We show that different versions of model merge and sync, including many-to-many correspondences between models, can be treated in a uniform, compact and well-defined mathematical way if we specify graphical models as directed graphs with associative arrow composition and identity loops, that is, as categories.", "num_citations": "6\n", "authors": ["34"]}
{"title": "DCLfix: A recommendation system for repairing architectural violations\n", "abstract": " Architectural erosion is a recurrent problem in software evolution. Despite this fact, the process is usually tackled in ad hoc ways, without adequate tool support at the architecture level. To address this shortcoming, this paper presents a recommendation system -- called DCLfix -- that provides refactoring guidelines for maintainers when tackling architectural erosion. In short, DCLfix suggests refactoring recommendations for violations detected after an architecture conformance process using DCL, an architectural constraint language", "num_citations": "6\n", "authors": ["34"]}
{"title": "Optimizing alloy for multi-objective software product line configuration\n", "abstract": " Software product line (SPL) engineering involves the modeling, analysis, and configuration of variability-rich systems. We improve the performance of the multi-objective optimization of SPLs in Alloy by several orders of magnitude with two techniques.             First, we rewrite the model to remove binary relations that map to integers, which enables removing most of the integer atoms from the universe. SPL models often require using large bitwidths, hence the number of integer atoms in the universe can be orders of magnitude more than the other atoms. In our approach, the tuples for these integer-valued relations are computed outside the sat solver before returning the solution to the user. Second, we add a checkpointing facility to Kodkod, which allows the multi-objective optimization algorithm to reuse previously computed internal sat solver state, after backtracking.             Together these result in orders of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["34"]}
{"title": "Generative and Component-Based Software Engineering: First International Symposium, GCSE'99, Erfurt, Germany, September 28-30, 1999. Revised Papers\n", "abstract": " In the past two years, the Smalltalk and Java in Industry and Education C-ference (STJA) featured a special track on generative programming, which was organized by the working group\\Generative and Component-Based Software Engineering\" of the\\Gesellschaft fur \u2560\u00ea Informatik\" FG 2.1. 9\\Object-Oriented Software Engineering.\" This track covered a wide range of related topics from domain analysis, software system family engineering, and software product-nes, to extendible compilers and active libraries. The talks and keynotes directed towards this new software engineering paradigm received much attention and-terest from the STJA audience. Hence the STJA organizers suggested enlarging this track, making it more visible and open to wider, international participation. This is how the GCSE symposium was born. The rst GCSE symposium attracted 39 submissions from all over the world. This impressive number demonstrates the international interest in generative programming and related elds. After a careful review by the program comm-tee, fteen papers were selected for presentation. We are very grateful to the members of the program committee, all of them renowned experts, for their dedication in preparing thorough reviews of the submissions. Special thanks go to Elke Pulvermuller \u2560\u00ea and Andreas Speck, who proposed and organized a special conference event, the Young Researches Workshop (YRW). This workshop provided a unique opportunity for young scientists and Ph. D.", "num_citations": "6\n", "authors": ["34"]}
{"title": "Multi-Paradigm Design with C++\n", "abstract": " Bibliografia a citovanie v technickom texte Page 1 Bibliografia a citovanie v technickom texte Pozn\u251c\u00edmky k predn\u251c\u00ed\u253c\u00edkam z predmetu Met\u251c\u2502dy in\u253c\u255binierskej pr\u251c\u00edce Valentino Vranic http://fiit.sk/~vranic/, vranic@stuba.sk \u251c\u00dcstav informatiky a softv\u251c\u2310rov\u251c\u2310ho in\u253c\u255binierstva Fakulta informatiky a informa\u2500\u00ecn\u251c\u255cch technol\u251c\u2502gi\u251c\u00a1 Slovensk\u251c\u00ed technick\u251c\u00ed univerzita v Bratislave 8. okt\u251c\u2502ber 2015 Page 2 OBSAH i Obsah 1 Zdroje a odkazy 1 2 Bibliografia v LaTEXu 3 3 Sumariz\u251c\u00edcia 5 Page 3 1 ZDROJE A ODKAZY 1 1 Zdroje a odkazy \u251c\u00dcloha Definujme z\u251c\u00edkladn\u251c\u255c pojem v na\u253c\u00edom \u2500\u00ecl\u251c\u00ednku z minulej predn\u251c\u00ed\u253c\u00edky. Princ\u251c\u00a1p \u0393\u00c7\u00f3 Pre ka\u253c\u255bd\u251c\u2310 tvrdenie, ktor\u251c\u2310 nie je vlastn\u251c\u2310 tvrdenie autora alebo v\u253c\u00edeobecne zn\u251c\u00edme, mus\u251c\u00a1 by\u253c\u00d1 uveden\u251c\u255c zdroj \u0393\u00c7\u00f3 Odkazov nikdy nie je ve\u2500\u255ba \u0393\u00c7\u00f4 len m\u251c\u00edlo alebo priemerane Pojmy \u0393\u00c7\u00f3 Bibliografick\u251c\u255c odkaz \u0393\u00c7\u00f4 alebo len odkaz \u0393\u00c7\u00f4 angl. reference; \u2500\u00ecasto cit\u251c\u00edcia \u0393\u00c7\u00f3 Cit\u251c\u00edt \u0393\u00c7\u00f4 angl. quotation, quote alebo citation \u0393\u00c7\u00f3 Citovanie: uvedenie cit\u251c\u00edtu alebo odkazu na zdroj \u0393\u00c7\u00f3 \u0393\u00c7\u00f4 ; \u0393\u00c7\u00f3 \u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["34"]}
{"title": "TruPercept: Trust modelling for autonomous vehicle cooperative perception from synthetic data\n", "abstract": " Inter-vehicle communication for autonomous vehicles (AVs) stands to provide significant benefits in terms of perception robustness. We propose a novel approach for AVs to communicate perceptual observations, tempered by trust modelling of peers providing reports. Based on the accuracy of reported object detections as verified locally, communicated messages can be fused to augment perception performance beyond line of sight and at great distance from the ego vehicle. Also presented is a new synthetic dataset which can be used to test cooperative perception. The TruPercept dataset includes unreliable and malicious behaviour scenarios to experiment with some challenges cooperative perception introduces. The TruPercept runtime and evaluation framework allows modular component replacement to facilitate ablation studies as well as the creation of new trust scenarios we are able to show.", "num_citations": "5\n", "authors": ["34"]}
{"title": "Efficacy of pixel-level OOD detection for semantic segmentation\n", "abstract": " The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.", "num_citations": "5\n", "authors": ["34"]}
{"title": "A safety analysis method for perceptual components in automated driving\n", "abstract": " The use of machine learning (ML) is increasing in many sectors of safety-critical software development and in particular, for the perceptual components of automated driving (AD) functionality. Although some traditional safety engineering techniques such as FTA and FMEA are applicable to ML components, the unique characteristics of ML create challenges. In this paper, we propose a novel safety analysis method called Classification Failure Mode Effects Analysis (CFMEA) which is specialized to assess classification-based perception in AD. Specifically, it defines a systematic way to assess the risk due to classification failure under adversarial attacks or varying degrees of classification uncertainty across the perception-control linkage. We first present the theoretical and methodological foundations for CFMEA, and then demonstrate it by applying it to an AD case study using semantic segmentation perception\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["34"]}
{"title": "Software engineering for automated vehicles: Addressing the needs of cars that run on software and data\n", "abstract": " Automated vehicles are AI-based safety-critical robots that fulfill transportation needs while interacting with the general public in traffic. Software engineering for automated vehicles requires a DevOps-style process with special considerations for functions based on machine learning and incremental safety assurance at vehicle and fleet level. This technical briefing reviews current challenges, industry practices, and opportunities for future research in software engineering for automated vehicles.", "num_citations": "5\n", "authors": ["34"]}
{"title": "On-road safety of automated driving system (ads)\u0393\u00c7\u00f6Taxonomy and safety analysis methods\n", "abstract": " This document provides a taxonomy of on-road safety of an Automated Driving System (ADS) and describes safety analysis methods applicable at requirements and early design stage of ADS development. In particular, it covers Driving Behavior Safety Assurance, Safety of The Intended Functionality (SOTIF) Assurance, and the Hazard Analysis and Risk Assessment (HARA) part of Functional Safety, and relates the three methods to System-Theoretic Process Analysis (STPA).", "num_citations": "5\n", "authors": ["34"]}
{"title": "The Problem of the Lack of Benefit of a Document to its Producer (PotLoBoaDtiP)\n", "abstract": " The paper lists some of the quotations gathered during interviews and focus groups during a consulting engagement to help the client company improve its requirements engineering (RE) process. The paper describes in detail one of the phenomena observed, namely that of the problem of the lack of benefit of a document to its producer (PotLoBoaDtiP). It goes on to report other manifestations, including building and maintaining traces, of the PotLoBoaDtiP reported in the literature. It concludes by suggesting that the PotLoBoaDtiP cannot be solved with only technology, but that the motivational issue must be addressed.", "num_citations": "5\n", "authors": ["34"]}
{"title": "Configuration challenges in Linux and eCos: A survey\n", "abstract": " Operating systems expose sophisticated configurability to handle variations in hardware platforms like desktops, servers, and mobile devices. The configuration of an operating system like Linux contains thousands of options guarded by hundreds of complex constraints. To guide users throughout the configuration activity, configurators implement various mechanisms to produce correct configurations. However, configuration still remains a difficult and challenging process. To better understand the challenges faced by users during configuration, we conducted surveys among Linux and eCos users to answer the two following questions:", "num_citations": "5\n", "authors": ["34"]}
{"title": "On-demand materialization of aspects for application development\n", "abstract": " Framework-based application development requires applications to be implemented according to rules, recipes and conventions that are documented or assumed by the framework's Application Programming Interface (API), thereby giving rise to systematic usage patterns. While such usage patterns can be captured cleanly using conventional aspects, their variations, which arise in exceptional conditions, typically cannot be. In this position paper, we propose materializable aspects as a solution to this problem. A materializable aspect behaves as a normal aspect for default joinpoints, but for exceptional joinpoints, it turns into a program transformation and analysis mechanism, with the IDE transforming the advice in-place and allowing the application developer to modify the materialized advice within the semantics of the aspect. We demonstrate the need for materializable aspects through a preliminary study of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["34"]}
{"title": "Model Driven Engineering Languages and Systems.\n", "abstract": " Model Driven Engineering Languages and Systems. FHNW Fachhochschule Nordwestschweiz Men\u251c\u255d Startseite Publikationen Projekte Studentische Arbeiten Login Eintraganzeige IRF Home Hochschule f\u251c\u255dr Wirtschaft Institut f\u251c\u255dr Wirtschaftsinformatik Eintraganzeige Hochschule f\u251c\u255dr Wirtschaft Institut f\u251c\u255dr Wirtschaftsinformatik Eintraganzeige Model Driven Engineering Languages and Systems. Datum 28.09.2008 Autorin/Autor Czarnecki, K Ober, I Bruel, JM Uhl, Axel V\u251c\u2562lter, M Metadata Zur Langanzeige Type 03 - Sammelband Primary target group Sonstige Created while belonging to FHNW? unbekannt URI http://hdl.handle.net/11654/9628 St\u251c\u2562bern Gesamter BestandBereiche & SammlungenErscheinungsdatumAutoren/AutorinnenTitelThemenDiese SammlungErscheinungsdatumAutoren/AutorinnenTitelThemen Mein Benutzerkonto EinloggenRegistrieren Statistics Most Popular ItemsStatistics by CountryMost Popular 6 \u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["34"]}
{"title": "Towards a framework for collaborative and coordinated product configuration\n", "abstract": " In system-family approaches, product configuration is the activity of selecting the features desired for a software product. Although this process is typically collaborative this aspect has long been neglected. On the other hand, enabling collaborative product configuration brings new and challenging problems such as the proper coordination of configuration decisions. This paper introduces a framework for collaborative configuration that addresses the major issues that arise in this context. Some aspects of the framework can be customized to accommodate specific collaboration goals.", "num_citations": "5\n", "authors": ["34"]}
{"title": "Eclipse workbench part interaction FSML\n", "abstract": " In this technical report we present the details of the Eclipse Workbench Part Interaction (WPI) FSML design and its prototype implementation. We describe the WPI domain, abstract syntax, mapping of the abstract syntax to the framework completion code, and agile round-trip engineering. Finally, we describe technical details of the WPI FSML prototype.", "num_citations": "5\n", "authors": ["34"]}
{"title": "Feature modeling\n", "abstract": " \u0393\u00c7\u00f3 A mandatory feature is part of a concept instance description only if its parent is also part of the description\u0393\u00c7\u00f3 Mandatory features are pointed to by edges with a filled circle, eg f1, f2, f3, and f4\u0393\u00c7\u00f3 All instances of C are described by the feature set {C, f1, f2, f3, f4}", "num_citations": "5\n", "authors": ["34"]}
{"title": "WiseMove: A Framework to Investigate Safe Deep Reinforcement Learning for Autonomous Driving\n", "abstract": " WiseMove is a platform to investigate safe deep reinforcement learning (DRL) in the context of motion planning for autonomous driving. It adopts a modular architecture that mirrors our autonomous vehicle software stack and can interleave learned and programmed components. Our initial investigation focuses on a state-of-the-art DRL approach from the literature, to quantify its safety and scalability in simulation, and thus evaluate its potential use on our vehicle.", "num_citations": "4\n", "authors": ["34"]}
{"title": "Self-Attention Based Context-Aware 3D Object Detection\n", "abstract": " Most existing point-cloud based 3D object detectors use convolution-like operators to process information in a local neighbourhood with fixed-weight kernels and aggregate global context hierarchically. However, recent work on non-local neural networks and self-attention for 2D vision has shown that explicitly modeling global context and long-range interactions between positions can lead to more robust and competitive models. In this paper, we explore two variants of self-attention for contextual modeling in 3D object detection by augmenting convolutional features with self-attention features. We first incorporate the pairwise self-attention mechanism into the current state-of-the-art BEV, voxel and point-based detectors and show consistent improvement over strong baseline models while simultaneously significantly reducing their parameter footprint and computational cost. We also propose a self-attention variant\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "Solution Concepts in Hierarchical Games under Bounded Rationality with Applications to Autonomous Driving\n", "abstract": " With autonomous vehicles (AV) set to integrate further into regular human traffic, there is an increasing consensus of treating AV motion planning as a multi-agent problem. However, the traditional game theoretic assumption of complete rationality is too strong for the purpose of human driving, and there is a need for understanding human driving as a \\emph{bounded rational} activity through a behavioral game theoretic lens. To that end, we adapt three metamodels of bounded rational behavior; two based on Quantal level-k and one based on Nash equilibrium with quantal errors. We formalize the different solution concepts that can be applied in the context of hierarchical games, a framework used in multi-agent motion planning, for the purpose of creating game theoretic models of driving behavior. Furthermore, based on a contributed dataset of human driving at a busy urban intersection with a total of ~4k agents and ~44k decision points, we evaluate the behavior models on the basis of model fit to naturalistic data, as well as their predictive capacity. Our results suggest that among the behavior models evaluated, modeling driving behavior as pure strategy NE with quantal errors at the level of maneuvers with bounds sampling of actions at the level of trajectories provides the best fit to naturalistic driving behavior, and there is a significant impact of situational factors on the performance of behavior models.", "num_citations": "3\n", "authors": ["34"]}
{"title": "Transferring pareto frontiers across heterogeneous hardware environments\n", "abstract": " Software systems provide user-relevant configuration options called features. Features affect functional and non-functional system properties, whereas selections of features represent system configurations. A subset of configuration space forms a Pareto frontier of optimal configurations in terms of multiple properties, from which a user can choose the best configuration for a particular scenario. However, when a well-studied system is redeployed on a different hardware, information about property value and the Pareto frontier might not apply. We investigate whether it is possible to transfer this information across heterogeneous hardware environments. We propose a methodology for approximating and transferring Pareto frontiers of configurable systems across different hardware environments. We approximate a Pareto frontier by training an individual predictor model for each system property, and by aggregating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "Universally safe swerve manoeuvres for autonomous driving\n", "abstract": " This paper characterizes safe following distances for on-road driving when vehicles can avoid collisions by either braking or by swerving into an adjacent lane. In particular, we focus on safety as defined in the Responsibility-Sensitive Safety (RSS) framework. We extend RSS by introducing swerve manoeuvres as a valid response in addition to the already present brake manoeuvre. These swerve manoeuvres use the more realistic kinematic bicycle model rather than the double integrator model of RSS. When vehicles are able to swerve and brake, it is shown that their required safe following distance at higher speeds is less than that required through braking alone. In addition, when all vehicles follow this new distance, they are provably safe. The use of the kinematic bicycle model is then validated by comparing these swerve manoeuvres to that of a dynamic single-track model.", "num_citations": "3\n", "authors": ["34"]}
{"title": "Learning-sensitive backdoors with restarts\n", "abstract": " Restarts are a pivotal aspect of conflict-driven clause-learning (CDCL) SAT solvers, yet it remains unclear when they are favorable in practice, and whether they offer additional power in theory. In this paper, we consider the power of restarts through the lens of backdoors. Extending the notion of learning-sensitive (LS) backdoors, we define a new parameter called learning-sensitive with restarts (LSR) backdoors. Broadly speaking, we show that LSR backdoors are a powerful parametric lens through which to understand the impact of restarts on SAT solver performance, and specifically on the kinds of proofs constructed by SAT solvers. First, we prove that when backjumping is disallowed, LSR backdoors can be exponentially smaller than LS backdoors. Second, we demonstrate that the size of LSR backdoors are dependent on the learning scheme used during search. Finally, we present new algorithms to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "Automated Driving System (ADS) High-\u0393\u00c7\u00c9Level Quality Requirements Analysis\u0393\u00c7\u00f4 Driving Behavior Safety\n", "abstract": " This document analyzes safety requirements on driving behavior for ADS-\u0393\u00c7\u00c9operated vehicles, both at operational and tactical levels. The first part of the document describes motor-\u0393\u00c7\u00c9vehicle crash typology and pre-\u0393\u00c7\u00c9crash scenarios based on existing traffic safety data and literature. The second part proposes a classification of safety requirements on driving behavior into five categories: vehicle stability, assured cleared distance ahead, minimum separation, traffic regulations, and best practices, and presents safety requirements on ADS driving behavior derived from human driving.", "num_citations": "3\n", "authors": ["34"]}
{"title": "Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems (SoSYM abstract)\n", "abstract": " In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today's engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multilevel, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of questions, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "Relating complexity-theoretic parameters with SAT solver performance\n", "abstract": " Over the years complexity theorists have proposed many structural parameters to explain the surprising efficiency of conflict-driven clause-learning (CDCL) SAT solvers on a wide variety of large industrial Boolean instances. While some of these parameters have been studied empirically, until now there has not been a unified comparative study of their explanatory power on a comprehensive benchmark. We correct this state of affairs by conducting a large-scale empirical evaluation of CDCL SAT solver performance on nearly 7000 industrial and crafted formulas against several structural parameters such as backdoors, treewidth, backbones, and community structure. Our study led us to several results. First, we show that while such parameters only weakly correlate with CDCL solving time, certain combinations of them yield much better regression models. Second, we show how some parameters can be used as a \"lens\" to better understand the efficiency of different solving heuristics. Finally, we propose a new complexity-theoretic parameter, which we call learning-sensitive with restarts (LSR) backdoors, that extends the notion of learning-sensitive (LS) backdoors to incorporate restarts and discuss algorithms to compute them. We mathematically prove that for certain class of instances minimal LSR-backdoors are exponentially smaller than minimal-LS backdoors.", "num_citations": "3\n", "authors": ["34"]}
{"title": "MATHCHECK: A math assistant via a combination of computer algebra systems and SAT solvers\n", "abstract": " We present a method and an associated system, called MATHCHECK, that embeds the functionality of a computer algebra system (CAS) within the inner loop of a conflict-driven clause-learning SAT solver. SAT+ CAS systems, \u251c\u00e1 la MATHCHECK, can be used as an assistant by mathematicians to either counterexample or finitely verify open universal conjectures on any mathematical topic (eg, graph and number theory, algebra, geometry, etc.) supported by the underlying CAS system. Such a SAT+ CAS system combines the efficient search routines of modern SAT solvers, with the expressive power of CAS, thus complementing both. The key insight behind the power of the SAT+ CAS combination is that the CAS system can help cut down the search-space of the SAT solver, by providing learned clauses that encode theory-specific lemmas, as it searches for a counterexample to the input conjecture. We demonstrate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "SATGraf: Visualizing community structure in boolean SAT instances\n", "abstract": " In this paper, we present SATGraf, a tool for visualizing the community structure of the variable-incidence graph of a Boolean SAT formula. In addition, SATGraf has an evolution mode that allows one to visualize the dynamically evolving community structure of an input SAT instance as a CDCL SAT solver processes it. This tool enabled us to learn how the solver morphs the community structure of real-world SAT instances, and in turn, led us to some meaningful hypotheses about the connection between community structure of such instances and the running time of the CDCL SAT solver.", "num_citations": "3\n", "authors": ["34"]}
{"title": "Mixin-Based Programming in C++.\n", "abstract": " Proposes a solution to the constructor problem through mixin-based programming in C++. Framework for designing the solution; Use of configuration repositories and generic parameter adapters; Factors to be considered in the manual creation of configuration repositories.", "num_citations": "3\n", "authors": ["34"]}
{"title": "Synthesizing objects\n", "abstract": " This paper argues that the current OO technology does not support reuse and configurability in an effective way. This problem can be addressed by augmenting OO Analysis and Design with feature modeling and by applying generative implementation techniques. Feature modeling allows capturing the variability of domain concepts. Concrete concept instances can then be synthesized from abstract specifications. Using a simple example of a configurable list component, we demonstrate the application of feature modeling and how to implement a feature model as a generator. We introduce the concepts of configuration repositories and configuration generators and show how to implement them using object\u0393\u00c7\u00c9oriented, generic, and generative language mechanisms. Interestingly, a configuration repository represents an effective approach for typing synthesized recursive classes. The configuration generator utilizes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["34"]}
{"title": "Modes of automated driving system scenario testing: Experience report and recommendations\n", "abstract": " With the widespread development of automated driving systems (ADS), it is imperative that standardized testing methodologies be developed to assure safety and functionality. Scenario testing evaluates the behavior of an ADS-equipped subject vehicle (SV) in predefined driving scenarios. This paper compares four modes of performing such tests: closed-course testing with real actors, closed-course testing with surrogate actors, simulation testing, and closed-course testing with mixed reality. In a collaboration between the Waterloo Intelligent Systems Engineering (WISE) Lab and AAA, six automated driving scenario tests were executed on a closed course, in simulation, and in mixed reality. These tests involved the University of Waterloo\u0393\u00c7\u00d6s automated vehicle, dubbed the \u0393\u00c7\u00a3UW Moose\u0393\u00c7\u00a5, as the SV, as well as pedestrians, other vehicles, and road debris. Drawing on both data and the experience gained from executing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["34"]}
{"title": "Simple Continual Learning Strategies for Safer Classifers.\n", "abstract": " Continual learning is often confounded by \u0393\u00c7\u00a3catastrophic forgetting\u0393\u00c7\u00a5 that prevents neural networks from learning tasks sequentially. In the case of real world classification systems that are safety-validated prior to deployment, it is essential to ensure that validated knowledge is retained. In this work, we propose methods that build on existing unconstrained continual learning solutions, which increase the model variance to better retain more of the existing knowledge (and hence safety). We demonstrate the improved performance of our methods against popular continual learning approaches, using variants of standard image classification datasets.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Example-driven modeling: on effects of using examples on structural model comprehension, what makes them useful, and how to create them\n", "abstract": " We present a controlled experiment for the empirical evaluation of example-driven modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate (Masters and Ph.D. level) and undergraduate (Bachelor level) students from electrical and computer engineering, computer science, and software engineering programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples on model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["34"]}
{"title": "Clafer: Lightweight Modeling of Structure, Behaviour, and Variability Research output: Journal Article or Conference Article in Journal\u0393\u00c7\u2551 Journal article\u0393\u00c7\u2551 Research\u0393\u00c7\u2551 peer-review\n", "abstract": " Embedded software is growing fast in size and complexity, leading to intimate mixture of complex architectures and complex control. Consequently, software speci cation requires modeling both structures and behaviour of systems. Unfortunately, existing languages do not integrate these aspects well, usually prioritizing one of them. It is common to develop a separate language for each of these facetsIn this paper, we contribute Clafer: a small language that attempts to tackle this challenge. It combines rich structural modeling with state of the art behavioural formalisms. We are not aware of any other modeling language that seamlessly combines these facets common to system and software modeling.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Operational Design Domain for Automated Driving Systems\n", "abstract": " This document defines a taxonomy of basic terms used in the description of an Operational Design Domain (ODD) for an Automated Driving System (ADS). Among others, the taxonomy defines operational world models and terms for specifying driving scenarios and their attributes.", "num_citations": "2\n", "authors": ["34"]}
{"title": "SHA-1 preimage instances for SAT\n", "abstract": " A brief description of the instances we submitted to the SAT Competition 2017 encoding SHA-1 preimage attacks.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Towards category theory foundations for model management\n", "abstract": " The paper aims to demonstrate that category theory (CT) methods are appropriate in the model management context. We show that CT naturally appears on stage, when we model such basic constructs as model merge and parallel composition accurately and consistently. We present categorical constructs in a tutorial-like style by applying them to simple labeled transition systems (LTSs), and reveal new technical aspects of merging and composing LTSs.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Software Language Engineering: 5th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers\n", "abstract": " This book constitutes the thoroughly refereed post-proceedings of the 5th International Conference on Software Language Engineering, SLE 2012, held in Dresden, Germany, in September 2012. The 17 papers presented together with 2 tool demonstration papers were carefully reviewed and selected from 62 submissions. SLE\u0393\u00c7\u00d6s foremost mission is to encourage and organize communication between communities that have traditionally looked at software languages from different, more specialized, and yet complementary perspectives. SLE emphasizes the fundamental notion of languages as opposed to any realization in specific technical spaces.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Compliance testing for wrapper-based API migration\n", "abstract": " Wrapping is an established technique for API migration: the use of a given API within the system under migration is replaced by the use of a wrapper-based re-implementation of said API while using a different (preferred) API underneath. Except for some special cases, wrapper development is a craft. In particular, the compliance of a wrapper with the original API is hard to assess and guidance of wrapper development is very limited. In this paper, we describe a method for wrapper development that is essentially inspired by notions of scenario-based differential testing, API contracts as well as selective capture and replay of program executions. The method supports compliance testing of the wrapper under development against the original API; it guides the developer in improving compliance incrementally; it also allows for precise capture of unresolved differences between original API and wrapper. The method is evaluated by a study of wrapper development with different wrappers in the domains of XML processing, byte-code engineering, and GUI programming.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Understanding variability abstraction and realization\n", "abstract": " Software product line engineering (SPLE) emerged as a successful software reuse paradigm. The essence of SPLE is the process of factoring out commonalities and systematizing variabilities, that is, differences, among the products in a SPL. In this talk, I will take the position that this process is the very act of abstraction. Thus, as suggested by Coplien et al. [8], the purpose of abstraction mechanisms, such as subroutines and inheritance in programming languages and architectural patterns and platforms in architectural design, is to support factoring out commonalities and making variabilities explicit.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Virtual separation of concerns: toward preprocessors 2.0\n", "abstract": " Conditional compilation with preprocessors such as cpp is a simple but effective means to implement variability. By annotating code fragments with# ifdef and# endif directives, different program variants with or without these annotated fragments can be created, which can be used (among others) to implement software product lines. Although, such annotation-based approaches are frequently used in practice, researchers often criticize them for their negative effect on code quality and maintainability. In contrast to modularized implementations such as components or aspects, annotation-based implementations typically neglect separation of concerns, can entirely obfuscate the source code, and are prone to introduce subtle errors.Our goal is to rehabilitate annotation-based approaches by showing how tool support can address these problems. With views, we emulate modularity; with a visual representation of annotations, we reduce source code obfuscation and increase program comprehension; and with disciplined annotations and a productline\u0393\u00c7\u00f4aware type system, we prevent or detect syntax and type errors in the entire software product line. At the same time we emphasize unique benefits of annotations, including simplicity, expressiveness, and being language independent. All in all, we provide tool-based separation of concerns without necessarily dividing source code into physically separated modules; we name this approach virtual separation of concerns.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Mining implementation recipes of framework-provided concepts in dynamic framework API interaction traces\n", "abstract": " Application developers often apply the Monkey See/Monkey Do rule for framework-based application development, ie, they use existing applications as a guide to understand how to implement a desired framework-provided concept (eg, a context menu in an Eclipse view). However, the code that implements the concept of interest might be scattered across and tangled with code implementing other concepts. To address this issue, we introduce a novel framework comprehension technique called FUDA (F rameworkAPIU nderstandingthroughD ynamicA nalysis). The main idea of this technique is to extract the implementation recipes of a given framework-provided concept from dynamic traces with the help of a dynamic slicing approach integrated with clustering and data mining techniques. In this demonstration, we present the prototype implementation of FUDA as two Eclipse plug-ins, and use them to generate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["34"]}
{"title": "Round-trip engineering of eclipse plug-ins using eclipse workbench part interaction FSML\n", "abstract": " A Framework-Specific Modeling Language (FSML) is a kind of Domain-Specific Modeling Language that is used for modeling framework-based software. FSMLs enable automated round-trip engineering over non-trivial model-to-code mappings and thereby simplify the task of creating and evolving framework-based applications. In this demonstration, we present a prototype implementation of Eclipse Workbench Part Interaction, a FSML capturing an aspect of Eclipse plug-in development. We walk through an example Eclipse plug-in development scenario and demonstrate the round-trip engineering capabilities of the prototype.", "num_citations": "2\n", "authors": ["34"]}
{"title": "Tutorial on generative software development\n", "abstract": " Software product line engineering (SPLE) [5] seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In SPLE, new system variants can be rapidly created based on a set of reusable assets, such as a common architecture, components, and models. Generative software development [6] aims at modeling and implementing product lines in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages (DSLs) [13, 4, 15, 8, 3, 1, 12, 14].", "num_citations": "2\n", "authors": ["34"]}
{"title": "Generative Programmierung\n", "abstract": " Frameworks und Entwurfsmuster sollen die Anpa\u251c\u0192barkeit und die Wiederverwendbarkeit von Softwaresystemen erh\u251c\u2562hen. CORBA, ActiveX, Java usw. dienen vor allem aber der verbesserten Interoperabilit\u251c\u00f1t. Mit den vorhandenen objektorientierten Mitteln ist es jedoch nicht praktikabel, viele Anteile eines Systems bzw. einer Komponente offen zu lassen, so da\u251c\u0192 vorhergesehene und unvorhersehbare \u251c\u00e4nderungen und Erweiterungen m\u251c\u2562glich sind. W\u251c\u00f1hrend des Designs und der Implementierung werden in der Regel zahlreiche konkrete Entwurfsentscheidungen getroffen, welche die Anpa\u251c\u0192barkeit und die Wiederverwendbarkeit beeintr\u251c\u00f1chtigen. Der Einsatz von Interoperabilit\u251c\u00f1tstechniken schafft hier zwar Abhilfe, f\u251c\u255dhrt aber auch zu einem Verlust von Effizienz und dem Ansteigen von Redundanz.", "num_citations": "2\n", "authors": ["34"]}
{"title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection\n", "abstract": " Existing point-cloud based 3D object detectors use convolution-like operators to process information in a local neighbourhood with fixed-weight kernels and aggregate global context hierarchically. However, non-local neural networks and self-attention for 2D vision have shown that explicitly modeling long-range interactions can lead to more robust and competitive models. In this paper, we propose two variants of self-attention for contextual modeling in 3D object detection by augmenting convolutional features with self-attention features. We first incorporate the pairwise self-attention mechanism into the current state-of-the-art BEV, voxel and point-based detectors and show consistent improvement over strong baseline models of up to 1.5 3D AP while simultaneously reducing their parameter footprint and computational cost by 15-80% and 30-50%, respectively, on the KITTI validation set. We next propose a self-attention variant that samples a subset of the most representative features by learning deformations over randomly sampled locations. This not only allows us to scale explicit global contextual modeling to larger point-clouds, but also leads to more discriminative and informative feature descriptors. Our method can be flexibly applied to most state-of-the-art detectors with increased accuracy and parameter and compute efficiency. We show our proposed method improves 3D object detection performance on KITTI, nuScenes and Waymo Open datasets. Code is available at https://github.com/AutoVision-cloud/SA-Det3D.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Improved Policy Extraction via Online Q-Value Distillation\n", "abstract": " Deep neural networks are capable of solving complex control tasks in challenging environments, but their learned policies are hard to interpret. Not being able to explain or verify them limits their practical applicability. By contrast, decision trees lend themselves well to explanation and verification, but are not easy to train, especially in an online fashion. In this work we introduce Q-BSP trees and propose an Ordered Sequential Monte Carlo training algorithm that efficiently distills the Q-function from fully trained deep Q-networks into a tree structure. Q-BSP forests are used to generate the partitioning rules that transparently reconstruct an accurate value function. We explain our approach and provide results that convincingly beat earlier online policy distillation methods with respect to their own performance benchmarks.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Autonomous Vehicle Visual Signals for Pedestrians: Experiments and Design Recommendations\n", "abstract": " Autonomous Vehicles (AV) will transform transportation, but also the interaction between vehicles and pedestrians. In the absence of a driver, it is not clear how an AV can communicate its intention to pedestrians. One option is to use visual signals. To advance their design, we conduct four human-participant experiments and evaluate six representative AV visual signals for visibility, intuitiveness, persuasiveness, and usability at pedestrian crossings. Based on the results, we distill twelve practical design recommendations for AV visual signals, with focus on signal pattern design and placement. Moreover, the paper advances the methodology for experimental evaluation of visual signals, including lab, closed-course, and public road tests using an autonomous vehicle. In addition, the paper also reports insights on pedestrian crosswalk behaviours and the impacts of pedestrian trust towards AVs on the behaviors. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["34"]}
{"title": "Safe Swerve Maneuvers for Autonomous Driving\n", "abstract": " This paper characterizes safe following distances for on-road driving when vehicles can avoid collisions by either braking or by swerving into an adjacent lane. In particular, we focus on safety as defined in the Responsibility-Sensitive Safety (RSS) framework. We extend RSS by introducing swerve maneuvers as a valid response in addition to the already present brake maneuver. These swerve maneuvers use the more realistic kinematic bicycle model rather than the double integrator model of RSS. We show that these swerve maneuvers allow a vehicle to safely follow a lead vehicle more closely than the RSS braking maneuvers do. The use of the kinematic bicycle model is then validated by comparing these swerve maneuvers to swerves of a dynamic single-track model. The analysis in this paper can be used to inform both offline safety validation as well as safe control and planning.", "num_citations": "1\n", "authors": ["34"]}
{"title": "MapleCOMSPS LRB VSIDS and MapleCOM-SPS CHB VSIDS in the 2019 Competition,\u0393\u00c7\u00a5\n", "abstract": " This document describes the SAT solvers Maple-COMSPS LRB VSIDS and MapleCOMSPS CHB VSIDS that implement our machine learning branching heuristics called the learning rate branching heuristic (LRB) and the conflict historybased branching heuristic (CHB).", "num_citations": "1\n", "authors": ["34"]}
{"title": "Modeling the effects of AUTOSAR overheads on application timing and schedulability\n", "abstract": " AUTOSAR (AUTomotive Open System ARchitecture) provides an open and standardized E/E architecture for automobiles. AUTOSAR systems exhibit real-time requirements, i.e., an AUTOSAR application must always be schedulable. In this paper, we propose an overhead-aware method to find schedulable design configurations for an AUTOSAR application. We show how to construct a timing model for the application, discuss how to quantify the overheads of an AUTOSAR stack implementation, and assess their impact on timing and schedulability. We demonstrate the proposed method on an automotive case study and evaluate the effects of different types of overheads using synthetic applications.", "num_citations": "1\n", "authors": ["34"]}
{"title": "A worst-case analysis of constraint-based algorithms for exact multi-objective combinatorial optimization\n", "abstract": " In a multi-objective combinatorial optimization (MOCO) problem, multiple objectives must be optimized simultaneously. In past years, several constraint-based algorithms have been proposed for finding Pareto-optimal solutions to MOCO problems that rely on repeated calls to a constraint solver. Understanding the properties of these algorithms and analyzing their performance is an important problem. Previous work has focused on empirical evaluations on benchmark instances. Such evaluations, while important, have their limitations. Our paper adopts a different, purely theoretical approach, which is based on characterizing the search space into subspaces and analyzing the worst-case performance of a MOCO algorithm in terms of the expected number of calls to the underlying constraint solver. We apply the approach to two important constraint-based MOCO algorithms. Our analysis reveals a deep\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["34"]}
{"title": "Modeling and reasoning with multisets and multirelations in Alloy\n", "abstract": " Multisets and multirelations arise naturally in modeling; however, most modeling languages either have limited or completely lack support for multisets and multirelations. Alloy, for instance, is a lightweight relational modeling language which provides automatic analysis of models. In Alloy, ordinary sets and relations are the only first-class language semantic constructs; therefore to work with multisets and multirelations, modelers need to invent ad-hoc ways to encode these multiconcepts or rely on a third-party library that provides their implementations, assuming there is such one. In fact, such a library has been missing for Alloy, and implementing a fully functional multiconcepts library is challenging, especially when it is required to encode an algebra of operations over multiconcepts. This thesis presents two sound and practical mathematical formalizations of multiconcepts, namely, index-based and multiplicity-based, which encode multisets and multirelations using only basic concepts such as ordinary sets, total functions and natural numbers. We implement two generic multiconcepts libraries in Alloy based on the corresponding formalizations. Each library has a carefully designed interface and can be seamlessly integrated into existing relational models. We also perform an empirical evaluation on both implementations; the result shows multiplicity-based encoding is more scalable in terms of performance; thus, it is more preferable in practice.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Reasoning about product lines of cyber-physical systems with clafer\n", "abstract": " Cyber-physical systems (CPS) combine complex machines, computers, networks, and people and are poised to change how we interact with the physical world. Example CPS include self-driving cars and intelligent buildings. Elements of CPS are often engineered as product lines in order to fit different contexts and markets. Computer modeling is essential to engineering such systems as analyzing and evolving virtual blueprints is easier and more cost-effective than performing these tasks on physical models or systems.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Modeling and Reasoning with Multirelations, and their encoding in Alloy.\n", "abstract": " Multisets and multirelations arise naturally in modeling. In this paper, we present a sound and practical mathematical framework, which encodes multisets and multirelations using only ordinary sets and total functions. We implement the encoding as a mutliconcepts library in Alloy, which is declarative, compatible with ordinary sets and relations, and can be incorporated into existing models seamlessly.", "num_citations": "1\n", "authors": ["34"]}
{"title": "MapleGlucose and MapleCMS\n", "abstract": " This document describes the SAT solvers MapleGlucose and MapleCMS, two solvers implementing our machine learning branching heuristic called the learning rate branching heuristic (LRB).", "num_citations": "1\n", "authors": ["34"]}
{"title": "Performance prediction upon toolchain migration in model-based software\n", "abstract": " Changing the development environment can have severe impacts on the system behavior such as the execution-time performance. Since it can be costly to migrate a software application, engineers would like to predict the performance parameters of the application under the new environment with as little effort as possible. In this paper, we concentrate on model-driven development and provide a methodology to estimate the execution-time performance of application models under different toolchains. Our approach has low cost compared to the migration effort of an entire application. As part of the approach, we provide methods for characterizing model-driven applications, an algorithm for generating application-specific microbenchmarks, and results on using different methods for estimating the performance. In the work, we focus on SCADE as the development toolchain and use a Cruise Control and a Water\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["34"]}
{"title": "Unifying product and software configuration (Dagstuhl Seminar 14172)\n", "abstract": " Research on computer-supported configuration of customizable products and services is currently carried out in two main communities: one community is mainly focused on the configuration of hardware artifacts, the other one is interested in configurable software systems and software product lines. Despite the significant overlap in research interests, the fields have mainly evolved in isolation in different fields such as Artificial Intelligence, Constraint Programming and Software Engineering. Yet, the communities have produced results that are applicable across the communities. The trend of products becoming increasingly heterogeneous, ie, consisting of hardware, software and services, is furthermore increasingly blurring the line between the configuration domains in practice. This report documents the program and the outcomes of Dagstuhl Seminar 14172\" Unifying Product and Software Configuration\". The seminar gathered researchers and practitioners working on configuration problems. The seminar consisted of invited presentations and working group sessions covering various topics of software and product configuration including knowledge representation issues, automated reasoning and configuration management and had a particular focus on the industry perspective.", "num_citations": "1\n", "authors": ["34"]}
{"title": "A Taxonomic Space for Increasingly Symmetric Model Synchronization\n", "abstract": " A pipeline of unidirectional model transformations is a wellunderstood architecture for model driven engineering tasks such as model compilation or view extraction. However, modern applications seem to require a shift towards networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools and a solid semantic foundation. We address the latter by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to its set of synchronization requirements and a corresponding algebraic structure modeling the intended semantics. The space aims to help with identifying and communicating the right tool and theory for the synchronization problem at hand. It also intends to guide future theoretical and tool research.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Symmetrization of Model Transformations: Towards a Rational Taxonomy of Model Synchronization Types\n", "abstract": " A pipeline of unidirectional model transformations is a wellunderstood architecture for model driven engineering tasks such as model compilation or view extraction. However, modern applications seem to require a shift towards networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools and a solid semantic foundation.To address the issue, we present a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to its set of synchronization requirements, and to a corresponding algebraic structure modeling the intended semantics. The space can help to identify and to communicate the right tool and theory for the synchronization problem at hand, and guide future theoretical and tool research.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Designing variability modeling languages\n", "abstract": " The essence of software product line engineering (SPLE) is the process of factoring out commonalities and systematizing variabilities, that is, differences, among the products in a SPL. A key discipline in SPLE is variability modeling. It focuses on abstracting the variability realized in the many development artifacts of an SPL, such as code, models, and documents.             This talk will explore the design space of languages that abstract variability, from feature modeling and decision modeling to highly expressive domain-specific languages. This design space embodies a progression of structural complexity, from lists and trees to graphs, correlating with the increasing closeness to implementation. I will also identify a set of basic variability realization mechanisms. I will illustrate the variability abstraction and realization concepts using Clafer, a modeling language designed to support these concepts using a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["34"]}
{"title": "The art of collecting bug reports\n", "abstract": " Kids love bugs, and some kids even collect bugs and keep them in precious \u0393\u00c7\u00a3kill jars.\u0393\u00c7\u00a5 Over a period of time, bug collectors can amass a large number of different species of bugs. Some kids study the bugs they collected and label them based on such characteristics as shape, size, color, number of legs, whether it can fly, and so on. The bugs may be valued differently depending upon how rare they are or how difficult they are to catch. The collection may have some duplicate bugs. But duplicates are rarely identical, as characteristics such as appearance and size can differ widely.But we software developers do not like bugs. We hope to have none in our software, and when they are found, we squash them! Unfortunately, squashing bugs, or more politely, responding to software change requests, is rarely easy. Developers have to study the information about the bug in detail, conduct a thorough investigation on how to resolve the bug, examine its side effects, and eventually decide on and take a course of action. This is a difficult task because, like earthly bugs, software bugs differ widely. Often software bugs that are collected in bug databases of projects are studied in isolation because they are different from the other bugs in their effects on the software system, their cause and location, and their severity. Over time, a project will accumulate duplicate bugs, just as a live-bug collector may have multiple bugs of the same species. And finally, nearly every project knows about more bug reports than it can fix, just as there are too many live bugs to be collected by a single person.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Bridging the Business-IT divide using BPM: Challenges and opportunities\n", "abstract": " Building successful enterprise software requires an effective collaboration among Business and IT stakeholders. Business analysts capture requirements in business terms. Software engineers interpret these requirements and translate them into the domain of software technology. Yet the different backgrounds and skill sets of these two stakeholder groups naturally lead to the proverbial Business-IT divide. The two groups across the divide speak different languages, each with their own terms and semantics. It is not uncommon to see projects fail because of poorly understood and miscommunicated requirements across the Business-IT divide.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Language Support for Domain Specific Languages\n", "abstract": " An attractive way to implement domain specific languages (DSLs) is by writing a library in a host language. Some DSLs, however, do not fit perfectly within the host language and a pure library solution is insufficient. In many cases, metaprogramming offers a solution to the problem. This paper surveys and compares the metaprogramming support offered by three languages\u0393\u00c7\u00f6Template Haskell, C++, and MetaOCaml\u0393\u00c7\u00f6and discusses the techniques they make available to the implementer of a DSL.", "num_citations": "1\n", "authors": ["34"]}
{"title": "Generative programming and software system families\n", "abstract": " Today\u0393\u00c7\u00d6s software engineering practices are aimed at developing single systems. There are attempts to achieve reuse through object- and component-based technologies with two specific goals:to cut development costs, and time-tomarket and to improve quality. But current research and practical experience suggest that only moving from the single system engineering to the systemfamily engineering approach can bring significant progress with respect to these goals [3,6,7].", "num_citations": "1\n", "authors": ["34"]}
{"title": "Model driven architecture\n", "abstract": " \u0393\u00c7\u00f4Modeling and development tools\u0393\u00c7\u00f4Data warehouse systems\u0393\u00c7\u00f4Metadata repositories\u0393\u00c7\u00f3 Metadata= data about data, eg, database schemas; but also: UML models, data transformation rules, APIs expressed in IDL, MIDL, C#, Java, WSDL, etc., business process and workflow models, product configuration descriptors and tuning parameters, information that drives deployment tools and runtime management,\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["34"]}
{"title": "Why CART works for variability-aware performance prediction? an empirical study on performance distributions\n", "abstract": " This report presents follow-up work for our previous technical report \u0393\u00c7\u00a3Variability-Aware Performance Modeling: A Statistical Learning Approach\"(GSDLAB-TR-2012-08-18). We try to give evidence why our approach, based on a statisticallearning technique called Classification And Regression Trees (CART), works for variability-aware performance prediction. To this end, we conduct a comparative analysis of performance distributions on the evaluated case studies and empirically explore why our approach works with small random samples.", "num_citations": "1\n", "authors": ["34"]}