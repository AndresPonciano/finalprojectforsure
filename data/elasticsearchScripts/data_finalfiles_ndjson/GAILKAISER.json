{"title": "An information retrieval approach for automatically constructing software libraries\n", "abstract": " Although software reuse presents clear advantages for programmer productivit. y and code reliability, it is not practiced enough. One of the reasons for the only moderate success of reuse is the lack of software libr'lries that facilitate the actual locating and understanding of reusable components. This paper dpscrihes II technology for automatically I\\Ssembling large softwar~ libraries that promote softwarp rpuse by helping the user locate the components closest to hfO'r/his neeels Softwarp libraries are aut. omatically assemblfO'd from a set. of Ilnorganizpd component. s by using informat. ion rptripval techniques. The constrllction of the library is done in!. wo steps. First, attribut. es; up alJtornal. ically pxtracted from nat. urlll language docllmpntat. ion by IIsing a npIV inJexing scheme ba~ fO'd an t. he nntinns of lexical affinities anci quantity of inforrnllt. ion. Then, a hierarchy for browsing is alJtomat. iclllly generaterl using a clustering techniq1le that draws only nn the informlltion provided by the attribulps. Thanks to the frfO'p-!. pxt inrlexing se-heme, tools following this approach can accept free-style nat'Jral language'luprips.This tpChllOlogy h1\\. S bfO'en impll\" ment.. d in the GURU~ ystpm, whirh h< l5 bpen applied 1.0 construct an organized library of Alx utilities. An exppriment. was rondllrtpd in orrlpr to pvalullte the n\u00b7 t. rievll\\effectivl\" nfO'ss of GURU 1\\. 5 compl\\red to INFOExPLORER a hypprt. pxt lihrary syslplll for Alx 3 on the IBM RISe System/6000 series. We followpr\\Ihe usual evaluation prOf\" pr\\lJr!'us...! in informat. ion rptrifO'vl\\l, hased upon recall and precision ml\" a5IJrPS, and d\" t. erminpd t 1111 I. nllr syst. f'Tll pr> rforms l! i% better on a random test\u00a0\u2026", "num_citations": "655\n", "authors": ["260"]}
{"title": "DOM-based content extraction of HTML documents\n", "abstract": " Web pages often contain clutter (such as pop-up ads, unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of\" useful and relevant\" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to removing clutter or making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage's inherent look and feel. Unlike\" Content Reformatting\", which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses\" Content Extraction\". We have developed a framework that employs easily extensible set of techniques that incorporate advantages of previous work on content extraction. Our key insight is to work with the DOM\u00a0\u2026", "num_citations": "577\n", "authors": ["260"]}
{"title": "Concurrency control in advanced database applications\n", "abstract": " Many advanced computer-based applications, such as computer-aided design and manufacturing(CAD/CAM), network management, financial instruments trading, medical in formatics, office automation, and software development environments(SDES), are data intensive in the sense that they generate and manipulate large amounts of data (eg, all the software artifacts in an SDE). It is desirable to base these kinds of application systems on data management capabilities similar to those provided by database management systems (DBMSS) for traditional data processing. These capabilities include adding, removing, retrieving, and updating data from on-line storage and maintaining the consistency of the information stored in a database. Consistency in a database is maintained if every data item satisfies specific consistency constraints. These are typically implicit in data processing in the sense they are known to the\u00a0\u2026", "num_citations": "514\n", "authors": ["260"]}
{"title": "Intelligent assistance for software development and maintenance\n", "abstract": " An environment is described, called Professor Marvel, that provides early error checking and answers questions about the program under development. The environment has a certain understanding of the systems being developed and how to use tools to produce software. It aids individual programmers and helps coordinate programming teams. The key components of Marvel are a database that stores data represented as objects, as in object-oriented languages, and a model of the development process that imposes a structure on programming activities. Marvel's support of insight and of opportunistic processing is discussed at length, as is the handling of side effects. A sample session is described.< >", "num_citations": "359\n", "authors": ["260"]}
{"title": "Split-transactions for open-ended activities\n", "abstract": " Open-ended activities such as CAD/Cam, VLSI layout and software development require consistent concurrent access and fault tolerance associated with database transactions, but their uncertain duration, uncertain developments during execution and long interaction with other concurrent activities break traditional transaction atomicity boundaries. We propose split-transaction as a new database operation that solves the above problems by permitting transactions to commit data that will not change. Thus an open-ended activity can release the committed data and serialize interactions with other concurrent activities through the committed data. This technical report was originally titled Support of Multiuser Software Development Environments.", "num_citations": "340\n", "authors": ["260"]}
{"title": "Adequate testing and object-oriented programming\n", "abstract": " Adequate testing and object-oriented programming | Journal of Object-Oriented Programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Journal of Object-Oriented Programming Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsJournal of Object-Oriented ProgrammingVol. , No. Adequate testing and object-oriented programming article Adequate testing and object-oriented programming Share on Authors: DE Perry View Profile , GE Kaiser View Profile Authors Info & Affiliations Journal of Object-Oriented ProgrammingVolume 2Issue 5Jan./Feb. 1990 pp 13\u201319 Published:01 January 1990 41citation 0 Downloads Metrics Total Citations41 0 \u2026", "num_citations": "302\n", "authors": ["260"]}
{"title": "The Apache HTTP server project\n", "abstract": " Most reports of Internet collaboration refer to small scale operations among a few authors or designers. However, several projects have shown that the Internet can also be the locus for large scale collaboration. In these projects, contributors from around the world combine their individual forces and develop a product that rivals those of multibillion dollar corporations. The Apache HTTP Server Project is a case in point. This collaborative software development effort has created a robust, feature-rich HTTP server software package that currently dominates the public Internet market (46 percent compared with 16 percent for Microsoft and 12 percent for Netscape, according to a June 1997 survey published by Netcraft). The software and its source code are free, but Apache's popularity is more often attributed to performance than price. The project is managed by the Apache Group, a geographically distributed group of\u00a0\u2026", "num_citations": "218\n", "authors": ["260"]}
{"title": "Automating content extraction of html documents\n", "abstract": " Web pages often contain clutter (such as unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of \u201cuseful and relevant\u201d content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage\u2019s inherent look and feel. Unlike \u201cContent Reformatting,\u201d which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses \u201cContent Extraction.\u201d We have developed a framework that employs an easily extensible set of techniques. It incorporates advantages of previous work on content extraction. Our key insight is to work with DOM trees, a W3C specified\u00a0\u2026", "num_citations": "170\n", "authors": ["260"]}
{"title": "A paradigm for decentralized process modeling and its realization in the oz environment\n", "abstract": " We present a model for decentralized Process Centered Environments (PCEs), which support concerted efforts among geographically-dispersed teams - each local team with its own autonomous process - and emphasize flexibility in the tradeoff between collaboration vs. autonomy. We consider both decentralized process modeling and decentralized process enaction. We describe a realization in the OZ decentralized PCE, which employs a rule-based formalism, and also investigate the application to PCEs based on Petri-nets.< >", "num_citations": "170\n", "authors": ["260"]}
{"title": "Kinesthetics extreme: An external infrastructure for monitoring distributed legacy systems\n", "abstract": " Autonomic computing - self-configuring, self-healing, self-optimizing applications, systems and networks - is widely believed to be a promising solution to ever-increasing system complexity and the spiraling costs of human system management as systems scale to global proportions. Most results to date, however, suggest ways to architect new software constructed from the ground up as autonomic systems, whereas in the real world organizations continue to use stovepipe legacy systems and/or build ''systems of systems'' that draw from a gamut of new and legacy components involving disparate technologies from numerous vendors. Our goal is to retrofit autonomic computing onto such systems, externally, without any need to understand or modify the code, and in many cases even when it is impossible to recompile. We present a meta-architecture implemented as active middleware infrastructure to explicitly add\u00a0\u2026", "num_citations": "167\n", "authors": ["260"]}
{"title": "Properties of machine learning applications for use in metamorphic testing\n", "abstract": " It is challenging to test machine learning (ML) applications, which are intended to learn properties of data sets where the correct answers are not already known. In the absence of a test oracle, one approach to testing these applications is to use metamorphic testing, in which properties of the application are exploited to define transformation functions on the input, such that the new output will be unchanged or can easily be predicted based on the original output; if the output is not as expected, then a defect must exist in the application. Here, we seek to enumerate and classify the metamorphic properties of some machine learning algorithms, and demonstrate how these can be applied to reveal defects in the applications of interest. In addition to the results of our testing, we present a set of properties that can be used to define these metamorphic relationships so that metamorphic testing can be used as a general approach to testing machine learning applications.", "num_citations": "163\n", "authors": ["260"]}
{"title": "Self-managing systems: A control theory foundation\n", "abstract": " The high cost of operating large computing installations has motivated a broad interest in reducing the need for human intervention by making systems self-managing. This paper explores the extent to which control theory can provide an architectural and analytic foundation for building self-managing systems, either from new components or layering on top of existing components. Further, we propose a deployable testbed for autonomic computing (DTAC) that we believe will reduce the barriers to addressing key research problems in autonomic computing. The initial DTAC architecture is described along with several problems that it can be used to investigate.", "num_citations": "159\n", "authors": ["260"]}
{"title": "Models of software development environments\n", "abstract": " We present a general model of software development environments that consists of three components: structures, mechanisms and policies. The advantage of this model is that it distinguishes intuitively those aspects of an environment that are useful in comparing and contrasting software development environments. Our initial application of the model is to characterize four classes of environments by means of a sociological metaphor based on scale: the individual, the family, the city and the state models. The utility of the IFCS taxonomy is that it delineates the important classes of interactions among software developers and exposes the ways in which current software development environments inadequately support the development of large systems. We demonstrate the generality of our model by also applying it to a previously published taxonomy that categorizes environments according to how they relate to four historical trends: language-centered, structure-oriented, toolkit and method-based environments.", "num_citations": "159\n", "authors": ["260"]}
{"title": "An Architecture for Intelligent Assistance in Software Development.\n", "abstract": " The authors define an architecture for a software engineering environment that behaves as an intelligent assistant. This architecture consists of two key aspects, an objectbase and a model of the software development process. The objectbase is adapted from other research, but the model is unique in that it consists primarily of rules that define the preconditions and multiple postconditions of software development tools. Metarules define forward and backward chaining among the rules. The most significant contribution is opportunistic processing, whereby the environment automatically performs software development activities at some time between when their preconditions are satisfied and when their postconditions are required. Further, this model defines strategies that guide the assistant in choosing an appropriate point for carrying out each activity. AuthorDescriptors:", "num_citations": "148\n", "authors": ["260"]}
{"title": "Experience with process modeling in the marvel software development environment kernel\n", "abstract": " We have been working for several years on rule-based process modeling and the implementation of such models as part of the foundation for software development environments. We have defined a kernel, called MARVEL, for such an architecture and implemented several successive versions of the kernel and several small environments using the kernel. We have evaluated our results to date, and discovered several significant flaws and delineated several important open problems. Although the details are specific to rule-based process modeling, we believe that our insights will be valuable to other researchers and developers contemplating process modeling mechanisms.", "num_citations": "139\n", "authors": ["260"]}
{"title": "A control theory foundation for self-managing computing systems\n", "abstract": " The high cost of operating large computing installations has motivated a broad interest in reducing the need for human intervention by making systems self-managing. This paper explores the extent to which control theory can provide an architectural and analytic foundation for building self-managing systems. Control theory provides a rich set of methodologies for building automated self-diagnosis and self-repairing systems with properties such as stability, short settling times, and accurate regulation. However, there are challenges in applying control theory to computing systems, such as developing effective resource models, handling sensor delays, and addressing lead times in effector actions. We propose a deployable testbed for autonomic computing (DTAC) that we believe will reduce the barriers to addressing research problems in applying control theory to computing systems. The initial DTAC architecture is\u00a0\u2026", "num_citations": "137\n", "authors": ["260"]}
{"title": "Scaling up rule-based software development environments\n", "abstract": " Rule-based software development environments (RBDEs) model the software development process in terms of rules that encapsulate development activities, and assist in executing the process via forward and backward chaining over the rule base. We investigate the scaling up of RBDEs to support (1) multiple views of the rule base for multiple users and (2) evolution of the rule base over the lifetime of a project. Our approach is based on clarifying two distinct functions of rules and chaining: maintaining consistency and automation. By definition, consistency is mandatory whereas automation is not. Distinguishing the consistency and automation aspects of RBDE assistance mechanisms makes it possible to formalize the range of compatible views and the scope of mechanizable evolution steps. Throughout the paper, we use the Marvel RBDE as an example application of our ideas.", "num_citations": "126\n", "authors": ["260"]}
{"title": "Automatic system testing of programs without test oracles\n", "abstract": " Metamorphic testing has been shown to be a simple yet effective technique in addressing the quality assurance of applications that do not have test oracles, ie, for which it is difficult or impossible to know what the correct output should be for arbitrary input. In metamorphic testing, existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can easily be computed based on the original output. That is, if input x produces output f (x), then we create input x'such that we can predict f (x') based on f (x); if the application does not produce the expected output, then a defect must exist, and either f (x), or f (x')(or both) is wrong.", "num_citations": "112\n", "authors": ["260"]}
{"title": "Database support for knowledge-based engineering environments\n", "abstract": " MARVEL is an architecture for implementing knowledge-based engineering environments that integrate collections of computerized engineering tools. The architecture uses expert system technology to automatically carry out many of the menial chores otherwise done by engineering project personnel. It consists of a knowledge representation language, a translator for the language, and a database manager that acts as the run-time support for the language. Three implementations of MARVEL have been completed, and we are currently designing the fourth.", "num_citations": "102\n", "authors": ["260"]}
{"title": "A flexible transaction model for software engineering\n", "abstract": " It is generally recognized that the classical transaction model, providing atOITIlClty and serializability, is too strong for certain application areas since it unnecessarily restricts concurrency. We are concerned with supporting cooperative work in multi-user design environments, particularly teams of programmers cooperating to develop and maintain software systems. We present an extended transaction model that meets the special requirements of software engineering projects, describe possible implementation techniques, and discuss a number of issues regarding the incorporation of such a model into multi-user software development environments.", "num_citations": "100\n", "authors": ["260"]}
{"title": "SWAP: leveraging the web to manage workflow\n", "abstract": " Many organizations are beginning to discover what workflow vendors already know-namely, that the real value of the Web lies not just in its documents and resources, but also in the activities surrounding them. Collaborative work involves not only handoff and routing of data between humans, but the coordination of activities among them and with automated agents as well. Workflow engines typically ensure that the information ends up on the right desktop along with the tools to accomplish a slated task. It is difficult to synchronize work and activity tracking within a technically diverse organization. Tools and formats typically differ among workgroups, as do skill levels and understanding among individual participants in a process. Browser-based user interfaces offer a mechanism to easily access distributed information and hand off documents and data over the Web, but at the expense of being able to effectively\u00a0\u2026", "num_citations": "95\n", "authors": ["260"]}
{"title": "An approach to autonomizing legacy systems\n", "abstract": " Adding adaptation capabilities to existing distributed systems is a major concern. The question addressed here is how to retrofit existing systems with self-healing, adaptation andor self-management capabilities. The problem is obviously intensified for systems of systems composed of components, whether new or legacy, that may have been developed by different vendors, mixing and matching COTS and open source components. This system composition model is expected to be increasingly common in high performance computing. The usual approach is to train technicians to understand the complexities of these components and their connections, including performance tuning parameters, so that they can then manually monitor and reconfigure the system as needed. We envision instead attaching a standard feedback-loop infrastructure to existing distributed systems for the purposes of continual monitoring and dynamically adapting their activities and performance. This approach can also be applied to new systems, as an alternative to building in adaptation facilities, but we do not address that here. Our proposed infrastructure consists of multiple layers with the objectives of probing, measuring and reporting of activity and state within the execution of the legacy system among its components and connectors gauging, analysis and interpretation of the reported events and possible feedback to focus the probes and gauges to drill deeper, or-when necessary-direct but automatic reconfiguration of the running system.Descriptors:", "num_citations": "94\n", "authors": ["260"]}
{"title": "Computer and world wide web accessibility by visually disabled patients: Problems and solutions\n", "abstract": " Rapid advances in information technology have dramatically transformed the world during the past several decades. Access to computers and the World Wide Web is increasingly required for education and employment, as well as for many activities of daily living. Although these changes have improved society in many respects, they present an obstacle for visually disabled patients who may have significant difficulty processing the visual cues presented by modern graphical user interfaces. This article reviews the specific barriers to computer and Web access faced by visually disabled patients, describes clinical evaluation methods, summarizes traditional low vision methods as well as newer assistive computer technologies for universal accessibility, and discusses emerging technologies and future directions in this area.", "num_citations": "93\n", "authors": ["260"]}
{"title": "Multiuser, distributed language-based environments\n", "abstract": " How do you keep teams of programmers informed of system changes without burying them in mail messages? Make the environment responsible for propagating changes.", "num_citations": "92\n", "authors": ["260"]}
{"title": "An architecture for multi-user software development environments\n", "abstract": " We present an architecture for multi-user software development environments, covering both general and process-centered MUSDEs. Our architecture is founded on componentization, with particular concern for the capability to replace the synchronization component-to allow experimentation with novel concurrency control mechanisms that support collaborative work-with minimal effects on other components while still supporting integration. The architecture has been implemented for the MeRvBI-SDE, and we report our experience replacing and tailoring several parts of the qynchronization component as part of MeRvnL.", "num_citations": "91\n", "authors": ["260"]}
{"title": "Retrofitting autonomic capabilities onto legacy systems\n", "abstract": " sec:abstractnak Autonomic computing\u2014self-configuring, self-healing, self-managing applications, systems and networks\u2014is a promising solution to ever-increasing system complexity and the spiraling costs of human management as systems scale to global proportions. Most results to date, however, suggest ways to architect new software designed from the ground up as autonomic systems, whereas in the real world organizations continue to use stovepipe legacy systems and/or build \u201csystems of systems\u201d that draw from a gamut of disparate technologies from numerous vendors. Our goal is to retrofit autonomic computing onto such systems, externally, without any need to understand, modify or even recompile the target system's code. We present an autonomic infrastructure that operates similarly to active middleware, to explicitly add autonomic services to pre-existing systems via continual monitoring and\u00a0\u2026", "num_citations": "90\n", "authors": ["260"]}
{"title": "Phosphor: Illuminating dynamic data flow in commodity jvms\n", "abstract": " Dynamic taint analysis is a well-known information flow analysis problem with many possible applications. Taint tracking allows for analysis of application data flow by assigning labels to data, and then propagating those labels through data flow. Taint tracking systems traditionally compromise among performance, precision, soundness, and portability. Performance can be critical, as these systems are often intended to be deployed to production environments, and hence must have low overhead. To be deployed in security-conscious settings, taint tracking must also be sound and precise. Dynamic taint tracking must be portable in order to be easily deployed and adopted for real world purposes, without requiring recompilation of the operating system or language interpreter, and without requiring access to application source code. We present Phosphor, a dynamic taint tracking system for the Java Virtual Machine\u00a0\u2026", "num_citations": "89\n", "authors": ["260"]}
{"title": "Dynamic restructuring of transactions\n", "abstract": " Open-ended activities are characterized by uncertain duration, unpredictable developments, and interactions with other concurrent activities. Like other database applications, they require consistent concurrent access and fault-tolerance, but their unconventional characteristics are incompatible with the conventional database mechanisms of concurrency and failure atomicity. We present the split-transaction and join-transaction operations for restructuring in-progress transactions, as an approach to consistent concurrent access and fault-tolerance for open-ended activities. Split-transaction divides an on-going transaction into two or more transactions that are serializable with respect to each other and all other transactions, and each of the new transactions is later committed or aborted independently of the others. Join-transaction merges two or more transactions that are serializable with respect to each other into a single transaction as if they had always been part of the same transaction, and all their work is now committed or aborted together. Split-transaction is useful for committing some work early or dividing on-going work among several co-workers. Join-transaction allows to hand over results to a co-worker to integrate into his or her own ongoing task. The transaction manager enforces that the new transaction (s) will in fact be serializable, and does not permit the split or join otherwise.", "num_citations": "89\n", "authors": ["260"]}
{"title": "Living with inconsistency in large systems\n", "abstract": " This technical report consists of the three related papers. Uving with Inconsistency in Large Systems describes CONMAN, an environment that identifies and tracks version inconsistencies, permitting debugging and testing to proceed even though the executable image contains cenain non-fatal inconsistencies. The next two papers are both from the INFUSE project. Change Management for Very Large Software Systems presents the new non-Euclidean hierarchical clustering algorithm used by the INFUSE change management system to cluster modules accord-ing to the strengths of their interdependencies. Models of Software Development Environments presents a general model of software development environments consisting of three components-policies, mechanisms and structures-and classifies existing and proposed environments into the individul, family. city and state classes according to the size of projects that could be adequately supponed.", "num_citations": "89\n", "authors": ["260"]}
{"title": "A paradigm for decentralized process modeling\n", "abstract": " A Paradigm for Decentralized Process Modeling presents a novel approach to decentralized process modeling that combines both trends and suggests a paradigm for decentralized PCEs, supporting concerted efforts among geographically-dispersed teams-each local individual or team with its own autonomous process-with emphasis on flexible control over the degree of collaboration versus autonomy provided. A key guideline in this approach is to supply abstraction mechanisms whereby pre-existing processes (or workflows) can be encapsulated and retain security of their internal artifacts and status data, while agreeing with other processes on formal interfaces through which all their interactions are conducted on intentionally shared information. This book is primarily intended to provide an in-depth discussion of decentralized process modeling and enactment technology, covering both high-level concepts and a full-blown realization of these concepts in a concrete system. Either the whole book or selected chapters could be used in a graduate course on software engineering, software process, or software development environments, or even for a course on workflow systems outside computer science (eg, in a classical engineering department for engineering design, or in a business school for business practices or enterprise-wide management, or in the medical informatics department of a health science institution concerned with computer-assistance for managed care). Selected portions of the book, such as section 2.2 on Marvel, could also be employed as a case study in advanced undergraduate software engineering courses. A\u00a0\u2026", "num_citations": "87\n", "authors": ["260"]}
{"title": "Systems and methods for content extraction\n", "abstract": " A content extraction process may parse markup language text into a hierarchical data model and then apply one or more filters. Output filters may be used to make the process more versatile. The operation of the content extraction process and the one or more filters may be controlled by one or more settings set by a user, or automatically by a classifier. The classifier may automatically enter settings by classifying markup language text and entering settings based on this classification. Automatic classification may be performed by clustering unclassified markup language texts with previously classified markup language texts.", "num_citations": "85\n", "authors": ["260"]}
{"title": "Using process technology to control and coordinate software adaptation\n", "abstract": " We have developed an infrastructure for end-to-end run-time monitoring, behavior/performance analysis, and dynamic adaptation of distributed software. This infrastructure is primarily targeted to pre-existing systems and thus operates outside the target application, without making assumptions about the target's implementation, internal communication/computation mechanisms, source code availability, etc. This paper assumes the existence of the monitoring and analysis components, presented elsewhere, and focuses on the mechanisms used to control and coordinate possibly complex repairs/reconfigurations to the target system. These mechanisms require lower-level effectors somehow attached to the target system, so we briefly sketch one such facility (elaborated elsewhere). Our main contribution is the model, architecture, and implementation of Workflakes, the decentralized process engine we use to tailor\u00a0\u2026", "num_citations": "85\n", "authors": ["260"]}
{"title": "Unit test virtualization with VMVM\n", "abstract": " Testing large software packages can become very time intensive. To address this problem, researchers have investigated techniques such as Test Suite Minimization. Test Suite Minimization reduces the number of tests in a suite by removing tests that appear redundant, at the risk of a reduction in fault-finding ability since it can be difficult to identify which tests are truly redundant. We take a completely different approach to solving the same problem of long running test suites by instead reducing the time needed to execute each test, an approach that we call Unit Test Virtualization. With Unit Test Virtualization, we reduce the overhead of isolating each unit test with a lightweight virtualization container. We describe the empirical analysis that grounds our approach and provide an implementation of Unit Test Virtualization targeting Java applications. We evaluated our implementation, VMVM, using 20 real-world Java\u00a0\u2026", "num_citations": "83\n", "authors": ["260"]}
{"title": "A holistic approach to service survivability\n", "abstract": " We present SABER (Survivability Architecture: Block, Evade, React), a proposed survivability architecture that blocks, evades and reacts to a variety of attacks by using several security and survivability mechanisms in an automated and coordinated fashion. Contrary to the ad hoc manner in which contemporary survivable systems are built-using isolated, independent security mechanisms such as firewalls, intrusion detection systems and software sandboxes-SABER integrates several different technologies in an attempt to provide a unified framework for responding to the wide range of attacks malicious insiders and outsiders can launch.", "num_citations": "81\n", "authors": ["260"]}
{"title": "Federating process-centered environments: the Oz experience\n", "abstract": " We describe two models for federating process-centered environments (PCEs): homogeneous federation among distinct instances of the same environment framework enacting the same or different process models, and heterogeneous federation among diverse process enactment systems. We identify the requirements and consider possible architectures for each model, although we concentrate primarily on the homogeneous case. The bulk of the paper presents our choice of architecture, and corresponding infrastructure, for homogeneous federation among MARVEL environment instances as realized in the OZ system. We briefly consider how a single MARVEL environment, or an OZ federation of MARVEL environments, might be integrated into a heterogeneous federation based on ProcessWall's facilities for interoperating PCEs.", "num_citations": "75\n", "authors": ["260"]}
{"title": "Rule chaining in marvel: Dynamic binding of parameters\n", "abstract": " Marvel is a software development environment that uses a rule-based model of the development process. When a user issues a command, Marvel invokes the corresponding rule and binds the rule's formal parameter to the object selected by the user. If firing the rule changes the object in a way that satisfies the conditions of other rules, Marvel automatically fires each of these rules and tries to bind their formal parameters to objects. Marvel must infer which objects to bind to the formal parameters of the rules in the chain. A problem arises when the classes of the objects manipulated by the rules are different, because Marvel has to determine which object to bind to the parameter of each rule in the chain. Recursive data definitions cause chaining between rules that act on different objects in the same class. Methods developed to address this chaining problem are discussed.< >", "num_citations": "75\n", "authors": ["260"]}
{"title": "Retina: helping students and instructors based on observed programming activities\n", "abstract": " It is difficult for instructors of CS1 and CS2 courses to get accurate answers to such critical questions as\" how long are students spending on programming assignments?\", or\" what sorts of errors are they making?\" At the same time, students often have no idea of where they stand with respect to the rest of the class in terms of time spent on an assignment or the number or types of errors that they encounter. In this paper, we present a tool called Retina, which collects information about students' programming activities, and then provides useful and informative reports to both students and instructors based on the aggregation of that data. Retina can also make real-time recommendations to students, in order to help them quickly address some of the errors they make. In addition to describing Retina and its features, we also present some of our initial findings during two trials of the tool in a real classroom setting.", "num_citations": "74\n", "authors": ["260"]}
{"title": "WWW\u2010based collaboration environments with distributed tool services\n", "abstract": " We have developed an architecture for a general\u2010purpose framework for hypermedia collaboration environments that support purposeful work by orchestrated teams. The hypermedia represents all plausible multimedia artifacts concerned with the collaborative task(s) at hand that can be placed or generated on\u2010line, from application\u2010specific materials (e.g., source code, chip layouts, blueprints) to formal documentation to digital library resources to informal email and chat transcripts. The framework capabilities support both internal (WWW\u2010style hypertext) and external (non\u2010WWW open hypertext link server) links among these artifacts, which can be added incrementally as useful connections are discovered; project\u2010specific intelligent hypermedia search and browsing; automated construction of artifacts and hyperlinks according to the semantics of the group and individual tasks and the overall workflow\u00a0\u2026", "num_citations": "72\n", "authors": ["260"]}
{"title": "A mobile agent approach to process-based dynamic adaptation of complex software systems\n", "abstract": " We describe an approach based upon software process technology to on-the-fly monitoring, redeployment, reconfiguration, and in general dynamic adaptation of distributed software applications. We choose the term dynamic adaptation to refer to modifications in structure and behavior that can be made to individual components, as well as sets thereof, or the overall target system configuration, such as adding, removing or substituting components, while the system is running and without bringing it down. The goal of dynamic adaptation is manifold: supporting run-time software composition, enforcing adherence to requirements, ensuring uptime and quality of service of mission-critical systems, recovering from and preventing faults, seamless system upgrading, etc. Our approach involves dispatching and coordinating software agents - named Worklets \u2014 via a process engine, since successful dynamic\u00a0\u2026", "num_citations": "71\n", "authors": ["260"]}
{"title": "Extending a tool integration language\n", "abstract": " The Marvel environment supports rule-based modeling of software processes. Marvel invokes external tools to carry out steps in a software process. One of the major objectives of this research is to invoke existing external tools without needing to modify them. This is achieved by encapsulating tools in envelopes, designed to abstract the details of a tool from the Marvel kernel, thereby providing a\\black box\" interface. Initially we used the Unix shell language to write envelopes. Due to several limitations of the shell language, however, the black box abstraction could not be fully supported. We describe these limitations and discuss how we extended the shell language to obtain a new envelope language that fully supports the black box abstraction. c 1991 Mark A. Gisi and Gail E. Kaiser", "num_citations": "71\n", "authors": ["260"]}
{"title": "Quality assurance of software applications using the in vivo testing approach\n", "abstract": " Software products released into the field typically have some number of residual defects that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, incorrect assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system; these defects may also be due to application states that were not considered during lab testing, or corrupted states that could arise due to a security violation. One approach to this problem is to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present a testing methodology we call in vivo testing, in which tests are continuously executed in the deployment environment. We also describe a type of test we call in vivo tests that are specifically designed for use with such an\u00a0\u2026", "num_citations": "70\n", "authors": ["260"]}
{"title": "A bi-level language for software process modeling\n", "abstract": " The authors present a multi-user implementation of a bi-level process modeling language (PML). Most process modeling formalisms are well-suited to one of two levels of specification, but not both. Some concentrate on global control flow and synchronization. These languages make it easy to define the broad outline of a process, but harder to refine the process by expressing constraints and policies on individual tools and data. Other process formalisms are inherently local. It is easy to define constraints, but far from straightforward to express control flow. Combining global and local formalisms is proposed to produce bi-level formalisms suitable for expressing the enacting large scale processes. The new PML is called the activity structures language. The activity structures language integrates global constrained expressions with local rules. Its implementation on top of the Marvel rule-based environment is\u00a0\u2026", "num_citations": "70\n", "authors": ["260"]}
{"title": "Using JML runtime assertion checking to automate metamorphic testing in applications without test oracles\n", "abstract": " It is challenging to test applications and functions for which the correct output for arbitrary input cannot be known in advance, e.g. some computational science or machine learning applications. In the absence of a test Oracle, one approach to testing these applications is to use metamorphic testing: existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can be easily be computed based on the original output. That is, if input x produces output f(x), then we create input x' such that we can predict f(x') based on f(x); if the application or function does not produce the expected output, then a defect must exist, and either f(x) or f(x') (or both) is wrong. By using metamorphic testing, we are able to provide built-in \"pseudo-oracles\" for these so-called \"nontestable programs\" that have no test oracles.In this paper, we describe an\u00a0\u2026", "num_citations": "68\n", "authors": ["260"]}
{"title": "Modeling concurrency in rule-based development environments\n", "abstract": " The problem of cooperative work in the software development domain is explored, and a solution that combines object-oriented programming with rule-based modeling is proposed. The solution divides the problem into three components: how to detect potential conflicts between developers' concurrent activities, how to specify the consistency requirements of a project, and how to use the consistency specification to resolve potential conflicts. The focus is on the first component; the other two are merely sketched. The solution exploits recent advances in object-oriented databases, extended transaction models, and computer-supported cooperative work, all of which provide clues as to how to support cooperation while guaranteeing data consistencyed.", "num_citations": "67\n", "authors": ["260"]}
{"title": "Efficient dependency detection for safe Java test acceleration\n", "abstract": " Slow builds remain a plague for software developers. The frequency with which code can be built (compiled, tested and packaged) directly impacts the productivity of developers: longer build times mean a longer wait before determining if a change to the application being built was successful. We have discovered that in the case of some languages, such as Java, the majority of build time is spent running tests, where dependencies between individual tests are complicated to discover, making many existing test acceleration techniques unsound to deploy in practice. Without knowledge of which tests are dependent on others, we cannot safely parallelize the execution of the tests, nor can we perform incremental testing (ie, execute only a subset of an application's tests for each build). The previous techniques for detecting these dependencies did not scale to large test suites: given a test suite that normally ran in two\u00a0\u2026", "num_citations": "65\n", "authors": ["260"]}
{"title": "An approach to software testing of machine learning applications\n", "abstract": " Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test such ML software, because there is no reliable test oracle. We describe a software testing approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines and MartiRank.", "num_citations": "63\n", "authors": ["260"]}
{"title": "A case study in software adaptation\n", "abstract": " We attach a feedback-control-loop infrastructure to an existing target system, to continually monitor and dynamically adapt its activities and performance.(This approach could also be applied to\" new\" systems, as an alternative to\" building in\" adaptation facilities, but we do not address that here.) Our infrastructure consists of multiple layers, with the objectives of 1. probing, measuring and reporting of activity and state during the execution of the target system among its components and connectors; 2. gauging, analysis and interpretation of the reported events; and 3. whenever necessary, feedback onto the probes and gauges, to focus them (eg, drill deeper), or onto the running target system, to direct its automatic adjustment and reconfiguration. We report on our successful experience using this approach in the dynamic adaptation of a large-scale commercial application requiring both coarse and fine-grained\u00a0\u2026", "num_citations": "63\n", "authors": ["260"]}
{"title": "Modeling concurrency in parallel debugging\n", "abstract": " We propose a debugging language, Data Path Expressions (DPEs), for modeling the behavior of parallel programs. The debugging paradigm is for the programmer to describe the expected program behavior and for the debugger to compare the actual program behavior during execution to detect program errors. We classify DPEs into five subclasses according to syntactic criteria, and characterize their semantics in terms of a hierarchy of extended Petri Net models. The characterization demonstrates the power of DPEs for modeling parallelism. We present predecessor automata as a mechanism for implementing the third subclass of DPEs, which expresses bounded parallelism. Predecessor automata extend finite state automata to provide efficient event recognizers for parallel debugging. We briefly describe the application of DPEs to race conditions, deadlock and starvation.", "num_citations": "61\n", "authors": ["260"]}
{"title": "Secure selecticast for collaborative intrusion detection systems\n", "abstract": " The problem domain of Collaborative Intrusion Detection Systems (CIDS) introduces distinctive data routing challenges, which we show are solvable through a sufficiently flexible publish-subscribe system. In general, CIDS aim to share intrusion detection data among organizations, usually to earlier and more accurately predict impending attacks, e.g., from Internet worms that tend to attack many sites at once. In particular, participants in the CIDS collect lists of suspect IP addresses, and want to be notified if others are suspicious of the same addresses. The matching must be done efficiently and anonymously, as most organizations are reluctant to share potentially revealing information about their networks, routing alerts regarding external probes only to other CIDS participants experiencing probes from the same source(s). We term this type of simultaneous publish/subscribe selecticast. We present a potential\u00a0\u2026", "num_citations": "57\n", "authors": ["260"]}
{"title": "WWW access to legacy client/server applications\n", "abstract": " We describe a method for accessing Client/Server applications from standard World Wide Web browsers. An existing client for the system is modified to perform HTTP Proxy duties. Web browser users simply configure their browsers to use this HTTP Proxy, and can then access the system via specially encoded URLs that the HTTP Proxy intercepts and sends to the legacy server system. An example implementation using the Oz Process Centered Software Development Environment is presented.", "num_citations": "57\n", "authors": ["260"]}
{"title": "Semantics for structure editing environments\n", "abstract": " This thesis addresses the processing of semantics by structure editor-based programming environments. This processing is performed incrementally while the user writes and tests her programs. The semantics processing involves the manipulation of two kinds of properties, static and dynamic. Static properties can be determined by inspection of the program while dynamic properties reflect the interaction between the user and the programming environment. The implementor of a programming environment describes the semantics processing in terms of these properties. For example, symbol resolution, type checking and code generation involve static properties while interpretation, run-time support and language-oriented debugging involve dynamic properties.  Recent research in structure editing environments has focused on the generation of programming environments from descriptions. Several mechanisms\u00a0\u2026", "num_citations": "56\n", "authors": ["260"]}
{"title": "Code relatives: detecting similarly behaving software\n", "abstract": " Detecting \u201csimilar code\u201d is useful for many software engineering tasks. Current tools can help detect code with statically similar syntactic and\u2013or semantic features (code clones) and with dynamically similar functional input/output (simions). Unfortunately, some code fragments that behave similarly at the finer granularity of their execution traces may be ignored. In this paper, we propose the term \u201ccode relatives\u201d to refer to code with similar execution behavior. We define code relatives and then present DyCLINK, our approach to detecting code relatives within and across codebases. DyCLINK records instruction-level traces from sample executions, organizes the traces into instruction-level dynamic dependence graphs, and employs our specialized subgraph matching algorithm to efficiently compare the executions of candidate code relatives. In our experiments, DyCLINK analyzed 422+ million prospective subgraph\u00a0\u2026", "num_citations": "55\n", "authors": ["260"]}
{"title": "Enveloping sophisticated tools into computer-aided software engineering environments\n", "abstract": " We present a CASE-tool integration strategy based on enveloping pre-existing tools without source access or assuming an API or any other special capabilities on the part of the tool. This Black Box enveloping (or wrapping) idea has been around for a long time, but was previously restricted to relatively simple tools such as compilers. We describe the design and implementation of a new Black Box enveloping facility intended for sophisticated tools-often with graphical user interfaces-with particular concern for the emerging class of groupware applications.< >", "num_citations": "55\n", "authors": ["260"]}
{"title": "Chronicler: Lightweight recording to reproduce field failures\n", "abstract": " When programs fail in the field, developers are often left with limited information to diagnose the failure. Automated error reporting tools can assist in bug report generation but without precise steps from the end user it is often difficult for developers to recreate the failure. Advanced remote debugging tools aim to capture sufficient information from field executions to recreate failures in the lab but often have too much overhead to practically deploy. We present Chronicler, an approach to remote debugging that captures non-deterministic inputs to applications in a lightweight manner, assuring faithful reproduction of client executions. We evaluated Chronicler by creating a Java implementation, ChroniclerJ, and then by using a set of benchmarks mimicking real world applications and workloads, showing its runtime overhead to be under 10% in most cases (worst case 86%), while an existing tool showed overhead over\u00a0\u2026", "num_citations": "53\n", "authors": ["260"]}
{"title": "An architecture for WWW-based hypercode environments\n", "abstract": " ABSTRACT A hypercode software engineering environment represents all plausible multimedia artifacts concerned with software development and evolution that can be placed or generated on-line, from source code to formal documentation to digital library resources to informal email and chat transcripts. A hypercode environment supports both internal (hypertext) and external (link server) links among these artifacts, which can be added incrementally as useful connections are discovered; project-specific hypermedia search and browsing; automated construction of artifacts and hyperlinks according the software process; application of tools to the artifacts according to the process workflow; and collaborative work for geographically dispersed teams. We present a general architecture for what we call hypermedia subwebs, and groupspace services operating on shared subwebs, based on World Wide Web\u00a0\u2026", "num_citations": "53\n", "authors": ["260"]}
{"title": "Extracting context to improve accuracy for html content extraction\n", "abstract": " Previous work on content extraction utilized various heuristics such as link to text ratio, prominence of tables, and identification of advertising. Many of these heuristics were associated with\" settings\", whereby some heuristics could be turned on or off and others parameterized by minimum or maximum threshold values. A given collection of settings-such as removing table cells with high linked to non-linked text ratios and removing all apparent advertising--might work very well for a news website, but leave little or no content left for the reader of a shopping site or a web portal We present a new technique, based on incrementally clustering websites using search engine snippets, to associate a newly requested website with a particular\" genre\", and then employ settings previously determined to be appropriate for that genre, with dramatically improved content extraction results overall.", "num_citations": "52\n", "authors": ["260"]}
{"title": "Integrating groupware activities into workflow management systems\n", "abstract": " Computer supported cooperative work (CSCW) has been recognized as a crucial enabling technology for multi-user computer-based systems, particularly in cases where synchronous human-human interaction is required between geographically dispersed users. Workflow is an emerging technology that supports complex business processes in modern corporations by allowing to explicitly define the process, and by supporting its execution in a workflow management system (WFMS). Since workflow inherently involves humans carrying out parts of the process, it is only natural to explore how to synergize these two technologies. The authors analyze the relationships between groupware and workflow management, present their general approach to integrating synchronous groupware tools into a WFMS, and conclude with an example process that was implemented in the Oz WFMS and integrated such tools. Their\u00a0\u2026", "num_citations": "52\n", "authors": ["260"]}
{"title": "Smarter recompilation\n", "abstract": " Tichy's Smart Recompilation method can be made smarter by permitting benign type inconsistencies between separately compiled modules. This enhanced method helps the programmer to make far-reaching changes in small, manageable steps.", "num_citations": "51\n", "authors": ["260"]}
{"title": "An environment for system version control\n", "abstract": " CiNii \u8ad6\u6587 - An Environment for System Version Control CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c \u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 An Environment for System Version Control KAISER GE \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 KAISER GE \u53ce\u9332\u520a\u884c\u7269 Proceedings of the COMPCON Spring 83, March Proceedings of the COMPCON Spring 83, March, 1983 IEEE \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Java \u30af\u30e9\u30b9\u306e\u52d5\u7684\u30d0\u30fc\u30b8\u30e7\u30f3 \u7ba1\u7406\u306e\u69cb\u60f3 \u6749\u5c71 \u5b89\u6d0b \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a. SE,\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u5de5\u5b66\u7814\u7a76\u4f1a\u5831\u544a 115, 49-56, 1997-07-31 \u53c2\u8003\u6587\u732e19\u4ef6 \u88ab\u5f15\u7528\u6587\u732e4\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 80001673779 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 /| \u2026", "num_citations": "49\n", "authors": ["260"]}
{"title": "Extracting content from accessible web pages\n", "abstract": " Web pages often contain clutter (such as ads, unnecessary animations and extraneous links) around the body of an article, which distracts a user from actual content. This can be especially inconvenient for blind and visually impaired users. The W3C's Web Accessibility Initiative (WAI) has defined a set of guidelines to make web pages more compatible with tools built specifically for persons with disabilities. While this initiative has put forth an excellent set of principles, unfortunately many websites continue to be inaccessible as well as cluttered. In order to address the clutter problem, we have developed a framework that employs a host of heuristics in the form of tunable filters for the purpose of content extraction. Our hypothesis is that automatically filtering out selected elements from websites will leave the base content that users are interested in and, as a side-effect, render them more accessible. Although our\u00a0\u2026", "num_citations": "48\n", "authors": ["260"]}
{"title": "An active events model for systems monitoring\n", "abstract": " We present an interaction model enabling data-source probes and action-based gauges to communicate using an intelligent event model known as ActEvents. ActEvents build on conventional event concepts by associating structural and semantic information with raw data, thereby allowing recipients to be able to dynamically understand the content of new kinds of events. Two submodels of ActEvents are proposed: SmartEvents, which are XML-structured events containing references to their syntactic and semantic models, and Gaugents, which are heavier but more flexible intelligent mobile software agents.This model is presented in light of DARPA\u2019s DASADA program, where ActEvents are used in a larger-scale subsystem, called KX, which supports continual validation of distributed, componentbased systems. ActEvents are emitted by probes in this architecture, and propagated to gauges, where \u201cmeasurements\u201d of the raw data associated with probes are made, thereby continually determining updated target-system properties. ActEvents are also proposed as solutions for a number of other applications, including a distributed collaborative virtual environment (CVE) known as CHIME.", "num_citations": "47\n", "authors": ["260"]}
{"title": "Cooperative transactions for multi-user environments\n", "abstract": " This chapter surveys extended transaction models proposed to support long duration, interactive and/or cooperative activities in the context of multi-user software development and CAD/CAM environments. Many of these are variants of the checkout model, which addresses the long duration and interactive nature of the activities supported by environments but still isolates environment users, making it difficult for them to collaborate while their activities are in progress. However, a few cooperative transaction models have been proposed to facilitate collaboration, usually while maintaining some guarantees of consistency.", "num_citations": "47\n", "authors": ["260"]}
{"title": "Improving efficiency and reliability of building systems using machine learning and automated online evaluation\n", "abstract": " A high percentage of newly-constructed commercial office buildings experience energy consumption that exceeds specifications and system failures after being put into use. This problem is even worse for older buildings. We present a new approach, `predictive building energy optimization', which uses machine learning (ML) and automated online evaluation of historical and real-time building data to improve efficiency and reliability of building operations without requiring large amounts of additional capital investment. Our ML approach uses a predictive model to generate accurate energy demand forecasts and automated analyses that can guide optimization of building operations. In parallel, an automated online evaluation system monitors efficiency at multiple stages in the system workflow and provides building operators with continuous feedback. We implemented a prototype of this application in a large\u00a0\u2026", "num_citations": "46\n", "authors": ["260"]}
{"title": "Increasing student engagement in software engineering with gamification\n", "abstract": " Gamification, or the use of game elements in non-game contexts, has become an increasingly popular approach to increasing end-user engagement in many contexts, including employee productivity, sales, recycling, and education. Our preliminary work has shown that gamification can be used to boost student engagement and learning in basic software testing. We seek to expand our gamified software engineering approach to motivate other software engineering best practices. We propose to build a game layer on top of traditional continuous integration technologies to increase student engagement in development, documentation, bug reporting, and test coverage. This poster describes to our approach and presents some early results showing feasability.", "num_citations": "46\n", "authors": ["260"]}
{"title": "Workspaces and experimental databases: Automated support for software maintenance and evolution\n", "abstract": " We introduce and compare two models of cooperation among programmers during software maintenance. Enforced cooperation is the normal mode of operation when the sheer size of the software maintenance effort makes laissez-faire management infeasible. Voluntary cooperation is more common when a small group works together to enhance a small system or modify a small portion of a large system. We describe a tool, Infuse, that provides change management in the context of both models of cooperation. We demonstrate how Infuse automates change propagation and enforces negotiation of conflicts for the enforced model, but provides less restrictive aids for maintaining consistency for the voluntary model.", "num_citations": "46\n", "authors": ["260"]}
{"title": "Secret ninja testing with HALO software engineering\n", "abstract": " Software testing traditionally receives little attention in early computer science courses. However, we believe that if exposed to testing early, students will develop positive habits for future work. As we have found that students typically are not keen on testing, we propose an engaging and socially-oriented approach to teaching software testing in introductory and intermediate computer science courses. Our proposal leverages the power of gaming utilizing our previously described system HALO. Unlike many previous approaches, we aim to present software testing in disguise-so that students do not recognize (at first) that they are being exposed to software testing. We describe how HALO could be integrated into course assignments as well as the benefits that HALO creates", "num_citations": "45\n", "authors": ["260"]}
{"title": "Halo (highly addictive, socially optimized) software engineering\n", "abstract": " In recent years, computer games have become increasingly social and collaborative in nature. Massively multiplayer online games, in which a large number of players collaborate with each other to achieve common goals in the game, have become extremely pervasive. By working together towards a common goal, players become more engrossed in the game. In everyday work environments, this sort of engagement would be beneficial, and is often sought out. We propose an approach to software engineering called HALO that builds upon the properties found in popular games, by turning work into a game environment. Our proposed approach can be viewed as a model for a family of prospective games that would support the software development process. Utilizing operant conditioning and flow theory, we create an immersive software development environment conducive to increased productivity. We describe\u00a0\u2026", "num_citations": "45\n", "authors": ["260"]}
{"title": "Intelligent assistance without artificial intelligence\n", "abstract": " Software Management and Incremental Language Editing system SMILE is a distributed, multi-user software engineering environment that behaves as an intelligent assistant. SMILE presents a fileless environment, derives and transforms data to shelter users from entering redundant information, automatically invokes programming tools, and actively participates in the software development and maintenance process. Unlike other intelligent assistants, SMILE is not a rule-based environment its knowledge of software objects and the programming process is hardcoded into the environment. We describe SMILEs functionality and explain how we achieved this functionality without reliance on artificial intelligence technology.Descriptors:", "num_citations": "43\n", "authors": ["260"]}
{"title": "Distributed in vivo testing of software applications\n", "abstract": " The in vivo software testing methodology focuses on testing live applications by executing unit tests throughout the lifecycle, including after deployment. The motivation is that the \"known state \" approach of traditional unit testing is not always sufficient; deployed applications rarely operate under such conditions, and it may be more informative to perform the testing in live environments. One of the limitations of this approach is the high performance cost it incurs, as the unit tests are executed in parallel with the application. Here we present distributed in vivo testing, which focuses on easing the burden by sharing the load across multiple instances of the application of interest. That is, we elevate the scope of in vivo testing from a single instance to a community of instances, all participating in the testing process. Our approach is different from prior work in that we are actively testing during execution, as opposed to\u00a0\u2026", "num_citations": "42\n", "authors": ["260"]}
{"title": "CHIME: a metadata-based distributed software development environment\n", "abstract": " We introduce CHIME, the Columbia Hypermedia IMmersion Environment, a metadata-based information environment, and describe its potential applications for internet and intranet-based distributed software development. CHIME derives many of its concepts from Multi-User Domains (MUDs), placing users in a semi-automatically generated 3D virtual world representing the software system. Users interact with project artifacts by \u201cwalking around\u201d the virtual world, where they potentially encounter and collaborate with other users\u2019 avatars. CHIME aims to support large software development projects, in which team members are often geographically and temporally dispersed, through novel use of virtual environment technology. We describe the mechanisms through which CHIME worlds are populated with project artifacts, as well as our initial experiments with CHIME and our future goals for the system.", "num_citations": "41\n", "authors": ["260"]}
{"title": "Rule-based modelling of the software development process\n", "abstract": " MARVEL is a knowledge-based programming environment that assists its users during the implementation, testing and maintenance phases of software projects. It currently addresses the technical aspects of building software systems rather than the managerial aspects of supporting large software projects. MARVEL\u2019S knowledge is supplied as a collection of srraregies, which are combined to define the structure and behavior of the programmin g environment. Each strategy defines facilities appropriate to some specific programmin g language, programming methodology, scale of the target software project, phase of the software life-cycle, and/or personnel role in the software process. The strategies are written in advance by a superuser familiar both with MARVEL and the site\u2019s family of projects, and are later configured to describe the appropriate facilities currently required by the given user.Each strategy consists\u00a0\u2026", "num_citations": "40\n", "authors": ["260"]}
{"title": "Pebbles: Fine-grained data management abstractions for modern operating systems\n", "abstract": " Support for fine-grained data management has all but disappeared from modern operating systems such as Android and iOS. Instead, we must rely on each individual application to manage our data properly\u2013eg, to delete our emails, documents, and photos in full upon request; to not collect more data than required for its function; and to back up our data to reliable backends. Yet, research studies and media articles constantly remind us of the poor data management practices applied by our applications. We have developed Pebbles, a fine-grained data management system that enables management at a powerful new level of abstraction: application-level data objects, such as emails, documents, notes, notebooks, bank accounts, etc. The key contribution is Pebbles\u2019s ability to discover such high-level objects in arbitrary applications without requiring any input from or modifications to these applications. Intuitively, it seems impossible for an OS-level service to understand object structures in unmodified applications, however we observe that the high-level storage abstractions embedded in modern OSes\u2013relational databases and object-relational mappers\u2013bear significant structural information that makes object recognition possible and accurate.", "num_citations": "39\n", "authors": ["260"]}
{"title": "Debugging multithreaded programs with MPD\n", "abstract": " MPD, a multiprocessor debugger, is described. In MPD, users specify sequential or parallel event (breakpoint) expressions and associated actions; MPD automatically generates code to recognize these patterns of events and invokes the corresponding actions when the event expressions are satisfied during the program execution. The richness of the expression-specification language gives users a great deal of precision in formulating queries, obviating tedious searches through long traces of multithreaded software. Implementation issues are discussed.< >", "num_citations": "39\n", "authors": ["260"]}
{"title": "Enveloping sophisticated tools into process-centered environments\n", "abstract": " We present a tool integration strategy based on enveloping pre-existing tools without source code modifications or recompilation, and without assuming an extension language, application programming interface, or any other special capabilities on the part of the tool. This Black Box enveloping (or wrapping) idea has existed for a long time, but was previously restricted to relatively simple tools. We describe the design and implementation of, and experimentation with, a new Black Box enveloping facility intended for sophisticated tools\u2014with particular concern for the emerging class of groupware applications.", "num_citations": "38\n", "authors": ["260"]}
{"title": "Incremental attribute evaluation in distributed language-based environments\n", "abstract": " We present a model of distributed program editing and algorithms for the incremental checking of static semantic properties of modules that are at once semantically interdependent and physically distributed across a number of workstations connected by a high speed network. This makes possible the synthesis of modern program development hardware--workstations on high speed networks--and modern program development software--incremental, language-based program development systems--that until now have suffered from the problem of not being able to support incremental checking across distributed modules.", "num_citations": "38\n", "authors": ["260"]}
{"title": "Configuration fuzzing for software vulnerability detection\n", "abstract": " Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations of the software together with its particular run-time environment. One approach to detecting these vulnerabilities is fuzz testing, which feeds a range of randomly modified inputs to a software application while monitoring it for failures. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, in this paper we present a new testing methodology called configuration fuzzing. Configuration fuzzing is a technique whereby the configuration of the running application is randomly modified at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously\u00a0\u2026", "num_citations": "37\n", "authors": ["260"]}
{"title": "Incremental dynamic semantics for language-based programming environments\n", "abstract": " Attribute grammars are a formal notation for expressing the static semantics of programming languages\u2014those properties that can be derived from inspection of the program text. Attribute grammars have become popular as a mechanism for generating language-based programming environments that incrementally perform symbol resolution, type checking, code generation, and derivation of other static semantic properties as the program is modified. However, attribute grammars are not suitable for expressing dynamic semantics\u2014those properties that reflect the history of program execution and/or user interactions with the programming environment. This paper presents action equations, an extension of attribute grammars suitable for specifying the static and the dynamic semantics of programming languages. It describes how action equations can be used to generate language-based programming environments\u00a0\u2026", "num_citations": "37\n", "authors": ["260"]}
{"title": "Identifying functionally similar code in complex codebases\n", "abstract": " Identifying similar code in software systems can assist many software engineering tasks such as program understanding and software refactoring. While most approaches focus on identifying code that looks alike, some techniques aim at detecting code that functions alike. Detecting these functional clones - code that functions alike - in object oriented languages remains an open question because of the difficulty in exposing and comparing programs' functionality effectively. We propose a novel technique, In-Vivo Clone Detection, that detects functional clones in arbitrary programs by identifying and mining their inputs and outputs. The key insight is to use existing workloads to execute programs and then measure functional similarities between programs based on their inputs and outputs, which mitigates the problems in object oriented languages reported by prior work. We implement such technique in our system\u00a0\u2026", "num_citations": "36\n", "authors": ["260"]}
{"title": "GESDOR\u2013a generic execution model for sharing of computer-interpretable clinical practice guidelines\n", "abstract": " We developed the Guideline Execution by Semantic Decomposition of Representation (GESDOR) model to share guidelines encoded in different formats at the execution level. For this purpose, we extracted a set of generalized guideline execution tasks from the existing guideline representation models. We then created the mappings between specific guideline representation models and the set of the common guideline execution tasks. Finally, we developed a generic task-scheduling model to harmonize the existing approaches to guideline task scheduling. The evaluation has shown that the GESDOR model can be used for the effective execution of guidelines encoded in different formats, and thus realizes guideline sharing at the execution level.", "num_citations": "36\n", "authors": ["260"]}
{"title": "Process evolution for the marvel environment\n", "abstract": " We present a schema and process evolution tool, called the Evolver, for the MARVEL process-centered environment. The Evolver analyzes the differences between the new and installed process models of an existing environment, detecting each case where the notion of consistency defined by the process model has been strengthened or weakened. The Evolver then automatically updates the environments object-base to guarantee that the objects are consistent according to the new specifications. ne Evolver can be applied while the installed process is in progress, temporarily halting normal operation while it updates the object-base, after which development continues using the new process. We have had several months of experience using the Evolver to make repeated changes in the process that supports our own further development of MARVEL, and include in this paper one small but practical example of a recent change made to a real MARVEL process.Descriptors:", "num_citations": "36\n", "authors": ["260"]}
{"title": "Data path debugging: Data-oriented debugging for a concurrent programming language\n", "abstract": " One trend in new programming languages, whether sequential or concurrent, is to include facilities that permit problem solving to be directed from the viewpoint of the problem domain. Object-oriented and dataflow languages are two prominent examples. A related trend in symbolic debuggers is for the debugger\u2019s command language to be both conceptually and syntactically close to the target programming language. These two trends am combined in data-oriented debugging, a form of problem-directed debugging.We first describe our goal of data-oriented debugging, and then present Data Path Expression (DPE) debugging, our approach to data-oriented debugging for concunent programming languages. DPE debugging is an extension of Bruegge\u2019s generalized path expression debugging [6, 73. In DPE debugging,(1) the debugger is aware of data flow as well as control flow and/or message flow,(2) the\u00a0\u2026", "num_citations": "36\n", "authors": ["260"]}
{"title": "Infuse: a tool for automatically managing and coordinating source changes in large systems\n", "abstract": " In current change management tools, the actual changes occur outside the tool. In contrast, Infuse concentrates on the actual change process and provides facilities for both managing and coordinating source changes. Infuse provides facilities for automatically structuring the cooperation among programmers, propagating changes, and determining the consistency of changes, and provides a basis for negotiating the resolution of conflicting changes and for iterating over a set of changes.", "num_citations": "36\n", "authors": ["260"]}
{"title": "The CORD approach to extensible concurrency control\n", "abstract": " Database management systems (DBMSs) have been increasingly used for advanced application domains, such as software development environments, workflow management systems, computer-aided design and manufacturing, and managed healthcare. In these domains, the standard correctness model of serializability is often too restrictive. The authors introduce the notion of a concurrency control language (CCL) that allows a database application designer to specify concurrency control policies to tailor the behavior of a transaction manager. A well-crafted set of policies defines an extended transaction model. The necessary semantic information required by the CCL run-time engine is extracted from a task manager, a (logical) module by definition included in all advanced applications. This module stores task models that encode the semantic information about the transactions submitted to the DBMS. They\u00a0\u2026", "num_citations": "35\n", "authors": ["260"]}
{"title": "An architecture for integrating concurrency control into environment frameworks\n", "abstract": " Layered and componentized systems promise substantial benefits from dividing responsibilities, but it is still unresolved how to construct a system from pre-esisting, independently developed pieces. Good solutions to this problem, in general or for specific classes of components, should reduce duplicate implementation efforts and promote reuse of large scale subsystems. We tackle the domain of software development environments and present an architecture for retrofitting external concurrency control components onto existing environment frameworks. We describe a sample ECC component, explain how we added concurrency control to a commercial product that had none, and briefly sketch how we replaced the concurrency control mechanism of a research system.", "num_citations": "35\n", "authors": ["260"]}
{"title": "A mobile agent approach to lightweight process workflow\n", "abstract": " The Programming Systems Lab at Columbia University has investigated software process modeling and enactment since its inception in the mid-1980s, initially in the Marvel project [1, 2]. In the early to mid-90s, we extended to cross-organizational processes operating over the Internet, in Oz [3, 4] and OzWeb [5]. That is, Oz enabled the software development team and other stakeholders to be geographically, temporally and/or organizationally dispersed. OzWeb added integration of Web and other external information resources whereas Oz and Marvel had assumed all project materials to be resident in their native objectbases. OzWeb's plugin services and tools were accessible via conventional Web browsers, HTTP proxies and Java GUIs, improving dramatically on Marvel's and Oz's X11 Windows XView/Motif user interface clients. The successive prototype frameworks we developed and demonstrated were used on a daily basis in-house to maintain, deploy and monitor their own components, APIs and user interfaces.Novel (at the relevant time) framework components included rule-based process modeling and a corresponding enactment engine supporting multi-process interoperability for joint and subcontracted tasks across autonomous organizations (Amber); a transaction monitor customizable to application-specific long duration and group extended transaction models over Web and legacy resources (Pern, succeeded by JPernLite); an object manager supporting multi-inheritance (Darkover); a decentralized intranet-remote tool-sharing service that turned legacy only-executables-available single-user tools into groupware (Multi-Tool\u00a0\u2026", "num_citations": "34\n", "authors": ["260"]}
{"title": "Infuse: Fusing integration test management with change management\n", "abstract": " Infuse is an experimental software development environment focusing on change coordination during the maintenance/evolution phase of large-scale software projects. Its core philosophy is to integrate strongly connected modules first and more weakly connected sets of modules later, moving up a hierarchy from singletons to clusters of interdependent modules and, finally, merging the change set into the baseline. The use of Infuse is extended to dynamic consistency, i.e. testing. Unit testing is done for the individual modules at the leaves of the hierarchy, integration testing for the intermediate clusters and acceptance testing at the root. Infuse supports this by partially automating the construction of test harnesses and regression test suites at each level of the hierarchy from components available from lower levels. Infuse is implemented in C and is used to support its own evolution, but the implementation does not\u00a0\u2026", "num_citations": "33\n", "authors": ["260"]}
{"title": "Divergence control for distributed database systems\n", "abstract": " This paper presents distributed divergence control algorithms for epsilon serializability for both homogeneous and heterogeneous distributed databases. Epsilon serializability allows for more concurrency by permitting non-serializable interleavings of database operations among epsilon transactions. We first present a strict 2-phase locking divergence control algorithm and an optimistic divergence control algorithm for a homogeneous distributed database system, where the local orderings of all the sub-transactions of a distributed epsilon transaction are the same. In such an environment, the total inconsistency of a distributed epsilon transaction is simply the sum of those of all its sub-transactions. We then describe a divergence control algorithm for a heterogeneous distributed database system, where the local orderings of all the sub-transactions of a distributed epsilon transaction may not be the same and\u00a0\u2026", "num_citations": "31\n", "authors": ["260"]}
{"title": "Distributed divergence control for epsilon serializability\n", "abstract": " Epsilon serializability (ESR) allows for more concurrency by permitting nonserializable interleavings of database operations among epsilon transactions (ETs). The authors present the design of distributed divergence control (DDC) algorithms for ESR in homogeneous and heterogeneous distributed databases. They first present a strict two-phase locking DDC algorithm (S2PLDDC) and an optimistic DDC algorithm (ODDC) for homogeneous distributed databases, where the local orderings of all the sub-ETs of a distributed ET are the same, and the total inconsistency of a distributed ET is simply the sum of that of all its sub-ETs. A superdatabase DDC algorithm is described for heterogeneous distributed databases, where the local orderings of all the sub-ETs of a distributed ET may not be the same, and the total inconsistency of a distributed ET may be greater than the sum of that of all its sub-ETs. As a result, in\u00a0\u2026", "num_citations": "31\n", "authors": ["260"]}
{"title": "Dynamic reconfiguration in an object-based programming language with distributed shared data\n", "abstract": " On-line distributed applications generally allow reconfiguration while the application is running, but changes are usually limited to adding new client and server processes and changing the bindings among such processes. In some application domains, such as on-line financial services, it is necessary to support finer grained reconfiguration at the level of entities within processes, but for performance reasons it is desirable to avoid conventional approaches such as interpretation and dynamic storage allocation. We present a scheme for special cases of fine grained dynamic reconfiguration sufficient for our application domain and show how it can be used for practical changes. We introduce new language concepts to implement this scheme in the context of an object-based programming language that supports shared data in a distributed environment.", "num_citations": "31\n", "authors": ["260"]}
{"title": "A retrospective on DOSE: an interpretive approach to structure editor generation\n", "abstract": " DOSE is unique among structure editor generatiors in its interpretive approach. This approach leads to very fast turn\u2010around time for changes and provides multi\u2010language facilities for no aditional effort or cost. This article compares the interpretive approach to the compilation approach of other structure editor generators. It describes some of the design and implementation decisions made and remade durign this project and the lessons learned. It emphasizes the advantages and disadvantages of DOSE with respect to other structure editing systems.", "num_citations": "31\n", "authors": ["260"]}
{"title": "FARE: A framework for benchmarking reliability of cyber-physical systems\n", "abstract": " A cyber-physical system (CPS) is a system featuring a tight combination of, and coordination between, the system's computational and physical elements. System reliability is a critical requirement of cyber-physical systems. An unreliable CPS often leads to system malfunctions, service disruptions, financial losses and even human life. Improving CPS reliability requires an objective measurement, estimation and comparison of the CPS system reliability. This paper describes FARE (Failure Analysis and Reliability Estimation), a framework for benchmarking reliability of cyber-physical systems. Some prior researches have proposed reliability benchmark for some specific CPS such as wind power plant and wireless sensor networks. There were also some prior researches on the components of CPS such as software and some specific hardware. But according to the best of our knowledge, there isn't any reliability\u00a0\u2026", "num_citations": "29\n", "authors": ["260"]}
{"title": "An interoperability model for process-centered software engineering environments and its implementation in Oz\n", "abstract": " A process-centered software engineering environment (PSEE) enables to model, evolve, and enact the process of software development and maintenance. This paper addresses the problem of process-interoperability among decentralized and autonomous PSEEs by presenting the generic International Alliance model, which consists of two elements, namely Treaty and Summit. The Treaty abstraction allows pairwise peerpeer de nition of multi-site shared sub-processes that are integrated inside each of the participating sites, while retaining the de nition-and evolution-autonomy of nonshared local sub-processes. Summits are the execution abstraction for Treaty-de ned sub-processes. They enact Treaty sub-processes in multiple sites by successively alternating between shared and private execution modes: the former is used for the synchronous execution of the shared activities, and the latter is used for the autonomous execution of any private subtasks emanating from the shared activities. We describe the realization of the models in the Oz multi-site PSEE and evaluate the models and system based on experience gained from using Oz for production purposes. We also consider the application of the model to Petri net-based and grammar-based PSEEs.", "num_citations": "29\n", "authors": ["260"]}
{"title": "Incremental parsing without a parser\n", "abstract": " This article describes an algorithm for incremental parsing of expressions in the context of syntax-directed editors for programming languages. Since a syntax-directed editor represents programs as trees and statements and expressions as nodes in trees, making minor modifications in an expression can be difficult. Consider, for example, changing a \u201c + \u201d operator to a \u201c\u2217\u201d operator or adding a short subexpression at a syntactically but not structurally correct position, such as inserting \u201c) \u2217 (d\u201c at the # mark in\u201d (a + b # + c)\u201d. To make these changes in a typical syntax-directed editor, the user must understand the tree structure and type a number of tree-oriented construction and manipulation commands. This article describes an algorithm that allows the user to think in terms of the syntax of the expression as it is displayed on the screen (in infix notation) rather than in terms of its internal representation (which is effectively\u00a0\u2026", "num_citations": "29\n", "authors": ["260"]}
{"title": "Using agents to enable collaborative work\n", "abstract": " With the commercialization of the Web, you can find the information you need, but you may also find an applet or CGI bin ready to charge you for it. In what was originally a cooperative environment where discovering and copying information were free, interactions (and the agents involved) are now acquiring an economic basis. We consider how the introduction of agents and, more specifically, agents whose interactions have an economic basis can actually lead to a new dimension for developing systems for computer supported cooperative work. Until recently, the gap between agent technology and the technology available for CSCW appeared to preclude much interaction between the two communities. However, agent based mechanisms have become sophisticated enough to be used by collaborative systems to perform task allocation, scheduling, triggering, and other low-level functions. The methods for\u00a0\u2026", "num_citations": "27\n", "authors": ["260"]}
{"title": "MELDing multiple granularities of parallelism\n", "abstract": " We are developing an experimental programming language, MELD, that suppons a range of concurrent styles by supporting multiple programming paradigms at multiple levels of granularity. MELD integrates three granularities of parallelism: macro dataflow among statements within a method or among methods for flne grain concurrency, synchronous or asynchronous message passing among local or remote objects for medium grain concurrency, and transactions for large grain concurrency among users.", "num_citations": "27\n", "authors": ["260"]}
{"title": "Dynamic inference of likely metamorphic properties to support differential testing\n", "abstract": " Metamorphic testing is an advanced technique to test programs without a true test oracle such as machine learning applications. Because these programs have no general oracle to identify their correctness, traditional testing techniques such as unit testing may not be helpful for developers to detect potential bugs. This paper presents a novel system, KABU, which can dynamically infer properties of methods' states in programs that describe the characteristics of a method before and after transforming its input. These Metamorphic Properties (MPs) are pivotal to detecting potential bugs in programs without test oracles, but most previous work relies solely on human effort to identify them and only considers MPs between input parameters and output result (return value) of a program or method. This paper also proposes a testing concept, Metamorphic Differential Testing (MDT). By detecting different sets of MPs\u00a0\u2026", "num_citations": "26\n", "authors": ["260"]}
{"title": "A distance learning approach to teaching eXtreme programming\n", "abstract": " As university-level distance learning programs become more and more popular, and software engineering courses incorporate eXtreme Programming (XP) into their curricula, certain challenges arise when teaching XP to students who are not physically co-located. In this paper, we present the results of a three-year study of such an online software engineering course targeted to graduate students, and describe some of the specific challenges faced, such as students' aversion to aspects of XP and difficulties in scheduling. We discuss our findings in terms of the course's educational objectives, and present suggestions to other educators who may face similar situations.", "num_citations": "26\n", "authors": ["260"]}
{"title": "Improving the dependability of machine learning applications\n", "abstract": " As machine learning (ML) applications become prevalent in various aspects of everyday life, their dependability takes on increasing importance. It is challenging to test such applications, however, because they are intended to learn properties of data sets where the correct answers are not already known. Our work is not concerned with testing how well an ML algorithm learns, but rather seeks to ensure that an application using the algorithm implements the specification correctly and fulfills the users\u2019 expectations. These are critical to ensuring the application\u2019s dependability. This paper presents three approaches to testing these types of applications. In the first, we create a set of limited test cases for which it is, in fact, possible to predict what the correct output should be. In the second approach, we use random testing to generate large data sets according to parameterization based on the application\u2019s equivalence classes. Our third approach is based on metamorphic testing, in which properties of the application are exploited to define transformation functions on the input, such that the new output can easily be predicted based on the original output. Here we discuss these approaches, and our findings from testing the dependability of three real-world ML applications.", "num_citations": "26\n", "authors": ["260"]}
{"title": "Adding self-healing capabilities to the common language runtime\n", "abstract": " Self-healing systems require that repair mechanisms are available to resolve problems that arise while the system executes. Managed execution environments such as the Common Language Runtime (CLR) and Java Virtual Machine (JVM) provide a number of application services (application isolation, security sandboxing, garbage collection and structured exception handling) which are geared primarily at making managed applications more robust. However, none of these services directly enables applications to perform repairs or consistency checks of their components. From a design and implementation standpoint, the preferred way to enable repair in a self-healing system is to use an externalized repair/adaptation architecture rather than hardwiring adaptation logic inside the system where it is harder to analyze, reuse and extend. We present a framework that allows a repair engine to dynamically attach and detach to/from a managed application while it executes essentially adding repair mechanisms as another application service provided in the execution environment.", "num_citations": "26\n", "authors": ["260"]}
{"title": "Backstop: a tool for debugging runtime errors\n", "abstract": " The errors that Java programmers are likely to encounter can roughly be categorized into three groups: compile-time (semantic and syntactic), logical, and runtime (exceptions). While much work has focused on the first two, there are very few tools that exist for interpreting the sometimes cryptic messages that result from runtime errors. Novice programmers in particular have difficulty dealing with uncaught exceptions in their code and the resulting stack traces, which are by no means easy to understand. We present Backstop, a tool for debugging runtime errors in Java applications. This tool provides more user-friendly error messages when an uncaught exception occurs, and also provides debugging support by allowing users to watch the execution of the program and the changes to the values of variables. We also present the results of two preliminary studies conducted on introductory-level programmers using the\u00a0\u2026", "num_citations": "25\n", "authors": ["260"]}
{"title": "P2P video synchronization in a collaborative virtual environment\n", "abstract": " We previously developed a collaborative virtual environment (CVE) for small-group virtual classrooms, intended for distance learning by geographically dispersed students. The CVE employs a P2P approach to the frequent real-time updates to the 3D virtual worlds required by avatar movements (fellow students in the same room). This paper focuses on our extensions to support group viewing of lecture videos, called VECTORS, for Video Enhanced Collaboration for Team Oriented Remote Synchronization. VECTORS supports synchronized viewing of lecture videos, so the students all see \u201cthe same thing at the same time\u201d, and can pause, rewind, etc. in synchrony while discussing the lecture via \u201cchat\u201d. We are particularly concerned with the needs of the technologically disenfranchised, e.g., whose only Internet access if via dialup networking. Thus VECTORS employs semantically compressed videos with\u00a0\u2026", "num_citations": "24\n", "authors": ["260"]}
{"title": "JPernLite: extensible transaction services for the WWW\n", "abstract": " Concurrency control is one of the key problems in design and implementation of collaborative systems such as hypertext/hypermedia systems, CAD/CAM systems, and software development environments. Most existing systems store data in specialized databases with built-in concurrency control policies, usually implemented via locking. It is desirable to construct such collaborative systems on top of the World Wide Web, but most Web servers do not support even conventional transactions, let alone distributed (multi-Website) transactions or flexible concurrency control mechanisms oriented toward team work-such as event notification, shared locks, and fine granularity locks. We present a transaction server that operates independently of Web servers or the collaborative systems, to fill the concurrency control gap. By default, the transaction server enforces the conventional atomic transaction model, where sets of\u00a0\u2026", "num_citations": "24\n", "authors": ["260"]}
{"title": "Coordinating distributed components over the Internet\n", "abstract": " Internet-based applications are likely to increase dramatically in the coming years, particularly with the emergence of electronic commerce and the growing interest in collaborative work (among humans). Incorporating autonomous and widely distributed computational components into a network-centric application is a complex and multidimensional task, encompassing integration, communication, negotiation, deployment and coordination. The key challenge is to balance execution autonomy with the potential payoff of leveraging existing components to perform collaborative computations.", "num_citations": "24\n", "authors": ["260"]}
{"title": "Disconnected operation in a multi-user software development environment\n", "abstract": " Software Development Environments have traditionally relied upon a central project database and file repository, accessible to a programmer's workstation via a local area network connection. The introduction of powerful mobile computers has demonstrated the need for a new model, which allows for machines with transient network connectivity to assist programmers in product development. The authors propose a process-based checkout model by which process and product files that may be needed during a planned period of disconnectivity are prefetched with minimal user effort. Rather than selecting each file by hand, which is tedious and error-prone, the user only informs the environment of the portion of the software development process intended to be executed while disconnected. The environment is then responsible for prefetching the necessary files. The authors hope that this approach will enable\u00a0\u2026", "num_citations": "24\n", "authors": ["260"]}
{"title": "A runtime adaptation framework for native C and bytecode applications\n", "abstract": " The need for self-healing software to respond with a reactive, proactive or preventative action as a result of changes in its environment has added the non-functional requirement of adaptation to the list of facilities expected in self-managing systems. The adaptations we are concerned with assist with problem detection, diagnosis and remediation. Many existing computing systems do not include such adaptation mechanisms, as a result these systems either need to be re-designed to include them or there needs to be a mechanism for retro-fitting these mechanisms. The purpose of the adaptation mechanisms is to ease the job of the system administrator with respect to managing software systems. This paper introduces Kheiron, a framework for facilitating adaptations in running programs in a variety of execution environments without requiring the re-design of the application. Kheiron manipulates compiled C\u00a0\u2026", "num_citations": "23\n", "authors": ["260"]}
{"title": "A flexible rule-chaining engine for process-based software engineering\n", "abstract": " We present the design of a new rule-based process engine that generalizes previous systems to support process enforcement, automation, guidance, monitoring, delegation, planning, simulation, instrumentation and potentially other applications. Our approach is fully knowledge-based, tailored by knowledge regarding the process assistance policies to be supported as well as the process definition.< >", "num_citations": "23\n", "authors": ["260"]}
{"title": "Multiple concurrency control policies in an object-oriented programming system\n", "abstract": " The transaction model provides a good solution to many programming problems, but has been recognized as too strict for many applications. Different parallel and distributed applications have different consistency requirements, so multiple concurrency control policies are needed. When data is shared among applications with different policies, then the policies must operate simultaneously and compatibly. The authors investigate the interoperability of transactions and another consistency model, atomic blocks, which provide exclusive access to an individual object. They explore the implications of 'uncontrolled' concurrency within the same COOPS. The work has been in the context of the MELD object-oriented programming language.< >", "num_citations": "23\n", "authors": ["260"]}
{"title": "Automated tutoring in interactive environments: A task centered approach\n", "abstract": " Tutoring in interactive computing environments is sometimes more properly understood as consulting. A tutor's implied curriculum may be adaptable to the user's knowledge and experience but still not meet the user's immediate needs-to get some task done. A consultant however, can dynamically adapt to address the task at hand. We present a user's goal-centered approach to tutoring in interactive environments, and describe how we automate certain tutoring strategies appropriate for consulting behavior. We have implemented our approach in GENIE, a question answering system for the Berkeley Unix Mail system. We focus on the pedagogical strategies employed by Genie to best meet the user's immediate needs.", "num_citations": "23\n", "authors": ["260"]}
{"title": "BugMiner: Software reliability analysis via data mining of bug reports\n", "abstract": " Software bugs reported by human users and automatic error reporting software are often stored in some bug tracking tools (eg, Bugzilla and Debbugs). These accumulated bug reports may contain valuable information that could be used to improve the quality of the bug reporting, reduce the quality assurance effort and cost, analyze software reliability, and predict future bug report trend. In this paper, we present BUGMINER, a tool that is able to derive useful information from historic bug report database using data mining, use these information to do completion check and redundancy check on a new or given bug report, and to estimate the bug report trend using statistical analysis. Our empirical studies of the tool using several real-world bug report repositories show that it is effective, easy to implement, and has relatively high accuracy despite low quality data.", "num_citations": "22\n", "authors": ["260"]}
{"title": "Software engineering in the internet age\n", "abstract": " The Internet is one of the most influential factors in today\u2019s software development activities. Part of its influence is as a common target platform. Countless software engineering projects are now addressing issues such as access to legacy systems via the Web, development of non-Web client-server applications based on TCP/IP (or higher level protocols that are mapped down to it), and so on. The technology used or devised for these purposes defines a buzzword set for the Internet age: Java, HTTP, HTML, XML, CORBA, ASP, JDBC, scripting languages\u2014to mention only a few.But the Internet is also an enabling technology that allows companies to meet the challenges of developing and evolving software under tightening market conditions, where getting to market first can be more important than actual development costs. The Internet supports globally distributed product development so that work can proceed around the clock in different locations. Furthermore, as distributed work becomes \u201cthe norm in most large, multinational companies,\u201d 1 the Internet supports the formation of virtual enterprises for a specific project. Members of a virtual enterprise nevertheless face the challenge of integrating heterogeneous processes. This challenge imposes requirements on software process support tools to", "num_citations": "22\n", "authors": ["260"]}
{"title": "Change management for very large software systems\n", "abstract": " The INFUSE change-management facility's methodology is briefly explained and its use of a hierarchy of experimental databases for controlling and coordinating changes is described. The authors then present the algorithm which INFUSE uses to automatically build and maintain this hierarchy. Some results of the application of INFUSE are given; one presented hierarchy is similar to one independently identified by a human expert.<>", "num_citations": "22\n", "authors": ["260"]}
{"title": "Parameterizing random test data according to equivalence classes\n", "abstract": " We are concerned with the problem of detecting bugs in machine learning applications. In the absence of sufficient real-world data, creating suitably large data sets for testing can be a difficult task. To address this problem, we have developed an approach to creating data sets called\" parameterized random data generation\". Our data generation framework allows us to isolate or combine different equivalence classes as desired, and then randomly generate large data sets using the properties of those equivalence classes as parameters. This allows us to take advantage of randomness but still have control over test case selection at the system testing level. We present our findings from using the approach to test two different machine learning ranking applications.", "num_citations": "21\n", "authors": ["260"]}
{"title": "Parallel and distributed incremental attribute evaluation algorithms for multiuser software development environments\n", "abstract": " The problem of change propagation in multiuser software development environments distributed across a local-area network is addressed. The program is modeled as an attributed parse tree segmented among multiple user processes and changes are modeled as subtree replacements requested asynchronously by individual users. Change propagation is then implemented using decentralized incremental evaluation of an attribute grammar that defines the static semantic properties of the programming language. Building up to our primary result, we first present algorithms that support parallel evaluation on a centralized tree in response to single edits using a singe editing cursor and multiple edits with multiple editing cursors. Then we present our algorithm for parallel evaluation on a decentralized tree. We also present a protocol to guarantee reliability of the evaluation algorithm as components of the\u00a0\u2026", "num_citations": "21\n", "authors": ["260"]}
{"title": "Towards self-adaptable monitoring framework for self-healing\n", "abstract": " Traditionally, monitoring solutions are based on collecting a reduced set of external metrics about the system such as performance, memory consumption or response time. However, these tools are limited to detecting and diagnosing failures or errors in complex systems like application servers and grid services. New applications have embedded monitoring logic merged with business logic to better monitor and trace applications. This approach creates dependence between the\" real\" code and the monitoring code though, reducing the monitoring approach flexibility. Furthermore, both approaches (external or embedded monitoring) are not adaptable; they cannot change monitoring process level or precision at runtime. In this paper, we present a fine-grain monitoring framework architecture based on aspect-oriented technology. Aspect Oriented Programming offers the possibility to inject monitoring code\u00a0\u2026", "num_citations": "20\n", "authors": ["260"]}
{"title": "Manipulating managed execution runtimes to support self-healing systems\n", "abstract": " Self-healing systems require that repair mechanisms are available to resolve problems that arise while the system executes. Managed execution environments such as the Common Language Runtime (CLR) and Java Virtual Machine (JVM) provide a number of application services (application isolation, security sandboxing, garbage collection and structured exception handling) which are geared primarily at making managed applications more robust. However, none of these services directly enables applications to perform repairs or consistency checks of their components. From a design and implementation standpoint, the preferred way to enable repair in a self-healing system is to use an externalized repair/adaptation architecture rather than hardwiring adaptation logic inside the system where it is harder to analyze, reuse and extend. We present a framework that allows a repair engine to dynamically attach and\u00a0\u2026", "num_citations": "19\n", "authors": ["260"]}
{"title": "The Web as enabling technology for software development and distribution\n", "abstract": " When confronted with a new technology, we instinctively consider it within the context of existing work and practices. Such is the case with the World Wide Web. But the Web is more than a new technology for leveraging existing work. It is, in fact, an enabling technology with the potential to change software development as dramatically as the transistor and microprocessor changed computer architecture. An enabling technology changes the fundamental assumptions ingrained in a discipline. The microprocessor, for example, changed the reliability, cost, circuit density, and performance assumptions underlying hardware design. As a result, new applications and design approaches for hardware systems became feasible. Just as the microprocessor changed the fundamental assumptions of hardware design, the Web changes some of the assumptions underlying software development. Thus it has the potential to\u00a0\u2026", "num_citations": "19\n", "authors": ["260"]}
{"title": "The SPLENDORS real time portfolio management system\n", "abstract": " We are concerned with knowledge-based applications that attempt to exploit changing market conditions to optimize the value of individual portfolios. The archetypical application is an on-line portfolio management system where a prices database is updated by a real time feed, and the user workstations repeatedly query the database as part of managing their portfolios. The massive amount of rapidly changing data, ie, the prices, places high performance demands on such systems. Consistency requirements pose additional difficulties. In particular, if the analysis of the market conditions is performed using an outdated price, and a decision is made on this basis but is then executed using a substantially different current price, the results can be disastrous.Profit represents an attempt to overcome these difficulties, by providing programming language support for developing applications with such soft real time requirements. Profit is a coordination language [1] built on top of the C computation language. The low-level analytic algorithms are implemented using conventional C statements and data structures, and Profit adds the communication, synchronization, multi-threading and timing facilities. Profit is intended for use by sophisticated programmers to construct distributed financial services applications.", "num_citations": "19\n", "authors": ["260"]}
{"title": "Implementation of a knowledge-based programming environment\n", "abstract": " MARVEL is a knowledge-based programming environment that assists its users during the implementation, testing, and maintenance phases of software projects. Solutions to the pragmatic problems of MARVELs implementation are discussed. MARVELs knowledge is supplied as strategies, where each strategy consists of classes that define the structure of objects, relations that define the semantic interconnections among objects, tools that define the processing that can be performed on objects to derive new objects, and rules that control the integration and automation of tools. The primary focus is the strategy language, its translator, and run-time support. An explanation is given of tool envelopes, which provide a mapping from the software objects defined in the current strategies to/from the file system representation expected by existing tools. The interactions between envelopes and the forward and backward\u00a0\u2026", "num_citations": "19\n", "authors": ["260"]}
{"title": "A uniform programming abstraction for effecting autonomic adaptations onto software systems\n", "abstract": " Most general-purpose work towards autonomic or self-managing systems has emphasized the front end of the feedback control loop, with some also concerned with controlling the back end enactment of runtime adaptations  u t usually employing an effector technology peculiar to one type of target system. While completely generic \"one size fits all\" effector technologies seem implausible, we propose a general-purpose programming model and interaction layer that abstracts away from the peculiarities of target-specific effectors, enabling a uniform approach to controlling and coordinating the low-level execution of reconfigurations, repairs, micro-reboots, etc", "num_citations": "18\n", "authors": ["260"]}
{"title": "OzCare: a workflow automation system for care plans.\n", "abstract": " An automated environment for implementing and monitoring care plans and practice guidelines is very important to the reduction of hospital costs and optimization of medical care. The goal of our research effort is to design a general system architecture that facilitates the implementation of (potentially) numerous care plans. Our approach is unique in that we apply the principles and technologies of Oz a multi-user collaborative workflow system that has been used as a software engineering environment framework, to hospital care planning. We utilize not only the workflow modeling and execution facilities of Oz, but also its open-system architecture to interface it with the World Wide Web, the Medical Logic Module server, and other components of the clinical information system. Our initial proof-of-concept system, OzCare, is constructed on top of the existing Oz system. Through several experiments in which we used\u00a0\u2026", "num_citations": "18\n", "authors": ["260"]}
{"title": "A configuration process for a distributed software development environment\n", "abstract": " The authors describe work-in-progress on a configuration facility for a multi-site software development environment. The environment supports collaboration among geographically-dispersed teams of software developers. Addition and deletion of local subenvironment sites to a global environment is performed interactively inside any one of the existing local subenvironments, with the same user interface normally employed for invoking software development tools. This registration process is defined and executed using the same notation and mechanisms, respectively, as for the software development process. Everything described by the authors has been implemented and is working, but since they are in the midst of experimentation, they do not expect that the \"final\" system will be exactly as described.< >", "num_citations": "18\n", "authors": ["260"]}
{"title": "Garp: graph abstractions for concurrent programming\n", "abstract": " Several research projects are investigating parallel processing languages where dynamic process topologies can be constructed. Failure to impose abstractions on interprocess connection patterns can result in arbitrary interconnection topologies that are difficult to understand. We propose the use of a graph-grammar based formalism to control the complexities arising from trying to program such dynamic networks.", "num_citations": "18\n", "authors": ["260"]}
{"title": "An autonomic reliability improvement system for cyber-physical systems\n", "abstract": " System reliability is a fundamental requirement of cyber-physical systems. Unreliable systems can lead to disruption of service, financial cost and even loss of human life. Typical cyber-physical systems are designed to process large amounts of data, employ software as a system component, run online continuously and retain an operator-in-the-loop because of human judgment and accountability requirements for safety-critical systems. This paper describes a data-centric runtime monitoring system named ARIS (Autonomic Reliability Improvement System) for improving the reliability of these types of cyber-physical systems. ARIS employs automated online evaluation, working in parallel with the cyber-physical system to continuously conduct automated evaluation at multiple stages in the system workflow and provide real-time feedback for reliability improvement. This approach enables effective evaluation of data\u00a0\u2026", "num_citations": "17\n", "authors": ["260"]}
{"title": "Evaluating machine learning for improving power grid reliability\n", "abstract": " Ensuring reliability as the electrical grid morphs into the \u201csmart grid\u201d will require innovations in how we assess the state of the grid, for the purpose of proactive maintenance, rather than reactive maintenance\u2013in the future, we will not only react to failures, but also try to anticipate and avoid them using predictive modeling (machine learning) techniques. To help in meeting this challenge, we present the Neutral Online Visualization-aided Autonomic evaluation framework (NOVA) for evaluating machine learning algorithms for preventive maintenance on the electrical grid. NOVA has three stages provided through a unified user interface: evaluation of input data quality, evaluation of machine learning results, and evaluation of the reliability improvement of the power grid. A prototype version of NOVA has been deployed for the power grid in New York City, and it is able to evaluate machine learning systems effectively and efficiently.", "num_citations": "17\n", "authors": ["260"]}
{"title": "Compass: A community-driven parallelization advisor for sequential software\n", "abstract": " The widespread adoption of multicores has renewed the emphasis on the use of parallelism to improve performance. The present and growing diversity in hardware architectures and software environments, however, continues to pose difficulties in the effective use of parallelism thus delaying a quick and smooth transition to the concurrency era. In this paper, we describe the research being conducted at Columbia University on a system called COMPASS that aims to simplify this transition by providing advice to programmers while they reengineer their code for parallelism. The advice proffered to the programmer is based on the wisdom collected from programmers who have already parallelized some similar code. The utility of COMPASS rests, not only on its ability to collect the wisdom unintrusively but also on its ability to automatically seek, find and synthesize this wisdom into advice that is tailored to the task at\u00a0\u2026", "num_citations": "17\n", "authors": ["260"]}
{"title": "Generation of run-time environments\n", "abstract": " Attribute grammars have been used for many years for automated compiler construction. Attribute grammars support the description of semantic analysis, code generation and some code optimization in a formal declarative style. Other tools support the automation of lexical analysis and parsing. However, there is one large part of compiler construction that is missing from our toolkit: run-time environments. This paper introduces an extension of attribute grammars that supports the generation of run-time environments. The extension also supports the generation of interpreters, symbolic debugging tools, and other execution-time facilities.", "num_citations": "17\n", "authors": ["260"]}
{"title": "Testing DNN image classifiers for confusion & bias errors\n", "abstract": " Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations.", "num_citations": "16\n", "authors": ["260"]}
{"title": "GenSpace: Exploring social networking metaphors for knowledge sharing and scientific collaborative work\n", "abstract": " Many collaborative applications, especially in scientific research, focus only on the sharing of tools or the sharing of data. We seek to introduce an approach to scientific collaboration that is based on the sharing of knowledge. We do this by automatically building organizational memory and enabling knowledge sharing by observing what users do with a particular tool or set of tools in the domain, through the addition of activity and usage monitoring facilities to standalone applications. Once this knowledge has been gathered, we apply social networking models to provide collaborative features to users, such as suggestions on tools to use, and automatically-generated sequences of actions based on past usage amongst the members of a social network or the entire community. In this work, we investigate social networking models as an approach to scientific knowledge sharing, and present an implementation called\u00a0\u2026", "num_citations": "16\n", "authors": ["260"]}
{"title": "A metalinguistic approach to process enactment extensibility\n", "abstract": " We present a model for developing rule based process servers with extensible syntax and semantics. New process enactment directives can be added to the syntax of the process modeling language, in which the process designer may specify specialized behavior for particular tasks or task segments. The process engine is peppered with callbacks to instance specific code in order to implement any new directives and to modify the default enactment behavior and the kind of assistance that the process centered environment provides to process participants. We realized our model in the Amber process server, and describe how we exploited Amber's extensibility to replace Oz's native process engine with Amber and to integrate the result with a mockup of TeamWare.", "num_citations": "16\n", "authors": ["260"]}
{"title": "An architecture for integrating OODBs with WWW\n", "abstract": " The main topic of this paper is how to structure information so that the view of the web, both within and across web pages, is dynamically customizable. We present an architecture that integrates Object-Oriented Databases with the World Wide Web to organize such dynamic structures. Different users, or the same user at different times, could have different views of the web. We discuss several architectural variants and implementation issues. Our chosen architecture provides high flexibility for a wide variety of applications, ranging from managed healthcare to software development environments, and has been realized in the dkweb system.", "num_citations": "16\n", "authors": ["260"]}
{"title": "A framework for immigrating existing software to new software development environments\n", "abstract": " A discussion is given on the problem of immigrating software artifacts from one software development environment (SDE) to another, for the purpose of upgrading to new SDEs as technology improves, while continuing development or maintenance of existing software systems. The authors taxonomise the larger problem of data migration, to establish the scope of immigration. They classify SDEs in terms of the ease of immigrating software artifacts from the data repository of the source SDE, without knowledge of its internal representation. A framework is presented for constructing automatic immigration tools as utilities provided by destination SDEs. They describe a specific immigration tool, called Marvelizer, implemented as part of the Marvel SDE and discuss experience using the tool.< >", "num_citations": "16\n", "authors": ["260"]}
{"title": "Display-oriented structure manipulation in a multi-purpose system\n", "abstract": " We present a multi-purpose system that supports maintenance and manipulation of structured information in a variety of application areas. The idea of a multi-purpose structure editor originates in Recently, syntax-directed editors have gotten more attention. syntax-directed editing. Information is maintained in an abstract Evolution of technology permits dedication of computing cycles to representation and a visible representation is generated on the fly. individuals in the form of intelligent terminals or personal Our structure editor is representation independent, i. e., it gains its computers. These cycles are immediately available to support dynamic generation of textual representation and interactive application-specific knowledge from a representation description.", "num_citations": "16\n", "authors": ["260"]}
{"title": "Dynamic taint tracking for java with phosphor\n", "abstract": " Dynamic taint tracking is an information flow analysis that can be applied to many areas of testing. Phosphor is the first portable, accurate and performant dynamic taint tracking system for Java. While previous systems for performing general-purpose taint tracking in the JVM required specialized research JVMs, Phosphor works with standard off-the-shelf JVMs (such as Oracle's HotSpot and OpenJDK's IcedTea). Phosphor also differs from previous portable JVM taint tracking systems that were not general purpose (eg tracked only tags on Strings and no other type), in that it tracks tags on all variables. We have also made several enhancements to Phosphor, to track taint tags through control flow (in addition to data flow), as well as to track an arbitrary number of relationships between taint tags (rather than be limited to only 32 tags). In this demonstration, we show how developers writing testing tools can benefit from\u00a0\u2026", "num_citations": "15\n", "authors": ["260"]}
{"title": "Data quality assurance and performance measurement of data mining for preventive maintenance of power grid\n", "abstract": " Ensuring reliability as the electrical grid morphs into the\" smart grid\" will require innovations in how we assess the state of the grid, for the purpose of proactive maintenance, rather than reactive maintenance; in the future, we will not only react to failures, but also try to anticipate and avoid them using predictive modeling (machine learning and data mining) techniques. To help in meeting this challenge, we present the Neutral Online Visualization-aided Autonomic evaluation framework (NOVA) for evaluating machine learning and data mining algorithms for preventive maintenance on the electrical grid. NOVA has three stages provided through a unified user interface: evaluation of input data quality, evaluation of machine learning and data mining results, and evaluation of the reliability improvement of the power grid. A prototype version of NOVA has been deployed for the power grid in New York City, and it is able to\u00a0\u2026", "num_citations": "15\n", "authors": ["260"]}
{"title": "Synthesizing programming environments from reusable features\n", "abstract": " Synthesizing programming environments from reusable features | Software reusability ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware reusability: vol. , applications and experienceSynthesizing programming environments from reusable features chapter Synthesizing programming environments from reusable features Share on Authors: Gail Elaine Kaiser profile image GE Kaiser View Profile , David Barnard Garlan profile image D. Garlan View Profile Authors Info & Affiliations Publication: Software reusability: vol. , applications and experienceJune 1989 Pages 35\u201355https://doi.org/10.1145/75722.75724 1citation 0 Metrics \u2026", "num_citations": "15\n", "authors": ["260"]}
{"title": "Using conceptual clustering for classifying reusable Ada code\n", "abstract": " One essential problem in reusability is classifying the reusable components. The library must be structured to facilitate the retrieving of code. Several approaches have been applied to this problem, but none addresses the evolution of software libraries. In this paper, we show how conceptual clustering can be used for dynamically classifying software components as the reusable library expands over time. Unimem is a conceptual clustering system that \u201clearns\u201d concepts by noticing similarities. We have adapted Unimem for developing and maintaining Ada libraries, where a large portion of the information necessary for our classifying method is directly extractable from the code.", "num_citations": "15\n", "authors": ["260"]}
{"title": "Confu: Configuration fuzzing testing framework for software vulnerability detection\n", "abstract": " Many software security vulnerabilities only reveal themselves under certain conditions, that is, particular configurations and inputs together with a certain runtime environment. One approach to detecting these vulnerabilities is fuzz testing. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, the authors present a new testing methodology called Configuration Fuzzing. Configuration Fuzzing is a technique whereby the configuration of the running application is mutated at certain execution points to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks \u201csecurity invariants\u2019\u2019that, if violated, indicate vulnerability. This paper discusses the approach and\u00a0\u2026", "num_citations": "14\n", "authors": ["260"]}
{"title": "Constructing subtle concurrency bugs using synchronization-centric second-order mutation operators\n", "abstract": " Mutation testing applies mutation operators to modify program source code or byte code in small ways, and then runs these modified programs (ie, mutants) against a test suite in order to evaluate the quality of the test suite. In this paper, we first describe a general fault model for concurrent programs and some limitations of previously developed sets of first-order concurrency mutation operators. We then present our new mutation testing approach, which employs synchronization-centric second-order mutation operators that are able to generate subtle concurrency bugs not represented by the first-order mutation. These operators are used in addition to the synchronization-centric first-order mutation operators to form a small set of effective concurrency mutation operators for mutant generation. Our empirical study shows that our set of operators is effective in mutant generation with limited cost and demonstrates that this new approach is easy to implement.", "num_citations": "14\n", "authors": ["260"]}
{"title": "Automatic detection of previously-unseen application states for deployment environment testing and analysis\n", "abstract": " For large, complex software systems, it is typically impossible in terms of time and cost to reliably test the application in all possible execution states and configurations before releasing it into production. One proposed way of addressing this problem has been to continue testing and analysis of the application in the field, after it has been deployed. A practical limitation of many such automated approaches is the potentially high performance overhead incurred by the necessary instrumentation. However, it may be possible to reduce this overhead by selecting test cases and performing analysis only in previously-unseen application states, thus reducing the number of redundant tests and analyses that are run. Solutions for fault detection, model checking, security testing, and fault localization in deployed software may all benefit from a technique that ignores application states that have already been tested or explored.", "num_citations": "14\n", "authors": ["260"]}
{"title": "Reliability in Distributed Programming Environments.\n", "abstract": " We describe a system for programming in the many that adapts language-based editors for in-dividual programmers to support the automatic checking of semantic interdependencies among mod-ules as they are developed in parallel by multiple programmers on a collection of workstations distributed across a local area network. We focus on the reliability of these distributed programming environments as some modules become inaccessi-ble and later return to availability. Our primary contributions are the decentralized control of the programming environment, firewalls, a mechanism that encapsulates individual modules to protect them from external failures, and a special network layer that enables the system to be highly available and reliable in the face of an unreliable network. The firewalls and network layer together support re-establishment of consistency among fully repli-cated data in the context of distributed programming environments. this problem using a distributed, language-based, program-ming environment (11). Using this system, programmers can develop modules in isolation and the system takes care of communicating changes among the relevant set of their colleagues, regardless of their physical location.", "num_citations": "14\n", "authors": ["260"]}
{"title": "Empirical evaluation of approaches to testing applications without test oracles\n", "abstract": " Software testing of applications in fields like scientific computing, simulation, machine learning, etc. is particularly challenging because many applications in these domains have no reliable \u201ctest oracle\u201d to indicate whether the program\u2019s output is correct when given arbitrary input. A common approach to testing such applications has been to use a \u201cpseudo-oracle\u201d, in which multiple independently-developed implementations of an algorithm process an input and the results are compared: if the results are not the same, then at least one of the implementations contains a defect. Other approaches include the use of program invariants, formal specification languages, trace and log file analysis, and metamorphic testing.In this paper, we present the results of two empirical studies in which we compare the effectiveness of some of these approaches, including metamorphic testing and runtime assertion checking. These results demonstrate that metamorphic testing is generally more effective at revealing defects in applications without test oracles in various application domains, including non-deterministic programs. We also analyze the results in terms of the software development process, and discuss suggestions for both practitioners and researchers who need to test software without the help of a test oracle.", "num_citations": "13\n", "authors": ["260"]}
{"title": "Granularity issues in a knowledge-based programming environment\n", "abstract": " Marvel is a knowledge-based programming environment that assists software development teams in performing and coordinating their activities. While designing Marvel, several granularity issues were discovered that have a strong impact on the degree of intelligence that can be exhibited, as well as on the friendliness and performance of the environment. The most significant granularity issues include the refinement of software entities in the software database and decomposition of the software tools that process the entities and report their results to the human users. This paper describes the many alternative granularities and explains the choices made for Marvel.", "num_citations": "13\n", "authors": ["260"]}
{"title": "Towards diversity in recommendations using social networks\n", "abstract": " While there has been a lot of research towards improving the accuracy of recommender systems, the resulting systems have tended to become increasingly narrow in suggestion variety. An emerging trend in recommendation systems is to actively seek out diversity in recommendations, where the aim is to provide unexpected, varied, and serendipitous recommendations to the user. Our main contribution in this paper is a new approach to diversity in recommendations called \u201cSocial Diversity,\u201d a technique that uses social network information to diversify recommendation results. Social Diversity utilizes social networks in recommender systems to leverage the diverse underlying preferences of different user communities to introduce diversity into recommendations. This form of diversification ensures that users in different social networks (who may not collaborate in real life, since they are in a different network) share information, helping to prevent siloization of knowledge and recommendations. We describe our approach and show its feasibility in providing diverse recommendations for the MovieLens dataset.", "num_citations": "12\n", "authors": ["260"]}
{"title": "Jpernlite: An extensible transaction server for the world wide web\n", "abstract": " Concurrency control is a well-known problem in design and implementation of multi-user hypermedia systems. Most existing systems store data and links in specialized databases (link servers or hyperbases) with a built-in concurrency control policy, typically the conventional atomic/serializable transaction model, usually implemented via locking. But this conventional model may not be appropriate for collaborative hypermedia systems, where the multiple users work together in groups on shared tasks.Further, it is desirable to construct collaborative hypermedia systems on top of the World Wide Web, but most web servers do not support even conventional transactions, let alone distributed (multi-website) transactions or flexible concurrency control mechanisms oriented towards teamwork--such as event notification, shared locks and fine granularity locks.", "num_citations": "12\n", "authors": ["260"]}
{"title": "Multi-Agent Rule-Based Software Development Environments\n", "abstract": " Researchers have explored the possibility of applying production systems to the domain of software development. One resulting approach is rule-based development environments (RBDEs), which provide expert assistance to developers working on large-scale projects. RBDEs model the development process in terms of rules, and then\" enact\" this model by automatically firing rules at the appropriate time to carry out chores that the developer would have otherwise had to do manually. In order to realistically model the domain of software development, RBDEs must support cooper-ation among multiplc developers, each of whom selects commands, causing the firing of multiple rules (either directly or via chaining) that concurrently access shared information. One of the problems resulting from executing rule chains concurrently is how to detect and resolve conflicts that occur between the chains. This paper presents the MARVEL rule-based development environment, explores the concurrency control problem in a multi-agent model of MARVEL, and suggests an approach to solving it.", "num_citations": "12\n", "authors": ["260"]}
{"title": "An object model for shared data\n", "abstract": " The classical object model supports private data within objects and clean interfaces among objects. and by defmition does not permit sharing of data among arbitrary objects. This is a problem for certain real-world applications. where the same data logically belongs 10 multiple objects and may be distributed over multiple nodes on the networX. Rather than give up the advantages of encapsulated objects in modeling real-world entities. we propose a new object model that supports shared data in a distributed environment The key is separating distribution of computation units from information hiding concerns. We introduce our new object model. describe a motivating example from the financial services domain, and then present a new language. PROm. based on the model", "num_citations": "12\n", "authors": ["260"]}
{"title": "Experience with marvel\n", "abstract": " This study was designed to elucidate the problem-solving skills used by frequent and infrequent Chinese video game players to negotiate impasses encountered while playing video game. All participants were instructed to think aloud while playing a video game for 20 consecutive minutes. Comments were made with thinking aloud. Findings showed that frequent players made significantly greater reference to insight and game strategies than infrequent players. After reaching an impasse, all players also were most likely to comment on their game progress and potential game strategies to use. Over the course of game play, there are some cross cultural differences were founded.", "num_citations": "12\n", "authors": ["260"]}
{"title": "Version and Configuration Control in Distributed Language-Based Environments.\n", "abstract": " We discuss a set of algorithms for supporting change propagation in distributed programming environments. Our previous papers presented algorithms suitable only for the over-simplified scenario of a single program made up of a collection of modules, each with only a current version where every change to the interface of any module was propagated to all the other modules. This paper describes new algorithms that handle realistic software development and maintenance by large teams of programmers, using real programming languages that permit hierarchical composition of modules, where each system evolves over time through multiple configurations specifying a particular version of each module. Furthermore, the new algorithms are dramatically more efficient in that they propagate exactly those changes that affect each module version.", "num_citations": "12\n", "authors": ["260"]}
{"title": "A competitive-collaborative approach for introducing software engineering in a CS2 class\n", "abstract": " Introductory Computer Science (CS) classes are typically competitive in nature. The cutthroat nature of these classes comes from students attempting to get as high a grade as possible, which may or may not correlate with actual learning. Further, there is very little collaboration allowed in most introductory CS classes. Most assignments are completed individually since many educators feel that students learn the most, especially in introductory classes, by working alone. In addition to completing \u201cnormal\u201d individual assignments, which have many benefits, we wanted to expose students to collaboration early (via, for example, team projects). In this paper, we describe how we leveraged competition and collaboration in a CS2 class to help students learn aspects of computer science better - in this case, good software design and software testing - and summarize student feedback.", "num_citations": "11\n", "authors": ["260"]}
{"title": "A large-scale, longitudinal study of user profiles in world of warcraft\n", "abstract": " We present a survey of usage of the popular Massively Multiplayer Online Role Playing Game, World of Warcraft. Players within this game often self-organize into communities with similar interests and/or styles of play. By mining publicly available data, we collected a dataset consisting of the complete player history for approximately six million characters, with partial data for another six million characters. The paper provides a thorough description of the distributed approach used to collect this massive community data set, and then focuses on an analysis of player achievement data in particular, exposing trends in play from this highly successful game. From this data, we present several findings regarding player profiles. We correlate achievements with motivations based upon a previously-defined motivation model, and then classify players based on the categories of achievements that they pursued. Experiments\u00a0\u2026", "num_citations": "11\n", "authors": ["260"]}
{"title": "Metamorphic runtime checking of non-testable programs\n", "abstract": " Previously we investigated the effectiveness of testing based on metamorphic properties of the entire application. Here, we improve upon that work by presenting a new technique called Metamorphic Runtime Checking, a testing approach that automatically conducts metamorphic testing of individual functions during the program\u2019s execution. We also describe an implementation framework called Columbus, and discuss the results of empirical studies that demonstrate that checking the metamorphic properties of individual functions increases the effectiveness of the approach in detecting defects, with minimal performance impact.", "num_citations": "11\n", "authors": ["260"]}
{"title": "Using runtime testing to detect defects in applications without test oracles\n", "abstract": " We address the testing of complex, highly-configurable systems-particularly those without test oracles-by testing in the field using built-in oracles from functions' metamorphic properties. This work is advised by Prof. Gail Kaiser.", "num_citations": "11\n", "authors": ["260"]}
{"title": "Verifying genre-based clustering approach to content extraction\n", "abstract": " The content of a webpage is usually contained within a small body of text and images, or perhaps several articles on the same page; however, the content may be lost in the clutter, particularly hurting users browsing on small cell phone and PDA screens and visually impaired users relying on speed rendering of web pages. Using the genre of a web page, we have created a solution, Crunch that automatically identifies clutter and removes it, thus leaving a clean content-full page. In order to evaluate the improvement in the applications for this technology, we identified a number of experiments. In this paper, we have those experiments, the associated results and their evaluation.", "num_citations": "11\n", "authors": ["260"]}
{"title": "Guru: Information retrieval for reuse\n", "abstract": " Although software reuse presents clear advantages for programmer productivity and code reliability, it is not practiced enough. One of the reasons for the only moderate success of reuse is the lack of software libraries that facilitate the actual locating and understanding of reusable components. This paper describes a technology for automatically assembling large software libraries that promote software reuse by helping the user locate the components closest to her/his needs. Software libraries are automatically assembled from a set of unorganized components by using information retrieval techniques. The construction of the library is done in two steps. First, attributes are automatically extracted from natural language documentation by using a new free-text indexing scheme based on the notions of lexical affinities and quantity of information. Then, a hierarchy for browsing is automatically generated using a clustering technique that draws only on the information provided b...", "num_citations": "11\n", "authors": ["260"]}
{"title": "On hard real-time management information\n", "abstract": " Maintaining the quality of services for time-critical broadband applications on a high-speed network is an important subject for network management. Since the managed services are time-sensitive, the management applications managing these services must be real-time. This further implies that the real-time management applications must first have real-time management information to correctly take management actions. In this paper, we investigate how to build a monitoring system to support hard real-time management information in the domain of network management. The term \"hard real-time\" means that there are serious consequences if the management information is older than a specified threshold. The key issue to design such a hard real-time system is to bound the amount of resources that would be needed at the time of monitoring. Three types of resources are under our consideration: communication\u00a0\u2026", "num_citations": "11\n", "authors": ["260"]}
{"title": "MeldC: A reflective object-oriented coordination language\n", "abstract": " A coordination language, MELDC, for open systems programming is presented. MELDC is a C-based, concurrent, distributed object-oriented language built on a reflective architecture. Unlike other language research, the focus of MELDC is not only to study what specific language features should be designed for solving certain open system problems but also to provide programmers a high-level and efficient way to construct new features without modifying the language internals. The key to the reflective feature is the metaclass that supports shadow objects to implement secondary behaviors of objects. Thus, the behavior of an object can be extended by dynamically composing multiple secondary behaviors with the object\u2019s primary behavior defined in the class. In this paper, both the MELDC programming model and the reflective architecture are described. Then, we introduce the mechanism of dynamic composition as well as its application in building distributed and persistent systems. In particular, a soft real-time network management system, MELDNET, is built on top of MELDC to monitor the EtherNet performance. Finally, the current status of MELDC is given.", "num_citations": "11\n", "authors": ["260"]}
{"title": "An object-based approach to implementing distributed concurrency control\n", "abstract": " We have added distributed concurrency control to the MELD object system by representing inprogress transactions as simulated objects. Transaction objects exploit MELD\u2019s normal message passing facilities to support the concurrency control mechanism. We have completed the implementation of an optimistic mechanism using transaction objects and have designed a twophase locking mechanism based on the same paradigm. We discuss the tradeoffs made and lessons learned, dealing both with transactions on objects and with transactions as objects.", "num_citations": "11\n", "authors": ["260"]}
{"title": "A network architecture for reliable distributed computing\n", "abstract": " A reliable distributed environment (RDE) is proposed that is based on an efficient and reliable extension to datagram communications. The concept of coupled relation is introduced to measure the degree to which distributed environments are reliable. View sections, a programming construct that protects against changes in node status (available or not), as support for distributed computing tasks, are presented. Simulation results are given for coupled relations based on different algorithms, node failure rate, recovery rate, message sending rate, and data missing rate to illustrate the behavior of distributed systems constructed using the view section model on top of RDE.< >", "num_citations": "11\n", "authors": ["260"]}
{"title": "Effectiveness of teaching metamorphic testing\n", "abstract": " This paper is an attempt to understand the effectiveness of teaching metamorphic properties in a senior/graduate software engineering course classroom environment through gauging the success achieved by students in identifying these properties on the basis of the lectures and materials provided in class. The main findings were:(1) most of the students either misunderstood what metamorphic properties are or fell short of identifying all the metamorphic properties in their respective projects,(2) most of the students that were successful in finding all the metamorphic properties in their respective projects had incorporated certain arithmetic rules into their project logic, and (3) most of the properties identified were numerical metamorphic properties. A possible reason for this could be that the two relevant lectures given in class cited examples of metamorphic properties that were based on numerical properties. Based on the findings of the case study, pertinent suggestions were made in order to improve the impact of lectures provided for Metamorphic Testing.", "num_citations": "10\n", "authors": ["260"]}
{"title": "Multi-perspective evaluation of self-healing systems using simple probabilistic models\n", "abstract": " In this paper we construct an evaluation framework for a self-healing system, VM-Rejuv--a virtual machine based rejuvenation scheme for web-application servers--using simple, yet powerful, probabilistic models that capture the behavior of its self-healing mechanisms from multiple perspectives (designer, operator, and end-user). We combine these analytical models with runtime fault-injection to study the operation of VM-Rejuv, and use the results from the fault-injection experiments and model-analysis to reason about the efficacy of VM-Rejuv, its limitations and strategies for mitigating these limitations in system-deployments. Whereas we use VM-Rejuv as the subject of our evaluation in this paper, our main contribution is the demonstration of a practical evaluation approach that can be generalized to other self-healing systems.", "num_citations": "10\n", "authors": ["260"]}
{"title": "A workgroup model for smart pushing and pulling\n", "abstract": " Our Workgroup Cache system operates as a virtual intranet, introducing a shared cache to members of the same workgroup. Users may be members of multiple workgroups at the same time. Criteria are associated with each workgroup to pull documents from an individual cache to the shared cache, or push from the shared cache to an individual cache. These criteria provide semantics of the the workgroup's tasks and interests to reduce latency for its members.", "num_citations": "10\n", "authors": ["260"]}
{"title": "Workgroup middleware for distributed projects\n", "abstract": " We have developed a middleware framework for workgroup environments that can support distributed software development and a variety of other application domains requiring document management and change management for distributed projects. The framework enables hypermedia based integration of arbitrary legacy and new information resources available via a range of protocols, nor necessarily known in advance to us as the general framework developers not even to the environment instance designers. The repositories in which such information resides may be dispersed across the Internet and/or an organizational intranet. The framework also permits a range of client models for user and tool interaction, and applies an extensible suite of collaboration services, including but not limited to multi-participant workflow and coordination, to their information retrievals and updates. That is, the framework is\u00a0\u2026", "num_citations": "10\n", "authors": ["260"]}
{"title": "Distributed Authoring and Versioning\n", "abstract": " The World Wide Web has made retrieving multimedia information on the Internet extremely easy, accessible now to millions. Authoring and publishing Web content, however, remain a difficult-some would say, onerous-task.", "num_citations": "10\n", "authors": ["260"]}
{"title": "An architecture for dynamic reconfiguration in a distributed object-based programming language\n", "abstract": " Distributed applications ideally allow reconfiguration while the application is running, but changes are usually limited to adding new client and server processes and changing the bindings among such processes. In some application domains, such as real-time financial services, it is necessary to support finer grained reconfiguration at the level of entities smaller than processes, but for performance reasons it is desirable to avoid conventional approaches that require dynamic storage allocation. We present a scheme for special cases of fine-grained dynamic reconfiguration sufficient for our application domain and show how it can be used for practical changes. We introduce new language concepts to apply this scheme in the context of an object-based programming language that supports shared data in a distributed environment.", "num_citations": "10\n", "authors": ["260"]}
{"title": "A discourse-based consultant for interactive environments\n", "abstract": " The authors present their automated consultant and describe its application to a practical domain, the Berkeley Unix mail system. They take advantage of previous work on discourse-based assistants that focuses on understanding the user's question and goal, and instead concentrate on the generation of suitable advice.<>", "num_citations": "10\n", "authors": ["260"]}
{"title": "Estimation of system reliability using a semiparametric model\n", "abstract": " An important problem in reliability engineering is to predict the failure rate, that is, the frequency with which an engineered system or component fails. This paper presents a new method of estimating failure rate using a semiparametric model with Gaussian process smoothing. The method is able to provide accurate estimation based on historical data and it does not make strong a priori assumptions of failure rate pattern (e.g., constant or monotonic). Our experiments of applying this method in power system failure data compared with other models show its efficacy and accuracy. This method can be used in estimating reliability for many other systems, such as software systems or components.", "num_citations": "9\n", "authors": ["260"]}
{"title": "Virtual Environment for Collaborative Distance Learning With Video Synchronization\n", "abstract": " We present a 3D collaborative virtual environment, CHIME, in which geographically dispersed students can meet together in study groups or to work on team projects. Conventional educational materials from heterogeneous backend data sources are reflected in the virtual world through an automated metadata extraction and projection process that structurally organizes container materials into rooms and interconnecting doors, with atomic objects within containers depicted as furnishings and decorations. A novel in-world authoring tool makes it easy for instructors to design environments, with additional in-world modification afforded to the students themselves, in both cases without programming. Specialized educational services can also be added to virtual environments via programmed plugins. We present an example plugin that supports synchronized viewing of lecture videos by groups of students with widely varying bandwidths.", "num_citations": "9\n", "authors": ["260"]}
{"title": "MARVEL 3.1: A multi-user software development environment\n", "abstract": " MARVEL 3.1 | Proceedings of the 1993 international symposium on Logic programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsILPS '93MARVEL 3.1: a multi-user software development environment ARTICLE MARVEL 3.1: a multi-user software development environment Share on Author: Gail Elaine Kaiser profile image Gail E. Kaiser View Profile Authors Info & Affiliations Publication: ILPS '93: Proceedings of the 1993 international symposium on Logic programmingDecember 1993 Pages 36\u201339 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts ! to\u2026", "num_citations": "9\n", "authors": ["260"]}
{"title": "Implementing activity structures process modeling on top of the marvel environment kernel\n", "abstract": " Our goal was to implement the activity structures model defined by Software Design & Analysis on top of the MARVEL environment kernel. This involved further design of the activity structures process definition language and enaction model as well as translation and run-time support in terms of facilities provided by MARVEL. The result is an elegant declarative control language for multi-user software processes, with data and activities defined as classes and rules in the previously existing MARVEL Strategy Language. Semantics-based concurrency control is provided by a combination of the MARVEL kernel\u2019s lock and transaction managers and the send/receive synchronization primitives of the activity structures model.", "num_citations": "9\n", "authors": ["260"]}
{"title": "Network management with consistently managed objects\n", "abstract": " A consistency constraint exists between two objects in a network management information base (MIB) if a change in value of one object will cause a change in another. Based on the definition of common knowledge, such constraints can be classified by the time needed to maintain them and the probability of correctly maintaining them. The practical issues of how to build an MIB with a set of objects and consistency constraints are discussed. Various kinds of consistency constraints and their relations with common knowledge are described. Also introduced is the concept of a consistency hierarchy, which presents a gateway between consistency constraints and the management protocols that would actually maintain the consistency between network entities. Building a network management information model using consistency constraints is considered.< >", "num_citations": "9\n", "authors": ["260"]}
{"title": "Sequence model design for code completion in the modern IDE\n", "abstract": " Code completion plays a prominent role in modern integrated development environments (IDEs). Machine learning has become ubiquitous in analogous natural language writing and search software, surfacing more relevant autocompletions and search suggestions in fewer keystrokes. Prior research has reported training high-accuracy, deep neural networks for modeling source code, but little attention has been given to the practical constraints imposed by interactive developer tools. In particular, neural language models for source code modeling like the one described in Maybe Deep Neural Networks are the Best Choice for Modeling Source Code are framed around code completion, but only report accuracy of next-token prediction. However, in order for a language model (LM) to work well within real-world code completion systems, it must also always make suggestions that produce valid code that typechecks to support code completion's role in correctness-checking; return instantaneous results to help programmers code more efficiently in fewer keystrokes; and be small enough to fit comfortably on disk and in memory on developer workstations, since virtually all modern IDEs run locally and support offline usage. To meet these additional requirements, we propose a novel design for predicting top-k next tokens that combines static analysis' ability to enumerate all valid keywords and in-scope identifiers with the ability of a language model to place a probability distribution over them. Our model mixes character-level input representation with token output to represent out-of-vocabulary (OOV) tokens meaningfully and minimize prediction latency\u00a0\u2026", "num_citations": "8\n", "authors": ["260"]}
{"title": "Challenges in behavioral code clone detection\n", "abstract": " When software engineering researchers discuss \"similar\" code, we often mean code determined by static analysis to be textually, syntactically or structurally similar, known as code clones (looks alike). Ideally, we would like to also include code that is behaviorally or functionally similar, even if it looks completely different. The state of the art in detecting these behavioral clones focuses on checking the functional equivalence of the inputs and outputs of code fragments, regardless of its internal behavior (focusing only on input and output states). We argue that with an advance in dynamic code clone detection towards detecting behavioral clones (i.e., those with similar execution behavior), we can greatly increase the applications of behavioral clones as a whole for general program understanding tasks.", "num_citations": "8\n", "authors": ["260"]}
{"title": "Vroom: Faster Build Processes for Java\n", "abstract": " Build processes are too slow. Because most of the build time for Java projects is spent executing tests, researchers have focused on speeding up testing. They've integrated two complementary approaches into a system that seamlessly supports Ant and Maven JUnit build processes. The first approach, unit test virtualization, isolates in-memory dependencies among test cases, which otherwise are isolated inefficiently by restarting the Java Virtual Machine (JVM) before every test. The system supports just-in-time reinitialization of only the small portion of memory needed by the next test, reusing a single JVM. The implementation of this approach is called VMVM (Virtual Machine in the Virtual Machine, pronounced \"vroom vroom\"). In addition, simple setup and tear-down resource management methods designed for sequential execution lead to conflicts when the resources are accessed concurrently. So, the second\u00a0\u2026", "num_citations": "8\n", "authors": ["260"]}
{"title": "Effecting runtime reconfiguration in managed execution environments\n", "abstract": " A self-healing system \u201c... automatically detects, diagnoses and repairs localized hardware and software problems\u201d[2]. Thus we expect a self-healing system to perform runtime reconfigurations or repairs of its components as part of a proactive, preventative or reactive response to conditions arising within its operating environment. This runtime response contrasts with the traditional approach to performing system reconfigurations or repairs\u2013stop the system, fix it, then restart\u2013which requires scheduled or unscheduled downtime and incurs costs that cannot always be expressed strictly in terms of money [3, 4]. Keeping the system running while adaptations are being carried out (even if it means operating in a degraded mode [5, 6]) is in many cases more desirable since it maintains some degree of availability. One software engineering challenge in implementing a self-healing system is managing the degree of coupling between the components that effect system adaptation (collectively referred to as the adaptation engine), and the components that realize the system\u2019s functional requirements (collectively referred to as the target system). For systems being built from scratch, designers can either hardwire adaptation logic into the target system or separate the concerns of adaptation and target system functionality, by means of specialized middleware like IQ-Services [7] and ACT [8] or externalized architectures that include a reconfiguration/repair engine, as in Kinesthetics eXtreme (KX)[9] or Rainbow [10]. For legacy systems\u2013which we define as any system for which the source code is not available, or for which it is undesirable to engage in substantial\u00a0\u2026", "num_citations": "8\n", "authors": ["260"]}
{"title": "Integrating an existing environment with a rule-based process server\n", "abstract": " Inspired by CS for All? Eager to contribute? The Programming Systems Lab, led by Professor Gail Kaiser, is building a collaborative game-based learning and assessment system that infuses computational thinking in grade 6-8 curricula. Near-term projects involve: Tooling Scratch with additional game design features Expanding a visual assessment language and authoring environment based in Blockly [\u2026]", "num_citations": "8\n", "authors": ["260"]}
{"title": "Incremental expression parsing for syntax-directed editors\n", "abstract": " This document describes an algorithm for incremental parsing of expressions in the context of syntax-directed editors for programming languages. Since a syntax-directed editor represents programs as trees and statements and expressions as nodes in trees, making minor modifications in an expression can be difficult. To make these changes in a typical syntax-directed editor, the user must understand the tree structure and type a number of tree-oriented construction and manipulation commands. This document describes an algorithm that allows the user to think in terms of the syntax of the expression as it is displayed on the screen in infix notation rather than in terms of its internal representation which is effectively prefix, while maintaining the benefits of syntax-directed editing. The time and space complexities of the modifications for each new token are linear in the depth of the syntax tree.Descriptors:", "num_citations": "8\n", "authors": ["260"]}
{"title": "Systems and methods for content extraction from mark-up language text accessible at an internet domain\n", "abstract": " Systems and methods are presented for content extraction from markup language text. The content extraction process may parse markup language text into a hierarchical data model and then apply one or more filters. Output filters may be used to make the process more versatile. The operation of the content extraction process and the one or more filters may be controlled by one or more settings set by a user, or automatically by a classifier. The classifier may automatically enter settings by classifying markup language text and entering settings based on this classification. Automatic classification may be performed by clustering unclassified markup language texts with previously classified markup language texts.", "num_citations": "7\n", "authors": ["260"]}
{"title": "Failure analysis of the New York City power grid\n", "abstract": " As US power grid transforms itself into Smart Grid, it has become less reliable in the past years. Power grid failures lead to huge financial cost and affect people\u2019s life. Using a statistical analysis and holistic approach, this paper analyzes the New York City power grid failures: failure patterns and climatic effects. Our findings include: higher peak electrical load increases likelihood of power grid failure; increased subsequent failures among electrical feeders sharing the same substation; underground feeders fail less than overhead feeders; cables and joints installed during certain years are more likely to fail; higher weather temperature leads to more power grid failures. We further suggest preventive maintenance, intertemporal consumption, and electrical load optimization for failure prevention. We also estimated that the predictability of the power grid component failures correlates with the cycles of the North Atlantic Oscillation (NAO) Index.", "num_citations": "7\n", "authors": ["260"]}
{"title": "The Role of Reliability, Availability and Serviceability (RAS) Models in the Design and Evaluation of Self-Healing Systems\n", "abstract": " In an idealized scenario, self-healing systems predict, prevent or diagnose problems and take the appropriate actions to mitigate their impact with minimal human intervention. To determine how close we are to reaching this goal we require analytical techniques and practical approaches that allow us to quantify the effectiveness of a system\u2019s remediations mechanisms. In this paper we apply analytical techniques based on Reliability, Availability and Serviceability (RAS) models to evaluate individual remediation mechanisms of select system components and their combined effects on the system. We demonstrate the applicability of RAS-models to the evaluation of self-healing systems by using them to analyze various styles of remediations (reactive, preventative etc.), quantify the impact of imperfect remediations, identify sub-optimal (less effective) remediations and quantify the combined effects of all the activated remediations on the system as a whole.", "num_citations": "7\n", "authors": ["260"]}
{"title": "Computer and World Wide Web accessibility by visually disabled patients: problems and solutions\n", "abstract": " Rapid advances in information technology have dramatically transformed the world during the past several decades. Access to computers and the World Wide Web is increasingly required for education and employment, as well as for many activities of daily living. While these changes have improved society in many respects, they present an obstacle for visually disabled patients who may have significant difficulty processing the visual cues presented by modern graphical user interfaces. This paper reviews the specific barriers to computer and Web access faced by visually disabled patients, describes clinical evaluation methods, summarizes traditional low vision methods as well as newer assistive computer technologies for universal accessibility, and discusses emerging technologies and future directions in this area.", "num_citations": "7\n", "authors": ["260"]}
{"title": "Genre classification of websites using search engine snippets\n", "abstract": " Web pages often contain clutter (such as ads, unnecessary images and extraneous links) around the body of an article, which distracts a user from actual content. Automatic extraction of \u201cuseful and relevant\u201d content from web pages has many applications, including browsing on small cell phone and PDA screens, speech rendering for the visually impaired, and reducing noise for information retrieval systems. Prior work has led to the development of Crunch, a framework which employs various heuristics in the form of filters and filter settings for content extraction. Crunch allows users to tune these settings, essentially the thresholds for applying each filter. However, in order to reduce human involvement in selecting these heuristic settings, we have extended this work to utilize a website\u2019s classification, defined by its genre and physical layout. In particular, Crunch would then obtain the settings for a previously unknown website by automatically classifying it as sufficiently similar to a cluster of known websites with previously adjusted settings-which in practice produces better content extraction results than a single one-size-fits-all set of setting defaults. In this paper, we present our approach to clustering a large corpus of websites by their genre, utilizing the snippets generated by sending the website\u2019s domain name to search engines as well as the website\u2019s own text. We find that exploiting these snippets not only increased the frequency of function words that directly assist in detecting the genre of a website, but also allow for easier clustering of websites. We use existing techniques like Manhattan distance measure and Hierarchical clustering, with\u00a0\u2026", "num_citations": "7\n", "authors": ["260"]}
{"title": "Ravages of time: Synchronized multimedia for internet-wide process-centered software engineering environments\n", "abstract": " The emergence of Internet-based software engineering projects and of multimedia software development artifacts introduces new opportunities and challenges for team software development environments, particularly process-centered environments. It is becoming increasingly inexpensive to make audio/video recordings of requirements elicitation meetings with customers, architectural design meetings among co-located development staff, videoconferences among dispersed development staff, etc., and store them on-line for future perusal during incremental development and evolutionary maintenance For practical retrieval, multimedia streams must be segmented and indexed according to semantically meaningful or useful units. However, even assuming excellent search precision and recall, and/or rich hypermedia cross-referencing with conventional development artifacts and process milestone records, there is still the problem of transferring or streaming the multimedia information over the relatively low bandwidth and unassured quality of service (QoS) commodity Internet. Thus, multimedia must also be semantically summarized and compressed, perhaps to be reconstructed using local resources. These are all currently major research topics among image processing and computer vision researchers (eg,[CMU99, HSG99, JC99, KY98, MYC95, WC99, ZCC96]).However, those approaches often beg the question as to where the \u201csemantics\u201d come from. They also do not yet address the additional coordination issues that arise when a geographically dispersed team would like to cooperatively employ multimedia for team work, eg,\u201cwatch\u201d video\u00a0\u2026", "num_citations": "7\n", "authors": ["260"]}
{"title": "Architectures for Federation of Process-Centered Environments\n", "abstract": " We describe two models for federating process-centered environments, homogeneous federation where the interoperability is among distinct process models enacted by di erent copies of the same system and heterogeneous federation with interoperability among distinct process enactment systems. We identify the requirements and possible architectures for each model. The bulk of the paper presents the speci c architecture and infrastructure for homogeneous federation we realized in the Oz system. We brie y consider how Oz might be integrated into a heterogeneous federation to serve as one of its interoperating PCEs.", "num_citations": "7\n", "authors": ["260"]}
{"title": "A Marvelous Extended Transaction Processing Model.\n", "abstract": " A key flaw in previous software development environment research is the lack of a transaction model supporting fault tolerance, concurrency control, consistent publication of changes and user control of commit and abort for software development activities. The atomicity properties of the classical transaction model make it unsuitable for industrial software development efforts. We sketch an extended transaction model with a commit-serializability semantics and describe the application of this model to the existing MARVEL architecture for software development environments. MARVEL supports rule-based automation of software development activities and integrates commercial off-the-shelf tools.", "num_citations": "7\n", "authors": ["260"]}
{"title": "AI techniques in software engineering\n", "abstract": " The idca of using artificial intelligence techniqucs to support programming has been around for a long time. The earliest notion was to avoid programming entirely. The human user would just tell the computer what to do, without saying how to do it, and the computer would do the right thing. Even if this were feasible, however, it would be much too tedious, since each time the user would have to repeat the details of what he wanted done. So the goal of programming was to eltplain things to the computer only once, and then later on be able to tell the computer to do the same thing again in some short form, such as the name of the\" program.\" Thus the idea evolved that a user would somehow tell the computer what program was desired, and the computer would write down the program in some internal form so that it could be remembered and repeated later. The assumption was that the resulting program would be correct, complete, efficient, easy to use, and so forth. It would also be exactly what the human user wanted. Several problems would have to be solved to achieve this goal. The first is determining exactly what the human really wants. This is a notorious problem in software engineering; the customer states elttensive requirements, the company develops software that seems to them to meet all requirements to the fullest elttent, hut then the software is next to useless for Ihe customer because what he said he wanted was not really what he needed. It may have heen a computeriled version of manual procedures Ihat were themselves idiosyncratic, or there may be a better way to do things once computers are introduced, or the customer's\u00a0\u2026", "num_citations": "7\n", "authors": ["260"]}
{"title": "Adequate testing and object-oriented programming\n", "abstract": " We are among them as well. However, we have uncovered a flaw in the general wisdom about object-oriented languages\u2014that \u201cproven\"(that is, well-understood, well-tested, and well-used) classes can be reused as superclasses without retesting the inherited code. On the contrary, inherited methods must be retested in most contexts of reuse in order to meet the standards of adequate testing. In this article, we prove this result by applying test adequacy axioms to certain major features of object-oriented languages\u2014in particular, encapsulation in classes, overriding of inherited methods, and multiple inheritance which pose various difficulties for adequately testing a program. Note that our results do not indicate that there is a flaw in the general wisdom that classes promote reuse (which they in fact do), but that some of the attendant assumptions about reuse are mistaken (that is, those concerning testing.)Our past work in object-oriented languages has been concerned with multiple inheritance and issues of granularity as they support reuse (10, 11). Independently, we have developed several technologies for change management in large systems (12, 14, 20) and recently have been investigating the problems of testing as a component of the change process (13), especially the issues of integration and regression testing. When we began to apply our testing approach to object-oriented programs, we expected that retesting object-oriented programs after changes would be easier than retesting equivalent programs writ-ten in conventional languages. Our results, however, have brought this thesis into doubt. Testing object-oriented programs may still\u00a0\u2026", "num_citations": "6\n", "authors": ["260"]}
{"title": "A transaction manager component supporting extended transaction models\n", "abstract": " Database management systems (DBMSs) have increasingly been used for advanced database application. Traditional database applications, such as banking and airline reservation systems, rely on the DBMS to guarantee the consistency of the data. Towards this end, most DBMSs provide a correctness model based on conflict serializability (serializability for short) that assumes no specialized knowledge of the transactions. Many researchers have argued that serializability, as a concurrency control policy, is too restrictive; advanced database applications have semantic information that should be employed when maintaining the consistency of the data. The term semantic information refers to any pertinent knowledge about a transaction, its operations, or its relationships with other transactions. Many extended transaction models (ETMs) have been developed, but since different applications require different ETMs\u00a0\u2026", "num_citations": "6\n", "authors": ["260"]}
{"title": "Distributed tool services via the world wide web\n", "abstract": " We present an architecture for a distributed tool service which operates over HTTP, the underlying protocol of the World Wide Web. This allows unmodified Web browsers to request tool executions from the server as well as making integration with existing systems easier. We describe Rivendell, a prototype implementation of the architecture described.", "num_citations": "6\n", "authors": ["260"]}
{"title": "Enveloping \u201cpersistent\u201d tools for a process-centered environment\n", "abstract": " We have implemented mtp as part of Marvel's successor, Oz, which adds a variety of other new functionality (see [2]). Example applications have included idraw as a UNI_QUEUE tool, where process steps are queued for one-at-a-time execution (the same userid may submit process steps from multiple clients, and the user interface is transferred as needed); emacs as a UNI_NO_QUEUE tool where steps are not queued but may overlap (typically on a single monitor); a local natural language processing system written in commonlisp as a MULTI-QUEUE tool, where steps are queued for one-at-at-time execution (and the UI is transferred among users participating in the same session as needed); and Marvel itself as a MULTI_NO_QUEUE tool (that supplies its own clients for multiple users).", "num_citations": "6\n", "authors": ["260"]}
{"title": "Interfacing Oz with the PCTE OMS\n", "abstract": " This paper details our experiment interfacing Oz with the Object Management System (OMS) of PCTE. Oz is a process-centered multi-user software development environment. PCTE is a specification which defines a language independent interface providing support mechanisms for software engineering environments (SEE) populated with CASE tools. Oz is, in theory, a SEE that can be built (or extended) using the services provided by PCTE. Oz historically has had a native OMS component whereas the PCTE OMS is an open data repository with an API for external software tools. Our experiment focused on changing Oz to use the PCTE OMS. This paper describes how several Oz components were changed in order to make the Oz server interface with the PCTE OMS. The resulting system of our experiment is an environment that has process control and integration services provided by Oz, data integration services provided by PCTE, and tool integration services provided by both. We discusses in depth the concurrency control problems that arise in such an environment and their solutions. The PCTE implementation used in our experiment is the Emeraude PCTE V 12.5. 1 supplied by Transtar Software Incorporation.", "num_citations": "6\n", "authors": ["260"]}
{"title": "Integrating a Transaction manager component with Process Weaver\n", "abstract": " This paper details our experience integrating a transaction manager component, called Pern with Process Weaver. Process Weaver's Petri-net based approach is excellent for explicitly modeling concurrent activities of cooperating agents, but there is no underlying mechanism for treating con icting actions of concurrent, independent agents. In addition, there is a need for advanced transaction support if we are to extend petri nets to use object management systems to store and access data. This paper shows several experiments we performed and our resulting implementation.", "num_citations": "6\n", "authors": ["260"]}
{"title": "Change in the software process\n", "abstract": " As the process community gains more experience with defining software processes, it is natural to consider the problems associated with redefining processes when they change, Change is a challenge both for process definition formalisms and their enaction mechanisms, as well as for the people performing or managing the process. The causes, characteristics, and consequences of change formed the topic for this session, which consisted of an introduction, four short presentations of related work, and open discussion.In her introduction to the session, Karen Huff surveyed some of the sources and characteristics of change. Processes change as new tools or methods become available. Feedback from process enactment leads to change in the form of improvement. Processes also have to be changed when enactment problems arise; a process can be blocked or not working, or in an unanticipated state.(In\u00a0\u2026", "num_citations": "6\n", "authors": ["260"]}
{"title": "A rule-based process server component for constructing rule-based development environments\n", "abstract": " We have been working on the MARVEL rule-based development environment (RBDE) for several years [9, 7], and are currently in the process of scaling up to supporting cooperation among multiple users working together on large projects [1]. This line of research led us to breaking up the previous single user MARVEL 2.6 implementation into the two main pieces of multiple user MARVEL 3.0: the clients representing user interface and activity management, and the server consisting of modules that support process model enaction, concurrency control, and object management [4, 2]. We are now entertaining the definition of a set of components, generalized from MARVEL\u2019s modules, which could be used to construct a range of RBDEs. Here we discuss our initial ideas regarding the most fundamental component, the rule-based process server.The proposed Rule-Based Process Server component (RBPS) would would provide a toolkit for defining rule languages for specifying the process, and corresponding chaining engines supporting a spectrum of assistance models that execute or interpret the process in some manner. RBPS would generalize our previous work on rule-based process modeling as part of the MARVEL environment kernel.", "num_citations": "6\n", "authors": ["260"]}
{"title": "Interfacing cooperative transactions to software development environments\n", "abstract": " Many cooperative transaction models have been proposed to support teams of programmers working together on a software development project. However, papers on this topic often neglect to describe how the programmers might actually interact with the cooperative transactions in terms of user commands and corresponding responses from the software development environment. In this paper, we consider a particular cooperative transaction model, Participant Transactions, and describe the corresponding user model that needs to be implemented by a software development environment.", "num_citations": "6\n", "authors": ["260"]}
{"title": "VMVM: unit test virtualization for Java\n", "abstract": " As software evolves and grows, its regression test suites tend to grow as well. When these test suites become too large, they can eventually reach a point where they become too length to regularly execute. Previous work in Test Suite Minimization has reduced the number of tests in such suites by attempting to identify those that are redundant (eg by a coverage metric). Our approach to ameliorating the runtime of these large test suites is complementary, instead focusing on reducing the overhead of running each test, an approach that we call Unit Test Virtualization. This Tool Demonstration presents our implementation of Unit Test Virtualization, VMVM (pronounced\" vroom-vroom\") and summarizes an evaluation of our implementation on 20 real-world Java applications, showing that it reduces test suite execution time by up to 97%(on average, 62%). A companion video to this demonstration is available online, at\u00a0\u2026", "num_citations": "5\n", "authors": ["260"]}
{"title": "weHelp: a reference architecture for social recommender systems\n", "abstract": " Recommender systems have become increasingly popular. Most of the research on recommender systems has focused on recommendation algorithms. There has been relatively little research, however, in the area of generalized system architectures for recommendation systems. In this paper, we introduce weHelp: a reference architecture for social recommender systems-systems where recommendations are derived automatically from the aggregate of logged activities conducted by the system\u2019s users. Our architecture is designed to be application and domain agnostic. We feel that a good reference architecture will make designing a recommendation system easier; in particular, weHelp aims to provide a practical design template to help developers design their own well-modularized systems.", "num_citations": "5\n", "authors": ["260"]}
{"title": "Empirical study of concurrency mutation operators for java\n", "abstract": " Mutation testing is a white-box fault-based software testing technique that applies mutation operators to modify program source code or byte code in small ways and then runs these modified programs (ie, mutants) against a test suite in order to measure its effectiveness and locate the weaknesses either in the test data or in the program that are seldom or never exposed during normal execution. In this paper, we describe our implementation of a generic mutation testing framework and the results of applying three sets of concurrency mutation operators on four example Java programs through empirical study and analysis.", "num_citations": "5\n", "authors": ["260"]}
{"title": "Towards in vivo testing of software applications\n", "abstract": " Software products released into the field typically have some number of residual bugs that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system. Testing approaches such as perpetual testing or continuous testing seek to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present our initial work towards a testing methodology we call \u201cin vivo testing\u201d, in which unit tests are continuously executed inside a running application in the deployment environment. In this novel approach, unit tests execute within the current state of the program (rather than by creating a clean slate) without affecting or altering that state. Our approach has been shown to reveal defects both in the applications of interest and in the unit tests themselves. It can also be used for detecting concurrency or robustness issues that may not have appeared in a testing lab. Here we describe the approach, the testing framework we have developed for Java applications, classes of bugs our approach can discover, and the results of experiments to measure the added overhead.", "num_citations": "5\n", "authors": ["260"]}
{"title": "Dynamic adaptation of temporal event correlation for qos management in distributed systems\n", "abstract": " Temporal event correlation is essential to managing quality of service in distributed systems, especially correlating events from multiple components to detect problems with availability, performance, and denial of service attacks. Two challenges in temporal event correlation are: (1) handling lost events and (2) dealing with inaccurate clocks. We show that both challenges are related to event propagation delays that result from contention for network and server resources. We develop an approach to adjusting the timer values of event correlation rules based on propagation delays in order to reduce missed alarms and false alarms. Our approach has three parts: an infrastructure for real-time measurement of propagation delay, a statistical approach to estimating propagation delays, and a controller that uses estimates of propagation delays to update timer values in temporal rules. Our approach eliminates the need for\u00a0\u2026", "num_citations": "5\n", "authors": ["260"]}
{"title": "A Framework for Quality Assurance of Machine Learning Applications\n", "abstract": " Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test and debug such ML software, because there is no reliable test oracle. We describe a framework and collection of tools aimed to assist with this problem. We present our findings from using the testing framework with three implementations of an ML ranking algorithm (all of which had bugs).", "num_citations": "5\n", "authors": ["260"]}
{"title": "CASPER: compiler-assisted securing of programs at runtime\n", "abstract": " Ensuring the security and integrity of computer systems deployed on the Internet is growing harder. This is especially true in the case of server systems based on open source projects like Linux, Apache, Sendmail, etc. since it is easier for a hacker to get access to the binary format of deployed applications if the source code to the software is publicly accessible. Often, having a binary copy of an application program is enough to help locate security vulnerabilities in the program. In the case of legacy systems where the source code is not available, advanced reverse-engineering and decompilation techniques can be used to construct attacks. This paper focuses on measures that reduce the effectiveness of hackers at conducting large-scale, distributed attacks. The first line of defense involves additional runtime checks that are able to counteract the majority of hacking attacks. Introducing diversity in deployed systems to severely diminish the probability of propagation to other systems helps to prevent effective attacks like the DDOS attack against the DNS root servers in October 21, 2002.Descriptors:", "num_citations": "5\n", "authors": ["260"]}
{"title": "Collaborative technologies for evolving software systems\n", "abstract": " The article discusses four major collaboration theme areas: collaborative information engineering; collaborative process and workflow; collaboration support (awareness); and integration of collaborative systems. Various projects incorporating these technologies are outlined. All of the projects described are developing component technologies intended for mixing and matching with each other and with technologies under investigation by other EDCS projects not particularly concerned with collaborative work, as well as with COTS software. Only GroupSpaces intends to assemble a full-blown, directly usable software development environment framework, and that comprises components, developed under EDCS and available off-the-shelf.", "num_citations": "5\n", "authors": ["260"]}
{"title": "Support algorithms for incremental attribute evaluation of asynchronous subtree replacements\n", "abstract": " A solution to the problem of incremental attribute evaluation for multiple asynchronous subtree replacements that is applicable to arbitrary noncircular attribute grammars is discussed. The algorithm supports multiple independent editing cursors. Concurrent evaluation processes proceed independently as long as they cover disjoint regions of the derivation tree. Evaluation processes are merged when they overlap, to prevent unnecessary attribute evaluations. The complexity of these three parts of the algorithm is discussed. The algorithm ensures that when evaluation terminates, the tree is consistently attributed. The results solve two open problems that arose in connection with the original algorithm for asynchronous subtree replacements reported by S.M. Kaplan and G.F. Kaiser (1986).< >", "num_citations": "5\n", "authors": ["260"]}
{"title": "MeldC Threads: Supporting Large-Scale Dynamic Parallelism\n", "abstract": " We present a new thread model that supports large-scale dynamic parallelism, ie, the number of coexisting threads is large and the life-time for each thread is short. We introduce the interleaving stack (IS), which melds the frames of multiple threads into one shared stack. The pure interleaving stack is not scalable due to severe internal fragmentation. We propose and evaluate an improvement, circular interleaving stack (CIS). Our simulation shows that in the domains of large-scale dynamic parallelism, CIS performs better than the traditional one-stack-per-thread (1SPT) mechanism with respect to memory utilization. The 1SPT mechanism gains better memory utilization as thread life-time increases. CIS and 1SPT show similar CPU utilization.", "num_citations": "5\n", "authors": ["260"]}
{"title": "Object-oriented programs and testing\n", "abstract": " Brooks, in his paper \u2018\u2018No Silver Bullet: Essence and Accidents of Software Engineering\u2019\u2019[3], states:", "num_citations": "5\n", "authors": ["260"]}
{"title": "Automatic detection of defects in applications without test oracles\n", "abstract": " In application domains that do not have a test oracle, such as machine learning and scientific computing, quality assurance is a challenge because it is difficult or impossible to know in advance what the correct output should be for general input. Previously, metamorphic testing has been shown to be a simple yet effective technique in detecting defects, even without an oracle. In metamorphic testing, the application\u2019s \u201cmetamorphic properties\u201d are used to modify existing test case input to produce new test cases in such a manner that, when given the new input, the new output can easily be computed based on the original output. If the new output is not as expected, then a defect must exist. In practice, however, metamorphic testing can be a manually intensive technique for all but the simplest cases. The transformation of input data can be laborious for large data sets, and errors can occur in comparing the outputs when they are very complex. In this paper, we present a tool called Amsterdam that automates metamorphic testing by allowing the tester to easily set up and conduct metamorphic tests with little manual intervention, merely by specifying the properties to check, configuring the framework, and running the software. Additionally, we describe an approach called Heuristic Metamorphic Testing, which addresses issues related to false positives and non-determinism, and we present the results of new empirical studies that demonstrate the effectiveness of metamorphic testing techniques at detecting defects in real-world programs without test oracles. Copyright c 2010 John Wiley & Sons, Ltd.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Autonomic Control for Quality Collaborative Video Viewing\n", "abstract": " We present an autonomic controller for quality collaborative video viewing, which allows groups of geographically dispersed users with different network and computer resources to view a video in synchrony while optimizing the video quality experienced. The autonomic controller is used within a tool for enhancing distance learning with synchronous group review of online multimedia material. The autonomic controller monitors video state at the clients\u2019 end, and adapts the quality of the video according to the resources of each client in (soft) real time. Experimental results show that the autonomic controller successfully synchronizes video for small groups of distributed clients and, at the same time, enhances the video quality experienced by users, in conditions of fluctuating bandwidth and variable frame rate.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Combining mobile agents and process-based coordination to achieve software adaptation\n", "abstract": " We have developed a model and a platform for end-to-end run-time monitoring behavior and performance analysis and consequent dynamic adaptation of distributed applications. This paper concentrates on how we coordinate and actuate the potentially multi-part adaptation operating externally to the target systems that is without requiring any a priori built-in adaptation facilities on the part of said target systems. The actual changes are performed on the fly onto the target by communities of mobil software agents coordinated by a decentralized process engine. These changes can be coarse grained, such as changing the operational parameters entire components or rearranging the connections among components, or fine-grained, such as changing the operational parameters internal state and functioning logic of individual components. We discuss our successful experience using our approach in dynamic adaptation of a large-scale commercial application which require both coarse and fine grained modifications.Descriptors:", "num_citations": "4\n", "authors": ["260"]}
{"title": "JPernLite: Extensible Transaction Services for WWW\n", "abstract": " Concurrency control is one of the key problems in the design and implementation of collaborative systems such as hypertexthypermedia systems, CADCAM systems and software development environments, Most existing systems store data in specialized database with built-in concurrency control policies, usually implemented via locking.Descriptors:", "num_citations": "4\n", "authors": ["260"]}
{"title": "WebPern: An extensible transaction server for the World Wide Web\n", "abstract": " Concurrency control is a well-known problem in collaborative hypermedia system design and implementation. Existing systems store data and links in databases (link servers or hyperbases) that have built-in concurrency control polices such as serializability and atomicity, usually implemented using locking.Whereas it is desirable to construct collaborative hypermedia systems on top of the World Wide Web, most web servers do not have concurrency control features such as support for distributed (database) transactions and extensible concurrency control policies such as event notification, shared locks and fine granularity locks.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Extending attribute grammars to support programming-in-the-large\n", "abstract": " Attribute grammars add specification of static semantic properties to context-free grammars, which, in turn, describe the syntactic structure of program units. However, context-free grammars cannot express programming-in-the-large features common in modern programming languages, including unordered collections of units, included units, and sharing of included units. We present extensions to context-free grammars, and corresponding extensions to attribute grammars, suitable for defining such features. We explain how batch and incremental attribute-evaluation algorithms can be adapted to support these extensions, resulting in a uniform approach to intraunit and interunit static semantic analysis and translation of multiunit programs.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Process evolution for constraint-enforcing environments\n", "abstract": " We present a process evolution approach for process-centered environments whose enaction engine enforces constraints in the process, in the sense that a process step cannot be performed unless its prerequisites have been completed. When a user discovers a problem with the installed process model of an ongoing process, it must be changed (as opposed to ignored). The algorithm analyzes the differences between the proposed and installed process models, and detects each new constraint that would be imposed. The algorithm then generates a list of parameterized process steps that may need to be enacted in order to apply the new constraints retroactively to the existing process state reflecting the progress through the installed process. The purpose of these steps is to revise the process state, with respect to their parameters, to a point consistent with the new constraints\u2014but without undoing any past work. The user can decide whether to actually enact each such step, or \u2018\u2018pretend\u2019\u2019the step was enacted (so that a new constraint would in practice be imposed only on future work). Copyright\u00a9 1994 Programming Systems Laboratory, Columbia University", "num_citations": "4\n", "authors": ["260"]}
{"title": "Incremental process support for code reengineering\n", "abstract": " Reengineering a large code base can be a monumental task, and the situation becomes even worse if the code is concomitantly being modi ed. For the past two years, we have been using the Marvelprocess centered environment (PCE) for all of our software development and are currently using it to develop the Oz PCE (Marvel's successor). Towards this e ort, we are reengineering Oz's code base to isolate the process engine, transaction manager, and object management system as separate components that can be mixed and matched in arbitrary systems. In this paper, we show how a PCE can guide and assist teams of users in carrying out code reengineering while allowing them to continue their normal code development. The key features to this approach are its incremental nature and the ability of the PCE to automate most of the tasks necessary to maintain the consistency of the code base.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Extending the MERCURY system to support teams of Ada programmers\n", "abstract": " The MERCURY system generates multi-user language-based environments from attribute grammars. The AG specifies the interface checking among modular units, to be applied by the environment to inform programmers of errors introduced by interface changes. Since AGs assume a monolithic program, we extended the formalism to support separate compilation units. Our previous work was based on an ideal language where programs consist of an unordered set of monolithic compilation units. We now augment our extensions to support Ada, to allow multiple kinds of compilation units and nested compilation units. We describe how these extensions are used to detect naming errors, determine compilation unit context, and check compilation order as mandated by Ada.", "num_citations": "4\n", "authors": ["260"]}
{"title": "Making progress in cooperative transaction models\n", "abstract": " In the classical transaction model, transactions are consistency preserving units: a transaction is made up of a series of actions which, when executed in isolation in a reliable environment, transforms the database from one consistent state to another [2]. A classical transaction management system guarantees the appearance of isolation and reliability, even though multiple transactions execute concurrently and hardware and software components fail. It does this by enforcing atomicity and serializability: atomicity means that either an entire transaction apparently executes to completion or not at all, while serializability means that the effects of the transactions are viewed as if the transactions had executed in some serial order, one completing before the next begins. This is accomplished by considering the objects read and written by concurrently executing transactions, and ensuring that either all updates are completed or none are, and that the read and write dependencies among the set of transactions correspond to some serial order of the transactions.There have been many proposals for extending the transaction model from its original data processing applications to software development, CAD/CAM and other forms of cooperative work. The notion of \u2018\u2018transaction\u2019\u2019is intuitively appealing in these domains, since forward progress often depends on making a related set of changes to a program, design or document in such a way that it is transformed from one consistent state to another. As with data processing applications,\u2018\u2018consistency\u2019\u2019depends on the requirements of the domain, such as a new system configuration passing regression and\u00a0\u2026", "num_citations": "4\n", "authors": ["260"]}
{"title": "Side channel attack on smartphone sensors to infer gender of the user\n", "abstract": " Smartphones incorporate a plethora of diverse and powerful sensors that enhance user experience. Two such sensors are accelerometer and gyroscope, which measure acceleration in all three spatial dimensions and rotation along the three axes of the smartphone, respectively. These sensors are used primarily for screen rotations and advanced gaming applications. However, they can also be employed to gather information about the user's activity and phone positions. In this work, we investigate using accelerometer and gyroscope as a side-channel to learn highly sensitive information, such as the user's gender. We present an unobtrusive technique to determine the gender of a user by mining data from the smartphone sensors, which do not require explicit permissions from the user. A preliminary study conducted on 18 participants shows that we can detect the user's gender with an accuracy of 80%.", "num_citations": "3\n", "authors": ["260"]}
{"title": "A case study on the impact of similarity measure on information retrieval based software engineering tasks\n", "abstract": " Information Retrieval (IR) plays a pivotal role in diverse Software Engineering (SE) tasks, e.g., bug localization and triaging, code retrieval, requirements analysis, etc. The choice of similarity measure is the core component of an IR technique. The performance of any IR method critically depends on selecting an appropriate similarity measure for the given application domain. Since different SE tasks operate on different document types like bug reports, software descriptions, source code, etc. that often contain non-standard domain-specific vocabulary, it is essential to understand which similarity measures work best for different SE documents. This paper presents two case studies on the effect of different similarity measure on various SE documents w.r.t. two tasks: (i) project recommendation: finding similar GitHub projects and (ii) bug localization: retrieving buggy source file(s) correspond to a bug report. These tasks contain a diverse combination of textual (i.e. description, readme) and code (i.e. source code, API, import package) artifacts. We observe that the performance of IR models varies when applied to different artifact types. We find that, in general, the context-aware models achieve better performance on textual artifacts. In contrast, simple keyword-based bag-of-words models perform better on code artifacts. On the other hand, the probabilistic ranking model BM25 performs better on a mixture of text and code artifacts. We further investigate how such an informed choice of similarity measure impacts the performance of SE tools. In particular, we analyze two previously proposed tools for project recommendation and bug localization tasks\u00a0\u2026", "num_citations": "3\n", "authors": ["260"]}
{"title": "Metamorphic runtime checking of applications without test oracles\n", "abstract": " Challenges arise in testing applications that do not have test oracles, ie, for which it is impossible or impractical to know what the correct output should be for general input. Metamorphic testing, introduced by Chen et al., has been shown to be a simple yet effective technique in testing these types of applications: test inputs are transformed in such a way that it is possible to predict the expected change to the output, and if the output resulting from this transformation is not as expected, then a fault must exist. Here, we improve upon previous work by presenting a new technique called Metamorphic Runtime Checking, which automatically conducts metamorphic testing of both the entire application and individual functions during a program\u2019s execution. This new approach improves the scope, scale, and sensitivity of metamorphic testing by allowing for the identification of more properties and execution of more tests, and increasing the likelihood of detecting faults not be found by application-level properties alone. We also discuss a technique for automatically discovering functions\u2019 metamorphic properties, and present the results of new studies that demonstrate that Metamorphic Runtime Checking advances the state of the art in testing applications without oracles.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Towards using cached data mining for large scale recommender systems\n", "abstract": " Recommender systems are becoming increasingly popular. As these systems become commonplace and the number of users increases, it will become important for these systems to be able to cope with a large and diverse set of users whose recommendation needs may be very different from each other. In particular, large scale recommender systems will need to ensure that users\u2019 requests for recommendations can be answered with low response times and high throughput. In this paper, we explore how to use caches and cached data mining to improve the performance of recommender systems by improving throughput and reducing response time for providing recommendations. We describe the structure of our cache, which can be viewed as a prefetch cache that prefetches all types of supported recommendations, and how it is used in our recommender system.We also describe the results of our\u00a0\u2026", "num_citations": "3\n", "authors": ["260"]}
{"title": "The weHelp reference architecture for community-driven recommender systems\n", "abstract": " Recommender systems have become increasingly popular. Most research on recommender systems has focused on recommendation algorithms. There has been relatively little research, however, in the area of generalized system architectures for recommendation systems. In this paper, we introduce weHelp-a reference architecture for social recommender systems. Our architecture is designed to be application and domain agnostic, but we briefly discuss here how it applies to recommender systems for software engineering.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Adaptive synchronization of semantically compressed instructional videos for collaborative distance learning\n", "abstract": " The increasing popularity of online courses has highlighted the need for collaborative learning tools for student groups. In this article, we present an e-Learning architecture and adaptation model called AI2TV (Adaptive Interactive Internet Team Video), which allows groups of students to collaboratively view instructional videos in synchrony. Video player actions, like play, pause and stop, can be initiated by any group member and and the results of those actions are synchronized with all the other students. These features allow students to review a lecture video in tandem, facilitating the learning process. AI2TV upholds the invariant that each student will receive semantically equivalent content at all times. Experimental trials show that AI2TV successfully synchronizes instructional videos for distributed students while concurrently optimizing the video quality, even under conditions of fluctuating bandwidth, by\u00a0\u2026", "num_citations": "3\n", "authors": ["260"]}
{"title": "Method and platform for the automated management of distributed system, corresponding telecommunications network and computer program product\n", "abstract": " A method for managing a distributed system having processing modules and based on the execution of at least a process within a process engine, which is possibly decentralised, entails interventions for run-time adaptation of the processing modules accomplished by means of effectors. A uniform implementation surface configured to interact with the effectors in transparent fashion relative to the technological implementation peculiarities of the effectors themselves. Preferably, the interface has associated therewith a repository of the effectors as well as an implementation module for selecting, instancing, invoking and managing the effectors according to the tasks implemented in the process. The related platform can be used, for instance, to manage personal messaging in a telecommunications network.", "num_citations": "3\n", "authors": ["260"]}
{"title": "A genre-based clustering approach to content extraction\n", "abstract": " The content of a webpage is usually contained within a small body of text and images, or perhaps several articles on the same page; however, the content may be lost in the clutter (defined as cosmetic features such as animations, menus, sidebars, obtrusive banners). Automatic content extraction has many applications, including browsing on small cell phone and PDA screens, speech rendering for the visually impaired, and reducing noise for information retrieval systems. We have developed a framework, Crunch, which employs various heuristics for content extraction in the form of filters applied to the webpage\u2019s DOM tree; the filters aim to prune or transform the clutter, leaving only the content. Crunch allows users to tune what we call \u201csettings\u201d, consisting of thresholds for applying a particular filter and/or for toggling a filter on/off, because the HTML components that characterize clutter can vary significantly from website to website. However, we have found that the same settings tend to work well across different websites of the same genre, eg, news or shopping, since the designers often employ similar page layouts. In particular, Crunch could obtain the settings for a previously unknown website by automatically classifying it as sufficiently similar to a cluster of known websites with previously adjusted settings. We present our approach to clustering a large corpus of websites into genres, using their pre-extraction textual material augmented by the snippets generated by searching for the website\u2019s domain name in web search engines. Including these snippets increases the frequency of function words needed for clustering. We use existing\u00a0\u2026", "num_citations": "3\n", "authors": ["260"]}
{"title": "Embedding model-based architecting in a collaborative environment\n", "abstract": " CHIME is an immersive, collaborative virtual environment designed to support the productivity of teams of software developers. Our initial experiences with CHIME have pointed out some deficiencies which limit its potential usefulness as a support environment for large software projects. In an attempt to address these deficiencies, we plan to create within CHIME \u201cstructured guidance\u201d based on the MBASE software engineering approach. In this paper, we describe CHIME and our initial experiences with it (which led us to consider adding a more structured underpinning), as well as the synergies we hope to exploit by embedding MBASE.", "num_citations": "3\n", "authors": ["260"]}
{"title": "WebCity: A WWW-based hypermedia environment for software development\n", "abstract": " Many SDEs (Software Development Environments) have been built to help increase software productivity. However, these systems often have several deficiencies: inability to operate/act on external data; hardwired behaviors; and finally lack of a universal remote access mechanism for geographically distributed users.In this paper we first introduce OzWeb, a system which combines the elements of SDE, Workflow Management, and the World Wide Web (WWW). We explain how it provides a solution for the above three problems. Then we describe WebCity, an instance of SDE which applies OzWeb's technology to a real environment. We will discuss how it solves those problems with SDE, and our experience in building and using it. Finally we compare WebCity with similar systems that also deploy Web technology.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Testing reliable distributed applications through simulated events\n", "abstract": " There are many distributed applications that Incorporate application-specific reliability algorithms which operate on top of general purpose networking. operating system and programming language facilities. We present a framework for application-level reliability testing suitable for a wide range of distributed applications, and desCribe how we've applied it to one particular application, Mercury, a distributed, multi-user programm. t. ng environment.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Rapid prototyping of concurrent programming languages\n", "abstract": " A novel approach is proposed for automatic generation of concurrent interpreters for formal specifications of the programming languages. It consists of a formal notation and supporting algorithms. The formal notation is called action equations, which are attribute grammars extended by the concepts of events and unification. The supporting algorithms include preprocessing algorithms to generate the interpreters and evaluation algorithms embedded in the generated interpreters. Attribute grammars are reviewed, the extension to the action equations paradigm is explained, and the synthesis of action equations with a unification strategy as a means for specifying and implementing synchronization primitives is presented. The approach is illustrated by a specification of CSP. A brief comparison to related work is included.<>", "num_citations": "3\n", "authors": ["260"]}
{"title": "SMILE/MARVEL: Two Approaches to Knowledge-Based Programming Environments\n", "abstract": " SMILE is a distributed, multi-user software engineering environment that behaves as an intelligent assistant SMll... E presents a'flleless environment', derives and transfonns data to shelter users from entering redundant infonnation, automatically invokes programming tools, and actively participates in the software development and maintenance process. Unlike other intelligent assistants, SMILE is not a rule-based environment: its knowledge of software objects and the programming process is hardcoded into the environment We describe SMll.. E's functionality and explain how we achieved this functionality without reliance on artificial intelligence technology.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Automatic extension of an ATN knowledge base\n", "abstract": " A computer program is described that acquires much of its knowledge from conversations among operators on Morse code radio networks. The system consists of a learning component and a language understander. The learning component extends a \u2018core\u2019 augmented transition network (ATN) knowledge base by generalizing from sentences taken from scripts of actual conversations. The extensions enable the understanding component to process a large number of sentences that are syntactically and semantically similar to the examples.", "num_citations": "3\n", "authors": ["260"]}
{"title": "Automatic extension of an augmented transition network grammar for Morse code conversations\n", "abstract": " This report describes a learning program that acquires much of the knowledge required by a parsing system that processes conversations in a natural language akin to ham-radio jargon. The learning program derives information from example sentences taken from transcripts of actual conversations, and uses this knowledge to extend the core augmented transition network ATN grammar. The parser can use the extended grammar to process the example sentences, plus a large number of syntactically and semantically related sentences. The learning program uses a set of heuristics to determine the difference between the existing version of the grammar and a superset that could process the example sentence. A set of models act as templates to produce possible extensions to the grammar. An evaluation measure selects one of the extensions and adds it to the grammar. This extension is henceforth an integral component of the knowledge base and may be used by the parser to process conversations and by the learning program to extend the grammar further. This report relates the mechanisms used by the learning program to grammatical inference of context-sensitive languages, which include the natural languages, and some proposed linguistic models of human language acquisition. These models describe language acquisition as a process of developing hypotheses according to the constraints of innate universal rules, and acceptance of those hypotheses that make it possible for the child to understand new sentences.Descriptors:", "num_citations": "3\n", "authors": ["260"]}
{"title": "Detecting Sensor-Based Repackaged Malware\n", "abstract": " Android is the most targeted mobile OS. Studies have found that repackaging is one of the most common techniques that adversaries use to distribute malware, and detecting such malware can be difficult because they share large parts of the code with benign apps. Other studies have highlighted the privacy implications of zero-permission sensors. In this work, we investigate if repackaged malicious apps utilize more sensors than the benign counterpart for malicious purposes. We analyzed 15,297 app pairs for sensor usage. We provide evidence that zero-permission sensors are indeed used by malicious apps to perform various activities. We use this information to train a robust classifier to detect repackaged malware in the wild.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Toward Optimal Selection of Information Retrieval Models for Software Engineering Tasks\n", "abstract": " Information Retrieval (IR) plays a pivotal role in diverse Software Engineering (SE) tasks, e.g., bug localization and triaging, bug report routing, code retrieval, requirements analysis, etc. SE tasks operate on diverse types of documents including code, text, stack-traces, and structured, semi-structured and unstructured meta-data that often contain specialized vocabularies. As the performance of any IR-based tool critically depends on the underlying document types, and given the diversity of SE corpora, it is essential to understand which models work best for which types of SE documents and tasks. We empirically investigate the interaction between IR models and document types for two representative SE tasks (bug localization and relevant project search), carefully chosen as they require a diverse set of SE artifacts (mixtures of code and text), and confirm that the models' performance varies significantly with mix of\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Deobfuscating Android Applications through Deep Learning\n", "abstract": " Android applications are nearly always obfuscated before release, making it difficult to analyze them for malware presence or intellectual property violations. Obfuscators might hide the true intent of code by renaming variables, modifying the control flow of methods, or inserting additional code. Prior approaches toward automated deobfuscation of Android applications have relied on certain structural parts of apps remaining as landmarks, un-touched by obfuscation. For instance, some prior approaches have assumed that the structural relationships between identifiers (eg that A represents a class, and B represents a field declared directly in A) are not broken by obfuscators; others have assumed that control flow graphs maintain their structure (eg that no new basic blocks are added). Both approaches can be easily defeated by a motivated obfuscator. We present a new approach to deobfuscating Android apps that leverages deep learning and topic modeling on machine code, MACNETO. MACNETO makes few assumptions about the kinds of modifications that an obfuscator might perform, and we show that it has high precision when applied to two different state-of-the-art obfuscators: ProGuard and Allatori.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Metamorphic runtime checking of applications without test oracles\n", "abstract": " SLOW SOFTWARE BUILD CYCLES substantially hinder continuous integration during development. They can be an even more significant nuisance for continuous delivery and other release processes. As a complex software system evolves and its compilation and packaging process becomes more complicated, building changes from a process that developers perform frequently on their desktop machines after every small code edit, to one performed nightly on a dedicated build machine, to one that can\u2019t even be performed in its entirety overnight. We aim to significantly reduce build time, with a sufficiently general solution applicable to both full (\u201cclean\u201d) and incremental builds. We decided to reduce building time by reducing testing time. To do this, we developed a system that combines two approaches. The first approach, unit test virtualization, isolates in-memory dependencies among test cases, which otherwise are isolated inefficiently by restarting the Java Virtual Machine (JVM) before every test. We call our implementation of this approach VMVM (Virtual Machine in the Virtual Machine, pronounced \u201cvroom vroom\u201d). The second approach, virtualized unit test virtualization, isolates external dependencies such as files and network ports while long-running tests execute in parallel. We call our implementation of this approach VMVMVM (Virtual Machine in a Virtual Machine on a Virtual Machine\u2014\u201cvroom vroom vroom\u201d).", "num_citations": "2\n", "authors": ["260"]}
{"title": "Code Relatives: Detecting Similar Software Behavior\n", "abstract": " Detecting \u201csimilar code\u201d is fundamental to many software engineering tasks. Current tools can help detect code with statically similar syntactic features (code clones). Unfortunately, some code fragments that behave alike without similar syntax may be missed. In this paper, we propose the term \u201ccode relatives\u201d to refer to code with dynamically similar execution features. Code relatives can be used for such tasks as implementation-agnostic code search and classification of code with similar behavior for human understanding, which code clone detection cannot achieve. To detect code relatives, we present DyCLINK, which constructs an approximate runtime representation of code using a dynamic instruction graph. With our link analysis based subgraph matching algorithm, DyCLINK detects fine-grained code relatives efficiently. In our experiments, DyCLINK analyzed 290+ million prospective subgraph matches. The results show that DyCLINK detects not only code relatives, but also code clones that the state-of-the-art system is unable to identify. In a code classification problem, DyCLINK achieved 96% precision on average compared with the competitor\u2019s 61%.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Phosphor: Illuminating dynamic data flow in the jvm\n", "abstract": " Dynamic taint analysis is a well-known information flow analysis problem with many possible applications. Taint tracking allows for analysis of application data flow by assigning labels to inputs, and then propagating those labels through data flow. Taint tracking systems traditionally compromise among performance, precision, accuracy, and portability. Performance can be critical, as these systems are typically intended to be deployed with software, and hence must have low overhead. To be deployed in securityconscious settings, taint tracking must also be accurate and precise. Dynamic taint tracking must be portable in order to be easily deployed and adopted for real world purposes, without requiring recompilation of the operating system or language interpreter, and without requiring access to application source code.We present PHOSPHOR, a dynamic taint tracking system for the Java Virtual Machine (JVM) that simultaneously achieves our goals of performance, accuracy, precision, and portability. Moreover, to our knowledge, it is the first portable general purpose taint tracking system for the JVM. We evaluated PHOSPHOR\u2019s performance on two commonly used JVM languages (Java and Scala), on two versions of two commonly used JVMs (Oracle\u2019s HotSpot and OpenJDK\u2019s IcedTea) and on Android\u2019s Dalvik Virtual Machine, finding its performance to be impressive: as low as 3%(53% on average), using the DaCapo macro benchmark suite. This paper describes the approach that PHOS-PHOR uses to achieve portable taint tracking in the JVM.", "num_citations": "2\n", "authors": ["260"]}
{"title": "A gameful approach to teaching software design and software testing-assignments and quests\n", "abstract": " Introductory CS classes typically do not focus on software testing [5, 6]. A lot of students\u2019 mental model when they start learning programming is that \u201cif it compiles and runs without crashing, it must work fine.\u201d Despite numerous attempts to introduce testing early in CS programs and many known benefits to inculcating good testing habits early in one\u2019s programming life [4, 6], students remain averse to software testing as there is low student interest in software testing [5]. To address this problem, we used an internally developed research system called HALO\u2014\u201cHighly Addictive sociaLly Optimized Software Engineering\u201d[1]. Our previous work describes early prototypes of HALO; in this paper, we describe how we used it for the CS2 class and the feedback from real users. HALO uses game-like elements and motifs from popular games like World of Warcraft [2] to make the whole software engineering process and in particular, the software testing process, more engaging and social. HALO is not a game; it leverages game mechanics and applies them to the software development process. For example, in HALO, students are given a number of \u201cquests\u201d that they need to complete. These quests are used to disguise standard software testing techniques like white and black box testing, unit testing, and boundary value analysis. Upon completing these quests, the students get social rewards in the form of achievements, titles, and experience points. They can see how they are doing compared to other students in the class. While the students think that they are competing just for points and achievements, the primary benefit of such a system is that the students\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Rust: A retargetable usability testbed for website authentication technologies\n", "abstract": " Website authentication technologies attempt to make the identity of a website clear to the user, by supplying information about the identity of the website. In practice however, usability issues can prevent users from correctly identifying the websites they are interacting with. To help identify usability issues we present RUST, a Retargetable USability Testbed for website authentication technologies. RUST is a testbed that consists of a test harness, which provides the ability to easily configure the environment for running usability study sessions, and a usability study design that evaluates usability based on spoofability, learnability, and acceptability. We present data collected by RUST and discuss preliminary results for two authentication technologies, Microsoft CardSpace and Verisign Secure Letterhead. Based on the data collected, we conclude that the testbed is useful for gathering data on a variety of technologies.", "num_citations": "2\n", "authors": ["260"]}
{"title": "The in vivo approach to testing software applications\n", "abstract": " Software products released into the field typically have some number of residual bugs that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system. Testing approaches such as perpetual testing or continuous testing seek to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present our initial work towards a testing methodology we call in vivo testing, in which unit tests are continuously executed inside a running application in the deployment environment. These tests execute within the current state of the program (rather than by creating a clean slate) without affecting or altering that state. Our approach can reveal defects both in the applications of interest and in the unit tests themselves. It can also be used for detecting concurrency or robustness issues that may not have appeared in a testing lab. Here we describe the approach and the testing framework called Invite that we have developed for Java applications. We also enumerate the classes of bugs our approach can discover, and provide the results of a case study on a publicly-available application, as well as the results of experiments to measure the added overhead.", "num_citations": "2\n", "authors": ["260"]}
{"title": "RAS-Models: A Building Block for Self-Healing Benchmarks\n", "abstract": " To evaluate the efficacy of self-healing systems a rigorous, objective, quantitative benchmarking methodology is needed. However, developing such a benchmark is a non-trivial task given the many evaluation issues to be resolved, including but not limited to: quantifying the impacts of faults, analyzing various styles of healing (reactive, preventative, proactive), accounting for partially automated healing and accounting for incomplete/imperfect healing. We posit, however, that it is possible to realize a self-healing benchmark using a collection of analytical techniques and practical tools as building blocks. This paper highlights the flexibility of one analytical tool, the Reliability, Availability and Serviceability (RAS) model, and illustrates its power and relevance to the problem of evaluating self-healing mechanisms/systems, when combined with practical tools for fault-injection.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Adaptive internet interactive team video\n", "abstract": " The increasing popularity of online courses has highlighted the lack of collaborative tools for student groups. In addition, the introduction of lecture videos into the online curriculum has drawn attention to the disparity in the network resources used by students. We present an e-Learning architecture and adaptation model called AI2TV (Adaptive Internet Interactive Team Video), which allows virtual students, possibly some or all disadvantaged in network resources, to collaboratively view a video in synchrony. AI2TV upholds the invariant that each student will view semantically equivalent content at all times. Video player actions, like play, pause and stop, can be initiated by any student and their results are seen by all the other students. These features allow group members to review a lecture video in tandem, facilitating the learning process. Experimental trials show that AI2TV can successfully synchronize\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Dynamic Adaptation of Rules for Temporal Event Correlation in Distributed Systems\n", "abstract": " Event correlation is essential to realizing self-managing distributed systems. For example, distributed systems often require that events be correlated from multiple systems using temporal patterns to detect denial of service attacks and to warn of problems with business critical applications that run on multiple servers. This paper addresses how to specify timer values for temporal patterns so as to manage the trade-off between false alarms and undetected alarms. A central concern is addressing the variability of event propagation delays due to factors such as contention for network and server resources. To this end, we develop an architecture and an adaptive control algorithm that dynamically compensate for variations in propagation delays. Our approach makes Management Stations more autonomic by avoiding the need for manual adjustments of timer values in temporal rules. Further, studies we conducted of a testbed system suggest that our approach produces results that are at least as good as an optimal fixed setting of timer values.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Coping with complexity: A standards-based kinesthetic approach to monitoring non-standard component-based systems\n", "abstract": " Technology was developed to support runtime monitoring ie, continual validation regarding the dynamic functional and extra-functional properties of component-based systems. Software probes are inserted or wrapped into component ports and actualized connector middleware, to report system events that crossimpact component and connector boundaries. Required an prohibited properties are defined as potentially complex patterns over collections of events. The monitoring infrastructure includes recognizers to detect the occurrence or omission of these patters as the components and connectors in response to monitored activities.Descriptors:", "num_citations": "2\n", "authors": ["260"]}
{"title": "Interfacing Oz with the PCTE OMS: A case study of integrating a legacy System with a standard Object Management System\n", "abstract": " The integration of a legacy system and a standard Object Management System (OMS) is often a very challenging task. This paper details a case study, our experiment in interfacing Oz with the PCTE (Portable Common Tool Environment) Object Management System. Oz is a multi-user process-centered software development environment that has been under development in our lab since 1987, originally under the name Marvel. PCTE is a specification that defines a language-independent interface providing support mechanisms for software engineering environments (SEE). One of the premises of PCTE is that, in theory, an SEE such as Oz can be built (or extended) using the services provided by PCTE. The purpose of our experiment was to study how a legacy system such as Oz can be integrated into a new environment framework, e.g., PCTE. The architecture of the legacy system and the services of the\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Distributed hypermedia collaboration environments supporting legacy protocols\n", "abstract": " We present a model and architecture for open hypermedia systems that facilitate integration of distributed information resources from a wide variety of legacy repositories while continuing to exploit their legacy user interfaces and tools. Our approach is different from other OHS research in that the legacy repositories and clients continue to use their native protocols or interfaces rather than being adapted to some interface peculiar to the OHS or to a standard such as HTTP or OHP, and thus scales practically to large numbers of media types and/or legacy systems. Another contribution is the capability to add on generic collaborative work services, such as workflow and transactions, to all accesses to arbitrary forms of information\u2014both static data and dynamic computation engines\u2014producing what we call hypermedia collaboration environments. We describe our realization in Xanth and evaluate initial experience with the prototype.", "num_citations": "2\n", "authors": ["260"]}
{"title": "An extensible process server component and its integration into heterogeneous process-centered environments\n", "abstract": " We present a process server component with extensible syntax and semantics. New process enactment directives can be added to the syntax of the process modeling language, which we deem a\\process assembly language\" because it supports translation from higher-level process formalisms for enactment by the\\process virtual machine\", or process engine. The process engine is parameterized by callbacks to instance-speci c mediator code in order to implement any new directives and to modify the default enactment behavior. The mediator architecture also enables integration with a wide variety of existing environment architectures, including adding on process to an environment without such a facility, replacement of an existing process engine, and interoperability with an existing process system.", "num_citations": "2\n", "authors": ["260"]}
{"title": "CSCW and software process\n", "abstract": " The session started off with instant controversy when Simon Kaplan put up figure 1, a Venn diagram with three ovals: the large outer one labeled CSCW, the middle one labeled Workflow, and the small inner one labeled Process. Several attendees suggested that the diagram would be fine if the roles of CSCW and Process were reversed. Bob Balzer complained about Workflow encompassing Process. Naser Barghouti thought overlapping circles would be more appropriate. The main issue was the relationship between CSCW and Process, since Workflow was agreed to be only a subpart of Process-assuming our discussion of Process was not restricted to software development processes. For example, Manny Lehman clarified (a bit later) that the relevant business processes are part of any software process.Lee Osterweil admitted that the CSCW community, per se, is larger than the (software) Process community\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Non-Sharable Resource Freshness in Real-Time Scheduling\n", "abstract": " We propose, DONUT, a real-time producers/consumers model, as an abstract model for developing real-time scheduling and resource allocation techniques in the domain of real-time network management. Under this model, we consider not only the availability but also the freshness of the consumed resources (ie,\u201cdonuts\u201d). Given a resource type<, two quantitative measures are defined to evaluate the freshness of the consumed type< resources: worst case freshness (WCF<) and average case freshness (ACF<). In this paper, we address the freshness problem when all tasks are periodic and all resources are non-sharable/nonreusable. We show generally how to find an allocation scheme to optimize both WCF< and ACF< simultaneously. Finally, we apply our result to extend the priority ceiling protocol to work with general unbounded semaphores, where W ait and Signal operations need not be in pairs in one task.", "num_citations": "2\n", "authors": ["260"]}
{"title": "An architectural survey of object management systems\n", "abstract": " Much work has been done in the last decade in the related areas of object-oriented programming languages and object-oriented databases. Researchers from both areas now seem to be working toward a common end, that of an object management system, or OMS. An OMS is constructed similarly to an OODB but provides a general purpose concurrent object-oriented programming language as well, complementing the OODB query facilities. In this paper, we will define several different types of object systems (object servers, persistent OOPL\u2019s, OODB\u2019s and OMS\u2019s) in terms of their interfaces and capabilities from the viewpoint of how these support the requirements of cooperative information systems. We will examine the distinguishing features and general architecture of systems of each type in the light of a general model of OMS architecture.", "num_citations": "2\n", "authors": ["260"]}
{"title": "SETA1 working group on Ada libraries, configuration management, and version control\n", "abstract": " There are two kinds of versions. The first kind, called revisions (or vertical versions or sequential versions) record changes made to the components of a system over time to fix bugs or to enhance the system. The changes made to a component over time forms a tree structure of revisions, where branches are formed when multiple programmers need to make changes to the same component simultaneously, in order to control the impact of changes. Branches of a revision tree are intended to ultimately be merged together (eg, using a diff tool or Horwitz\u2019s and Reps\u2019s merging algorithms).The second kind of versions, called variants (or horizontal versions or parallel versions) are not meant to be merged. There are a variety of reasons for variants: software for different platforms, implementations that emphasize the various time and space trade-offs, software that provide various features that can be selected or rejected as\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "MELDing transactions and objects\n", "abstract": " MELD is an experimental object system under development at Columbia University. The object-oriented programming language supports classes, strong typing of instance variables, active values, multiple inheritance, and separate compilation of modular units called features that bundle together related classes and objects. These facilities were developed for an early version of MELD [3, 4], without persistence, concurrency or distribution.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Extended Transaction Models\n", "abstract": " The classical transaction model is based on failure atomicity for crash recovery and serializability for concurrency control, but these properties are too weak for some applications and too strong for othcrs. Thus, we use the term \u2018transaction\u201d loosely to encompass facilities that support some subset of fault tolerance, controlled concurrent access to data, commitment of a consistent set of changes, user control over commit and abort, and nested activities.Extended transaction models might be based on the semantics of abstract data types, objects, tools, etc., might incorporate transient and/or persistent versions, migl? involve locking, validation or other kinds of protocols, might be centralized or decentralized, might support short or long-duration activities, might apply at varying granularities, might be closely coupled with scheduling policies and performance concerns, or might mix and match several fault tolerance or\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "Transactions for concurrent object-oriented programming systems\n", "abstract": " Concurrent object-oriented programming systems (COOPS) require support for fault tolerance, concurrency control, consistent commitment of changes and program-initiated rollback. It is sometimes suggested that the classical transaction processing model successfully applied in databases and operating systems be integrated directly into COOPS facilities. This is clearly desirable, but by itself is too limiting. COOPS applications require several granularities of transaction-like facilities. A number of transaction-like mechanisms were addressed at the workshop, and no doubt there are many others suitable for COOPS. Here I briefly survey four levels of granularity that were discussed at the workshop.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Concurrent meld\n", "abstract": " Our original goal was to design a programming language for writing software engineering environments. The most important requirements were reusability and the ability to integrate separately developed tools [1]. Our scope was later expanded to general applications, and then to parallel and distributed systems. Our current focus is on 'growing' distributed software environments and tools, that is, building a core environment or tool assuming a long-term evolution path. MELD is a multiparadigm language that combines object-oriented, macro dataflow, transaction processing and module interconnection styles of programming [2]. The most unusual aspect is the dataflow at the source level among the inputs and outputs of statements. Classes may define constraints in addition to instance variables and methods, which are triggered by changes to instance variables and interleaved along with the statements of a\u00a0\u2026", "num_citations": "2\n", "authors": ["260"]}
{"title": "An expert system for software design and development\n", "abstract": " This paper is about MARVEL, an expert system for software design and development. Several previous papers presented the architecture and implementation of MARVEL (Kaiser 87a, Feiler 87, Barghouti 88a, Kaiser 88a, Kaiser 88b). The purpose of this paper is two-fold: to describe what MARVEL is useful for and to explain how to use the system to create a knowledge-based software development environment. Software development environments (SDEs) try to solve the problems associated with the development of large-scale software projects. There are two classes of problems that SDEs address: product-related problems and process-related problems. Product-related problems result from the size of the product being developed, such as how to organize thousands of components in a logical hierarchy and how to present programmers with the latest versions of the components that they need to modify. Process-related problems have to do with the number of people working on the development of a project. Often, a large-scale project involves many program-mers, managers, and technical and administrative staff. An SDE provides facilities to manage the communication of project personnel and to guarantee that the project team can work on the project simultaneously without getting in each", "num_citations": "2\n", "authors": ["260"]}
{"title": "Version Inconsistency in Large Systems\n", "abstract": " Every programmer, sooner or later, will spend hours, days, or even weeks searching for a bug that turns out to be caused by failing to recompile a module after a change; hence the popularity of tools like make [2], which help the programmer restore a system to a consistent state after changes to one or more source modules. However, for large systems, the cost of restoring consistency after a change can be quite large and sometimes prohibitive. We claim that inconsistency in large software systems is common during development, and that therefore we need methods to manage inconsistency, rather than avoid it.", "num_citations": "2\n", "authors": ["260"]}
{"title": "Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles\n", "abstract": " Self-driving cars and trucks, autonomous vehicles (AVs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability -- which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of AV controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving AVs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used AV simulators' API grammars. to generate semantically and temporally valid complex driving scenarios (sequences of scenes). AutoFuzz is guided by a constrained Neural Network (NN) evolutionary search over the API grammar to generate scenarios seeking to find unique traffic violations. Evaluation of our prototype on one state-of-the-art learning-based controller and two rule-based controllers shows that AutoFuzz efficiently finds hundreds of realistic traffic violations resembling real-world crashes. Further, fine-tuning the learning-based controller with the traffic violations found by AutoFuzz successfully reduced the traffic violations found in the new version of the AV controller software.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Metamorphic Detection of Repackaged Malware\n", "abstract": " Machine learning-based malware detection systems are often vulnerable to evasion attacks, in which a malware developer manipulates their malicious software such that it is misclassified as benign. Such software hides some properties of the real class or adopts some properties of a different class by applying small perturbations. A special case of evasive malware hides by repackaging a bonafide benign mobile app to contain malware in addition to the original functionality of the app, thus retaining most of the benign properties of the original app. We present a novel malware detection system based on metamorphic testing principles that can detect such benign-seeming malware apps. We apply metamorphic testing to the feature representation of the mobile app rather than to the app itself. That is, the source input is the original feature vector for the app and the derived input is that vector with selected features removed. If the app was originally classified benign and is indeed benign, the output for the source and derived inputs should be the same class, i.e., benign, but if they differ, then the app is exposed as likely malware. Malware apps originally classified as malware should retain that classification since only features prevalent in benign apps are removed. This approach enables the machine learning model to classify repackaged malware with reasonably few false negatives and false positives. Our training pipeline is simpler than many existing ML-based malware detection methods, as the network is trained end-to-end to learn appropriate features and perform classification. We pre-trained our classifier model on 3 million apps collected\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Binary quilting to generate patched executables without compilation\n", "abstract": " When applying patches, or dealing with legacy software, users are often reluctant to change the production executables for fear of unwanted side effects. This results in many active systems running vulnerable or buggy code even though the problems have already been identified and resolved by developers. Furthermore when dealing with old or proprietary software, users can't view or compile source code so any attempts to change the application after distribution requires binary level manipulation. We present a new technique we call binary quilting that allows users to apply the designated minimum patch that preserves core semantics without fear of unwanted side effects introduced either by the build process or by additional code changes. Unlike hot patching, binary quilting is a one-time procedure that creates an entirely new reusable binary. Our case studies show the efficacy of this technique on real software\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Ad hoc Test Generation Through Binary Rewriting\n", "abstract": " When a security vulnerability or other critical bug is not detected by the developers\u2019 test suite, and is discovered post-deployment, developers must quickly devise a new test that reproduces the buggy behavior. Then the developers need to test whether their candidate patch indeed fixes the bug, without breaking other functionality, while racing to deploy before attackers pounce on exposed user installations. This can be challenging when factors in a specific user environment triggered the bug. If enabled, however, record-replay technology faithfully replays the execution in the developer environment as if the program were executing in that user environment under the same conditions as the bug manifested. This includes intermediate program states dependent on system calls, memory layout, etc. as well as any externally-visible behavior. Many modern record-replay tools integrate interactive debuggers, to help\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Obfuscation resilient search through executable classification\n", "abstract": " Android applications are usually obfuscated before release, making it difficult to analyze them for malware presence or intellectual property violations. Obfuscators might hide the true intent of code by renaming variables and/or modifying program structures. It is challenging to search for executables relevant to an obfuscated application for developers to analyze efficiently. Prior approaches toward obfuscation resilient search have relied on certain structural parts of apps remaining as landmarks, un-touched by obfuscation. For instance, some prior approaches have assumed that the structural relationships between identifiers are not broken by obfuscators; others have assumed that control flow graphs maintain their structures. Both approaches can be easily defeated by a motivated obfuscator. We present a new approach, MACNETO, to search for programs relevant to obfuscated executables leveraging deep\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Accelerating Maven by delaying test dependencies\n", "abstract": " Modular build systems (such as Maven) may simplify build maintenance, but significantly reduce opportunities for parallelism where they may be most helpful: when running tests. If tests are contained in each module, and modules contain dependencies on each other, their tests can not run in parallel between modules that have build dependencies. This poster will present a technique for achieving significantly greater parallelism in running the tests of Maven-built Java projects, cutting build times in half in our case study.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Effectiveness of teaching metamorphic testing, part II\n", "abstract": " We study the ability of students in a senior/graduate software engineering course to understand and apply metamorphic testing, a relatively recently invented advance in software testing research that complements conventional approaches such as equivalence partitioning and boundary analysis. We previously reported our investigation of the fall 2011 offering of the Columbia University course COMS W4156 Advanced Software Engineering [6], and here report on the fall 2012 offering and contrast it to the previous year. Our main findings are: 1) Although the students in the second offering did not do very well on the newly added individual assignment specifically focused on metamorphic testing, thereafter they were better able to find metamorphic properties for their team projects than the students from the previous year who did not have that preliminary homework and, perhaps most significantly, did not have the solution set for that homework. 2) Students in the second offering did reasonably well using the relatively novel metamorphic testing technique vs. traditional black box testing techniques in their projects (such comparison data is not available for the first offering). 3) Finally, in both semesters, the majority of the student teams were able to apply metamorphic testing to their team projects after only minimal instruction, which would imply that metamorphic testing is a viable strategy for student testers.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Money for Nothing and Privacy for Free?\n", "abstract": " Privacy in the context of ubiquitous social computing systems has become a major concern for the society at large. As the number of online social computing systems that collect user data grows, this privacy threat is further exacerbated. There has been some work (both, recent and older) on addressing these privacy concerns. These approaches typically require extra computational resources, which might be beneficial where privacy is concerned, but when dealing with Green Computing and sustainability, this is not a great option. Spending more computation time results in spending more energy and more resources that make the software system less sustainable. Ideally, what we would like are techniques for designing software systems that address these privacy concerns but which are also sustainable-systems where privacy could be achieved \u201cfor free,\u201d ie, without having to spend extra computational effort. In this paper, we describe how privacy can be achieved for free-an accidental and beneficial side effect of doing some existing computation-and what types of privacy threats it can mitigate. More precisely, we describe a \u201cPrivacy for Free\u201d design pattern and show its feasibility, sustainability, and utility in building complex social computing systems.", "num_citations": "1\n", "authors": ["260"]}
{"title": "A Large-Scale, Longitudinal Study of Player Achievements in World of Warcraft\n", "abstract": " We present a survey of usage of the popular Massively Multiplayer Online Role Playing Game, World of Warcraft. By mining publicly available data, we collected a dataset consisting of the player history for approximately six million characters, with partial data for another six million characters. This paper focuses on player achievement data in particular, exposing trends in play from this highly successful game. From this data, we present several findings on players\u2019 play styles. We correlate achievements with motivations based upon a previously-defined motivation model, and then classify players based on the categories of achievements that they pursued. Experiments show players who fall within each of these buckets can play differently, and that as players progress through game content, their play style evolves as well.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Comparing Speed of Provider Data Entry: Electronic Versus Paper Methods\n", "abstract": " Electronic health record (EHR) systems have significant potential advantages over traditional paper-based systems, but they require that providers assume responsibility for data entry. One significant barrier to adoption of EHRs is the perception of slowed data-entry by providers. This study compares the speed of data-entry using computer-based templates vs. paper for a large eye clinic, using 10 subjects and 10 simulated clinical scenarios. Dataentry into the EHR was significantly slower (p< 0.01) than traditional paper forms.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Baseline: Metrics for setting a baseline for web vulnerability scanners\n", "abstract": " As web scanners are becoming more popular because they are faster and cheaper than security consultants, the trend of relying on these scanners also brings a great hazard: users can choose a weak or outdated scanner and trust incomplete results. Therefore, benchmarks are created to both evaluate and compare the scanners. Unfortunately, most existing benchmarks suffer from various drawbacks, often by testing against inappropriate criteria that does not reflect the user\u2019s needs. To deal with this problem, we present an approach called Baseline that coaches the user in picking the minimal set of weaknesses (ie, a baseline) that a qualified scanner should be able to detect and also helps the user evaluate the effectiveness and efficiency of the scanner in detecting those chosen weaknesses. Baseline\u2019s goal is not to serve as a generic ranking system for web vulnerability scanners, but instead to help users choose the most appropriate scanner for their specific needs.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Rapid parallelization by collaboration\n", "abstract": " The widespread adoption of Chip Multiprocessors has renewed the emphasis on the use of parallelism to improve performance. The present and growing diversity in hardware architectures and software environments, however, continues to pose difficulties in the effective use of parallelism thus delaying a quick and smooth transition to the concurrency era.In this document, we describe the research being conducted at Columbia University on a system called COMPASS that aims to simplify this transition by providing advice to programmers considering parallelizing their code. The advice proffered to the programmer is based on the wisdom collected from programmers who have already parallelized some code. The utility of COMPASS rests, not only on its ability to collect the wisdom unintrusively but also on its ablility to automatically seek, find and synthesize this wisdom into advice that is tailored to the code the user is considering parallelizing and to the environment in which the optimized program will execute in. COMPASS provides a platform and an extensible framework for sharing human expertise about code parallelization\u2013widely, and on diverse hardware and software. By leveraging the \u201cwisdom of crowds\u201d model [78] which has been conjunctured to scale exponentially and which has successfully worked for wikis, COMPASS aims to enable rapid parallelization of code and thus continue to extend the benefits of Moore\u2019s law scaling to science and society.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Rust: The reusable security toolkit\n", "abstract": " We describe the design of a reusable toolkit for testing antiphishing technologies. It is based on web bugs and a set of small, simple tools. We show that its existence would have simplified the design of other studies in the field.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Deux: Autonomic testing system for operating system upgrades\n", "abstract": " Operating system upgrades and patches sometimes break applications that worked fine on the older version. We present an autonomic approach to testing of OS updates while minimizing downtime, usable without local regression suites or IT expertise. Deux utilizes a dual-layer virtual machine architecture, with lightweight application process checkpoint and resume across OS versions, enabling simultaneous execution of the same applications on both OS versions in different VMs. Inputs provided by ordinary users to the production old version are also fed to the new version. The old OS acts as a pseudo-oracle for the update, and application state is automatically re-cloned to continue testing after any output discrepancies (intercepted at system call level)-all transparently to users. If all differences are deemed inconsequential, then the VM roles are switched with the application state already in place. Our empirical evaluation with both LAMP and standalone applications demonstrates Deux\u2019s efficiency and effectiveness.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Survivor: An Approach for Adding Dependability to Legacy Workflow Systems\n", "abstract": " Although they often provide critical services, most workflow systems are not dependable. There has been much research on dependablesurvivable distributed systems most is concerned with developing new architectures, not adapting pre-existing ones. Additionally, the research has focused on hardening, security-based defense, as opposed to recovery. For deployed systems, it is often infeasible to completely replace existing infrastructure what is needed are ways to adapt existing distributed systems to offer better dependability. In this paper, we outline a general architecture that can easily be retrofitted to legacy workflow systems in order to improve dependability and fault tolerance. We do this by monitoring enactment and replicating partial workflow states as tools for detection, analysis and recovery. We discuss some policies that can guide these mechanisms. Finally, we describe and evaluate our implementation, Survivor, which modified an existing workflow system provided by the Naval Research Lab.Descriptors:", "num_citations": "1\n", "authors": ["260"]}
{"title": "Optimizing Quality for Collaborative Video Viewing\n", "abstract": " The increasing popularity of distance learning and online courses has highlighted the lack of collaborative tools for student groups. In addition, the introduction of lecture videos into the online curriculum has drawn attention to the disparity in the network resources used by the students. We present an architecture and adaptation model called AI2TV (Adaptive Internet Interactive Team Video), a system that allows geographically dispersed participants, possibly some or all disadvantaged in network resources, to collaboratively view a video in synchrony. AI2TV upholds the invariant that each participant will view semantically equivalent content at all times. Video player actions, like play, pause and stop, can be initiated by any of the participants and the results of those actions are seen by all the members. These features allow group members to review a lecture video in tandem to facilitate the learning process. We employ an autonomic (feedback loop) controller that monitors clients\u2019 video status and adjusts the quality of the video according to the resources of each client. We show in experimental trials that our system can successfully synchronize video for distributed clients while, at the same time, optimizing the video quality given actual (fluctuating) bandwidth by adaptively adjusting the quality level for each participant.", "num_citations": "1\n", "authors": ["260"]}
{"title": "On the yellow brick road to component-based product lines\n", "abstract": " We present our experience using process-centered environments to build, maintain, reengineer and develop sets of related products. A process-centered environment is itself a collection of applications, such as clients and servers, that work together to provide a set of services. We discuss the OzMarvel and EmeraldCity environments that we designed to develop the Oz process-centered environment. Oz's support for process interoperability played a key role in constructing a component-based product line process.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Integrating a Rule-Based Process Server with an Existing Environment\n", "abstract": " The widespread interest in software process has led to the creation of a wide range of process-centered environments, each seemingly with a different process formalism. This eventuality presents us with two questions that we must address in order to build a flexible process component for incorporation into existing software environments. Such a component would be desirable, for example, to facilitate adding a notion of process to a development organization\u2019s legacy environment. One of these questions is this question of competing formalisms; the other is the ease with which the component can be integrated with existing systems.This paper describes an environment integration experiment directed toward these questions. In this experiment, we replaced the process interpretation components of another process-based environment with our Amber rule-based process server, translating also from that environment\u2019s process formalism into our \u2018\u2018process assembly language\u2019\u2019.", "num_citations": "1\n", "authors": ["260"]}
{"title": "The design and implementation of late binding in a distributed programming language\n", "abstract": " In distributed application domains where data change rapidly, it is often desirable for programs to obtain the latest available data values to achieve accurate computations. Example applications are financial services and network management. Such data are logically shared by a network of programs. Unlike data in traditional databases, rapidly changing data are usually not lockable by (client) programs and it is crucial to the computations to access their values in a timely manner. In these application domains, a typical program usually performs computations based on recently available data values obtained from the network. However, these data values may be inconsistent or obsolete, since the real data are external to the system and may change more rapidly than can be reflected by their copies within the system. Decision making based on such inaccurate computations can lead to substantial penalties. In this\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Sharable versus Non-Sharable Real-Time Resources\n", "abstract": " In [WK94], we define a real-time producers/consumers problem where all tasks are periodic and all resources are non-sharable/non-reusable. In this paper, we study the same problem but with sharable resources. We define a quantitative measure, resource freshness ratio (RFR), to represent the effect of sharable versus non-sharable resources on improving the resource freshness when tasks start simultaneously. We consider the RFR values for both average case freshness (ACF) and worst case freshness (WCF), ie, RFR< ACF and RFR< ACF respectively. The following results are obtained:1. 8 resource type<, 0< RFR< ACF 1; 0< RFR< WCF 1.2. Given a set of consumers and a resource type<, then the RFR< ACF and RFR< WCF values when we have only one fast<-producer to serve all the<-consumers are smaller than the values when we have multiple<-producers and all these<-producers have shorter cycle times than any of the<-consumers.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Consistency and automation in multi-user rule-based development environments\n", "abstract": " We investigate the scaling up of a class of single-user software development environments, which we call rule-based development environments (RBDEs), to support multiple developers cooperating together on a project. RBDEs model the software development process in terms of rules that encapsulate activities, and execute forward and backward chaining on the rules to provide assistance in carrying out the development process. There is a spectrum of assistance models, ranging from pure automation to strict consistency preservation. We describe three problems whose solutions are dependent on the choice of assistance model:(1) multiple views;(2) evolution; and (3) concurrency control. We discuss how the two extremes of the spectrum restrict the possible approaches to multiple views and evolution. In order to explore different aspects of the concurrency control problem across multiple points on the spectrum of RBDEs, we develop a maximalist assistance model and propose an approach to synchronization of cooperating developers within the context of this model.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Preliminary Design of an Object Management System for Multi-User MARVEL\n", "abstract": " We have been working on the MARVEL process-oriented environment architecture for several years [4, 5, 6], and are now distributing version 2.6 externally. But MARVEL 2.6 is a single-user system, and its object management system will not scale up to permit multi-user environments. We are working on several problems that must be solved before it will be possible to construct a robust multi-user MARVEL kernel.The current implementation of MARVEL consists of five components:(1) an object-oriented nile-based language, called h4ARVEL Strategy L. anguage (MSL), for data and process model definition;(2) a pair of end-user interfaces, one graphical and the other command-line oriented;(3) a loader, for dynamically loading and unloading MSL modules to change the view of the database andfor the behavior of the environment, to reflect different phases of the software process;(4) an engine for forward and\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Experiments with rule based process modelling in an SDE\n", "abstract": " Even though software development environments (SDEs) have become the focus of a considerable amount of research, there are few SDEs available that really satisfy the needs of today's software developers. We believe these needs include facilities that support arbitrary, existing tools and processes, both within and outside the SDE, and facilities that provide useful automated assistance to software developers. Additionally, there seems to be general agreement that facilities that support the evolution both of the system being developed and the software process itself are necessary. Our research towards the support of these facilities is on an SDE called Marvel [KFP88]~ named after the famous Professor Marvel, the man behind the curtain in\" The Wizard of Oz\". Many SDEs today suffer from an initial limitation of trying to provide support for a particular software development task, such as implementing or\u00a0\u2026", "num_citations": "1\n", "authors": ["260"]}
{"title": "Object-Oriented Programming Language Facilities for Concurrency Control\n", "abstract": " Concurrent object-oriented programming systems require support for concurrency control, to enforce consistent commitment of changes and to support program-initiated rollback after application-specific failures. We have explored three different concurrency control modelsatomic blocks, serializable transactions, and commit-serializable transactions-as part of the MELD programming language. We present our designs, discuss certain programming problems and implementation issues, and compare our work on MELD to other concurrent object-based systems.", "num_citations": "1\n", "authors": ["260"]}
{"title": "Specification of Interpreters and Debuggers Using an Extension of Attribute Grammars\n", "abstract": " Introduction 1.1. Two New Paradigms 2. Compile-Time Semantics 2.1. Comparison with Reps 2.2. Comparison with Johnson", "num_citations": "1\n", "authors": ["260"]}