{"title": "Availability in globally distributed storage systems\n", "abstract": " Highly available cloud storage is often implemented with complex, multi-tiered distributed systems built on top of clusters of commodity servers and disk drives. Sophisticated management, load balancing and recovery techniques are needed to achieve high performance and availability amidst an abundance of failure sources that include software, hardware, network connectivity, and power issues. While there is a relative wealth of failure studies of individual components of storage systems, such as disk drives, relatively little has been reported so far on the overall availability behavior of large cloudbased storage services.We characterize the availability properties of cloud storage systems based on an extensive one year study of Google\u2019s main storage infrastructure and present statistical models that enable further insight into the impact of multiple design choices, such as data placement and replication strategies. With these models we compare data availability under a variety of system parameters given the real patterns of failures observed in our fleet.", "num_citations": "698\n", "authors": ["862"]}
{"title": "Janus: Optimal flash provisioning for cloud storage workloads\n", "abstract": " Janus is a system for partitioning the flash storage tier between workloads in a cloud-scale distributed file system with two tiers, flash storage and disk. The file system stores newly created files in the flash tier and moves them to the disk tier using either a First-In-First-Out (FIFO) policy or a Least-Recently-Used (LRU) policy, subject to per-workload allocations. Janus constructs compact metrics of the cacheability of the different workloads, using sampled distributed traces because of the large scale of the system. From these metrics, we formulate and solve an optimization problem to determine the flash allocation to workloads that maximizes the total reads sent to the flash tier, subject to operator-set priorities and bounds on flash write rates. Using measurements from production workloads in multiple data centers using these recommendations, as well as traces of other production workloads, we show that the resulting allocation improves the flash hit rate by 47\u201376% compared to a unified tier shared by all workloads. Based on these results and an analysis of several thousand production workloads, we conclude that flash storage is a cost-effective complement to disks in data centers.", "num_citations": "85\n", "authors": ["862"]}
{"title": "Using a market economy to provision compute resources across planet-wide clusters\n", "abstract": " We present a practical, market-based solution to the resource provisioning problem in a set of heterogeneous resource clusters. We focus on provisioning rather than immediate scheduling decisions to allow users to change long-term job specifications based on market feedback. Users enter bids to purchase quotas, or bundles of resources for long-term use. These requests are mapped into a simulated clock auction which determines uniform, fair resource prices that balance supply and demand. The reserve prices for resources sold by the operator in this auction are set based on current utilization, thus guiding the users as they set their bids towards under-utilized resources. By running these auctions at regular time intervals, prices fluctuate like those in a real-world economy and provide motivation for users to engineer systems that can best take advantage of available resources. These ideas were implemented\u00a0\u2026", "num_citations": "74\n", "authors": ["862"]}
{"title": "Projecting disk usage based on historical trends in a cloud environment\n", "abstract": " Provisioning scarce resources among competing users and jobs remains one of the primary challenges of operating large-scale, distributed computing environments. Distributed storage systems, in particular, typically rely on hard operator-set quotas to control disk allocation and enforce isolation for space and I/O bandwidth among disparate users. However, users and operators are very poor at predicting future requirements and, as a result, tend to over-provision grossly.", "num_citations": "37\n", "authors": ["862"]}
{"title": "Take me to your leader! online optimization of distributed storage configurations\n", "abstract": " The configuration of a distributed storage system typically includes, among other parameters, the set of servers and their roles in the replication protocol. Although mechanisms for changing the configuration at runtime exist, it is usually left to system administrators to manually determine the \u201cbest\u201d configuration and periodically reconfigure the system, often by trial and error. This paper describes a new workload-driven optimization framework that dynamically determines the optimal configuration at runtime. We focus on optimizing leader and quorum based replication schemes and divide the framework into three optimization tiers, dynamically optimizing different configuration aspects: 1) leader placement, 2) roles of different servers in the replication protocol, and 3) replica locations. We showcase our optimization framework by applying it to a large-scale distributed storage system used internally in Google and demonstrate that most client applications significantly benefit from using our framework, reducing average operation latency by up to 94%.", "num_citations": "33\n", "authors": ["862"]}
{"title": "Optimizing allocation of flash memory to file groups\n", "abstract": " Systems and methods are discussed relating to allocation of memory from a fixed pool of fast memory within a data center having a data storage area equipped with that memory. Techniques include: receiving a request to write data in the storage area; identifying a file group associated with the write request; analyzing previous data activity traces associated with the file group; determining an available fast memory amount based on the total amount of fast memory in the fixed pool and a currently allocated amount of fast memory; determining a fast memory allocation for the file group based on the previous data activity traces, the available fast memory, and a fast memory constraint, the memory allocation including an allocation amount and a write probability; and providing information about the memory allocation to a file system of the data center, which writes the data based on the allocation amount and write\u00a0\u2026", "num_citations": "20\n", "authors": ["862"]}
{"title": "Large-scale parallel statistical forecasting computations in R\n", "abstract": " We demonstrate the utility of massively parallel computational infrastructure for statistical computing using the MapReduce paradigm for R. This framework allows users to write computations in a high-level language that are then broken up and distributed to worker tasks in Google datacenters. Results are collected in a scalable, distributed data store and returned to the interactive user session. We apply our approach to a forecasting application that f its a variety of models, prohibiting an analytical description of the statistical uncertainty associated with the overall forecast. To overcome this, we generate simulation-based uncertainty bands, which necessitates a large number of computationally intensive realizations. Our technique cut total run time by a factor of 300. Distributing the computation across many machines permits analysts to focus on statistical issues while answering questions that would be intractable without signi ficant parallel computational infrastructure. We present real-world performance characteristics from our application to allow practitioners to better understand the nature of massively parallel statistical simulations in R.", "num_citations": "18\n", "authors": ["862"]}
{"title": "FreeBSD Handbook: The FreeBSD Documentation Project\n", "abstract": " Redistribution and use in source (SGML DocBook) and'compiled'forms (SGML, HTML, PDF, PostScript, RTF and so forth) with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code (SGML DocBook) must retain the above copyright notice, this list of conditions and the following disclaimer as the first lines of this file unmodified.", "num_citations": "16\n", "authors": ["862"]}
{"title": "RProtoBuf: Efficient cross-language data serialization in R\n", "abstract": " Modern data collection and analysis pipelines often involve a sophisticated mix of applications written in general purpose and specialized programming languages. Many formats commonly used to import and export data between different programs or systems, such as CSV or JSON, are verbose, inefficient, not type-safe, or tied to a specific programming language. Protocol Buffers are a popular method of serializing structured data between applications - while remaining independent of programming languages or operating systems. They offer a unique combination of features, performance, and maturity that seems particularly well suited for data-driven applications and numerical computing. The RProtoBuf package provides a complete interface to Protocol Buffers from the R environment for statistical computing. This paper outlines the general class of data serialization requirements for statistical computing, describes the implementation of the RProtoBuf package, and illustrates its use with example applications in large-scale data collection pipelines and web services.", "num_citations": "11\n", "authors": ["862"]}
{"title": "Programmatically choosing preferred storage parameters for files in large-scale distributed storage systems\n", "abstract": " Methods to determine and automatically recommend or adjust configuration parameters for storing files in large-scale distributed storage systems are disclosed. These methods may receive file metadata and trace data that allows the system to identify file access patterns. Additionally, the methods may receive information about distributed storage systems in a datacenter. This information can be used to choose storage parameters on a per-file basis for storing files.", "num_citations": "10\n", "authors": ["862"]}
{"title": "FreeBSD Release Engineering\n", "abstract": " The development of FreeBSD is a very open process. FreeBSD is comprised of contributions from thousands of people around the world. The FreeBSD Project provides anonymous CVS [1] access to the general public so that", "num_citations": "8\n", "authors": ["862"]}
{"title": "Automatic reconfiguration of distributed storage\n", "abstract": " The configuration of a distributed storage system with multiple data replicas typically includes the set of servers and their roles in the replication protocol. The configuration can usually be changed manually, but in most cases, system administrators have to determine a good configuration by trial and error. We describe a new workload-driven optimization framework that dynamically determines the optimal configuration at run time. Applying the framework to a large-scale distributed storage system used internally in Google resulted in halving the operation latency in 17% of the tested databases, and reducing it by more than 90% in some cases.", "num_citations": "6\n", "authors": ["862"]}
{"title": "HistogramTools for Distributions of Large Data Sets\n", "abstract": " Histograms are a common graphical representation of the distribution of a data set. They are particularly useful for collecting very large data sets into a binned form for easier data storage and analysis. The HistogramTools R package augments the built-in support for histograms with a number of methods that are useful for analyzing large data sets. Specifically, methods are included for serializing histograms into a compact Protocol Buffer representation for sharing between distributed tasks, functions for manipulating the resulting aggregate histograms, and functions for measuring and visualizing the information loss associated with histogram representations of a data set.", "num_citations": "3\n", "authors": ["862"]}
{"title": "Package \u2018HistogramTools\u2019\n", "abstract": " Description This package provides a number of utility functions useful for manipulating large histograms. This includes methods to trim, subset, merge buckets, merge histograms, convert to CDF, and calculate information loss due to binning. It also provides a protocol buffer representations of the default R histogram class to allow histograms over large data sets to be computed and manipulated in a MapReduce environment.", "num_citations": "2\n", "authors": ["862"]}
{"title": "Uncertainty in aggregate estimates from sampled distributed traces\n", "abstract": " Tracing mechanisms in distributed systems give important insight into system properties and are usually sampled to control overhead. At Google, Dapper [8] is the always-on system for distributed tracing and performance analysis, and it samples fractions of all RPC traffic. Due to difficult implementation, excessive data volume, or a lack of perfect foresight, there are times when system quantities of interest have not been measured directly, and Dapper samples can be aggregated to estimate those quantities in the short or long term. Here we find unbiased variance estimates of linear statistics over RPCs, taking into account all layers of sampling that occur in Dapper, and allowing us to quantify the sampling uncertainty in the aggregate estimates. We apply this methodology to the problem of assigning jobs and data to Google datacenters, using estimates of the resulting cross-datacenter traffic as an optimization criterion, and also to the detection of change points in access patterns to certain data partitions.", "num_citations": "2\n", "authors": ["862"]}
{"title": "Package \u2018digest\u2019\n", "abstract": " Description Implementation of a function'digest ()'for the creation of hash digests of arbitrary R objects (using the'md5','sha-1','sha-256','crc32','xxhash','murmurhash','spookyhash'and'blake3'algorithms) permitting easy comparison of R language objects, as well as functions such as' hmac ()'to create hash-based message authentication code. Please note that this package is not meant to be deployed for cryptographic purposes for which more comprehensive (and widely tested) libraries such as' OpenSSL'should be used.", "num_citations": "1\n", "authors": ["862"]}
{"title": "Storage provisioning and allocation in a large cloud environment\n", "abstract": " Provisioning scarce resources among competing users and jobs remains one of the primary challenges of operating large-scale, distributed computing environments. Distributed storage systems, in particular, typically rely on hard operator-set quotas to control disk allocation and enforce isolation for space and I/O bandwidth among disparate users.", "num_citations": "1\n", "authors": ["862"]}