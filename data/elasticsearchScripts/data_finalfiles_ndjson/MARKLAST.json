{"title": "Graph-based keyword extraction for single-document summarization\n", "abstract": " In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summarization of text documents. Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features. In the supervised approach, we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model. In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords. Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classification provides the highest", "num_citations": "359\n", "authors": ["327"]}
{"title": "Knowledge discovery in time series databases\n", "abstract": " Adding the dimension of time to databases produces time series databases (TSDB) and introduces new aspects and difficulties to data mining and knowledge discovery. In this correspondence, we introduce a general methodology for knowledge discovery in TSDB. The process of knowledge discovery in TSDR includes cleaning and filtering of time series data, identifying the most important predicting attributes, and extracting a set of association rules that can be used to predict the time series behavior in the future. Our method is based on signal processing techniques and the information-theoretic fuzzy approach to knowledge discovery. The computational theory of perception (CTP) is used to reduce the set of extracted rules by fuzzification and aggregation. We demonstrate our approach on two types of time series: stock-market data and weather data.", "num_citations": "291\n", "authors": ["327"]}
{"title": "Data mining in time series databases\n", "abstract": " Adding the time dimension to real-world databases produces Time Series Databases (TSDB) and introduces new aspects and difficulties to data mining and knowledge discovery. This book covers the state-of-the-art methodology for mining time series databases. The novel data mining methods presented in the book include techniques for efficient segmentation, indexing, and classification of noisy and dynamic time series. A graph-based method for anomaly detection in time series is described and the book also studies the implications of a novel and potentially useful representation of time series as strings. The problem of detecting changes in data mining models that are induced from temporal databases is additionally discussed. Contents: A Survey of Recent Methods for Efficient Retrieval of Similar Time Sequences (HM Lie); Indexing of Compressed Time Series (E Fink & K Pratt); Boosting Interval-Based Literal: Variable Length and Early Classification (JJ Rodriguez Diez); Segmenting Time Series: A Survey and Novel Approach (E Keogh et al.); Indexing Similar Time Series under Conditions of Noise (M Vlachos et al.); Classification of Events in Time Series of Graphs (H Bunke & M Kraetzl); Median Strings--A Review (X Jiang et al.); Change Detection in Classfication Models of Data Mining (G Zeira et al.). Readership: Graduate students, reseachers and practitioners in the fields of data mining, machine learning, databases and statistics.", "num_citations": "254\n", "authors": ["327"]}
{"title": "Online classification of nonstationary data streams\n", "abstract": " Most classification methods are based on the assumption that the data conforms to a stationary distribution. However, the real-world data is usually collected over certain periods of time, ranging from seconds to years, and ignoring possible changes in the underlying concept, also known as concept drift, may degrade the predictive performance of a classification model. Moreover, the computation time, the amount of required memory, and the model complexity may grow indefinitely with the continuous arrival of new training instances. This paper describes and evaluates OLIN, an online classification system, which dynamically adjusts the size of the training window and the number of new examples between model re-constructions to the current rate of concept drift. By using a fixed amount of computer resources, OLIN produces models, which have nearly the same accuracy as the ones that would be produced by\u00a0\u2026", "num_citations": "238\n", "authors": ["327"]}
{"title": "Graph-Theoretic Techniques for Web Content Mining\n", "abstract": " This book describes exciting new opportunities for utilizing robust graph representations of data with common machine learning algorithms. Graphs can model additional information which is often not present in commonly used data representations, such as vectors. Through the use of graph distance\u2014a relatively new approach for determining graph similarity\u2014the authors show how well-known algorithms, such as k-means clustering and k-nearest neighbors classification, can be easily extended to work with graphs instead of vectors. This allows for the utilization of additional information found in graph representations, while at the same time employing well-known, proven algorithms. To demonstrate and investigate these novel techniques, the authors have selected the domain of web content mining, which involves the clustering and classification of web documents based on their textual substance. Several methods of representing web document content by graphs are introduced; an interesting feature of these representations is that they allow for a polynomial time distance computation, something which is typically an NP-complete problem when using graphs. Experimental results are reported for both clustering and classification in three web document collections using a variety of graph representations, distance measures, and algorithm parameters. In addition, this book describes several other related topics, many of which provide excellent starting points for researchers and students interested in exploring this new area of machine learning further. These topics include creating graph-based multiple classifier ensembles through random node\u00a0\u2026", "num_citations": "216\n", "authors": ["327"]}
{"title": "Information-theoretic algorithm for feature selection\n", "abstract": " Feature selection is used to improve the efficiency of learning algorithms by finding an optimal subset of features. However, most feature selection techniques can handle only certain types of data. Additional limitations of existing methods include intensive computational requirements and inability to identify redundant variables. In this paper, we present a novel, information-theoretic algorithm for feature selection, which finds an optimal set of attributes by removing both irrelevant and redundant features. The algorithm has a polynomial computational complexity and is applicable to datasets of a mixed nature. The method performance is evaluated on several benchmark datasets by using a standard classifier (C4.5).", "num_citations": "160\n", "authors": ["327"]}
{"title": "Classification of web documents using a graph model\n", "abstract": " In this paper we describe work relating to classification of Web documents using a graph-based model instead of the traditional vector-based model for document representation. We compare the classification accuracy of the vector model approach using the k-nearest neighbor (k-NN) algorithm to a novel approach which allows the use of graphs for document representation in the k-NN algorithm. The proposed method is evaluated on three different Web document collections using the leave-one-out approach for measuring classification accuracy. The results show that the graph-based k-NN approach can outperform traditional vector-based k-NN methods in terms of both accuracy and execution time.", "num_citations": "131\n", "authors": ["327"]}
{"title": "Using a neural network in the software testing process\n", "abstract": " Software testing forms an integral part of the software development life cycle. Since the objective of testing is to ensure the conformity of an application to its specification, a test \u201coracle\u201d is needed to determine whether a given test case exposes a fault or not. Using an automated oracle to support the activities of human testers can reduce the actual cost of the testing process and the related maintenance costs. In this paper, we present a new concept of using an artificial neural network as an automated oracle for a tested software system. A neural network is trained by the backpropagation algorithm on a set of test cases applied to the original version of the system. The network training is based on the \u201cblack\u2010box\u201d approach, since only inputs and outputs of the system are presented to the algorithm. The trained network can be used as an artificial oracle for evaluating the correctness of the output produced by new and\u00a0\u2026", "num_citations": "128\n", "authors": ["327"]}
{"title": "A new approach to improving multilingual summarization using a genetic algorithm\n", "abstract": " Automated summarization methods can be defined as \u201clanguage-independent,\u201d if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as \u201cprocessing several languages, with summary in the same language as input.\u201d In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages\u2014English and Hebrew\u2014and evaluated its performance with ROUGE-1 Recall vs. stateof-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.", "num_citations": "121\n", "authors": ["327"]}
{"title": "Metaphor identification in large texts corpora\n", "abstract": " Identifying metaphorical language-use (e.g., sweet child) is one of the challenges facing natural language processing. This paper describes three novel algorithms for automatic metaphor identification. The algorithms are variations of the same core algorithm. We evaluate the algorithms on two corpora of Reuters and the New York Times articles. The paper presents the most comprehensive study of metaphor identification in terms of scope of metaphorical phrases and annotated corpora size. Algorithms\u2019 performance in identifying linguistic phrases as metaphorical or literal has been compared to human judgment. Overall, the algorithms outperform the state-of-the-art algorithm with 71% precision and 27% averaged improvement in prediction over the base-rate of metaphors in the corpus.", "num_citations": "119\n", "authors": ["327"]}
{"title": "Classification of web documents using graph matching\n", "abstract": " In this paper we describe a classification method that allows the use of graph-based representations of data instead of traditional vector-based representations. We compare the vector approach combined with the k-Nearest Neighbor (k-NN) algorithm to the graph-matching approach when classifying three different web document collections, using the leave-one-out approach for measuring classification accuracy. We also compare the performance of different graph distance measures as well as various document representations that utilize graphs. The results show the graph-based approach can outperform traditional vector-based methods in terms of accuracy, dimensionality and execution time.", "num_citations": "109\n", "authors": ["327"]}
{"title": "Real-time data mining of non-stationary data streams from sensor networks\n", "abstract": " In real-world sensor networks, the monitored processes generating time-stamped data may change drastically over time. An online data-mining algorithm called OLIN (on-line information network) adapts itself automatically to the rate of concept drift in a non-stationary data stream by repeatedly constructing a classification model from every sliding window of training examples. In this paper, we introduce a new real-time data-mining algorithm called IOLIN (incremental on-line information network), which saves a significant amount of computational effort by updating an existing model as long as no major concept drift is detected. The proposed algorithm builds upon the oblivious decision-tree classification model called \u201cinformation network\u201d (IN) and it implements three different types of model updating operations. In the experiments with multi-year streams of traffic sensors data, no statistically significant difference\u00a0\u2026", "num_citations": "97\n", "authors": ["327"]}
{"title": "Data mining in software metrics databases\n", "abstract": " We investigate the use of data mining for the analysis of software metric databases, and some of the issues in this application domain. Software metrics are collected at various phases of the software development process, in order to monitor and control the quality of a software product. However, software quality control is complicated by the complex relationship between these metrics and the attributes of a software development process. Data mining has been proposed as a potential technology for supporting and enhancing our understanding of software metrics and their relationship to software quality. In this paper, we use fuzzy clustering to investigate three datasets of software metrics, along with the larger issue of whether supervised or unsupervised learning is more appropriate for software engineering problems. While our findings generally confirm the known linear relationship between metrics and change\u00a0\u2026", "num_citations": "87\n", "authors": ["327"]}
{"title": "The data mining approach to automated software testing\n", "abstract": " In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called\" catastrophic\" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs\u00a0\u2026", "num_citations": "86\n", "authors": ["327"]}
{"title": "Graph-theoretic techniques for web content mining\n", "abstract": " In this dissertation we introduce several novel techniques for performing data mining on web documents which utilize graph representations of document content. Graphs are more robust than typical vector representations as they can model structural information that is usually lost when converting the original web document content to a vector representation. For example, we can capture information such as the location, order and proximity of term occurrence, which is discarded under the standard document vector representation models. Many machine learning methods rely on distance computations, centroid calculations, and other numerical techniques. Thus many of these methods have not been applied to data represented by graphs since no suitable graph-theoretical concepts were previously available.", "num_citations": "86\n", "authors": ["327"]}
{"title": "A fuzzy-based lifetime extension of genetic algorithms\n", "abstract": " In knowledge discovery, Genetic Algorithms have been used for classification, model selection, and other optimization tasks. However, behavior and performance of genetic algorithms are directly affected by the values of their input parameters, while poor parameter settings usually lead to several problems such as the premature convergence. Adaptive techniques have been suggested for adjusting the parameters in the process of running the genetic algorithm. None of these techniques have yet shown a significant overall improvement, since most of them remain domain-specific. In this paper, we attempt to improve the performance of genetic algorithms by providing a new, fuzzy-based extension of the LifeTime feature. We use a Fuzzy Logic Controller (FLC) to adapt the crossover probability as a function of the chromosomes\u2019 age. The general principle is that for both young and old individuals the crossover\u00a0\u2026", "num_citations": "81\n", "authors": ["327"]}
{"title": "A compact and accurate model for classification\n", "abstract": " We describe and evaluate an information-theoretic algorithm for data-driven induction of classification models based on a minimal subset of available features. The relationship between input (predictive) features and the target (classification) attribute is modeled by a tree-like structure termed an information network (IN). Unlike other decision-tree models, the information network uses the same input attribute across the nodes of a given layer (level). The input attributes are selected incrementally by the algorithm to maximize a global decrease in the conditional entropy of the target attribute. We are using the prepruning approach: when no attribute causes a statistically significant decrease in the entropy, the network construction is stopped. The algorithm is shown empirically to produce much more compact models than other methods of decision-tree learning while preserving nearly the same level of classification\u00a0\u2026", "num_citations": "81\n", "authors": ["327"]}
{"title": "Applied graph theory in computer vision and pattern recognition\n", "abstract": " Graph theory has strong historical roots in mathematics, especially in topology. Its birth is usually associated with the \u201cfour-color problem\u201d posed by Francis Guthrie 1 in 1852, but its real origin probably goes back to the Seven Bridges of Konigsber \u0308 g 2 problem proved by Leonhard Euler in 1736. A computational solution to these two completely different problems could be found after each problem was abstracted to the level of a graph model while ignoring such irrelevant details as country shapes or cross-river distances. In general, a graph is a nonempty set of points (vertices) and the most basic information preserved by any graph structure refers to adjacency relationships (edges) between some pairs of points. In the simplest graphs, edges do not have to hold any attributes, except their endpoints, but in more sophisticated graph structures, edges can be associated with a direction or assigned a label. Graph vertices can be labeled as well. A graph can be represented graphically as a drawing (vertex= dot, edge= arc), but, aslongaseverypairofadjacentpointsstaysconnected by the same edge, the graph vertices can be moved around on a drawing without changing the underlying graph structure. The expressive power of the graph models placing a special emphasis on c-nectivity between objects has made them the models of choice in chemistry, physics, biology, and other? elds.", "num_citations": "79\n", "authors": ["327"]}
{"title": "Improving stability of decision trees\n", "abstract": " Decision-tree algorithms are known to be unstable: small variations in the training set can result in different trees and different predictions for the same validation examples. Both accuracy and stability can be improved by learning multiple models from bootstrap samples of training data, but the \"meta-learner\"  approach makes the extracted knowledge hardly interpretable. In the following paper, we present the Info-Fuzzy Network (IFN), a novel information-theoretic method for building stable and comprehensible decision-tree models. The stability of the IFN algorithm is ensured by restricting the tree structure to using the same feature for all nodes of the same tree level and by the built-in statistical significance tests. The IFN method is shown empirically to produce more compact and stable models than the \"meta-learner\" techniques,  while preserving a reasonable level of predictive accuracy.", "num_citations": "77\n", "authors": ["327"]}
{"title": "Automated detection of outliers in real-world data\n", "abstract": " Most real-world databases include a certain amount of exceptional values, generally termed as \u201coutliers\u201d. The isolation of outliers is important both for improving the quality of original data and for reducing the impact of outlying values in the process of knowledge discovery in databases. Most existing methods of outlier detection are based on manual inspection of graphically represented data. In this paper, we present a new approach to automating the process of detecting and isolating outliers. The process is based on modeling the human perception of exceptional values by using the fuzzy set theory. Separate procedures are developed for detecting outliers in discrete and continuous univariate data. The outlier detection procedures are demonstrated on several standard datasets of varying data quality.", "num_citations": "76\n", "authors": ["327"]}
{"title": "Clustering of web documents using a graph model\n", "abstract": " In this chapter we enhance the representation of web documents by utilizing graphs instead of vectors. In typical content-based representations of web documents based on the popular vector model, the structural (term adjacency and term location) information cannot be used for clustering. We have created a new framework for extending traditional numerical vector-based clustering algorithms to work with graphs. This approach is demonstrated by an extended version of the classical -means clustering algorithm which uses the maximum common subgraph distance measure and the concept of median graphs in the place of the usual distance and centroid calculations, respectively. An interesting feature of our approach is that the determination of the maximum common subgraph for measuring graph similarity, which is an NP-Complete problem, becomes polynomial time with our graph representation. By applying this\u00a0\u2026", "num_citations": "70\n", "authors": ["327"]}
{"title": "Info-fuzzy algorithms for mining dynamic data streams\n", "abstract": " Most data-mining algorithms assume static behavior of the incoming data. In the real world, the situation is different and most continuously collected data streams are generated by dynamic processes, which may change over time, in some cases even drastically. The change in the underlying concept, also known as concept drift, causes the data-mining model generated from past examples to become less accurate and relevant for classifying the current data. Most online learning algorithms deal with concept drift by generating a new model every time a concept drift is detected. On one hand, this solution ensures accurate and relevant models at all times, thus implying an increase in the classification accuracy. On the other hand, this approach suffers from a major drawback, which is the high computational cost of generating new models. The problem is getting worse when a concept drift is detected more frequently\u00a0\u2026", "num_citations": "63\n", "authors": ["327"]}
{"title": "ADMIRAL: A data mining based financial trading system\n", "abstract": " This paper presents a novel framework for predicting stock trends and making financial trading decisions based on a combination of data and text mining techniques. The prediction models of the proposed system are based on the textual content of time-stamped Web documents in addition to traditional numerical time series data, which is also available from the Web. The financial trading system based on the model predictions (ADMIRAL) is using three different trading strategies. In this paper, the ADMIRAL system is simulated and evaluated on real-world series of news stories and stocks data using the C4.5 decision tree induction algorithm. The main performance measures are the predictive accuracy of the induced models and, more importantly, the profitability of each trading strategy using these predictions", "num_citations": "62\n", "authors": ["327"]}
{"title": "DegExt\u2014A language-independent graph-based keyphrase extractor\n", "abstract": " In this paper, we introduce DegExt, a graph-based languageindependent keyphrase extractor,which extends the keyword extraction method described in [6]. We compare DegExt with two state-of-the-art approaches to keyphrase extraction: GenEx [11] and TextRank [8].               Our experiments on a collection of benchmark summaries show that DegExt outperforms TextRank and GenEx in terms of precision and area under curve (AUC) for summaries of 15 keyphrases or more at the expense of a non-significant decrease of recall and F-measure. Moreover, DegExt surpasses both GenEx and TextRank in terms of implementation simplicity and computational complexity.", "num_citations": "60\n", "authors": ["327"]}
{"title": "Large-scale analysis of self-disclosure patterns among online social networks users: a Russian context\n", "abstract": " Online social network services (SNS) provide an unprecedented rich source of information about millions of users worldwide. However, most existing studies of this emerging phenomenon are limited to relatively small data samples, with an emphasis on mostly \u201cwestern\u201d online communities (such as Facebook and MySpace users in Western countries). To understand the cultural characteristics of users of online social networks, this paper explores the behavioral patterns of more than 16 million users of a popular social network in the Russian segment of the Internet, namely, My.Mail.Ru (also known as \u201cMy World\u201d or \u201cMoj Mir\u201d in Russian). Our main goal is to study the self-disclosure patterns of the site users as a function of their age and gender. We compare the findings of our analysis to the previous studies on Western users of SNS and discuss the culturally distinctive aspects. Our study highlights some\u00a0\u2026", "num_citations": "59\n", "authors": ["327"]}
{"title": "Short-term load forecasting in smart meters with sliding window-based ARIMA algorithms\n", "abstract": " Forecasting of electricity consumption for residential and industrial customers is an important task providing intelligence to the smart grid. Accurate forecasting should allow a utility provider to plan the resources as well as to take control actions to balance the supply and the demand of electricity. This paper presents two non-seasonal and two seasonal sliding window-based ARIMA (auto regressive integrated moving average) algorithms. These algorithms are developed for short-term forecasting of hourly electricity load at the district meter level. The algorithms integrate non-seasonal and seasonal ARIMA models with the OLIN (online information network) methodology. To evaluate our approach, we use a real hourly consumption data stream recorded by six smart meters during a 16-month period.", "num_citations": "58\n", "authors": ["327"]}
{"title": "Anomaly detection in web documents using crisp and fuzzy-based cosine clustering methodology\n", "abstract": " Cluster analysis is a primary tool for detecting anomalous behavior in real-world data such as web documents, medical records of patients or other personal data. Most existing methods for document clustering are based on the classical vector-space model, which represents each document by a fixed-size vector of weighted key terms often referred to as key phrases. Since vector representations of documents are frequently very sparse, inverted files are used to prevent a tremendous computational overload which may be caused in large and diverse document collections such as pages downloaded from the World Wide Web. In order to reduce computation costs and space complexity, many popular methods for clustering web documents, including those using inverted files, usually assume a relatively small prefixed number of clusters.We propose several new crisp and fuzzy approaches based on the cosine\u00a0\u2026", "num_citations": "58\n", "authors": ["327"]}
{"title": "Modeling software testing costs and risks using fuzzy logic paradigm\n", "abstract": " The overall lifecycle cost associated with product failures exceeds 10% of yearly corporations\u2019 turnover. A major factor contributing to this loss is ineffective performance of software and systems Verification, Validation and Testing (VVT). Given these realities, we proposed a set of quantitative probabilistic models for estimating costs and risks stemming from carrying out any given VVT strategy [Engel, A., Barad, M., 2003. A methodology for modeling VVT risks and costs. Systems Engineering Journal 6 (3), 135\u2013151, Wiley InterScience, Online ISSN: 1520-6858, Print ISSN: 1098-1241]. We also demonstrated that quality costs in software-intensive projects are likely to consume as much as 60% of the development budget. Finally, we showed that project cost and duration could be reduced by optimizing the VVT strategy, yielding about 10\u201315% reduction in development costs and project schedule [Engel, A., Shachar, S\u00a0\u2026", "num_citations": "56\n", "authors": ["327"]}
{"title": "Effective black-box testing with genetic algorithms\n", "abstract": " Black-box (functional) test cases are identified from functional requirements of the tested system, which is viewed as a mathematical function mapping its inputs onto its outputs. While the number of possible black-box tests for any non-trivial program is extremely large, the testers can run only a limited number of test cases under their resource limitations. An effective set of test cases is the one that has a high probability of detecting faults presenting ina computer program.In this paper, we introduce a new, computationally intelligent approach to automated generation of effective test cases based on a novel, Fuzzy-Based Age Extension of Genetic Algorithms (FAexGA). The basic idea is to eliminate \"bad\" test cases that are unlikely to expose any error, while increasing the number of \"good\" test cases that have a high probability of producing an erroneous output. The promising performance of the\u00a0\u2026", "num_citations": "55\n", "authors": ["327"]}
{"title": "Incremental clustering of mobile objects\n", "abstract": " Moving objects are becoming increasingly attractive to the data mining community due to continuous advances in technologies like GPS, mobile computers, and wireless communication devices. Mining spatio-temporal data can benefit many different functions: marketing team managers for identifying the right customers at the right time, cellular companies for optimizing the resources allocation, web site administrators for data allocation matters, animal migration researchers for understanding migration patterns, and meteorology experts for weather forecasting. In this research we use a compact representation of a mobile trajectory and define a new similarity measure between trajectories. We also propose an incremental clustering algorithm for finding evolving groups of similar mobile objects in spatio-temporal data. The algorithm is evaluated empirically by the quality of object clusters (using Dunn and Rand\u00a0\u2026", "num_citations": "51\n", "authors": ["327"]}
{"title": "3-dimensional curve similarity using string matching\n", "abstract": " In this paper, we present a new approach to measuring the similarity between 3D-curves. Our approach allows the possibility of using strings, where each element is a vector rather than just a symbol. We present two different approaches for representing 3D-curves. One possibility is to represent a 3D-curve as two 2D-curves, one being the projection of the 3D-curve in the XY-plane, and the other one in the YZ-plane. For the case that we need geometric rotation invariance, we have used a second approach to the symbolic representation of the 3D-curve using the curvature and the tension as their symbolic representation. We validate this approach through experiments using synthetic and digitalized data.", "num_citations": "48\n", "authors": ["327"]}
{"title": "Using data mining techniques for detecting terror-related activities on the web\n", "abstract": " An innovative knowledge-based methodology for terrorist detection by using Web traffic content as the audit information is presented. The proposed methodology learns the typical behavior (\u2018profile\u2019) of terrorists by applying a data mining algorithm to the textual content of terror-related Web sites. The resulting profile is used by the system to perform real-time detection of users suspected of being engaged in terrorist activities. The Receiver-Operator Characteristic (ROC) analysis shows that this methodology can outperform a command-based intrusion detection system.", "num_citations": "48\n", "authors": ["327"]}
{"title": "Using data mining for automated software testing\n", "abstract": " In today's software industry, the design of test cases is mostly based on human expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outcomes is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. In this paper, we demonstrate the potential use of data mining algorithms for automated modeling of tested systems. The data mining models can be utilized for recovering system requirements, designing a minimal set of regression tests, and evaluating the correctness of software outputs. To study the feasibility of the proposed approach, we have applied a state-of-the-art data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a complex mathematical package. The IFN\u00a0\u2026", "num_citations": "47\n", "authors": ["327"]}
{"title": "Automatic identification of conceptual metaphors with limited knowledge\n", "abstract": " Full natural language understanding requires identifying and analyzing the meanings of metaphors, which are ubiquitous in both text and speech. Over the last thirty years, linguistic metaphors have been shown to be based on more general conceptual metaphors, partial semantic mappings between disparate conceptual domains. Though some achievements have been made in identifying linguistic metaphors over the last decade or so, little work has been done to date on automatically identifying conceptual metaphors. This paper describes research on identifying conceptual metaphors based on corpus data. Our method uses as little background knowledge as possible, to ease transfer to new languages and to mini-mize any bias introduced by the knowledge base construction process. The method relies on general heuristics for identifying linguistic metaphors and statistical clustering (guided by Wordnet) to form conceptual metaphor candidates. Human experiments show the system effectively finds meaningful conceptual metaphors.", "num_citations": "46\n", "authors": ["327"]}
{"title": "Knowledge discovery in data streams with regression tree methods\n", "abstract": " This paper presents an advanced review of regression tree methods for mining data streams. Batch regression tree methods are known for their simplicity, interpretability, accuracy, and efficiency. They use fast divide\u2010and\u2010conquer greedy algorithms that recursively partition the given training data into smaller subsets. The result is a tree\u2010shaped model with splitting rules in the internal nodes and predictions in the leaves. Most batch regression tree methods take a complete dataset and build a model using that data. Generally, this tree model cannot be modified if new data is acquired later. Their successors, the incremental model and interval trees algorithms, are able to build and retrain a model on a step\u2010by\u2010step basis by incorporating new numerical training instances into the model as they become available. Moreover, these algorithms produce even more compact and accurate models than batch regression tree\u00a0\u2026", "num_citations": "46\n", "authors": ["327"]}
{"title": "The hybrid representation model for web document classification\n", "abstract": " Most Web content categorization methods are based on the vector space model of information retrieval. One of the most important advantages of this representation model is that it can be used by both instance\u2010based and model\u2010based classifiers. However, this popular method of document representation does not capture important structural information, such as the order and proximity of word occurrence or the location of a word within the document. It also makes no use of the markup information that can easily be extracted from the Web document HTML tags. A recently developed graph\u2010based Web document representation model can preserve Web document structural information. It was shown to outperform the traditional vector representation using the k\u2010Nearest Neighbor (k\u2010NN) classification algorithm. The problem, however, is that the eager (model\u2010based) classifiers cannot work with this representation\u00a0\u2026", "num_citations": "39\n", "authors": ["327"]}
{"title": "Quality and comprehension of UML interaction diagrams-an experimental comparison\n", "abstract": " UML (Unified Modeling Language) is a collection of somewhat overlapping modeling techniques, thus creating a difficulty in establishing practical guidelines for selecting the most suitable techniques for modeling OO artifacts. This is true mainly with respect to two types of interaction diagrams: Sequence and collaboration. Attempts have been made to evaluate the comprehensibility of these diagram types for various types of applications, but they did not address the issue of quality of diagrams created by analysts. This article reports the findings from a controlled experiment where both the comprehensibility and quality of the interaction diagrams were investigated in two application domains: management information systems (MIS) and real-time (RT) systems.Our results indicate that collaboration diagrams are easier to comprehend than sequence diagrams in RT systems, but there is no difference in comprehension\u00a0\u2026", "num_citations": "38\n", "authors": ["327"]}
{"title": "A feature-based serial approach to classifier combination\n", "abstract": " : A new approach to the serial multi-stage combination of classifiers is proposed. Each classifier in the sequence uses a smaller subset of features than the subsequent classifier. The classification provided by a classifier is rejected only if its decision is below a predefined confidence level. The approach is tested on a two-stage combination of k-Nearest Neighbour classifiers. The features to be used by the first classifier in the combination are selected by two stand-alone algorithms (Relief and Info-Fuzzy Network, or IFN) and a hybrid method, called \u2018IFN + Relief\u2019. The feature-based approach is shown empirically to provide a substantial decrease in the computational complexity, while maintaining the accuracy level of a single-stage classifier or even improving it.", "num_citations": "37\n", "authors": ["327"]}
{"title": "Information-theoretic fuzzy approach to data reliability and data mining\n", "abstract": " A novel, information-theoretic fuzzy approach to discovering unreliable data in a relational database is presented. A multilevel information-theoretic connectionist network is constructed to evaluate activation functions of partially reliable database values. The degree of value reliability is defined as a fuzzy measure of difference between the maximum attribute activation and the actual value activation. Unreliable values can be removed from the database or corrected to the values predicted by the network. The method is applied to a real-world relational database which is extended to a fuzzy relational database by adding fuzzy attributes representing reliability degrees of crisp attributes. The highest connection weights in the network are translated into meaningful if, then rules. This work aims at improving reliability of data in a relational database by developing a framework for discovering, accessing and correcting\u00a0\u2026", "num_citations": "37\n", "authors": ["327"]}
{"title": "A fuzzy-based path ordering algorithm for QoS routing in non-deterministic communication networks\n", "abstract": " We present a novel, fuzzy-based method for path selection under additive quality of service constraints, where the information available for making routing decisions is inaccurate. The goal of the path selection process is to identify a feasible path while minimizing the overall setup time required for establishing a successful connection. Under the assumption that a list of candidate paths already exists, we explore the fuzzy logic approach for this purpose. The performance evaluation of the proposed algorithm is done by a simulation program that compares between the fuzzy logic approach, the optimal solution and a standard approach suggested in the literature, where the main performance metric is the average setup time for successfully establishing a feasible connection. The results show that the fuzzy approach has a lower average setup time for establishing a connection than the standard approach and that it is\u00a0\u2026", "num_citations": "36\n", "authors": ["327"]}
{"title": "Improving accuracy of classification models induced from anonymized datasets\n", "abstract": " The performance of classifiers and other data mining models can be significantly enhanced using the large repositories of digital data collected nowadays by public and private organizations. However, the original records stored in those repositories cannot be released to the data miners as they frequently contain sensitive information. The emerging field of Privacy Preserving Data Publishing (PPDP) deals with this important challenge. In this paper, we present NSVDist (Non-homogeneous generalization with Sensitive Value Distributions)\u2014a new anonymization algorithm that, given minimal anonymity and diversity parameters along with an information loss measure, issues corresponding non-homogeneous anonymizations where the sensitive attribute is published as frequency distributions over the sensitive domain rather than in the usual form of exact sensitive values. In our experiments with eight datasets and\u00a0\u2026", "num_citations": "34\n", "authors": ["327"]}
{"title": "Test case generation and reduction by automated input-output analysis\n", "abstract": " In the software testing process, selecting the test cases and verifying their results requires a lot of subjective decisions and human intervention. For a program having a large number of inputs, the number of corresponding combinatorial black-box test cases is huge. A method needs to be established in order to limit the number of test cases and to choose the most important ones. In this research effort we present a novel methodology for identifying important test cases automatically. These test cases involve input attributes which contribute to the value of an output and hence are significant. The reduction in the number of test cases is attributed to identifying input-output relationships. A ranked list of features and equivalence classes for input attributes of a given code are the main outcomes of this methodology. Reducing the number of test cases results directly in the saving of software testing resources.", "num_citations": "34\n", "authors": ["327"]}
{"title": "Graph representations for web document clustering\n", "abstract": " In this paper we describe clustering of web documents represented by graphs rather than vectors. We present a novel method for clustering graph-based data using the standard k-means algorithm and compare its performance to the conventional vector-model approach using cosine similarity. The proposed method is evaluated when using five different graph representations under two different clustering performance indices. The experiments are performed on two separate web document collections.", "num_citations": "34\n", "authors": ["327"]}
{"title": "Visual evaluation of text features for document summarization and analysis\n", "abstract": " Thanks to the Web-related and other advanced technologies, textual information is increasingly being stored in digital form and posted online. Automatic methods to analyze such textual information are becoming inevitable. Many of those methods are based on quantitative text features. Analysts face the challenge to choose the most appropriate features for their tasks. This requires effective approaches for evaluation and feature-engineering.", "num_citations": "32\n", "authors": ["327"]}
{"title": "Theory of actionable data mining with application to semiconductor manufacturing control\n", "abstract": " Accurate and timely prediction of a manufacturing process yield and flow times is often desired as a means of reducing overall production costs. To this end, this paper develops a new decision-theoretic classification framework and applies it to a real-world semiconductor wafer manufacturing line that suffers from constant variations in the characteristics of the chip-manufacturing process. The decision-theoretic framework is based on a model for evaluating classifiers in terms of their value in decision-making. Recognizing that in many practical applications the values of the class probabilities as well as payoffs are neither static nor known exactly, a precise condition under which one classifier \u2018dominates\u2019 another classifier (i.e. achieves higher payoff), regardless of payoff or class distribution information, is presented. Building on the decision-theoretic model, two robust ensemble classification methods are proposed\u00a0\u2026", "num_citations": "32\n", "authors": ["327"]}
{"title": "Discovering useful and understandable patterns in manufacturing data\n", "abstract": " Accurate planning of produced quantities is a challenging task in semiconductor industry where the percentage of good parts (measured by yield) is affected by multiple factors. However, conventional data mining methods that are designed and tuned on \u201cwell-behaved\u201d data tend to produce a large number of complex and hardly useful patterns when applied to manufacturing databases. This paper presents a novel, perception-based method, called Automated Perceptions Network (APN), for automated construction of compact and interpretable models from highly noisy data sets. We evaluate the method on yield data of two semiconductor products and describe possible directions for the future use of automated perceptions in data mining and knowledge discovery.", "num_citations": "32\n", "authors": ["327"]}
{"title": "Comparison of distance measures for graph-based clustering of documents\n", "abstract": " In this paper we describe work relating to clustering of document collections. We compare the conventional vector-model approach using cosine similarity and Euclidean distance to a novel method we have developed for clustering graph-based data with the standard k-means algorithm. The proposed method is evaluated using five different graph distance measures under three clustering performance indices. The experiments are performed on two separate document collections. The results show the graph-based approach performs as well as vector-based methods or even better when using normalized graph distance measures.", "num_citations": "32\n", "authors": ["327"]}
{"title": "Artificial intelligence methods in software testing\n", "abstract": " An inadequate infrastructure for software testing is causing major losses to the world economy. The characteristics of software quality problems are quite similar to other tasks successfully tackled by artificial intelligence techniques. The aims of this book are to present state-of-the-art applications of artificial intelligence and data mining methods to quality assurance of complex software systems, and to encourage further research in this important and challenging area.", "num_citations": "30\n", "authors": ["327"]}
{"title": "A Scalable Algorithm for One-to-One, Onto, and Partial Schema Matching with Uninterpreted Column Names and Column Values\n", "abstract": " In this paper, the authors propose a five-step approach to the problem of identifying semantic correspondences between attributes of two database schemas. It is one of the key challenges in many database applications such as data integration and data warehousing. The authors' research is focused on uninterpreted schema matching, where the column names and column values are uninterpreted or unreliable. The approach implements Bayesian networks, Pearson's correlation and mutual information to identify inter-attribute dependencies. Additionally, the authors propose an extension to their algorithm that allows the user to manually enter the known mappings to improve the automated matching results. The five-step approach also allows data privacy preservation. The authors' evaluation experiments show that the proposed approach enhances the current set of schema matching techniques.", "num_citations": "29\n", "authors": ["327"]}
{"title": "Why \u201cdark thoughts\u201d aren't really dark: A novel algorithm for metaphor identification\n", "abstract": " Distinguishing between literal and metaphorical language is a major challenge facing natural language processing. Heuristically, metaphors can be divided into three general types in which type III metaphors are those involving an adjective-noun relationship (e.g. \u201cdark humor\u201d). This paper describes our approach for automatic identification of type III metaphors. We propose a new algorithm, the Concrete-Category Overlap (CCO) algorithm, that distinguishes between literal and metaphorical use of adjective-noun relationships and evaluate it on two data sets of adjective-noun phrases. Our results point to the superiority of the CCO algorithm to past and contemporary approaches in determining the presence and conceptual significance of metaphors, and provide a better understanding of the conditions under which each algorithm should be applied.", "num_citations": "28\n", "authors": ["327"]}
{"title": "Cross-lingual training of summarization systems using annotated corpora in a foreign language\n", "abstract": " The increasing trend of cross-border globalization and acculturation requires text summarization techniques to work equally well for multiple languages. However, only some of the automated summarization methods can be defined as \u201clanguage-independent,\u201d i.e., not based on any language-specific knowledge. Such methods can be used for multilingual summarization, defined in Mani (Automatic summarization. Natural language processing. John Benjamins Publishing Company, Amsterdam, 2001) as \u201cprocessing several languages, with a summary in the same language as input\u201d, but, their performance is usually unsatisfactory due to the exclusion of language-specific knowledge. Moreover, supervised machine learning approaches need training corpora in multiple languages that are usually unavailable for rare languages, and their creation is a very expensive and labor-intensive process. In this article\u00a0\u2026", "num_citations": "28\n", "authors": ["327"]}
{"title": "Incremental info-fuzzy algorithm for real time data mining of non-stationary data streams\n", "abstract": " Most real-world data streams are generated by nonstationary processes that may change drastically over time. In our previous work, we have presented a real-time data mining algorithm called OLIN (On-Line Information Network), which adapts itself automatically to the rate of concept drift in a non-stationary data stream by repeatedly constructing a new model from a sliding window of latest examples. In this paper, we introduce an incremental version of the OLIN algorithm, which saves a significant amount of computational effort by updating an existing model as long as no concept drift is detected. The approach is evaluated on large real-world streams of traffic and stock data.", "num_citations": "28\n", "authors": ["327"]}
{"title": "Anytime algorithm for feature selection\n", "abstract": " Feature selection is used to improve performance of learning algorithms by finding a minimal subset of relevant features. Since the process of feature selection is computationally intensive, a trade-off between the quality of the selected subset and the computation time is required. In this paper, we are presenting a novel, anytime algorithm for feature selection, which gradually improves the quality of results by increasing the computation time. The algorithm is interruptible, i.e., it can be stopped at any time and provide a partial subset of selected features. The quality of results is monitored by a new measure: fuzzy information gain. The algorithm performance is evaluated on several benchmark datasets.", "num_citations": "28\n", "authors": ["327"]}
{"title": "Data mining for process and quality control in the semiconductor industry\n", "abstract": " Like in any other industry, manufacturing departments of semiconductor plants are evaluated by their ability to meet the delivery schedules. However, the final quantities and the flow times of individual semiconductor batches are affected by multiple uncertain factors, like material quality, process variability, equipment condition, and others. Thus, the tasks of predicting the batch quality (measured by yield) and its total flow time are an important part of the production planning activities. Beyond prediction, the plant management is interested in identifying the main causes of yield excursion and process delays.               In this paper, we are applying several methods of data mining and knowledge discovery to WIP (Work-in-Process) data, collected in a semiconductor plant. The information on each manufacturing batch includes its design parameters, process tracking data, line yield, etc. The data is prepared for\u00a0\u2026", "num_citations": "27\n", "authors": ["327"]}
{"title": "Automated perceptions in data mining\n", "abstract": " Visualization is known to be one of the most efficient data mining approaches. The human eye can capture complex patterns and relationships, along with detecting the outlying (exceptional) cases in a data set. The main limitation of the visual data analysis is its poor scalability: it is hardly applicable to data sets of high dimensionality. We use the concepts of fuzzy set theory to automate the process of human perception. The automated tasks include comparison of frequency distributions, evaluating reliability of dependent variables, and detecting outliers in noisy data. Multiple perceptions (related to different users) can be represented by adjusting the parameters of the fuzzy membership functions. The applicability of automated perceptions is demonstrated on several real-world data sets.", "num_citations": "27\n", "authors": ["327"]}
{"title": "Predicting the maximum earthquake magnitude from seismic data in Israel and its neighboring countries\n", "abstract": " This paper explores several data mining and time series analysis methods for predicting the magnitude of the largest seismic event in the next year based on the previously recorded seismic events in the same region. The methods are evaluated on a catalog of 9,042 earthquake events, which took place between 01/01/1983 and 31/12/2010 in the area of Israel and its neighboring countries. The data was obtained from the Geophysical Institute of Israel. Each earthquake record in the catalog is associated with one of 33 seismic regions. The data was cleaned by removing foreshocks and aftershocks. In our study, we have focused on ten most active regions, which account for more than 80% of the total number of earthquakes in the area. The goal is to predict whether the maximum earthquake magnitude in the following year will exceed the median of maximum yearly magnitudes in the same region. Since the analyzed catalog includes only 28 years of complete data, the last five annual records of each region (referring to the years 2006\u20132010) are kept for testing while using the previous annual records for training. The predictive features are based on the Gutenberg-Richter Ratio as well as on some new seismic indicators based on the moving averages of the number of earthquakes in each area. The new predictive features prove to be much more useful than the indicators traditionally used in the earthquake prediction literature. The most accurate result (AUC = 0.698) is reached by the Multi-Objective Info-Fuzzy Network (M-IFN) algorithm, which takes into account the association between two target variables: the number of earthquakes and the\u00a0\u2026", "num_citations": "26\n", "authors": ["327"]}
{"title": "Predictive maintenance with multi-target classification models\n", "abstract": " Unexpected failures occurring in new cars during the warranty period increase the warranty costs of car manufacturers along with harming their brand reputation. A predictive maintenance strategy can reduce the amount of such costly incidents by suggesting the driver to schedule a visit to the dealer once the failure probability within certain time period exceeds a pre-defined threshold. The condition of each subsystem in a car can be monitored onboard vehicle telematics systems, which become increasingly available in modern cars. In this paper, we apply a multi-target probability estimation algorithm (M-IFN) to an integrated database of sensor measurements and warranty claims with the purpose of predicting the probability and the timing of a failure in a given subsystem. The multi-target algorithm performance is compared to a single-target probability estimation algorithm (IFN) and reliability modeling\u00a0\u2026", "num_citations": "26\n", "authors": ["327"]}
{"title": "Multi-lingual detection of terrorist content on the web\n", "abstract": " Since the web is increasingly used by terrorist organizations for propaganda, disinformation, and other purposes, the ability to automatically detect terrorist-related content in multiple languages can be extremely useful. In this paper we describe a new, classification-based approach to multi-lingual detection of terrorist documents. The proposed approach builds upon the recently developed graph-based web document representation model combined with the popular C4.5 decision-tree classification algorithm. Evaluation is performed on a collection of 648 web documents in Arabic language. The results demonstrate that documents downloaded from several known terrorist sites can be reliably discriminated from the content of Arabic news reports using a simple decision tree.", "num_citations": "26\n", "authors": ["327"]}
{"title": "Automated test reduction using an info-fuzzy network\n", "abstract": " In today\u2019s software industry, design of black-box test cases is a manual activity, based mostly on human expertise, while automation tools are dedicated to execution of pre-planned tests only. However, the manual process of selecting test cases can rarely be considered as satisfactory both in terms of associated costs and the quality of produced software. This paper presents an attempt to automate a common task in black-box testing, namely reducing the number of combinatorial tests. Our approach is based on automated identification of relationships between inputs and outputs of a data-driven application. The set of input variables relevant to each output is extracted from execution data by a novel data mining algorithm called the info-fuzzy network (IFN). The proposed method does not require the knowledge of either the tested code, or the system specification, except for the list of software\u00a0\u2026", "num_citations": "26\n", "authors": ["327"]}
{"title": "Exploring gender differences in member profiles of an online dating site across 35 countries\n", "abstract": " Online communities such as forums, general purpose social networking and dating sites, have rapidly become one of the important data sources for analysis of human behavior fostering research in different scientific domains such as computer science, psychology, anthropology, and social science. The key component of most of the online communities and Social Networking Sites (SNS) in particular, is the user profile, which plays a role of a self-advertisement in the aggregated form. While some scientists investigate privacy implications of information disclosure, others test or generate social and behavioral hypotheses based on the information provided by users in their profiles or by interviewing members of these SNS. In this paper, we apply a number of analytical procedures on a large-scale SNS dataset of 10 million public profiles with more than 40 different attributes from one of the largest dating sites in\u00a0\u2026", "num_citations": "25\n", "authors": ["327"]}
{"title": "Fast categorization of Web documents represented by graphs\n", "abstract": " Most text categorization methods are based on the vector-space model of information retrieval. One of the important advantages of this representation model is that it can be used by both instance-based and model-based classifiers for categorization. However, this popular method of document representation does not capture important structural information, such as the order and proximity of word occurrence or the location of a word within the document. It also makes no use of the mark-up information that is available from web document HTML tags.             A recently developed graph-based representation of web documents can preserve the structural information. The new document model was shown to outperform the traditional vector representation, using the k-Nearest Neighbor (k-NN) classification algorithm. The problem, however, is that the eager (model-based) classifiers cannot work with this\u00a0\u2026", "num_citations": "25\n", "authors": ["327"]}
{"title": "Content-based methodology for anomaly detection on the web\n", "abstract": " As became apparent after the tragic events of September 11, 2001, terrorist organizations and other criminal groups are increasingly using the legitimate ways of Internet access to conduct their malicious activities. Such actions cannot be detected by existing intrusion detection systems that are generally aimed at protecting computer systems and networks from some kind of \u201ccyber attacks\u201d. Preparation of an attack against the human society itself can only be detected through analysis of the content accessed by the users. The proposed study aims at developing an innovative methodology for abnormal activity detection, which uses web content as the audit information provided to the detection system. The new behavior-based detection method learns the normal behavior by applying an unsupervised clustering algorithm to the contents of publicly available web pages viewed by a group of similar users. In this\u00a0\u2026", "num_citations": "25\n", "authors": ["327"]}
{"title": "The first wave of COVID-19 in Israel\u2014Initial analysis of publicly available data\n", "abstract": " The first case of COVID-19 was confirmed in Israel on February 21, 2020. Within approximately 30 days, the total number of confirmed cases climbed up to 1, 000, accompanied by a doubling period of less than 3 days. About one week later, after this number exceeded 4, 000 cases, and following some extreme lockdown measures taken by the Israeli government, the daily infection rate started a sharp decrease from the peak value of 1, 131 down to slightly more than 100 new confirmed cases on April 30. Motivated by this encouraging data, similar to the trends observed in many other countries, along with the growing economic pressures, the Israeli government has quickly lifted most of its emergency regulations. Throughout May, the daily number of new cases stayed at a very low level of 20\u201340 until at the end of May it started a steady increase, exceeding 1, 000 by the end of June and 2, 000 on July 22. As suggested by some experts and popular media, this disturbing trend may be even a part of a \u201csecond wave\u201d. This article attempts to analyze the data available on Israel at the end of July 2020, compared to three European countries (Greece, Italy, and Sweden), in order to understand the local dynamics of COVID-19, assess the effect of the implemented intervention measures, and discuss some plausible scenarios for the foreseeable future.", "num_citations": "24\n", "authors": ["327"]}
{"title": "Content-based detection of terrorists browsing the web using an advanced terror detection system (ATDS)\n", "abstract": " The Terrorist Detection System (TDS) is aimed at tracking down suspected terrorists by analyzing the content of information they access. TDS operates in two modes: a training mode and a detection mode. During the training mode TDS is provided with Web pages accessed by a normal group of users and computes their typical interests. During the detection mode TDS performs real-time monitoring of the traffic emanating from the monitored group of users, analyzes the content of the Web pages accessed, and issues an alarm if the access information is not within the typical interests of the group. In this paper we present an advanced version of TDS (ATDS), where the detection algorithm was enhanced to improve the performance of the basic TDS system. ATDS was implemented and evaluated in a network environment of 38 users comparing it to the performance of the basic TDS. Behavior of suspected\u00a0\u2026", "num_citations": "24\n", "authors": ["327"]}
{"title": "A comparison of two novel algorithms for clustering web documents\n", "abstract": " In this paper we investigate the clustering of web document collections using two variants of the popular kmeans clustering algorithm. The first variant is the global k-means method, which computes \u201cgood\u201d initial cluster centers deterministically rather than relying on random initialization. The second variant allows for the use of graphs as fundamental representations of data items instead of the simpler vector model. We perform experiments comparing global k-means with random initialization using both the graph-based and the vectorbased representations. Experiments are carried out on two web document collections and performance is evaluated using two clustering performance measures.", "num_citations": "24\n", "authors": ["327"]}
{"title": "Information-theoretic fuzzy approach to knowledge discovery in databases\n", "abstract": " We suggest a novel, unified approach to automating the entire process of Knowledge Discovery in Databases (KDD). The approach builds upon Shannon\u2019s Information Theory, statistical estimation methods, and Fuzzy Logic. The KDD stages to be automated include: dimensionality reduction, discovering informative rules (patterns), predicting values of unknown attributes, and cleaning a dataset from lowly reliable data.             The relational database model is extended by partitioning the relation scheme into a subset of candidate input (predictive) attributes and a subset of target (classification) attributes. A multilevel information-theoretic connectionist network is constructed to evaluate the mutual information between input and target attributes. The optimal network structure is found by a stepwise gradient procedure. The network connection weights are used to extract informative rules and to calculate\u00a0\u2026", "num_citations": "24\n", "authors": ["327"]}
{"title": "Degext: a language-independent keyphrase extractor\n", "abstract": " In this paper, we introduce DegExt, a graph-based language-independent keyphrase extractor, which extends the keyword extraction method described in Litvak and Last (Graph-based keyword extraction for single-document summarization. In: Proceedings of the workshop on multi-source multilingual information extraction and summarization, pp 17\u201324, 2008). We compare DegExt with two state-of-the-art approaches to keyphrase extraction: GenEx (Turney in Inf Retr 2:303\u2013336, 2000) and TextRank (Mihalcea and Tarau in Textrank\u2014bringing order into texts. In: Proceedings of the conference on empirical methods in natural language processing. Barcelona, Spain, 2004). We evaluated DegExt on collections of benchmark summaries in two different languages: English and Hebrew. Our experiments on the English corpus show that DegExt significantly outperforms TextRank and GenEx in terms of precision\u00a0\u2026", "num_citations": "23\n", "authors": ["327"]}
{"title": "Model-based classification of web documents represented by graphs\n", "abstract": " Most web content classification methods are based on the vectorspace model of information retrieval. One of the important advantages of this representation model is that it can be used by both instance-based and model-based classifiers for categorization. However, this popular method of document representation does not capture important structural information, such as the order and proximity of word occurrence or the location of a word within the document. It also makes no use of the mark-up information that can be easily extracted from the web document HTML tags.A recently developed graph-based web document representation model can preserve web document structural information. It was shown to outperform the traditional vector representation, using the k-Nearest Neighbor (k-NN) classification algorithm. The problem, however, is that the eager (model-based) classifiers cannot work with this representation directly. In this paper, three new, hybrid approaches to web document classification are presented, built upon both graph and vector space representations, thus preserving the benefits and discarding the limitations of each. The hybrid methods presented here are compared to vector-based models using two model-based classifiers (C4. 5 decision-tree algorithm and probabilistic Na\u00efve Bayes) on two benchmark web document collections. The results demonstrate that the hybrid methods presented in this paper outperform, in most cases, existing approaches in terms of classification accuracy, and in addition, achieve a significant reduction in the classification time.", "num_citations": "23\n", "authors": ["327"]}
{"title": "Museec: A multilingual text summarization tool\n", "abstract": " The MUSEEC (MUltilingual SEntence Extraction and Compression) summarization tool implements several extractive summarization techniques\u2013at the level of complete and compressed sentences\u2013that can be applied, with some minor adaptations, to documents in multiple languages.The current version of MUSEEC provides the following summarization methods:(1) MUSE\u2013a supervised summarizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top\u2013ranking sentences into a summary,(2) POLY\u2013an unsupervised summarizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM\u2013an unsupervised extension of POLY that compiles a document summary from compressed sentences. In this paper, we provide an overview of MUSEEC methods and its architecture in general.", "num_citations": "22\n", "authors": ["327"]}
{"title": "The uncertainty principle of cross-validation.\n", "abstract": " Data miners have often to deal with data sets of limited size due to economic, timing and other constraints. Usually their task is two-fold: to induce the most accurate model from a given dataset and to estimate the model's accuracy on future (unseen) examples. Crossvalidation is the most common approach to estimating the true accuracy of a given model and it is based on splitting the available sample between a training set and a validation set. The practical experience shows that any cross-validation method suffers from either an optimistic or a pessimistic bias in some domains. In this paper, we present a series of large-scale experiments on artificial and real-world datasets, where we study the relationship between the model's true accuracy and its cross-validation estimator. Two stable classification algorithms (ID3 and info-fuzzy network) are used for inducing each model. The results of our experiments have a striking resemblance to the well-known Heisenberg Uncertainty Principle: the more accurate is a model induced from a small amount of real-world data, the less reliable are the values of simultaneously measured cross-validation estimates. We suggest calling this phenomenon\" the uncertainty principle of cross-validation\".", "num_citations": "22\n", "authors": ["327"]}
{"title": "Improving data mining utility with projective sampling\n", "abstract": " Overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre-processing training examples, the CPU cost of model induction, and the cost of committed errors. Recently, several progressive sampling strategies for maximizing the overall data mining utility have been proposed. All these strategies are based on repeated acquisitions of additional training examples until a utility decrease is observed. In this paper, we present an alternative, projective sampling strategy, which fits functions to a partial learning curve and a partial run-time curve obtained from a small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size. The proposed approach is evaluated on a variety of benchmark datasets using the RapidMiner\u00a0\u2026", "num_citations": "21\n", "authors": ["327"]}
{"title": "A syntactic approach to domain-specific automatic question generation\n", "abstract": " Factoid questions are questions that require short fact-based answers. Automatic generation (AQG) of factoid questions from a given text can contribute to educational activities, interactive question answering systems, search engines, and other applications. The goal of our research is to generate factoid source-question-answer triplets based on a specific domain. We propose a four-component pipeline, which obtains as input a training corpus of domain-specific documents, along with a set of declarative sentences from the same domain, and generates as output a set of factoid questions that refer to the source sentences but are slightly different from them, so that a question-answering system or a person can be asked a question that requires a deeper understanding and knowledge than a simple word-matching. Contrary to existing domain-specific AQG systems that utilize the template-based approach to question generation, we propose to transform each source sentence into a set of questions by applying a series of domain-independent rules (a syntactic-based approach). Our pipeline was evaluated in the domain of cyber security using a series of experiments on each component of the pipeline separately and on the end-to-end system. The proposed approach generated a higher percentage of acceptable questions than a prior state-of-the-art AQG system.", "num_citations": "20\n", "authors": ["327"]}
{"title": "A simple, structure-sensitive approach for web document classification\n", "abstract": " In this paper we describe a new approach to classification of web documents. Most web classification methods are based on the vector space document representation of information retrieval. Recently the graph based web document representation model was shown to outperform the traditional vector representation using k-Nearest Neighbor (k-NN) classification algorithm. Here we suggest a new hybrid approach to web document classification built upon both, graph and vector representations. K-NN algorithm and three benchmark document collections were used to compare this method to graph and vector based methods separately. Results demonstrate that we succeed in most cases to outperform graph and vector approaches in terms of classification accuracy along with a significant reduction in classification time.", "num_citations": "20\n", "authors": ["327"]}
{"title": "Test set generation and reduction with artificial neural networks\n", "abstract": " Reducing the number of test cases results directly in the saving of software testing resources. Based on the success of Neural Networks as classifiers in many fields we propose to use neural networks for automated input-output analysis of data-driven programs. Identifying input-output relationships, ranking input features and building equivalence classes of input attributes for a given code are three important outcomes of this research in addition to reducing the number of test cases. The proposed methodology is based on the three-phase algorithm for efficient network pruning developed by R. Setiono and his colleagues. A detailed study shows that the neural network pruning and rule-extraction can significantly reduce the number of test cases.", "num_citations": "20\n", "authors": ["327"]}
{"title": "The theory and applications of generalized complex fuzzy propositional logic\n", "abstract": " The current definition of complex fuzzy logic has two limitations. First, the derivation uses complex fuzzy relations; hence, it assumes the existence of complex fuzzy sets. Second, current theory is based on a restricted polar representation of complex fuzzy proposition, where only one component of a complex fuzzy proposition carries fuzzy information. In this chapter we present a novel form of complex fuzzy logic. The new theory, referred to as generalized complex fuzzy logic, overcomes the limitations of the current theory and provides several advantages. First, the derivation of the new theory is based on axiomatic approach and does not assume the existence of complex fuzzy sets or complex fuzzy classes. Second, the new form supports Cartesian and polar representation of complex logical propositions with two components of fuzzy information. Hence, the new form significantly improves the expressive\u00a0\u2026", "num_citations": "19\n", "authors": ["327"]}
{"title": "Differentiation between viral and bacterial acute infections using chemiluminescent signatures of circulating phagocytes\n", "abstract": " Oftentimes the etiological diagnostic differentiation between viral and bacterial infections is problematic, while clinical management decisions need to be made promptly upon admission. Thus, alternative rapid and sensitive diagnostic approaches need to be developed. Polymorphonuclear leukocytes (PMNs) or phagocytes act as major players in the defense response of the host during an episode of infection, and thereby undergo functional changes that differ according to the infections. PMNs functional activity can be characterized by quantification and localization of respiratory burst production and assessed by chemiluminescent (CL) byproduct reaction. We have assessed the functional states of PMNs of patients with acute infections in a luminol-amplified whole blood system using the component CL approach. In this study, blood was drawn from 69 patients with fever (>38 \u00b0C), and diagnosed as mainly viral or\u00a0\u2026", "num_citations": "19\n", "authors": ["327"]}
{"title": "Detection of access to terror\u2010related Web sites using an Advanced Terror Detection System (ATDS)\n", "abstract": " Terrorist groups use the Web as their infrastructure for various purposes. One example is the forming of new local cells that may later become active and perform acts of terror. The Advanced Terrorist Detection System (ATDS), is aimed at tracking down online access to abnormal content, which may include terrorist\u2010generated sites, by analyzing the content of information accessed by the Web users. ATDS operates in two modes: the training mode and the detection mode. In the training mode, ATDS determines the typical interests of a prespecified group of users by processing the Web pages accessed by these users over time. In the detection mode, ATDS performs real\u2010time monitoring of the Web traffic generated by the monitored group, analyzes the content of the accessed Web pages, and issues an alarm if the accessed information is not within the typical interests of that group and similar to the terrorist interests\u00a0\u2026", "num_citations": "19\n", "authors": ["327"]}
{"title": "Design of test inputs and their sequences in multi-function system testing\n", "abstract": " This discussion paper addresses combinatorial models in system testing from the perspective of system usage (utilization) and corresponding examination of system functions and their groups. Thus the following aspects of multi-function system testing are under study: analysis of system requirements and revelation of atomic system functions and their relationships, analysis of system function groups (clusters), design of the most important test inputs and sequences of the test inputs. The basic combinatorial problem is: composition of the best (the most important) test input(s) for each group of atomic system functions. Additional combinatorial problems are the following: (a) design of test input sequence for a trail (chain) of function clusters, (b) design of collection of test input sequences as covering of function cluster digraph, (c) structural fusion of unit test results. Numerical and real world examples illustrate\u00a0\u2026", "num_citations": "19\n", "authors": ["327"]}
{"title": "Context-aware location prediction\n", "abstract": " Predicting the future location of mobile objects has become an important and challenging problem. With the widespread use of mobile devices, applications of location prediction include location-based services, resource allocation, handoff management in cellular networks, animal migration research, and weather forecasting. Most current techniques try to predict the next location of moving objects such as vehicles, people or animals, based on their movement history alone. However, ignoring the dynamic nature of mobile behavior may yield inaccurate predictions, at least part of the time. Analyzing movement in its context and choosing the best movement pattern by the current situation, can reduce some of the errors and improve prediction accuracy. In this chapter, we present a context-aware location prediction algorithm that utilizes various types of context information to predict future location of vehicles\u00a0\u2026", "num_citations": "18\n", "authors": ["327"]}
{"title": "Discrete complex fuzzy logic\n", "abstract": " In this paper we propose a complex fuzzy logic (CFL) system that is based on the extended Post (multi-valued logic) system (EPS) of order p >; 2, and demonstrate its utility for reasoning with fuzzy facts and rules. The advantage of this formalism is that it is discrete. Hence, it better fits real time applications, digital signal processing, and embedded systems that use integer processing units. Propositional calculus as well as first-order predicate calculus of EPS based CFL systems are developed. The application to approximate reasoning is described.", "num_citations": "18\n", "authors": ["327"]}
{"title": "Predicting future locations using clusters' centroids\n", "abstract": " As technology advances we encounter more available data on moving objects, thus increasing our ability to mine spatio-temporal data. We can use this data for learning moving objects behavior and for predicting their locations at future times according to the extracted movement patterns.", "num_citations": "18\n", "authors": ["327"]}
{"title": "A compact representation of spatio-temporal data\n", "abstract": " As technology advances we encounter more available data on moving objects, which can be mined to our benefit. In order to efficiently mine this large amount of data we propose an enhanced segmentation algorithm for representing a periodic spatio-temporal trajectory, as a compact set of minimal bounding boxes (MBBs). We also introduce a new, \"data-amount-based\" similarity measure between mobile trajectories which is compared empirically to an existing similarity measure by clustering spatio-temporal data and evaluating the quality of clusters and the execution times. Finally, we evaluate the values of segmentation thresholds used by the proposed segmentation algorithm through studying the tradeoff between running times and clustering validity as the segmentation resolution increases.", "num_citations": "18\n", "authors": ["327"]}
{"title": "Building graph-based classifier ensembles by random node selection\n", "abstract": " In this paper we introduce a method of creating structural (i.e.\u00a0graph-based) classifier ensembles through random node selection. Different k-Nearest Neighbor classifiers, based on a graph distance measure, are created automatically by randomly removing nodes in each prototype graph, similar to random feature subset selection for creating ensembles of statistical classifiers. These classifiers are then combined using a Borda ranking scheme to form a multiple classifier system. We examine the performance of this method when classifying a web document collection; experimental results show the proposed method can outperform a single classifier approach (using either a graph-based or vector-based representation).", "num_citations": "18\n", "authors": ["327"]}
{"title": "Change detection in classification models induced from time series data\n", "abstract": " Most classification methods are based on the assumption that the historic data involved in building and verifying the model is the best estimator of what will happen in the future. One important factor that must not be set aside is the time factor. As more data is accumulated into the problem domain, incrementally over time, one must examine whether the new data agrees with the previous datasets and make the relevant assumptions about the future. This work presents a new change detection methodology, with a set of statistical estimators. These changes can be detected independently of the data mining algorithm, which is used for constructing the corresponding model. By implementing the novel approach on a set of artificially generated datasets, all significant changes were detected in the relevant periods. Also, in the real-world datasets evaluation, the method produced similar results.", "num_citations": "18\n", "authors": ["327"]}
{"title": "Selecting a representative decision tree from an ensemble of decision-tree models for fast big data classification\n", "abstract": " The goal of this paper is to reduce the classification (inference) complexity of tree ensembles by choosing a single representative model out of ensemble of multiple decision-tree models. We compute the similarity between different models in the ensemble and choose the model, which is most similar to others as the best representative of the entire dataset. The similarity-based approach is implemented with three different similarity metrics: a syntactic, a semantic, and a linear combination of the two. We compare this tree selection methodology to a popular ensemble algorithm (majority voting) and to the baseline of randomly choosing one of the local models. In addition, we evaluate two alternative tree selection strategies: choosing the tree having the highest validation accuracy and reducing the original ensemble to five most representative trees. The comparative evaluation experiments are performed on six big datasets using two popular decision-tree algorithms (J48 and CART) and splitting each dataset horizontally into six different amounts of equal-size slices (from 32 to 1024). In most experiments, the syntactic similarity approach, named SySM\u2014Syntactic Similarity Method, provides a significantly higher testing accuracy than the semantic and the combined ones. The mean accuracy of SySM over all datasets is                                                                                                                                                            for CART and                                                                                                                                                            for J48. On the other hand, we find no statistically significant difference between the testing accuracy of the trees selected by\u00a0\u2026", "num_citations": "17\n", "authors": ["327"]}
{"title": "Classification of infectious diseases based on chemiluminescent signatures of phagocytes in whole blood\n", "abstract": " ObjectivesDespite medical advances, infectious diseases are still a major cause of mortality and morbidity, disability and socio-economic upheaval worldwide. Early diagnosis, appropriate choice and immediate initiation of antibiotic therapy can greatly affect the outcome of any kind of infection. Phagocytes play a central role in the innate immune response of the organism to infection. They comprise the first-line of defense against infectious intruders in our body, being able to produce large quantities of reactive oxygen species, which can be detected by means of chemiluminescence (CL). The data preparation approach implemented in this work corresponds to a dynamic assessment of phagocytic respiratory burst localization in a luminol-enhanced whole blood CL system. We have previously applied this approach to the problem of identifying various intra-abdominal pathological processes afflicting peritoneal\u00a0\u2026", "num_citations": "17\n", "authors": ["327"]}
{"title": "Towards multi-lingual summarization: A comparative analysis of sentence extraction methods on English and Hebrew corpora\n", "abstract": " The trend toward the growing multilinguality of the Internet requires text summarization techniques that work equally well in multiple languages. Only some of the automated summarization methods proposed in the literature, however, can be defined as \u201clanguageindependent\u201d, as they are not based on any morphological analysis of the summarized text. In this paper, we perform an in-depth comparative analysis of language-independent sentence scoring methods for extractive single-document summarization. We evaluate 15 published summarization methods proposed in the literature and 16 methods introduced in (Litvak et al., 2010). The evaluation is performed on English and Hebrew corpora. The results suggest that the performance ranking of the compared methods is quite similar in both languages. The top ten bilingual scoring methods include six methods introduced in (Litvak et al., 2010).", "num_citations": "16\n", "authors": ["327"]}
{"title": "Clustering of web documents using graph representations\n", "abstract": " In this paper we describe a clustering method that allows the use of graph-based representations of data instead of traditional vector-based representations. Using this new method we conduct content-based clustering of two web document collections. Clustering of web documents is performed to organize the documents with little or no human intervention. Benefits of clustering include easier browsing and improved retrieval speed. In order to measure the performance of our graph-matching approach, we compare it to the popular vector-based k-means method. We perform experiments using different graph distance measures as well as various document representations that utilize graphs. The results with the k-means clustering algorithm show that the graph-based approach can outperform traditional vector-based methods.", "num_citations": "16\n", "authors": ["327"]}
{"title": "Multi-objective classification with info-fuzzy networks\n", "abstract": " The supervised learning algorithms assume that the training data has a fixed set of predicting attributes and a single-dimensional class which contains the class label of each training example. However, many real-world domains may contain several objectives each characterized by its own set of labels. Though one may induce a separate model for each objective, there are several reasons to prefer a shared multi-objective model over a collection of single-objective models. We present a novel, greedy algorithm, which builds a shared classification model in the form of an ordered (oblivious) decision tree called Multi-Objective Info-Fuzzy Network (M-IFN). We compare the M-IFN structure to Shared Binary Decision Diagrams and bloomy decision trees and study the information-theoretic properties of the proposed algorithm. These properties are further supported by the results of empirical experiments, where\u00a0\u2026", "num_citations": "16\n", "authors": ["327"]}
{"title": "A comparative study of artificial neural networks and info-fuzzy networks as automated oracles in software testing\n", "abstract": " Software quality is one of the main concerns of software users. Hence, software testing is an utterly important phase in the software development life cycle. Nevertheless, manual evaluation of program compliance with its specification may be prohibitively time consuming. As a remedy, several software testing systems are using an automatic oracle to confirm that the developed software complies with its specification and determine whether a given test case exposes faults. The use of artificial neural networks and info-fuzzy networks as automated oracles has been explored elsewhere. Nevertheless, there is not enough research comparing these two popular approaches to automated evaluation of the test outcome. This paper fills the gap and reports on a set of experiments designed to compare the two methods based on ROC curves, training time, and dispersion analysis.", "num_citations": "15\n", "authors": ["327"]}
{"title": "Discovering regular groups of mobile objects using incremental clustering\n", "abstract": " As technology advances, detailed data on the position of moving objects, such as humans and vehicles is available. In order to discover groups of mobile objects that usually move in similar ways we propose an incremental clustering algorithm that clusters mobile objects according to similarity of their movement patterns. The proposed clustering algorithm uses a new, \"data-amount-based\" similarity measure between mobile trajectories. The clustering algorithm is evaluated on two spatio-temporal datasets using clustering validity measures.", "num_citations": "15\n", "authors": ["327"]}
{"title": "Classification of Web documents using concept extraction from ontologies\n", "abstract": " In this paper, we deal with the problem of analyzing and classifying web documents in a given domain by information filtering agents. We present the ontology-based web content mining methodology that contains such main stages as creation of ontology for the specified domain, collecting a training set of labeled documents, building a classification model in this domain using the constructed ontology and a classification algorithm, and classification of new documents by information agents via the induced model. We evaluated the proposed methodology in two specific domains: the chemical domain (web pages containing information about production of certain chemicals), and Yahoo! collection of web news documents divided into several categories. Our system receives as input the domain-specific ontology, and a set of categorized web documents, and then perfroms concept generalization on these\u00a0\u2026", "num_citations": "15\n", "authors": ["327"]}
{"title": "Design and implementation of a web mining system for organizing search engine results\n", "abstract": " We present the design and implementation of a web mining system that creates a hierarchical clustering of web documents retrieved by commercial web search engines. The cluster hierarchy is produced by a novel method called the Cluster Hierarchy Construction Algorithm (CHCA) and it can be used to explore the topics of interest related to the search query and their relationships. We discuss important design issues for our system, including stemming and dimensionality reduction, as well as some implementation details. We show examples of system results, compare them with results from similar systems, and analyze the responses to a survey of the system's users. \u00a9 2005 Wiley Periodicals, Inc. Int J Int Syst 20: 607\u2013625, 2005.", "num_citations": "15\n", "authors": ["327"]}
{"title": "A graph-based framework for web document mining\n", "abstract": " In this paper we describe methods of performing data mining on web documents, where the web document content is represented by graphs. We show how traditional clustering and classification methods, which usually operate on vector representations of data, can be extended to work with graph-based data. Specifically, we give graph-theoretic extensions of the k-Nearest Neighbors classification algorithm and the k-means clustering algorithm that process graphs, and show how the retention of structural information can lead to improved performance over the case of the vector model approach. We introduce several different types of web document representations that utilize graphs and compare their performance for clustering and classification.", "num_citations": "15\n", "authors": ["327"]}
{"title": "A term-based algorithm for hierarchical clustering of web documents\n", "abstract": " In this paper we introduce the novel class hierarchy construction algorithm (CHCA) in order to create hierarchical clusterings of Web documents. Unlike most clustering methods, CHCA operates on nominal data (the words occurring in each document) and it differs from other hierarchical clustering techniques in that it uses the object-oriented concept of inheritance to create the parent/child relationship between clusters. A prototype system has been developed using CHCA to create cluster hierarchies from web search results returned by conventional search engines. CHCA, without any guidance, creates term-based clusters from the contents of the retrieved pages and assigns each page to a cluster; the clusters correspond to topics and sub-topics in the investigated domain. The performance of our system is compared with a similar web search clustering system (Vivisimo).", "num_citations": "15\n", "authors": ["327"]}
{"title": "Evolving classification of intensive care patients from event data\n", "abstract": " ObjectiveThis work aims at predicting the patient discharge outcome on each hospitalization day by introducing a new paradigm\u2014evolving classification of event data streams. Most classification algorithms implicitly assume the values of all predictive features to be available at the time of making the prediction. This assumption does not necessarily hold in the evolving classification setting (such as intensive care patient monitoring), where we may be interested in classifying the monitored entities as early as possible, based on the attributes initially available to the classifier, and then keep refining our classification model at each time step (e.g., on daily basis) with the arrival of additional attributes.Materials and methodsAn oblivious read-once decision-tree algorithm, called information network (IN), is extended to deal with evolving classification. The new algorithm, named incremental information network (IIN\u00a0\u2026", "num_citations": "14\n", "authors": ["327"]}
{"title": "Incremental classification of nonstationary data streams\n", "abstract": " Traditional data mining algorithms used in the process of knowledge discovery are based on the assumption that the training data is a random sample drawn from a stationary distribution. In contrast, when dealing with non-stationary data sets, most of the processes generating time-stamped data may change drastically over time. Therefore, the results of a data-mining algorithm, which was run in the past, will be hardly relevant to the future data. In this paper, we present two new incremental approaches to real-time classification of continuous non-stationary data. The proposed approaches apply the data-mining algorithm repeatedly to a sliding window of examples in order to update the existing model, replace it by a former model, or construct a new model if a major concept drift is detected. The algorithms are evaluated on multi-year real-world streams of traffic data vs. the CVFDT online learning algorithm.", "num_citations": "14\n", "authors": ["327"]}
{"title": "Efficient graph-based representation of web documents\n", "abstract": " In this paper we describe a new approach to classification of web documents. Most web classification methods are based on the vector space document representation of information retrieval. Recently the graph based web document representation model was shown to outperform the traditional vector representation using k-Nearest Neighbor (k-NN) classification algorithm. Here we suggest two new hybrid approaches to web document classification built upon both graph and vector representations. K-NN algorithm and three benchmark document collections were used to compare this method to graph and vector based methods separately. The results demonstrate that we succeed in most cases to outperform graph and vector approaches in terms of classification accuracy along with a significant reduction in classification time.", "num_citations": "14\n", "authors": ["327"]}
{"title": "Multi-document summarization by extended graph text representation and importance refinement\n", "abstract": " Automatic multi-document summarization is aimed at recognizing important text content in a collection of topic-related documents and representing it in the form of a short abstract or extract. This chapter presents a novel approach to the multi-document summarization problem, focusing on the generic summarization task. The proposed SentRel (Sentence Relations) multi-document summarization algorithm assigns importance scores to documents and sentences in a collection based on two aspects: static and dynamic. In the static aspect, the significance score is recursively inferred from a novel, tripartite graph representation of the text corpus. In the dynamic aspect, the significance score is continuously refined with respect to the current summary content. The resulting summary is generated in the form of complete sentences exactly as they appear in the summarized documents, ensuring the summary's grammatical\u00a0\u2026", "num_citations": "13\n", "authors": ["327"]}
{"title": "Predicting and optimizing classifier utility with the power law\n", "abstract": " When data collection is costly and/or takes a significant amount of time, an early prediction of the classifier performance is extremely important for the design of the data mining process. Power law has been shown in the past to be a good predictor of decision- tree error rates as a function of the sample size. In this paper, we show that the optimal training set size for a given dataset can be computed from a learning curve characterized by a power law. Such a curve can be approximated using a small subset of potentially available data and then used to estimate the expected trade-off between the error rate and the amount of additional observations. The proposed approach to projected optimization of classifier utility is demonstrated and evaluated on several benchmark datasets.", "num_citations": "13\n", "authors": ["327"]}
{"title": "Interpretable decision-tree induction in a big data parallel framework\n", "abstract": " EN When running data-mining algorithms on big data platforms, a parallel, distributed framework, such as MAPREDUCE, may be used. However, in a parallel framework, each individual model fits the data allocated to its own computing node without necessarily fitting the entire dataset. In order to induce a single consistent model, ensemble algorithms such as majority voting, aggregate the local models, rather than analyzing the entire dataset directly. Our goal is to develop an efficient algorithm for choosing one representative model from multiple, locally induced decision-tree models. The proposed SySM (syntactic similarity method) algorithm computes the similarity between the models produced by parallel nodes and chooses the model which is most similar to others as the best representative of the entire dataset. In 18.75% of 48 experiments on four big datasets, SySM accuracy is significantly higher than that of the ensemble; in about 43.75% of the experiments, SySM accuracy is significantly lower; in one case, the results are identical; and in the remaining 35.41% of cases the difference is not statistically significant. Compared with ensemble methods, the representative tree models selected by the proposed methodology are more compact and interpretable, their induction consumes less memory, and, as confirmed by the empirical results, they allow faster classification of new records.", "num_citations": "12\n", "authors": ["327"]}
{"title": "Soft computing based epidemical crisis prediction\n", "abstract": " Epidemical crisis prediction is one of the most challenging examples of decision making with uncertain information. As in many other types of crises, epidemic outbreaks may pose various degrees of surprise as well as various degrees of \u201cderivatives\u201d of the surprise (i.e., the speed and acceleration of the surprise). Often, crises such as epidemic outbreaks are accompanied by a secondary set of crises, which might pose a more challenging prediction problem. One of the unique features of epidemic crises is the amount of fuzzy data related to the outbreak that spreads through numerous communication channels, including media and social networks. Hence, the key for improving epidemic crises prediction capabilities is in employing sound techniques for data collection, information processing, and decision making under uncertainty and exploiting the modalities and media of the spread of the fuzzy information\u00a0\u2026", "num_citations": "12\n", "authors": ["327"]}
{"title": "Optimizing a batch manufacturing process through interpretable data mining models\n", "abstract": " In this paper, we present a data mining based methodology for optimizing the outcome of a batch manufacturing process. Predictive data mining techniques are applied to a multi-year set of manufacturing data with the purpose of reducing the variation of a crystal manufacturing process, which suffers from frequent fluctuations of the average outgoing yield. Our study is focused on specific defects that are the most common causes for scraping a manufactured crystal. A set of probabilistic rules explaining the likelihood of each defect as a function of interaction between the controllable variables are induced using the single-target and the multi-target Information Network algorithms. The rules clearly define the worst and the best conditions for the manufacturing process, also providing a complete explanation of all major fluctuations in the outgoing quality observed over the recent years. In addition, we show that\u00a0\u2026", "num_citations": "12\n", "authors": ["327"]}
{"title": "Using Machine Learning Methods and Linguistic Features in Single-Document Extractive Summarization.\n", "abstract": " Extractive summarization of text documents usually consists of ranking the document sentences and extracting the top-ranked sentences subject to the summary length constraints. In this paper, we explore the contribution of various supervised learning algorithms to the sentence ranking task. For this purpose, we introduce a novel sentence ranking methodology based on the similarity score between a candidate sentence and benchmark summaries. Our experiments are performed on three benchmark summarization corpora: DUC-2002, DUC-2007 and MultiLing-2013. The popular linear regression model achieved the best results in all evaluated datasets. Additionally, the linear regression model, which included POS (Part-of-Speech)-based features, outperformed the one with statistical features only.", "num_citations": "11\n", "authors": ["327"]}
{"title": "Krimping texts for better summarization\n", "abstract": " Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results.", "num_citations": "11\n", "authors": ["327"]}
{"title": "Applied pattern recognition\n", "abstract": " A sharp increase in the computing power of modern computers, accompanied by a decrease in the data storage costs, has triggered the development of extremely powerful algorithms that can analyze complex patterns in large amounts of data within a very short period of time. Consequently, it has become possible to apply pattern recognition techniques to new tasks characterized by tight real-time requirements (eg, person identification) and/or high complexity of raw data (eg, clustering trajectories of mobile objects). The main goal of this book is to cover some of the latest application domains of pattern recognition while presenting novel techniques that have been developed or customized in those domains.", "num_citations": "11\n", "authors": ["327"]}
{"title": "Class Diagrams and Use Cases-Experimental Examination of the Preferred Order of Modeling\n", "abstract": " In most UML-based methodologies, the analysis tasks include mainly modeling the functional requirements using use cases, and modeling the problem domain using a class diagram. Different methodologies prescribe different orders of carrying out these tasks, and there is no commonly agreed order for performing them. In order to find out whether the order of these analysis activities makes any difference, and which order leads to better results, we carried out a comparative experiment. Subjects were asked to create the two analysis models for a certain system in two opposite orders, and the qualities of the produced models were then compared. The results of the experiment reveal that the class diagram is of better quality when created as the first modeling task, while no significant effect of the analysis order was found on the quality of the use cases. We also found out that analysts prefer starting the analysis with data modeling.", "num_citations": "11\n", "authors": ["327"]}
{"title": "A new approach for fuzzy clustering of web documents\n", "abstract": " Most existing methods of document clustering are based on the classical vector-space model, which represents each document by a fixed-size vector of key terms or key phrases. In large and diverse document collections such as the World Wide Web, this approach suffers from a tremendous computational overload, since the constant size of the term vector equals to the total number of key terms in all documents. We propose a new fuzzy-based approach to clustering documents that are represented by vectors of variable size. Each entry in a vector consists of two fields. The first field is the name of a key phrase in the document and the second denotes an importance weight associated with this key phrase within the particular document. We will describe the proposed approach in detail and show how it is implemented in a real world application from the area of web monitoring.", "num_citations": "11\n", "authors": ["327"]}
{"title": "A fuzzy-based algorithm for web document clustering\n", "abstract": " Most existing methods of document clustering are based on a model that assumes a fixed-size vector representation of key terms or key phrases within each document. This assumption is not realistic in large and diverse document collections such as the World Wide Web. We propose a new fuzzy-based document clustering method (FDCM), to cluster documents that are represented by variable length vectors. Each vector element consists of two fields. The first is an identification of a key phrase (its name) in the document and the second denotes a frequency associated with this key phrase within the particular document. A new averaging method is defined for the cluster centroid calculating, and a membership function is developed for relating new documents to existing clusters. The proposed approach is described in detail and we show how it is implemented in a real world application from the area of Web monitoring.", "num_citations": "11\n", "authors": ["327"]}
{"title": "Using data mining techniques for optimizing traffic signal plans at an urban intersection\n", "abstract": " A key problem in traffic engineering is the optimization of the flow of vehicles through urban intersections by improving the timing policy of traffic signals. Current methods of signal control policy are based on the junction topography and prespecified static traffic volumes. However, the actual daily traffic volumes can be affected by many time\u2010dependent factors making a static policy hardly optimal. In this paper, we induce nonstationary predictive models of traffic flow by applying novel methods of time\u2010series data mining to the traffic sensors data collected from a signalized intersection in Jerusalem over a period of 3 years. Our methodology for modeling dynamic traffic volumes combines clustering and segmentation algorithms. The results of a case study based on real\u2010world traffic data demonstrate that a dynamic signal policy using the data mining approach can produce a decrease of about 33.7% in the total\u00a0\u2026", "num_citations": "10\n", "authors": ["327"]}
{"title": "Condition-based maintenance with multi-target classification models\n", "abstract": " Condition-based maintenance (CBM) recommends maintenance actions based on the information collected through condition monitoring. In many modern cars, the condition of each subsystem can be monitored by onboard vehicle telematics systems. Prognostics is an important aspect in a CBM program as it deals with prediction of future faults. In this paper, we present a data mining approach to prognosis of vehicle failures. A multitarget probability estimation algorithm (M-IFN) is applied to an integrated database of sensor measurements and warranty claims with the purpose of predicting the probability and the timing of a failure in a given subsystem. The results of the multi-target\u00a0algorithm are shown to be superior to a singletarget probability estimation algorithm (IFN) and reliability modeling based on Weibull analysis.", "num_citations": "10\n", "authors": ["327"]}
{"title": "PCM-SABRE: a platform for benchmarking and comparing outcome prediction methods in precision cancer medicine\n", "abstract": " Numerous publications attempt to predict cancer survival outcome from gene expression data using machine-learning methods. A direct comparison of these works is challenging for the following reasons: (1) inconsistent measures used to evaluate the performance of different models, and (2) incomplete specification of critical stages in the process of knowledge discovery. There is a need for a platform that would allow researchers to replicate previous works and to test the impact of changes in the knowledge discovery process on the accuracy of the induced models. We developed the PCM-SABRE platform, which supports the entire knowledge discovery process for cancer outcome analysis. PCM-SABRE was developed using KNIME. By using PCM-SABRE to reproduce the results of previously published works on breast cancer survival, we define a baseline for evaluating future attempts to predict cancer outcome\u00a0\u2026", "num_citations": "9\n", "authors": ["327"]}
{"title": "Multilingual sentence extractor\n", "abstract": " The invention relates to a multilingual method for summarizing an article, which comprises an offline stage in which a weights vector is determined using, among others, plurality of predefined metrics, a collection of documents and expert prepared summaries, subjection of all the document sentences to all said metrics, guess of a population of weights matrices, subjection of the population to said metrics, ranking of sentences, generation of a new population using a genetic algorithm, and repetition of the same until convergence. The invention further comprises a real time stage in which the weights vector, as determined, as well as said metrics are used to determine an extract of any new document.", "num_citations": "9\n", "authors": ["327"]}
{"title": "Induction of mean output prediction trees from continuous temporal meteorological data\n", "abstract": " In this paper, we present a novel method for fast data-driven construction of regression trees from temporal datasets including continuous data streams. The proposed mean output prediction tree (MOPT) algorithm transforms continuous temporal data into two statistical moments according to a user-specified time resolution and builds a regression tree for estimating the prediction interval of the output (dependent) variable. Results on two benchmark data sets show that the MOPT algorithm produces more accurate and easily interpretable prediction models than other state-of-the-art regression tree methods.", "num_citations": "9\n", "authors": ["327"]}
{"title": "Measuring similarity between trajectories of mobile objects\n", "abstract": " With technological progress we encounter more available data on the locations of moving objects and therefore the need for mining moving objects data is constantly growing. Mining spatio-temporal data can direct products and services to the right customers at the right time; it can also be used for resources optimization or for understanding mobile patterns. In this chapter, we cluster trajectories in order to find movement patterns of mobile objects. We use a compact representation of a mobile trajectory, which is based on a list of minimal bounding boxes (MBBs). We introduce a new similarity measure between mobile trajectories and compare it empirically to an existing similarity measure by clustering spatio-temporal data and evaluating the quality of resulting clusters and the algorithm run times.", "num_citations": "9\n", "authors": ["327"]}
{"title": "Utilization of data-mining techniques for evaluation of patterns of asthma drugs use by ambulatory patients in a large health maintenance organization\n", "abstract": " A major problem of drugs utilization is to identify outlier patients who are using large quantities of drugs over extended periods of time. Today, healthcare and health insurance systems have to deal with an increased number of patients suffering from chronic diseases, such as asthma, who are continuously using a combination of several medications. This has caused a substantial increase in the cost of providing healthcare for such patients. In Israel, 11% of the national health care budget is spent on medications. However, healthcare management operations do not have the information that can assist in determining whether extensive multi-year drug utilization by a chronic patient is an outlier or misuse of resources. In this work, we construct a prediction model for asthma drug utilization by applying novel methods of knowledge discovery in time-series databases to a multi-year asthma drug utilization data set\u00a0\u2026", "num_citations": "9\n", "authors": ["327"]}
{"title": "Comparison of algorithms for web document clustering using graph representations of data\n", "abstract": " In this paper we compare the performance of several popular clustering algorithms, including k-means, fuzzy c-means, hierarchical agglomerative, and graph partitioning. The novelty of this work is that the objects to be clustered are represented by graphs rather than the usual case of numeric feature vectors. We apply these techniques to web documents, which are represented by graphs instead of vectors, in order to perform web document clustering. Web documents are structured information sources and thus appropriate for modeling by graphs. We will examine the performance of each clustering algorithm when the web documents are represented as both graphs and vectors. This will allow us to investigate the applicability of each algorithm to the problem of web document clustering.", "num_citations": "9\n", "authors": ["327"]}
{"title": "Perception-based analysis of engineering experiments in the semiconductor industry\n", "abstract": " Comparing frequency distributions of experimental data is a routine engineering task in the semiconductor industry. The existing statistical approaches to the problem suffer from several limitations, which can be partially overcome via the time-consuming visual examination of frequency histograms by an experienced process engineer. This paper presents a novel, fuzzy-based method for automating the cognitive process of comparing frequency histograms. We use the evolving approach of type-2 fuzzy logic to utilize the domain knowledge of human experts. The proposed method is evaluated on the actual results of an engineering experiment, where it is shown to represent the experts' perception of the visualized data more accurately than a wide range of statistical tests. We also outline the potential directions for integrating the perception-based approach with other methods of data visualization and data mining.", "num_citations": "9\n", "authors": ["327"]}
{"title": "Identifying turning points in animated cartoons\n", "abstract": " Detecting key story elements such as protagonist, opponent, desire, turning points, battle, and victory, etc. is essential for various narrative work applications including content retrieval and content recommendation systems. The task of automatically identifying story elements is challenging because of its complexity and subjectiveness and currently, there are no available algorithms for this task. In this paper, we focus on identifying turning points in a story of a cartoon movie. The proposed methodology extends the novel two-clocks theory, originally validated on scripts of theatre plays, to video stories. The assumption behind the two-clocks theory is that the perception of time is different when some special event happens to a certain agent (e.g., time flows slower for a patient and quicker for a tourist). The story timeline is monitored with two clocks: an event clock, which measures the regular time flow of the story; and a\u00a0\u2026", "num_citations": "8\n", "authors": ["327"]}
{"title": "Web intelligence and security: advances in data and text mining techniques for detecting and preventing terrorist activities on the web\n", "abstract": " Terrorists are continuously learning to use the Internet as an accessible and costeffective information infrastructure. Secure and non-secure web sites, online forums, and file-sharing services are routinely used by terrorist groups for spreading their propaganda, recruiting new members, and communicating with their supporters, along with sharing knowledge on forgery, explosive preparation, and other\" core\" terrorist activities. The current number of known terrorist sites and active extremist forums is so large and their URL addresses are so volatile that a constant manual monitoring of their multilingual content is definitely out of the question. Moreover, terrorist web sites often try to conceal their real identity, eg, by masquerading themselves as news portals or religious forums. This is why automated Web Intelligence and Web Mining methods are so important for efficiently securing the Web against its misuse by terrorists and other dangerous criminals.This book contains chapters by the key speakers of the NATO Advanced Research Workshop on Web Intelligence and Security that took place on November 18-20, 2009 in Ein-Bokek, Israel. The goal of the Advanced Research Workshop was to bring together scientists and practitioners interested in recent developments in exploiting data and text mining techniques for countering terrorist activities on the Web. The emphasis was placed on presenting available methods and tools that can alleviate the information overload of intelligence and security experts. The main areas of discussion included terrorism origins, the threats of the\" Dark Web\", web content mining and Open Source Intelligence (OSI), text\u00a0\u2026", "num_citations": "8\n", "authors": ["327"]}
{"title": "Dynamic component chemiluminescent sensor for assessing circulating polymorphonuclear leukocyte activity of peritoneal dialysis patients\n", "abstract": " Recurrent bacterial peritonitis is a major complication in peritoneal dialysis (PD) patients, which is associated with polymorphonuclear leukocyte (PMN) functional changes and can be assessed by a chemiluminescent (CL) reaction. We applied a new approach of a dynamic component chemiluminescence sensor for the assessment of functional states of PMNs in a luminol-amplified whole-blood system. This method is based on the evaluation of CL kinetic patterns of stimulated PMNs, while the parallel measurements of intracellular and extracellular production of reactive oxygen species (ROS) from the same sample can be conducted. Blood was drawn from diabetic and nondiabetic patients during follow-up, and during peritonitis. Healthy medical personnel served as the control group. Chemiluminescence curves were recorded and presented as a sum of three biological components. CL kinetic parameters were\u00a0\u2026", "num_citations": "8\n", "authors": ["327"]}
{"title": "Data mining for software testing\n", "abstract": " Software testing activities are usually planned by human experts, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outcomes is also associated with a considerable effort by software testers who may have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world\u2019s economy. As demonstranted in this chapter, Data Mining algorithms can be efficiently used for automated modeling of tested systems. Induced Data Mining models can be utilized for recovering system requirements, identifying equivalence classes in system inputs, designing a minimal set of regression tests, and evaluating the correctness of software outputs.", "num_citations": "8\n", "authors": ["327"]}
{"title": "Automated dimensionality reduction of data warehouses.\n", "abstract": " A data warehouse is designed to consolidate and maintain all attributes that are relevant for the analysis processes. Due to the rapid increase in the size of the modern operational systems, it becomes neither practical, nor necessary to load and maintain in the data warehouse every operational attribute. This paper presents a novel methodology for automated selection of the most relevant independent attributes in a data warehouse. The method is based on the information-theoretic approach to knowledge discovery in databases. Attributes are selected by a stepwise forward procedure aimed at minimizing the uncertainty in the values of key performance indicators (KPI\u2019s). Each selected attribute is assigned a score, expressing its degree of relevance. Using the method does not require any prior expertise in the domain of the data and it can be equally applied to nominal and ordinal attributes. An attribute will be included in a data warehouse schema, if it is found as relevant to at least one KPI. We demonstrate the applicability of the method by reducing the dimensionality of a direct marketing database.", "num_citations": "8\n", "authors": ["327"]}
{"title": "Applying fuzzy hypothesis testing to medical data\n", "abstract": " Classical statistics and many data mining methods rely on \u201cstatistical significance\u201d as a sole criterion for evaluating alternative hypotheses. In this paper, we use a novel, fuzzy logic approach to perform hypothesis testing. The method involves four major steps: hypothesis formulation, data selection (sampling), hypothesis testing (data mining), and decision (results). In the hypothesis formulation step, a null hypothesis and set of alternative hypotheses are created using conjunctive antecedents and consequent functions. In the data selection step, a subset D of the set of all data in the database is chosen as a sample set. This sample should contain enough objects to be representative of the data to a certain degree of satisfaction. In the third step, the fuzzy implication is performed for the data in D for each hypothesis and the results are combined using some aggregation function. These results are used in the\u00a0\u2026", "num_citations": "8\n", "authors": ["327"]}
{"title": "Listen to the sound of data\n", "abstract": " Timestamped observations, generally known as time series, may contain valuable information about a variety of natural and man-made phenomena ranging from weather changes to stock markets. Our capability to collect such data has increased dramatically due to advances in computing and sensory technologies. Visualization is known as a very effective tool for interactive data exploration tasks. In this research, we have tested the hypothesis that musical sonification (the use of musical audio) can serve as a viable alternative to visualization of time-series data whenever the visual representation is unavailable or impossible to use. We have developed a time-series sonification technique, which utilizes some important features of Western tonal music to convert univariate and multivariate time series into a musical equivalent. The technique was used to conduct two online user studies, where the subjects\u00a0\u2026", "num_citations": "7\n", "authors": ["327"]}
{"title": "Multilingual Single-Document Summarization with MUSE\n", "abstract": " MUltilingual Sentence Extractor (MUSE) is aimed at multilingual single-document summarization. MUSE implements a supervised language-independent summarization approach based on optimization of multiple sentence ranking methods using a Genetic Algorithm. The main advantage of MUSE is its language-independency\u2013it is using statistical sentence features, which can be calculated for sentences in any language.In our previous work, the performance of MUSE was found to be significantly better than the best known state-of-the-art extractive summarization approaches and tools in three different languages: English, Hebrew, and Arabic. Moreover, our experimental results in the cross-lingual domain suggest that MUSE does not need to be retrained on a summarization corpus in each new language, and the same weighting model can be used across several languages (Last and Litvak, 2012).", "num_citations": "7\n", "authors": ["327"]}
{"title": "Special issue on advances in fuzzy logic\n", "abstract": " Special issue on advances in fuzzy logic \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced search Browse series books journals \u00d7 \u2026", "num_citations": "7\n", "authors": ["327"]}
{"title": "Improving classification of multi-lingual web documents using domain ontologies\n", "abstract": " In this paper, we deal with the problem of analyzing and classifying web documents to several major categories/classes in a given domain using domain ontology. We present the ontology-based web content mining methodology that contains such main stages as collecting a training set of labeled documents from a given domain, building a classification model above this domain given the domain ontology, and classification of new documents via the induced model. We tested the proposed methodology in a specific domain, namely web pages containing information about production of certain chemicals. Using our methodology, we are interested to identify all relevant web documents while ignoring the documents that do not contain any relevant information. Our system receives as input an OWL file built in Protege tool, which contains the domain-specific ontology, and a set of web documents classified by a human expert as\u201d relevant\u201d or\u201d non-relevant\u201d. We use a language-independent key-phrase extractor with integrated ontology parser (defined in a given language) for creating the database from input documents and use it as a training set for the classification algorithm. The system classification accuracy using various levels of ontology is evaluated. The current version of our system supports web content mining in English, Arabic, Russian, and Hebrew languages.", "num_citations": "7\n", "authors": ["327"]}
{"title": "Multi-function system testing: composition of test sets\n", "abstract": " This paper focuses on the following aspects of multi-function system testing: analysis of system requirements and revelation of atomic system functions and their relationships, analysis of input/output variables, analysis of system function groups (clusters), composition of the test sets for each group of atomic system functions. Related problems associated with these aspects are briefly described. Numerical and real world examples illustrate the proposed approach.", "num_citations": "7\n", "authors": ["327"]}
{"title": "Fuzzification and reduction of information-theoretic rule sets\n", "abstract": " If-then rules are one of the most common forms of knowledge discovered by data mining methods. The number and the length of extracted rules tend to increase with the size of a database, making the rulesets less interpretable and useful. Existing methods of extracting fuzzy rules from numerical data improve the interpretability aspect, but the dimensionality of fuzzy rulesets remains high. In this paper, we present a new methodology for reducing the dimensionality of rulesets discovered in data. Our method builds upon the information-theoretic fuzzy approach to knowledge discovery. We start with constructing an information-theoretic network from a data table and extracting a set of association rules based on the network connections. The set of information-theoretic rules is fuzzified and significantly reduced by using the principles of the Computational Theory of Perception (CTP). We demonstrate the method\u00a0\u2026", "num_citations": "7\n", "authors": ["327"]}
{"title": "Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures\n", "abstract": " The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.", "num_citations": "6\n", "authors": ["327"]}
{"title": "An unsupervised constrained optimization approach to compressive summarization\n", "abstract": " Automatic summarization is typically aimed at selecting as much information as possible from text documents using a predefined number of words. Extracting complete sentences into a summary is not an optimal way to solve this problem due to redundant information that is contained in some sentences. Removing the redundant information and compiling a summary from compressed sentences should provide a much more accurate result. Major challenges of compressive approaches include the cost of creating large summarization corpora for training the supervised methods, the linguistic quality of compressed sentences, the coverage of the relevant content, and the time complexity of the compression procedure. In this work, we attempt to address these challenges by proposing an unsupervised polynomial-time compressive summarization algorithm. The proposed algorithm iteratively removes redundant parts\u00a0\u2026", "num_citations": "6\n", "authors": ["327"]}
{"title": "Hypotensive episode prediction in ICUs via observation window splitting\n", "abstract": " Hypotension, defined as dangerously low blood pressure, is a significant risk factor in intensive care units (ICUs), which requires a prompt therapeutic intervention. The goal of our research is to predict an impending Hypotensive Episode (HE) by time series analysis of continuously monitored physiological vital signs. Our prognostic model is based on the last Observation Window (OW) at the prediction time. Existing clinical episode prediction studies used a single OW of 5\u2013120\u00a0min to extract predictive features, with no significant improvement reported when longer OWs were used. In this work we have developed the In-Window Segmentation (InWiSe) method for time series prediction, which splits a single OW into several sub-windows of equal size. The resulting feature set combines the features extracted from each observation sub-window and then this combined set is used by the Extreme Gradient\u00a0\u2026", "num_citations": "6\n", "authors": ["327"]}
{"title": "Early Warning from Car Warranty Data using a Fuzzy Logic Technique\n", "abstract": " Car manufacturers are interested to detect evolving problems in a car fleet as early as possible so they can take preventive actions and deal with the problems before they become widespread. The vast amount of warranty claims recorded by the car dealers makes the manual process of analyzing this data hardly feasible. This chapter describes a fuzzy-based methodology for automated detection of evolving maintenance problems in massive streams of car warranty data. The empirical distributions of time-to-failure and mileage-to-failure are monitored over time using the advanced, fuzzy approach to comparison of frequency distributions. The authors\u2019 fuzzy-based early warning tool builds upon an automated interpretation of the differences between consecutive histogram plots using a cognitive model of human perception rather than \u201ccrisp\u201d statistical models. They demonstrate the effectiveness and the efficiency of\u00a0\u2026", "num_citations": "6\n", "authors": ["327"]}
{"title": "Polynomial time complexity graph distance computation for web content mining\n", "abstract": " Utilizing graphs with unique node labels reduces the complexity of the maximum common subgraph problem, which is generally NP-complete, to that of a polynomial time problem. Calculating the maximum common subgraph is useful for creating a graph distance measure, since we observe that graphs become more similar (and thus have less distance) as their maximum common subgraphs become larger and vice versa. With a computationally practical method of determining distances between graphs, we are no longer limited to using simpler vector representations for machine learning applications.We can perform well-known algorithms, such as k-means clustering and k-nearest neighbors classification, directly on data represented by graphs, losing none of the inherent structural information. We demonstrate the benefits of the additional information retained in a graph-based data model for web\u00a0\u2026", "num_citations": "6\n", "authors": ["327"]}
{"title": "Knowledge Discovery in Mortality Records: An Info-Fuzzy Approach\n", "abstract": " The original dataset is extended to a fuzzy relational table by adding a new, fuzzy attribute: the reliability degree of the recorded diagnosis. Our approach to the calculation of the reliability degree is based on the Computational Theory of Perception (CTP). Most records having the lowest reliability degree are shown to represent exceptional and possibly inaccurate information.", "num_citations": "6\n", "authors": ["327"]}
{"title": "Using Graphs for Word Embedding with Enhanced Semantic Relations\n", "abstract": " Word embedding algorithms have become a common tool in the field of natural language processing. While some, like Word2Vec, are based on sequential text input, others are utilizing a graph representation of text. In this paper, we introduce a new algorithm, named WordGraph2Vec, or in short WG2V, which combines the two approaches to gain the benefits of both. The algorithm uses a directed word graph to provide additional information for sequential text input algorithms. Our experiments on benchmark datasets show that text classification algorithms are nearly as accurate with WG2V as with other word embedding models while preserving more stable accuracy rankings.", "num_citations": "5\n", "authors": ["327"]}
{"title": "Preserving differential privacy and utility of non-stationary data streams\n", "abstract": " Data publishing poses many challenges regarding the efforts to preserve data privacy, on one hand, and maintain its high utility, on the other hand. The Privacy Preserving Data Publishing field (PPDP) has emerged as a possible solution to such trade-off, allowing data miners to analyze the published data, while providing a sufficient degree of privacy. Most existing anonymization platforms deal with static and stationary data, which can be scanned at least once before its publishing. More and more real-world applications generate streams of data which can be non-stationary, i.e., subject to a concept drift. In this paper, we introduce MiDiPSA (Microaggregation-based Differential Private Stream Anonymization) algorithm for non-stationary data streams, which aims at satisfying the constraints of k-anonymity, recursive (c, l)-diversity, and differential privacy while minimizing the information loss and the possible\u00a0\u2026", "num_citations": "5\n", "authors": ["327"]}
{"title": "MIL: Automatic Metaphor Identification by Statistical Learning.\n", "abstract": " Metaphor identification in text is an open problem in natural language processing. In this paper, we present a new, supervised learning approach called MIL (Metaphor Identification by Learning), for identifying three major types of metaphoric expressions without using any knowledge resources or handcrafted rules. We derive a set of statistical features from a corpus representing a given domain (eg, news articles published by Reuters). We also use an annotated set of sentences, which contain candidate expressions labelled as' metaphoric'or'literal'by native English speakers. Then we induce a metaphor identification model for each expression type by applying a classification algorithm to the set of annotated expressions. The proposed approach is evaluated on a set of annotated sentences extracted from a corpus of Reuters articles. We show a significant improvement vs. a state-of-the-art learning-based algorithm and comparable results to a recently presented rule-based approach.", "num_citations": "5\n", "authors": ["327"]}
{"title": "MUSE\u2013A Multilingual Sentence Extractor\n", "abstract": " MUltilingual Sentence Extractor (MUSE) is aimed at multilingual single-document summarization. MUSE implements the supervised language-independent summarization approach based on optimization of multiple statistical sentence ranking methods. The MUSE tool consists of two main modules: the training module activated in the offline mode, and the on-line summarization module. The training module can be provided with a corpus of summarized texts in any language. Then, it learns the best linear combination of user specified sentence ranking measures applying a Genetic Algorithm to the given training data. The summarization module performs real-time sentence extraction by computing sentence rankings according to the weighted model induced in the training phase. The main advantage of MUSE is its language-independency\u2013it can be applied to any language given a gold standard summaries in that language. The performance of MUSE in our previous works was found to be significantly better than the best known state-of-theart extractive summarization approaches and tools in the three different languages: English, Hebrew, and Arabic. Moreover, our experimental results in a cross-lingual domain suggest that MUSE does not need to be retrained on each new language, and the same weighting model can be used across several languages.", "num_citations": "5\n", "authors": ["327"]}
{"title": "Predicting Wine Quality from Agricultural Data with Single-Objective and Multi-Objective Data Mining Algorithms\n", "abstract": " Wine quality is determined by a series of complex chemical processes. Factors affecting grape and wine performance range from climate conditions during the growing period to harvesting decisions controlled by humans. In this chapter, we apply single-objective and multi-objective classification algorithms for prediction of grape and wine quality in a multi-year agricultural database maintained by Yarden-Golan Heights Winery in Katzrin, Israel. The goal of the study is to discover relationships between 138 agricultural and meteorological attributes collected or derived during a single season and 27 dependent parameters measuring grapevine and wine quality. We have induced ordered (oblivious) decision-tree models from the target dataset using information-theoretic classification algorithms. The induced models, called single-objective and multi-objective information networks, have been combined into multi-level information graphs, each level standing for a different stage of the wine production process. The results clearly demonstrate the hitherto unexploited potential of the KDD technology for knowledge discovery in agricultural data.", "num_citations": "5\n", "authors": ["327"]}
{"title": "Predicting stock trends with time series data mining and web content mining\n", "abstract": " This paper presents a new methodology for predicting stock trends and making trading decisions based on the combination of Data Mining and Web Content Mining techniques. While research in both areas is quite extensive, inference from time series stock data and time-stamped news stories collected from the World Wide Web require further exploration. Our prediction models are based on the content of time-stamped web documents in addition to traditional Numerical Time Series Data. The stock trading system based on the proposed methodology (ADMIRAL) will be simulated and evaluated on real-world series of news stories and stocks data using several known classification algorithms. The main performance measures will be the prediction accuracy of the induced models and, more importantly, the profitability of the investments made by using system recommendations based on these predictions.", "num_citations": "5\n", "authors": ["327"]}
{"title": "Advances in Web Intelligence: Third International Atlantic Web Intelligence Conference, AWIC 2005, Lodz, Poland, June 6-9, 2005, Proceedings\n", "abstract": " In recent years the Internet has become a source of data and information of indisputable importance and has immensely gained in acceptance and popularity. The World Wide Web (WWW or Web, for short), frequently named \u201cthe nervous system of the infor-tion society,\u201d offers numerous valuable services leaving no doubt about the signi? cance of the Web in our daily activities at work and at home. Consequently, we have a clear aspiration to meet the obvious need for effective use of its potential by making-provements in both the methods and the technology applied. Among the new research directions observable in Web-related applications, intelligent methods from within the broadly perceived topic of soft computing occupy an important place. AWIC, the \u201cAtlantic Web Intelligence Conferences\u201d are intended to be a forum for exchange of new ideas and novel practical solutions in this new and exciting? eld. The conference was born as an initiative of the WIC-Poland and the WIC-Spain Research Centres, both belonging to the Web Intelligence Consortium\u2013WIC (http://wi-consortium. org/). So far, three AWIC conferences have been held: in Madrid, Spain (2003), in Cancun, Mexico (2004), and in? \u00f3dz, \u0301 Poland (2005).", "num_citations": "5\n", "authors": ["327"]}
{"title": "Automated quality assurance of continuous data\n", "abstract": " Most real-world databases contain some amount of inaccurate data. Reliability of critical attributes can be evaluated from the values of other attributes in the same data table. This paper presents a new fuzzy-based measure of data reliability in continuous attributes. We partition the relational schema of a database into a subset of input (predicting) and a subset of target (dependent) attributes. A data mining model, called information-theoretic connectionist network, is constructed for predicting the values of a continuous target attribute. The network calculates the degree of reliability of the actual target values in each record by using their distance from the predicted values. The approach is demonstrated on the voting data from the 2000 Presidential Elections in the US.", "num_citations": "5\n", "authors": ["327"]}
{"title": "Color face segmentation using a fuzzy min-max neural network\n", "abstract": " This work presents an automated method of segmentation of faces in color images with complex backgrounds. Segmentation of the face from the background in an image is performed by using face color feature information. Skin regions are determined by sampling the skin colors of the face in a Hue Saturation Value (HSV) color model, and then training a fuzzy min-max neural network (FMMNN) to automatically segment these skin colors. This work appears to be the first application of Simpson's FMMNN algorithm to the problem of face segmentation. Results on several test cases showed recognition rates of both face and background pixels to be above 93%, except for the case of a small face embedded in a large background. Suggestions for dealing with this difficult case are proffered. The image pixel classifier is linear of order O(Nh) where N is the number of pixels in the image and h is the number of fuzzy\u00a0\u2026", "num_citations": "5\n", "authors": ["327"]}
{"title": "Fuzzy comparison of frequency distributions\n", "abstract": " Comparing empirical distributions is one of the fundamental tasks in data analysis. We start with a survey of existing statistical approaches to this problem. The current numeric methods are shown to suffer from several limitations, including restrictive assumptions about the underlying distributions and non-use of available domain knowledge. These limitations can be partially overcome via the time-consuming visual examination of frequency histograms by a human expert. In this paper, we present a fuzzy-based method for automating the process of comparing frequency histograms. Our approach builds upon a novel concept of automated perceptions, introduced in our previous work. We use the evolving approach of type-2 fuzzy logic for representing the domain knowledge of human experts. The proposed method provides an automated interpretation of the differences between histogram plots, based on a\u00a0\u2026", "num_citations": "5\n", "authors": ["327"]}
{"title": "Fuzzy approach to data reliability\n", "abstract": " A novel, fuzzy approach to deal with reliability of attribute values in a relational database is presented. The degree of reliability is defined as a fuzzy measure of certainty that the data is correct from user\u2019s point of view. The relation scheme is partitioned into a subset of input (completely reliable) and a subset of target (partially reliable) attributes. An information theoretic connectionist network is constructed to evaluate the information content of links between input and target attributes. The network connection weights are used to calculate the reliability degrees of target attribute values. The method is applied to a real-world database, which includes partially reliable information. This work aims at improving reliability of data in a relational database by developing a framework for evaluating and representing reliability of attribute values in database tuples.", "num_citations": "5\n", "authors": ["327"]}
{"title": "An Information-Theoretic Approach to Data Mining\n", "abstract": " The Knowledge Discovery in Databases (KDD) process is defined by Fayyad et al.[5] as \u201cthe nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data\u201d. Since the potential number of patterns presenting in a real-world database (containing, at least, tens of attributes and thousands of records) may be nearly infinite, most data mining techniques are concerned with a computationally-efficient enumeration of patterns (models, causation relations, association rules, etc.) to discover the most useful patterns for the user or the task.Our work aims at finding a minimal set of database attributes involved in discovered patterns by using an information-theoretic statistical approach. The problem of minimizing the number of attributes is important for several reasons. First, data mining requires a significant effort associated with data preparation (including the rapidly evolving approach of data warehousing), and this effort can be decreased, if the unimportant attributes are filtered out. Secondly, the number of rules is expected to go down almost exponentially with decreasing the number of attributes. Finally, less attributes in a rule make it shorter and more understandable.", "num_citations": "5\n", "authors": ["327"]}
{"title": "CryptoRNN-privacy-preserving recurrent neural networks using Homomorphic encryption\n", "abstract": " Recurrent Neural Networks (RNNs) are used extensively for mining sequential datasets. However, performing inference over an RNN model requires the data owner to expose his or her raw data to the machine learning service provider. Homomorphic encryption allows calculations to be performed on ciphertexts, where the decrypted result is the same as if the calculation has been made directly on the plaintext. In this research, we suggest a Privacy-Preserving RNN\u2013based inference system using homomorphic encryption. We preserve the functionality of RNN and its ability to make the same predictions on sequential data, within the limitations of homomorphic encryption, as those obtained for plaintext on the same RNN model. In order to achieve this goal, we need to address two main issues. First, the noise increase between successive calculations and second, the inability of homomorphic encryption to work\u00a0\u2026", "num_citations": "4\n", "authors": ["327"]}
{"title": "Using discretization for extending the set of predictive features\n", "abstract": " To date, attribute discretization is typically performed by replacing the original set of continuous features with a transposed set of discrete ones. This paper provides support for a new idea that discretized features should often be used in addition to existing features and as such, datasets should be extended, and not replaced, by discretization. We also claim that discretization algorithms should be developed with the explicit purpose of enriching a non-discretized dataset with discretized values. We present such an algorithm, D-MIAT, a supervised algorithm that discretizes data based on minority interesting attribute thresholds. D-MIAT only generates new features when strong indications exist for one of the target values needing to be learned and thus is intended to be used in addition to the original data. We present extensive empirical results demonstrating the success of using D-MIAT on 28 benchmark datasets\u00a0\u2026", "num_citations": "4\n", "authors": ["327"]}
{"title": "Data Mining in Time Series and Streaming Databases\n", "abstract": " This compendium is a completely revised version of an earlier book, Data Mining in Time Series Databases, by the same editors. It provides a unique collection of new articles written by leading experts that account for the latest developments in the field of time series and data stream mining. The emerging topics covered by the book include weightless neural modeling for mining data streams, using ensemble classifiers for imbalanced and evolving data streams, document stream mining with active learning, and many more. In particular, it addresses the domain of streaming data, which has recently become one of the emerging topics in Data Science, Big Data, and related areas. Existing titles do not provide sufficient information on this topic.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Language-independent Techniques for Automated Text Summarization\n", "abstract": " Text summarization is the process of distilling the most important information from source/sources to produce an abridged version for a particular user/users and task/tasks. Automatically generated summaries can significantly reduce the information overload on intelligence analysts in their daily work. Moreover, automated text summarization can be utilized for automated classification and filtering of text documents, information search over the Internet, content recommendation systems, online social networks, etc.", "num_citations": "4\n", "authors": ["327"]}
{"title": "A spatio-temporal simulation model for movement data generation\n", "abstract": " The real-world process of generating a large spatio-temporal data collection presents a very difficult technical problem. First, this process is very expensive, requiring a lot of various high-technology software tools and modern hardware infrastructure (sensors, servers, GPS infrastructure etc.) installations; second, the recorded trajectories sometimes cannot represent any special traffic or movement patterns. The simulation framework introduced in this paper can generate diverse trajectory datasets based on predetermined movement patterns.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Multi-lingual detection of Web terrorist content\n", "abstract": " The role of the Internet in the infrastructure of the global terrorist organizations is increasing dramatically. Beyond propaganda, the WWW is being heavily used for practical training, fundraising, communication, and other purposes. Terrorism experts are interested in identifying who is behind the material posted on terrorist web sites and online forums and what links they have to active terror groups. The current number of known terrorist sites is so large and their URL addresses are so volatile that a continuous manual monitoring of their multilingual content is definitely out of question. Moreover, terrorist web sites and forums often try to conceal their real identity. This is why automated multi-lingual detection methods are so important in the cyber war against the international terror. In this chapter, we describe a classification-based approach to multi-lingual detection and categorization of terrorist documents. The\u00a0\u2026", "num_citations": "4\n", "authors": ["327"]}
{"title": "Advances in web intelligence and data mining\n", "abstract": " Today, in the middle of the? rst decade of the 21st century, the Internet has become a major communication medium, where virtually any kind of content can be transferred instantly and reliably between individual users and entire-ganizations located in any part of the globe. The World Wide Web (WWW) has a tremendous e? ect on our daily activities at work and at home. Consequently, more e? ective and e? cient methods and technologies are needed to make the most of the Web\u2019s nearly unlimited potential. The new Web-related research directions include intelligent methods usually associated with the? elds of c-putational intelligence, soft computing, and data mining. AWIC, the \u201cAtlantic Web Intelligence Conferences\u201d continue to be a forum for exchange of new ideas and novel practical solutions in this new and exciting area. The conference was born as an initiative of the WIC-Poland and the WIC-Spain Research Centers, both belonging to the Web Intelligence Consortium\u2013WIC (http://wi-consortium. org/). Prior to this year, three AWIC conferences have been held: in Madrid, Spain (2003), in Cancun, Mexico (2004), and in L \u0301 od \u0301 z, Poland (2005). AWIC 2006 took place in Beer-Sheva, Israel during June 5\u20137, 2006, organized locally by Ben-Gurion University of the Negev. The book presents state-of-the-art developments in the? eld of computati-ally intelligent methods applied to various aspects and ways of Web exploration.", "num_citations": "4\n", "authors": ["327"]}
{"title": "An Apriori-like algorithm for Extracting Fuzzy Association Rules between Keyphrases in Text Documents\n", "abstract": " In this paper we present an algorithm for extracting fuzzy association rules between weighted keyphrases in collections of text documents. First, we discuss some classical approaches to association rule extraction and then we show the fuzzy association rules algorithm. The proposed method integrates the fuzzy set concept and the apriori algorithm. The algorithm emphasizes the distinction between three important parameters: the support of a rule, its strength, and its confidence. It searches for rules containing different number of phrases and having confidence level and strength level above certain thresholds. The algorithm makes the distinction between a small number of occurrences with high support intersections and large number of occurrences with low support intersections. Finally we present results of initial experiments on real-world data that illustrate the usefulness of the proposed approach.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Terrorist detection system\n", "abstract": " Terrorist Detection System (TDS) is aimed at detecting suspicious users on the Internet by the content of information they access. TDS consists of two main modules: a training module activated in batch mode, and an on-line detection module. The training module is provided with web pages that include terror related content and learns the typical interests of terrorists by applying data mining algorithms to the training data. The detection module performs real-time monitoring on users\u2019 traffic and analyzes the content of the pages they access. An alarm is issued upon detection of a user whose content of accessed pages is \u201ctoo\u201d similar to typical terrorist content. TDS feasibility was tested in a network environment. Its detection rate was better than the rate of a state of the art Intrusion Detection System based on anomaly detection.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Using data mining for detecting terror-related activities on the Web\n", "abstract": " We present an innovative knowledge-based methodology for terrorist detection by using Web traffic content as the audit information. The proposed methodology learns the typical behaviour (\" profile\") of terrorists by applying a data mining algorithm to the textual content of terror-related Web sites. The resulting profile is used by the system to perform real-time detection of users suspected of being engaged in terrorist activities. The Receiver-Operator Characteristic (ROC) analysis shows that this methodology can outperform a command-based intrusion detection system.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Geometric approach to data mining\n", "abstract": " In this paper, a new, geometric approach to pattern identification in data mining is presented. It is based on applying string edit distance computation to measuring the similarity between multi-dimensional curves. The string edit distance computation is extended to allow the possibility of using strings, where each element is a vector rather than just a symbol. We discuss an approach for representing 3D-curves using the curvature and the tension as their symbolic representation. This transformation preserves all the information contained in the original 3D-curve. We validate this approach through experiments using synthetic and digitalized data. In particular, the proposed approach is suitable to measure the similarity of 3D-curves invariant under translation, rotation, and scaling. It also can be applied for partial curve matching.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Information-efficient design of an automatic aircraft maintenance supervisor\n", "abstract": " The work builds upon an economic approach to designing an automatic supervisor of a discrete event stochastic process. The supervisory model ignores some available information without a significant decrease in the expected performance of the controlled process. We develop an information-efficient decision system performing as a maintenance supervisor of a combat squadron. A discrete event stochastic model of the controlled system is represented both mathematically and graphically. The problem of information-efficient control of some maintenance activities is formulated and then solved by a heuristic algorithm. We also discuss an efficient use of some computation parameters of the search procedure.", "num_citations": "4\n", "authors": ["327"]}
{"title": "Responsive news summarization for ubiquitous consumption on multiple mobile devices\n", "abstract": " With the proliferation of online news read on devices ranging from desktops to smart watches, the need for meaningful summaries of long texts is growing. Manual summaries are labour-intensive and cannot be offered for all display sizes, whereas today's abstracts of most news texts are teasers designed to attract the reader's interest more than to provide an overview of an article's content suited to the reader's information needs. We propose responsive news summarization as a technological approach for filling this gap. Responsive news summarization provides an automatically generated content summary that has the right length for the device requesting the article, plus access to the full text. We describe the system prototype available at multisizenews. com along with the initial user study results and give an outlook on future work.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Multi-target classification: Methodology and practical case studies\n", "abstract": " Most classification algorithms are aimed at predicting the value or values of a single target (class) attribute. However, some real-world classification tasks involve several targets that need to be predicted simultaneously. The Multi-objective Info-Fuzzy Network (M-IFN) algorithm builds an ordered (oblivious) decision-tree model for a multi-target classification task. After summarizing the principles and the properties of the M-IFN algorithm, this paper reviews three case studies of applying M-IFN to practical problems in industry and science.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Comparison of three classifiers for breast cancer outcome prediction\n", "abstract": " Predicting the outcome of cancer is a challenging task; researchers have an interest in trying to predict the relapse-free survival of breast cancer patients based on gene expression data. Data mining methods offer more advanced approaches for dealing with survival data. The main objective in cancer treatment is to improve overall survival or, at the very least, the time to relapse (\" relapse-free survival\"). In this work, we compare the performance of three popular interpretable classifiers (decision tree, probabilistic neural networks and Na\u00efve Bayes) for the task of classifying breast cancer patients into recurrence risk groups (low or high risk of recurrence within 5 or 10 years). For the 5-year recurrence risk prediction, the highest prediction accuracy was reached by the probabilistic neural networks classifier (Acc= 76.88%\u00b11.09%, AUC= 77.41%). For the 10-year recurrence risk prediction, the decision tree classifier and\u00a0\u2026", "num_citations": "3\n", "authors": ["327"]}
{"title": "Bridging Semantic Interoperability gaps with SILF\n", "abstract": " Information exchange among coalition command and control (C2) systems in network-enabled environments requires ensuring that each recipient system understands and interprets messages exactly as the source system intended. The Semantic Interoperability Logical Framework (SILF) aims at meeting NATO's needs for semantically correct interoperability between C2 systems, as well as the need to adapt quickly to new missions and new combinations of coalition partners and systems. This paper presents an overview of the SILF framework and performs a detailed analysis of a case study for implementing SILF in a real-world military scenario.", "num_citations": "3\n", "authors": ["327"]}
{"title": "New formats and interfaces for multi-document news summarization and its evaluation\n", "abstract": " News production, delivery, and consumption are increasing in ubiquity and speed, spreading over more software and hardware platforms, in particular mobile devices. This has led to an increasing interest in automated methods for multi-document summarization. The authors start this chapter with discussing several new alternatives for automated news summarization, with a particular focus on temporal text mining, graph-based methods, and graphical interfaces. Then they present automated and user-centric frameworks for cross-evaluating summarization methods that output different summary formats and describe the challenges associated with each evaluation framework. Based on the results of the user studies, the authors argue that it is crucial for effective summarization to integrate the user into sense-making through usable, entertaining, and ultimately useful interactive summarization-plus-document-search\u00a0\u2026", "num_citations": "3\n", "authors": ["327"]}
{"title": "Summarize to learn: summarization and visualization of text for ubiquitous learning\n", "abstract": " Visualizations can stand in many relations to texts\u2013and, as research into learning with pictures has shown, they can become particularly valuable when they transform the contents of the text (rather than just duplicate its message or structure it). But what kinds of transformations can be particularly helpful in the learning process? In this paper, we argue that interacting with, and creating, summaries of texts is a key transformation technique, and we investigate how textual and graphical summarization approaches, as well as automatic and manual summarization, can complement one another to support effective learning.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Using sonification for mining time series data\n", "abstract": " In recent years, there is a growing interest in mining time series databases by both automated and interactive tools. In this paper, we present an interactive methodology for mining of time series data using a novel sonification technique which uses some important properties of time series and tonal music to achieve effective (accurate) and efficient (fast) results. We have created an experimental website, where participants were asked to perform some basic data exploration and mining tasks by listening to a musical display of several time series. The initial results indicate that the proposed methodology for musical representation of data allows, on one hand, to efficiently perform some decision-making tasks\" on the fly\"-by only listening to some short music examples, and on the other hand, it provides an alternative data representation for blind or visually impaired users or users who are due to their professional or\u00a0\u2026", "num_citations": "3\n", "authors": ["327"]}
{"title": "Efficient learning algorithms for agents mining time-changing data streams\n", "abstract": " Many continuously recorded data streams are generated by non-stationary processes, which may change over time, in some cases even drastically. Some adaptive learning agents deal with time-changing data streams by generating a new model from every incoming window of training examples. Though this solution should ensure an accurate and relevant model at all times, it may waste significant computational resources on continuous re-generation of nearly identical models during periods of stability. In this paper, we evaluate a series of efficient incremental algorithms that are nearly as accurate as existing online methods, sometimes even outperforming them, while being considerably cheaper in terms of the processing time. The proposed incremental techniques are based on the Information Network classification algorithm. The incremental methods efficiency is demonstrated on real-world streams of road\u00a0\u2026", "num_citations": "3\n", "authors": ["327"]}
{"title": "OHT-Online-HTML Tracer for Detecting Terrorist Activities on the Web\n", "abstract": " The Terrorist Detection System (TDS) is aimed at tracking down suspected terrorists by the content of information that they access. One requirement identified during the implementation of TDS was the need for a module able to intercept textual Web pages from the network traffic in real time. Online-HTML Tracer (OHT) is the module of TDS in charge of tracing HTML pages transferred on to the Web. The design considerations of OHT and its architecture are described. Initial evaluations of OHT were performed in a LAN environment.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Collection of test case sequences: Covering of function cluster digraph\n", "abstract": " The paper focuses on multi-function system testing in the case of multi-stage testing process. In this pa per we deal with the problem of covering a digraph of system function clusters by chains (trails) of test cases. The problem is solved by the combinatorial al gorithms which include special case algorithms (e.g., tree-like digraph); approximation-based, partitioning based, and greedy algorithms; and an algorithm based on the maximal matching. Numerical examples illus trate our approach.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Fuzzy based paths ordering algorithm in networks with imperfect QoS information\n", "abstract": " In this paper, we study the problem of path selection under additive QoS (quality of service) constraints, where the information available for making routing decisions is inaccurate. The goal of the path selection process is to identify a feasible path while minimizing the overall setup time required for establishing a successful connection. Under the assumption that a list of candidate paths already exists, we explore the fuzzy logic approach to minimize the overall setup time by finding an optimal preference order of the given paths. A simulation program was used to evaluate the performance of the proposed algorithm. In the simulation, we compare between the fuzzy logic approach and another approach suggested in the literature to solve a similar problem, where the main performance metric is the average setup time for successfully establishing a feasible connection. The results show that the fuzzy approach has a\u00a0\u2026", "num_citations": "3\n", "authors": ["327"]}
{"title": "Fuzzy clustering with genetically adaptive scaling\n", "abstract": " In this paper we present a genetically enhanced version of the classical fuzzy c-means clustering algorithm. Our algorithm uses an evolutionary method to find optimal values for some scaling constants which are used to scale the various dimensions of the given data set so that clusters can be more easily detected by compensating for differences in distributions among features. We demonstrate how using un-scaled data with the conventional fuzzy c-means algorithm can lead to incorrect classification and how our algorithm overcomes the problem. We present the results of applying our method to both a synthetic data set, which we created to demonstrate the problem, and the standard Iris data set. In both cases, reduction of misclassifications was obtained by the new method, demonstrating improvement over the standard fuzzy c-means algorithm.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Information-efficient control of discrete event stochastic systems\n", "abstract": " A state of a discrete event stochastic system (DESS) can be represented by a tuple of time-varying discrete parameters. The authors have extended the theory of controlled discrete event systems developed by Ramadge, Wonham and other researchers to stochastic modeling and performance measurement. This work presents some important characteristics of the DESS model. The problem of making the most efficient use of information-processing resources (sensors, computer capacity, etc.) in a special class of controlled systems is stated in a new, two-stage format. The format is used to develop a heuristic solution procedure of the problem. Finally, we perform a brief discussion of the model applicability to real-life design of automatic supervisors.", "num_citations": "3\n", "authors": ["327"]}
{"title": "Exploring Long-Term Temporal Trends in the Use of Multiword Expressions\n", "abstract": " Differentiating between outdated expressions and current expressions is not a trivial task for foreign language learners, and could be beneficial for lexicographers, as they examine expressions. Assuming that the usage of expressions over time can be represented by a time-series of their periodic frequencies over a large lexicographic corpus, we test the hypothesis that there exists an old\u2013new relationship between the time-series of some synonymous expressions, a hint that a later expression has replaced an earlier one. Another hypothesis we test is that Multiword Expressions (MWEs) can be characterized by sparsity & frequency thresholds.Using a dataset of 1 million English books, we choose MWEs having the most positive or the most negative usage trends from a ready-made list of known MWEs. We identify synonyms of those expressions in a historical thesaurus and visualize the temporal relationships between the resulting expression pairs. Our empirical results indicate that old\u2013new usage relationships do exist between some synonymous expressions, and that new candidate expressions, not found in dictionaries, can be found by analyzing usage trends.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Survival Analysis Meets Data Stream Mining\n", "abstract": " Survival analysis deals with monitoring entities over their lifetime. The definition of\" birth\" and\" death\" events depends on the nature of a given entity. When we observe an infinite stream of birth and death events, at each point in time some of the monitored entities are\" right-censored\", ie we know the time elapsed since their birth event, but their death event has not occurred yet and we do not know when it will occur in the future. Often, the snapshots of partially censored observations keep arriving over time in the form of a data stream. Given each snapshot, we may be interested to predict the timing of death events for all live entities or, alternatively, to predict their label (\" survived\" or\" failed\") as a function of time. In this research, our intention is to modify standard classification algorithms, such as decision trees, so that they can seamlessly handle a snapshot stream of both censored and non-censored data. The objective is to provide reasonably accurate predictions after observing relatively few snapshots of the data stream and to improve the classification model with additional information obtained from each new snapshot.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Uninterpreted Semi-Automatic Schema Matching Approach Using Inter-Attribute Dependencies\n", "abstract": " Schema matching is aimed at identifying semantic correspondences between elements of two database schemas. It is one of the key challenges in many database applications such as data integration and data warehousing. Before any data can be integrated, table columns in the two databases should be matched. It is a strenuous and time consuming process. To cope with this problem, many automated/semi-automated solutions have been proposed. Most of the existing solutions mainly rely on textual similarity of the data to be matched. While these approaches are valuable in many cases, they are not enough, and there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are opaque or difficult to interpret. Our research scope is focused on the uninterpreted matching. In this\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Data Modeling and Functional Modeling: Examining the Preferred Order of Using UML Class Diagrams and Use Cases\n", "abstract": " In the analysis phase of the information system development, the user requirements are studied, and analysis models are created. In most UML-based methodologies, the analysis activities include mainly modeling the problem domain using a class diagram, and modeling the user/functional requirements using use cases. Different development methodologies prescribe different orders of carrying out these activities, but there is no commonly agreed order for performing them. In order to find out whether the order of analysis activities makes any difference, and which order leads to better results, a comparative controlled experiment was carried out in a laboratory environment. The subjects were asked to create two analysis models of a given system while working in two opposite orders. The main results of the experiment are that the class diagrams are of better quality when created as the first modeling task, and that\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Enhancement to the Advanced Terrorist Detection System (ATDS)\n", "abstract": " The ATDS system is aimed at detecting potential terrorists on the Web by tracking and analyzing the content of pages accessed by users in a known environment (eg, university, organization). The system would alert and report on any user who is\" too\" interested in terrorist-related content. The system learns and represents the typical interests of the users in the environment. It then monitors the content of pages the users access and compares it to the typical interests of the users in the environment. The system issues an alarm if it discovers a user whose interests are significantly and consistently dissimilar to the other users' interests. This paper briefly reviews the main ideas of the system and suggests improving the detection accuracy by learning terrorists' typical behaviors from known terrorist related sites. An alarm would be issued only if a\" non-typical\" user is found to be similar to the typical interests of terrorists. Another enhancement suggested is the analysis of the visual content of the pages in addition to the textual content.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Comparing representative selection strategies for dissimilarity representations\n", "abstract": " Many of the computational intelligence techniques currently used do not scale well in data type or computational performance, so selecting the right dimensionality reduction technique for the data is essential. By employing a dimensionality reduction technique called representative dissimilarity to create an embedded space, large spaces of complex patterns can be simplified to a fixed\u2010dimensional Euclidean space of points. The only current suggestions as to how the representatives should be selected are principal component analysis, projection pursuit, and factor analysis. Several alternative representative strategies are proposed and empirically evaluated on a set of term vectors constructed from HTML documents. The results indicate that using a representative dissimilarity representation with at least 50 representatives can achieve a significant increase in classification speed, with a minimal sacrifice in\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Anomaly Detection in Web Documents Using Computationally Intelligent Methods of Fuzzy-Based Clustering\n", "abstract": " Intelligent environments, such as Www, are leading to unprecedented scenarios where people (eg, Web users) interact with electronic devices embedded in envi-ronments that are sensitive and responsive to the presence of people. The term ambient intelligence [1](or, Aml) reflects this tendency, gathering best results from three key technologies: ubiquitous computing, ubiquitous communication, and intelligent user friendly Interfaces. The emphasis of Aml is on greater user-friendliness, more efficient services support, user empowerment, and support for human interactions. An Aml environment is capable of recognizing and responding to the presence of different individuals, working in a seamless, unobtru-sive, and often invisible way. An important function of the Aml environment is security, ie, using Ami technologies to protect people from internal and external threats ranging from natural disasters to cyberattacks by malicious hackers. In recent years, the Internet has become an efficient communication infrastruc-ture that can be seamlessly used at a workplace, a private home, or an Internet cafe. Its connection speed, accessibility, and user-friendliness, built upon behind-the-scenes data mining and information retrieval techniques, provide it with more and more characteristics of the ambient intelligence environment. Unfortunately, the Web is also increasingly used by illegal organizations to safely communicate with their affiliates, coordinate action plans, spread propaganda, raise funds, and introduce possible new supporters to their networks. Facing this reality, governments and intelligence agencies around the globe are calling for major efforts\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Look-ahead mechanism integration in decision tree induction models\n", "abstract": " Most of decision tree induction algorithms use a greedy splitting criterion. One of the possible solutions to avoid this greediness is looking ahead to make better splits. Look-Ahead has not been used in most decision tree methods primarily because of its high computational complexity and its questionable contribution to predictive accuracy. In this paper we describe a new Look-Ahead approach to induction of decision tree models. We present a computationally efficient algorithm which evaluates quality of subtrees of variable-depth in order to determine the best split attribute out of a set of candidate attributes with a splitting criterion statistically indifferent from the best one.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Identification of Terrorist Web Sites with Cross-Lingual Classification Tools\n", "abstract": " Terrorist sites are available on the net in different languages and can be viewed by almost everyone. Under such circumstances ability to detect terrorist sites automatically can be extremely useful. It can enable to reduce terrorist activity and propaganda on the web by finding and blocking those sites, detect users that request to download terrorist content online etc. In order to achieve this ability document classification1 should be used. Automated content-based document management tasks, generally known as information retrieval, have a long history and in early 90's became highly important in the information systems engineering due to the rapid growth in quantity of available digital documents. Classification is one of those tasks. Automated classification of previously unseen data items has been an active research area for many years. A lot of efficient and scalable classification techniques were developed in the areas of Machine Learning [Mitchell (1997)] and Data Mining [Han et. al (2001)]. Those techniques are used in wide range of applications and natural language, such as text and web, document categorization is one of them.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Data mining in time series databases\n", "abstract": " Adding the time dimension to real-world databases produces Time Series Databases (TSDB) and introduces new aspects and difficulties to data mining and knowledge discovery. This book covers the state-of-the-art methodology for mining time series databases. The novel data mining methods presented in the book include techniques for efficient segmentation, indexing, and classification of noisy and dynamic time series. A graph-based method for anomaly detection in time series is described and the book also studies the implications of a novel and potentially useful representation of time series as strings. The problem of detecting changes in data mining models that are induced from temporal databases is additionally discussed.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Automated detection of injected faults in a differential equation solver\n", "abstract": " Analysis of logical relationships between inputs and outputs of a computational system can significantly reduce the test execution effort via minimizing the number of required test cases. Unfortunately, the available specification documents are often insufficient to build a complete and reliable model of the tested system. In this paper, we demonstrate the use of a data mining method, called Info-Fuzzy Network (IFN), which can automatically induce logical dependencies from execution data of a stable software version, construct a set of non-redundant test cases, and identify faulty outcomes in new, potentially faulty releases of the same system. The proposed approach is applied to the Unstructured Mesh Finite Element Solver (UMFES) which is a general finite element program for solving 2D elliptic partial differential equations. Experimental results demonstrate the capability of the IFN-based testing methodology to\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Test Case Sequences in System Testing: Selection of Test Cases for a Chain (Sequence) of Function Clusters\n", "abstract": " The paper describes design of test case sequences in a multi-function system testing process. Groups of system functions (function clusters) are considered. It is assumed a set of test cases for each function cluster is designed or selected. The problem is: compose a test case sequence for a chain of function clusters (selection of a test case for each function cluster). Our approach is based on multicriteria decision making and Numerical examples illustrate the materials.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Re-granulating a fuzzy rulebase\n", "abstract": " We describe a computing-with-words algorithm for changing the granularity of an existing rulebase. A fuzzy rulebase is regranulated by manually selecting the granularity of each input dimension, and then automatically calculating the rulebase consequents. We demonstrate this algorithm by studying the common \"fuzzy-PD\" rulebase at various granularities. We find evidence that the \"3/spl times/3\" fuzzy-PD rulebase may be too coarse a granularity for this problem.", "num_citations": "2\n", "authors": ["327"]}
{"title": "Fuzzification of an object-oriented database system\n", "abstract": " The design and implementation of a fuzzy object-oriented database (FOODB) is presented within the framework of a commercial object-oriented database product (POET from POET Software Corporation). The database system is enhanced by a fuzzy query capability, including the ability to do both crisp and fuzzy queries on either crisp or fuzzy data, as well as linguistic hedging. The fuzzy query capability allows database users to specify queries using natural language, such as \"find computers with good performance\" or \"find computers with cache size of about 16 k\". The application can store both crisp and fuzzy data; the fuzzy data may be entered directly by the user or created automatically by fuzzifying crisp data. A Graphical User Interface is developed for performing fuzzy queries, entering fuzzy data, and examining results of fuzzy queries. The system's capability is demonstrated by storing information on 209\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Information-efficient robotic control\n", "abstract": " The work demonstrates a new approach to design of a level of intelligent control of robotic systems. The analytical model results in operational decisions. The structure of these decisions make them readily available to be implemented as an expert system. The approach is applied to a case study of mobile supervisory robots. The model presented here was motivated by manufacturing robotic systems and a type of autonomous robots that collect information at different sites for safety and other control purposes. The robot actions are directly affected by the obta~ned data. At each site the amount of available information (and thus the correctness of the robot decision) can be increased if the robot keeps collecting data at that site for a longer period of t~me. This means a delay in reacting and in reaching next site and accordingly, a decrease in the general amount of robot's information on the whole system.The method of\u00a0\u2026", "num_citations": "2\n", "authors": ["327"]}
{"title": "Facing airborne attacks on ADS-B data with autoencoders\n", "abstract": " The automatic dependent surveillance-broadcast (ADS-B) represents a major change in flight tracking and it is one of the key components in building the next generation of air transportation systems. However, several concerns have been raised regarding its vulnerabilities to cyber attacks. In recent years, a new and promising approach of utilizing large-scale and publicly available flight recordings for training machine learning models that can detect anomalous flight patterns has been demonstrated as a valid countermeasure for several ADS-B attacks. The new approach differs significantly from previously proposed methods in the simplicity of its integration with the current ADS-B system. It also provides a valid countermeasure against highly sophisticated airborne attackers. However, previously proposed machine learning methods require training a different model for each flight route or geographic location to give\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Towards Automatic Textual Summarization of Movies\n", "abstract": " With the rapidly increasing number of online video resources, the ability of automatically understanding those videos becomes more and more important, since it is almost impossible for people to watch all of the videos and provide textual descriptions. The duration of online videos varies in a extremely wide range, from several seconds to more than 5\u00a0h. In this paper, we focus on long videos, especially on full-length movies, and propose the first pipeline for automatically generating textual summaries of such movies. The proposed system takes an entire movie as input (including subtitles), splits it into scenes, generates a one-sentence description for each scene and summarizes those descriptions and subtitles into a final summary. In our initial experiment on a popular cinema movie (Forrest Gump), we utilize several existing algorithms and software tools for implementing the different components of our system\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Detecting Troll Tweets in a Bilingual Corpus\n", "abstract": " During the past several years, a large amount of troll accounts has emerged with efforts to manipulate public opinion on social network sites. They are often involved in spreading misinformation, fake news, and propaganda with the intent of distracting and sowing discord. This paper aims to detect troll tweets in both English and Russian assuming that the tweets are generated by some \u201ctroll farm.\u201d We reduce this task to the authorship verification problem of determining whether a single tweet is authored by a \u201ctroll farm\u201d account or not. We evaluate a supervised classification approach with monolingual, cross-lingual, and bilingual training scenarios, using several machine learning algorithms, including deep learning. The best results are attained by the bilingual learning, showing the area under the ROC curve (AUC) of 0.875 and 0.828, for tweet classification in English and Russian test sets, respectively. It is noteworthy that these results are obtained using only raw text features, which do not require manual feature engineering efforts. In this paper, we introduce a resource of English and Russian troll tweets containing original tweets and translation from English to Russian, Russian to English. It is available for academic purposes.", "num_citations": "1\n", "authors": ["327"]}
{"title": "Towards story-based classification of movie scenes\n", "abstract": " Humans are entertained and emotionally captivated by a good story. Artworks, such as operas, theatre plays, movies, TV series, cartoons, etc., contain implicit stories, which are conveyed visually (e.g., through scenes) and audially (e.g., via music and speech). Story theorists have explored the structure of various artworks and identified forms and paradigms that are common to most well-written stories. Further, typical story structures have been formalized in different ways and used by professional screenwriters as guidelines. Currently, computers cannot yet identify such a latent narrative structure of a movie story. Therefore, in this work, we raise the novel challenge of understanding and formulating the movie story structure and introduce the first ever story-based labeled dataset\u2014the Flintstones Scene Dataset (FSD). The dataset consists of 1, 569 scenes taken from a manual annotation of 60 episodes of a famous cartoon series, The Flintstones, by 105 distinct annotators. The various labels assigned to each scene by different annotators are summarized by a probability vector over 10 possible story elements representing the function of each scene in the advancement of the story, such as the Climax of Act One or the Midpoint. These elements are learned from guidelines for professional script-writing. The annotated dataset is used to investigate the effectiveness of various story-related features and multi-label classification algorithms for the task of predicting the probability distribution of scene labels. We use cosine similarity and KL divergence to measure the quality of predicted distributions. The best approaches demonstrated 0.81 average\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Sentence compression as a supervised learning with a rich feature space\n", "abstract": " We present a novel supervised approach to sentence compression, based on classification and removal of word sequences generated from subtrees of the original sentence dependency tree. Our system may use any known classifier like Support Vector Machines or Logistic Model Tree to identify word sequences that can be removed without compromising the grammatical correctness of the compressed sentence. We trained our system using several classifiers on a small annotated dataset of 100 sentences, which included around 1500 manually labeled subtrees (removal candidates) represented by 25 features. The highest cross-validation classification accuracy of 80% was obtained with the SMO (Normalized Poly Kernel) algorithm. We evaluated the readability and the informativeness of the sentences compressed by the SMO-based classification model with the help of human raters using a separate benchmark dataset of 200 sentences.", "num_citations": "1\n", "authors": ["327"]}
{"title": "Avoiding the Look\u2010Ahead Pathology of Decision Tree Learning\n", "abstract": " Most decision\u2010tree induction algorithms are using a local greedy strategy, where a leaf is always split on the best attribute according to a given attribute\u2010selection criterion. A more accurate model could possibly be found by looking ahead for alternative subtrees. However, some researchers argue that the look\u2010ahead should not be used due to a negative effect (called \u201cdecision\u2010tree pathology\u201d) on the decision\u2010tree accuracy. This paper presents a new look\u2010ahead heuristics for decision\u2010tree induction. The proposed method is called look\u2010ahead J48 ( LA\u2010J48) as it is based on J48, the Weka implementation of the popular C4.5 algorithm. At each tree node, the LA\u2010J48 algorithm applies the look\u2010ahead procedure of bounded depth only to attributes that are not statistically distinguishable from the best attribute chosen by the greedy approach of C4.5. A bootstrap process is used for estimating the standard deviation of\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Multi-dimensional failure probability estimation in automotive industry based on censored warranty data\n", "abstract": " The warranty datasets available for various car models are characterized by extremely imbalanced classes, where a very low amount of under-warranty vehicles have at least one matching claim (\u201cfailure\u201d) of a given type. The failure probability estimation becomes even more complex in the presence of censored warranty data, where some of the vehicles have not reached yet the upper limit of the predicted interval. The actual mileage rate of under-warranty vehicles is another source of uncertainty in warranty datasets. In this paper, we present a new, continuous-time methodology for failure probability estimation from multi-dimensional censored datasets in automotive industry.", "num_citations": "1\n", "authors": ["327"]}
{"title": "JOURNAL of AUTOMATION, MOBILE ROBOTICS & INTELLIGENT SYSTEMS\n", "abstract": " JOURNAL of AUTOMATION, MOBILE ROBOTICS & INTELLIGENT SYSTEMS Editor-in-Chief Co-Editors: Janusz Kacprzyk Dimitar Filev Kaoru Hirota Witold Pedrycz Roman Szewczyk (Systems Research Institute, Polish Academy of Sciences, Poland) (Research & Advanced Engineering, Ford Motor Company, USA) (Interdisciplinary Graduate School of Science and Engineering, Tokyo Institute of Technology, Japan) (ECERF, University of Alberta, Canada) (PIAP, Warsaw University of Technology) ; PIAP , Poland (Polish Academy of Sciences; PIAP, Poland) Editorial Office: Al. Jerozolimskie 202, 02-486 Warsaw, POLAND Tel.+ 48-22-8740109, Chairman: Industrial Research Institute for Automation and Measurements PIAP Janusz Kacprzyk Plamen Angelov Zenn Bien Adam Borkowski Wolfgang Borutzky Oscar Castillo Chin Chen Chang Jorge Manuel Miranda Dias Bogdan Gabry\u015b Jan Jab\u0142kowski Stanis\u0142aw Tadeusz (\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Computing Temporal Trends in Web Documents.\n", "abstract": " Most existing methods of web content mining assume a static nature of the web documents. This approach is inadequate for long-term monitoring and analysis of the web content, since both the users' interests and the content of most web sites are subject to continuous changes over time. In this research, we are interested in developing computationally intelligent and efficient text mining techniques that will enable continuous comparison between documents provided by the same source (website, institute, organization, cult, author etc.) or viewed by the same group of users (eg, university students) and timely detection of temporal trends in those documents. Our approach builds upon the recently developed methodology for fuzzy comparison of frequency distributions. The proposed techniques are evaluated on a real-world stream of web traffic.", "num_citations": "1\n", "authors": ["327"]}
{"title": "Segmentation of Continuous Data Streams Based on a Change Detection Methodology\n", "abstract": " Most data mining algorithms assume that the historic data are the best estimator of what will happen in the future. As more data are accumulated in a database, one should examine whether the new data agrees with the model induced from previous instances. The problem of recognizing the change of the underlying model is known as a change detection problem. Once all change points have been detected, a data stream can be represented as a series of nonoverlapping segments.             This work presents a new methodology for change detection and segmentation based on a set of statistical estimators. While traditional segmentation methods are aimed at analyzing univariate time series, our methodology detects statistically significant changes in incrementally built classification models of data mining. In our previous work, we have shown the methodology to be valid for change detection in a set of\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Experimental Comparison of Sequence and Collaboration Diagrams in Different Application Domains\n", "abstract": " This article reports the findings from a controlled experiment where both the comprehensibility and the quality of UML interaction diagrams were investigated in two application domains: management information system (MIS) and real-time (RT) system. The results indicate that collaboration diagrams are easier to comprehend than sequence diagrams in RT systems, while there is no difference in their comprehension in MIS. With respect to quality of diagrams constructed by analysts, in MIS collaboration diagrams are of better quality than sequence diagrams, while in RT there is no significant difference in their quality.", "num_citations": "1\n", "authors": ["327"]}
{"title": "E-mail: mlfridg) netvision. net. il\n", "abstract": " The ultimate goal of function testing is to verify that the program works according to its specification and there are no undiscovered errors left. Software functionality is mostly tested using the black-box approach, where the actual outputs are compared to the expected ones based on the tester's understanding and knowledge of system requirements. In a properly documented system, behavior requirements are covered by a series of use-case scenarios [36]. Use cases are further modeled by", "num_citations": "1\n", "authors": ["327"]}
{"title": "Comparative Study\n", "abstract": " This section compares the performance of the information-theoretic network to leading classification methods of data mining. The comparison is based on public datasets and it includes the following performance criteria:                                          Dimensionality Reduction is measured by the portion of candidate-input attributes removed by the algorithm (excluded from the network) and by the size of the information-theoretic network vs. other predictive models.                                                            Prediction Accuracy is the average accuracy of the network on validation cases vs. published accuracy of other classifiers.                                                            Stability represents the ability of the algorithms to provide similar results from different random samples of the same dataset. The benchmark classification methods used for the comparison include:                                                            Naive Bayes Classifier. This is a\u00a0\u2026", "num_citations": "1\n", "authors": ["327"]}
{"title": "Post-Processing of Data Mining Results\n", "abstract": " In the information-theoretic network (see Chapter 3 above), each connection between a terminal (unsplit / final layer) node and a target node represents a probabilistic rule between a conjunction of input attribute-values and a target value. An information-theoretic weight of each rule is given by:", "num_citations": "1\n", "authors": ["327"]}
{"title": "Evaluation of fuzzy rules extracted from data\n", "abstract": " A general methodology for evaluation of fuzzy rules extracted from data is presented. Though the primary goal of most data mining systems is high classification or prediction accuracy, the user may be interested in rules which are not necessarily the most accurate. Our approach provides an alternative measure of rule validity, based on methods of fuzzy set theory. When the rules to be tested come from a human expert, the method can be viewed as a verification-based data mining method. If the rules are generated by another (discovery-based) data mining method (such as a decision-tree algorithm), the method can be seen as a post-processing step in the KDD process, aimed at evaluating the extracted rules. The method involves four major steps: hypothesis formulation, data selection, hypothesis testing, and decision. In the hypothesis formulation step, a set of fuzzy rules are created using conjunctive antecedents and consequent functions. In the data selection step, a subset of all data in the database is chosen as a sample set. This sample should contain enough records to be representative of the data to a certain degree of user satisfaction. In hypothesis testing, a fuzzy implication is applied to the selected data for each extracted rule and the results are combined using some aggregation function. These results are used in the final step to evaluate the validity of each rule. The presented technique is applied to the rules generated by the C4.5 algorithm from two sample databases. The experimental results demonstrate potential benefits of using validity-based evaluation of rules.", "num_citations": "1\n", "authors": ["327"]}
{"title": "FMS control design: an information value approach\n", "abstract": " The work presented in this paper demonstrates a new approach to design of an information-efficient automatic supervisor of manufacturing systems. Such a supervisor ignores some available information without a significant decrease in the expected performance of the system. In addition to efficient supervisor (space and time), this leads to a design that requires less sensory equipment. The design process itself is made simpler and cheaper. The model of an automated system combines the basic concepts of the theory of controlled discrete event systems with some ideas of the information economics. A constructive algorithm of control design uses heuristic techniques of stochastic optimization. The method is applied to design of a flexible manufacturing cell.", "num_citations": "1\n", "authors": ["327"]}