{"title": "Broken promises: An empirical study into evolution problems in java programs caused by library upgrades\n", "abstract": " It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. We have studied the extent of the problem on the qualitas corpus, a data set consisting of Java open-source programs widely used in empirical studies. In this paper, we\u00a0\u2026", "num_citations": "70\n", "authors": ["935"]}
{"title": "How Java APIs break\u2013an empirical study\n", "abstract": " ContextIt has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.ObjectiveWe have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and\u00a0\u2026", "num_citations": "46\n", "authors": ["935"]}
{"title": "Practical verification of component substitutability using subtype relation\n", "abstract": " The flexibility which components provide for assembling applications makes them an appealing solution to many engineering problems. Its darker side is the need to exercise much greater care when replacing and upgrading components within deployed applications, to ensure their stability. Formally strong methods for substitutability checks are therefore desirable but so far, not many are practically used. This paper presents a method of checking component substitutability based on subtyping relation. It uses a representation of the subtype evaluation on different levels of the component type structure, and makes it possible to perform the checks simply by comparing this representation. Two usage scenarios are described, as well as experiences from a prototype implementation for mainstream platforms", "num_citations": "43\n", "authors": ["935"]}
{"title": "Automated versioning in OSGi: A mechanism for component software consistency guarantee\n", "abstract": " Consistency of component software is a crucial condition required for correct program execution. The existing consistency controls of OSGi at build time or in runtime cannot prevent type mismatch failures caused by independent client and server bundle development. This paper describes our solution to this problem using automated versioning of components. Version identifiers are generated from results of subtype-based comparison of component representations, thus achieving a consistent and formally backed interpretation of the version numbering scheme. The implementation of the approach allows its integration into standard OSGi bundle development and build cycle.", "num_citations": "38\n", "authors": ["935"]}
{"title": "Specification-based component substitutability and revision identification\n", "abstract": " After years of research and industrial development, software component technology [Szy98] has become well established as an important approach to engineering complex and flexible software systems. Building on extensive research in the field (systems like Darwin [M+ 95], UniCon [S+ 95], SOFA [PBJ98], ArchJava [ACN02b] or Fractal [C+ 02]), commercial component frameworks (Enterprise JavaBeans [Sun01a], CORBA Component Model [OMG02f], and Microsoft\u2019s DCOM and .NET technologies [Mic95, Rog97, Cor02]) have gained commercial success. From the outset, the main aim of the component technology has been to create pluggable \u201csoftware integrated circuits\u201d[McI68] that can be easily traded and assembled, even by knowledgeable end-users. This is believed to increase productivity (via the reuse of general as well as domain-specific components) in building software applications at large. But components do not seem to live up to these \u201csilver bullet\u201d expectations [Bro95]. Their current use concentrates on user interface widgets (eg Borland Delphi components [Sof01], Sun\u2019s JavaBeans [Sun97]), server-side data-centric components (Enterprise JavaBeans [Sun01a], CORBA Components [OMG02f]) or specialised applications in communications or embedded devices (the Koala model [vO01] or the Robocop project [Lav02]). The functionality provided by commercial component frameworks is limited to the basic wiring of components (setting up their interconnections) and supporting key system-level aspects (deployment to target hosts, location transparency, communication and transaction management). We believe that much more is\u00a0\u2026", "num_citations": "37\n", "authors": ["935"]}
{"title": "Towards automated component compatibility assessment\n", "abstract": " With the increasing use of software components, the methods for safe replacement of currently used components by newer versions will become of greater importance. While component specification and application composition issues have been researched, working approaches to assess compatibility of different components are hard to find. In this paper we define compatibility at two levels, of which contextual compatibility takes into account the actual usage of the component as bound in an assembled application. The presented method for determining compatibility of two components uses analysis of differences in their specifications structured into traits of related declarations.", "num_citations": "28\n", "authors": ["935"]}
{"title": "The CoSi component model: Reviving the black-box nature of components\n", "abstract": " Many component models and frameworks have been created since the advent of component-based software engineering. While they all claim to follow fundamental component principles, the black-box nature in particular, a deeper look reveals surprising problems mainly in the component models built into the mainstream frameworks. In this paper we elaborate on these fundamental principles, analyse a selection of industrial and research component models in light of them, and propose a component model named CoSi. Its aim is to address the problems uncovered by the analysis while keeping the good aspects of current state state of the art models. It supports OSGi-style lightweight components with a rich set of features, and puts a strong emphasis on facilitating component comprehension and application consistency.", "num_citations": "24\n", "authors": ["935"]}
{"title": "Metadata support for safe component upgrades\n", "abstract": " Component platforms play a major role in current distributed information systems. As these systems evolve, components need to be upgraded without breaking the consistency of component interconnections. In this paper we present a method for the support of automated safe upgrades of black-box components. It relates component versioning with indication of changes between revisions of the same component and uses pre-computed information stored in component meta-data. We show how this information is derived from component specifications and used to speed up pre-upgrade checks.", "num_citations": "22\n", "authors": ["935"]}
{"title": "Towards context independent extra-functional properties descriptor for components\n", "abstract": " Architectures based on composing target application functionality from pre-existing components have been successfully used in many projects, yet there are several aspects in which they fail to reach the desirable level of maturity. Since different vendors may provide components with the same functionality, extra-functional properties must be taken into account to help the developer to select the component which suits the final system. In addition, a selected component must conform to the target deployment environment. This paper addresses the problem of inadequate means to define extra-functional properties on components in a way that allows to express component's properties with respect to different computational environments. We provide a representative survey of the current state of the art for extra-functional properties and propose a formalism based on existing approaches which addresses this\u00a0\u2026", "num_citations": "21\n", "authors": ["935"]}
{"title": "Component change and version identification in SOFA\n", "abstract": " In the area of component software, the work so far has con centrated primarily on the key issues of architecture specification and component updating. However, the problems of maintaining application consistency and versioning of components as they evolve have received less attention, and the available solutions are either ad-hoc or not well suited for component applications. In this paper we evaluate these solutions and present a new approach to component versioning developed for the SOFA architecture. Based on the analysis of changes between two versions, component revision numbers and change indications are derived as a lightweight compatibility description which is subsequently used for consistency verification during application composition or component update. Thus it is shown how giving the revision numbers a precise semantics and providing additional versioning information provides\u00a0\u2026", "num_citations": "16\n", "authors": ["935"]}
{"title": "Component revision identification based on IDL/ADL component specification\n", "abstract": " Although software components have become one of the mainstream technologies, they still lack a supportive versioning scheme. This paper describes a system for revision identification of released components with well defined semantics. It is based on the analysis of changes in the component IDL or ADL specification structured into a hierarchy of traits and categories of declarations.", "num_citations": "15\n", "authors": ["935"]}
{"title": "Enhanced type-based component compatibility using deployment context information\n", "abstract": " Consistency and compatibility in component-based applications have been the subject of many methods and approaches, from formally sound ones with difficult practical implementation to pragmatic rules for comparing version meta-data which offer only weak guarantees. This is especially true of many industrial component frameworks in routine use. In this paper we contribute a formal description of a method which ensures application run-time type consistency, by performing type-based substitutability checks as part of the component binding and update processes. The method takes into account the environment of the currently deployed component version and uses its so-called contextual complement in the checks. This novel approach overcomes the limitations of the standard notion of compatibility by allowing non-contravariant differences on the required side of the component\u02bcs surface. The method was\u00a0\u2026", "num_citations": "14\n", "authors": ["935"]}
{"title": "Alm tool data usage in software process metamodeling\n", "abstract": " Project Management and Software Process Improvement (SPI) are essential parts of software engineering. A multitude of project management techniques and tools, such as Application Lifecycle Management (ALM) ones, are available, and there is an abundance of software methodologies, process metamodels and best practice descriptions. Despite the role of tools in enacting the processes, the underlying domain models used in these two fields -- ALM tools and software process metamodels -- often significantly differ. This hinders the project execution analysis using the data available from the tools, such as the verification of the project's alignment with a given methodology or the detection of bad practices (anti-patterns). In this paper we describe our work towards enabling such analyses. First we survey the domain models of major process metamodels and tools, and discuss the commonalities and open issues\u00a0\u2026", "num_citations": "13\n", "authors": ["935"]}
{"title": "What Java developers know about compatibility, and why this matters\n", "abstract": " Real-world programs are neither monolithic nor static\u2014they are constructed using platform and third party libraries, and both programs and libraries continuously evolve in response to change pressure. In case of the Java language, rules defined in the Java Language and Java Virtual Machine Specifications define when library evolution is safe. These rules distinguish between three types of compatibility\u2014binary, source and behavioural. We claim that some of these rules are counter intuitive and not well-understood by many developers. We present the results of a survey where we quizzed developers about their understanding of the various types of compatibility. 414 developers responded to our survey. We find that while most programmers are familiar with the rules of source compatibility, they generally lack knowledge about the rules of binary and behavioural compatibility. This can be problematic\u00a0\u2026", "num_citations": "13\n", "authors": ["935"]}
{"title": "The ENT model: A general model for software interface structuring\n", "abstract": " Software modules and components have always played a key role in software engineering, primarily as key abstractions that embody the principle of information hiding, using separation of interface and implementation. In most module-and component-based systems, the specification of the interface therefore plays an important role.This paper presents a model for structuring module interfaces (called the ENT model) which allows their multi-faceted views and analyses. The design of the model is motivated by two factors. First, we feel a need to unify the variety of approaches to module-and component-based software descriptions. Secondly, we want to provide a vehicle for modeling the different roles which the features on module/component interface play for the players in software composition (users, developers, tools).", "num_citations": "13\n", "authors": ["935"]}
{"title": "A method for semi-automated generation of test scenarios based on use cases\n", "abstract": " Use cases are a widely accepted way to define application functionality. They can therefore form a solid basis for testing the correct functionality and quality of service of a developed application. In this paper, we describe a method for semi-automated generation of test scenarios for simulation testing of software components and component-based applications. It derives the scenarios from the use cases written in natural language, enriched with annotations that allow us to connect the specification with the source code of the application. This helps us to generate a sequence of method invocations within the tested application that forms a testing scenario. The achieved benefit is that the utilization of use cases not only helps to keep tests directly related to the original requirements of the application, it also makes it possible to easily generate new test cases when the requirements on the application change.", "num_citations": "12\n", "authors": ["935"]}
{"title": "Software components compatibility verification based on static byte-code analysis\n", "abstract": " Current enterprise systems are widely implemented using statically typed languages such as Java. One of the reasons are strong type checks at compile time that help prevent runtime errors. The static type checks are also well integrated into current development tools. One of the less explored area are, however, static type checks of binary software components. Since current software usually contains a large amount of third-party components, the compilers can no longer cover the static type checks of the final product, especially the inter-dependencies of the components are out of their reach. In this work, we propose an approach that analyses the byte-code of Java classes to reconstruct the mutual dependencies of respective components first. Then, the dependencies are examined to find any type inconsistencies. As a result, this approach detects dependency problems early in the development phase and\u00a0\u2026", "num_citations": "12\n", "authors": ["935"]}
{"title": "Towards architect\u2019s activity detection through a common model for project pattern analysis\n", "abstract": " Software development projects leave a large amount of data in repositories of Application Lifecycle Management (ALM) tools. These data contain detailed histories of their respective projects, their results and decisions made along the way. Analysis of such data helps uncover various interesting facts about projects, e.g. their socio-technical structures and the actual (vs. purported) roles of team members. Based on experiences with tools supporting our research we are convinced that it is feasible to consolidate data from different ALM tools, tapping into the situation common in real-life projects. In this paper we report on our work towards a shared common data model and tool integration aimed at improved project analysis. We discuss how this can help in the identification of architects in the project organizational structures, their activity patterns and collaboration with other team roles.", "num_citations": "10\n", "authors": ["935"]}
{"title": "Contracts in the wild: A study of java programs\n", "abstract": " The use of formal contracts has long been advocated as an approach to develop programs that are provably correct. However, the reality is that adoption of contracts has been slow in practice. Despite this, the adoption of lightweight contracts\u2014typically utilising runtime checking\u2014has progressed. In the case of Java, built-in features of the language (eg assertions and exceptions) can be used for this. Furthermore, a number of libraries which facilitate contract checking have arisen. In this paper, we catalogue 25 techniques and tools for lightweight contract checking in Java, and present the results of an empirical study looking at a dataset extracted from the 200 most popular projects found on Maven Central, constituting roughly 351,034 KLOC. We examine (1) the extent to which contracts are used and (2) what kind of contracts are used. We then investigate how contracts are used to safeguard code, and study problems in the context of two types of substitutability that can be guarded by contracts:(3) unsafe evolution of APIs that may break client programs and (4) violations of Liskovs Substitution Principle (LSP) when methods are overridden. We find that:(1) a wide range of techniques and constructs are used to represent contracts, and often the same program uses different techniques at the same time;(2) overall, contracts are used less than expected, with significant differences between programs;(3) projects that use contracts continue to do so, and expand the use of contracts as they grow and evolve; and,(4) there are cases where the use of contracts points to unsafe subtyping (violations of Liskov's Substitution Principle) and unsafe evolution.", "num_citations": "10\n", "authors": ["935"]}
{"title": "Lowering visual clutter in large component diagrams\n", "abstract": " Nowadays component applications can easily consist of hundreds or thousands of components and it is thus difficult to understand their structure. Diagram visualisation does not help much because of visual clutter caused by big amount of elements and connections, especially in the case of flat component models. This paper describes a novel approach of removing a large part of connections from the diagram while preserving the information about component interconnections. It uses a separated components area to show the components with big amount of connections. For each component in this area, clustered interfaces are shown instead of all interfaces, with the ability to show details on demand. The main idea of this technique can be used in a similar way to reduce the clutter in node-link graphs. To show the effect of this technique we discuss example lines reductions for several component applications.", "num_citations": "10\n", "authors": ["935"]}
{"title": "ENT: A generic meta-model for the description of component-based applications\n", "abstract": " Current best practice in modeling component-based applications is the use of UML extended by a profile. This solution provides a general and common approach of application description and allows to capture some details based on the concrete component model. It has however disadvantages due to the limitations of UML itself, like little scalability or lack of inherent model semantics. In this paper we propose a solution to overcome these limitations, in the form of a meta-model developed directly for the description of components and component-based applications. Its unique aspect is the use of faceted classification to introduce additional semantics and structuring to the derived models. We describe the features and advantages of this meta-model and illustrate its aspects on a model example of a simple OSGi application. At the end of paper we also propose the usage of this meta-model in visualization of\u00a0\u2026", "num_citations": "10\n", "authors": ["935"]}
{"title": "Repository and meta-data design for efficient component consistency verification\n", "abstract": " Composing a complete component-based application as well as updating a subset of its components may involve complex issues of maintaining application consistency. Its verification is a computationally-intensive problem, especially for behavioural compatibility or extra-functional property assessment. This poses serious challenge on resource-constrained devices which represent an important future computing platform. This work describes an approach that addresses this challenge by separating the tasks of obtaining the results of component consistency evaluation and using them in deployment and update processes. The first task in our approach is performed by a repository with sufficient computational resources. The results are transformed into rich, remotely accessible meta-data which are easily checked by the component frameworks and application management agents on the devices. Experiences with a\u00a0\u2026", "num_citations": "9\n", "authors": ["935"]}
{"title": "CORRECT MATCHING OF COMPONENTS WITH EXTRA-FUNCTIONAL PROPERTIES\n", "abstract": " A lot of current approaches attempt to enrich software systems with extra-functional properties. These attempts become remarkably important with the gradual adoption of component-based programming. Typically, extrafunctional properties need to be taken into account in the phase of component binding and therefore included in the process of verifying component compatibility. Although a lot of research has been done, practical usage of extra-functional properties is still rather scarce. The main problem could be in a slow adaptability of specialized research models to rapidly changing industrial needs. We have designed a solution to this issue in the form of a modular framework which provides a formally sound yet practical means to declare, assign and evaluate extra-functional properties in the context of component-based applications. One of its strengths is applicability to a variety of industrial as well as research component models. This paper describes the models and algorithms of the framework and introduces a prototype implementation proving the concept.", "num_citations": "9\n", "authors": ["935"]}
{"title": "A visualization tool for reverse-engineering of complex component applications\n", "abstract": " Nowadays, component applications can contain thousands of components whose structure is difficult to understand. As a solution, we proposed a visualization technique that removes large part of connections from component binding diagrams. This technique uses a separated components area to display components with a big amount of connections detached from the main diagram. In this area, component interfaces are shown clustered instead of showing them all. Benefit of this approach is improvement of application understanding by reducing the diagram's visual clutter during its reverse engineering. In this work, we present implementation of the technique in a form of a tool, called CoCA-Ex. CoCA-Ex is a publicly accessible web application and a reverse-engineering solution for various component systems. The tool is built on modern technologies such as HTML5 and JavaScript and has Java EE server\u00a0\u2026", "num_citations": "8\n", "authors": ["935"]}
{"title": "Ensuring component application consistency on small devices: A repository-based approach\n", "abstract": " Deployment of component-based applications may involve complex issues of maintaining their consistency, both when composing a complete application and updating a small subset of its components. Evaluating consistency may be a computationally-intensive problem, especially if it involves behavioural compatibility or extra-functional property assessment. This poses serious challenge to its application on resource-constrained devices which represent an important future computing platform. This work describes an approach that aims to address this challenge by separating the tasks of obtaining the results of component consistency evaluation and using them in deployment and update processes. The first task is in our approach performed by a repository with sufficient computational resources. The results are transformed into rich, remotely accessible meta-data which are easily checked by the component\u00a0\u2026", "num_citations": "8\n", "authors": ["935"]}
{"title": "Design of a component-based simulation framework for component testing using SpringDM\n", "abstract": " In this paper, we present the design of a simulation framework aimed to support testing of real components in a simulation environment. This enables thorough tests of software components without the need to create their models.", "num_citations": "8\n", "authors": ["935"]}
{"title": "A look at current component models from the black-box perspective\n", "abstract": " Software modules and components have always played a key role in software engineering, primarily as coarse-grained abstractions that embody the principle of information hiding. The fundamental contribution of component-based software engineering lies in making the black-box property a core requirement. In this paper we point out that this important notion tends to be neglected in component model analyses, and define the black-box property in terms of detailed characteristics which help to evaluate component models from this perspective. A survey of typical representatives of industrial and research component models shows that especially the former ones leave much to be desired, and the paper discusses the consequences of the weak aspects of their design.", "num_citations": "8\n", "authors": ["935"]}
{"title": "Evaluating component architecture visualization tools-criteria and case study\n", "abstract": " There are many software architecture modeling tools and their use is very common in practice. But a closer look reveals that in the ever more important area of component-based architectures, system architects or component assemblers are constrained by the lack of good model representations. On one hand, a generic representation like UML provides insufficient support for component-specific needs, on the other hand, tools focused on component development sometimes force the user to learn new visual syntax specific for the component model. Advanced features offered by the tools on top of basic architecture visualization are often limited. In this paper we propose a set of criteria for the evaluation of tools for component architecture visualization, considering the needs of both architectural modeling and analytical tasks. These criteria are thoroughly discussed and their use is shown on a case study which\u00a0\u2026", "num_citations": "7\n", "authors": ["935"]}
{"title": "Formalisation of a generic extra-functional properties framework\n", "abstract": " Approaches to improve software composition become remarkably important with the gradual enlargement of software systems. Together with adaptation of component-based programming to cope with software complexity, extra-functional properties are playing a more important role. The problem addressed in this paper concerns an insufficient adoption of extra-functional properties into practise that consequently limits approaches to modularised software. As a solution this paper presents a comprehensive framework which enables the use of extra-functional properties in existing systems with the promise to improve component application consistency. The framework is described in a formal manner and its practical application is shown on the Spring and OSGi component models.", "num_citations": "7\n", "authors": ["935"]}
{"title": "Reconstruction of type information from java bytecode for component compatibility\n", "abstract": " The Java type system is strictly checked by both the compiler and the runtime bytecode interpreter of the JVM. These mechanisms together guarantee appropriate usage of class instances. Using modern component systems can however circumvent these static checks, because incompatible versions of classes can be bound together during component installation or update. Such problematic bindings result in ClassCastException or NoSuchMethodException runtime errors. In this paper we describe a representation of Java language types suitable for checking component compatibility. The presented approach applies various bytecode handling techniques to reconstruct a representation of the Java types contained in a component implementation, using different sources of class data. The representation is then used during build- and run-time type system verifications with the aim to prevent these kinds of errors. We\u00a0\u2026", "num_citations": "7\n", "authors": ["935"]}
{"title": "Enhanced OSGi bundle updates to prevent runtime exceptions\n", "abstract": " Explicit declaration of provided and required features facilitates easier updates of components within an application. A necessary precondition is that sufficient and correct meta-data about the component and its features is available. In this paper we describe a method that ensures safe OSGi bundle updates and package bindings despite potentially errorneous meta-data. It uses subtype checks on feature types, implemented as user-space enhancements of the standard bundle update process. The method was successfully applied in the Knopflerfish and Apache Felix frameworks and the paper discusses the general experiences with the OSGi framework gained during the implementation.", "num_citations": "7\n", "authors": ["935"]}
{"title": "Software process anti-pattern detection in project data\n", "abstract": " There is a significant amount of guidance on Project Management (PM) including software development methodologies, best practices and anti-patterns (APs). There is, however, a lack of automated way of applying this knowledge by analyzing readily available data from tools aiding in software PM, such as Application Lifecycle Management (ALM) tools. We propose a method of detecting process and PM anti-patterns in project data which can be used to warn software development teams about a potential threat to the project, or to conduct more general studies on the impact of AP occurrence on project success and product quality. We previously published a concept for the data mining and analysis toolset distinct from other research approaches and related work. Based on this toolset, we devised a formalized basis for our detection method in the form of standardized AP description template and a model for\u00a0\u2026", "num_citations": "6\n", "authors": ["935"]}
{"title": "Supplying compiler's static compatibility checks by the analysis of third-party libraries\n", "abstract": " Statically typed languages and their compile time checks prevent a lot of runtime errors thanks to type mismatches detection, namely calls of incompatible methods. Since current applications typically include tens of already compiled third-party libraries, the compile checks are powerless to detect their mutual dependencies. However, the calls among third-party library methods are not less error prone due to bugs or wrong library usage. These all lead to runtime failures. In this paper, we describe a partial solution to this problem based on the static analysis of third-party libraries and verification of their dependencies. This verification is invoked as the application is compiled and assembled, essentially supplying the compiler detecting errors before the application runs. This approach promises improved error detection of complex applications on the static type checking level.", "num_citations": "6\n", "authors": ["935"]}
{"title": "Visualization of component-based applications structure using AIVA\n", "abstract": " The architecture of component-based applications can easily contain hundreds of components with a complex structure. Such structure is created from various types of relations between these components. However, component architects and assemblers need to study more than just simple structure in order to be able to analyze these applications. Many details might be necessary to check to ensure compatibility and re-usability of existing components. Thus, comprehensible visualization of structure is important for the maintenance and evolution of these complex systems. AIVA is the implementation of an approach that is able to visualize these structures comprehensibly while making details easy to reach. It combines various interactive techniques to achieve this goal. This paper describes the possibilities of AIVA as they are implemented, while briefly discussing the theory behind such functionality.", "num_citations": "6\n", "authors": ["935"]}
{"title": "AIVA vs UML: Comparison of component application visualizations in a case-study\n", "abstract": " UML is the state of the art in visualization of software. However, it does not satisfy the needs of modeling in the domain of component applications and complex software architectures. UML has to be extended with profiles so as to be able to model the specifics of a particular component model; its diagrams are not scalable, therefore one has to balance the amount of information and readability; and it presents all the information at once so visual data mining is harder. These were the reasons for developing a completely new approach called AIVA (Advanced Interactive Visualization Approach) that focuses solely on the component domain and addresses the problems of scalability and readability by adopting interactive techniques like information hiding and details on demand. In this paper we analyse how this new approach stands compared with UML by modelling a nontrivial component application.", "num_citations": "6\n", "authors": ["935"]}
{"title": "Software process anti-patterns catalogue\n", "abstract": " For software project managers and other practitioners, an important activity is to detect, and consequently find solutions to, insufficiencies and mistakes in process and other project management (PM) activities. Particularly interesting among these are anti-patterns: commonly occurring solutions with known negative effects. Their detection in running, as well as finished, projects is often hard as it needs to be performed by specialists, demands expertise and skill, and is prone to biases. Obtaining the expertise is a long-time effort and the sources of relevant knowledge are scattered and vary in the depth of treatment.", "num_citations": "5\n", "authors": ["935"]}
{"title": "Verification of SimCo\u2014Simulation tool for testing of component-based application\n", "abstract": " In software testing, most attention is given to verify the functionality of produced software. However, the quality of services (such as time to response or amount of produced data) is also important, especially in world of mobile or embedded devices. These properties are often estimated by using models of components, instead of testing the components from real applications. We have developed a simulation-based, non-invasive tool for testing and verifying characteristics of components or parts of component applications. In this paper, we present our approach to testing these properties and verification of functionality of our tool. Our approach allows testing of components without changing their implementation, as a black box objects. The results of our simulation tool are compared with results obtained from self-measuring application in order to validate the functionality of our simulation.", "num_citations": "5\n", "authors": ["935"]}
{"title": "SimCo\u2013Hybrid Simulator for Testing of Component Based Applications\n", "abstract": " Testing of component-based applications is important in order to ensure that third-party components do not compromise the functionality or properties of the whole system. However, thorough testing of functionality, behaviour and extra-functional properties is a tedious and time consuming process. In this paper we present an approach to discrete event simulation testing of components and component sets. Its unique feature is the ability to execute a mixture of real, implemented components and simulated mock-ups of the remaining parts of the application. Together, this approach allows faster testing on a wide scale of different inputs for tested components. At the same time, the use of actual components increases the confidence in the simulation test results. The approach has been implemented using the OSGi platform in the form of the SimCo framework and toolset, for which the key architectural\u00a0\u2026", "num_citations": "5\n", "authors": ["935"]}
{"title": "Enhancing osgi with explicit, vendor independent extra-functional properties\n", "abstract": " Current industry and research organisations invest considerable effort to adopt component based programming which is promising rapid development process. Several issues, however, hinder its wider adoption. One of them is the practical use of extra-functional properties (EFPs) that research community aims at integrating to component composition but for which industrial applications are still rare. When extra-functional properties are not considered or mis-interpreted, inconsistencies in application performance, security, reliability, etc. can result at run-time. As a possible solution we have proposed a general extra-functional properties system called EFFCC. In this paper we show how it can be applied to an industrial component model, namely the OSGi framework. This work analyses OSGi from the extra-functional properties viewpoint and shows how it can be enhanced by EFPs, expressed as OSGi\u00a0\u2026", "num_citations": "5\n", "authors": ["935"]}
{"title": "Viewport for component diagrams\n", "abstract": " This paper describes a viewport technique for use in the visualization of large graphs, e.g. UML component diagrams. This technique should help to work with complex diagrams (hundreds or thousands of components) by highlighting details of the important parts of the diagram and their related surroundings without losing the global perspective. To avoid visual clutter it uses clusters of interfaces and components.", "num_citations": "5\n", "authors": ["935"]}
{"title": "Interactive component visualization\n", "abstract": " UML is considered to be a universal solution for diagramming any application, but UML also has its shortcomings. It needs several diagrams to describe one problem, it cannot create different views on one diagram and it is not interactive. This leads to hours spent drawing the same thing from different views, any change has to be applied several times and the author of a UML diagram has to balance between good readability and providing a sufficient amount of information. In particular, the UML component diagram has insufficient expressive power to capture all the facts of even today\u2019s component models and architectures. In this paper, we propose a visualization aimed at modular and composed architecture that is content-aware, so it can present the model of component-based architecture in different ways, depending on user needs. By default, it presents minimum information to reduce cognitive load and keep the diagrams comprehensible, while making the additional information available when the user needs it. This paper thus suggests a possible substitute for UML in the domain of component-based applications.", "num_citations": "5\n", "authors": ["935"]}
{"title": "OSGi component substitutability using enhanced ent metamodel implementation\n", "abstract": " Software components can be found in both enterprise-wide and mobile/embedded solutions. Components are mutually linked and dependent, but encapsulated as black boxes and developed independently. They can be replaced without affecting the rest of the application. This advantage requires careful and complex compatibility checks between both component versions though, otherwise the whole application can be broken down. In this paper we present and describe the implementation of the ENT metamodel and the ENT based component comparison algorithm. This algorithm is used in the practical case: OSGi Release 4 components are being compared. On the basis of the change, version identifiers of the newer component are assigned.", "num_citations": "5\n", "authors": ["935"]}
{"title": "Interface-based semi-automated testing of software components\n", "abstract": " The component-based software development enables to construct applications from reusable components providing particular functionalities and simplifies application evolution. To ensure the correct functioning of a given component-based application and its preservation across evolution steps, it is necessary to test not only the functional properties of the individual components but also the correctness of their mutual interactions and cooperation. This is complicated by the fact that third-party components often come without source code and/or documentation of functional and interaction properties. In this paper, we describe an approach for performing rigorous semi-automated testing of software components with unavailable source code. Utilizing an automated analysis of the component interfaces, scenarios invoking methods with generated parameter values are created. When they are performed on a stable\u00a0\u2026", "num_citations": "4\n", "authors": ["935"]}
{"title": "Dependency injection refined by extra-functional properties\n", "abstract": " The Dependency Injection has been widely implemented in a lot of frameworks to decouple software parts. However, current frameworks use simple matching algorithms to determine candidates to be injected. For instance, Spring does type-based matching, optionally enriched with a qualifier string. This is limiting mainly in dynamic systems where a lot of candidates may exist. As a result, the matching fails and developer's interaction is needed. In this work, we propose to enrich Dependency Injection with extra-functional properties serving as additional parameters of the matching algorithm to reliably select the most suitable candidate.", "num_citations": "4\n", "authors": ["935"]}
{"title": "Implementation of a data layer for the visualization of component-based applications\n", "abstract": " Current approaches to the visualization of component-based applications mostly use only associations and dependencies between components and provide limited supplementary information. In this paper, we introduce a data layer that is able to store and later present more information about component elements which are bound together, and through this knowledge provide more understanding about the component itself. These information could be presented in different ways to provide different views for component software developers, designers, and architects. This data layer is general and is able to visualize component based applications of any component model. It is presented here together with its structure, implementation and tooling. We share experiences obtained in the process of designing and implementing this layer. Special care is given to the implementation details which were solved in the process and relevant tools like MOF and EMF are presented. Results from the test application are also part of this paper.", "num_citations": "4\n", "authors": ["935"]}
{"title": "SOFA component revision identification\n", "abstract": " In the area of component software, the work so far has concentrated primarily on the key issues of architecture specification and component updating. However, the problems of maintaining application consistency and versioning of components as they evolve have received less attention, and the available solutions are either ad-hoc or not well suited for component applications. In this paper we evaluate these solutions and present a new approach to component versioning developed for the SOFA architecture. Based on the analysis of changes in the specification between two versions, component revision numbers and change indications are derived as a wellfounded version identification and lightweight compatibility description at the same time. This version data, in its generic form called \u201ccomponent revisions\u201d and the basic fine-grained \u201ctype revisions\u201d, can subsequently be used for consistency verification during application composition or component update. This paper presents first the founding view on the structure of software specifications into traits of the \u201cprovides\u201d,\u201crequires\u201d and \u201cbehaviour\u201d categories. Then, the versioning system with its implementation for the SOFA component framework are described in detail. It is shown how giving the revision numbers a precise semantics and providing additional versioning information provides a combined support for the so-far separated tasks of configuration management in component applications.", "num_citations": "4\n", "authors": ["935"]}
{"title": "UML-Test Application for Automated Validation of Students' UML Class Diagram\n", "abstract": " This article deals with the issue of how to semi-automatically validate novice programmers' UML class diagrams which document their software assignments. This problem is rather serious given the large numbers of students in the introductory programming courses. The article discusses a self-contained tool which makes it easy to prepare very detailed automated tests (hundreds of them) of UML class diagrams. The tool allows to run these tests and evaluate them both individually and statistically. The described tool, which is freely available, has been used for one year in an elementary course of object oriented programming based on the Java programming language, alongside with three other tools used for automatic validation of students' projects. The article discusses both the tool's principles including a technical solution and the experience gained from its use.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Duck testing enhancements for automated validation of student programmes\n", "abstract": " This article deals with the issue how to test the quality of novice programmers\u2019 software assignments. This problem is becoming serious due to the hundreds of students in the introductory courses of programming. The article discusses the motivation for using quality of implementation tests of students\u2019 programmes, their principles and a practical solution. So called \u201cduck tests\u201d are used for this type of validation. A combination of a framework Duckapter, JUnit library and own programmes constitutes the practical solution. It is represented by a self-contained tool which is freely at disposal. The described tool has been used for three years in the elementary course of object oriented programming based on the Java programming language, alongside three other tools used for automatic validation of students projects. The article discusses the experience gained from its use and the effects on student\u2019s programming skills.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Static component compatibility visualisation for various component models\n", "abstract": " Current software development sees a trend towards modular architectures such as software libraries or components. The main idea is to increase the reusability and decrease the total price and time to ship of a final product. While this trend is slowly meeting its expectation, some issues still need to be solved. One of them is a consistency verification of application components performed before the product is shipped and deployed. Application failures at a target environment caused by the inconsistencies can be more expensive than the overall save of the modular design was. For that reason, we propose a visualisation tool that shows static incompatibilities of components from various component models. We assume this tool helps developers to discover inconsistencies of component applications before they are released.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Testing a Component-based Application for Road Traffic Crossroad Control using the SimCo Simulation Framework\n", "abstract": " Component-based software development is an important trend in software engineering. Using this approach, a system can be constructed from a set of individual components (i.e. pieces of software with defined interfaces and functionality). On deployment, components are usually not tested for their correct functionality since this is considered to be implied. Simulation tests are nevertheless often used to determine extra-functional properties and quality of services. However, only models of the components are usually used for such testing. In this paper, we present the SimCo -- a simulation framework for testing of real software components in a simulation environment. This enables thorough tests of components without the need to create their (potentially incorrect) models. The SimCo itself is constructed from components as well in order to enable its modularity and usability. The utilization of the SimCo is\u00a0\u2026", "num_citations": "3\n", "authors": ["935"]}
{"title": "Visual clutter reduction for UML component diagrams: A tool presentation\n", "abstract": " Nowadays component applications can easily consist of hundreds or thousands of components and it is thus difficult to understand their structure. Earlier, we proposed a novel approach of removing large part of connections from the diagram while preserving the information about component interconnections. It uses a separated components area to remove the components with a big amount of connections from the main diagram. For each component in this area, clustered interfaces are shown instead of all interfaces, with the ability to show details on demand. In this work we present the implementation of this technique as a web application and a reverse-engineering solution for component applications.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Evaluation Component Architecture Visualization Tools\n", "abstract": " There are many software architecture modeling tools and their use is very common in practice. But a closer look reveals that in the ever more important area of component-based architectures, system architects or component assemblers are constrained by the lack of good model representations. On one hand, a generic representation like UML provides insufficient support for component-specific needs, on the other hand, tools focused on component development sometimes force the user to learn new visual syntax specific for the component model. Advanced features offered by the tools on top of basic architecture visualization are often limited. In this paper we propose a set of criteria for the evaluation of tools for component architecture visualization, considering the needs of both architectural modeling and analytical tasks. These criteria are thoroughly discussed and their use is shown on a case study which evaluates a current state of the art tool.", "num_citations": "3\n", "authors": ["935"]}
{"title": "COMPONENT-BASED SIMULATION FRAMEWORK FOR COMPONENT TESTING USING SPRINGDM.\n", "abstract": " In this paper, a proposal for component-based simulation framework for testing of real software components in simulated environment is presented. Using this framework, the real software components can be tested directly, without necessity for creation of a simulation model of the real components. The main issues of this framework are presented in the paper and possible solutions are discussed.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Issues in static verification of component substitutability\n", "abstract": " Components are an efficient means of implementing software application, ranging from mobile/embedded to enterprise-wide, since they enable to assemble functionality in the form of interdependent but separately maintained parts. This enables fine-grained upgrades where only the affected components are replaced. In this paper we argue that the replacement must include a careful verification step otherwise an incompatible component may break the whole application. We discuss the characteristics of Java-based component models (EJB, OSGi) with respect to this verification and describe a case study of a system which applies static subtyping checks to ensure safe upgrades of Enterprise JavaBeans components.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Component-based software decomposition of flexible manufacturing systems\n", "abstract": " In the environment of worldwide globalization, industrial companies must seek for new technologies to be able to compete in the ongoing years. This paper suggests one solution: Flexible Manufacturing Systems (FMS) made of Software Appliances (SOFA) components. A FMS, designed at Chalmers University of Technology, GSteborg, is a generic, flexible, self-controlling production control system for machining cells, batch plants and similar applications. SOFA is a component software architecture developed at Charles University, Prague. The assumption is that future programs will consist of many independent and reusable modules software components. Dynamic Updating (DCUP) infrastructure allows to upgrade applications at their runtime. The paper describes a design of FMS using SOFA components and outlines the impact on system design, scalability and performance.", "num_citations": "3\n", "authors": ["935"]}
{"title": "Detection of the Fire Drill anti-pattern: Nine real-world projects with ground truth, issue-tracking data, source code density, models and code\n", "abstract": " This package contains items for 9 real-world software projects. The data is supposed to aid the detection of the presence of the Fire Drill anti-pattern. We include data, ground truth, code, and notebooks. The data supports two distinct methods of detecting the AP: a) through issue-tracking data, and b) through the underlying source code. Therefore, this package includes the following:", "num_citations": "2\n", "authors": ["935"]}
{"title": "Service API Modeling and Comparison: A Technology-Independent Approach\n", "abstract": " When service-based applications are used in systems where context varies in time or location (mobile, adaptive systems), clients may need to switch service providers for various reasons like temporary outage or geographical relocation. To prevent negative impacts on overall functionality, both the client and the provider need a way of discovering whether the application programming interface (API) of an alternative web service is compatible with client\u2019s expected contract. However, the diversity of API specification technologies poses a challenge in such evaluation. Building on previous results, in this paper we present a general model of (web) service APIs, a method for creating corresponding service API representations for several technologies, and a method for comparing these representations to evaluate API compatibility from client\u2019s point of view. Both the model and the comparison method have been\u00a0\u2026", "num_citations": "2\n", "authors": ["935"]}
{"title": "Software Engineering Projects Analysis using Interactive Multimodal Graph Explorer-IMiGEr.\n", "abstract": " This paper describes a visualization technique designed to help work with complex diagrams containing multiple types of nodes and edges, by using a combination of visual clutter reduction and graph exploration techniques. We show its application, including preliminary evaluation, on software engineering projects data gathered from various tools and repositories used for software development. An online tool implementing the technique and plans for its extension by a connected view of time perspective data are briefly presented.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Contracts in the wild: A study of java programs (artifact)\n", "abstract": " This artefact contains a dataset of open-source programs obtained from the Maven Central Repository and scripts that first extract contracts from these programs and then perform several analyses on the contracts extracted. The extraction and analysis is fully automated and directly produces the tables presented in the accompanying paper. The results show how contracts are used in real-world program, and how their usage changes between versions and within inheritance hierarchies.", "num_citations": "2\n", "authors": ["935"]}
{"title": "An interactive uml-like visualization for large software diagrams\n", "abstract": " As current software keeps growing in size and complexity, the means to visualize its structure become insufficient. Noticeably, standard UML diagrams and their implementations in the industrial tools can depict only diagrams of certain level of complexity. When the complexity rises above this level, the diagrams become no longer visually understandable and start to hinder analytical reasoning. This is mostly a problem of diagrams created during automated reverse-engineering processes. In this study we summarize and validate a new approach for software structure visualization which aims at supporting visual presentation of large software systems. It combines a notation derived from the UML component diagram with tool-supported interaction, utilizing features like hiding of unnecessary information that can be revealed on demand to reduce complexity of the diagrams. To validate the approach, we implemented\u00a0\u2026", "num_citations": "2\n", "authors": ["935"]}
{"title": "Lowering visual clutter of clusters in component diagrams\n", "abstract": " Nowadays, component applications can easily consist of hundreds or thousands of components and it is thus difficult to understand their structure. Diagram visualization does not help much because of visual clutter caused by big amount of elements and connections. This paper describes an approach of removing a large part of connections from the diagram while preserving the information about component interconnections. It also describes a viewport technique for showing all information about interfaces for selected group of components right in the diagram area. After that it presents novel integration of above mentioned techniques which maps a group of components to the content of a viewport. These techniques are among other benefits useful in the reverse engineering process. The main idea of this technique can be used in a similar way to reduce the clutter in the node-link graphs. To show the effect of this technique, example reduction of lines is discussed. So the better understanding of a diagram is also shown on preliminary results.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Automated generating of OSGi component versions\n", "abstract": " Software components can be found in both enterprise-wide and mobile/embedded solutions. Components are mutually linked and dependent, but encapsulated as black boxes and developed independently. They can be replaced without affecting the rest of the application. This advantage requires careful and complex compatibility checks between both component versions though, otherwise the whole application can be broken down. In present systems, versioning is often used as an instrument representing changes during software development. In this paper we discuss the versioning scheme of the OSGi Release 4 component model and propose an automated generation of component version identifiers. The mechanism is based on an automatic analysis of changes in components\u2019 interfaces which ensures safety of OSGi component upgrades.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Modelov\u00e1n\u00ed existuj\u00edc\u00edch OSGi komponent\n", "abstract": " Pohl\u00ed\u017e\u00edme-li na komponenty jako na zapouzd\u0159en\u00e9 objekty, pracujeme pouze s jejich rozhran\u00edm. Toto rozhran\u00ed je mo\u017eno form\u00e1ln\u011b popsat n\u011bjak\u00fdm modelem a pracovat tak s abstraktn\u00ed reprezentac\u00ed rozhran\u00ed komponenty. P\u0159i skl\u00e1d\u00e1n\u00ed komponentov\u00e9 aplikace lze tedy s v\u00fdhodou pou\u017e\u00edvat tuto reprezentaci. \u010casto se v\u0161ak setk\u00e1v\u00e1me se situac\u00ed, kdy je pot\u0159eba do modelu zahrnout komponentu z\u00edskanou (typicky zakoupenou) od t\u0159et\u00ed strany\u2013v\u011bt\u0161inou pak nem\u00e1me k dispozici zdrojov\u00fd k\u00f3d ani dosta\u010duj\u00edc\u00ed technickou dokumentaci. Modelovou reprezentaci takov\u00e9 komponenty je pak pot\u0159eba z\u00edskat p\u0159\u00edmo z jej\u00ed distribu\u010dn\u00ed formy. Tento \u010dl\u00e1nek shrnuje na\u0161e zku\u0161enosti a probl\u00e9my s t\u00edmto postupem pro komponentov\u00fd model OSGi.", "num_citations": "2\n", "authors": ["935"]}
{"title": "The ENT Meta-Model of Component Interface, version 2\n", "abstract": " Software modules and components have always played a key role in software engineering, primarily as key abstractions that embody the principle of information hiding. Modelling components is an increasingly important task, especially with the current interest in model-driven development and server-side component technologies.This technical report presents an update to the ENT meta-model for structuring component interfaces, originally defined in an earlier report (TR-2002-10 \u201cThe ENT Model: A General Model for Software Interface Structuring\u201d) and author\u2019s PhD thesis [6]. While the core of the meta-model has not changed, three important improvements have been made which warrant the creation this version 2 of ENT.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Simulation approach to embedded system programming and testing\n", "abstract": " We describe how simulation using production code can help in achieving higher quality software while reducing the effort associated with embedded application development. The recommended steps in developing embedded safety-critical software are outlined, and the issues of proper verification of the simulation models are discussed in detail. The feasibility of the approach is shown on a case study of a brake-by-wire application development.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Parametrized visual representation of software components\n", "abstract": " Software components are used successfully for efficient decomposition of complex systems. Despite the variations of existing component models and platforms, they share many characteristics. What is missing however is a tool support for visual construction of software from components. This paper proposes a novel graphical representation of components. It is based on a structured meta-model of component interface, called the ENT model, which uses human classification of interface elements. The representation makes it possible to parametrize the display of components by pre-defined categories of element types, grouped according to their classification.", "num_citations": "2\n", "authors": ["935"]}
{"title": "Towards Translation of Semantics of Automotive Interface Description Models from Franca to AUTOSAR Frameworks: An Approach using Semantic Synergies\n", "abstract": " The automotive industry is eventually evolving into a complex network of services. The heterogeneous and distributed nature of automotive software systems demands flexible software components which can operate in different environments. Because of heterogeneous automotive development environments, the domain experts, must cope with too many diversities, adaption layers, and incompatibilities to design applications for the current generation of autonomous driving vehicles. In this context, interface adaptation is a promising approach to achieve flexibility without directly changing the respective components. AUTOSAR, which is the de-facto standard for describing automotive system architecture and is a hugely comprehensive standard allowing designers full control from abstract system description to bare metal level deployment. However, the vehicle subsystems have still evolved to include multifarious\u00a0\u2026", "num_citations": "1\n", "authors": ["935"]}
{"title": "An advanced interactive visualization approach with extra functional properties\n", "abstract": " This work presents an Advanced Interactive Visualization Approach (AIVA) that is able to visualize the structure of many component-based applications. Moreover, AIVA is able to visualize extra-functional properties (EFP) in combination with the traditional visualization of the component structure. To our best knowledge, existing tools cover both areas separately. Hence, the approach of this work fills this gap, in a form of an advanced tool. AIVA is currently being scientifically evaluated, however, the use of extra-functional properties is still in the initial phase. In this paper, we demonstrate our early approach to display a variety of EFPs in a visually convenient manner, integrated in AIVA.", "num_citations": "1\n", "authors": ["935"]}
{"title": "Properties and Verification of Component-Based Systems\n", "abstract": " Component-based software engineering is concerned with the composition of software systems from well-defined building blocks, software components. They embody the principles of modularity and information hiding in perhaps the clearest form so far and can be in theory treated as pure black boxes, accessed only according to the specification of their interface. Third-party composition and substitution is one of the key consequences (and also benefits) of this nature of components. For these benefits to materialize, the concrete component models must however adhere to the principles and provide support for safe component substitution. Several studies have shown that this is not always the case, leading to problems in component application consistency caused by hard-to-analyse hidden dependencies and weak verification mechanisms. Safe component substitution is particularly challenging in the case of component models enabling dynamic evolution of application architecture. This thesis summarizes author\u2019s contributions to this field of research in two complementary areas. First, it presents the work on conceptually clean component models with complete type system representation of component interface amenable to automated processing. Secondly, it shows that efficient consistency verification mechanisms can be built on this basis, utilizing the unique characteristics of components and helping to reduce application failures caused by incorrect component composition or architectural reconfigurations, in particular substitution. The thesis has the form of a collection of nine articles for which the textual part introduces key concepts and\u00a0\u2026", "num_citations": "1\n", "authors": ["935"]}
{"title": "High-level simulation of embedded systems: experiences from the FIT project\n", "abstract": " This paper summarizes the experiences gained from the EU project FIT which was aimed at the verification of TTP/C protocol. Several fault injection techniques have been successfully applied during the project, but we focus mainly on the \"high level simulation\" approach. The key contribution of the paper is a summary of the lessons learned from our experiences with functional verification of embedded systems, using the discrete-time simulation method", "num_citations": "1\n", "authors": ["935"]}
{"title": "O v\u00fdvoji p\u0159eklad\u016f odborn\u00fdch term\u00edn\u016f v oboru ICT a slovn\u00edkov\u00e9m projektu SPOT\n", "abstract": " Zamy\u0161len\u00ed nad problematikou kvality p\u0159eklad\u016f odborn\u00e9 literatury do \u010de\u0161tiny, zejm\u00e9na v oblasti informa\u010dn\u00edch a komunika\u010dn\u00edch technologi\u00ed. P\u0159edstaven\u00ed projektu on-line slovn\u00edku pro p\u0159\u00edpravu a sd\u00edlen\u00ed odborn\u00e9 terminologie v duchu kolektivn\u00ed moudrosti.", "num_citations": "1\n", "authors": ["935"]}