{"title": "Crowdoracles: Can the crowd solve the oracle problem?\n", "abstract": " Despite the recent advances in test generation, fully automatic software testing remains a dream: Ultimately, any generated test input depends on a test oracle that determines correctness, and, except for generic properties such as \u201cthe program shall not crash\u201d, such oracles require human input in one form or another. CrowdSourcing is a recently popular technique to automate computations that cannot be performed by machines, but only by humans. A problem is split into small chunks, that are then solved by a crowd of users on the Internet. In this paper we investigate whether it is possible to exploit CrowdSourcing to solve the oracle problem: We produce tasks asking users to evaluate CrowdOracles - assertions that reflect the current behavior of the program. If the crowd determines that an assertion does not match the behavior described in the code documentation, then a bug has been found. Our experiments\u00a0\u2026", "num_citations": "104\n", "authors": ["319"]}
{"title": "Combining search-based and constraint-based testing\n", "abstract": " Many modern automated test generators are based on either meta-heuristic search techniques or use constraint solvers. Both approaches have their advantages, but they also have specific drawbacks: Search-based methods get stuck in local optima and degrade when the search landscape offers no guidance; constraint-based approaches, on the other hand, can only handle certain domains efficiently. In this paper we describe a method that integrates both techniques and delivers the best of both worlds. On a high-level view, our method uses a genetic algorithm to generate tests, but the twist is that during evolution a constraint solver is used to ensure that mutated offspring efficiently explores different control flow. Experiments on 20 case study examples show that on average the combination improves branch coverage by 28% over search-based techniques and by 13% over constraint-based techniques.", "num_citations": "93\n", "authors": ["319"]}
{"title": "Modeling readability to improve unit tests\n", "abstract": " Writing good unit tests can be tedious and error prone, but even once they are written, the job is not done: Developers need to reason about unit tests throughout software development and evolution, in order to diagnose test failures, maintain the tests, and to understand code written by other developers. Unreadable tests are more difficult to maintain and lose some of their value to developers. To overcome this problem, we propose a domain-specific model of unit test readability based on human judgements, and use this model to augment automated unit test generation. The resulting approach can automatically generate test suites with both high coverage and also improved readability. In human studies users prefer our improved tests and are able to answer maintenance questions about them 14% more quickly at the same level of accuracy.", "num_citations": "86\n", "authors": ["319"]}
{"title": "A survey on unit testing practices and problems\n", "abstract": " Unit testing is a common practice where developers write test cases together with regular code. Automation frameworks such as JUnit for Java have popularised this approach, allowing frequent and automatic execution of unit test suites. Despite the appraisals of unit testing in practice, software engineering researchers see potential for improvement and investigate advanced techniques such as automated unit test generation. To align such research with the needs of practitioners, we conducted a survey amongst 225 software developers, covering different programming languages and 29 countries, using a global online marketing research platform. The survey responses confirm that unit testing is an important factor in software development, and suggest that there is indeed potential and need for research on automation of unit testing. The results help us to identify areas of importance on which further research will\u00a0\u2026", "num_citations": "79\n", "authors": ["319"]}
{"title": "Entropy-based test generation for improved fault localization\n", "abstract": " Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average\u00a0\u2026", "num_citations": "76\n", "authors": ["319"]}
{"title": "Automatically testing self-driving cars with search-based procedural content generation\n", "abstract": " Self-driving cars rely on software which needs to be thoroughly tested. Testing self-driving car software in real traffic is not only expensive but also dangerous, and has already caused fatalities. Virtual tests, in which self-driving car software is tested in computer simulations, offer a more efficient and safer alternative compared to naturalistic field operational tests. However, creating suitable test scenarios is laborious and difficult. In this paper we combine procedural content generation, a technique commonly employed in modern video games, and search-based testing, a testing technique proven to be effective in many domains, in order to automatically create challenging virtual scenarios for testing self-driving car soft-ware. Our AsFault prototype implements this approach to generate virtual roads for testing lane keeping, one of the defining features of autonomous driving. Evaluation on two different self-driving car\u00a0\u2026", "num_citations": "63\n", "authors": ["319"]}
{"title": "Search-based data-flow test generation\n", "abstract": " Coverage criteria based on data-flow have long been discussed in the literature, yet to date they are still of surprising little practical relevance. This is in part because 1) manually writing a unit test for a data-flow aspect is more challenging than writing a unit test that simply covers a branch or statement, 2) there is a lack of tools to support data-flow testing, and 3) there is a lack of empirical evidence on how well data-flow testing scales in practice. To overcome these problems, we present 1) a search-based technique to automatically generate unit tests for data-flow criteria, 2) an implementation of this technique in the Evosuite test generation tool, and 3) a large empirical study applying this tool to the SF100 corpus of 100 open source Java projects. On average, the number of coverage objectives is three times as high as for branch coverage. However, the level of coverage achieved by Evosuite is comparable to other\u00a0\u2026", "num_citations": "59\n", "authors": ["319"]}
{"title": "Generating test suites with augmented dynamic symbolic execution\n", "abstract": " Unit test generation tools typically aim at one of two objectives: to explore the program behavior in order to exercise automated oracles, or to produce a representative test set that can be used to manually add oracles or to use as a regression test set. Dynamic symbolic execution (DSE) can efficiently explore all simple paths through a program, exercising automated oracles such as assertions or code contracts. However, its original intention was not to produce representative test sets. Although DSE tools like Pex can retain subsets of the tests seen during the exploration, customer feedback revealed that users expect different values than those produced by Pex, and sometimes also more than one value for a given condition or program path. This threatens the applicability of DSE in a scenario without automated oracles. Indeed, even though all paths might be covered by DSE, the resulting tests are usually not\u00a0\u2026", "num_citations": "36\n", "authors": ["319"]}
{"title": "Gamification of software testing\n", "abstract": " Writing good software tests is difficult, not every software developer's favorite occupation, and not a prominent aspect in programming education. However, human involvement in testing is unavoidable: What makes a test good is often down to intuition; what makes a test useful depends on an understanding of the program context; what makes a test find bugs depends on understanding the intended program behaviour. Because the consequences of insufficient testing can be dire, this paper explores a new angle to address the testing problem: Gamification is the approach of converting potentially tedious or boring tasks to components of entertaining gameplay, where the competitive nature of humans motivates them to compete and excel. By applying gamification concepts to software testing, there is potential to fundamentally change software testing in several ways: First, gamification can help to overcome\u00a0\u2026", "num_citations": "35\n", "authors": ["319"]}
{"title": "Generating unit tests for concurrent classes\n", "abstract": " As computers become more and more powerful, programs are increasingly split up into multiple threads to leverage the power of multi-core CPUs. However, writing correct multi-threaded code is a hard problem, as the programmer has to ensure that all access to shared data is coordinated. Existing automated testing tools for multi-threaded code mainly focus on re-executing existing test cases with different schedules. In this paper, we introduce a novel coverage criterion that enforces concurrent execution of combinations of shared memory access points with different schedules, and present an approach that automatically generates test cases for this coverage criterion. Our CONSUITE prototype demonstrates that this approach can reliably reproduce known concurrency errors, and evaluation on nine complex open source classes revealed three previously unknown data-races.", "num_citations": "34\n", "authors": ["319"]}
{"title": "An evaluation of model checkers for specification based test case generation\n", "abstract": " Under certain constraints the test case generation problem can be represented as a model checking problem, thus enabling the use of powerful model checking tools to perform the test case generation automatically. There are, however, several different model checking techniques, and to date there is little evidence and comparison on which of these techniques is best suited for test case generation. This paper presents the results of an evaluation of several different model checkers on a set of realistic formal specifications given in the SCR notation. For each specification test cases are generated for a set of coverage criteria with each of the model checkers using different configurations. The evaluation shows that the best suited model checking technique and optimization very much depend on the specification that is used to generate test cases. However, from the experiments we can draw general conclusions\u00a0\u2026", "num_citations": "34\n", "authors": ["319"]}
{"title": "Experiments on the test case length in specification based test case generation\n", "abstract": " Many different techniques have been proposed to address the problem of automated test case generation, varying in a range of properties and resulting in very different test cases. In this paper we investigate the effects of the test case length on resulting test suites: Intuitively, longer test cases should serve to find more difficult faults but will reduce the number of test cases necessary to achieve the test objectives. On the other hand longer test cases have disadvantages such as higher computational costs and they are more difficult to interpret manually. Consequently, should one aim to generate many short test cases or fewer but longer test cases? We present the results of a set of experiments performed in a scenario of specification based testing for reactive systems. As expected, a long test case can achieve higher coverage and fault detecting capability than a short one, while giving preference to longer test cases\u00a0\u2026", "num_citations": "33\n", "authors": ["319"]}
{"title": "Behaviourally adequate software testing\n", "abstract": " Identifying a finite test set that adequately captures the essential behaviour of a program such that all faults are identified is a well-established problem. Traditional adequacy metrics can be impractical, and may be misleading even if they are satisfied. One intuitive notion of adequacy, which has been discussed in theoretical terms over the past three decades, is the idea of behavioural coverage, if it is possible to infer an accurate model of a system from its test executions, then the test set must be adequate. Despite its intuitive basis, it has remained almost entirely in the theoretical domain because inferred models have been expected to be exact (generally an infeasible task), and have not allowed for any pragmatic interim measures of adequacy to guide test set generation. In this work we present a new test generation technique that is founded on behavioural adequacy, which combines a model evaluation\u00a0\u2026", "num_citations": "29\n", "authors": ["319"]}
{"title": "Generating minimal fault detecting test suites for general boolean specifications\n", "abstract": " ContextBoolean expressions are a central aspect of specifications and programs, but they also offer dangerously many ways to introduce faults. To counter this effect, various criteria to generate and evaluate tests have been proposed. These are traditionally based on the structure of the expressions, but are not directly related to the possible faults. Often, they also require expressions to be in particular formats such as disjunctive normal form (DNF), where a strict hierarchy of faults is available to prove fault detection capability.ObjectiveThis paper describes a method that generates test cases directly from an expression\u2019s possible faults, guaranteeing that faults of any chosen class will be detected. In contrast to many previous criteria, this approach does not require the Boolean expressions to be in DNF, but allows expressions in any format, using any deliberate fault classes.MethodThe presented approach is based\u00a0\u2026", "num_citations": "29\n", "authors": ["319"]}
{"title": "Assessing and generating test sets in terms of behavioural adequacy\n", "abstract": " Identifying a finite test set that adequately captures the essential behaviour of a program such that all faults are identified is a well\u2010established problem. This is traditionally addressed with syntactic adequacy metrics (e.g.\u2009branch coverage), but these can be impractical and may be misleading even if they are satisfied. One intuitive notion of adequacy, which has been discussed in theoretical terms over the past three decades, is the idea of behavioural coverage: If it is possible to infer an accurate model of a system from its test executions, then the test set can be deemed to be adequate. Despite its intuitive basis, it has remained almost entirely in the theoretical domain because inferred models have been expected to be exact (generally an infeasible task) and have not allowed for any pragmatic interim measures of adequacy to guide test set generation. This paper presents a practical approach to incorporate\u00a0\u2026", "num_citations": "27\n", "authors": ["319"]}
{"title": "Uncertainty-driven black-box test data generation\n", "abstract": " We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as \"Query Strategy Framework\": We infer a behavioural model of the system under test and select those tests which the inferred model is \"least certain\" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an\u00a0\u2026", "num_citations": "25\n", "authors": ["319"]}
{"title": "Generating effective test cases for self-driving cars from police reports\n", "abstract": " Autonomous driving carries the promise to drastically reduce the number of car accidents; however, recently reported fatal crashes involving self-driving cars show that such an important goal is not yet achieved. This calls for better testing of the software controlling self-driving cars, which is difficult because it requires producing challenging driving scenarios. To better test self-driving car soft-ware, we propose to specifically test car crash scenarios, which are critical par excellence. Since real car crashes are difficult to test in field operation, we recreate them as physically accurate simulations in an environment that can be used for testing self-driving car software. To cope with the scarcity of sensory data collected during real car crashes which does not enable a full reproduction, we extract the information to recreate real car crashes from the police reports which document them. Our extensive evaluation, consisting of a\u00a0\u2026", "num_citations": "24\n", "authors": ["319"]}
{"title": "Bytecode testability transformation\n", "abstract": " Bytecode as produced by modern programming languages is well suited for search-based testing: Different languages compile to the same bytecode, bytecode is available also for third party libraries, all predicates are atomic and side-effect free, and instrumentation can be performed without recompilation. However, bytecode is also susceptible to the flag problem; in fact, regular source code statements such as floating point operations might create unexpected flag problems on the bytecode level. We present an implementation of state-of-the-art testability transformation for Java bytecode, such that all Boolean values are replaced by integers that preserve information about branch distances, even across method boundaries. The transformation preserves both the original semantics and structure, allowing it to be transparently plugged into any bytecode-based testing tool. Experiments on flag problem\u00a0\u2026", "num_citations": "24\n", "authors": ["319"]}
{"title": "Generating minimal fault detecting test suites for boolean expressions\n", "abstract": " New coverage criteria for Boolean expressions are regularly introduced with two goals: to detect specific classes of realistic faults and to produce as small as possible test suites. In this paper we investigate whether an approach targeting specific fault classes using several reduction policies can achieve that less test cases are generated than by previously introduced testing criteria. In our approach, the problem of finding fault detecting test cases can be formalized as a logical satisfiability problem, which can be efficiently solved by a SAT algorithm. We compare this approach with respect to the well-known MUMCUT and Minimal-MUMCUT strategies by applying it to a series of case studies commonly used as benchmarks, and show that it can reduce the number of test cases further than Minimal-MUMCUT.", "num_citations": "23\n", "authors": ["319"]}
{"title": "Improving random GUI testing with image-based widget detection\n", "abstract": " Graphical User Interfaces (GUIs) are amongst the most common user interfaces, enabling interactions with applications through mouse movements and key presses. Tools for automated testing of programs through their GUI exist, however they usually rely on operating system or framework specific knowledge to interact with an application. Due to frequent operating system updates, which can remove required information, and a large variety of different GUI frameworks using unique underlying data structures, such tools rapidly become obsolete, Consequently, for an automated GUI test generation tool, supporting many frameworks and operating systems is impractical. We propose a technique for improving GUI testing by automatically identifying GUI widgets in screen shots using machine learning techniques. As training data, we generate randomized GUIs to automatically extract widget information. The resulting\u00a0\u2026", "num_citations": "22\n", "authors": ["319"]}
{"title": "Testing scratch programs automatically\n", "abstract": " Block-based programming environments like Scratch foster engagement with computer programming and are used by millions of young learners. Scratch allows learners to quickly create entertaining programs and games, while eliminating syntactical program errors that could interfere with progress. However, functional programming errors may still lead to incorrect programs, and learners and their teachers need to identify and understand these errors. This is currently an entirely manual process. In this paper, we introduce a formal testing framework that describes the problem of Scratch testing in detail. We instantiate this formal framework with the Whisker tool, which provides automated and property-based testing functionality for Scratch programs. Empirical evaluation on real student and teacher programs demonstrates that Whisker can successfully test Scratch programs, and automatically achieves an average\u00a0\u2026", "num_citations": "21\n", "authors": ["319"]}
{"title": "A survey on the tool support for the automatic evaluation of mobile accessibility\n", "abstract": " To make a mobile app or website accessible, developers must incorporate accessibility practices into the development activities and perform accessibility evaluations. Accessibility evaluation is usually a manual approach, which can be laborious and difficult to scale or reproduce. Often, the implementation and evaluation of accessibility requirements are overlooked by developers due to high demand and time to market pressure. Automating mobile accessibility evaluation can help, but few approaches and tools are currently available. In addition, few studies discuss for which accessibility guidelines automatic evaluation is supported. Therefore, the aim of this paper is to survey automated accessibility evaluation tools for mobile applications to identify which accessibility properties can be automatically evaluated given the available tools. We expect our findings will provide practitioners with an overview of which tools\u00a0\u2026", "num_citations": "20\n", "authors": ["319"]}
{"title": "Relating counterexamples to test cases in CTL model checking specifications\n", "abstract": " Counterexamples produced by model checkers are frequently exploited for the purpose of testing. Counterexamples and test cases are generally treated as essentially the same thing, while in fact they can differ significantly. For example, it might take more than one test case to\" cover\" a given counterexample, because not all property violations can be illustrated with linear counterexamples. This paper presents a formal relationship between counterexamples and test cases in the context of the Computation Tree Logic (CTL), the logic of the popular model checker SMV. Given a test requirement as a CTL formula, we define what it means for a set of test cases to cover a counterexample associated with that requirement. This result can not only be used in the generation of a test set that satisfies a given test coverage criterion, but also in the determination of whether an extant test set satisfies the criterion. Our results\u00a0\u2026", "num_citations": "20\n", "authors": ["319"]}
{"title": "Semi-automatic search-based test generation\n", "abstract": " Search-based testing techniques can efficiently generate test data to achieve high code coverage. However, when the fitness function does not provide sufficient guidance, the search will only generate optimal results by chance. Yet, where the search algorithm struggles, a human tester with domain knowledge can often produce solutions easily. We therefore include the tester in the test generation process: When the search stagnates, the tester is given an opportunity to improve the current solution, and these improvements are fed back to the search. In particular, relevant problems occur often when generating tests for object-oriented languages, where test cases are sequences of method calls. Constructing complex objects through sequences of method calls is difficult, and often the traditional branch distance offers little guidance - yet for a human tester the same task is often trivial. In this paper, we present a semi\u00a0\u2026", "num_citations": "19\n", "authors": ["319"]}
{"title": "Mostly harmless team description\n", "abstract": " Reference: G. Steinbauer, M. Faschinger, G. Fraser, A. M\u00fchlenfeld, S. Richter, G. W\u00f6ber, and J. Wolf. Mostly harmless team description. In D. Polani, B. Browning, A. Bondarini, and K. Yoshida, editors, RoboCup 2003: Robot Soccer World Cup VII, volume 3020 of Lecture Notes in Artificial Intelligence, Padova, Italy, 2003. Springer.", "num_citations": "17\n", "authors": ["319"]}
{"title": "Common bugs in scratch programs\n", "abstract": " Bugs in SCRATCH programs can spoil the fun and inhibit learning success. Many common bugs are the result of recurring patterns of bad code. In this paper we present a collection of common code patterns that typically hint at bugs in SCRATCH programs, and the LitterBox tool which can automatically detect them. We empirically evaluate how frequently these patterns occur, and how severe their consequences usually are. While fixing bugs inevitably is part of learning, the possibility to identify the bugs automatically provides the potential to support learners.", "num_citations": "15\n", "authors": ["319"]}
{"title": "Automated software testing with model checkers\n", "abstract": " Testing is the most commonly applied technique to ensure a sufficiently high quality of software. Automation is desirable because testing is very complex, time consuming, and error prone when done manually. The use of model checkers, tools intended for formal verification, to automatically derive test cases is a promising technique. Model checkers produce counterexamples, which are linear sequences illustrating property violations. These counterexamples are suitable as test cases. While full automation is possible and different concrete techniques have been proposed, there are still many issues that need resolving to encourage industrial adoption.The applicablity of model checker based testing is limited by the performance of the model checker in use. While the well known state space explosion is a major contributor to this issue, there are further problems. Often, too many test cases are created, or identical test cases are created several times unnecessarily. This thesis describes techniques to improve the performance of the test case generation.", "num_citations": "13\n", "authors": ["319"]}
{"title": "Automatic testing of natural user interfaces\n", "abstract": " Automated test generation can effectively explore programs through their programmer interfaces and traditional graphical user interfaces, but the recent advent of natural user interfaces (NUI) based on motion and gesture detection, for example the Microsoft Kinect, has outrun software testing research. This leaves a rapidly growing domain of software ranging from entertainment to medical applications without suitable test automation techniques. To address this issue, we propose a technique that automatically tests Kinect-based applications by synthesising realistic sequences of skeletal movement. The novel test cases are generated by a statistical model, which is trained on a corpus of common gestures. Evaluation on a gesture-controlled Kinect web browser application demonstrates that our approach achieves significantly higher code coverage than random test inputs.", "num_citations": "12\n", "authors": ["319"]}
{"title": "Asfault: Testing self-driving car software using search-based procedural content generation\n", "abstract": " Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common, but costly and risky method, which has already caused fatalities. As a safer alternative, virtual tests, in which self-driving car software is tested in computer simulations, have been proposed. One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for systematically testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car software, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https\u00a0\u2026", "num_citations": "11\n", "authors": ["319"]}
{"title": "Generating readable unit tests for Guava\n", "abstract": " Unit tests for object-oriented classes can be generated automatically using search-based testing techniques. As the search algorithms are typically guided by structural coverage criteria, the resulting unit tests are often long and confusing, with possible negative implications for developer adoption of such test generation tools, and the difficulty of the test oracle problem and test maintenance. To counter this problem, we integrate a further optimization target based on a model of test readability learned from human annotation data. We demonstrate on a selection of classes from the Guava library how this approach produces more readable unit tests without loss of coverage.", "num_citations": "11\n", "authors": ["319"]}
{"title": "An evaluation of specification based test generation techniques using model checkers\n", "abstract": " Test case generation can be represented as a model checking problem, such that model checking tools automatically generate test cases. This has previously been applied to testing techniques such as coverage criteria, combinatorial testing, mutation testing, or requirements testing, and further criteria can be used similarly. However, little comparison between the existing techniques has been done to date, making it difficult to choose a technique. In this paper we define existing and new criteria in a common framework, and evaluate and compare them on a set of realistic specifications. Part of our findings is that because testing with model checkers represents the test case generation problem in a very flexible way best results can be achieved by combining several techniques from different categories. A best effort approach where test cases are only created for uncovered test requirements can create relatively\u00a0\u2026", "num_citations": "11\n", "authors": ["319"]}
{"title": "Modelling hand gestures to test leap motion controlled applications\n", "abstract": " Programs that use a Natural User Interface (NUI) are not controlled with a mouse and keyboard, but through input devices that monitor the user's body movements. Manually testing applications through such interfaces is time-consuming. Generating realistic test data automatically is also challenging, because the input is a complex data structure that represents real body structures and movements. Previously, it has been shown that models learned from user interactions can be used to generate tests for NUI applications controlled by the Microsoft Kinect. In this paper, we study the case of the Leap Motion input device, which allows applications to be controlled with hand movements and finger positions, resulting in substantially more complex input data structures. We present a framework to model human hand data interacting with applications, and generate test data automatically from these models. We also\u00a0\u2026", "num_citations": "10\n", "authors": ["319"]}
{"title": "An empirical study of flaky tests in python\n", "abstract": " Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular PyPI package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more\u00a0\u2026", "num_citations": "9\n", "authors": ["319"]}
{"title": "Verified from scratch: program analysis for learners' programs\n", "abstract": " Block-based programming languages like Scratch support learners by providing high-level constructs that hide details and by preventing syntactically incorrect programs. Questions nevertheless frequently arise: Is this program satisfying the given task? Why is my program not working? To support learners and educators, automated program analysis is needed for answering such questions. While adapting existing analyses to process blocks instead of textual statements is straightforward, the domain of programs controlled by block-based languages like Scratch is very different from traditional programs: In Scratch multiple actors, represented as highly concurrent programs, interact on a graphical stage, controlled by user inputs, and while the block-based program statements look playful, they hide complex mathematical operations that determine visual aspects and movement. Analyzing such programs is further\u00a0\u2026", "num_citations": "9\n", "authors": ["319"]}
{"title": "Automated Unit Test Generation for Python\n", "abstract": " Automated unit test generation is an established research field, and mature test generation tools exist for statically typed programming languages such as Java. It is, however, substantially more difficult to automatically generate supportive tests for dynamically typed programming languages such as Python, due to the lack of type information and the dynamic nature of the language. In this paper, we describe a foray into the problem of unit test generation for dynamically typed languages. We introduce Pynguin, an automated unit test generation framework for Python. Using Pynguin, we aim to empirically shed light on two central questions: (1) Do well-established search-based test generation methods, previously evaluated only on statically typed languages, generalise to dynamically typed languages? (2) What is the influence of incomplete type information and dynamic typing on the problem of automated\u00a0\u2026", "num_citations": "8\n", "authors": ["319"]}
{"title": "A tutorial on using and extending the evosuite search-based test generator\n", "abstract": " EvoSuite is an automated unit test generation tool for Java. It takes as input a Java class under test, and produces JUnit tests optimised for code coverage, and enhanced with regression assertions, as output. This paper is a tutorial on how to use EvoSuite to generate tests, on how to build and extend EvoSuite, and how to use EvoSuite to run experiments on search-based testing.", "num_citations": "7\n", "authors": ["319"]}
{"title": "Reachability and propagation for LTL requirements testing\n", "abstract": " It is important to test software with respect to user requirements, especially when adhering to safety standards, which require traceability from requirements to test cases. While research has resulted in many different model based testing techniques, only a few consider requirement properties; this paper helps fill this gap. We identify two fundamental characteristics of a test case intended to evaluate a given requirement property. The two characteristics are adapted from the venerable Reachability, Infection, and Propagation (RIP) model for faults and failures in ordinary code. In the context of requirements testing, we propose the reachability property amounts to the property not being vacuously true on a given test case, and the propagation property amounts to a potential violation of the property on the test case being observable. In particular, we formalize these notions in the context of requirement properties\u00a0\u2026", "num_citations": "7\n", "authors": ["319"]}
{"title": "Ai-planning system for robotic soccer\n", "abstract": " Robotic soccer was created as a playground for a wealth of scientific fields of research highlighting in the RoboCup middle-size league as its environment comes very close to the real world: two teams each of four autonomous robots compete against each other in a soccer game using only slightly modified FIFA soccer rules. There are no external sensors, all data is gathered by the robots using sensors they bear. The task of teaching these robots to act logically, plan their actions and behave as a team, in spite of all the negative influences like uncertain or incomplete data and communication problems, is intriguing.Currently many of the dominating teams in RoboCup base their success on sophisticated reactive systems with limited planning capabilities. However the AI-planning approach remains attractive, as this thesis shows. After a detailed background of planning theory and current research topics, the planning system for the RoboCup team \u201cMostly Harmless\u201d, which was implemented as part of this thesis, is introduced. However, focus is not only laid on plan creation but in contrast also on plan execution.", "num_citations": "7\n", "authors": ["319"]}
{"title": "Causes and effects of fitness landscapes in unit test generation\n", "abstract": " Search-based unit test generation applies evolutionary search to maximize code coverage. Although the performance of this approach is often good, sometimes it is not, and how the fitness landscape affects this performance is poorly understood. This paper presents a thorough analysis of 331 Java classes by (i) characterizing their fitness landscape using six established fitness landscape measures,(ii) analyzing the impact of these fitness landscape measures on the search, and (iii) investigating the underlying properties of the source code influencing these measures. Our results reveal that classical indicators for rugged fitness landscapes suggest well searchable problems in the case of unit test generation, but the fitness landscape for most problem instances is dominated by detrimental plateaus. A closer look at the underlying source code suggests that these plateaus are frequently caused by code in private\u00a0\u2026", "num_citations": "6\n", "authors": ["319"]}
{"title": "Search\u2010based testing using constraint\u2010based mutation\n", "abstract": " Many modern automated test generators are based on either metaheuristic search techniques or use constraint solvers. Both approaches have their advantages, but they also have specific drawbacks: Search\u2010based methods may get stuck in local optima and degrade when the search landscape offers no guidance; constraint\u2010based approaches, on the other hand, can only handle certain domains efficiently. This paper describes a method that integrates both techniques and delivers the best of both worlds. On a high\u2010level view, the proposed method uses a genetic algorithm to generate tests, but the twist is that during evolution, a constraint solver is used to ensure that mutated offspring efficiently explores different control flow. Experiments on 20 case study programmes show that on average the combination improves branch coverage by 28% over search\u2010based techniques while reducing the number of tests by 55\u00a0\u2026", "num_citations": "6\n", "authors": ["319"]}
{"title": "AC3R: automatically reconstructing car crashes from police reports\n", "abstract": " Autonomous driving carries the promise to drastically reduce car accidents, but recently reported fatal crashes involving self-driving cars suggest that the self-driving car software should be tested more thoroughly. For addressing this need, we introduce AC3R (Automatic Crash Constructor from Crash Report) which elaborates police reports to automatically recreate car crashes in a simulated environment that can be used for testing self-driving car software in critical situations. AC3R enables developers to quickly generate relevant test cases from the massive historical dataset of recorded car crashes. We demonstrate how AC3R can generate simulations of different car crashes and report the findings of a large user study which concluded that AC3R simulations are accurate. A video illustrating AC3R in action is available at: https://youtu.be/V708fDG_ux8.", "num_citations": "5\n", "authors": ["319"]}
{"title": "Verification of platform-independent and platform-specific semantics of dependable embedded systems\n", "abstract": " The behavior of a dependable embedded system is affected by many factors. The verification process of such a system has to consider the platform-independent as well as the platform-specific semantics of the developed software. In this work we focus on the differentiation of the platformdependent and platform-specific characteristics of a system under test and present a unified framework that shows the correctness of a an embedded system in two steps: First we verify the platform-independent semantics by showing that the system under test conforms to the specification. This is done by means of formal verification. Secondly we prove if the platform-specific semantics of the system on the target platform still conforms to the requirements defined in the specification. This step is realized by applying automatically generated test cases to the system under test on the target platform.", "num_citations": "5\n", "authors": ["319"]}
{"title": "An empirical evaluation of search algorithms for app testing\n", "abstract": " Automated testing techniques can effectively explore mobile applications in order to find faults that manifest as program crashes. A number of different techniques for automatically testing apps have been proposed and empirically compared, but previous studies focused on comparing different tools, rather than techniques. Although these studies have shown search-based approaches to be effective, it remains unclear whether superior performance of one tool compared to another is due to fundamental advantages of the underlying search technique, or due to certain engineering choices made during the implementation of the tools. In order to provide a better understanding of app testing as a search problem, we empirically study different search algorithms within the same app testing framework. Experiments on a selection of 10 non-trivial apps reveal that the costs of fitness evaluations are inhibitive, and\u00a0\u2026", "num_citations": "4\n", "authors": ["319"]}
{"title": "Parameter Control in Search-Based Generation of Unit Test Suites\n", "abstract": " Search-based testing supports developers by automatically generating test suites with high coverage, but the effectiveness of a search-based test generator depends on numerous parameters. It is unreasonable to expect developers to understand search algorithms well enough to find the optimal parameter settings for a problem at hand, and even if they did, a static value for a parameter can be suboptimal at any given point during the search. To counter this problem, parameter control methods have been devised to automatically determine and adapt parameter values throughout the search. To investigate whether parameter control methods can also improve search-based generation of test suites, we have implemented and evaluated different methods to control the crossover and mutation rate in the EvoSuite unit test generation tool. Evaluation on a selection of open source Java classes reveals that\u00a0\u2026", "num_citations": "4\n", "authors": ["319"]}
{"title": "LitterBox: A Linter for Scratch Programs\n", "abstract": " Creating programs with block-based programming languages like SCRATCH is easy and fun. Block-based programs can nevertheless contain bugs, in particular when learners have misconceptions about programming. Even when they do not, SCRATCH code is often of low quality and contains code smells, further inhibiting understanding, reuse, and fun. To address this problem, in this paper we introduce LITTERBOX, a linter for SCRATCH programs. Given a program or its public project ID, LITTERBOX checks the program against patterns of known bugs and code smells. For each issue identified, LITTERBOX provides not only the location in the code, but also a helpful explanation of the underlying reason and possible misconceptions. Learners can access LITTERBOX through an easy to use web interface with visual information about the errors in the block-code, while for researchers LITTERBOX provides a\u00a0\u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "Tutorial on a Gamification Toolset for Improving Engagement of Students in Software Engineering Courses\n", "abstract": " Few if any would dispute that educating software engineering is a challenging endeavour. Although programming and creating new artefacts can motivate the creativity of students. Other software engineering topics (like e.g. requirement specifications and testing) are not considered very exciting by students. However, these topics are important to develop quality software and insufficient knowledge of students \u2014 Europe's future software engineers \u2014 in the long run contributes to failing software. The EU Erasmus+ project IMPRESS was set to explore the use of gamification in educating software engineering at the university level. The objective has been to develop a toolset that can help to improve students' engagement, and hence their appreciation, for the taught subjects like software testing and specifications. The proposed tutorial will guide participants through the set of tools developed by the project and\u00a0\u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "Measuring and Maintaining Population Diversity in Search-Based Unit Test Generation\n", "abstract": " Genetic algorithms (GAs) have been demonstrated to be effective at generating unit tests. However, GAs often suffer from a loss of population diversity, which causes the search to prematurely converge, thus negatively affecting the resulting code coverage. One way to prevent premature convergence is to maintain and increase population diversity. Although the impact of population diversity on the performance of GAs is well-studied in the literature, little attention has been given to population diversity in unit test generation. We study how maintaining population diversity influences the Many-Objective Sorting Algorithm (MOSA), a state-of-the-art evolutionary search algorithm for generating unit tests. We define three diversity measures based on fitness entropy, test executions (phenotypic diversity), and Java statements (genotypic diversity). To improve diversity, we apply common methods that fall into two\u00a0\u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "IMPRESS: Improving engagement in software engineering courses through gamification\n", "abstract": " Software Engineering courses play an important role for preparing students with the right knowledge and attitude for software development in practice. The implication is far reaching, as the quality of the software that we use ultimately depends on the quality of the people that make them. Educating Software Engineering, however, is quite challenging, as the subject is not considered as most exciting by students, while teachers often have to deal with exploding number of students. The EU project IMPRESS seeks to explore the use of gamification in educating software engineering at the university level to improve students\u2019 engagement and hence their appreciation for the taught subjects. This paper presents the project, its objectives, and its current progress.", "num_citations": "3\n", "authors": ["319"]}
{"title": "Automatically reconstructing car crashes from police reports for testing self-driving cars\n", "abstract": " Autonomous driving carries the promise to drastically reduce the number of car accidents; however, recently reported fatal crashes involving self-driving cars show this important goal is not yet achieved, and call for better testing of the software controlling self-driving cars. To better test self-driving car software, we propose to specifically test critical scenarios. Since these are difficult to test in field operation, we create simulations of critical situations. These simulations are automatically derived from natural language police reports of actual car crashes, which are available in historical datasets. Our initial evaluation shows that we can generate accurate simulations in a matter of minutes.", "num_citations": "3\n", "authors": ["319"]}
{"title": "Proceedings of the 4th international conference on Search Based Software Engineering\n", "abstract": " Proceedings of the 4th international conference on Search Based Software Engineering | Guide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsSSBSE'12 ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide Proceedings cover image SSBSE'12: Proceedings of the 4th international conference on Search Based Software Engineering September 2012 291 pages ISBN:9783642331183 Editors: Gordon Fraser , de \u00a9 \u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "Mostly harmless team description 2004\n", "abstract": " In this paper we describe the Mostly Harmless RoboCup Middle-Size League (MSL) team. The team is an interdisciplinary cooperation of three faculties of our university (Mechanical Engineering, Electrical Engineering and Computer Science). Such cooperations are crucial in robotics as the related topics are manifold. The project has a strong focus on education. We think that it is very important to interest students for robotics and to provide them a sound education in this area. The MSL is an ideal basis for that purpose. The team comprises of graduate or PhD students who carry out their master or PhD thesis within the project under supervision of senior researchers. Furthermore, the MSL is an ideal testbed for various research carried out at our university. The research interests of the team comprise of planning, planexecution, qualitative representation and interpretation of the world, software engineering, model-based diagnosis and illumination insensitive vision. We will present ongoing research in those areas.Mostly Harmless participated in RoboCup for the first time in 2003. Encouraging results achieved in the RoboCup competition 2003 proved that the designs and concepts used within our team are sound. We present recent developments for our robots. These improvements mainly concern navigation and motion planning, which have been detected as weak points in past tournaments.", "num_citations": "3\n", "authors": ["319"]}
{"title": "Ai-planning for robotic soccer\n", "abstract": " AI-Planning for Robotic Soccer \u2014 Technische Universit\u00e4t Graz Zur Hauptnavigation wechseln Zur Suche wechseln Zum Hauptinhalt wechseln Technische Universit\u00e4t Graz Logo English Deutsch Home Personen Forschungsgruppen Ver\u00f6ffentlichungen Projekte Verwandte T\u00e4tigkeiten Pr\u00e4mien Presseberichte AI-Planning for Robotic Soccer Gordon Fraser Institut f\u00fcr Softwaretechnologie (7160) Publikation: Studienabschlussarbeit \u203a Diplomarbeit \u203a Forschung \u00dcbersicht (Administrator/-in) Originalsprache englisch Publikationsstatus Ver\u00f6ffentlicht - 2003 Treatment code (N\u00e4here Zuordnung) Theoretical Experimental Dieses zitieren APA Standard Harvard Vancouver Author BIBTEX RIS Fraser, G. (2003). AI-Planning for Robotic Soccer. AI-Planning for Robotic Soccer. / Fraser, Gordon. 2003. Publikation: Studienabschlussarbeit \u203a Diplomarbeit \u203a Forschung Fraser, G 2003, 'AI-Planning for Robotic Soccer'. Fraser G. AI-for . . , . \u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "Plan Description and Execution with Invariants, The Planning System of the RoboCup Team Mostly Harmless\n", "abstract": " Plan Description and Execution with Invariants, The Planning System of the RoboCup Team Mostly Harmless \u2014 Technische Universit\u00e4t Graz Zur Hauptnavigation wechseln Zur Suche wechseln Zum Hauptinhalt wechseln Technische Universit\u00e4t Graz Logo English Deutsch Home Personen Forschungsgruppen Ver\u00f6ffentlichungen Projekte Verwandte T\u00e4tigkeiten Pr\u00e4mien Presseberichte Plan Description and Execution with Invariants, The Planning System of the RoboCup Team Mostly Harmless Gordon Fraser, Franz Wotawa Institut f\u00fcr Softwaretechnologie (7160) Publikation: Beitrag in einer Fachzeitschrift \u203a Artikel \u203a Forschung \u00dcbersicht (Administrator/-in) Projekte (1) Originalsprache englisch Seiten (von - bis) 2-7 Fachzeitschrift \u00d6GAI-Journal Jahrgang 22 Ausgabenummer 4 Publikationsstatus Ver\u00f6ffentlicht - 2003 Projekte 2002 2002 1 Laufend RoboCup Wotawa, F., Kandlhofer, M., Weiglhofer, M., Reip, M., Gspandl, .\u2026", "num_citations": "3\n", "authors": ["319"]}
{"title": "Finding Anomalies in Scratch Assignments\n", "abstract": " In programming education, teachers need to monitor and assess the progress of their students by investigating the code they write. Code quality of programs written in traditional programming languages can be automatically assessed with automated tests, verification tools, or linters. In many cases these approaches rely on some form of manually written formal specification to analyze the given programs. Writing such specifications, however, is hard for teachers, who are often not adequately trained for this task. Furthermore, automated tool support for popular block-based introductory programming languages like SCRATCH is lacking. Anomaly detection is an approach to automatically identify deviations of common behavior in datasets without any need for writing a specification. In this paper, we use anomaly detection to automatically find deviations of SCRATCH code in a classroom setting, where anomalies can\u00a0\u2026", "num_citations": "2\n", "authors": ["319"]}
{"title": "SnapCheck: Automated Testing for Snap Programs\n", "abstract": " Programming environments such as Snap, Scratch, and Processing engage learners by allowing them to create programming artifacts such as apps and games, with visual and interactive output. Learning programming with such a media-focused context has been shown to increase retention and success rate. However, assessing these visual, interactive projects requires time and laborious manual effort, and it is therefore difficult to offer automated or real-time feedback to students as they work. In this paper, we introduce SnapCheck, a dynamic testing framework for Snap that enables instructors to author test cases with Condition-Action templates. The goal of SnapCheck is to allow instructors or researchers to author property-based test cases that can automatically assess students' interactive programs with high accuracy. Our evaluation of SnapCheck on 162 code snapshots from a Pong game assignment in an introductory programming course shows that our automated testing framework achieves at least 98% accuracy over all rubric items, showing potentials to use SnapCheck for auto-grading and providing formative feedback to students.", "num_citations": "2\n", "authors": ["319"]}
{"title": "Motivating Adult Learners by Introducing Programming Concepts with Scratch\n", "abstract": " Block-based programming languages like Scratch are popular for introducing children to programming. As programming is becoming an increasingly desired skill in almost every working environment, a growing number of adults are seeking to learn basic programming skills. Unlike children, adults often immediately start with a text-based language like Python or Java. This raises the question of whether there is an opportunity to improve adult programming education using block-based programming. In order to investigate this question, we conducted a study in the environment of a beginner's Python programming class for non-computer science university students and staff, into which we integrated an initial Scratch exercise. While the additional exercise had no significant effect on the participants' abilities, we do see a positive effect on their self-perception and motivation to continue learning programming.", "num_citations": "2\n", "authors": ["319"]}
{"title": "Parallel Many-Objective Search for Unit Tests\n", "abstract": " Meta-heuristic search algorithms such as genetic algorithms have been applied successfully to generate unit tests, but typically take long to produce reasonable results, achieve sub-optimal code coverage, and have large variance due to their stochastic nature. Parallel genetic algorithms have been shown to be an effective improvement over sequential algorithms in many domains, but have seen little exploration in the context of unit test generation to date. In this paper, we describe a parallelised version of the many-objective sorting algorithm (MOSA) for test generation. Through the use of island models, where individuals can migrate between independently evolving populations, this algorithm not only reduces the necessary search time, but produces overall better results. Experiments with an implementation of parallel MOSA on the EvoSuite test generation tool using a large corpus of complex open source Java\u00a0\u2026", "num_citations": "2\n", "authors": ["319"]}
{"title": "Guest editorial: Search-based software engineering\n", "abstract": " With modern computer systems growing in size and complexity, the demands to functionality, scalability, and robustness of software engineering techniques present ever new challenges. Search-based software engineering (SBSE) has emerged as a promising direction of research that can successfully counter some of these challenges. SBSE involves the automatic\u2013or semi-automatic-resolution of complex software engineering problems modeled as optimization problems, using search algorithms, in particular metaheuristics. An exponential growth of the number of published papers in this area in the last decade bears witness to the popularity search-based approaches within the software engineering research community, and SBSE has been successfully applied to solve problems in nearly all software development life cycle phases. This issue presents three articles that are all based on search-techniques, yet\u00a0\u2026", "num_citations": "2\n", "authors": ["319"]}
{"title": "Effects of Hints on Debugging Scratch Programs: An Empirical Study with Primary School Teachers in Training\n", "abstract": " Bugs in learners\u2019 programs are often the result of fundamental misconceptions. Teachers frequently face the challenge of first having to understand such bugs, and then suggest ways to fix them. In order to enable teachers to do so effectively and efficiently, it is desirable to support them in recognising and fixing bugs. Misconceptions often lead to recurring patterns of similar bugs, enabling automated tools to provide this support in terms of hints on occurrences of common bug patterns. In this paper, we investigate to what extent the hints improve the effectiveness and efficiency of teachers in debugging learners\u2019 programs using a cohort of 163 primary school teachers in training, tasked to correct buggy programs, with and without hints on bug patterns. Our experiment suggests that automatically generated hints can reduce the effort of finding and fixing bugs from 8.66 to 5.24 minutes, while increasing the effectiveness\u00a0\u2026", "num_citations": "1\n", "authors": ["319"]}
{"title": "Data-driven Analysis of Gender Differences and Similarities in Scratch Programs\n", "abstract": " Block-based programming environments such as Scratch are an essential entry point to computer science. In order to create an effective learning environment that has the potential to address the gender imbalance in computer science, it is essential to better understand gender-specific differences in how children use such programming environments. In this paper, we explore gender differences and similarities in Scratch programs along two dimensions: In order to understand what motivates girls and boys to use Scratch, we apply a topic analysis using unsupervised machine learning for the first time on Scratch programs, using a dataset of 317 programs created by girls and boys in the range of 8\u201310 years. In order to understand how they program for these topics, we apply automated program analysis on the code implemented in these projects. We find that, in-line with common stereotypes, girls prefer topics that\u00a0\u2026", "num_citations": "1\n", "authors": ["319"]}
{"title": "Code Perfumes: Reporting Good Code to Encourage Learners\n", "abstract": " Block-based programming languages like enable children to be creative while learning to program. Even though the block-based approach simplifies the creation of programs, learning to program can nevertheless be challenging. Automated tools such as linters therefore support learners by providing feedback about potential bugs or code smells in their programs. Even when this feedback is elaborate and constructive, it still represents purely negative criticism and by construction ignores what learners have done correctly in their programs. In this paper we introduce an orthogonal approach to linting: We complement the criticism produced by a linter with positive feedback. We introduce the concept of code perfumes as the counterpart to code smells, indicating the correct application of programming practices considered to be good. By analysing not only what learners did wrong but also what they did right we hope\u00a0\u2026", "num_citations": "1\n", "authors": ["319"]}
{"title": "Guiding Next-Step Hint Generation Using Automated Tests\n", "abstract": " Learning basic programming with Scratch can be hard for novices and tutors alike: Students may not know how to advance when solving a task, teachers may face classrooms with many raised hands at a time, and the problem is exacerbated when novices are on their own in online or virtual lessons. It is therefore desirable to generate next-step hints automatically to provide individual feedback for students who are stuck, but current approaches rely on the availability of multiple hand-crafted or hand-selected sample solutions from which to draw valid hints, and have not been adapted for Scratch. Automated testing provides an opportunity to automatically select suitable candidate solutions for hint generation, even from a pool of student solutions using different solution approaches and varying in quality. In this paper we present Catnip, the first next-step hint generation approach for Scratch, which extends existing data-driven hint generation approaches with automated testing. Evaluation of Catnip on a dataset of student Scratch programs demonstrates that the generated hints point towards functional improvements, and the use of automated tests allows the hints to be better individualized for the chosen solution path.", "num_citations": "1\n", "authors": ["319"]}
{"title": "Automated Classification of Visual, Interactive Programs Using Execution Traces\n", "abstract": " Offering students immediate, formative feedback when they are programming can increase students\u2019 learning outcomes and self-efficacy. However, visual and interactive programs include dynamic user input and visual outputs that change over time, making it difficult to automatically assess students\u2019 code with traditional functional tests to offer this feedback. In this work, we introduce Execution Trace Based Feature Engineering (ETF), a feature engineering approach that extracts sequential patterns from execution traces, which capture the runtime behavior of students\u2019 code. We evaluated ETF on 162 students\u2019 code snapshots from a Pong game assignment in an introductory programming course, on a challenging task to predict students\u2019 success on fine-grained rubrics. We found that ETF achieves an average F1 score of 0.93 over 10 grading rubrics, which is 0.1\u20130.2 higher than a high-performing syntax-based code classification approach from prior work. These results show that ETF has strong potential to be used for code classification, to enable formative feedback for students\u2019 visual, interactive programs.", "num_citations": "1\n", "authors": ["319"]}
{"title": "Search-Based Testing for Scratch Programs\n", "abstract": " Block-based programming languages enable young learners to quickly implement fun programs and games. The Scratch programming environment is particularly successful at this, with more than 50 million registered users at the time of this writing. Although Scratch simplifies creating syntactically correct programs, learners and educators nevertheless frequently require feedback and support. Dynamic program analysis could enable automation of this support, but the test suites necessary for dynamic analysis do not usually exist for Scratch programs. It is, however, possible to cast test generation for Scratch as a search problem. In this paper, we introduce an approach for automatically generating test suites for Scratch programs using grammatical evolution. The use of grammatical evolution clearly separates the search encoding from framework-specific implementation details, and allows us to use\u00a0\u2026", "num_citations": "1\n", "authors": ["319"]}
{"title": "2nd international workshop on crowd sourcing in software engineering (CSI-SE 2015)\n", "abstract": " Crowdsourcing is increasingly revolutionizing the ways in which software is engineered. Programmers increasingly crowdsource answering their questions through Q&A sites. Non-programmers may contribute human-intelligence to development projects, by, for example, usability testing software or even play games with a purpose to implicitly construct formal specifications. Crowdfunding helps to democratize decisions about what software to build. Software engineering researchers may even benefit from new opportunities to evaluate their work with real developers by recruiting developers from the crowd. CSI- SE will inform the software engineering community of current techniques and trends in crowdsourcing, discuss the application of crowdsourcing to software engineering to date, and identify new opportunities to apply crowdsourcing to solve software engineering problems.", "num_citations": "1\n", "authors": ["319"]}
{"title": "Preface: Special section on Mutation testing (Mutation 2010)\n", "abstract": " Preface: Special section on Mutation testing (Mutation 2010): Science of Computer Programming: Vol 78, No 4 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Science of Computer Programming Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsScience of Computer ProgrammingVol. , No. Preface: Special section on Mutation testing (Mutation 2010) article Preface: Special section on Mutation testing (Mutation 2010) Share on Authors: Lydie Du Bousquet profile image Lydie Du Bousquet View Profile , Jeremy S. Bradbury profile image Jeremy S. Bradbury View Profile , Gordon Fraser profile image Gordon Fraser View Profile Authors Info & : of ://\u2026", "num_citations": "1\n", "authors": ["319"]}