{"title": "Delivering clinical decision support services: there is nothing as practical as a good theory\n", "abstract": " In \u2018\u2018Grand challenges for decision support\u201d Sittig et al.[51] set out 10 critical problems for \u2018\u2018designing, developing, presenting, implementing, evaluating, and maintaining all types of clinical decision support capabilities for clinicians, patients and consumers\u201d. Sittig et al.\u2019s identification and prioritization of obstacles to the successful development and deployment of clinical decision support (CDS) technology drew on the experience of the authors and a number of consultants who are recognised leaders in the field. Their top 10 challenges include many that others in the field would strongly agree with; indeed they were identified and prioritized on the basis of empirical experience and with the expectation that overcoming these challenges will depend heavily on practical problem solving and finding out \u2018\u2018what works\u201d in clinical use. They published their discussion in order to \u2018\u2018inspire stakeholders in a position to advance the state of CDS technology and practice\u201d. As active participants in this field we take the position that an empirical approach to design must be accompanied by sound theoretical principles and safe engineering methods. We illustrate this with an approach to CDS design that starts with a formal model of knowledge-based decision-making, clinical processes and distributed care services, identifying four key pillars of theory, and relationships between them. Sittig et al.\u2019s challenges are reviewed to consider how such a framework can facilitate application design and implementation, clinical use, service interoperability etc. We do not claim a formal approach to design is an alternative to empirical evolution of clinical services but is a\u00a0\u2026", "num_citations": "89\n", "authors": ["1717"]}
{"title": "Basic Gene Grammars and DNA-ChartParser for language processing of Escherichia coli promoter DNA sequences\n", "abstract": " Motivation: The field of \u2018DNA linguistics\u2019 has    emerged from pioneering work in computational linguistics and    molecular biology. Most formal grammars in this field are expressed    using Definite Clause Grammars but these have computational    limitations which must be overcome. The present study provides a new    DNA parsing system, comprising a logic grammar formalism called    Basic Gene Grammars and a bidirectional chart parser    DNA-ChartParser.         Results: The use of Basic Gene Grammars is demonstrated in    representing many formulations of the knowledge of Escherichia coli    promoters, including knowledge acquired from human experts,    consensus sequences, statistics (weight matrices), symbolic    learning, and neural network learning. The DNA-ChartParser provides    bidirectional parsing facilities for BGGs in handling overlapping    categories, gap categories, approximate pattern\u00a0\u2026", "num_citations": "67\n", "authors": ["1717"]}
{"title": "Programming the social computer\n", "abstract": " The aim of \u2018programming the global computer\u2019 was identified by Milner and others as one of the grand challenges of computing research. At the time this phrase was coined, it was natural to assume that this objective might be achieved primarily through extending programming and specification languages. The Internet, however, has brought with it a different style of computation that (although harnessing variants of traditional programming languages) operates in a style different to those with which we are familiar. The \u2018computer\u2019 on which we are running these computations is a social computer in the sense that many of the elementary functions of the computations it runs are performed by humans, and successful execution of a program often depends on properties of the human society over which the program operates. These sorts of programs are not programmed in a traditional way and may have to be\u00a0\u2026", "num_citations": "53\n", "authors": ["1717"]}
{"title": "A visual syntax for logic and logic programming\n", "abstract": " It is commonly accepted that non-logicians have difficulty in expressing themselves in first-order logic. Part of the visual language community is concerned with providing visual notations which use visual cues (\u2018declarative diagrams\u2019) to make the structuring of logical expressions more intuitive. One of the more successful metaphors used in such diagrammatic languages is that of set inclusion, making use of the graphical intuitions which most of us are taught at school. Existing declarative diagrammatic languages do not make full use of such set-based intuitions. We present a more uniform use of sets which allow simple but highly expressive diagrams to be constructed from a small number of primitive components. These diagrams provide an alternative notation for a computational logic and, as we show in this paper, are the basis of a visual logic programming language. The first implementation of this language and\u00a0\u2026", "num_citations": "41\n", "authors": ["1717"]}
{"title": "Enterprise modelling: A declarative approach for fbpml\n", "abstract": " Enterprise Modelling (EM) methods are well-recognised for their value in describing complex, informal domains in an organised structure. EM methods are used in practice, particularly during the early stages of software system development, e.g. during the phase of business requirements elicitation. The built model, however, has not always provided direct input to software system development.  Despite the provision of adequate training to understand and use EM methods, informality is often seen in enterprise models and presents a major obstacle. This paper focuses on one type of EM methods: business process modelling (BPM) methods. We advocate the use of a BPM language within a three-layer framework. The BPM language merges two main and complimentary business process representations, IDEF3 and PSL, to introduce a Fundamental Business Process Modelling Language (FBPML) that is designed for simplicity of use and under-pinned by rich formality that may be used directly to support software and workflow system development.", "num_citations": "40\n", "authors": ["1717"]}
{"title": "Rapid prototyping of large multi-agent systems through logic programming\n", "abstract": " Prototyping is a valuable technique to help software engineers explore the design space while gaining insight on the dynamics of the system. In this paper, we describe a method for rapidly building prototypes of large multi-agent systems using logic programming. Our method advocates the use of a description of all permitted interactions among the components of the system, that is, the protocol, as the starting specification. The protocol is represented in a way that allows us to automatically check for desirable properties of the system to be built. We then employ the same specification to synthesise agents that will correctly follow the protocol. These synthesised agents are simple logic programs that engineers can further customise into more sophisticated software. Our choice of agents as logic programs allows us to provide semi-automatic support for the customisation activity. In our method, a prototype is a\u00a0\u2026", "num_citations": "39\n", "authors": ["1717"]}
{"title": "Applying Prolog programming techniques\n", "abstract": " Much of the skill of Prolog programming comes from the ability to harness its comparatively simple syntax in sophisticated ways. It is possible to provide an account of part of the activity of Prolog programming in terms of the application of techniques\u2014standard patterns of program development which may be applied to a variety of different programming problems. Numerous researchers have attempted to provide formal definitions of Prolog techniques but there has been little standardization of the approach and the computational use of techniques has been limited to small portions of the programming task. We demonstrate that techniques knowledge can be used to support programming in a wide variety of areas: editing, analysis, tracing, transformation and techniques acquisition. We summarize the main features of systems implemented by the authors for each of these types of activity and set these in the context of\u00a0\u2026", "num_citations": "37\n", "authors": ["1717"]}
{"title": "A simple Prolog techniques editor for novice users\n", "abstract": " This paper describes a working prototype system which uses descriptions of standard Prolog techniques to provide a basic techniques editing system, ultimately intended for use by novice programmers. A notation for representing techniques, based on Definite Clause Grammars, is described in the context of previous theoretical work by Kirschenbaum, Lakhotia and Sterling. Details are supplied of a mechanism for using these techniques to provide guidance during program construction and an example is provided of the system in operation. I conclude by suggesting the extensions needed in order to make the prototype useful for practical applications.", "num_citations": "37\n", "authors": ["1717"]}
{"title": "A heuristic approach to modelling thinnings\n", "abstract": " Thinnings play an important role in guiding forest development and are considered by many to be the most important influence on forests in Central Europe. Due to their importance, thinning models are a major part of any forest growth model for managed forests. Existing thinning model approaches have a number of problems associated with structure and model development that weaken their reliability and accuracy. To overcome some of these problems this paper proposes a heuristic approach to modelling thinnings, where the focus is on distance-dependent, single-tree models. This alternative approach tries to capture the information, strategies and deductive processes likely to be employed by a forester deciding on the removal of individual trees in a stand. Use of heuristics to represent thinning knowledge simplifies the construction and refinement of a thinning model and increases its plausibility. The\u00a0\u2026", "num_citations": "35\n", "authors": ["1717"]}
{"title": "Flexible multi-agent protocols\n", "abstract": " In this paper we define a novel technique for the specification of agent protocols in Multi-Agent-Systems. This technique addresses a number of shortcomings of previous Electronic Institution based specifications. In particular, we relax the static specification of agent protocols as state-based diagrams and allow protocols to be defined and disseminated in a flexible manner during agent interaction. Our flexible specification is derived from process algebra and thus forms a sound basis for the verification of such systems.", "num_citations": "30\n", "authors": ["1717"]}
{"title": "A framework for requirements elicitation through mixed-initiative dialogue\n", "abstract": " We present our work on requirements elicitation. The elicitation process is a complex task which necessitates computer support. Elicitation systems should ideally help their users check the correctness of the specifications obtained but also actively guide them in the acquisition of the requirements. We consider systems that communicate in natural language. We describe a framework that tries to improve the quality of the guidance it provides to its users by taking into account natural language constraints. We discuss the need for a theory of natural language dialogue structure, and we show how we have integrated such a theory within an early prototype of an elicitation system.", "num_citations": "29\n", "authors": ["1717"]}
{"title": "A collaboration model for community-based software development with social machines\n", "abstract": " Today's crowdsourcing systems are predominantly used for processing independent tasks with simplistic coordination. As such, they offer limited support for handling complex, intellectually and organizationally challenging labour types, such as software development. In order to support crowdsourcing of the software development processes, the system needs to enact coordination mechanisms which integrate human creativity with machine support. While workflows can be used to handle highly-structured and predictable labour processes, they are less suitable for software development methodologies where unpredictability is an unavoidable part the process. This is especially true in phases of requirement elicitation and feature development, when both the client and development communities change with time. In this paper we present models and techniques for coordination of human workers in crowdsourced\u00a0\u2026", "num_citations": "27\n", "authors": ["1717"]}
{"title": "Open Knowledge: Semantic webs through peer-to-peer interaction\n", "abstract": " We present a manifesto for a new form of knowledge sharing that is based not on direct sharing of \"true\" statements about the world but, instead, is based on sharing descriptions of interactions. By making interaction specifications the currency of knowledge sharing we gain a context to interpreting knowledge that can be transmitted between peers. The narrower notion of semantic commitment we thus obtain requires peers only to commit to meanings of terms for the purposes and duration of the interactions in which they appear. This lightweight semantics allows networks of interaction to be formed between peers using comparatively simple means of tackling the perennial issues of query routing, service composition and ontology matching. Although the entire system described in this manifesto has not been built, all its components use established methods; many of these have been deployed in substantial applications; and we summarize a simple means of integration.", "num_citations": "27\n", "authors": ["1717"]}
{"title": "Formal support for an informal business modelling method\n", "abstract": " Business modelling methods are popular but, since they operate primarily in the early stages of software lifecycles, most are informal. This paper describes how we have used a conventional formal notation (first order predicate logic) in combination with automated support tools to replicate the key components of an established, informal, business modelling method: IBM's Business System Development Method (BSDM). We describe the knowledge which we represent formally at each stage in the method and explain how the move from informal to formal representation allows us to provide guidance and consistency checking during the development lifecycle of the model. It also allows us to extend the original method to a model execution phase which is not described in the original informal method. The role of the formal notation in this case is not to provide a formal semantics for BSDM but to provide a framework\u00a0\u2026", "num_citations": "26\n", "authors": ["1717"]}
{"title": "A visual logic programming language\n", "abstract": " It is commonly accepted that non-logicians have difficulty in expressing themselves in first order logic. Part of the visual language community is concerned with providing visual notations (declarative diagrams) which use visual cues to make the structuring of logical expressions more intuitive. One of the more successful metaphors used in such languages is that of set inclusion, making use of the graphical intuitions which most of us are taught at school. Existing declarative diagramming languages do not make full use of such set-based intuitions. We present a more uniform use of sets in this form of description, which allows simple, but highly expressive diagrams to be constructed from a small number of primitive components. These diagrams, we claim, provide a good alternative notation for a limited, but useful, subset of FOL and, as we show in this paper are the basis of a visual logic programming language.", "num_citations": "26\n", "authors": ["1717"]}
{"title": "How service choreography statistics reduce the ontology mapping problem\n", "abstract": " In open and distributed environments ontology mapping provides interoperability between interacting actors. However, conventional mapping systems focus on acquiring static information, and on mapping whole ontologies, which is infeasible in open systems. This paper shows that the interactions themselves between the actors can be used to predict mappings, simplifying dynamic ontology mapping. The intuitive idea is that similar interactions follow similar conventions and patterns, which can be analysed. The computed model can be used to suggest the possible mappings for the exchanged messages in new interactions. The suggestions can be evaluate by any standard ontology matcher: if they are accurate, the matchers avoid evaluating mappings unrelated to the interaction.               The minimal requirement in order to use this system is that it is possible to describe and identify the interaction\u00a0\u2026", "num_citations": "24\n", "authors": ["1717"]}
{"title": "Exploiting interaction contexts in P2P ontology mapping.\n", "abstract": " Agents in peer-to-peer networks may gather into virtual communities, interacting continuously with agents that represent disparate actors, each of them with different interests, needs and views, and having dissimilar ontologies. Mapping all the combination of ontologies in advance is not feasible simply because all the possible combinations cannot be foreseen. Mapping complete ontologies at run time is a computationally expensive task. The framework proposed in this paper maps only the terms encountered in a dialogue, or those needed to map them. The efficiency in the mapping process is increased by accumulating experience and exploiting it in order to reduce the number of mapping candidates to verify, focusing only on the most likely ones.", "num_citations": "24\n", "authors": ["1717"]}
{"title": "A case study in applying ontologies to augment and reason about the correctness of specifications\n", "abstract": " In this paper we investigate how software specifications can benefit from the presence of formal ontologies to augment and enrich their context. This makes it possible to verify the correctness of the specification with respect to formally represented domain knowledge. We present a meta-interpretation technique that allows us to perform checks for conceptual error occurrences in specifications. We illustrate this approach through a case study: we augmented an existing formal specification presented by Luqi & Cooke with a formal ontology produced by the Information Sciences Institute at USC, the AIRCRAFT ontology. In addition, we explore how we can build and use application specific ontological constraints to detect conceptual errors in specifications.", "num_citations": "23\n", "authors": ["1717"]}
{"title": "Extension of the temporal synchrony approach to dynamic variable bindingin a connectionist inference system\n", "abstract": " The relationship between symbolism and connectionism has been one of the major issues in recent artificial intelligence research. An increasing number of researchers from each side have tried to adopt the desirable characteristics of the approach. A major open question in this field is the extent to which a connectionist architecture can accommodate basic concepts of symbolic inference, such as a dynamic variable binding mechanism and a rule and fact encoding mechanism involving nary predicates. One of the current leaders in this area is the connectionist rule-based system proposed by Shastri and Ajjanagadde. The paper demonstrates that the mechanism for variable binding which they advocate is fundamentally limited, and it shows how a reinterpretation of the primitive components and corresponding modifications of their system can extend the range of inference which can be supported. Our extension\u00a0\u2026", "num_citations": "23\n", "authors": ["1717"]}
{"title": "A case-based reasoning system for regulatory information\n", "abstract": " Describes a knowledge-intensive case-based reasoning system (KICS) which is under development in the domain of statutory building regulations, using case histories from The Scottish Office's Building Directorate. A key aspect of reasoning in this domain is that much of the interpretation of the regulations is not stated directly in the regulatory documents but is derived from experience in applying them to particular cases.< >", "num_citations": "22\n", "authors": ["1717"]}
{"title": "Managing ontological constraints\n", "abstract": " We explore the use of ontological constraints in a new way: deploying them in a software system's formal evaluation. We present a formalism for ontological constraints and elaborate on a meta interpretation technique in the field of ontologies. Ontological constraints often need enhancements to capture application-specific discrepancies. We propose an editing system that provides guidance in building those constraints and we explain how this helps us to detect conceptual errors that reflect a misuse of ontological constructs. We describe a multilayer architecture for performing such checks and we demonstrate its usage via an example case. We speculate on the potential impact of the approach for the system's design process.", "num_citations": "21\n", "authors": ["1717"]}
{"title": "LSCitter: building social machines by augmenting existing social networks with interaction models\n", "abstract": " We present LSCitter, an implemented framework for supporting human interaction on social networks with formal models of interaction, designed as a generic tool for creating social machines on existing infrastructure. Interaction models can be used to choreograph distributed systems, providing points of coordination and communication between multiple interacting actors. While existing social networks specify how interactions happen---who messages go to and when, the effects of carrying out actions---these are typically implicit, opaque and non user-editable. Treating interaction models as first class objects allows the creation of electronic institutions, on which users can then choose the kinds of interaction they wish to engage in, with protocols which are explicit, visible and modifiable. However, there is typically a cost to users to engage with these institutions. In this paper we introduce the notion of\" shadow\u00a0\u2026", "num_citations": "19\n", "authors": ["1717"]}
{"title": "An architecture for the deployment of mobile decision support systems\n", "abstract": " The widespread use of mobile computing devices offers the opportunity of supplying decision support systems, targeted at specific situations and operated at the point of decision. Unfortunately, the computing environment within which such systems are deployed is radically different from the environments in which decision procedures are most easily designed. We summarise experience in reconciling these differences, using a general architecture but working in a specific domain (forest management). This required us to reconcile the constraints of the mobile computing device (limited resources, small touch-screen, support for procedural programming languages only) with the ability to rapidly select and configure different decision procedures for deployment in differing management conditions. We managed to reconcile these conflicting demands by taking advantage of the restricted forms of inference needed in\u00a0\u2026", "num_citations": "19\n", "authors": ["1717"]}
{"title": "Supporting customised reasoning in the agroforestry domain\n", "abstract": " Supporting Customised Reasoning in the Agroforestry Domain \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Supporting Customised Reasoning in the Agroforestry Domain G. Kendon, D. Walker, D. Robertson, M. Haggith, F. Sinclair, R. Muetzelfeldt School of Informatics School of GeoSciences Research output: Contribution to journal \u203a Article \u203a peer-review Overview Original language English Journal The New Review of Applied Expert Systems Volume 1 Publication status Published - 1995 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Kendon, G., Walker, D., Robertson, D., Haggith, M., Sinclair, F., & Muetzelfeldt, R. (1995). \u2026", "num_citations": "19\n", "authors": ["1717"]}
{"title": "Metadata-supported automated ecological modelling\n", "abstract": " Ecological models should be rooted in data derived from observation, allowing methodical model construction and clear accounts of model results with respect to the data. Unfortunately, many models are retrospectively fitted to data because in practice it is difficult to bridge the gap between concrete data and abstract models. Our research is on automated methods to support bridging this gap. The approach proposed consists of raising the data level of abstraction via an ecological metadata ontology and from that, through logic-based knowledge representation and inference, to automatically generate prototypical partial models to be further improved by the modeler. In this chapter we aim to: 1) give an overview of current automated modelling approaches applied to ecology, and relate them to our metadata-based approach under investigation; and 2) explain and demonstrate how it is realized using logic-based\u00a0\u2026", "num_citations": "18\n", "authors": ["1717"]}
{"title": "Motion sickness\n", "abstract": " CiNii \u8ad6\u6587 - Motion sickness CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248 \u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Motion sickness GRIFFIN MJ \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 GRIFFIN MJ \u53ce\u9332\u520a\u884c\u7269 Handbook of Human Vibration Handbook of Human Vibration Chap.7, 271-332, 1990 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u4e57\u308a\u7269 \u9154\u3044(<\u30b7\u30ea\u30fc\u30ba>\u30d2\u30e5\u30fc\u30de\u30f3\u30fb\u30d5\u30a1\u30af\u30bf\u30fc) \u7d30\u7530 \u9f8d\u4ecb , \u6709\u99ac \u6b63\u548c \u3089\u3093\uff1a\u7e9c 60(0), 24-28, 2003 \u53c2\u8003\u6587\u732e 23\u4ef6 \u5927\u5b66\u9662\u8aac\u660e\u4f1a Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10011678559 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u306e\u4e00\u90e8 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/BiblX\u3067\u8868\u793a RIS\u3067\u8868\u793a BibTeX\u3067\u8868\u793a TSV\u3067\u8868\u793a \u554f\u984c\u306e\u6307\u6458 \u30da\u30fc\u30b8\u30c8\u30c3\u30d7\u3078 \u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u2026", "num_citations": "18\n", "authors": ["1717"]}
{"title": "Probabilistic dialogue models for dynamic ontology mapping\n", "abstract": " Agents need to communicate in order to accomplish tasks that they are unable to perform alone. Communication requires agents to share a common ontology, a strong assumption in open environments where agents from different backgrounds meet briefly, making it impossible to map all the ontologies in advance. An agent, when it receives a message, needs to compare the foreign terms in the message with all the terms in its own local ontology, searching for the most similar one. However, the content of a message may be described using an interaction model: the entities to which the terms refer are correlated with other entities in the interaction, and they may also have prior probabilities determined by earlier, similar interactions. Within the context of an interaction it is possible to predict the set of possible entities a received message may contain, and it is possible to sacrifice recall for efficiency by\u00a0\u2026", "num_citations": "17\n", "authors": ["1717"]}
{"title": "The social computer: Combining machine and human computation\n", "abstract": " The social computer is a future computational system that harnesses the innate problem solving, action and information gathering powers of humans and the environments in which they live in order to tackle large scale social problems that are beyond our current capabilities. The hardware of a social computer is supplied by people\u2019s brains and bodies, the environment where they live, including artifacts, e.g., buildings and roads, sensors into the environment, networks and computers; while the software is the people\u2019s minds, laws, organizational and social rules, social conventions, and computer software. Similarly to what happens within a conventional computer and more interestingly within naturally occurring reasoning and control systems like the human body, a social computer exhibits an algorithmic behavior and problem solving capabilities which are the result of very large numbers of local computations, decisions, interactions, data and control information transfers.", "num_citations": "16\n", "authors": ["1717"]}
{"title": "Using multi-agent platform for pure decentralised business workflows\n", "abstract": " This paper describes the development of a distributed multi-agent workflow enacting mechanism starting from a BPEL4WS specification. Our work demonstrates that a multi-agent interaction protocol (Lightweight Coordination Calculus) can be derived from a BPEL4WS specification to enable pure decentralised business workflows. The key difference between our work and other existing multi-agent based systems is that our approach proposes a pure distributed architecture (no centralised controller/coordinator) for deploying workflow systems in an open environment (internat). Moreover, with our approach, existing workflow developing methodologies and business process models can be adopted in as much as possible for multi-agent system development. Some problems we encountered such as mapping problem between the used business process modelling and multi-agent interaction protocol description\u00a0\u2026", "num_citations": "16\n", "authors": ["1717"]}
{"title": "Adaptive agent model: an agent interaction and computation model\n", "abstract": " Software systems must be capable of coping with continuous requirements changes and at the same time wisely make use of emerging components and services to remain useful in their environment. In this paper, the adaptive agent model (AAM) approach is proposed. The AAM uses configurable interaction model to drive adaptive agent behaviour. The model captures user requirements and is maintained by experts at a high level of abstraction. The AAM interaction model has been discussed with regard to interaction specification and interaction coordination, in line with a coordination language for the OpenKnowledge project. A major benefit of using the approach is agents can dynamically choose disparate components and services already developed for computation via their interaction with each other at runtime, when a new interaction model has been configured for them towards an emerging business goal\u00a0\u2026", "num_citations": "16\n", "authors": ["1717"]}
{"title": "Automating business modelling: a guide to using logic to represent informal methods and support reasoning\n", "abstract": " Enterprise Modelling (EM) methods are frequently used by entrepreneurs as an analysis tool for describing and redesigning their businesses. The resulting product, an enterprise model, is commonly used as a blueprint for reconstructing organizations and such effort is often a part of business process re-engineering and improvement initiatives. Automating Business Modelling describes different techniques of providing automated support for enterprise modelling methods and introduces universally used approaches. A running example of a business modelling method is included; providing a framework and detailed explanation as to how to construct automated support for modelling, allowing readers to follow the method to create similar support. Suitable for senior undergraduates and postgraduates of Business Studies, Computer Science and Artificial Intelligence, practitioners in the fields of Knowledge Management, Enterprise Modelling and Software Engineering, this book offers insight and know-how to both student and professional.", "num_citations": "14\n", "authors": ["1717"]}
{"title": "Enacting the distributed business workflows using bpel4ws on the multi-agent platform\n", "abstract": " This paper describes the development of a distributed multi-agent workflow enactment mechanism using the BPEL4WS[1] specification. It demonstrates that a multi-agent protocol (Lightweight Coordination Calculus (LCC)[8]) can be used to interpret a BPEL4WS specification to enable distributed business workflow[5] using web services[2] composition on the multi-agent platform. The key difference between our system and other existing multi-agent based web services composition systems is that with our approach, a business process model(system requirement) can be adopted directly in the multi-agent system, thus reduce the effort on the validation and verification of the interaction protocol (system specification). This approach also provides us with a lightweight way of re-design of large component based systems.", "num_citations": "13\n", "authors": ["1717"]}
{"title": "Applying experienceware to support ontology deployment\n", "abstract": " Experienceware is a paradigm which emerged in the late eighties and evolved during the nineties, resulting in technologies such as experience factories and their constituent experience bases. These are designed to manage experiences collected throughout the life-cycle of a software project. Ontologies emerged round about the same time as a way to represent consensual knowledge about a domain of interest in reusable and sharable formats. Despite their diverse origins and ways of development, there is an overlap of scope regarding one of their goals: to support reuse. In this paper we make use of this overlap by applying the experience factories paradigm to ontology deployment and in particular, to support ontology verification.", "num_citations": "13\n", "authors": ["1717"]}
{"title": "Evaluating focus theories for dialogue management\n", "abstract": " Interactive reasoning tools are usually driven by an agenda of tasks to perform, rather than by conventions of human dialogue. On the other hand, theories of dialogue in natural language tend to ignore the constraints imposed by reasoning tools. This paper presents a system composed of a reasoning module and a dialogue manager which cooperate to produce dialogues that are suitable for reasoning and follow human dialogue conventions. The dialogue manager is driven by focus rules. Various competing focus theories exist but there have been few comparative studies of their use in non-trivial tasks. We make a comparative study of the use of focus theories, which requires us to be precise about our interpretation of our chosen focus theories, and to develop an innovative means of empirical testing for them. We evaluate the theories on an example of combined dialogue and reasoning from the domain of\u00a0\u2026", "num_citations": "13\n", "authors": ["1717"]}
{"title": "Automated Support for Composition of Transformational Components in Knowledge Engineering\n", "abstract": " The knowledge engineering world provides a rich source of software components for transforming formally expressed knowledge on a large scale, such as induction systems, knowledge base refiners and ontology merging tools. Although most of these systems have been designed as stand-alone components, there is interest in making them accessible on the Web, with the ultimate goal in mind that a knowledge engineer should be able, with a small amount of intellectual effort, to locate and assemble sequences of these components to perform complex transformations on large repositories of knowledge. The sorts of transformations used in knowledge engineering are not always trustworthy: some may not preserve the semantics of the knowledge transformed; some may not be able to perform a given transformation reliably under all circumstances. Therefore, it is crucial to have ways of inspecting the key properties we expect to be preserved by each transformational component and of describing how these properties change as new transformations are applied.We present initial experiments on a large-scale knowledge engineering problem and show how an abstract characterisation of knowledge-transformation steps, accompanied by a customisable editor, can allow a high degree of automation in this task. With such an editor we can analyse and represent sequences of general transformation steps and check if properties such as subsumption, completeness and soundness are preserved during different stages of the transformation, by analysing the structure of these sequences.", "num_citations": "12\n", "authors": ["1717"]}
{"title": "Using Meta-Knowledge at the application level\n", "abstract": " Ontologies have become popular in the Artificial Intelligence community as a way to standardise representation of domain knowledge. Despite their advocated use in areas such as knowledge sharing and reuse there is little discussion in the community regarding their application in other areas. In this paper we explore the use of ontologies in improving systems engineering reliability by consistency checking with respect to ontological axioms - making it possible to reason about the correctness of an application with respect to ontological constraints. We applied this approach to diverse areas, each of which demonstrates a different use: deploying ontological axioms in business process modelling and enriching the axiomatisation of an Air Campaign Planning(ACP)-based application; identification of ontological constraints in ecological modelling; and evaluation of ontologies on system dynamics theory at the application level.", "num_citations": "12\n", "authors": ["1717"]}
{"title": "GraSp: A GRAphical SPecification Language for the Preliminary Specification of Logci Programs\n", "abstract": " this paper) is implemented in LPA MacProlog. 1 Requirements Statement for a Simple Design Task", "num_citations": "12\n", "authors": ["1717"]}
{"title": "KICS: a knowledge-intensive case-based reasoning system for statutory building regulations and case histories\n", "abstract": " There have been several knowledge-based systems for statutory building regulations during the last decade, such as Fenves et al's systems using the SASE model, Stone and Wilcox's system using a rule-based approach, and Waard's system using Cornick et al;'s model-based approach. However, they take into account only one side of building regulations, considering them only in the context of design systems and ignoring the existence of case histories. Building regulations are also part of a legal system and have characteristics of law. In this paper, we propose a Knowledge-Intensive Case-based reasoning System which can be used for the retrieval and maintenance of building regulations and case histories. First, we propose a unified knowledge representation scheme for both statutory building regulations and case histories. Second, we describe the retrieval of regulations information, which uses the notion\u00a0\u2026", "num_citations": "12\n", "authors": ["1717"]}
{"title": "Automated reasoning with uncertainties\n", "abstract": " In this work we assume that uncertainty is a multifaceted concept and present a system for automated reasoning with multiple representations of uncertainty.             We present a case study on developing a computational language for reasoning with uncertainty, starting with a semantically sound and computationally tractable language and gradually extending it with specialised syntactic constructs to represent measures of uncertainty, while preserving its unambiguous semantic characterization and computability properties. Our initial language is the language of normal clauses with SLDNF as the inference rule, and we select three specific facets of uncertainty for our study: vagueness, statistics and degrees of belief.             The resulting language is semantically sound and computationally tracable. It also admits relatively efficient implementations employing \u03b1-\u03b2 pruning and caching.", "num_citations": "12\n", "authors": ["1717"]}
{"title": "Discovery and uncertainty in semantic web services\n", "abstract": " Although Semantic Web service discovery has been extensively studied in the literature ([1], [2], [3] and [4]), we are far from achieving an effective, complete and automated discovery process. Using the Incidence Calculus [5], a truth-functional probabilistic calculus, and a lightweight brokering mechanism [6], this article explores the suitability of integrating probabilistic reasoning in Semantic Web services environments. We show how the combination of relaxation of the matching process and evaluation of Web service capabilities based on previous performances of Web service providers enables new possibilities in service discovery.", "num_citations": "11\n", "authors": ["1717"]}
{"title": "The effect of composition on properties of blends from recylced rubber and polypropylene\n", "abstract": " Incorporation of waste (ethylene propylene diene rubber) EPDM into polyolefins has emerged as a new recycling technique that is eco-friendly and cost effective. The purpose of this study was to recycle EPDM, as well as to develop new impact modified blends. This study, which involved reactive blending of waste EPDM and polypropylene (PP) in a co-rotating twin screw extruder in the ratio range of 10/90 to 60/40, determined the effect of t-butyl hydroperoxide compatibilizer, low and high MFI grade of PP and ethylene-propylene impact copolymer on the mechanical properties of the blends. Formulations were injection molded and subsequently tested for tensile, flexural and impact properties. It was noted that the peroxide and the impact copolymer significantly improved the tensile elongation at break and impact resistance, but resulted in a decrease in the modulus. The approach of this investigation was to determine the optimum blending ratio of the components to achieve a balance in the flexural modulus and impact strength.", "num_citations": "11\n", "authors": ["1717"]}
{"title": "A case-based reasoning framework for enterprise model building, sharing and reusing\n", "abstract": " Enterprise model development is essentially a labour-intensive exercise. Human experts depend heavily on prior experience when they are building new models making it a natural domain to apply Case Based Reasoning techniques. Through the provision of model building knowledge, automatic testing and design guidance can be provided by rule-based facilities. Exploring these opportunities requires us not only to determine which forms of knowledge are generic and therefore re-usable, but also how this knowledge can be used to provide useful model building support. This paper presents our experiences in identifying and classifying the knowledge which exists in IBM's BSDM Business Models and applying AI techniques, CBR and Rule-Based reasoning together with a symbolic simulator, to provide more complete support throughout the enterprise model development life cycle.", "num_citations": "11\n", "authors": ["1717"]}
{"title": "Representing interaction of agents at different time granularities\n", "abstract": " In this paper we describe NatureTime logic which we use to represent and reason about the behaviour of interacting agents (in an ecological domain), which behave at different time granularities. Although the traditional application fields of temporal representation and reasoning still raise many interesting theoretical issues, we have been investigating some practical problems of ecological systems which suit different representations of time than those embodied in traditional simulation models of ecosystems. These seem well suited to reconstruction using temporal logic programs.", "num_citations": "11\n", "authors": ["1717"]}
{"title": "Time granularity in simulation models of ecological systems\n", "abstract": " Granularity of time is an important issue for the understanding of how actions performed at coarse levels of time interact with others, working at ner levels. However, it has not received much attention from most AI work on temporal logic. In traditional domains of application (eg databases, planning, natural language, etc.), we may not need to consider it as a problem, but it becomes important in more complex domains, such as ecological modelling. In this domain, aggregation of processes working at di erent time granularities is very di cult to do reliably. We have proposed a new time granularity theory based on modular temporal classes, and have developed a temporal reasoning system to reason about seasonal cycles. This time theory may be a suitable framework for an executable temporal logic for the speci cation of ecological models, where each ecological entity is an active agent.", "num_citations": "11\n", "authors": ["1717"]}
{"title": "User-system dialogues and the notion of focus\n", "abstract": " In recent years, the capabilities of knowledge-based systems to communicate with their users have evolved from simple interactions to complex dialogues. With this evolution comes a need to understand what makes a good dialogue. In this paper, we are concerned with dialogue coherence. We review the notion of focus, which partly explains this property, and its use for user-system communication. First, we examine the major theories dealing with this notion. We describe what their contribution is and how they differ. Then, we illustrate the benefits of using the notion of focus and especially the improvement in text coherence. We pay particular attention to how the notion can concretely be implemented. Its integration with other techniques and theories is described. We conclude the paper by pointing out remaining issues in the understanding of the notion of focus. The contribution of this paper is to provide a\u00a0\u2026", "num_citations": "10\n", "authors": ["1717"]}
{"title": "Cooperation between knowledge based systems\n", "abstract": " For as long as there has been interest in knowledge based systems there has been interest in sharing formally expressed knowledge. It is traditional for this to require a high degree of social interaction between the suppliers and recipients of such information but the Internet has brought with it an interest in more opportunistic, semi-automatic cooperation between systems. This raises a variety of technical problems relevant to logic programming. We discuss these-concentrating on the use of protocols and interlinguas.", "num_citations": "10\n", "authors": ["1717"]}
{"title": "Use of case-based reasoning in the domain of building regulations\n", "abstract": " In traditional legal decision support systems, it has been regarded as natural to represent statutes in terms of decision rules and to link these to a separate case-based reasoning system for handling precedent. Statutory legal rules used in these systems are formal and prescriptive. Building regulations in Scotland are part of statute law and constitute part of a legal system together with case histories. In recent years, the regulations have been becoming less prescriptive and more emphasis has been put onto the interpretive use of the regulations. In developing a system which can assist domain experts in interpreting the regulations, this trend has presented us with difficulties in employing this traditional approach and has led us to a unified case-based model of the regulations and case histories. In this paper, we first describe the characteristics of the regulations and the activities involved in this domain\u00a0\u2026", "num_citations": "10\n", "authors": ["1717"]}
{"title": "Expressing program requirements using refinement lattices\n", "abstract": " Requirements capture is a term used in software engineering, referring to the process of obtaining a problem description\u2013a high level account of the problem which a user wants to solve. This description is then used to control the generation of a program appropriate to the solution of this problem. Reliable requirements capture is seen as a key component of future automated program construction systems, since even small amounts of information about the type of problem being tackled can often vastly reduce the space of appropriate application programs. Many special purpose requirements capture systems exist but few of these are logic based and all of them operate in tightly constrained domains. In previous research, we have used a combination of order sorted logic (for problem description) and Prolog (for the generated program) in an attempt to provide a more general purpose requirements capture system\u00a0\u2026", "num_citations": "10\n", "authors": ["1717"]}
{"title": "RESPIRE: The National Institute for Health Research's (NIHR) Global Respiratory Health Unit\n", "abstract": " EDITORIAL www. jogh. org\u2022 doi: 10.7189/jogh. 08.020101 2 December 2018\u2022 Vol. 8 No. 2\u2022 020101 cally focused approach allows RESPIRE to develop a regional network of excellence in four South Asian countries, namely Bangladesh, India, Malaysia and Pakistan, though over time the intention is to extend the Unit\u2019s activities. Interested colleagues from other countries are invited to contact RESPIRE via the website (www. ed. ac. uk/usher/respire) which gives more information on the work of the Unit, and provides contact details on how to get involved and support RESPIRE\u2019s goals in other countries.In the 2017 Global Burden of Disease (GBD) study, lower respiratory tract infections (LRTI), chronic obstructive pulmonary disease (COPD), tuberculosis (TB) and lung cancer were all ranked among the top 12 medical conditions (out of nearly 300) in terms of global premature mortality measured as Years of Life Lost\u00a0\u2026", "num_citations": "9\n", "authors": ["1717"]}
{"title": "Bootstrapping the next generation of social machines\n", "abstract": " The term \u201csocial machines\u201d denotes a class of systems where humans and machines interact so that computational infrastructure supports human creativity. Flagship examples such as Wikipedia and Ushahidi demonstrate how computational coordination can enhance information sharing and aggregation, while the Zooniverse family of projects show how social machines can produce scientific knowledge. These socio-technical systems cannot easily be analysed in purely computational or purely sociological terms, and they cannot be reduced to Turing machines. Social machines are used in the creation of software, from software crowdsourcing projects such as TopCoder and oDesk, to distributed development platforms such at GitHub and Bitbucket                 . Hence, social machines are increasingly used to create the software infrastructure                  for new social machine. However, social machine\u00a0\u2026", "num_citations": "9\n", "authors": ["1717"]}
{"title": "Peer-to-peer experimentation in protein structure prediction: an architecture, experiment and initial results\n", "abstract": " Abstract                              Peer-to-peer approaches offer some direct solutions to modularity and scaling properties in large scale distributed systems but their role in supporting precise experimental analysis in bioinformatics has not been explored closely in practical settings. We describe a method by which precision in experimental process can be maintained within a peer-to-peer architecture and show how this can support experiments. As an example we show how our system is used to analyse real data of relevance to the structural bioinformatics community. Comparative models of yeast protein structures from three individual resources were analysed for consistency between them. We created a new resource containing only model fragments supported by agreement between the methods. Resources of this kind provide small sets of likely accurate predictions for non-expert users and are of interest in\u00a0\u2026", "num_citations": "9\n", "authors": ["1717"]}
{"title": "Addressing Constraint Failures in Agent Interaction Protocol\n", "abstract": " The field of multi-agent systems shifts attention from one particular agent to a society of agents; hence the interactions between agents in the society become critical towards the achievement of their goals. We assume that the interactions are managed via an agent protocol which enables agents to coordinate their actions in order to handle the dependencies that exist between their activities. An agent\u2019s failure to comply with the constraints attached within the protocol might cause a brittle protocol to fail. To address this problem, a constraint relaxation approach is applied using a distributed protocol language called the Lightweight Coordination Calculus (LCC). This paper describes the design and implementation of a constraint relaxation module to be integrated within the LCC framework. The working of this module is later illustrated using a scenario involving the ordering and configuration of a computer\u00a0\u2026", "num_citations": "9\n", "authors": ["1717"]}
{"title": "Knowledge life cycle management over a distributed architecture\n", "abstract": " In order to address problems stemming from the dynamic nature of distributed systems, there is a need to be able to express the often neglected notions of the evolution and change of the knowledge components of such systems. This need becomes more pressing when one considers the potential of the Internet for distributed knowledge-based problem solving\u2014and the pragmatic issues surrounding knowledge integrity.In this paper, we introduce a formal calculus for describing transformations in the \u2018life cycles\u2019 of knowledge components, along with ideas about the nature of distributed environments in which the ideas underpinning the calculus can be realised. The formality and level of abstraction of this language encourage the analysis of knowledge histories and allows useful properties about this knowledge to be inferred. These ideas are illustrated through the discussion of a particular case-study in knowledge evolution.", "num_citations": "9\n", "authors": ["1717"]}
{"title": "A localist network architecture for logical inference based on temporal synchrony approach to dynamic variable binding\n", "abstract": " This paper describes a localist network architecture which translates a signi cant subset of Horn-clause logic into a connectionist representation which may be executed very e ciently. The proposed architecture is based on an extension of the temporal synchrony approach to dynamic variable binding originally proposed by Shastri & Ajjanagadde and provides a rule and fact encoding mechanism.", "num_citations": "9\n", "authors": ["1717"]}
{"title": "Bounded ontological consistency for scalable dynamic knowledge infrastructures\n", "abstract": " Both semantic web applications and individuals are in need of knowledge infrastructures that can be used in dynamic and distributed environments where different autonomous entities create knowledge and build their own view of a domain. Our framework represents this using evolving simple contextual ontologies and mappings between them, at the same time as incremental logical coherence is maintained. The definition of semantic autonomy includes these aspects. Our earlier research has shown that a knowledge infrastructure can have semantic autonomy that maintains global consistency, if the knowledge representation is kept simple. We generalize that research by investigating what happens if the consistency of a knowledge infrastructure is bounded 1) within certain regions called spheres of consistency, and 2) by allowing a limited variable degree of inconsistency. Our experiments show that a\u00a0\u2026", "num_citations": "8\n", "authors": ["1717"]}
{"title": "Open Knowledge\n", "abstract": " The drive to extend the Web by taking advantage of automated symbolic reasoning (the so-called Semantic Web) has been dominated by a traditional model of knowledge sharing, in which the focus is on task-independent standardisation of knowledge. It appears to be difficult, in practice, to standardise in this way because the way in which we represent knowledge is strongly influenced by the ways in which we expect to use it. We present a form of knowledge sharing that is based not on direct sharing of \u201ctrue\u201d statements about the world but, instead, is based on sharing descriptions of interactions. By making interaction specifications the currency of knowledge sharing we gain a context to interpreting knowledge that can be transmitted between peers, in a manner analogous to the use of electronic institutions in multi-agent systems. The narrower notion of semantic commitment we thus obtain requires peers\u00a0\u2026", "num_citations": "8\n", "authors": ["1717"]}
{"title": "The role of agent interaction in models of computing: Panelist reviews\n", "abstract": " I am pleased that panelists agreed to contribute short reviews relating to the panel on \u201cThe Role of Agent Interaction in Models of Computation\u201d, at the Workshop on Foundations of Interactive Computation\u201d, held in Edinburgh in April 2005. The panelists were asked, prior to the panel, to address the following questions:", "num_citations": "8\n", "authors": ["1717"]}
{"title": "Visual logic programming through set inclusion and chaining\n", "abstract": " Visual reasoning and visual programming are two areas sharing a common interest in the use of visual representations where nowadays textual formalisms are used. In this paper we present the design of a visual logic programming language, research that falls at the intersection of the two areas. On the one hand we are designing a (declarative) programming language and therefore many issues of visual programming will still apply. On the other hand, due to its declarative nature, its foundations are close to those of visual reasoning. We will first explain our research framework, and the motivation for designing a visual logic programming language.Our recent work has focused on the design of an intuitive and easily accessible language to formalize at very high level the preliminary specification of systems. A standard approach to formalization is to divide languages into two sublanguages, the type expressions sublanguage where a kind of preliminary specification is given and then the value expressions language where computation takes place. In this approach the ideal is to have rich (complicated) type expressions, where the typing relation between types and values could be automatically checked. In the design of the Calculus of Refinements [6] we explored\u2014as an alternative to the former standard schema\u2014to give up the clear cut distinction between type and value expressions. That is, to consider everything as types and the typing relation (a non-transitive relation) as a transitive subtyping relation between types. If we consider types as sets then all our expressions would be set expressions and the subtyping relation would be the inclusion\u00a0\u2026", "num_citations": "8\n", "authors": ["1717"]}
{"title": "Building Large-Scale Prolog Programs using a Techniques Editing System\n", "abstract": " We describe an integrated environment which addresses three distinct aspects of Large-Scale Prolog Programs: the formalization of the programming practices one should use in order to build reliable and maintainable programs, the computer-aided use of these practices to develop programs, and the combination of these programs into more sophisticated and e cient programs. We also propose the use of program histories (ie, the information of how programs were developed using a techniques-based editor) to improve the process of program combination.", "num_citations": "8\n", "authors": ["1717"]}
{"title": "OKBook: Peer-to-peer community formation\n", "abstract": " Many systems exist for community formation in extensions of traditional Web environments but little work has been done for forming and maintaining communities in the more dynamic environments emerging from ad hoc and peer-to-peer networks. This paper proposes an approach for forming and evolving peer communities based on the sharing of choreography specifications (Interaction Models (IMs)). Two mechanisms for discovering IMs and collaborative peers are presented based on a meta-search engine and a dynamic peer grouping algorithm respectively. OKBook, a system allowing peers to publish, discover and subscribe or unsubscribe to IMs, has been implemented in accordance with our approach. For the meta-search engine, a strategy for integrating and re-ranking search results obtained from Semantic Web search engines is also described. This allows peers to discover IMs from their group\u00a0\u2026", "num_citations": "7\n", "authors": ["1717"]}
{"title": "A case-based reasoning system to support the relaxation of building regulations\n", "abstract": " We present a case-based reasoning system which can assist domain experts in interpreting building regulations in the relaxation process. When a relaxation application is submitted, experts make a decision on relaxation through consultation, in which relevant information is gathered and a variety of assertations by the parties concerned are put forward and considered by those experts. The aim of our system is to build knowledge bases of regulatory information and case histories and to provide experts with relevant information so that they can make an informed decision on relaxation. In order to provide a framework for the interpretation of building regulations, the system maintains abstraction hierarchies of legal rules from the statutory regulations and previously relaxed cases. The system retrieves legal rules and cases relevant to the input case by using the notion of implied similarity as well as structural mapping\u00a0\u2026", "num_citations": "7\n", "authors": ["1717"]}
{"title": "Agroforestry knowledge toolkit: methodological guidelines, computer software and manual for AKT1 and AKT2 supporting the use of a knowledge-based systems approach in\u00a0\u2026\n", "abstract": " Agroforestry knowledge toolkit: methodological guidelines, computer software and manual for AKT1 and AKT2 supporting the use of a knowledge-based systems approach in agroforestry research and extension FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Bibliographic information Type : Other In AGRIS since : 2012 All titles : \" Agroforestry knowledge toolkit: methodological guidelines, computer software and manual for AKT1 and AKT2 supporting the use of a knowledge-based systems approach in agroforestry research and extension \" Save as: AGRIS_AP RIS EndNote(XML) Agroforestry knowledge toolkit: methodological guidelines, computer software and manual for AKT1 and AKT2 supporting the use of a knowledge-based systems approach in agroforestry research and \u2026", "num_citations": "7\n", "authors": ["1717"]}
{"title": "Social machines for all\n", "abstract": " In today\u2019s interconnected world, people interact to a unprecedented degree through the use of digital platforms and services, forming complex \u2018social machines\u2019. These are now homes to autonomous agents as well as people, providing an open space where human and computational intelligence can mingle\u2014a new frontier for distributed agent systems. However, participants typically have limited autonomy to define and shape the machines they are part of. In this paper, we envision a future where individuals are able to develop their own Social Machines, enabling them to interact in a trustworthy, decentralized way. To make this possible, development methods and tools must see their barriers-to-entry dramatically lowered. People should be able to specify the agent roles and interaction patterns in an intuitive, visual way, analyse and test their designs and deploy them as easy to use systems. We argue that this is a challenging but realistic goal, which should be tackled by navigating the trade-off between the accessibility of the design methods\u2013primarily the modelling formalisms\u2013and their expressive power. We support our arguments by drawing ideas from different research areas including electronic institutions, agent-based simulation, process modelling, formal verification, and model-driven engineering.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Softening electronic institutions to support natural interaction\n", "abstract": " A necessary feature of social networks is a model of interaction which is followed on the network---some structure which coordinates activity between the participants. These interaction models are typically implicit, making it a challenge to both design and communicate the protocols for interaction and coordination. Electronic institution systems are one of the principal ways in which multi-agent systems engineers address this issue of coordination in complex interactions between groups of agents. In electronic institutions, interaction models can be concisely specified as protocols which encode the norms which computational agents follow. However, the formality and the up-front costs of discovering and choosing to engage with these systems has limited their applicability to human interaction. The vast majority of human (and, increasingly, automated) social interaction is now taking place in social media systems where social norms are softer concepts regulated essentially by the people involved. Being able to leverage the power of electronic institutions in these systems would ease the application of computational intelligence in support of social tasks. We describe a method by which electronic institutions can act in synergy with these sorts of social media streams and, in doing so, we define a``softer''style of system that, nevertheless, retains connection to precise specifications of coordination. In addition, we question the tacit assumption that participating agents deliberately join appropriate institutions. Although our method is independent of choice of social media stream (given a few standard characteristics of these) we describe an implementation\u00a0\u2026", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Knowledge management using business process modeling and workflow techniques\n", "abstract": " Enterprise Modeling (EM) methods are recognized for their value in providing a more organized way to describe a complex, informal domain. A problem with EM is that it does not always provide direct input for software system development. There is a\u201d gap\u201d between EM and software systems. One way of bridging this gap is to provide a formalization, here called that subsumes a wide variety of core modeling notations in a single using a business process language. It is possible to have different views of what is core to such a language but our attempt at such a view is articulated in the Fundamental Business Process Modeling Language (FBPML), which is a merger of IDEF3 and PSL. A workflow language, the FBPML Workflow Language (FWFL), is constructed and used to provide a declarative description of a workflow system. FWFL is tested in the\u201d PC-configuration\u201d domain. We also suggest using a validation and verification support framework to analyze and verify a business process model (BPM). Finally, some complexity results are presented for this type of modeling, implemented based on\u201d FBPML\u201d and\u201d FWFL\u201d. This would play an important communication role in the operation of an organization. In order to verify and analyze the BPM, a three-level framework is also introduced as a means of analysing BPMs and workflow systems. Finally, the complexity of BPMs and some comparisons with other related work are discussed.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Enabling services for distributed environments: Ontology extraction and knowledge-base characterisation\n", "abstract": " Existing knowledge base resources have the potential to be valuable components of the Semantic Web and similar knowledge-based environments. However, from the perspective of these environments, these resources are often undercharacterised, lacking the ontological and structural characterisation that would enable them to be exploited fully.In this paper we discuss two currently independent services, both integrated with their environment via a brokering mechanism. The first of these services is an ontology extraction tool, which can be used to identify ontological knowledge implicit in a knowledge base. The second service involves characterising a given knowledge base in terms of the topic it addresses and the structure of its knowledge. This characterisation should permit a knowledge base to be located and assessed as a potential candidate for re-use in a more intelligent and flexible manner. The discussion of some related research into brokering systems illustrates the roles that these services can play in distributed knowledge architectures as precursors to problem-directed transformation and reuse of knowledge resources.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Distributed specification\n", "abstract": " Most existing work on formal specification is focussed on a particular method or specification language, considered in isolation. In practice, few non-trivial specifications are produced by a single person, or by a group of persons with a common view of the world. It is far more common for a variety of views of a problem to coexist, each with different forms of communication. It has proved difficult to assemble such heterogeneous specifications-leading to breakdowns in communication and consequentfailures of systems. As more of this communication is conducted remotely by electronic means the need to support distributed specification in a controlled way is increased. This paper presents one way of tackling this problem, based on a set of tools for describing specifications at a variety of stages in their development. By constraining the interfaces between tools, we aim to provide a more structured system for collaborative specification.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "A connectionist representation of symbolic components, dynamic bindings and basic inference operations\n", "abstract": " If we wish to replicate symbolic styles of inference in a connectionist system, we are constrained to work with simple network elements, with localised behaviours and simple forms of link to others. However, there remains considerable scope for experiment with di erent forms of element and links. This paper motivates our choice of a particular element and link arrangement which we believe to be more uniform in structure than other related architectures while also allowing a closer match to the basic elements of symbolic inference which we wish to explore. Using this element, this paper describes how dynamic bindings and basic symbolic inference procedures (matching and substitution) are represented in a connectionist manner. The paper also provides entry points to more extensive descriptions of the system and its use.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Application of logic programming to decision support systems in ecology\n", "abstract": " The complexity of ecological systems makes it difficult for resource managers to be rigorous in their decision making. It is impossible to include all the detail of a natural system within a simulation model, so it is necessary to isolate a few key features of the system and represent these in a form that remains faithful to the original while being computationally tractable. Unfortunately, it is easy to be vague about what requirements must be met by a model of a system, making it impossible to validate in terms of more general management objectives. More attention should be paid to representing explicitly the expertise employed by resource managers in describing ecological problems and analyzing arguments about how they might be solved. Techniques from logic programming may be used to support parts of this process: providing guidance during model construction; assisting in validating the model; handling non-deterministic, qualitative information; and analyzing ecological knowledge bases.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Case-based Selection Requirements Specifications for Telecommunication Systems\n", "abstract": " Using formal specifications based on varieties of mathematical logic is becoming common in the process of designing and implementing software. The advantage of this procedure is that it enables us to verify the specification's properties before the real system is implemented. Up to now, formal methods were usually applied to include all details of the final system in the specification. in large, complex systems this requires sophisticated logic, which makes theorem proving a difficult and complex task. Telecommunication systems are large and complex. However, our case-based approach uses course-grained requirements sketches to outline the basic behaviour of the system's functional components, thereby allowing us to identify, re-use and adapt requirements (from cases stored in a library) to construct new cases. By using cases that have already been tested, integrated and implemented, less effort is needed to produce requirements specifications on a large scale. Using a hypothetical telecommunication system as our example, we shall show how comparatively simple logic can be used to capture course-grained behaviour and how a case-based approach benefits from this. The input from these examples is used to induce a set of state-transition rules that are applied to match and identify the cases whose behaviour corresponds most closely to the designer's intentions.", "num_citations": "6\n", "authors": ["1717"]}
{"title": "Reasoning about Distributed Knowledge-Transforming Peer Interactions\n", "abstract": " We address the problem of how to reason about properties of knowledge transformations as they occur in distributed and decentralized interactions between large and complex artifacts, such as databases, web services, and ontologies. Based on the conceptual distinction between specifications of interactions and properties of knowledge transformations that follow from these interactions, we explore a novel mixture of process calculus and property inference by connecting interaction models with knowledge transformation rules. We aim at being generic in our exploration, hence our emphasis on abstract knowledge transformations, although we exemplify it using a lightweight specification language for interaction modeling (for which an executable peer-to-peer environment already exists) and provide a formal semantics for knowledge transformation rules using the theory of institutions. Consequently, our\u00a0\u2026", "num_citations": "5\n", "authors": ["1717"]}
{"title": "Service choreography meets the web of data via micro-data\n", "abstract": " Several solutions exist for semantically describing Web Services (WSs) from the perspective of orchestration but little is known about how semantics benefit WS choreography. The most extreme example of a choreography problem occurs in peer-to-peer systems where shared semantics of data may need to be established via services interactions. We present a solution to this problem by sharing micro-data via interaction models. No pre-unified ontology is required in our approach so peers can make use of existing heterogeneous resources having been described in the RDF data model flexibly and compatibly. The experimental results indicate that our approach semantically enhances WS choreography in a lightweight way which complies with principles of Linked Data and republished Interaction Models (IMs) can further facilitate the progress of the Web of data as well as the formation of peer communities generated through peers' interactions.", "num_citations": "5\n", "authors": ["1717"]}
{"title": "Contexts in dynamic ontology mapping\n", "abstract": " Agents in open systems interact continuously, each possibly having a different ontology. Mapping in advance all the ontologies that an agent can encounter is not feasible, as all the possible combinations cannot be foreseen. Mapping complete ontologies at run time is a computationally expensive task. This paper proposes a framework in which mappings between terms may be hypothesised dynamically as the terms are encountered during interaction. In this way, the interaction itself defines the context in which small, relevant portions of ontologies are mapped. We use this way of scoping the ontology mapping problem in order to apply mapping heuristics in a more focused way.", "num_citations": "5\n", "authors": ["1717"]}
{"title": "Automated reasoning about an uncertain domain\n", "abstract": " In this paper we introduce a resolution-based logic programming language that handles probabilities and fuzzy events. The language can be viewed as a simple knowledge representation formalism, with the features of being operational and presenting a complete declarative semantics. An extended version of this paper can be found in [3].", "num_citations": "5\n", "authors": ["1717"]}
{"title": "Building the Knowledge Graph for UK Health Data Science\n", "abstract": " Extracting patient phenotypes from routinely collected health data (such as Electronic Health Records) requires translating clinically-sound phenotype definitions into queries/computations executable on the underlying data sources by clinical researchers. This requires significant knowledge and skills to deal with heterogeneous and often imperfect data. Translations are time-consuming, error-prone and, most importantly, hard to share and reproduce across different settings. This paper proposes a knowledge driven framework that (1) decouples the specification of phenotype semantics from underlying data sources; (2) can automatically populate and conduct phenotype computations on heterogeneous data spaces. We report preliminary results of deploying this framework on five Scottish health datasets. Big data analytics in healthcare has great potential to reveal deep insights from health data, which would\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Towards executable representations of social machines\n", "abstract": " Human interaction is increasingly mediated through technological systems, resulting in the emergence of a new class of socio-technical systems, often called Social Machines. However, many systems are designed and managed in a centralised way, limiting the participants\u2019 autonomy and ability to shape the systems they are part of.                 In this paper we are concerned with creating a graphical formalism that allows novice users to simply draw the patterns of interaction that they desire, and have computational infrastructure assemble around the diagram. Our work includes a series of participatory design workshops, that help to understand the levels and types of abstraction that the general public are comfortable with when designing socio-technical systems. These design studies lead to a novel formalism that allows us to compose rich interaction protocols into functioning, executable architecture. We\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Social machines as an approach to group privacy\n", "abstract": " This chapter introduces the notion of social machines as a way of conceptualising and formalising the interactions between people and private networked technology for problem-solving. It is argued that formalisation of such \u2018social computing\u2019 will generate requirements for information flow within social machines and across their boundaries with the outside world. These requirements provide the basis for a notion of group privacy that is neither derivative from the idea of individual privacy preferences, nor founded in political or moral argument, but instead related to the integrity of the social machine and its capabilities for bottom-up problem-solving. This notion of group privacy depends on a particular technological setup, and is not intended to be a general definition, but it has purchase in the context of pervasive technology and big data which has made the question of group privacy pressing and timely.", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Choreographing web services with semantically enhanced scripting\n", "abstract": " Several solutions to describing service choreography have emerged, mainly focused on encoding capabilities of services especially for those deployed on the Web. These solutions are either derived from traditional Web service standards such as WSDL or inspired by the theory of process calculus. Little attention has however been paid to finding a lightweight solution which can enable peers to obtain, publish and share service choreography in an open environment or peer-to-peer network. This paper proposes a framework for choreographing semantically enhanced Web Services encoded in a extended lightweight coordinative language which is derived from process calculus and is dedicated to running in modern Web browsers. A proof-of-concept prototype has been implemented and demoed as a decentralised service choreography-management platform based on this framework. There is no need for users\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Interaction model language definition\n", "abstract": " In this deliverable we introduce ambient LCC, a language to program interaction models for P2P networks. The language is based on process algebra concepts and is specially designed to support the execution of electronic institutions. An algorithm that automatically translates electronic institution specifications into ambient LCC code is presented and illustrated though an example. Background material on electronic institutions, LCC, and ambient calculus is provided to facilitate the understanding of the language. This document will serve as the reference for the implementation of the interaction model interpreter of the OpenKnowledge project architecture.", "num_citations": "4\n", "authors": ["1717"]}
{"title": "A localist network architecture for logical inference\n", "abstract": " The primary interest of this chapter is in understanding how systems which are described in a symbolic specification style may be implemented using a connectionist architecture. As a focus of attention, techniques have been developed for compiling Horn clauses into a connectionist network. This offers significant practical benefits but also forces limitations on the scope of the compiled system. Executable symbolic specifications are in the right hands) effective for describing systems and are comparatively easy for designers to understand. However, they normally require extra machinery, in the form of an interpreter or theorem proving system in order to be executed. For many applications (particularly when the system is to be implemented in hardware) such extra mechanisms are both inefficient and structurally complex. Connectionist systems, on the other hand, use structurally simple components, may provide very fast inference and have no need of a separate interpreter, but are difficult to use directly for specification because of the mass of connections between elements. By providing automatic translation from symbolic to connectionist representations, one should be able to cancel out the deficiencies of each style whilst retaining the advantages of both. Unfortunately, this type of compilation is not straightforward; since an interpreter has to be merged into the connectionist networks, the compilation process has to take into account not only the Horn clauses themselves but also the strategy, which is intended to be used for drawing inferences from them. It also appears that some fundamental aspects of symbolic inference are difficult to translate\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Case-based support for the design of dynamic system requirements\n", "abstract": " Using formal specifications based on varieties of mathematical logic is becoming common in the process of designing and implementing software. Formal methods are usually intended to include all important details of the final system in the specification with the aim of proving that it possesses certain mathematical properties. In large, complex systems, this task requires sophisticated theorem proving, which can be difficult and complicated. Telecommunication systems are large and complex, making detailed formal specification impractical with current technology. However roughly formal \u201csketches\u201d of the behaviours these services provide can be produced, and these can be very helpful in locating which service might be relevant to a given problem. Our case-based approach uses coarse-grained requirements specification sketches to outline the basic behaviour of the system's functional modules (called\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "An Environment for Building Prolog Programs Based on Knowledge about their Construction\n", "abstract": " Program combination can be used to promote the reuse of software by allowing complex programs to be built by repeated combination of other programs. Previous attempts at automatic systems which assist programmers in the task of combining programs have generally required lots of interaction from a user, who also typically needs a good understanding of the particular program transformation process being applied. A system for transforming programs expressed as recursion equations is given in| BD77|, but its use requires intervention of a human with a good understanding of program transformation methods. In procedural languages, there are ways| H PR88|| to merge programs derived from an initial generic template, however, this approach is restricted to a limited class of programs. In|| TS83, TS84], an unfold/fold based transformation sys-tem was given, but requires user intervention and is restricted to\u00a0\u2026", "num_citations": "4\n", "authors": ["1717"]}
{"title": "Metadata-driven hypertext content publishing and styling\n", "abstract": " A growing number of approaches and tools have been utilised attempting at generating hypertext content with embedded metadata. However, little work has been carried out on finding a generic solution for publishing and styling Web pages with annotations derived from existing RDF data sets available in various formats. This paper proposes a metadata-driven publishing framework assisting publishers or webmasters in generating semantically-enriched content (HTML pages or snippets) by harnessing distributed RDF (a) documents or repositories with little human intervention. This framework also helps users to create and share so-called micro-themes, which is applicable to the above generated content for the purpose of page styling and also highly reusable thanks to the adopted semantic attribute selectors.", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Automated Deployment of Argumentation Protocols.\n", "abstract": " The objective of this paper is to try to fill the gap between: argumentation, electronic institutions and protocols by using a combination of automated synthesis and model checking methods. More precisely, this paper proposes a means of moving rapidly from argument specification to protocol implementation, using an extension of the Argument Interchange Format as the specification language and the Lightweight Coordination Calculus as an implementation language.", "num_citations": "3\n", "authors": ["1717"]}
{"title": "LCC argument patterns\n", "abstract": " LCC-Argument patterns capture the different relationships and interactions between LCC agents' roles. These patterns provide common LCC argument code for developing protocols and their components along with explaining how two or more agents can interact with each other. They are generic solutions to the common LCC argumentation protocol development problem that recur across protocols repeatedly and can be adapted to generate specific protocols.", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Constraint relaxation approach for over-constrained agent interaction\n", "abstract": " The interactions among agents in a multi-agent system for coordinating a distributed, problem solving task can be complex, as the distinct sub-problems of the individual agents are interdependent. A distributed protocol provides the necessary framework for specifying these interactions. In a model of interactions where the agents\u2019 social norms are expressed as the message passing behaviours associated with roles, the dependencies among agents can be specified as constraints. The constraints are associated with roles to be adopted by agents as dictated by the protocol. These constraints are commonly handled using a conventional constraint solving system that only allows two satisfactory states to be achieved \u2013 completely satisfied or failed. Agent interactions then become brittle as the occurrence of an over-constrained state can cause the interaction between agents to break prematurely, even though\u00a0\u2026", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Distributed Workflows: The OpenKnowledge Experience\n", "abstract": " Software systems are becoming ever more complex, and one source of complexity lies in integrating heterogeneous subsystems. Service Oriented Architectures are part of the answer: they decouple the components of the system. However normally SOA is used from a centralised perspective: a single process invokes remote services, unaware of being part of a workflow. We claim that the centralised, or orchestration-based, approach cannot scale well with increasing complexity and heterogeneity of the components, and we propose an alternative distributed, or choreography-based, approach, that forces developers to think in terms of actors, roles and interactions. We first present the OpenKnowledge framework, designed according to choreography-based principles and then show how a complex, distributed model for managing the triple assessment of patients suspected with breast cancer can be easily\u00a0\u2026", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Webs of interactions: Exploring peer ranking via simulation in openknowledge\n", "abstract": " The effort to share knowledge on a large scale has concentrated on formalisation of knowledge. Meta-data describe properties of databases. Interface specifications describe the input/output data of services. Tags describe features of documents and images. The ideal being pursued is, as far as possible, to describe knowledge independent of the tasks for which that knowledge might be deployed. Then we can devise new ways of deploying knowledge without having to re-assess how it is encoded. In practice, we seldom come close to an ideal knowledge representation in real domains of application (many would argue that we never could) but for some tasks this may not matter because, like the conventional Web, enough human involvement makes shallow knowledge representation effective. If we can develop a plausible ontology for a document collection that hitherto had been accessed only by keyword search then we are likely to improve accuracy of document recall and, since people can check each result, we have a natural human buffer against inaccuracy. This symbiotic relationship has proved more difficult to achieve, however, when our knowledge sources are complex programs and our tasks involve coordinating them.Why is it more difficult to share knowledge for coordinated tasks involving programs? One reason is that it is coordinated, so the sequence in which the programs communicate influences how each will interpret knowledge communicated to it. A second reason is that programs have local state, so the program with which we interacted previously may not be quite the same when we interact with it again. A third reason is that\u00a0\u2026", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Using focus rules in requirements elicitation dialogues\n", "abstract": " Requirements engineering is a complex task which benefits from computer support. Despite the pro gress made in automatic reasoning on require ments, the tools supporting requirements elicitation remain difficult to use. In this paper we propose a novel approach where a tool's reasoning is intim ately linked to the dialogue it has with its users. Because the dialogue is guided by rules ensuring coherence, the interaction with the tool is more nat ural. We discuss in detail the rules we use to or ganise the dialogue and how we apply them to the requirements elicitation tool. We present an evalu ation of this approach demonstrating improvements in usability during the elicitation process.", "num_citations": "3\n", "authors": ["1717"]}
{"title": "Reliable and accountable system design\n", "abstract": " Few would disagree with the assertion that safe engineering starts from the early stages of system design and should be maintained throughout the lifecycle. Different engineering domains have developed, mostly informal, frameworks with which they hope to promote this attitude. An interesting question for the KBS community is whether some of our methods for knowledge representation and reasoning can be used to assist in understanding, representing and interpreting such frameworks. This paper concentrates on what is (arguably) the area of greatest concern: relating system requirements to high level design. We highlight what appear to be the major difficulties which face us in this area, using examples from systems which have been built to tackle them.", "num_citations": "3\n", "authors": ["1717"]}