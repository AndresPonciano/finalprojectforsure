{"title": "The mobile data challenge: Big data for mobile computing research\n", "abstract": " This paper presents an overview of the Mobile Data Challenge (MDC), a large-scale research initiative aimed at generating innovations around smartphone-based research, as well as community-based evaluation of related mobile data analysis methodologies. First we review the Lausanne Data Collection Campaign (LDCC)\u2013an initiative to collect unique, longitudinal smartphone data set for the basis of the MDC. Then, we introduce the Open and Dedicated Tracks of the MDC; describe the specific data sets used in each of them; and discuss some of the key aspects in order to generate privacy-respecting, challenging, and scientifically relevant mobile data resources for wider use of the research community. The concluding remarks will summarize the paper.", "num_citations": "553\n", "authors": ["1189"]}
{"title": "Stresssense: Detecting stress in unconstrained acoustic environments using smartphones\n", "abstract": " Stress can have long term adverse effects on individuals' physical and mental well-being. Changes in the speech production process is one of many physiological changes that happen during stress. Microphones, embedded in mobile phones and carried ubiquitously by people, provide the opportunity to continuously and non-invasively monitor stress in real-life situations. We propose StressSense for unobtrusively recognizing stress from human voice using smartphones. We investigate methods for adapting a one-size-fits-all stress model to individual speakers and scenarios. We demonstrate that the StressSense classifier can robustly identify stress across multiple individuals in diverse acoustic environments: using model adaptation StressSense achieves 81% and 76% accuracy for indoor and outdoor environments, respectively. We show that StressSense can be implemented on commodity Android phones and\u00a0\u2026", "num_citations": "496\n", "authors": ["1189"]}
{"title": "Modeling scenes with local descriptors and latent aspects\n", "abstract": " We present a new approach to model visual scenes in image collections, based on local invariant features and probabilistic latent space models. Our formulation provides answers to three open questions:(l) whether the invariant local features are suitable for scene (rather than object) classification; (2) whether unsupennsed latent space models can be used for feature extraction in the classification task; and (3) whether the latent space formulation can discover visual co-occurrence patterns, motivating novel approaches for image organization and segmentation. Using a 9500-image dataset, our approach is validated on each of these issues. First, we show with extensive experiments on binary and multi-class scene classification tasks, that a bag-of-visterm representation, derived from local invariant descriptors, consistently outperforms state-of-the-art approaches. Second, we show that probabilistic latent semantic\u00a0\u2026", "num_citations": "472\n", "authors": ["1189"]}
{"title": "Automatic analysis of multimodal group actions in meetings\n", "abstract": " This paper investigates the recognition of group actions in meetings. A framework is employed in which group actions result from the interactions of the individual participants. The group actions are modeled using different HMM-based approaches, where the observations are provided by a set of audiovisual features monitoring the actions of individuals. Experiments demonstrate the importance of taking interactions into account in modeling the group actions. It is also shown that the visual modality contains useful information, even for predominantly audio-based events, motivating a multimodal approach to meeting analysis.", "num_citations": "442\n", "authors": ["1189"]}
{"title": "Mining large-scale smartphone data for personality studies\n", "abstract": " In this paper, we investigate the relationship between automatically extracted behavioral characteristics derived from rich smartphone data and self-reported Big-Five personality traits (extraversion, agreeableness, conscientiousness, emotional stability and openness to experience). Our data stem from smartphones of 117 Nokia N95 smartphone users, collected over a continuous period of 17\u00a0months in Switzerland. From the analysis, we show that several aggregated features obtained from smartphone usage data can be indicators of the Big-Five traits. Next, we describe a machine learning method to detect the personality trait of a user based on smartphone usage. Finally, we study the benefits of using gender-specific models for this task. Apart from a psychological viewpoint, this study facilitates further research on the automated classification and usage of personality traits for personalizing services on\u00a0\u2026", "num_citations": "429\n", "authors": ["1189"]}
{"title": "Automatic nonverbal analysis of social interaction in small groups: A review\n", "abstract": " An increasing awareness of the scientific and technological value of the automatic understanding of face-to-face social interaction has motivated in the past few years a surge of interest in the devising of computational techniques for conversational analysis. As an alternative to existing linguistic approaches for the automatic analysis of conversations, a relatively recent domain is using findings in social cognition, social psychology, and communication that have established the key role that nonverbal communication plays in the formation, maintenance, and evolution of a number of fundamental social constructs, which emerge from face-to-face interactions in time scales that range from short glimpses all the way to long-term encounters. Small group conversations are a specific case on which much of this work has been conducted. This paper reviews the existing literature on automatic analysis of small group\u00a0\u2026", "num_citations": "372\n", "authors": ["1189"]}
{"title": "Semi-supervised adapted hmms for unusual event detection\n", "abstract": " We address the problem of temporal unusual event detection. Unusual events are characterized by a number of features (rarity, unexpectedness, and relevance) that limit the application of traditional supervised model-based approaches. We propose a semi-supervised adapted hidden Markov model (HMM) framework, in which usual event models are first learned from a large amount of (commonly available) training data, while unusual event models are learned by Bayesian adaptation in an unsupervised manner. The proposed framework has an iterative structure, which adapts a new unusual event model at each iteration. We show that such a framework can address problems due to the scarcity of training data and the difficulty in pre-defining unusual events. Experiments on audio, visual, and audiovisual data streams illustrate its effectiveness, compared with both supervised and unsupervised baseline methods.", "num_citations": "369\n", "authors": ["1189"]}
{"title": "Towards rich mobile phone datasets: Lausanne data collection campaign\n", "abstract": " Mobile phones have recently been used to collect large-scale continuous data about human behavior. In a paradigm known as people centric sensing, users are not only the carriers of sensing devices, but also the sources and consumers of sensed events. This paper describes a data collection campaign wherein Nokia N95 phones are allocated to a heterogeneous sample of nearly 170 participants from Lausanne, a mid-tier city in Switzerland, to be used over a period of one year. The data collection software runs on the background of the phones in a non-intrusive manner, yielding data on modalities such as social interaction and spatial behavior. The main motivations for organizing a new campaign on top of the ones that have been successfully conducted in the past are the following: First, in comparison to the Reality Mining data, generated in 2004-2005, the present data set is expected to provide a richer means to study location attributes, in particular, because today\u2019s mobile phones are more powerful and equipped with more sensors. Second, we aim to recruit a heterogeneous set of participants, comprising family and leisure related social networks in addition to organizationally driven ones. This paper provides a methodological description of the project and shows the potential of the resulting data set in terms of illuminating multiple aspects of human behavior.", "num_citations": "368\n", "authors": ["1189"]}
{"title": "On image auto-annotation with latent space models\n", "abstract": " Image auto-annotation, ie, the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset.", "num_citations": "366\n", "authors": ["1189"]}
{"title": "PLSA-based image auto-annotation: constraining the latent space\n", "abstract": " We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.", "num_citations": "322\n", "authors": ["1189"]}
{"title": "Discovering routines from large-scale human locations using probabilistic topic models\n", "abstract": " In this work, we discover the daily location-driven routines that are contained in a massive real-life human dataset collected by mobile phones. Our goal is the discovery and analysis of human routines that characterize both individual and group behaviors in terms of location patterns. We develop an unsupervised methodology based on two differing probabilistic topic models and apply them to the daily life of 97 mobile phone users over a 16-month period to achieve these goals. Topic models are probabilistic generative models for documents that identify the latent structure that underlies a set of words. Routines dominating the entire group's activities, identified with a methodology based on the Latent Dirichlet Allocation topic model, include \u201cgoing to work late\u201d, \u201cgoing home early\u201d, \u201cworking nonstop\u201d and \u201chaving no reception (phone off)\u201d at different times over varying time-intervals. We also detect routines which are\u00a0\u2026", "num_citations": "306\n", "authors": ["1189"]}
{"title": "Using particles to track varying numbers of interacting people\n", "abstract": " In this paper, we present a Bayesian framework for the fully automatic tracking of a variable number of interacting targets using a fixed camera. This framework uses a joint multi-object state-space formulation and a trans-dimensional Markov Chain Monte Carlo (MCMC) particle filter to recursively estimates the multi-object configuration and efficiently search the state-space. We also define a global observation model comprised of color and binary measurements capable of discriminating between different numbers of objects in the scene. We present results which show that our method is capable of tracking varying numbers of people through several challenging real-world tracking situations such as full/partial occlusion and entering/leaving the scene.", "num_citations": "290\n", "authors": ["1189"]}
{"title": "A thousand words in a scene\n", "abstract": " This paper presents a novel approach for visual scene modeling and classification, investigating the combined use of text modeling methods and local invariant features. Our work attempts to elucidate (1) whether a textlike bag-of-visterms (BOV) representation (histogram of quantized local visual features) is suitable for scene (rather than object) classification, (2) whether some analogies between discrete scene representations and text documents exist, and 3) whether unsupervised, latent space models can be used both as feature extractors for the classification task and to discover patterns of visual co-occurrence. Using several data sets, we validate our approach, presenting and discussing experiments on each of these issues. We first show, with extensive experiments on binary and multiclass scene classification tasks using a 9,500-image data set, that the BOV representation consistently outperforms classical\u00a0\u2026", "num_citations": "265\n", "authors": ["1189"]}
{"title": "Modeling dominance in group conversations using nonverbal activity cues\n", "abstract": " Dominance - a behavioral expression of power - is a fundamental mechanism of social interaction, expressed and perceived in conversations through spoken words and audiovisual nonverbal cues. The automatic modeling of dominance patterns from sensor data represents a relevant problem in social computing. In this paper, we present a systematic study on dominance modeling in group meetings from fully automatic nonverbal activity cues, in a multi-camera, multi-microphone setting. We investigate efficient audio and visual activity cues for the characterization of dominant behavior, analyzing single and joint modalities. Unsupervised and supervised approaches for dominance modeling are also investigated. Activity cues and models are objectively evaluated on a set of dominance-related classification tasks, derived from an analysis of the variability of human judgment of perceived dominance in group\u00a0\u2026", "num_citations": "264\n", "authors": ["1189"]}
{"title": "Modeling semantic aspects for cross-media image indexing\n", "abstract": " To go beyond the query-by-example paradigm in image retrieval, there is a need for semantic indexing of large image collections for intuitive text-based image search. Different models have been proposed to learn the dependencies between the visual content of an image set and the associated text captions, then allowing for the automatic creation of semantic indexes for unannotated images. The task, however, remains unsolved. In this paper, we present three alternatives to learn a probabilistic latent semantic analysis (PLSA) model for annotated images and evaluate their respective performance for automatic image indexing. Under the PLSA assumptions, an image is modeled as a mixture of latent aspects that generates both image features and text captions, and we investigate three ways to learn the mixture of aspects. We also propose a more discriminative image representation than the traditional Blob\u00a0\u2026", "num_citations": "264\n", "authors": ["1189"]}
{"title": "Smartphone usage in the wild: a large-scale analysis of applications and context\n", "abstract": " This paper presents a large-scale analysis of contextualized smartphone usage in real life. We introduce two contextual variables that condition the use of smartphone applications, namely places and social context. Our study shows strong dependencies between phone usage and the two contextual cues, which are automatically extracted based on multiple built-in sensors available on the phone. By analyzing continuous data collected on a set of 77 participants from a European country over 9 months of actual usage, our framework automatically reveals key patterns of phone application usage that would traditionally be obtained through manual logging or questionnaire. Our findings contribute to the large-scale understanding of applications and context, bringing out design implications for interfaces on smartphones.", "num_citations": "261\n", "authors": ["1189"]}
{"title": "Who's who with big-five: Analyzing and classifying personality traits with smartphones\n", "abstract": " In this paper, we investigate the relationship between behavioral characteristics derived from rich smart phone data and self-reported personality traits. Our data stems from smart phones of a set of 83 individuals collected over a continuous period of 8 months. From the analysis, we show that aggregated features obtained from smart phone usage data can be indicators of the Big-Five personality traits. Additionally, we develop an automatic method to infer the personality type of a user based on cell phone usage using supervised learning. We show that our method performs significantly above chance and up to 75.9% accuracy. To our knowledge, this constitutes the first study on the analysis and classification of personality traits using smartphone data.", "num_citations": "261\n", "authors": ["1189"]}
{"title": "Modeling individual and group actions in meetings with layered HMMs\n", "abstract": " We address the problem of recognizing sequences of human interaction patterns in meetings, with the goal of structuring them in semantic terms. The investigated patterns are inherently group-based (defined by the individual activities of meeting participants, and their interplay), and multimodal (as captured by cameras and microphones). By defining a proper set of individual actions, group actions can be modeled as a two-layer process, one that models basic individual activities from low-level audio-visual (AV) features,and another one that models the interactions. We propose a two-layer hidden Markov model (HMM) framework that implements such concept in a principled manner, and that has advantages over previous works. First, by decomposing the problem hierarchically, learning is performed on low-dimensional observation spaces, which results in simpler models. Second, our framework is easier to\u00a0\u2026", "num_citations": "221\n", "authors": ["1189"]}
{"title": "The youtube lens: Crowdsourced personality impressions and audiovisual analysis of vlogs\n", "abstract": " Despite an increasing interest in understanding human perception in social media through the automatic analysis of users' personality, existing attempts have explored user profiles and text blog data only. We approach the study of personality impressions in social media from the novel perspective of crowdsourced impressions, social attention, and audiovisual behavioral analysis on slices of conversational vlogs extracted from YouTube. Conversational vlogs are a unique case study to understand users in social media, as vloggers implicitly or explicitly share information about themselves that words, either written or spoken cannot convey. In addition, research in vlogs may become a fertile ground for the study of video interactions, as conversational video expands to innovative applications. In this work, we first investigate the feasibility of crowdsourcing personality impressions from vlogging as a way to obtain\u00a0\u2026", "num_citations": "218\n", "authors": ["1189"]}
{"title": "Evaluating multi-object tracking\n", "abstract": " Multiple object tracking (MOT) is an active and challenging research topic. Many different approaches to the MOT problem exist, yet there is little agreement amongst the community on how to evaluate or compare these methods, and the amount of literature addressing this problem is limited. The goal of this paper is to address this issue by providing a comprehensive approach to the empirical evaluation of tracking performance. To that end, we explore the tracking characteristics important to measure in a real-life application, focusing on configuration (the number and location of objects in a scene) and identification (the consistent labeling of objects over time), and define a set of measures and a protocol to objectively evaluate these characteristics.", "num_citations": "206\n", "authors": ["1189"]}
{"title": "A nonverbal behavior approach to identify emergent leaders in small groups\n", "abstract": " Identifying emergent leaders in organizations is a key issue in organizational behavioral research, and a new problem in social computing. This paper presents an analysis on how an emergent leader is perceived in newly formed, small groups, and then tackles the task of automatically inferring emergent leaders, using a variety of communicative nonverbal cues extracted from audio and video channels. The inference task uses rule-based and collective classification approaches with the combination of acoustic and visual features extracted from a new small group corpus specifically collected to analyze the emergent leadership phenomenon. Our results show that the emergent leader is perceived by his/her peers as an active and dominant person; that visual information augments acoustic information; and that adding relational information to the nonverbal cues improves the inference of each participant's\u00a0\u2026", "num_citations": "191\n", "authors": ["1189"]}
{"title": "Audiovisual probabilistic tracking of multiple speakers in meetings\n", "abstract": " Tracking speakers in multiparty conversations constitutes a fundamental task for automatic meeting analysis. In this paper, we present a novel probabilistic approach to jointly track the location and speaking activity of multiple speakers in a multisensor meeting room, equipped with a small microphone array and multiple uncalibrated cameras. Our framework is based on a mixed-state dynamic graphical model defined on a multiperson state-space, which includes the explicit definition of a proximity-based interaction model. The model integrates audiovisual (AV) data through a novel observation model. Audio observations are derived from a source localization algorithm. Visual observations are based on models of the shape and spatial structure of human heads. Approximate inference in our model, needed given its complexity, is performed with a Markov Chain Monte Carlo particle filter (MCMC-PF), which results in\u00a0\u2026", "num_citations": "172\n", "authors": ["1189"]}
{"title": "What did you do today? Discovering daily routines from large-scale mobile data\n", "abstract": " We present a framework built from two Hierarchical Bayesian topic models to discover human location-driven routines from mobile phones. The framework uses location-driven bag representations of people's daily activities obtained from celltower connections. Using 68,000+ hours of real-life human data from the Reality Mining dataset, we successfully discover various types of routines. The first studied model, Latent Dirichlet Allocation (LDA), automatically discovers characteristic routines for all individuals in the study, including\" going to work at 10am\",\" leaving work at night\", or\" staying home for the entire evening\". In contrast, the second methodology with the Author Topic model (ATM) finds routines characteristic of a selected groups of users, such as\" being at home in the mornings and evenings while being out in the afternoon\", and ranks users by their probability of conforming to certain daily routines.", "num_citations": "170\n", "authors": ["1189"]}
{"title": "Human-centered computing: a multimedia perspective\n", "abstract": " Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. In this paper, we give an overview of HCC from a Multimedia perspective. We describe what we consider to be the three main areas of Human-Centered Multimedia (HCM): media production, analysis, and interaction. In addition, we identify the core characteristics of HCM, describe example applications, and propose a research agenda for HCM.", "num_citations": "168\n", "authors": ["1189"]}
{"title": "AV16. 3: An audio-visual corpus for speaker localization and tracking\n", "abstract": " Assessing the quality of a speaker localization or tracking algorithm on a few short examples is difficult, especially when the ground-truth is absent or not well defined. One step towards systematic performance evaluation of such algorithms is to provide time-continuous speaker location annotation over a series of real recordings, covering various test cases. Areas of interest include audio, video and audio-visual speaker localization and tracking. The desired location annotation can be either 2-dimensional (image plane) or 3-dimensional (physical space). This paper motivates and describes a corpus of audio-visual data called \u201cAV16.3\u201d, along with a method for 3-D\u00a0location annotation based on calibrated cameras. \u201c16.3\u201d\u00a0stands for 16\u00a0microphones and 3\u00a0cameras, recorded in a fully synchronized manner, in a meeting room. Part of this corpus has already been successfully used to report research results.", "num_citations": "167\n", "authors": ["1189"]}
{"title": "Discovering places of interest in everyday life from smartphone data\n", "abstract": " In this paper, a new framework to discover places-of-interest from multimodal mobile phone data is presented. Mobile phones have been used as sensors to obtain location information from users\u2019 real lives. A place-of-interest is defined as a location where the user usually goes and stays for a while. Two levels of clustering are used to obtain places of interest. First, user location points are grouped using a time-based clustering technique which discovers stay points while dealing with missing location data. The second level performs clustering on the stay points to obtain stay regions. A grid-based clustering algorithm has been used for this purpose. To obtain more user location points, a client-server system has been installed on the mobile phones, which is able to obtain location information by integrating GPS, Wifi, GSM and accelerometer sensors, among others. An extensive set of experiments has been\u00a0\u2026", "num_citations": "166\n", "authors": ["1189"]}
{"title": "Hire me: Computational inference of hirability in employment interviews based on nonverbal behavior\n", "abstract": " Understanding the basis on which recruiters form hirability impressions for a job applicant is a key issue in organizational psychology and can be addressed as a social computing problem. We approach the problem from a face-to-face, nonverbal perspective where behavioral feature extraction and inference are automated. This paper presents a computational framework for the automatic prediction of hirability. To this end, we collected an audio-visual dataset of real job interviews where candidates were applying for a marketing job. We automatically extracted audio and visual behavioral cues related to both the applicant and the interviewer. We then evaluated several regression methods for the prediction of hirability scores and showed the feasibility of conducting such a task, with ridge regression explaining 36.2% of the variance. Feature groups were analyzed, and two main groups of behavioral cues were\u00a0\u2026", "num_citations": "162\n", "authors": ["1189"]}
{"title": "Tracking the visual focus of attention for a varying number of wandering people\n", "abstract": " In this paper, we define and address the problem of finding the visual focus of attention for a varying number of wandering people (VFOA-W), determining where a person is looking when their movement is unconstrained. The VFOA-W estimation is a new and important problem with implications in behavior understanding and cognitive science and real-world applications. One such application, presented in this paper, monitors the attention passers-by pay to an outdoor advertisement by using a single video camera. In our approach to the VFOA-W problem, we propose a multiperson tracking solution based on a dynamic Bayesian network that simultaneously infers the number of people in a scene, their body locations, their head locations, and their head pose. For efficient inference in the resulting variable-dimensional state-space, we propose a Reversible-Jump Markov Chain Monte Carlo (RJMCMC) sampling\u00a0\u2026", "num_citations": "161\n", "authors": ["1189"]}
{"title": "Estimating cohesion in small groups using audio-visual nonverbal behavior\n", "abstract": " Cohesiveness in teams is an essential part of ensuring the smooth running of task-oriented groups. Research in social psychology and management has shown that good cohesion in groups can be correlated with team effectiveness or productivity, so automatically estimating group cohesion for team training can be a useful tool. This paper addresses the problem of analyzing group behavior within the context of cohesion. Four hours of audio-visual group meeting data were used for collecting annotations on the cohesiveness of four-participant teams. We propose a series of audio and video features, which are inspired by findings in the social sciences literature. Our study is validated on a set of 61 2-min meeting segments which showed high agreement amongst human annotators when asked to identify meetings that have high or low cohesion.", "num_citations": "159\n", "authors": ["1189"]}
{"title": "Modeling human interaction in meetings\n", "abstract": " The paper investigates the recognition of group actions in meetings by modeling the joint behaviour of participants. Many meeting actions, such as presentations, discussions and consensus, are characterised by similar or complementary behaviour across participants. Recognising these meaningful actions is an important step towards the goal of providing effective browsing and summarisation of processed meetings. A corpus of meetings was collected in a room equipped with a number of microphones and cameras. The corpus was labeled in terms of a predefined set of meeting actions characterised by global behaviour. In experiments, audio and visual features for each participant are extracted from the raw data and the interaction of participants is modeled using HMM-based approaches. Initial results on the corpus demonstrate the ability of the system to recognise the set of meeting actions.", "num_citations": "153\n", "authors": ["1189"]}
{"title": "What your face vlogs about: expressions of emotion and big-five traits impressions in YouTube\n", "abstract": " Social video sites where people share their opinions and feelings are increasing in popularity. The face is known to reveal important aspects of human psychological traits, so the understanding of how facial expressions relate to personal constructs is a relevant problem in social media. We present a study of the connections between automatically extracted facial expressions of emotion and impressions of Big-Five personality traits in YouTube vlogs (i.e., video blogs). We use the Computer Expression Recognition Toolbox (CERT) system to characterize users of conversational vlogs. From CERT temporal signals corresponding to instantaneously recognized facial expression categories, we propose and derive four sets of behavioral cues that characterize face statistics and dynamics in a compact way. The cue sets are first used in a correlation analysis to assess the relevance of each facial expression of emotion\u00a0\u2026", "num_citations": "151\n", "authors": ["1189"]}
{"title": "Discovering human places of interest from multimodal mobile phone data\n", "abstract": " In this paper, a new framework to discover places-of-interest from multimodal mobile phone data is presented. Mobile phones have been used as sensors to obtain location information from users' real lives. Two levels of clustering are used to obtain places of interest. First, user location points are grouped using a time-based clustering technique which discovers stay points while dealing with missing location data. The second level performs clustering on the stay points to obtain stay regions. A grid-based clustering algorithm has been used for this purpose.", "num_citations": "151\n", "authors": ["1189"]}
{"title": "Detecting group interest-level in meetings\n", "abstract": " Finding relevant segments in meeting recordings is important for summarization, browsing, and retrieval purposes. In this paper, we define relevance as the interest-level that meeting participants manifest as a group during the course of their interaction (as perceived by an external observer), and investigate the automatic detection of segments of high-interest from audio-visual cues. This is motivated by the assumption that there is a relationship between segments of interest to participants, and those of interest to the end user, e.g. of a meeting browser. We first address the problem of human annotation of group interest-level. On a 50-meeting corpus, recorded in a room equipped with multiple cameras and microphones, we found that the annotations generated by multiple people exhibit a good degree of consistency, providing a stable ground-truth for automatic methods. For the automatic detection of high-interest\u00a0\u2026", "num_citations": "148\n", "authors": ["1189"]}
{"title": "Where and what: Using smartphones to predict next locations and applications in daily life\n", "abstract": " This paper investigates the prediction of two aspects of human behavior using smartphones as sensing devices. We present a framework for predicting where users will go and which app they will use in the next ten minutes by exploiting the rich contextual information from smartphone sensors. Our first goal is to understand which smartphone sensor data types are important for the two prediction tasks. Secondly, we aim at extracting generic (i.e., user-independent) behavioral patterns and study how generic behavior models can improve the predictive performance of personalized models. Experimental validation was conducted on the Lausanne Data Collection Campaign (LDCC) dataset, with longitudinal smartphone data collected over a period of 17 months from 71 users.", "num_citations": "147\n", "authors": ["1189"]}
{"title": "Contextual conditional models for smartphone-based human mobility prediction\n", "abstract": " Human behavior is often complex and context-dependent. This paper presents a general technique to exploit this\" multidimensional\" contextual variable for human mobility prediction. We use an ensemble method, in which we extract different mobility patterns with multiple models and then combine these models under a probabilistic framework. The key idea lies in the assumption that human mobility can be explained by several mobility patterns that depend on a sub-set of the contextual variables and these can be learned by a simple model. We showed how this idea can be applied to two specific online prediction tasks: what is the next place a user will visit? and how long will he stay in the current place?. Using smartphone data collected from 153 users during 17 months, we show the potential of our method in predicting human mobility in real life.", "num_citations": "146\n", "authors": ["1189"]}
{"title": "On the use of information retrieval measures for speech recognition evaluation\n", "abstract": " This paper discusses the evaluation of automatic speech recognition (ASR) systems developed for practical applications, suggesting a set of criteria for application-oriented performance measures. The commonly used word error rate (WER), which poses ASR evaluation as a string editing process, is shown to have a number of limitations with respect to these criteria, motivating alternative or additional measures. This paper suggests that posing speech recognition evaluation as an information retrieval problem, where each word is one unit of information, offers a flexible framework for application-oriented performance analysis based on the concepts of recall and precision.", "num_citations": "143\n", "authors": ["1189"]}
{"title": "The places of our lives: Visiting patterns and automatic labeling from longitudinal smartphone data\n", "abstract": " The location tracking functionality of modern mobile devices provides unprecedented opportunity to the understanding of individual mobility in daily life. Instead of studying raw geographic coordinates, we are interested in understanding human mobility patterns based on sequences of place visits which encode, at a coarse resolution, most daily activities. This paper presents a study on place characterization in people's everyday life based on data recorded continuously by smartphones. First, we study human mobility from sequences of place visits, including visiting patterns on different place categories. Second, we address the problem of automatic place labeling from smartphone data without using any geo-location information. Our study on a large-scale data collected from 114 smartphone users over 18 months confirm many intuitions, and also reveals findings regarding both regularly and novelty trends in\u00a0\u2026", "num_citations": "136\n", "authors": ["1189"]}
{"title": "Detecting abandoned luggage items in a public space\n", "abstract": " Visual surveillance is an important computer vision research problem. As more and more surveillance cameras appear around us, the demand for automatic methods for video analysis is increasing. Such methods have broad applications including surveillance for safety in public transportation, public areas, and in schools and hospitals. Automatic surveillance is also essential in the fight against terrorism. In this light, the PETS 2006 data corpus contains seven leftluggage scenarios with increasing scene complexity. The challenge is to automatically determine when pieces of luggage have been abandoned by their owners using video data, and set an alarm. In this paper, we present a solution to this problem using a two-tiered approach. The first step is to track objects in the scene using a trans-dimensional Markov Chain Monte Carlo tracking model suited for use in generic blob tracking tasks. The tracker uses a single camera view, and it does not differentiate between people and luggage. The problem of determining if a luggage item is left unattended is solved by analyzing the output of the tracking system in a detection process. Our model was evaluated over the entire data set, and successfully detected the left-luggage in all but one of the seven scenarios.", "num_citations": "134\n", "authors": ["1189"]}
{"title": "Analyzing flickr groups\n", "abstract": " There is an explosion of community-generated multimedia content available online. In particular, Flickr constitutes a 200-million photo sharing system where users participate following a variety of social motivations and themes. Flickr groups are increasingly used to facilitate the explicit definition of communities sharing common interests, which translates into large amounts of content (eg pictures and associated tags) about specific subjects. However, to our knowledge, an in-depth analysis of user behavior in Flickr groups remains open, as does the existence of effective tools to find relevant groups. Using a sample of about 7 million user-photos and about 51000 Flickr groups, we present a novel statistical group analysis that highlights relevant patterns of photo-to-group sharing practices. Furthermore, we propose a novel topic-based representation model for groups, computed from aggregated group tags. Groups\u00a0\u2026", "num_citations": "131\n", "authors": ["1189"]}
{"title": "You are known by how you vlog: Personality impressions and nonverbal behavior in youtube\n", "abstract": " An increasing interest in understanding human perception in social media has led to the study of the processes of personality self-presentation and impression formation based on user profiles and text blogs. However, despite the popularity of online video, we do not know of any attempt to study personality impressions that go beyond the use of text and still photos. In this paper, we analyze one facet of YouTube as a repository of brief behavioral slices in the form of personal conversational vlogs, which are a unique medium for self-presentation and interpersonal perception. We investigate the use of nonverbal cues as descriptors of vloggers' behavior and find significant associations between automatically extracted nonverbal cues for several personality judgments. As one notable result, audio and visual cues together can be used to predict 34% of the variance of the Extraversion trait of the Big Five model. In addition, we explore the associations between vloggers' personality scores and the level of social attention that their videos received in YouTube. Our study is conducted on a dataset of 442 YouTube vlogs and 2,210 annotations collected using Amazon's Mechanical Turk.", "num_citations": "121\n", "authors": ["1189"]}
{"title": "From big smartphone data to worldwide research: The mobile data challenge\n", "abstract": " This paper presents an overview of the Mobile Data Challenge (MDC), a large-scale research initiative aimed at generating innovations around smartphone-based research, as well as community-based evaluation of mobile data analysis methodologies. First, we review the Lausanne Data Collection Campaign (LDCC), an initiative to collect unique longitudinal smartphone dataset for the MDC. Then, we introduce the Open and Dedicated Tracks of the MDC, describe the specific datasets used in each of them, discuss the key design and implementation aspects introduced in order to generate privacy-preserving and scientifically relevant mobile data resources for wider use by the research community, and summarize the main research trends found among the 100+\u00a0challenge submissions. We finalize by discussing the main lessons learned from the participation of several hundred researchers worldwide in the MDC\u00a0\u2026", "num_citations": "115\n", "authors": ["1189"]}
{"title": "Detection and application of influence rankings in small group meetings\n", "abstract": " We address the problem of automatically detecting participant's influence levels in meetings. The impact and social psychological background are discussed. The more influential a participant is, the more he or she influences the outcome of a meeting. Experiments on 40 meetings show that application of statistical (both dynamic and static) models while using simply obtainable features results in a best prediction performance of 70.59% when using a static model, a balanced training set, and three discrete classes: high, normal and low. Application of the detected levels are shown in various ways ie in a virtual meeting environment as well as in a meeting browser system.", "num_citations": "113\n", "authors": ["1189"]}
{"title": "Modeling individual and group actions in meetings: a two-layer hmm framework\n", "abstract": " We address the problem of recognizing sequences of human interaction patterns in meetings, with the goal of structuring them in semantic terms. The investigated patterns are inherently group-based (defined by the individual activities of meeting participants, and their interplay), and multimodal (as captured by cameras and microphones). By defining a proper set of individual actions, group actions can be modeled as a two-layer process, one that models basic individual activities from low-level audio-visual features, and another one that models the interactions. We propose a two-layer Hidden Markov Model (HMM) framework that implements such concept in a principled manner, and that has advantages over previous works. First, by decomposing the problem hierarchically, learning is performed on low-dimensional observation spaces, which results in simpler models. Second, our framework is easier to interpret\u00a0\u2026", "num_citations": "113\n", "authors": ["1189"]}
{"title": "Human interaction discovery in smartphone proximity networks\n", "abstract": " Since humans are fundamentally social beings and interact frequently with others in their daily life, understanding social context is of primary importance in building context-aware applications. In this paper, using smartphone Bluetooth as a proximity sensor to create social networks, we present a probabilistic approach to mine human interaction types in real life. Our analysis is conducted on Bluetooth data continuously sensed with smartphones for over one year from 40 individuals who are professionally or personally related. The results show that the model can automatically discover a variety of social contexts. We objectively validated our model by studying its predictive and retrieval performance.", "num_citations": "106\n", "authors": ["1189"]}
{"title": "Social sensing for psychology: Automated interpersonal behavior assessment\n", "abstract": " In this article, we show how the use of state-of-the-art methods in computer science based on machine perception and learning allows the unobtrusive capture and automated analysis of interpersonal behavior in real time (social sensing). Given the high ecological validity of the behavioral sensing, the ease of behavioral-cue extraction for large groups over long observation periods in the field, the possibility of investigating completely new research questions, and the ability to provide people with immediate feedback on behavior, social sensing will fundamentally impact psychology.", "num_citations": "102\n", "authors": ["1189"]}
{"title": "One of a kind: Inferring personality impressions in meetings\n", "abstract": " We present an analysis on personality prediction in small groups based on trait attributes from external observers. We use a rich set of automatically extracted audio-visual nonverbal features, including speaking turn, prosodic, visual activity, and visual focus of attention features. We also investigate whether the thin sliced impressions of external observers generalize to the whole meeting in the personality prediction task. Using ridge regression, we have analyzed both the regression and classification performance of personality prediction. Our experiments show that the extraversion trait can be predicted with high accuracy in a binary classification task and visual activity features give higher accuracies than audio ones. The highest accuracy for the extraversion trait, is 75\\%, obtained with a combination of audio-visual features. Openness to experience trait also has a significant accuracy, only when the whole meeting\u00a0\u2026", "num_citations": "101\n", "authors": ["1189"]}
{"title": "Using audio and video features to classify the most dominant person in a group meeting\n", "abstract": " The automated extraction of semantically meaningful information from multi-modal data is becoming increasingly necessary due to the escalation of captured data for archival. A novel area of multi-modal data labelling, which has received relatively little attention, is the automatic estimation of the most dominant person in a group meeting. In this paper, we provide a framework for detecting dominance in group meetings using different audio and video cues. We show that by using a simple model for dominance estimation we can obtain promising results.", "num_citations": "101\n", "authors": ["1189"]}
{"title": "Method and apparatus for determining contextually relevant geographical locations\n", "abstract": " An approach is provided for determining and utilizing geographical locations contextually relevant to a user. A contextually relevant location platform determines location-based data associated with a user and/or user device. The contextually relevant location platform determines stationary points based, at least in part, on the location-based data. The contextually relevant location platform determines context data associated with the stationary points. The contextually relevant location platform determines at least one location anchor based, at least in part, on the stationary points and the associated context data, wherein the at least one location anchor represents a bounded geographical area of contextual relevance to the user.", "num_citations": "98\n", "authors": ["1189"]}
{"title": "Guest Editors' Introduction: Human-Centered Computing--Toward a Human Revolution\n", "abstract": " Human-centered computing studies the design, development, and deployment of mixed-initiative human-computer systems. HCC is emerging from the convergence of multiple disciplines that are concerned both with understanding human beings and with the design of computational artifacts.", "num_citations": "94\n", "authors": ["1189"]}
{"title": "Probabilistic mining of socio-geographic routines from mobile phone data\n", "abstract": " There is relatively little work on the investigation of large-scale human data in terms of multimodality for human activity discovery. In this paper, we suggest that human interaction data, or human proximity, obtained by mobile phone Bluetooth sensor data, can be integrated with human location data, obtained by mobile cell tower connections, to mine meaningful details about human activities from large and noisy datasets. We propose a model, called bag of multimodal behavior, that integrates the modeling of variations of location over multiple time-scales, and the modeling of interaction types from proximity. Our representation is simple yet robust to characterize real-life human behavior sensed from mobile phones, which are devices capable of capturing large-scale data known to be noisy and incomplete. We use an unsupervised approach, based on probabilistic topic models, to discover latent human activities in\u00a0\u2026", "num_citations": "93\n", "authors": ["1189"]}
{"title": "By their apps you shall understand them: mining large-scale patterns of mobile phone usage\n", "abstract": " Mobile phones are becoming more and more widely used nowadays, and people do not use the phone only for communication: there is a wide variety of phone applications allowing users to select those that fit their needs. Aggregated over time, application usage patterns exhibit not only what people are consistently interested in but also the way in which they use their phones, and can help improving phone design and personalized services. This work aims at mining automatically usage patterns from apps data recorded continuously with smartphones. A new probabilistic framework for mining usage patterns is proposed. Our methodology involves the design of a bag-of-apps model that robustly represents level of phone usage over specific times of the day, and the use of a probabilistic topic model that jointly discovers patterns of usage over multiple applications and describes users as mixtures of such patterns\u00a0\u2026", "num_citations": "92\n", "authors": ["1189"]}
{"title": "Semantic video object extraction using four-band watershed and partition lattice operators\n", "abstract": " We conceive the problem of multiple semantic video object (SVO) extraction as an issue of designing extensive operators on a complete lattice of partitions. As a result, we propose a framework based on spatial partition generation and application of optimal operators on the generated partitions. Based on a statistical analysis of the watershed algorithm, we develop a multivalued morphological spatial segmentation method that incorporates an edge-driven marker extraction algorithm and a growing method which integrates both color and edge information. Having embedded the problem in the partition lattice framework, we propose a spatio-temporal regional maximum likelihood operator for extraction purposes. Some theoretical properties of the operator are established. Experimental results on several MPEG-4 test video sequences show that our scheme improves the precision of the extracted SVO boundaries\u00a0\u2026", "num_citations": "91\n", "authors": ["1189"]}
{"title": "Groupus: Smartphone proximity data and human interaction type mining\n", "abstract": " There is an increasing interest in analyzing social interaction from mobile sensor data, and smart phones are rapidly becoming the most attractive sensing option. We propose a new probabilistic relational model to analyze long-term dynamic social networks created by physical proximity of people. Our model can infer different interaction types from the network, revealing the participants of a given group interaction, and discovering a variety of social contexts. Our analysis is conducted on Bluetooth data sensed with smart phones for over one year on the life of 40 individuals related by professional or personal links. We objectively validate our model by studying its predictive performance, showing a significant advantage over a recently proposed model.", "num_citations": "87\n", "authors": ["1189"]}
{"title": "Emergent leaders through looking and speaking: from audio-visual data to multimodal recognition\n", "abstract": " In this paper we present a multimodal analysis of emergent leadership in small groups using audio-visual features and discuss our experience in designing and collecting a data corpus for this purpose. The ELEA Audio-Visual Synchronized corpus (ELEA AVS) was collected using a light portable setup and contains recordings of small group meetings. The participants in each group performed the winter survival task and filled in questionnaires related to personality and several social concepts such as leadership and dominance. In addition, the corpus includes annotations on participants\u2019 performance in the survival task, and also annotations of social concepts from external viewers. Based on this corpus, we present the feasibility of predicting the emergent leader in small groups using automatically extracted audio and visual features, based on speaking turns and visual attention, and we focus specifically\u00a0\u2026", "num_citations": "85\n", "authors": ["1189"]}
{"title": "Estimating dominance in multi-party meetings using speaker diarization\n", "abstract": " With the increase in cheap commercially available sensors, recording meetings is becoming an increasingly practical option. With this trend comes the need to summarize the recorded data in semantically meaningful ways. Here, we investigate the task of automatically measuring dominance in small group meetings when only a single audio source is available. Past research has found that speaking length as a single feature, provides a very good estimate of dominance. For these tasks we use speaker segmentations generated by our automated faster than real-time speaker diarization algorithm, where the number of speakers is not known beforehand. From user-annotated data, we analyze how the inherent variability of the annotations affects the performance of our dominance estimation method. We primarily focus on examining of how the performance of the speaker diarization and our dominance tasks vary\u00a0\u2026", "num_citations": "84\n", "authors": ["1189"]}
{"title": "Speech enhancement and recognition in meetings with an audio\u2013visual sensor array\n", "abstract": " This paper addresses the problem of distant speech acquisition in multiparty meetings, using multiple microphones and cameras. Microphone array beamforming techniques present a potential alternative to close-talking microphones by providing speech enhancement through spatial filtering. Beamforming techniques, however, rely on knowledge of the speaker location. In this paper, we present an integrated approach, in which an audio-visual multiperson tracker is used to track active speakers with high accuracy. Speech enhancement is then achieved using microphone array beamforming followed by a novel postfiltering stage. Finally, speech recognition is performed to evaluate the quality of the enhanced speech signal. The approach is evaluated on data recorded in a real meeting room for stationary speaker, moving speaker, and overlapping speech scenarios. The results show that the speech enhancement\u00a0\u2026", "num_citations": "84\n", "authors": ["1189"]}
{"title": "Hi YouTube! Personality impressions and verbal content in social video\n", "abstract": " Despite the evidence that social video conveys rich human personality information, research investigating the automatic prediction of personality impressions in vlogging has shown that, amongst the Big-Five traits, automatic nonverbal behavioral cues are useful to predict mainly the Extraversion trait. This finding, also reported in other conversational settings, indicates that personality information may be coded in other behavioral dimensions like the verbal channel, which has been less studied in multimodal interaction research. In this paper, we address the task of predicting personality impressions from vloggers based on what they say in their YouTube videos. First, we use manual transcripts of vlogs and verbal content analysis techniques to understand the ability of verbal content for the prediction of crowdsourced Big-Five personality impressions. Second, we explore the feasibility of a fully-automatic framework\u00a0\u2026", "num_citations": "79\n", "authors": ["1189"]}
{"title": "Embedding motion in model-based stochastic tracking\n", "abstract": " Particle filtering is now established as one of the most popular methods for visual tracking. Within this framework, there are two important considerations. The first one refers to the generic assumption that the observations are temporally independent given the sequence of object states. The second consideration, often made in the literature, uses the transition prior as the proposal distribution. Thus, the current observations are not taken into account, requiring the noise process of this prior to be large enough to handle abrupt trajectory changes. As a result, many particles are either wasted in low likelihood regions of the state space, resulting in low sampling efficiency, or more importantly, propagated to distractor regions of the image, resulting in tracking failures. In this paper, we propose to handle both considerations using motion. We first argue that, in general, observations are conditionally correlated, and propose\u00a0\u2026", "num_citations": "79\n", "authors": ["1189"]}
{"title": "Vlogsense: Conversational behavior and social attention in youtube\n", "abstract": " We introduce the automatic analysis of conversational vlogs (VlogSense, for short) as a new research domain in social media. Conversational vlogs are inherently multimodal, depict natural behavior, and are suitable for large-scale analysis. Given their diversity in terms of content, VlogSense requires the integration of robust methods for multimodal analysis and for social media understanding. We present an original study on the automatic characterization of vloggers' audiovisual nonverbal behavior, grounded in work from social psychology and behavioral computing. Our study on 2,269 vlogs from YouTube shows that several nonverbal cues are significantly correlated with the social attention received by videos.", "num_citations": "78\n", "authors": ["1189"]}
{"title": "I would hire you in a minute: Thin slices of nonverbal behavior in job interviews\n", "abstract": " In everyday life, judgments people make about others are based on brief excerpts of interactions, known as thin slices. Inferences stemming from such minimal information can be quite accurate, and nonverbal behavior plays an important role in the impression formation. Because protagonists are strangers, employment interviews are a case where both nonverbal behavior and thin slices can be predictive of outcomes. In this work, we analyze the predictive validity of thin slices of real job interviews, where slices are defined by the sequence of questions in a structured interview format. We approach this problem from an audio-visual, dyadic, and nonverbal perspective, where sensing, cue extraction, and inference are automated. Our study shows that although nonverbal behavioral cues extracted from thin slices were not as predictive as when extracted from the full interaction, they were still predictive of hirability impressions\u00a0\u2026", "num_citations": "77\n", "authors": ["1189"]}
{"title": "A probabilistic approach to mining mobile phone data sequences\n", "abstract": " We present a new approach to address the problem of large sequence mining from big data. The particular problem of interest is the effective mining of long sequences from large-scale location data to be practical for Reality Mining applications, which suffer from large amounts of noise and lack of ground truth. To address this complex data, we propose an unsupervised probabilistic topic model called the distant n-gram topic model (DNTM). The DNTM is based on latent Dirichlet\u00a0allocation (LDA), which is extended to integrate sequential information. We define the generative process for the model, derive the inference procedure, and evaluate our model on both synthetic data and real mobile phone data. We consider two different mobile phone datasets containing natural human mobility patterns obtained by location sensing, the first considering GPS/wi-fi locations and the second considering cell tower\u00a0\u2026", "num_citations": "77\n", "authors": ["1189"]}
{"title": "Latent semantic analysis of facial action codes for automatic facial expression recognition\n", "abstract": " For supervised training of automatic facial expression recognition systems, adequate ground truth labels that describe relevant facial expression categories are necessary. One possibility is to label facial expressions into emotion categories. Another approach is to label facial expressions independently from any interpretation attempts. This can be achieved via the facial action coding system (FACS). In this paper we present a novel approach that allows to automatically cluster FACScodes into meaningful categories. Our approach exploits the fact that FACScodes can be seen as documents containing terms-the action units (AUs) present in the codes-and so text modeling methods that capture co-occurrence information in low-dimensional spaces can be used. The FACScode derived descriptions are computed by Latent Semantic Analysis (LSA) and Probabilistic Latent Semantic Analysis (PLSA). We show that, as a\u00a0\u2026", "num_citations": "77\n", "authors": ["1189"]}
{"title": "Finding structure in home videos by probabilistic hierarchical clustering\n", "abstract": " Accessing, organizing, and manipulating home videos present technical challenges due to their unrestricted content and lack of storyline. We present a methodology to discover cluster structure in home videos, which uses video shots as the unit of organization, and is based on two concepts: (1) the development of statistical models of visual similarity, duration, and temporal adjacency of consumer video segments and (2) the reformulation of hierarchical clustering as a sequential binary Bayesian classification process. A Bayesian formulation allows for the incorporation of prior knowledge of the structure of home video and offers the advantages of a principled methodology. Gaussian mixture models are used to represent the class-conditional distributions of intra- and inter-segment visual and temporal features. The models are then used in the probabilistic clustering algorithm, where the merging order is a variation\u00a0\u2026", "num_citations": "75\n", "authors": ["1189"]}
{"title": "Pervasive sensing to model political opinions in face-to-face networks\n", "abstract": " Exposure and adoption of opinions in social networks are important questions in education, business, and government. We describe a novel application of pervasive computing based on using mobile phone sensors to measure and model the face-to-face interactions and subsequent opinion changes amongst undergraduates, during the 2008 US presidential election campaign. We find that self-reported political discussants have characteristic interaction patterns and can be predicted from sensor data. Mobile features can be used to estimate unique individual exposure to different opinions, and help discover surprising patterns of dynamic homophily related to external political events, such as election debates and election day. To our knowledge, this is the first time such dynamic homophily effects have been measured. Automatically estimated exposure explains individual opinions on election day. Finally\u00a0\u2026", "num_citations": "74\n", "authors": ["1189"]}
{"title": "Learning influence among interacting Markov chains\n", "abstract": " We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.", "num_citations": "74\n", "authors": ["1189"]}
{"title": "A probabilistic kernel method for human mobility prediction with smartphones\n", "abstract": " Human mobility prediction is an important problem that has a large number of applications, especially in context-aware services. This paper presents a study on location prediction using smartphone data, in which we address modeling and application aspects. Building personalized location prediction models from smartphone data remains a technical challenge due to data sparsity, which comes from the complexity of human behavior and the typically limited amount of data available for individual users. To address this problem, we propose an approach based on kernel density estimation, a popular smoothing technique for sparse data. Our approach contributes to existing work in two ways. First, our proposed model can estimate the probability that a user will be at a given location at a specific time in the future, by using both spatial and temporal information via multiple kernel functions. Second, we also show how\u00a0\u2026", "num_citations": "73\n", "authors": ["1189"]}
{"title": "Analyzing group interactions in conversations: a review\n", "abstract": " Multiparty face-to-face conversations in professional and social settings represent an emerging research domain for which automatic activity-based analysis is relevant for scientific and practical reasons. The activity patterns emerging from groups engaged in conversations are intrinsically multimodal and thus constitute interesting target problems for multistream and multisensor fusion techniques. In this paper, a summarized review of the literature on automatic analysis of group activities in face-to-face conversational settings is presented. A basic categorization of group activities is proposed based on their typical temporal scale, and existing works are then discussed for various types of activities and trends including addressing, turn taking, interest, and dominance", "num_citations": "72\n", "authors": ["1189"]}
{"title": "Real-time face detection using boosting in hierarchical feature spaces\n", "abstract": " Boosting-based methods have recently led to the state-of-the-art-face detection systems. In these systems, weak classifiers to be boosted are based on simple, local, Haar-like features. However, it can be empirically observed that in later stages of the boosting process, the non-face examples collected by bootstrapping become very similar to the face examples, and the classification error of Haar-like feature based weak classifiers is thus very close to 50%. As a result, the performance of a face detector cannot be further improved. This paper proposed a solution to this problem, introducing a face detection method based on boosting in hierarchical feature spaces (both local and global). We argue that global features, like those derived from principal component analysis, can be advantageously used in the later stages of boosting, when local features do not provide any further benefit. We show that weak classifiers\u00a0\u2026", "num_citations": "71\n", "authors": ["1189"]}
{"title": "Modeling flickr communities through probabilistic topic-based analysis\n", "abstract": " With the increased presence of digital imaging devices, there also came an explosion in the amount of multimedia content available online. Users have transformed from passive consumers of media into content creators and have started organizing themselves in and around online communities. Flickr has more than 30 million users and over 3 billion photos, and many of them are tagged and public. One very important aspect in Flickr is the ability of users to organize in self-managed communities called groups. This paper examines an unexplored problem, which is jointly analyzing Flickr groups and users. We show that although users and groups are conceptually different, in practice they can be represented in a similar way via a bag-of-tags derived from their photos, which is amenable for probabilistic topic modeling. We then propose a probabilistic topic model representation learned in an unsupervised manner\u00a0\u2026", "num_citations": "70\n", "authors": ["1189"]}
{"title": "Audio-visual speaker tracking with importance particle filters\n", "abstract": " We present a probabilistic method for audio-visual (AV) speaker tracking, using an uncalibrated wide-angle camera and a micro- phone array. The algorithm fuses 2-D object shape and audio information via importance particle filters (I-PFs), allowing for the asymmetrical integration of AV information in a way that efficiently exploits the complementary features of each modality. Audio localization information is used to generate an importance sampling (IS) function, which guides the random search process of a particle filter towards regions of the configuration space likely to contain the true configuration (a speaker). The measurement process integrates contour-based and audio observations, which results in reliable head tracking in realistic scenarios. We show that imperfect single modalities can be combined into an algorithm that automatically initializes and tracks a speaker, switches between multiple speakers\u00a0\u2026", "num_citations": "65\n", "authors": ["1189"]}
{"title": "Broadcasting oneself: Visual discovery of vlogging styles\n", "abstract": " We present a data-driven approach to discover different styles that people use to present themselves in online video blogging (vlogging). By vlogging style, we denote the combination of conscious and unconscious choices that the vlogger made during the production of the vlog, affecting the video quality, appearance, and structure. A compact set of vlogging styles is discovered using clustering methods based on a fast and robust spatio-temporal descriptor to characterize the visual activity in a vlog. On 2268 YouTube vlogs, our results show that the vlogging styles are differentiated with respect to the vloggers' level of editing and conversational activity in the video. Furthermore, we show that these automatically discovered styles relate to vloggers with different personality trait impressions and to vlogs that receive different levels of social attention.", "num_citations": "64\n", "authors": ["1189"]}
{"title": "In the mood for vlog: Multimodal inference in conversational social video\n", "abstract": " The prevalent \u201cshare what's on your mind\u201d paradigm of social media can be examined from the perspective of mood: short-term affective states revealed by the shared data. This view takes on new relevance given the emergence of conversational social video as a popular genre among viewers looking for entertainment and among video contributors as a channel for debate, expertise sharing, and artistic expression. From the perspective of human behavior understanding, in conversational social video both verbal and nonverbal information is conveyed by speakers and decoded by viewers. We present a systematic study of classification and ranking of mood impressions in social video, using vlogs from YouTube. Our approach considers eleven natural mood categories labeled through crowdsourcing by external observers on a diverse set of conversational vlogs. We extract a comprehensive number of nonverbal\u00a0\u2026", "num_citations": "63\n", "authors": ["1189"]}
{"title": "Analyzing ancient maya glyph collections with contextual shape descriptors\n", "abstract": " This paper presents an original approach for shape-based analysis of ancient Maya hieroglyphs based on an interdisciplinary collaboration between computer vision and archeology. Our work is guided by realistic needs of archaeologists and scholars who critically need support for search and retrieval tasks in large Maya imagery collections. Our paper has three main contributions. First, we introduce an overview of our interdisciplinary approach towards the improvement of the documentation, analysis, and preservation of Maya pictographic data. Second, we present an objective evaluation of the performance of two state-of-the-art shape-based contextual descriptors (Shape Context and Generalized Shape Context) in retrieval tasks, using two datasets of syllabic Maya glyphs. Based on the identification of their limitations, we propose a new shape descriptor named Histogram of Orientation Shape Context\u00a0\u2026", "num_citations": "63\n", "authors": ["1189"]}
{"title": "The workshop on computational personality recognition 2014\n", "abstract": " The Workshop on Computational Personality Recognition aims to define the state-of-the-art in the field and to provide tools for future standard evaluations in personality recognition tasks. In the WCPR14 we released two different datasets: one of Youtube Vlogs and one of Mobile Phone interactions. We structured the workshop in two tracks: an open shared task, where participants can do any kind of experiment, and a competition. We also distinguished two tasks: A) personality recognition from multimedia data, and B) personality recognition from text only. In this paper we discuss the results of the workshop.", "num_citations": "61\n", "authors": ["1189"]}
{"title": "Video structuring by probabilistic merging of video segments\n", "abstract": " A method for structuring video by probabilistic merging of video segments includes the steps of obtaining a plurality of frames of unstructured video; generating video segments from the unstructured video by detecting shot boundaries based on color dissimilarity between consecutive frames; extracting a feature set by processing pairs of segments for visual dissimilarity and their temporal relationship, thereby generating an inter-segment visual dissimilarity feature and an inter-segment temporal relationship feature; and merging video segments with a merging criterion that applies a probabilistic analysis to the feature set, thereby generating a merging sequence representing the video structure. The probabilistic analysis follows a Bayesian formulation and the merging sequence is represented in a hierarchical tree structure.", "num_citations": "60\n", "authors": ["1189"]}
{"title": "Anomaly detection in elderly daily behavior in ambient sensing environments\n", "abstract": " Current ubiquitous computing applications for smart homes aim to enhance people\u2019s daily living respecting age span. Among the target groups of people, elderly are a population eager for \u201cchoices for living arrangements\u201d, which would allow them to continue living in their homes but at the same time provide the health care they need. Given the growing elderly population, there is a need for statistical models able to capture the recurring patterns of daily activity life and reason based on this information. We present an analysis of real-life sensor data collected from 40 different households of elderly people, using motion, door and pressure sensors. Our objective is to automatically observe and model the daily behavior of the elderly and detect anomalies that could occur in the sensor data. For this purpose, we first introduce an abstraction layer to create a common ground for home sensor configurations. Next\u00a0\u2026", "num_citations": "59\n", "authors": ["1189"]}
{"title": "Hirability in the wild: Analysis of online conversational video resumes\n", "abstract": " Online social media is changing the personnel recruitment process. Until now, resumes were among the most widely used tools for the screening of job applicants. The advent of inexpensive sensors combined with the success of online video platforms has enabled the introduction of a new type of resume. Video resumes are short video messages where job applicants present themselves to potential employers. Online video resumes represent an opportunity to study the formation of first impressions in an employment context at a scale never attempted before, and to our knowledge they have not been studied from a behavioral standpoint. We collected a dataset of 939 conversational English-speaking video resumes from YouTube. Annotations of demographics, skills, and first impressions were collected using the Amazon Mechanical Turk crowdsourcing platform. Basic demographics were then analyzed to\u00a0\u2026", "num_citations": "59\n", "authors": ["1189"]}
{"title": "Investigating automatic dominance estimation in groups from visual attention and speaking activity\n", "abstract": " We study the automation of the visual dominance ratio (VDR); a classic measure of displayed dominance in social psychology literature, which combines both gaze and speaking activity cues. The VDR is modified to estimate dominance in multi-party group discussions where natural verbal exchanges are possible and other visual targets such as a table and slide screen are present. Our findings suggest that fully automated versions of these measures can estimate effectively the most dominant person in a meeting and can match the dominance estimation performance when manual labels of visual attention are used.", "num_citations": "58\n", "authors": ["1189"]}
{"title": "Spectral structuring of home videos\n", "abstract": " Accessing and organizing home videos present technical challenges due to their unrestricted content and lack of storyline. In this paper, we propose a spectral method to group video shots into scenes based on their visual similarity and temporal relations. Spectral methods have been shown to be effective in capturing perceptual organization features. In particular, we investigate the problem of automatic model selection, which is currently an open research issue for spectral methods, and propose measures to assess the validity of a grouping result. The methodology is used to group scenes from a six-hour home video database, and is assessed with respect to a ground-truth generated by multiple people. The results indicate the validity of the proposed approach, both compared to existing techniques as well as the human ground-truth.", "num_citations": "58\n", "authors": ["1189"]}
{"title": "Extensive partition operators, gray-level connected operators, and region merging/classification segmentation algorithms: theoretical links\n", "abstract": " The relation between morphological gray-level connected operators and segmentation algorithms based on region merging/classification strategies has been pointed out several times in the literature. However, to the best of our knowledge, the formal relation between them has not been established. This paper presents the link between the two domains based on the observation that both connected operators and segmentation algorithms share a key mechanism: they simultaneously operate on images and on partitions, and therefore they can be described as operations on a joint image-partition model. As a result, we analyze both segmentation algorithms and connected operators by defining operators on complete product lattices, that explicitly model gray-level and partition attributes. In the first place, starting with a complete lattice of partitions, we initially define the concept of the segmentation model as a\u00a0\u2026", "num_citations": "58\n", "authors": ["1189"]}
{"title": "Identifying emergent leadership in small groups using nonverbal communicative cues\n", "abstract": " This paper addresses firstly an analysis on how an emergent leader is perceived in newly formed small-groups, and secondly, explore correlations between perception of leadership and automatically extracted nonverbal communicative cues. We hypothesize that the difference in individual nonverbal features between emergent leaders and non-emergent leaders is significant and measurable using speech activity. Our results on a new interaction corpus show that such an approach is promising, identifying the emergent leader with an accuracy of up to 80%.", "num_citations": "57\n", "authors": ["1189"]}
{"title": "Flickr hypergroups\n", "abstract": " The amount of multimedia content available online constantly increases, and this leads to problems for users who search for content or similar communities. Users in Flickr often self-organize in user communities through Flickr Groups. These groups are particularly interesting as they are a natural instantiation of the content~+~ relations social media paradigm. We propose a novel approach to group searching through hypergroup discovery. Starting from roughly 11,000 Flickr groups' content and membership information, we create three different bag-of-word representations for groups, on which we learn probabilistic topic models. Finally, we cast the hypergroup discovery as a clustering problem that is solved via probabilistic affinity propagation. We show that hypergroups so found are generally consistent and can be described through topic-based and similarity-based measures. Our proposed solution could be\u00a0\u2026", "num_citations": "57\n", "authors": ["1189"]}
{"title": "The vernissage corpus: A conversational human-robot-interaction dataset\n", "abstract": " We introduce a new conversational Human-Robot-Interaction (HRI) dataset with a real-behaving robot inducing interactive behavior with and between humans. Our scenario involves a humanoid robot NAO 1  explaining paintings in a room and then quizzing the participants, who are naive users. As perceiving nonverbal cues, apart from the spoken words, plays a major role in social interactions and socially-interactive robots, we have extensively annotated the dataset. It has been recorded and annotated to benchmark many relevant perceptual tasks, towards enabling a robot to converse with multiple humans, such as speaker localization and speech segmentation; tracking, pose estimation, nodding, visual focus of attention estimation in visual domain; and an audio-visual task such as addressee detection. NAO system states are also available. As compared to recordings done with a static camera, this corpus\u00a0\u2026", "num_citations": "56\n", "authors": ["1189"]}
{"title": "Predicting two facets of social verticality in meetings from five-minute time slices and nonverbal cues\n", "abstract": " This paper addresses the automatic estimation of two aspects of social verticality (status and dominance) in small-group meetings using nonverbal cues. The correlation of nonverbal behavior with these social constructs have been extensively documented in social psychology, but their value for computational models is, in many cases, still unknown. We present a systematic study of automatically extracted cues-including vocalic, visual activity, and visual attention cues-and investigate their relative effectiveness to predict both the most-dominant person and the high-status project manager from relative short observations. We use five hours of task-oriented meeting data with natural behavior for our experiments. Our work suggests that, although dominance and role-based status are related concepts, they are not equivalent and are thus not equally explained by the same nonverbal cues. Furthermore, the best cues\u00a0\u2026", "num_citations": "56\n", "authors": ["1189"]}
{"title": "Characterizing the demographics behind the# blacklivesmatter movement\n", "abstract": " The debates on minority issues are often dominated by or held among the concerned minorities: gender equality debates have often failed to engage men, while those about race fail to engage the dominant group. To test this observation, we study the# BlackLivesMatter movement and hashtag on Twitter--that has emerged and gained traction after a series of events typically involving the death of African-Americans as a result of police brutality--aiming to quantify the population biases across user types (individuals vs. organizations), and (for individuals) across 3 demographics factors (race, gender and age). Our results suggest that more African-Americans engage with the hashtag, and that they are also more active than other demographic groups. We also discuss ethical caveats with broader implications for studies on sensitive topics (eg mental health or religion) that focus on users.", "num_citations": "55\n", "authors": ["1189"]}
{"title": "Discovering human routines from cell phone data with topic models\n", "abstract": " We present a framework to automatically discover people's routines from information extracted by cell phones. The framework is built from a probabilistic topic model learned on novel bag type representations of activity-related cues (location, proximity and their temporal variations over a day) of peoples' daily routines. Using real-life data from the Reality Mining dataset, covering 68 000+ hours of human activities, we can successfully discover location-driven (from cell tower connections) and proximity-driven (from Bluetooth information) routines in an unsupervised manner. The resulting topics meaningfully characterize some of the underlying co-occurrence structure of the activities in the dataset, including ldquogoing to work early/laterdquo, ldquobeing home all dayrdquo, ldquoworking constantlyrdquo, ldquoworking sporadicallyrdquo and ldquomeeting at lunch timerdquo.", "num_citations": "53\n", "authors": ["1189"]}
{"title": "Linking speaking and looking behavior patterns with group composition, perception, and performance\n", "abstract": " This paper addresses the task of mining typical behavioral patterns from small group face-to-face interactions and linking them to social-psychological group variables. Towards this goal, we define group speaking and looking cues by aggregating automatically extracted cues at the individual and dyadic levels. Then, we define a bag of nonverbal patterns (Bag-of-NVPs) to discretize the group cues. The topics learnt using the Latent Dirichlet Allocation (LDA) topic model are then interpreted by studying the correlations with group variables such as group composition, group interpersonal perception, and group performance. Our results show that both group behavior cues and topics have significant correlations with (and predictive information for) all the above variables. For our study, we use interactions with unacquainted members ie newly formed groups.", "num_citations": "52\n", "authors": ["1189"]}
{"title": "Voices of vlogging\n", "abstract": " Vlogs have rapidly evolved from the\u2019chat from your bedroom\u2019format to a highly creative form of expression and communication. However, despite the high popularity of vlogging, automatic analysis of conversational vlogs have not been attempted in the literature. In this paper, we present a novel analysis of conversational vlogs based on the characterization of vloggers\u2019 nonverbal behavior. We investigate the use of four nonverbal cues extracted automatically from the audio channel to measure the behavior of vloggers and explore the relation to their degree of popularity and that of their videos. Our study is validated on over 2200 videos and 150 hours of data, and shows that one nonverbal cue (speaking time) is correlated with levels of popularity with a medium size effect.", "num_citations": "52\n", "authors": ["1189"]}
{"title": "Fusing audio-visual nonverbal cues to detect dominant people in group conversations\n", "abstract": " This paper addresses the multimodal nature of social dominance and presents multimodal fusion techniques to combine audio and visual nonverbal cues for dominance estimation in small group conversations. We combine the two modalities both at the feature extraction level and at the classifier level via score and rank level fusion. The classification is done by a simple rule-based estimator. We perform experiments on a new 10-hour dataset derived from the popular AMI meeting corpus. We objectively evaluate the performance of each modality and each cue alone and in combination. Our results show that the combination of audio and visual cues is necessary to achieve the best performance.", "num_citations": "48\n", "authors": ["1189"]}
{"title": "Rotation-invariant neoperceptron\n", "abstract": " Approaches based on local features and descriptors are increasingly used for the task of object recognition due to their robustness with regard to occlusions and geometrical deformations of objects. In this paper we present a local feature based, rotation-invariant Neoperceptron. By extending the weight-sharing properties of convolutional neural networks to orientations, we obtain a neural network that is inherently robust to object rotations, while still being capable to learn optimally discriminant features from training data. The performance of the network is evaluated on a facial expression database and compared to a standard Neoperceptron as well as to the scale invariant feature transform (SIFT), a-state-of-the-art local descriptor. The results confirm the validity of our approach", "num_citations": "48\n", "authors": ["1189"]}
{"title": "Personality trait classification via co-occurrent multiparty multimodal event discovery\n", "abstract": " This paper proposes a novel feature extraction framework from mutli-party multimodal conversation for inference of personality traits and emergent leadership. The proposed framework represents multi modal features as the combination of each participant's nonverbal activity and group activity. This feature representation enables to compare the nonverbal patterns extracted from the participants of different groups in a metric space. It captures how the target member outputs nonverbal behavior observed in a group (eg the member speaks while all members move their body), and can be available for any kind of multiparty conversation task. Frequent co-occurrent events are discovered using graph clustering from multimodal sequences. The proposed framework is applied for the ELEA corpus which is an audio visual dataset collected from group meetings. We evaluate the framework for binary classification task of 10\u00a0\u2026", "num_citations": "47\n", "authors": ["1189"]}
{"title": "Nonverbal social sensing in action: Unobtrusive recording and extracting of nonverbal behavior in social interactions illustrated with a research example\n", "abstract": " Nonverbal behavior coding is typically conducted by \u201chand\u201d. To remedy this time and resource intensive undertaking, we illustrate how nonverbal social sensing, defined as the automated recording and extracting of nonverbal behavior via ubiquitous social sensing platforms, can be achieved. More precisely, we show how and what kind of nonverbal cues can be extracted and to what extent automated extracted nonverbal cues can be validly obtained with an illustrative research example. In a job interview, the applicant\u2019s vocal and visual nonverbal immediacy behavior was automatically sensed and extracted. Results show that the applicant\u2019s nonverbal behavior can be validly extracted. Moreover, both visual and vocal applicant nonverbal behavior predict recruiter hiring decision, which is in line with previous findings on manually coded applicant nonverbal behavior. Finally, applicant average turn\u00a0\u2026", "num_citations": "47\n", "authors": ["1189"]}
{"title": "The night is young: urban crowdsourcing of nightlife patterns\n", "abstract": " We present a mobile crowdsourcing study to capture and examine the nightlife patterns of two youth populations in Switzerland. Our contributions are three fold. First, we developed a smartphone application to capture data on places, social context and nightlife activities, and to record mobile videos capturing the ambiance of places. Second, we conducted an\" in-the-wild\" study with more than 200 participants over a period of three months in two Swiss cities, resulting in a total of 1,394 unique place visits and 843 videos that spread across place categories (including personal homes and public parks), social and ambiance variables. Finally, we investigated the use of automatic ambiance features to estimate the loudness and brightness of places at scale, and found that while features are reliable with respect to video content, videos do not always reflect the place ambiance reported by people in-situ. We believe that\u00a0\u2026", "num_citations": "44\n", "authors": ["1189"]}
{"title": "Daily routine classification from mobile phone data\n", "abstract": " The automatic analysis of real-life, long-term behavior and dynamics of individuals and groups from mobile sensor data constitutes an emerging and challenging domain. We present a framework to classify people\u2019s daily routines (defined by day type, and by group affiliation type) from real-life data collected with mobile phones, which include physical location information (derived from cell tower connectivity), and social context (given by person proximity information derived from Bluetooth). We propose and compare single- and multi-modal routine representations at multiple time scales, each capable of highlighting different features from the data, to determine which best characterized the underlying structure of the daily routines. Using a massive data set of 87000+ hours spanning four months of the life of 30 university students, we show that the integration of location and social context and the use of\u00a0\u2026", "num_citations": "44\n", "authors": ["1189"]}
{"title": "Video shot clustering using spectral methods\n", "abstract": " The automatic segmentation and structuring of videos present technical challenges due to the large variation of content, spatial layout, and possible lack of storyline. In this paper, we propose a spectral method to group video shots into scenes based on their visual similarity and temporal relations. Spectral methods have been shown to be effective in capturing perceptual organization features. In particular, we investigate the problem of automatic model selection, which is currently an open research issue for spectral methods, and propose measures to assess the validity of a grouping result. The methodology is used to group shots from home videos and soccer games. The results indicate the validity of the proposed approach, both compared to existing techniques as well as to human performance.", "num_citations": "44\n", "authors": ["1189"]}
{"title": "Training on the job: Behavioral analysis of job interviews in hospitality\n", "abstract": " First impressions play a critical role in the hospitality industry and have been shown to be closely linked to the behavior of the person being judged. In this work, we implemented a behavioral training framework for hospitality students with the goal of improving the impressions that other people make about them. We outline the challenges associated with designing such a framework and embedding it in the everyday practice of a real hospitality school. We collected a dataset of 169 laboratory sessions where two role-plays were conducted, job interviews and reception desk scenarios, for a total of 338 interactions. For job interviews, we evaluated the relationship between automatically extracted nonverbal cues and various perceived social variables in a correlation analysis. Furthermore, our system automatically predicted first impressions from job interviews in a regression task, and was able to explain up to 32% of\u00a0\u2026", "num_citations": "40\n", "authors": ["1189"]}
{"title": "Mining group nonverbal conversational patterns using probabilistic topic models\n", "abstract": " The automatic discovery of group conversational behavior is a relevant problem in social computing. In this paper, we present an approach to address this problem by defining a novel group descriptor called bag of group-nonverbal-patterns (NVPs) defined on brief observations of group interaction, and by using principled probabilistic topic models to discover topics. The proposed bag of group NVPs allows fusion of individual cues and facilitates the eventual comparison of groups of varying sizes. The use of topic models helps to cluster group interactions and to quantify how different they are from each other in a formal probabilistic sense. Results of behavioral topics discovered on the Augmented Multi-Party Interaction (AMI) meeting corpus are shown to be meaningful using human annotation with multiple observers. Our method facilitates \u201cgroup behavior-based\u201d retrieval of group conversational segments without\u00a0\u2026", "num_citations": "40\n", "authors": ["1189"]}
{"title": "Estimating the dominant person in multi-party conversations using speaker diarization strategies\n", "abstract": " In this paper, we apply speaker diarization strategies from a single source to the task of estimating the dominant person in a group meeting. Previous work has shown that speaking length is strongly correlated with perceived dominance. Here we investigate this in more depth by considering two dominance tasks where there is full and majority agreement amongst ground-truth annotators. In addition, we investigate how 24 different speed-up and algorithmic strategies, and source types lead to interesting outcomes when applied to dominance estimation. We obtained the best performance of 77% using our slowest scheme and a single distant microphone (SDM). Within the top 3 out of 24 performing experiments in both dominance tasks, we show that we can use the furthest SDM, with no prior knowledge of the number of speakers and the fastest diarization scheme, which performs 1.3 times faster than real-time.", "num_citations": "39\n", "authors": ["1189"]}
{"title": "An audio visual corpus for emergent leader analysis\n", "abstract": " In this paper we discuss our experience designing and collecting a data corpus called ELEA (Emergent LEader Analysis), and describe the use of a light portable scenario to record small group meetings. The corpus was gathered with the aim of analyzing emergent leadership, as a social phenomenon that occurs in newly formed groups. For each group in the corpus, the participants performed the winter survival task. To date, the annotations of the corpus include personality, concepts related to leadership, and participants\u2019 performance in the survival task. In addition, several features have been extracted automatically from audio and video.", "num_citations": "38\n", "authors": ["1189"]}
{"title": "Multimodal analysis of body communication cues in employment interviews\n", "abstract": " Hand gestures and body posture are intimately linked to speech as they are used to enrich the vocal content, and are therefore inherently multimodal. As an important part of nonverbal behavior, body communication carries relevant information that can reveal social constructs as diverse as personality, internal states, or job interview outcomes. In this work, we analyze body communication cues in real dyadic employment interviews, where the protagonists of the interaction are seated. We use a mixture of body communicative features based on manual annotations and automated extraction methods to successfully predict two key organizational constructs, namely personality and job interview ratings. Our work also confirms the multimodal nature of body communication and shows that the speaking status can be used to improve the prediction performance of personality and hirability.", "num_citations": "37\n", "authors": ["1189"]}
{"title": "Body communicative cue extraction for conversational analysis\n", "abstract": " Nonverbal communication plays an important role in many aspects of our lives, such as in job interviews, where vis-\u03b1-vis conversations take place. This paper proposes a method to automatically detect body communicative cues by using video sequences of the upper body of individuals in a conversational context. To our knowledge, our work brings novelty by explicitly addressing the recognition of visual activity in a seated, conversational setting from monocular video, compared to most existing work in video-based motion capture, which targets full-body with lower limb activities. We first detect the person hands in the sequence by searching for the higher speed parts along the whole video. Then, aided by training a set of typical conversational movements, we infer the approximate 3D upper body pose, that we transfer to a low-dimensionality space in order to perform action recognition. We test our system in the\u00a0\u2026", "num_citations": "37\n", "authors": ["1189"]}
{"title": "A multimodal corpus for studying dominance in small group conversations\n", "abstract": " We present a new multimodal corpus with dominance annotations on small group conversations. We used five-minute non-overlapping slices from a subset of meetings selected from the popular Augmented Multi-party Interaction (AMI) corpus. The total length of the annotated corpus corresponds to 10 hours of meeting data. Each meeting is observed and assessed by three annotators according to their level of perceived dominance. We analyzed the annotations with respect to dominance, status, gender and behavior. The results of the analysis reflect the findings in the social psychology literature on dominance. The described dataset provides an appropriate testbed for automatic dominance analysis.", "num_citations": "37\n", "authors": ["1189"]}
{"title": "Unsupervised speech/non-speech detection for automatic speech recognition in meeting rooms\n", "abstract": " The goal of this work is to provide robust and accurate speech detection for automatic speech recognition (ASR) in meeting room settings. The solution is based on computing long-term modulation spectrum, and examining specific frequency range for dominant speech components to classify speech and non-speech signals for a given audio signal. Manually segmented speech segments, short-term energy, short-term energy and zero-crossing based segmentation techniques, and a recently proposed multi layer perceptron (MLP) classifier system are tested for comparison purposes. Speech recognition evaluations of the segmentation methods are performed on a standard database and tested in conditions where the signal-to-noise ratio (SNR) varies considerably, as in the cases of close-talking headset, lapel, distant microphone array output, and distant microphone. The results reveal that the proposed method is\u00a0\u2026", "num_citations": "37\n", "authors": ["1189"]}
{"title": "Towards computer understanding of human interactions\n", "abstract": " People meet in order to interact \u2013 disseminating information, making decisions, and creating new ideas. Automatic analysis of meetings is therefore important from two points of view: extracting the information they contain, and understanding human interaction processes. Based on this view, this article presents an approach in which relevant information content of a meeting is identified from a variety of audio and visual sensor inputs and statistical models of interacting people. We present a framework for computer observation and understanding of interacting people, and discuss particular tasks within this framework, issues in the meeting context, and particular algorithms that we have adopted. We also comment on current developments and the future challenges in automatic meeting analysis.", "num_citations": "37\n", "authors": ["1189"]}
{"title": "Rapport with virtual agents: What do human social cues and personality explain?\n", "abstract": " Rapport has been recognized as an important aspect of relationship building. While rapport in the context of human-human interaction has been widely studied, how it can be established and maintained in human-agent interaction has been studied only recently. Our study investigates how social cues and personality of a human interacting with an agent can be used for automatic prediction of rapport in this context. We conduct experiments with two emotional virtual agents. Alongside the audio-visual data, we also collect human personality measures and two measures of rapport: self-reported rapport and rapport judged by observers. The social cues, such as turn-taking patterns and facial expressions are extracted from audio-visual data. Our results show that the most significant cues that infer the rapport judgments are the number of turn-taking cues and pauses. We also find that some of the significant social cues\u00a0\u2026", "num_citations": "36\n", "authors": ["1189"]}
{"title": "Using self-context for multimodal detection of head nods in face-to-face interactions\n", "abstract": " Head nods occur in virtually every face-to-face discussion. As part of the backchannel domain, they are not only used to express a'yes', but also to display interest or enhance communicative attention. Detecting head nods in natural interactions is a challenging task as head nods can be subtle, both in amplitude and duration. In this study, we make use of findings in psychology establishing that the dynamics of head gestures are conditioned on the person's speaking status. We develop a multimodal method using audio-based self-context to detect head nods in natural settings. We demonstrate that our multimodal approach using the speaking status of the person under analysis significantly improved the detection rate over a visual-only approach.", "num_citations": "36\n", "authors": ["1189"]}
{"title": "Multimodal group action clustering in meetings\n", "abstract": " We address the problem of clustering multimodal group actions in meetings using a two-layer HMM framework. Meetings are structured as sequences of group actions. Our approach aims at creating one cluster for each group action, where the number of group actions and the action boundaries are unknown a priori. In our framework, the first layer models typical actions of individuals in meetings using supervised HMM learning and low-level audio-visual features. A number of options that explicitly model certain aspects of the data (eg, asynchrony) were considered. The second layer models the group actions using unsupervised HMM learning. The two layers are linked by a set of probability-based features produced by the individual action layer as input to the group action layer. The methodology was assessed on a set of multimodal turn-taking group actions, using a public five-hour meeting corpus. The results\u00a0\u2026", "num_citations": "36\n", "authors": ["1189"]}
{"title": "Order matters: A distributed sampling method for multiple-object tracking\n", "abstract": " Multi-Object tracking (MOT) is an important problem in a number of vision applications. For particle filter (PF) tracking, as the number of objects tracked increases, the search space for random sampling explodes in dimension. Partitioned sampling (PS) solves this problem by partitioning the search space, then searching each partition sequentially. However, sequential weighted resampling steps cause an impoverishment effect that increases with the number of objects. This effect depends on the specific order in which the partitions are explored, creating an erratic and undesirable performance. We propose a method to search the state space that fairly distributes these impoverishment effects between the objects by defining a set of mixture components and performing PS in each of these components using one of a small set of representative object orderings. Using synthetic and real data, we show that our method retains the overall performance and reduced computational cost of PS, while improving performance in scenes where the impoverishment effect is significant. 1", "num_citations": "36\n", "authors": ["1189"]}
{"title": "Multimodal integration for meeting group action segmentation and recognition\n", "abstract": " We address the problem of segmentation and recognition of sequences of multimodal human interactions in meetings. These interactions can be seen as a rough structure of a meeting, and can be used either as input for a meeting browser or as a first step towards a higher semantic analysis of the meeting. A common lexicon of multimodal group meeting actions, a shared meeting data set, and a common evaluation procedure enable us to compare the different approaches. We compare three different multimodal feature sets and our modelling infrastructures: a higher semantic feature approach, multi-layer HMMs, a multi-stream DBN, as well as a multi-stream mixed-state DBN for disturbed data.", "num_citations": "35\n", "authors": ["1189"]}
{"title": "Wearing a YouTube hat: Directors, comedians, gurus, and user aggregated behavior\n", "abstract": " While existing studies on YouTube's massive user-generated video content have mostly focused on the analysis of videos, their characteristics, and network properties, little attention has been paid to the analysis of users' long-term behavior as it relates to the roles they self-define and (explicitly or not) play in the site. In this paper, we present a novel statistical analysis of aggregated user behavior in YouTube from the novel perspective of user categories, a feature that allows people to ascribe to popular roles and to potentially reach certain communities. Using a sample of 270,000 users, we found that a high level of interaction and participation is concentrated on a relatively small, yet significant, group of users, following recognizable patterns of personal and social involvement. Based on our analysis, we also show that by using simple behavioral features from user profiles, people can be automatically classified\u00a0\u2026", "num_citations": "34\n", "authors": ["1189"]}
{"title": "Learning and predicting multimodal daily life patterns from cell phones\n", "abstract": " In this paper, we investigate the multimodal nature of cell phone data in terms of discovering recurrent and rich patterns in people's lives. We present a method that can discover routines from multiple modalities (location and proximity) jointly modeled, and that uses these informative routines to predict unlabeled or missing data. Using a joint representation of location and proximity data over approximately 10 months of 97 individuals' lives, Latent Dirichlet Allocation is applied for the unsupervised learning of topics describing people's most common locations jointly with the most common types of interactions at these locations. We further successfully predict where and with how many other individuals users will be, for people with both highly and lowly varying lifestyles.", "num_citations": "31\n", "authors": ["1189"]}
{"title": "Cross-domain personality prediction: from video blogs to small group meetings\n", "abstract": " In this study, we investigate the use of social media content as a domain to learn personality trait impressions, particularly extraversion. Our aim is to transfer the knowledge that can be extracted from conversational videos in video blogging sites to small group settings to predict the extraversion trait with nonverbal cues. We use YouTube data containing personality impression scores of 442 people as the source domain and a small-group meeting data from a total of 102 people as our target domain. Our results show that, for the extraversion trait, by using user-created video blogs, as part of the training data, and a small amount of adaptation data from the target domain, we are able to achieve higher prediction accuracies than using only the data recorded in small group settings.", "num_citations": "30\n", "authors": ["1189"]}
{"title": "Speech acquisition in meetings with an audio-visual sensor array\n", "abstract": " Close-talk headset microphones have been traditionally used for speech acquisition in a number of applications, as they naturally provide a higher signal-to-noise ratio -needed for recognition tasks-than single distant microphones. However, in multi-party conversational settings like meetings, microphone arrays represent an important alternative to close-talking microphones, as they allow for localisation and tracking of speakers and signal-independent enhancement, while providing a non-intrusive, hands-free operation mode. In this article, we investigate the use of an audio-visual sensor array, composed of a small table-top microphone array and a set of cameras, for speaker tracking and speech enhancement in meetings. Our methodology first fuses audio and video for person tracking, and then integrates the output of the tracker with a beamformer for speech enhancement. We compare and discuss the features\u00a0\u2026", "num_citations": "30\n", "authors": ["1189"]}
{"title": "Loud and trendy: Crowdsourcing impressions of social ambiance in popular indoor urban places\n", "abstract": " New research cutting across architecture, urban studies, and psychology is contextualizing the understanding of urban spaces according to the perceptions of their inhabitants. One fundamental construct that relates place and experience is ambiance, which is defined as\" the mood or feeling associated with a particular place\". We posit that the systematic study of ambiance dimensions in cities is a new domain for which multimedia research can make pivotal contributions. We present a study to examine how images collected from social media can be used for the crowdsourced characterization of indoor ambiance impressions in popular urban places. We design a crowdsourcing framework to understand suitability of social images as data source to convey place ambiance, to examine what type of images are most suitable to describe ambiance, and to assess how people perceive places socially from the perspective\u00a0\u2026", "num_citations": "29\n", "authors": ["1189"]}
{"title": "You are fired! nonverbal role analysis in competitive meetings\n", "abstract": " This paper addresses the problem of social interaction analysis in competitive meetings, using nonverbal cues. For our study, we made use of ldquoThe Apprenticerdquo reality TV show, which features a competition for a real, highly paid corporate job. Our analysis is centered around two tasks regarding a person's role in a meeting: predicting the person with the highest status and predicting the fired candidates. The current study was carried out using nonverbal audio cues. Results obtained from the analysis of a full season of the show, representing around 90 minutes of audio data, are very promising (up to 85.7% of accuracy in the first case and up to 92.8% in the second case). Our approach is based only on the nonverbal interaction dynamics during the meeting without relying on the spoken words.", "num_citations": "29\n", "authors": ["1189"]}
{"title": "Tracking the multi person wandering visual focus of attention\n", "abstract": " Estimating the wandering visual focus of attention (WVFOA) for multiple people is an important problem with many applications in human behavior understanding. One such application, addressed in this paper, monitors the attention of passers-by to outdoor advertisements. This paper investigates the problem of tracking the wandering visual focus-of-attention (VFOA) of multiple people, an important problem with many applications in human behavior understanding. We address the specific problem of monitoring attention to outdoor advertisements. To solve the WVFOA problem, we propose a multi-person tracking approach based on a hybrid Dynamic Bayesian Network that simultaneously infers the number of people in the scene, their body and head locations, and their head pose, in a joint state-space formulation that is amenable for person interaction modeling. The model exploits both global measurements and\u00a0\u2026", "num_citations": "29\n", "authors": ["1189"]}
{"title": "Multimodal multispeaker probabilistic tracking in meetings\n", "abstract": " Tracking speakers in multiparty conversations constitutes a fundamental task for automatic meeting analysis. In this paper, we present a probabilistic approach to jointly track the location and speaking activity of multiple speakers in a multisensor meeting room, equipped with a small microphone array and multiple uncalibrated cameras. Our framework is based on a mixed-state dynamic graphical model defined on a multiperson state-space, which includes the explicit definition of a proximity-based interaction model. The model integrates audio-visual (AV) data through a novel observation model. Audio observations are derived from a source localization algorithm. Visual observations are based on models of the shape and spatial structure of human heads. Approximate inference in our model, needed given its complexity, is performed with a Markov Chain Monte Carlo particle filter (MCMC-PF), which results in high\u00a0\u2026", "num_citations": "29\n", "authors": ["1189"]}
{"title": "A mixed-state i-particle filter for multi-camera speaker tracking\n", "abstract": " Tracking speakers in multi-party conversations represents an important step towards automatic analysis of meetings. In this paper, we present a probabilistic method for audio-visual (AV) speaker tracking in a multi-sensor meeting room. The algorithm fuses information coming from three uncalibrated cameras and a microphone array via a mixed-state importance particle filter, allowing for the integration of AV streams to exploit the complementary features of each modality. Our method relies on several principles. First, a mixed state space formulation is used to define a generative model for camera switching. Second, AV localization information is used to define an importance sampling function, which guides the search process of a particle filter towards regions of the configuration space likely to contain the true configuration (a speaker). Finally, the measurement process integrates shape, color, and audio observations. We show that the principled combination of imperfect modalities results in an algorithm that automatically initializes and tracks speakers engaged in real conversations, reliably switching across cameras and between participants.", "num_citations": "29\n", "authors": ["1189"]}
{"title": "Checking in or checked in: comparing large-scale manual and automatic location disclosure patterns\n", "abstract": " Studies on human mobility are built on two fundamentally different data sources: manual check-in data that originates from location-based social networks and automatic check-in data that can be automatically collected through various smartphone sensors. In this paper, we analyze the differences and similarities of manual check-ins from Foursquare and automatic check-ins from Nokia's Mobile Data Challenge. Several new findings follow from our analysis:(1) While automatic checking-in overall results in more visits than manual checking-in, the check-in levels are comparable when visiting new places.(2) Daily and weekly check-in activity patterns are similar for both systems except for Saturdays--when manual check-ins are relatively more probable.(3) A recently proposed rank distribution to describe human mobility, so far validated on manual check-in data, also holds for automatic check-in data given a slight\u00a0\u2026", "num_citations": "27\n", "authors": ["1189"]}
{"title": "DrinkSense: Characterizing youth drinking behavior using smartphones\n", "abstract": " Alcohol consumption is the number one risk factor for morbidity and mortality among young people. In late adolescence and early adulthood, excessive drinking and intoxication are more common than in any other life period, increasing the risk of adverse physical and psychological health consequences. In this paper, we examine the feasibility of using smartphone sensor data and machine learning to automatically characterize and classify drinking behavior of young adults in an urban, ecologically valid nightlife setting. Our work has two contributions. First, we use previously unexplored data from a large-scale mobile crowdsensing study involving 241 young participants in two urban areas in a European country, which includes phone data (location. accelerometer, Wifi, Bluetooth, battery, screen, and app usage) along with self-reported, fine-grain data on individual alcoholic drinks consumed on Friday and\u00a0\u2026", "num_citations": "26\n", "authors": ["1189"]}
{"title": "SenseCityVity: Mobile crowdsourcing, urban awareness, and collective action in Mexico\n", "abstract": " This work describes SenseCityVity, an approach to engage and support youth in a city in Mexico as they investigate, document, and reflect upon urban problems through mobile crowdsourcing. SenseCityVity focused on the development of a mobile crowdsourcing platform; the deployment of the Urban Data Challenge, codesigned by the authors' research team and actors to collect geolocalized images, audio, and video; and the analysis, appropriation, and creative use of the collected data for community reflection and artistic creation. The approach integrates mobile technology and community practices involving a large population of young people for urban engagement. The collective action generated a new multimedia dataset that is rich in terms of content and is enabling a number of studies aimed at better understanding the urban landscape of cities in the Global South. This article is part of a special issue on\u00a0\u2026", "num_citations": "25\n", "authors": ["1189"]}
{"title": "Communisense: Crowdsourcing road hazards in nairobi\n", "abstract": " Nairobi is one of the fastest growing metropolitan cities and a major business and technology powerhouse in Africa. However, Nairobi currently lacks monitoring technologies to obtain reliable data on traffic and road infrastructure conditions. In this paper, we investigate the use of mobile crowdsourcing as means to gather and document Nairobi's road quality information. We first present the key findings of a city-wide road quality survey about the perception of existing road quality conditions in Nairobi. Based on the survey's findings, we then developed a mobile crowdsourcing application, called CommuniSense, to collect road quality data. The application serves as a tool for users to locate, describe, and photograph road hazards. We tested our application through a two-week field study amongst 30 participants to document various forms of road hazards from different areas in Nairobi. To verify the authenticity of user\u00a0\u2026", "num_citations": "24\n", "authors": ["1189"]}
{"title": "Searching the past: an improved shape descriptor to retrieve maya hieroglyphs\n", "abstract": " Archaeologists often spend significant time looking at traditional printed catalogs to identify and classify historical images. Our collaborative efforts between archaeologists and multimedia researchers seek to develop a tool to retrieve two specific types of ancient Maya visual information: hieroglyphs and iconographic elements. Towards that goal we present two contributions in this paper. The first one is the introduction and analysis of a new dataset of 3400+ Maya hieroglyphs, whose compilation involved manual search, annotation and segmentation by experts. This dataset presents several challenges for visual description and automatic retrieval as it is rich in complex visual details. The second and main contribution is the in-depth analysis of the Histogram Of Orientation Shape Context (HOOSC), and more precisely, the development of 4 improvements that were designed to handle the visual complexity of Maya\u00a0\u2026", "num_citations": "23\n", "authors": ["1189"]}
{"title": "Contextual grouping: discovering real-life interaction types from longitudinal bluetooth data\n", "abstract": " By exploiting built-in sensors, mobile smart phone have become attractive options for large-scale sensing of human behavior as well as social interaction. In this paper, we present a new probabilistic model to analyze longitudinal dynamic social networks created by the physical proximity of people sensed continuously by the phone Bluetooth sensors. A new probabilistic model is proposed in order to jointly infer emergent grouping modes of the community together with their temporal context. We present experimental results on a Bluetooth proximity network sensed with mobile smart-phones over 9 months of continuous real-life, and show the effectiveness of our method.", "num_citations": "23\n", "authors": ["1189"]}
{"title": "The good, the bad, and the angry: Analyzing crowdsourced impressions of vloggers\n", "abstract": " We address the study of interpersonal perception in social conversational video based on multifaceted impressions collected from short video-watching. First, we crowdsourced the annotation of personality, attractiveness, and mood impressions for a dataset of YouTube vloggers, generating a corpora that has potential to develop automatic techniques for vlogger characterization. Then, we provide an analysis of the crowdsourced annotations focusing on the level of agreement among annotators, as well as the interplay between different impressions. Overall, this work provides interesting new insights on vlogger impressions and the use of crowdsourcing to collect behavioral annotations from multimodal data.", "num_citations": "22\n", "authors": ["1189"]}
{"title": "Tracking people in meetings with particles\n", "abstract": " Automatic meeting analysis is an emerging research field. In this paper, we present stochastic algorithms for tracking people in multi-sensor meeting rooms, for a number of relevant tasks, including tracking multiple people, tracking head pose towards analysis of visual focus-of-attention, and tracking speaker activity using audio-visual information. A Bayesian framework based on Sequential Monte Carlo methods is used in all cases. We discuss the advantages and limitations of our approach, illustrate it with results, and highlight a number of open issues.", "num_citations": "22\n", "authors": ["1189"]}
{"title": "A hierarchical keyframe user interface for browsing video over the Internet\n", "abstract": " We present an interactive content-based video browser allowing fast, non linear and hierarchical navigation of video over the Internet through multiple levels of key-frames that provide a visual summary of video content. Our method is based on an XML framework, dynamically generated parameterized XSL style sheets, and SMIL. The architecture is designed to incorporate additional recognized features (eg from audio) in future versions. The last part of this paper describes a user study which indicates that this browsing interface is more comfortable to use and approximately three times faster for locating remembered still images within videos compared to the simple VCR controls built into RealPlayer", "num_citations": "22\n", "authors": ["1189"]}
{"title": "Bites \u2018n\u2019bits: Inferring eating behavior from contextual mobile data\n", "abstract": " We collect and analyze mobile data about everyday eating occasions to study eating behavior in relation to its context (time, location, social context, related activities and physical activity). Our contributions are three-fold. First, we deployed a data collection campaign with 122 Swiss university students, resulting in 1208 days of food data, 3414 meal occasions, 1034 snacking occasions, 5097 photos, and 998 days of physical activity. Second, we analyzed the collected data and report findings associated to the compliance, snacks vs. meals patterns, physical activity, and contextual differences between snacks and meals. Third, we addressed a novel ubicomp task, namely the classification of eating occasions (meals vs. snacks) in everyday life. We show that a machine learning method using time of day, time since last intake, and location is able to discriminate eating occasions with 84% accuracy, which significantly\u00a0\u2026", "num_citations": "20\n", "authors": ["1189"]}
{"title": "Inferring social activities with mobile sensor networks\n", "abstract": " While our daily activities usually involve interactions with others, the current methods on activity recognition do not often exploit the relationship between social interactions and human activity. This paper addresses the problem of interpreting social activity from human interactions captured by mobile sensing networks. Our first goal is to discover different social activities such as chatting with friends from interaction logs and then characterize them by the set of people involved, and the time and location of the occurring event. Our second goal is to perform automatic labeling of the discovered activities using predefined semantic labels such as coffee breaks, weekly meetings, or random discussions. Our analysis was conducted on a real-life interaction network sensed with Bluetooth and infrared sensors of about fifty subjects who carried sociometric badges over 6 weeks. We show that the proposed system reliably\u00a0\u2026", "num_citations": "20\n", "authors": ["1189"]}
{"title": "Capturing drinking and nightlife behaviours and their social and physical context with a smartphone application\u2013investigation of users\u2019 experience and reactivity\n", "abstract": " Background: Many addictive behaviours are influenced by the context in which they occur, but methods for simultaneously capturing the characteristics of a behaviour and its context are scarce. This study describes a smartphone application developed to document young adults\u2019 nightlife and drinking behaviours and investigates its impact on participants\u2019 lives.Methods: 241 participants, aged 16\u201325 (46.5% women), were asked to document 10 Friday and Saturday nights over seven weekends. Using their own smartphones, they documented the beverages consumed and the social and physical context by means of questionnaires, photos, and video clips, while phone sensors (e.g., GPS, Bluetooth, accelerometer) were running in the background. Quantitative and additional qualitative data (40 in-depth interviews) were used to investigate response burden, assessment reactivity, and disruption of usual activities\u00a0\u2026", "num_citations": "19\n", "authors": ["1189"]}
{"title": "Emergent power hierarchies and group performance\n", "abstract": " In newly formed groups, informal hierarchies emerge automatically and readily. In this study, we argue that emergent group hierarchies enhance group performance (Hypothesis 1) and we assume that the more the power hierarchy within a group corresponds to the task\u2010competence differences of the individual group members, the better the group performs (Hypothesis 2). Twelve three\u2010person groups and 28 four\u2010person groups were investigated while solving the Winter Survival Task. Results show that emerging power hierarchies positively impact group performance but the alignment between task\u2010competence and power hierarchy did not affect group performance. Thus, emergent power hierarchies are beneficial for group performance and although they were on average created around individual group members' competence, this correspondence was not a prerequisite for better group performance.", "num_citations": "19\n", "authors": ["1189"]}
{"title": "Let your body speak: Communicative cue extraction on natural interaction using RGBD data\n", "abstract": " Employment interviews are relevant scenarios for the study of social interaction. In this setting, social skills play an important role, even though the interactions between potential employers and candidates are often limited. One fundamental aspect of social interaction is the use of nonverbal communication , which affects how we are socially perceived. We present a method to automatically extract body communicative cues from one-on-one conversations recorded with Kinect devices. First, we find the three-dimensional position of hands and head of the subject, and, aided by training data, we infer the upper body pose. Then, we use the inferred poses to perform action recognition and build person-specific activity descriptors. We evaluate our system with both domain-specific and public, generic datasets, and show competitive performance.", "num_citations": "19\n", "authors": ["1189"]}
{"title": "How do you like your virtual agent?: Human-agent interaction experience through nonverbal features and personality traits\n", "abstract": " Recent studies suggest that human interaction experience with virtual agents can be, to a very large degree, described by people\u2019s personality traits. Moreover, the nonverbal behavior of a person has been known to indicate several social constructs in different settings. In this study, we analyze human-agent interaction from the perspective of the personality of the human and the nonverbal behaviors he/she displays during the interaction. Based on existing work in psychology, we designed and recorded an experiment on human-agent interactions, in which a human communicates with two different virtual agents. Human-agent interactions are described with three self-reported measures: quality, rapport and likeness of the agent. We investigate the use of self-reported personality traits and extracted audio-visual nonverbal features as descriptors of these measures. Our results on a correlation analysis show\u00a0\u2026", "num_citations": "19\n", "authors": ["1189"]}
{"title": "Characterizing conversational group dynamics using nonverbal behaviour\n", "abstract": " This paper addresses the novel problem of characterizing conversational group dynamics. It is well documented in social psychology that depending on the objectives a group, the dynamics are different. For example, a competitive meeting has a different objective from that of a collaborative meeting. We propose a method to characterize group dynamics based on the joint description of a group members' aggregated acoustical nonverbal behaviour to classify two meeting datasets (one being cooperative-type and the other being competitive-type). We use 4.5 hours of real behavioural multi-party data and show that our methodology can achieve a classification rate of upto 100%.", "num_citations": "19\n", "authors": ["1189"]}
{"title": "Constructing visual models with a latent space approach\n", "abstract": " We propose the use of latent space models applied to local invariant features for object classification. We investigate whether using latent space models enables to learn patterns of visual co-occurrence and if the learned visual models improve performance when less labeled data are available. We present and discuss results that support these hypotheses. Probabilistic Latent Semantic Analysis (PLSA) automatically identifies aspects from the data with semantic meaning, producing unsupervised soft clustering. The resulting compact representation retains sufficient discriminative information for accurate object classification, and improves the classification accuracy through the use of unlabeled data when less labeled training data are available. We perform experiments on a 7-class object database containing 1776 images.", "num_citations": "19\n", "authors": ["1189"]}
{"title": "The young and the city: Crowdsourcing urban awareness in a developing country\n", "abstract": " We present a crowdsourcing study that investigates impressions of urban spaces by young inhabitants in a city in the developing world. Our goal is to obtain collective perceptions from the actual inhabitants of the city under study, and more specifically youth (16-18 year-old) about issues like danger, accessibility, and dirtiness. We collect over 9000 judgments for 102 photos of outdoor urban spaces in a city in Central Mexico using standard scales in social sciences. We present reliability and response analyses and demonstrate how local youth can provide relevant urban insights in a crowdsourcing setting.", "num_citations": "18\n", "authors": ["1189"]}
{"title": "Wordless sounds: Robust speaker diarization using privacy-preserving audio representations\n", "abstract": " This paper investigates robust privacy-sensitive audio features for speaker diarization in multiparty conversations: i.e., a set of audio features having low linguistic information for speaker diarization in a single and multiple distant microphone scenarios. We systematically investigate Linear Prediction (LP) residual. Issues such as prediction order and choice of representation of LP residual are studied. Additionally, we explore the combination of LP residual with subband information from 2.5 kHz to 3.5 kHz and spectral slope. Next, we propose a supervised framework using deep neural architecture for deriving privacy-sensitive audio features. We benchmark these approaches against the traditional Mel Frequency Cepstral Coefficients (MFCC) features for speaker diarization in both the microphone scenarios. Experiments on the RT07 evaluation dataset show that the proposed approaches yield diarization\u00a0\u2026", "num_citations": "18\n", "authors": ["1189"]}
{"title": "Associating audio-visual activity cues in a dominance estimation framework\n", "abstract": " We address the problem of both estimating the dominant person in a meeting from a single audio source and identifying them visually in a multi-camera setting. We use a speaker diarization algorithm to perform speaker segmentation and clustering, representing when they spoke. Using a greedy ordered audio-visual association algorithm, we investigate using the speaker clusters to find the corresponding person in one of the video channels. The difficulty of the problem is that firstly the speaker diarization output is noisy (e.g. for participants who speak little) and often produces an unequal number of clusters to true participants. Secondly, personal visual activity from natural upper torso motion, which can include highly deformable pose changes and perspective distortion, is computed through computationally efficient coarse features. Our results using almost 2 hours of audio-visual data from 4-participant meetings\u00a0\u2026", "num_citations": "18\n", "authors": ["1189"]}
{"title": "Automatic blinking detection towards stress discovery\n", "abstract": " We present a robust method to automatically detect blinks in video sequences of conversations, aimed to discovering stress. Psychological studies have shown a relationship between blink frequency and dopamine levels, which in turn are affected by stress. Task performance correlates through an inverted U shape to both dopamine and stress levels. This shows the importance of automatic blink detection as a way of reducing human coding burden. We use an off-the-shelf face tracker in order to extract the eye region. Then, we perform per-pixel classification of the extracted eye images to later identify blinks through their dynamics. We evaluate the performance of our system with a job interview database with annotations of psychological variables, and show statistically significant correlation between perceived stress resistance and the automatically detected blink patterns.", "num_citations": "17\n", "authors": ["1189"]}
{"title": "Modeling dominance effects on nonverbal behaviors using granger causality\n", "abstract": " In this paper we modeled the effects that dominant people might induce on the nonverbal behavior (speech energy and body motion) of the other meeting participants using Granger causality technique. Our initial hypothesis that more dominant people have generalized higher influence was not validated when using the DOME-AMI corpus as data source. However, from the correlational analysis some interesting patterns emerged: contradicting our initial hypothesis dominant individuals are not accounting for the majority of the causal flow in a social interaction. Moreover, they seem to have more intense causal effects as their causal density was significantly higher. Finally dominant individuals tend to respond to the causal effects more often with complementarity than with mimicry.", "num_citations": "17\n", "authors": ["1189"]}
{"title": "Inferring competitive role patterns in reality TV show through nonverbal analysis\n", "abstract": " This paper introduces a new facet of social media, namely that depicting social interaction. More concretely, we address this problem from the perspective of nonverbal behavior-based analysis of competitive meetings. For our study, we made use of \u201cThe Apprentice\u201d reality TV show, which features a competition for a real, highly paid corporate job. Our analysis is centered around two tasks regarding a person\u2019s role in a meeting: predicting the person with the highest status, and predicting the fired candidates. We address this problem by adopting both supervised and unsupervised strategies. The current study was carried out using nonverbal audio cues. Our approach is based only on the nonverbal interaction dynamics during the meeting without relying on the spoken words. The analysis is based on two types of data: individual and relational measures. Results obtained from the analysis of a full season of\u00a0\u2026", "num_citations": "17\n", "authors": ["1189"]}
{"title": "Recognizing conversational context in group interaction using privacy-sensitive mobile sensors\n", "abstract": " The availability of mobile sociometric sensors allows Computer-Supported Cooperative Work (CSCW) designers the possibility to enhance online meeting support through automatic recognition of conversational context. This paper addresses the task of discriminating one conversational context against another, specifically brainstorming from decision-making interactions using easily computable nonverbal behavioral cues. We hypothesize that the difference in the dynamics between brainstorming and decision-making discussions is significant and measurable using speech activity based nonverbal cues. We employ a set of nonverbal cues to characterize the entire group by the aggregation (both temporal and person-wise) of their nonverbal behavior. Our results on a dataset collected using privacy-sensitive sociometric badges show that the floor-occupation patterns in a brain-storming interaction are different from\u00a0\u2026", "num_citations": "17\n", "authors": ["1189"]}
{"title": "Integrating co-occurrence and spatial contexts on patchbased scene segmentation\n", "abstract": " We present a novel approach for contextual segmentation of complex visual scenes, based on the use of bags of local invariant features (visterms) and probabilistic aspect models. Our approach uses context in two ways: (1) by using the fact that specific learned aspects correlate with the semantic classes, which resolves some cases of visual polysemy, and (2) by formalizing the notion that scene context is image-specific -what an individual visterm represents depends on what the rest of the visterms in the same bag represent too-. We demonstrate the validity of our approach on a man-made vs. natural visterm classification problem. Experiments on an image collection of complex scenes show that the approach improves region discrimination, producing satisfactory results, and outperforming a non-contextual method. Furthermore, through the later use of a Markov Random Field model, we also show that co\u00a0\u2026", "num_citations": "17\n", "authors": ["1189"]}
{"title": "Looking at cities in Mexico with crowds\n", "abstract": " Mobile and social technologies are providing new opportunities to document, characterize, and gather impressions of urban environments. In this paper, we present a study that examines urban perceptions of three cities in central Mexico (Guanajuato, Leon and Silao), which integrates a mobile crowdsourcing framework to collect geo-localized images of urban environments by a local youth community, and an online crowdsourcing platform (Amazon Mechanical Turk) to gather impressions of urban environments along twelve physical and psychological dimensions. Our study resulted in a collection of 7,000 geo-localized images containing outdoor scenes and views of each city's built environment, including touristic, historical, and residential neighbourhoods; and 156,000 individual judgments from MTurk. Statistical analyses show that outdoor environments can be reliably assessed with respect to most urban\u00a0\u2026", "num_citations": "16\n", "authors": ["1189"]}
{"title": "A Bayesian hierarchical model for classifying craniofacial malformations from CT imaging\n", "abstract": " Single-suture craniosynostosis is a condition of the sutures of the infant's skull that causes major craniofacial deformities and is associated with an increased risk of cognitive deficits and learning/language disabilities. In this paper we adapt to classification of synostostic head shapes a Bayesian methodology that overcomes the limitations of our previously published shape representation and classification techniques. We evaluate our approach in a series of large-scale experiments and show performance superior to those of standard approaches such as Fourier descriptors, cranial spectrum, and Euclidian-distance-based analyses.", "num_citations": "16\n", "authors": ["1189"]}
{"title": "Discovering groups of people in google news\n", "abstract": " In this paper, we study the problem of content-based social network discovery among people who frequently appear in world news. Google news is used as the source of data. We describe a probabilistic framework for associating people with groups. A low-dimensional topic-based representation is first obtained for news stories via probabilistic latent semantic analysis (PLSA). This is followed by construction of semantic groups by clustering such representations. Unlike many existing social network analysis approaches, which discover groups based only on binary relations (eg co-occurrence of people in a news article), our model clusters people using their topic distribution, which introduces contextual information in the group formation process (eg some people belong to several groups depending on the specific subject). The model has been used to study evolution of people with respect to topics over time. We\u00a0\u2026", "num_citations": "16\n", "authors": ["1189"]}
{"title": "Discovering group nonverbal conversational patterns with topics\n", "abstract": " This paper addresses the problem of discovering conversational group dynamics from nonverbal cues extracted from thin-slices of interaction. We first propose and analyze a novel thin-slice interaction descriptor-a bag of group nonverbal patterns-which robustly captures the turn-taking behavior of the members of a group while integrating its leader's position. We then rely on probabilistic topic modeling of the interaction descriptors which, in a fully unsupervised way, is able to discover group interaction patterns that resemble prototypical leadership styles proposed in social psychology. Our method, validated on the Augmented Multi-Party Interaction (AMI) meeting corpus, facilitates the retrieval of group conversational segments where semantically meaningful group behaviours emerge, without the need of any previous labeling.", "num_citations": "15\n", "authors": ["1189"]}
{"title": "Semi-supervised meeting event recognition with adapted HMMs\n", "abstract": " This paper investigates the use of unlabeled data to help labeled data for audio-visual event recognition in meetings. To deal with situations in which it is difficult to collect enough labeled data to capture event characteristics, but collecting a large amount of unlabeled data is easy, we present a semi-supervised framework using HMM adaptation techniques. Instead of directly training one model for each event, we first train a well-estimated general event model for all events using both labeled and unlabeled data, and then adapt the general model to each specific event model using its own labeled data. We illustrate the proposed approach with a set of eight audio-visual events defined in meetings. Experiments and comparison with the fully-supervised baseline method show the validity of the proposed semi-supervised approach.", "num_citations": "15\n", "authors": ["1189"]}
{"title": "Multiview extensive partition operators for semantic video object extraction\n", "abstract": " Occlusion/disocclusion is one of the fundamental problems for semantic video object (SVO) extraction, where pixel-wise accuracy is required. This issue is critical because the degradation in tracking due to object occlusion/disocclusion significantly increases the amount of user interaction required in off-line video editing applications. We present an approach based on the application of an extensive operator on a lattice of partitions, which exploits information from various views of the scene, based on a probabilistic formulation. Our multiview operator builds on the regional application of the maximum a posteriori principle, by integrating a single-view region classification stage with a multiview stage that improves classification for those disoccluded regions labeled as uncertain. Results on several real sequences show that our approach improves the SVO tracking compared to the single-view case and that, as a\u00a0\u2026", "num_citations": "15\n", "authors": ["1189"]}
{"title": "Validity of pervasive computing based continuous physical activity assessment in community-dwelling old and oldest-old\n", "abstract": " In older adults, physical activity is crucial for healthy aging and associated with numerous health indicators and outcomes. Regular assessments of physical activity can help detect early health-related changes and manage physical activity targeted interventions. The quantification of physical activity, however, is difficult as commonly used self-reported measures are biased and rather unprecise point in time measurements. Modern alternatives are commonly based on wearable technologies which are accurate but suffer from usability and compliance issues. In this study, we assessed the potential of an unobtrusive ambient-sensor based system for continuous, long-term physical activity quantification. Towards this goal, we analysed one year of longitudinal sensor-and medical-records stemming from thirteen community-dwelling old and oldest old subjects. Based on the sensor data the daily number of room\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Looking south: Learning urban perception in developing cities\n", "abstract": " Mobile and social technologies are providing new opportunities to document, characterize, and gather impressions of urban environments. In this article, we present a study that examines urban perceptions of three cities in central Mexico; the study integrates a mobile crowdsourcing framework to collect geo-localized images of urban environments by a local youth community, an online crowdsourcing platform to gather impressions of urban environments along 12 physical and psychological dimensions, and a deep learning framework to automatically infer human impressions of outdoor urban scenes. Our study resulted in a collection of 7,000 geo-localized images containing outdoor scenes and views of each city\u2019s built environment, including touristic, historical, and residential neighborhoods, and 144,000 individual judgments from Amazon Mechanical Turk. Statistical analyses show that outdoor environments\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Healthy# fondue# dinner: analysis and inference of food and drink consumption patterns on instagram\n", "abstract": " Social media generate large-scale data to study food and drink consumption in everyday life. Using Instagram posts in Switzerland over five years, our goal is two-fold. First, we extract key food & drink consumption patterns, through the lenses of a data-driven dictionary of popular items extracted from hashtags, and of a food categorization system used by the Swiss Federal government for national statistics purposes. Patterns related to spatial and temporal distributions of food & drink consumption, demographics, and eating events are extracted and compared to official statistics. Second, using the insights from this analysis, we define two eating event classification tasks, including a two-class task (healthy vs. unhealthy) and a six-class task (the three main meals break-fast/lunch/dinner/plus brunch/coffee/tea). Both tasks use hash-tags as labels for supervised learning. We study how content (hashtags and food\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Elderly people living alone: Detecting home visits with ambient and wearable sensing\n", "abstract": " Ubiquitous computing techniques are enabling the possibility to provide remote health care services to elderly citizens. In such systems, daily activities are extracted from raw sensor signals, based on which users? health status can be inferred. Due to the ambiguity of raw sensor signals, it is challenging to distinguish the number of people in the ambient, and most such systems assume user live alone. We present an algorithm to automatically detect home visits to elderly people living alone, using an ambient and wearable sensing network. We use visiting reports from caregivers as partially labeled positive data, and conduct statistical analysis to gain insights of visit events in terms of raw sensor data, based on which a set of features are extracted. A one-class support vector machine is trained on a small set of positive data from one user, and tested on five installations. Experimental results show that our algorithm\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Stressful first impressions in job interviews\n", "abstract": " Stress can impact many aspects of our lives, such as the way we interact and work with others, or the first impressions that we make. In the past, stress has been most commonly assessed through self-reported questionnaires; however, advancements in wearable technology have enabled the measurement of physiological symptoms of stress in an unobtrusive manner. Using a dataset of job interviews, we investigate whether first impressions of stress (from annotations) are equivalent to physiological measurements of the electrodermal activity (EDA). We examine the use of automatically extracted nonverbal cues stemming from both the visual and audio modalities, as well EDA stress measurements for the inference of stress impressions obtained from manual annotations. Stress impressions were found to be significantly negatively correlated with hireability ratings ie individuals who were perceived to be more\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "A semi-automated system for accurate gaze coding in natural dyadic interactions\n", "abstract": " In this paper we propose a system capable of accurately coding gazing events in natural dyadic interactions. Contrary to previous works, our approach exploits the actual continuous gaze direction of a participant by leveraging on remote RGB-D sensors and a head pose-independent gaze estimation method. Our contributions are: i) we propose a system setup built from low-cost sensors and a technique to easily calibrate these sensors in a room with minimal assumptions; ii) we propose a method which, provided short manual annotations, can automatically detect gazing events in the rest of the sequence; iii) we demonstrate on substantially long, natural dyadic data that high accuracy can be obtained, showing the potential of our system. Our approach is non-invasive and does not require collaboration from the interactors. These characteristics are highly valuable in psychology and sociology research.", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Assessing the impact of language style on emergent leadership perception from ubiquitous audio\n", "abstract": " Leaders stand out for what they say and how they say it. This work describes the impact of the language style of emergent leaders in small group discussions based on 7 hours of audio from English spoken discussions recorded with a ubiquitous platform. For the language style analysis, word categories are extracted from manual transcriptions of the discussions as well as from automatically detected keywords. The most relevant word categories are then used to predict the emergent leader in each group. Our findings reveal that non-privacy sensitive word categories like amount of words, conjunctions and assent are good predictors of emergent leadership. The emergent leader can be correctly inferred in a fully automatic approach with up to 82% accuracy using categories derived from keywords, and up to 86% using categories derived from full manual transcriptions.", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Speaker localization for microphone array-based asr: the effects of accuracy on overlapping speech\n", "abstract": " Accurate speaker location is essential for optimal performance of distant speech acquisition systems using microphone array techniques. However, to the best of our knowledge, no comprehensive studies on the degradation of automatic speech recognition (ASR) as a function of speaker location accuracy in a multi-party scenario exist. In this paper, we describe a framework for evaluation of the effects of speaker location errors on a microphone array-based ASR system, in the context of meetings in multi-sensor rooms comprising multiple cameras and microphones. Speakers are manually annotated in videos in different camera views, and triangulation is used to determine an accurate speaker location. Errors in the speaker location are then induced in a systematic manner to observe their influence on speech recognition performance. The system is evaluated on real overlapping speech data collected with\u00a0\u2026", "num_citations": "14\n", "authors": ["1189"]}
{"title": "Modeling dyadic and group impressions with intermodal and interperson features\n", "abstract": " This article proposes a novel feature-extraction framework for inferring impression personality traits, emergent leadership skills, communicative competence, and hiring decisions. The proposed framework extracts multimodal features, describing each participant\u2019s nonverbal activities. It captures intermodal and interperson relationships in interactions and captures how the target interactor generates nonverbal behavior when other interactors also generate nonverbal behavior. The intermodal and interperson patterns are identified as frequent co-occurring events based on clustering from multimodal sequences. The proposed framework is applied to the SONVB corpus, which is an audiovisual dataset collected from dyadic job interviews, and the ELEA audiovisual data corpus, which is a dataset collected from group meetings. We evaluate the framework on a binary classification task involving 15 impression variables\u00a0\u2026", "num_citations": "13\n", "authors": ["1189"]}
{"title": "Development of the Geographical Proportional-to-size Street-Intercept Sampling (GPSIS) method for recruiting urban nightlife-goers in an entire city\n", "abstract": " We developed the Geographical Proportional-to-size Street-Intercept Sampling (GPSIS) method in order to obtain a sample of nightlife-goers which accounted for the diversity of spaces, patrons and locations within two Swiss cities. Popular nightlife zones were identified and quantified using social media data and local experts\u2019 knowledge. Young people were recruited in the streets on Friday and Saturday nights on three consecutive weekends using the \u2018fixed-line method, pro-rated for the zone\u2019s estimated popularity. Of the 3092 young adults approached, 896 agreed to pre-register. The importance of recruitment in multiple zones and over multiple weekend-days was evidenced by significant variations in participant demographics and registration rates between recruitment zones, times and weather conditions. To conclude, by combining a geographical approach with in situ recruitment, GPSIS has considerable\u00a0\u2026", "num_citations": "13\n", "authors": ["1189"]}
{"title": "From foursquare to my square: Learning check-in behavior from multiple sources\n", "abstract": " Location-based services often use only a single mobility data source, which typically will be scarce for any new user when the system starts out. We propose a transfer learning method to characterize the temporal distribution of places of individuals by using an external, additional, large-scale check-in data set such as Foursquare data. The method is applied to the next place prediction problem, and we show that the incorporation of additional data through the proposed method improves the prediction accuracy when there is a limited amount of prior data.", "num_citations": "13\n", "authors": ["1189"]}
{"title": "Analysis of group conversations: Modeling social verticality\n", "abstract": " This chapter presents computational methods for the analysis of social interaction. We focus on nonverbal behavior of social interaction, in particular social verticality, such as dominance, leadership, and roles. We describe processing, feature extraction, and inference methods that are widely used in the computational social interaction analysis literature. In the last section of the chapter, we present four case studies on dominance estimation, identifying emergent leadership, role recognition, and analysis of leadership styles.", "num_citations": "13\n", "authors": ["1189"]}
{"title": "InnerView: Learning place ambiance from social media images\n", "abstract": " In the recent past, there has been interest in characterizing the physical and social ambiance of urban spaces to understand how people perceive and form impressions of these environments based on physical and psychological constructs. Building on our earlier work on characterizing ambiance of indoor places, we present a methodology to automatically infer impressions of place ambiance, using generic deep learning features extracted from images publicly shared on Foursquare. We base our methodology on a corpus of 45,000 images from 300 popular places in six cities on Foursquare. Our results indicate the feasibility to automatically infer place ambiance with a maximum R 2 of 0.53 using features extracted from a pre-trained convolutional neural network. We found that features extracted from deep learning with convolutional nets consistently outperformed individual and combinations of several low-level\u00a0\u2026", "num_citations": "12\n", "authors": ["1189"]}
{"title": "Evaluating shape representations for Maya glyph classification\n", "abstract": " Shape representations are critical for visual analysis of cultural heritage materials. This article studies two types of shape representations in a bag-of-words-based pipeline to recognize Maya glyphs. The first is a knowledge-driven Histogram of Orientation Shape Context (HOOSC) representation, and the second is a data-driven representation obtained by applying an unsupervised Sparse Autoencoder (SA). In addition to the glyph data, the generalization ability of the descriptors is investigated on a larger-scale sketch dataset. The contributions of this article are four-fold: (1) the evaluation of the performance of a data-driven auto-encoder approach for shape representation; (2) a comparative study of hand-designed HOOSC and data-driven SA; (3) an experimental protocol to assess the effect of the different parameters of both representations; and (4) bridging humanities and computer vision/machine learning for\u00a0\u2026", "num_citations": "12\n", "authors": ["1189"]}
{"title": "Signal processing in the workplace [social sciences]\n", "abstract": " In this paper, a framework developed with collaborators in organizational psychology is described, aimed at inferring high-level constructs of interest in the workplace from nonverbal behavior. We summarize our experience tackling two tasks: identifying emergent leaders in small groups and assessing the hirability of candidates in employment interviews. The examples discussed in this paper have been recorded in a standard lab setting [9], in which sensors are fixed in a specific environment that volunteer participants have to visit, but also in moderately in-the-wild settings, where a portable sensing solution has been used to bring participants to quiet indoor environments for recordings [5], which gives flexibility for volunteer recruits.", "num_citations": "12\n", "authors": ["1189"]}
{"title": "Evaluating the robustness of privacy-sensitive audio features for speech detection in personal audio log scenarios\n", "abstract": " Personal audio logs are often recorded in multiple environments. This poses challenges for robust front-end processing, including speech/nonspeech detection (SND). Motivated by this, we investigate the robustness of four different privacy-sensitive features for SND, namely energy, zero crossing rate, spectral flatness, and kurtosis. We study early and late fusion of these features in conjunction with modeling temporal context. These combinations are evaluated in mismatched conditions on a dataset of nearly 450 hours. While both combinations yield improvements over individual features, generally feature combinations perform better. Comparisons with a state-of-the-art spectral based and a privacy-sensitive feature set are also provided.", "num_citations": "12\n", "authors": ["1189"]}
{"title": "Audio-visual processing in meetings: Seven questions and current AMI answers\n", "abstract": " The project Augmented Multi-party Interaction (AMI) is concerned with the development of meeting browsers and remote meeting assistants for instrumented meeting rooms \u2013 and the required component technologies R&D themes: group dynamics, audio, visual, and multimodal processing, content abstraction, and human-computer interaction. The audio-visual processing workpackage within AMI addresses the automatic recognition from audio, video, and combined audio-video streams, that have been recorded during meetings. In this article we describe the progress that has been made in the first two years of the project. We show how the large problem of audio-visual processing in meetings can be split into seven questions, like \u201cWho is acting during the meeting?\u201d. We then show which algorithms and methods have been developed and evaluated for the automatic answering of these questions.", "num_citations": "12\n", "authors": ["1189"]}
{"title": "Evaluating shape descriptors for detection of maya hieroglyphs\n", "abstract": " In this work we address the problem of detecting instances of complex shapes in binary images. We investigated the effects of combining DoG and Harris-Laplace interest points with SIFT and HOOSC descriptors. Also, we propose the use of a retrieval-based detection framework suitable to deal with images that are sparsely annotated, and where the objects of interest are very small in proportion to the total size of the image. Our initial results suggest that corner structures are suitable points to compute local descriptors for binary images, although there is the need for better methods to estimate their appropriate characteristic scale when used on binary images.", "num_citations": "11\n", "authors": ["1189"]}
{"title": "Privacy-sensitive recognition of group conversational context with sociometers\n", "abstract": " Recognizing the conversational context in which group interactions unfold has applications in machines that support collaborative work and perform automatic social inference using contextual knowledge. This paper addresses the task of discriminating one conversational context from another, specifically brainstorming from decision-making interactions, using easily computable nonverbal behavioral cues. Privacy-sensitive mobile sociometers are used to record the interaction data. We hypothesize that the difference in the conversational dynamics between brainstorming and decision-making discussions is significant and measurable using speaking activity-based nonverbal cues. We characterize the communication patterns of the entire group by the aggregation (both temporal and person-wise) of their nonverbal behavior. The results on our interaction data set show that the floor-occupation patterns in a\u00a0\u2026", "num_citations": "11\n", "authors": ["1189"]}
{"title": "Modelling interest in face-to-face conversations from multimodal nonverbal behaviour\n", "abstract": " Publisher SummaryThis chapter discusses a research, which involves both dyads and groups, and focuses on the multimodal cues and machine learning models that have been used for detection and recognition of interest and related concepts. The literature on computational modeling of interest in face-to-face conversations can be categorized according to different perspectives: interaction type, processing units, target tasks, and single versus multimodal cues. Most existing work on automatic interest modeling has focused on the relations between interest (or related concepts) and the speech modality, using both verbal and nonverbal cues. Wrede and Shriberg introduced the notion of hot spots in group meetings, defining it in terms of participants highly involved in a discussion and relating it to the concept of activation in emotion modeling. In subsequent work, the study was extended to analyze the relation\u00a0\u2026", "num_citations": "11\n", "authors": ["1189"]}
{"title": "Speaker change detection with privacy-preserving audio cues\n", "abstract": " In this paper we investigate a set of privacy-sensitive audio features for speaker change detection (SCD) in multiparty conversations. These features are based on three different principles: characterizing the excitation source information using linear prediction residual, characterizing subband spectral information shown to contain speaker information, and characterizing the general shape of the spectrum. Experiments show that the performance of the privacy-sensitive features is comparable or better than that of the state-of-the-art full-band spectral-based features, namely, mel frequency cepstral coefficients, which suggests that socially acceptable ways of recording conversations in real-life is feasible.", "num_citations": "11\n", "authors": ["1189"]}
{"title": "Modeling interactions from email communication\n", "abstract": " E-mail plays an important role as a medium for the spread of information, ideas, and influence among its users. We present a framework to learn topic-based interactions between pairs of E-mail users, i.e., the extent to which the E-mail topic dynamics of one user are likely to be affected by the others. The proposed framework is built on the influence model and the probabilistic latent semantic analysis (PLSA) language model. This paper makes two contributions. First, we model interactions between E-mail users using the semantic content of E-mail body, instead of E-mail header. Second, our framework models not only E-mail topic dynamics of individual E-mail users, but also the interactions within a group of individuals. Experiments on the Enron E-mail corpus show some interesting results that are potentially useful to discover the hierarchy of the Enron organization", "num_citations": "11\n", "authors": ["1189"]}
{"title": "Linking objects in videos by importance sampling\n", "abstract": " We present an approach to create hyper-links between video segments that contain objects of interest, based on video structuring, object definition, and stochastic object localization in the video structure. Localization is formulated in the metric mixture model framework, which allows for the joint probabilistic modeling of a (user-defined) set of color appearance exemplars and their geometric transformations. Candidate object configurations are drawn from a prior distribution using importance sampling - which guides the search towards regions of the configuration space likely to contain the correct object configuration, thus avoiding exhaustive processing - and evaluated using Bayes' rule. Results of linking real objects (with changes of size and pose) in several home videos illustrate the performance of the method.", "num_citations": "11\n", "authors": ["1189"]}
{"title": "How to tell ancient signs apart? recognizing and visualizing maya glyphs with cnns\n", "abstract": " Thanks to the digital preservation of cultural heritage materials, multimedia tools (e.g., based on automatic visual processing) considerably ease the work of scholars in the humanities and help them to perform quantitative analysis of their data. In this context, this article assesses three different Convolutional Neural Network (CNN) architectures along with three learning approaches to train them for hieroglyph classification, which is a very challenging task due to the limited availability of segmented ancient Maya glyphs. More precisely, the first approach, the baseline, relies on pretrained networks as feature extractor. The second one investigates a transfer learning method by fine-tuning a pretrained network for our glyph classification task. The third approach considers directly training networks from scratch with our glyph data. The merits of three different network architectures are compared: a generic sequential\u00a0\u2026", "num_citations": "10\n", "authors": ["1189"]}
{"title": "Mining crowdsourced first impressions in online social video\n", "abstract": " While multimedia and social computing research have used crowdsourcing techniques to annotate objects, actions, and scenes in social video sites like YouTube, little work has addressed the crowdsourcing of personal and social traits in online social video or social media content in general. In this paper, we address the problems of (1) crowdsourcing the annotation of first impressions of video bloggers (vloggers) personal and social traits in conversational YouTube videos, and (2) mining the impressions with the goal of modeling the interplay of different vlogger facets. First, we design a human annotation task to crowdsource impressions of vloggers that extends a tradition of studies of personality impressions with the addition of attractiveness and mood impressions. Second, we propose a probabilistic framework using Topic Models to discover prototypical impressions that are data driven, and that combine\u00a0\u2026", "num_citations": "10\n", "authors": ["1189"]}
{"title": "Exploiting observers' judgements for nonverbal group interaction analysis\n", "abstract": " Incorporating annotators' knowledge into a machine-learning framework for detecting psychological traits using multimodal data is an open issue in human communication and social computing. We present a model that is designed to exploit the subjective judgements of multiple annotators on a social trait labeling task. Our two-stage model first estimates a ground truth by modeling the annotators using both the annotations and annotators' self-reported confidences. In the second stage, we train a classifier using the estimated ground truth as labels. We also define ways to verify the consistency of our model and validate it using annotations and nonverbal cues for a dominance estimation task in a group interaction scenario on the publicly available DOME corpus, in addition to synthetically generated data. Our models give satisfactory results, outperforming the commonly used majority voting as well as other\u00a0\u2026", "num_citations": "10\n", "authors": ["1189"]}
{"title": "Contextual classification of image patches with latent aspect models\n", "abstract": " We present a novel approach for contextual classification of image patches in complex visual scenes, based on the use of histograms of quantized features and probabilistic aspect models. Our approach uses context in two ways: (1) by using the fact that specific learned aspects correlate with the semantic classes, which resolves some cases of visual polysemy often present in patch-based representations, and (2) by formalizing the notion that scene context is image-specific\u2014what an individual patch represents depends on what the rest of the patches in the same image are. We demonstrate the validity of our approach on a man-made versus natural patch classification problem. Experiments on an image collection of complex scenes show that the proposed approach improves region discrimination, producing satisfactory results and outperforming two noncontextual methods. Furthermore, we also show\u00a0\u2026", "num_citations": "10\n", "authors": ["1189"]}
{"title": "Predicting remote versus collocated group interactions using nonverbal cues\n", "abstract": " This paper addresses two problems: Firstly, the problem of classifying remote and collocated small-group working meetings, and secondly, the problem of identifying the remote participant, using in both cases nonverbal behavioral cues. Such classifiers can be used to improve the design of remote collaboration technologies to make remote interactions as effective as possible to collocated interactions. We hypothesize that the difference in the dynamics between collocated and remote meetings is significant and measurable using speech activity based nonverbal cues. Our results on a publicly available dataset-the Augmented Multi-Party Interaction with Distance Access (AMIDA) corpus-show that such an approach is promising, although more controlled settings and more data are needed to explore the addressed problems further.", "num_citations": "10\n", "authors": ["1189"]}
{"title": "Tracking attention for multiple people: Wandering visual focus of attention estimation\n", "abstract": " The problem of finding the visual focus of attention of multiple people free to move in an unconstrained manner is defined here as the wandering visual focus of attention (WVFOA) problem. Estimating the WVFOA for multiple unconstrained people is a new and important problem with implications for human behavior understanding and cognitive science, as well as real-world applications. One such application, which we present in this article, monitors the attention passers-by pay to an outdoor advertisement. In our approach to the WVFOA problem, we propose a multi-person tracking solution based on a hybrid Dynamic Bayesian Network that simultaneously infers the number of people in a scene, their body locations, their head locations, and their head pose. It is defined in a joint state-space formulation that allows for the modeling of interactions between people. For inference in the resulting high-dimensional state-space, we propose a trans-dimensional Markov Chain Monte Carlo (MCMC) sampling scheme, which not only handles a varying number of people, but also efficiently searches the state-space by allowing person-part state updates. Our model was rigorously evaluated for tracking quality and ability to recognize people looking at an outdoor advertisement, and the results indicate good performance for these tasks.", "num_citations": "10\n", "authors": ["1189"]}
{"title": "The emotional entanglements of smartphones in the field: On emotional discomfort, power relations, and research ethics\n", "abstract": " Despite human geographers\u2019 growing recognition of the need to explore how digital technologies are increasingly co\u2010producing geographies, the methodological implications of such forms of data production are rarely discussed. This paper explores how smartphones co\u2010constitute fieldwork when they are used as research instruments. Drawing from a research project on young people's nightlife in Switzerland, we use Ahmed's ideas of emotions to show how smartphones are not inert research tools but emotionally entangled in the field. We argue that doing research with smartphones visibly in fieldwork has an effect on the relationships between the people, practices, and places of the field site. More specifically, we propose that these effects of emotions call for a renewed scrutiny of research ethics, particularly as smartphones increasingly become part of research designs.", "num_citations": "9\n", "authors": ["1189"]}
{"title": "How may I help you? behavior and impressions in hospitality service encounters\n", "abstract": " In the service industry, customers often assess quality of service based on the behavior, perceived personality, and other attributes of the front line service employees they interact with. Interpersonal communication during these interactions is key to determine customer satisfaction and perceived service quality. We present a computational framework to automatically infer perceived performance and skill variables of employees interacting with customers in a hotel reception desk setting using nonverbal behavior, studying a dataset of 169 dyadic interactions involving students from a hospitality management school. We also study the connections between impressions of Big-5 personality traits, attractiveness, and performance of receptionists. In regression tasks, our automatic framework achieves R 2= 0.30 for performance impressions using audio-visual nonverbal cues, compared to 0.35 using personality\u00a0\u2026", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Check out this place: Inferring ambiance from airbnb photos\n", "abstract": " Airbnb is changing the landscape of the hospitality industry, and to this day, little is known about the inferences that guests make about Airbnb listings. Our work constitutes a first attempt at understanding how potential Airbnb guests form first impressions from images, one of the main modalities featured on the platform. We contribute to the multimedia community by proposing the novel task of automatically predicting human impressions of ambiance from pictures of listings on Airbnb. We collected Airbnb images, focusing on the countries Switzerland and Mexico as case studies, and used crowdsourcing mechanisms to gather annotations on physical and ambiance attributes, finding that agreement among raters was high for most of the attributes. Our cluster analysis showed that both physical and psychological attributes could be grouped into three clusters. We then extracted state-of-the-art features from the images\u00a0\u2026", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Multimedia analysis and access of ancient maya epigraphy: Tools to support scholars on Maya hieroglyphics\n", "abstract": " This article presents an integrated framework for multimedia access and analysis of ancient Maya epigraphic resources, which is developed as an interdisciplinary effort involving epigraphers (someone who deciphers ancient inscriptions) and computer scientists. Our work includes several contributions: a definition of consistent conventions to generate high-quality representations of Maya hieroglyphs from the three most valuable ancient codices, which currently reside in European museums and institutions; a digital repository system for glyph annotation and management; as well as automatic glyph retrieval and classification methods. We study the combination of statistical Maya language models and shape representation within a hieroglyph retrieval system, the impact of applying language models extracted from different hieroglyphic resources on various data types, and the effect of shape representation choices\u00a0\u2026", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Is that a jaguar? Segmenting ancient maya glyphs via crowdsourcing\n", "abstract": " Crowdsourcing is popular in multimedia research to obtain image annotation and segmentation data at scale. In the context of analysis of cultural heritage materials, we propose a novel crowdsourced task, namely the segmentation of ancient Maya hieroglyph-blocks by non-experts. This is a task that is highly perceptual and thus potentially feasible even though the crowd is not likely to have prior specialized knowledge about hieroglyphics. Based on a new data set of glyph-block line drawings for which ground-truth segmentation exists, we study how non-experts perceive glyph blocks (eg whether they see closed contours as a separate glyph, or how they combine visual components under plausible hypotheses of the number of glyphs present in a block.) Using Amazon Mechanical Turk as platform, we perform block-based and worker-based objective analyses to assess the difficulty of glyph blocks and the\u00a0\u2026", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Nonverbal behavior analysis\n", "abstract": " The last decade marked the emergence of the automated understanding of face-to-face social interaction as a research problem in computing. IM2 was originally focused on meetings (a quintessential form of interaction), and so over the years a body of work directed towards analyzing and inferring a variety of behaviors and interactions resulted from the project. One key aspect of the IM2 work has been the use of nonverbal communication as measurable evidence of social phenomena. The role of nonverbal behavioral cues (gaze, facial expressions, gestures, vocalizations, postures, etc.) as carriers of socially relevant information has been the subject of research in psychology and communication for decades. Furthermore, computing research has developed a large number of approaches aimed at automatic analysis and synthesis of gaze, facial expressions, gestures, and paralanguage. Over 12 years, IM2 enabled both the development of perceptual technologies (computer vision and signal processing) to extract behavioral cues, and their integration to address questions connected to inference of various social variables in increasingly diverse situations (Gatica-Perez, 2009, Vinciarelli et al., 2009b). Table 12.1 shows a timeline of some of the investigated research lines. The initial research that linked audio-visual perception and social behavior in IM2 can be traced back to 2002 with the initial use of the Smart Meeting Room as a sensing platform to study small group interaction (refer to Chap. 1). The original contribution was the dual realization that groups could be studied as units (as opposed to considering individuals as the ultimate analysis\u00a0\u2026", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Speaking swiss: languages and venues in foursquare\n", "abstract": " Due to increasing globalization, urban societies are becoming more multicultural. The availability of large-scale digital mobility traces eg from tweets or checkins provides an opportunity to explore multiculturalism that until recently could only be addressed using survey-based methods. In this paper we examine a basic facet of multiculturalism through the lens of language use across multiple cities in Switzerland. Using data obtained from Foursquare over 330 days, we present a descriptive analysis of linguistic differences and similarities across five urban agglomerations in a multicultural, western European country.", "num_citations": "9\n", "authors": ["1189"]}
{"title": "Vlogging over time: Longitudinal impressions and behavior in youtube\n", "abstract": " YouTube vlogging, as a popular genre of ubiquitous social video, engages people in entertainment, civic, and social activities. Although several aspects of vlogging have been studied in media studies and multimedia analysis, the longitudinal angle of vlogging regarding recognition of personal state and trait impressions from behavior has not been yet analyzed. We present a study using behavioral data of vloggers who posted vlogs on YouTube for a period between three and six years. We use online crowdsourcing to collect a rich set of 21 impression variables for each video, including perceived personality, mood, skills, and expertise. Acoustic and motion features are extracted to characterize basic nonverbal behavior. The analysis shows that only a couple of perceived variables, including perceived expertise and perceived quality of audio and video, display weak temporal patterns. Furthermore, we show that\u00a0\u2026", "num_citations": "8\n", "authors": ["1189"]}
{"title": "25 Analysis of Small Groups\n", "abstract": " Teams are key components of organizations and, although complexity and scale are typical features of large institutions worldwide, much of the work is still implemented by small groups. The small-group meeting, where people discuss around the table, is pervasive and quintessential of collaborative work. For many years now, this setting has been studied in computing with the goal of developing methods that automatically analyze the interaction using both the spoken words and the nonverbal channels as information sources. The current literature offers the possibility of inferring key aspects of the interaction, ranging from personal traits to hierarchies and other relational constructs, which in turn can be used for a number of applications. Overall, this domain is rapidly evolving and studied in multiple subdisciplines in computing and engineering as well as the cognitive sciences.We present a concise review of recent literature on computational analysis of face-toface small-group interaction. Our goal is to provide the reader with a quick pointer to work on analysis of conversational dynamics, verticality in groups, personality of group members, and characterization of groups as a whole, with a focus on nonverbal behavior as information source. The value of the nonverbal channel (including voice, face, and body) to infer high-level information about individuals has been documented at length in psychology and communication (Knapp & Hall, 2009) and is one of the main themes of this volume.", "num_citations": "8\n", "authors": ["1189"]}
{"title": "Assessing sparse coding methods for contextual shape indexing of Maya hieroglyphs\n", "abstract": " Bag-of-visual-words or bag-of-visterms (bov) is a common technique used to index Multimedia information with the purposes of retrieval and classification. In this work we address the problem of constructing efficient bov representations of complex shapes as are the Maya syllabic hieroglyphs. Based on retrieval experiments, we assess and evaluate the performance of several variants of the recent sparse coding method KSVD, and compare it with the traditional k-means clustering algorithm. We investigate the effects of a thresholding procedure used to facilitate the sparse decomposition of signals that are potentially sparse, and we also assess the performance of different pooling techniques to construct bov representations. Although the bov\u2019s computed via Sparse Coding do not outperform the retrieval precision of those computed by k-means, they achieve competitive results after an adequate enforcement of the sparsity, which leads to more discriminative bag representations with respect to using the original non-sparse descriptors. Also, we propose a simplified formulation of the HOOSC descriptor that improves the retrieval performance.", "num_citations": "8\n", "authors": ["1189"]}
{"title": "Call me Guru: user categories and large-scale behavior in YouTube\n", "abstract": " While existing studies on YouTube\u2019s massive user-generated video content have mostly focused on the analysis of videos, their characteristics, and network properties, little attention has been paid to the analysis of users\u2019 long-term behavior as it relates to the roles they self-define and (explicitly or not) play in the site. In this chapter, we present a statistical analysis of aggregated user behavior in YouTube from the perspective of user categories, a feature that allows people to ascribe to popular roles and to potentially reach certain communities. Using a sample of 270,000 users, we found that a high level of interaction and participation is concentrated on a relatively small, yet significant, group of users, following recognizable patterns of personal and social involvement. Based on our analysis, we also show that by using simple behavioral features from user profiles, people can be automatically classified\u00a0\u2026", "num_citations": "8\n", "authors": ["1189"]}
{"title": "Extracting information from multimedia meeting collections\n", "abstract": " Multimedia meeting collections, composed of unedited audio and video streams, handwritten notes, slides, and electronic documents that jointly constitute a raw record of complex human interaction processes in the workplace, have attracted interest due to the increasing feasibility of recording them in large quantities, by the opportunities for information access and retrieval applications derived from the automatic extraction of relevant meeting information, and by the challenges that the extraction of semantic information from real human activities entails. In this paper, we present a succint overview of recent approaches in this field, largely influenced by our own experiences. We first review some of the existing and potential needs for users of multimedia meeting information systems. We then summarize recent work on various research areas addressing some of these requirements. In more detail, we describe our\u00a0\u2026", "num_citations": "8\n", "authors": ["1189"]}
{"title": "Locally private graph neural networks\n", "abstract": " Graph Neural Networks (GNNs) have demonstrated superior performance in learning node representations for various graph inference tasks. However, learning over graph data can raise privacy concerns when nodes represent people or human-related variables that involve sensitive or personal information. In this paper, we study the problem of node data privacy, where graph nodes (eg, social network users) have potentially sensitive data that is kept private, but they could be beneficial for a central server for training a GNN over the graph. To address this problem, we propose a privacy-preserving, architecture-agnostic GNN learning framework with formal privacy guarantees based on Local Differential Privacy (LDP). Specifically, we develop a locally private mechanism to perturb and compress node features, which the server can efficiently collect to approximate the GNN's neighborhood aggregation step\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Facing Employers and Customers: What Do Gaze and Expressions Tell About Soft Skills?\n", "abstract": " Eye gaze and facial expressions are central to face-to-face social interactions. These behavioral cues and their connections to first impressions have been widely studied in psychology and computing literature, but limited to a single situation. Utilizing ubiquitous multimodal sensors coupled with advances in computer vision and machine learning, we investigate the connections between these behavioral cues and perceived soft skills in two diverse workplace situations (job interviews and reception desk). Pearson's correlation analysis shows a moderate connection between certain facial expressions, eye gaze cues and perceived soft skills in job interviews (r \u03f5 [-30, 30]) and desk (r \u03f5 [20, 36]) situations. Results of our computational framework to infer perceived soft skills indicates a low predictive power of eye gaze, facial expressions, and their combination in both interviews (R2 \u03f5 [0.02, 0.21]) and desk (R2 \u03f5 [0.05\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Ambiance in social media venues: visual cue interpretation by machines and crowds\n", "abstract": " We study the perception of ambiance of places captured in social media images by both machines and crowdworkers. This task is challenging due to the subjective nature of the ambiance construct as well as the large variety in layout, style, and visual characteristics of venues. For machine recognition of ambiance, we use Residual Deep Convolutional Neural Networks (ResNets), followed by gradient-weighted class activation mapping (Grad-CAM) visualizations. This form of visual explanation obtained from the trained ResNet-50 models were assessed by crowdworkers based on a carefully designed crowdsourcing task, in which both visual ambiance cues of venues and subjective assessment of Grad-CAM results were collected and analyzed. The results show that paintings, photos, and decorative items are strong cues for artsy ambiance, whereas type of utensils, type of lamps and presence of flowers may indicate formal ambiance. Layout and design-related cues such as type of chairs, type of tables/tablecloth and type of windows are noted to have impact for both ambiances. Overall, the ambiance visual cue recognition results are promising, and the crowd-based assessment approach may motivate other studies on subjective perception of place attributes.", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Insiders and outsiders: Comparing urban impressions between population groups\n", "abstract": " There is a growing interest in social and urban computing to employ crowdsourcing as means to gather impressions of urban perception for indoor and outdoor environments. Previous studies have established that reliable estimates of urban perception can be obtained using online crowdsourcing systems, but implicitly assumed that the judgments provided by the crowd are not dependent on the background knowledge of the observer. In this paper, we investigate how the impressions of outdoor urban spaces judged by online crowd annotators, compare with the impressions elicited by the local inhabitants, along six physical and psychological labels. We focus our study in a developing city where understanding and characterization of these socio-urban perceptions is of societal importance. We found statistically significant differences between the two population groups. Locals perceived places to be more\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Kodak moments and flickr diamonds: how users shape large-scale media\n", "abstract": " In today's age of digital multimedia deluge, a clear understanding of the dynamics of online communities is capital. Users have abandoned their role of passive consumers and are now the driving force behind large-scale media repositories, whose dynamics and shaping factors are not yet fully understood. In this paper we present a novel human-centered analysis of two major photo sharing websites, Flickr and Kodak Gallery. On a combined dataset of over 5 million tagged photos, we investigate fundamental differences and similarities at the level of tag usage and propose a joint probabilistic topic model to provide further insight into semantic differences between the two communities. Our results show that the effects of the users' motivations and needs can be strongly observed in this large-scale data, in the form of what we call Kodak Moments and Flickr Diamonds. They are an indication that system designers\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Predicting the dominant clique in meetings through fusion of nonverbal cues\n", "abstract": " This paper addresses the problem of automatically predicting the dominant clique (ie, the set of K-dominant people) in face-to-face small group meetings recorded by multiple audio and video sensors. For this goal, we present a framework that integrates automatically extracted nonverbal cues and dominance prediction models. Easily computable audio and visual activity cues are automatically extracted from cameras and microphones. Such nonverbal cues, correlated to human display and perception of dominance, are well documented in the social psychology literature. The effectiveness of the cues were systematically investigated as single cues as well as in unimodal and multimodal combinations using unsupervised and supervised learning approaches for dominant clique estimation. Our framework was evaluated on a five-hour public corpus of teamwork meetings with third-party manual annotation of\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Assessing scene structuring in consumer videos\n", "abstract": " Scene structuring is a video analysis task for which no common evaluation procedures have been fully adopted. In this paper, we present a methodology to evaluate such task in home videos, which takes into account human judgement, and includes a representative corpus, a set of objective performance measures, and an evaluation protocol. The components of our approach are detailed as follows. First, we describe the generation of a set of home video scene structures produced by multiple people. Second, we define similarity measures that model variations with respect to two factors: human perceptual organization and level of structure granularity. Third, we describe a protocol for evaluation of automatic algorithms based on their comparison to human performance. We illustrate our methodology by assessing the performance of two recently proposed methods: probabilistic hierarchical clustering and\u00a0\u2026", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Compact representation of planar curves based on a wavelet shape descriptor for multimedia applications\n", "abstract": " We present an analysis of a 2-D planar shape descriptor based on the Discrete Wavelet Transform (DWT), as an approach for compact object representation in multimedia applications. To generate the wavelet shape descriptor (WD), a partial 1-D DWT is applied on the vertical and horizontal components of a parameterized closed curve that represents the contour of an object of interest. With this shape representation tool and based on a multiresolution analysis, the curve can be reconstructed on a scale-by-scale basis to the desired degree of approximation. We show that contours of typical multimedia video objects are well represented with a small number of the largest magnitude wavelet coefficients, allowing for an efficient object shape representation. Some issues on the selection of the coefficients are studied. A comparison between the performance of the WD using different wavelet filters and a Fourier shape descriptor is presented, for synthetic and natural contours. The potential use of this descriptor in a multimedia application framework is discussed.", "num_citations": "7\n", "authors": ["1189"]}
{"title": "Alone or with others? understanding eating episodes of college students with mobile sensing\n", "abstract": " Understanding food consumption patterns and contexts using mobile sensing is fundamental to build mobile health applications that require minimal user interaction to generate mobile food diaries. Many available mobile food diaries, both commercial and in research, heavily rely on self-reports, and this dependency limits the long term adoption of these apps by people. The social context of eating (alone, with friends, with family, with a partner, etc.) is an important self-reported feature that influences aspects such as food type, psychological state while eating, and the amount of food, according to prior research in nutrition and behavioral sciences. In this work, we use two datasets regarding the everyday eating behavior of college students in two countries, namely Switzerland (Nch= 122) and Mexico (Nmx= 84), to examine the relation between the social context of eating and passive sensing data from wearables\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Mi Casa es su Casa? Examining Airbnb hospitality exchange practices in a developing economy\n", "abstract": " We present a study involving twenty in-depth, semi-structured interviews, a street survey, and online data to understand Airbnb hospitality exchange practices in the context of a developing country. As case studies, we investigate Airbnb practices of both hosts and guests in two tourist venues in Mexico -- the eighth most visited country worldwide. The analysis of the data revealed that Airbnb practices in Mexico have some similarities but also important differences with those previously reported in the literature. We found (1) that money is the main motivation for hosts to participate in Airbnb and that the earned money contributes significantly to the overall income of hosts; (2) that traditions that permeate the Mexican culture motivate hosts to engage in more personal hospitality experiences; (3) that Airbnb host practices lead to the creation of informal jobs that support the local community; and (4) that Airbnb local guests\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Words Worth: Verbal Content and Hirability Impressions in YouTube Video Resumes\n", "abstract": " Automatic hirability prediction from video resumes is gaining increasing attention in both psychology and computing. Most existing works have investigated hirability from the perspective of nonverbal behavior, with verbal content receiving little interest. In this study, we leverage the advances in deep-learning based text representation techniques (like word embedding) in natural language processing to investigate the relationship between verbal content and perceived hirability ratings. To this end, we use 292 conversational video resumes from YouTube, develop a computational framework to automatically extract various representations of verbal content, and evaluate them in a regression task. We obtain a best performance of R\u00b2= 0.23 using GloVe, and R\u00b2= 0.22 using Word2Vec representations for manual and automatically transcribed texts respectively. Our inference results indicate the feasibility of using deep learning based verbal content representation in inferring hirability scores from online conversational video resumes.", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Examining linguistic content and skill impression structure for job interview analytics in hospitality\n", "abstract": " First impressions are critical to professional interactions especially in the context of employment interviews. This work investigates connections between linguistic content and first impressions in job interviews and the structure of ten soft skills and overall impressions. Towards this, we transcribe 169 role-played job interviews conducted at a hospitality school and analyze the linguistic content using off-the-shelf software. To understand the structure of the soft skill impressions, we conduct a principal component analysis. We then develop methods to automatically infer impressions using verbal and nonverbal features and their combination. Results indicate low predictive power of verbal cues for overall impression (R 2= 0.11). Combined verbal and nonverbal cues explain up to 34% of variance, a marginal improvement over R 2= 0.32 using only nonverbal cues. The use of principal components reveals a major\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Venues in social media: examining ambiance perception through scene semantics\n", "abstract": " We address the question of what visual cues, including scene objects and demographic attributes, contribute to the automatic inference of perceived ambiance in social media venues. We first use a state-of-art, deep scene semantic parsing method and a face attribute extractor to understand how different cues present in a scene relate to human perception of ambiance on Foursquare images of social venues. We then analyze correlational links between visual cues and thirteen ambiance variables, as well as the ability of the semantic attributes to automatically infer place ambiance. We study the effect of the type and amount of image data used for learning, and compare regression results to previous work, showing that the proposed approach results in marginal-to-moderate performance increase for up to ten of the ambiance dimensions, depending on the corpus.", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Maya codical glyph segmentation: A crowdsourcing approach\n", "abstract": " This paper focuses on the crowd-annotation of an ancient Maya glyph dataset derived from the three ancient codices that survived up to date. More precisely, nonexpert annotators are asked to segment glyph-blocks into their constituent glyph entities. As a means of supervision, available glyph variants are provided to the annotators during the crowdsourcing task. Compared to object recognition in natural images or handwriting transcription tasks, designing an engaging task and dealing with crowd behavior is challenging in our case. This challenge originates from the inherent complexity of Maya writing and an incomplete understanding of the signs and semantics in the existing catalogs. We elaborate on the evolution of the crowdsourcing task design, and discuss the choices for providing supervision during the task. We analyze the distributions of similarity and task difficulty scores, and the segmentation\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Dites-moi: wearable feedback on conversational behavior\n", "abstract": " Interpersonal communication skills are critical in certain industry sectors like sales and marketing. Recent advances in wearable technology are enabling the design of real-time behavioral feedback tools for apprentices in aforementioned industries. This paper describes the design and implementation of a conversational behavior awareness tool based on Google Glass. The goal of the system is to provide real-time feedback to young sales apprentices about the amount of time they talk in an interaction with a client. We evaluated our system with a pilot study involving 15 apprentices (ages 16--20). Overall, participants found the system fun, little distracting and useful. Furthermore, manual coding of the recorded videos, showed that wearable sensing and real-time feedback did not negatively influence the dyadic social interaction.", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Happy and agreeable? Multi-label classification of impressions in social video\n", "abstract": " The mobile and ubiquitous nature of conversational social video has placed video blogs among the most popular forms of online video. For this reason, there has been an increasing interest in conducting studies of human behavior from video blogs in affective and social computing. In this context, we consider the problem of mood and personality trait impression inference using verbal and nonverbal audio-visual features. Under a multi-label classification framework, we show that for both mood and personality trait binary label sets, not only the simultaneous inference of multiple labels is feasible, but also that classification accuracy increases moderately for several labels, compared to a single-label approach. The multi-label method we consider naturally exploits label correlations, which motivate our approach, and our results are consistent with models proposed in psychology to define human emotional states and\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "The MAAYA project: Multimedia analysis and access for documentation and decipherment of Maya epigraphy\n", "abstract": " Archaeology and epigraphy have made significant progress to decipher the hieroglyphic writings of the Ancient Maya, which today can be found spread over space (in sites in Mexico and Central America and museums in the US and Europe) and media types (in stone, ceramics, and codices.) While the deciphering goal remains unfinished, technological advances in automatic analysis of digital images and large-scale information management systems are enabling the possibility to analyze, organize, and visualize hieroglyphic data that can ultimately support and accelerate the deciphering challenge.We present an overview of the MAAYA project (http://www. idiap. ch/project/maaya/), an interdisciplinary effort integrating the work of epigraphists and computer scientists with three goals:(1) Design and development of computational tools for visual analysis and information management that effectively support the work of Maya hieroglyphic scholars;(2) Advancement of the state of Maya epigraphy through the coupling of expert knowledge and the use of these tools; and", "num_citations": "6\n", "authors": ["1189"]}
{"title": "People-centric mobile sensing with a pragmatic twist: From behavioral data points to active user involvement\n", "abstract": " Mobile phones have recently been used to collect largescale continuous data about human behavior. This people centric sensing paradigm is useful not only from a scientific point of view: Contextual user data has pragmatic value, too. Individuals whose data is collected in such long-term people centric sensing projects can be engaged in user centric design activities aiming to generate data driven services that benefit the end user. This paper demonstrates the value of such user centric approach. In a two-stage approach, we analyse mobile phone data to extract mobile phone usage c ategories. We then go on to interview the participants concerning their perceptions toward context-aware services. The two sta ges, combined as we present here, offer a clear value in ter ms of provi ding complementary insights, both to researchers and users, about the feasibility of and the expectations about personalized mobile\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Tagging and retrieving images with co-occurrence models: from corel to flickr\n", "abstract": " This paper presents two models for content-based automatic image annotation and retrieval in web image repositories, based on the co-occurrence of tags and visual features in the images. In particular, we show how additional measures can be taken to address the noisy and limited tagging problems, in datasets such as Flickr, to improve performance. An image is represented as a bag of visual terms computed using edge and color information. The first model begins with a naive Bayes approach and then improves upon it by using image pairs as single documents to significantly reduce the noise and increase annotation performance. The second method models the visual features and tags as a graph, and uses query expansion techniques to improve the retrieval performance. We evaluate our methods on the commonly used 150 concept Corel dataset, and a much harder 2000 concept Flickr dataset.", "num_citations": "6\n", "authors": ["1189"]}
{"title": "On automatic annotation of meeting databases\n", "abstract": " In this paper, we discuss meetings as an application domain for multimedia content analysis. Meeting databases are a rich data source suitable for a variety of audio, visual and multi-modal tasks, including speech recognition, people and action recognition, and information retrieval. We specifically focus on the task of semantic annotation of audio-visual (AV) events, where annotation consists of assigning labels (event names) to the data. In order to develop an automatic annotation system in a principled manner, it is essential to have a well-defined task, a standard corpus and an objective performance measure. In this work we address each of these issues to automatically annotate events based on participant interactions.", "num_citations": "6\n", "authors": ["1189"]}
{"title": "Segmentation algorithm for image sequences from a pel-recursive motion field\n", "abstract": " We present an algorithm to segment image sequences form motion information. A dense vector filed estimated by a Wiener-based pel-recursive method represents the key to separate a viewed scene into regions with different apparent displacement, according to a four-parameter motion model. A preprocessing stage using mathematical morphology enhances pel-recursive motion estimation. The proposed segmentation model, based on Markov Random Fields theory , considers-- besides the motion field--other information sources that help describe the problem more accurately. The maximum a posteriori criterion is used for the optimization of the solution, and performed with a deterministic approach. The complete segmentation algorithm includes initializing, region numbering and labeling, parameter estimation of the motion model in each region, and optimization of the segmentation field. Results on synthetic\u00a0\u2026", "num_citations": "6\n", "authors": ["1189"]}
{"title": "When Differential Privacy Meets Graph Neural Networks\n", "abstract": " Graph Neural Networks have demonstrated superior performance in learning graph representations for several subsequent downstream inference tasks. However, learning over graph data types can raise privacy concerns when nodes represent people or human-related variables that involve personal information about individuals. Previous works have presented various techniques for privacy-preserving deep learning over non-relational data, such as image, audio, video, and text, but there is less work addressing the privacy issues involved in applying deep learning algorithms on graphs. As a result and for the first time, in this paper, we develop a privacy-preserving learning algorithm with formal privacy guarantees for Graph Convolutional Networks (GCNs) based on Local Differential Privacy (LDP) to tackle the problem of node-level privacy, where graph nodes have potentially sensitive features that need to be\u00a0\u2026", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Drinks & crowds: Characterizing alcohol consumption through crowdsensing and social media\n", "abstract": " The design of computational methods to recognize alcohol intake is a relevant problem in ubiquitous computing. While mobile crowdsensing and social media analytics are two current approaches to characterize alcohol consumption in everyday life, the question of how they can be integrated, to examine their relative value as informative of the drinking phenomenon and to exploit their complementarity towards the classification of drinking-related attributes, remains as an open issue. In this paper, we present a comparative study based on five years of Instagram data about alcohol consumption and a 200+ person crowdsensing campaign collected in the same country (Switzerland). Our contributions are two-fold. First, we conduct data analyses that uncover temporal, spatial, and social contextual patterns of alcohol consumption on weekend nights as represented by both crowdsensing and social media. This\u00a0\u2026", "num_citations": "5\n", "authors": ["1189"]}
{"title": "# drink or# drunk: Multimodal signals and drinking practices on instagram\n", "abstract": " The understanding of alcohol consumption patterns, especially those indicating negative drinking behavior, is an important issue to researchers and health policymakers. On social media, people share daily activities, including alcohol consumption, representing these moments through images and text. This work, using a five-year dataset from Instagram, analyzes what machine-extracted textual and visual cues reveal about trends of casual drinking (concepts gathered around# drink) and possible negative drinking (concepts gathered around# drunk). Our analysis reveals that# drunk posts occur more frequently in party occasions and nightlife locations, with a higher presence of people, while# drink posts occur at food locations, with a higher presence of drink containers. Manual coding further shows that# drunk posts have a higher chance of being perceived as potentially objectionable. A random forest classifier\u00a0\u2026", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Extracting Maya Glyphs from degraded ancient documents via image segmentation\n", "abstract": " We present a system for automatically extracting hieroglyph strokes from images of degraded ancient Maya codices. Our system adopts a region-based image segmentation framework. Multi-resolution super-pixels are first extracted to represent each image. A Support Vector Machine (SVM) classifier is used to label each super-pixel region with a probability to belong to foreground glyph strokes. Pixelwise probability maps from multiple super-pixel resolution scales are then aggregated to cope with various stroke widths and background noise. A fully connected Conditional Random Field model is then applied to improve the labeling consistency. Segmentation results show that our system preserves delicate local details of the historic Maya glyphs with various stroke widths and also reduces background noise. As an application, we conduct retrieval experiments using the extracted binary images. Experimental results\u00a0\u2026", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Automatic maya hieroglyph retrieval using shape and context information\n", "abstract": " We propose an automatic Maya hieroglyph retrieval method integrating shape and glyph context information. Two recent local shape descriptors, Gradient Field Histogram of Orientation Gradient (GF-HOG) and Histogram of Orientation Shape Context (HOOSC), are evaluated. To encode the context information, we propose to convert each Maya glyph block into a first-order Markov chain and apply the co-occurrence of neighbouring glyphs. The retrieval results obtained based on visual matching are therefore re-ranked. Experimental results show that our method can significantly improve the glyph retrieval accuracy even with a basic co-occurrence model. Furthermore, two unique glyph datasets are contributed which can be used as novel shape benchmarks in future research.", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Revisiting the generality of the rank-based human mobility model\n", "abstract": " Location-based social networks, in addition to revealing users' online social network, also informs users' actual movements in the offline physical world. Due to this, they have recently been used in large-scale mobility and urban studies. In this paper, using a rigorous statistical methodology, we have found that a rank-distance distribution, which in recent research has been suggested to be a universal mobility law across cultural, demographic and national boundaries, does not follow a power-law distribution as originally claimed. Using a large-scale dataset obtained from Foursquare in Switzerland and New York City, we have shown that place transitions can be better explained using a log-normal and power-law with exponential cutoff model. Our study suggests that urban mobility patterns are more nuanced than previously reported and that goodness-of-fit tests need to be done in view of the generality of human\u00a0\u2026", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Inferring truth from multiple annotators for social interaction analysis\n", "abstract": " This study focuses on incorporating knowledge from multiple annotators into a machine-learning framework for detecting psychological traits using multimodal data. We present a model that is designed to exploit the judgements of multiple annotators on a social trait labeling task. Our two-stage model first estimates a ground truth by modeling the annotators using both the annotations and annotators\u2019 self-reported confidences. In the second stage, we train a classifier using the estimated ground truth as labels. Our experiments on a dominance estimation task in a group interaction scenario on the DOME corpus, in addition to synthetically generated data, give satisfactory results, outperforming the commonly used majority voting as well as other approaches in the literature.", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Investigating privacy-sensitive features for speech detection in multiparty conversations\n", "abstract": " We investigate four different privacy-sensitive features, namely energy, zero crossing rate, spectral flatness, and kurtosis, for speech detection in multiparty conversations. We liken this scenario to a meeting room and define our datasets and annotations accordingly. The temporal context of these features is modeled. With no temporal context, energy is the best performing single feature. But by modeling temporal context, kurtosis emerges as the most effective feature. Also, we combine the features. Besides yielding a gain in performance, certain combinations of features also reveal that a shorter temporal context is sufficient. We then benchmark other privacy-sensitive features utilized in previous studies. Our experiments show that the performance of all the privacy-sensitive features modeled with context is close to that of state-of-the-art spectral-based features, without extracting and using any features that can be used to reconstruct the speech signal.", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Identifying dominant people in meetings from audio-visual sensors\n", "abstract": " This paper provides an overview of the area of automated dominance estimation in group meetings. We describe research in social psychology and use this to explain the motivations behind suggested automated systems. With the growth in availability of conversational data captured in meeting rooms, it is possible to investigate how multi-sensor data allows us to characterize non-verbal behaviors that contribute towards dominance. We use an overview of our own work to address the challenges and opportunities in this area of research.", "num_citations": "5\n", "authors": ["1189"]}
{"title": "2d multi-person tracking: A comparative study in ami meetings\n", "abstract": " In this paper, we present the findings of the Augmented Multiparty Interaction (AMI) project investigation on the localization and tracking of 2D head positions in meetings. The focus of the study was to test and evaluate various multi-person tracking methods developed in the project using a standardized data set and evaluation methodology.", "num_citations": "5\n", "authors": ["1189"]}
{"title": "Smartphone Sensing for the Well-Being of Young Adults: A Review\n", "abstract": " Over the years, mobile phones have become versatile devices with a multitude of capabilities due to the plethora of embedded sensors that enable them to capture rich data unobtrusively. In a world where people are more conscious regarding their health and well-being, the pervasiveness of smartphones has enabled researchers to build apps that assist people to live healthier lifestyles, and to diagnose and monitor various health conditions. Motivated by the high smartphone coverage among young adults and the unique issues they face, in this review paper, we focus on studies that have used smartphone sensing for the well-being of young adults. We analyze existing work in the domain from two perspectives, namely Data Perspective and System Perspective. For both these perspectives, we propose taxonomies motivated from human science literature, which enable to identify important study areas\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Understanding Heavy Drinking at Night through Smartphone Sensing and Active Human Engagement\n", "abstract": " Heavy alcohol consumption can lead to many severe consequences. In this paper, we study the phenomenon of heavy drinking at night (4+ drinks for women or 5+ for men on a single evening), using a smartphone sensing dataset depicting about nightlife and drinking behaviors for 240 young adult participants. Our work has three contributions. First, we segment nights into moving and static episodes as anchors to aggregate mobile sensing features. Second, we show that young adults tend to be more mobile, have more activities, and attend more crowded areas outside home on heavy drinking nights compared to other nights. Third, we develop a machine learning framework to classify a given weekend night as involving heavy or non-heavy drinking, comparing automatically captured sensor features versus manually contributed contextual cues and images provided over the course of the night. Results show that\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "My own private nightlife: understanding youth personal spaces from crowdsourced video\n", "abstract": " Private nightlife environments of young people are likely characterized by their physical attributes, particular ambiance, and activities, but relatively little is known about it from social media studies. For instance, recent work has documented ambiance and physical characteristics of homes using pictures from Airbnb, but questions remain on whether this kind of curated data reliably represents everyday life situations. To describe the physical and ambiance features of homes of youth using manual annotations and machine-extracted features, we used a unique dataset of 301 crowdsourced videos of home environments recorded in-situ by young people on weekend nights. Agreement among five independent annotators was high for most studied variables. Results of the annotation task revealed various patterns of youth home spaces, such as the type of room attended (e.g., living room and bedroom), the number and\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "BookTubing across regions: examining differences based on nonverbal and verbal cues\n", "abstract": " BookTubers are a rapidly growing community in YouTube who shares content related to books. Previous literature has addressed problems related to automatically analyzing opinions and mood of video logs (vlogs) as a generic category in YouTube. Unfortunately, the population studied is not diverse. In this work, we study and compare some aspects of the geographic/cultural context of BookTube videos, comparing non-western (Indian) and Western populations. The role played by nonverbal and verbal cues in each of these contexts are analyzed automatically using audio, visual, and text features. The analysis shows that cultural context and popularity can be inferred to some degree using multimodal fusion of these features. The best obtained results are an average precision-recall score of 0.98 with Random Forest in a binary India vs. Western video classification task, and 0.75 in inferring binary popularity\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Multi-modal analysis of small-group conversational dynamics\n", "abstract": " This chapter provides an overview of the basic problems realted to automatic understanding of conversational group dynamics. It provides an overview of current research in automatic detection of the addressee (s) of the speaker in multiparty conversations, the visual focus of attention of participants in small group meetings, as well as methods for automatic dominance detection.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Privacy-sensitive audio features for speech/nonspeech detection\n", "abstract": " The goal of this paper is to investigate features for speech/nonspeech detection (SND) having low linguistic information from the speech signal. Towards this, we present a comprehensive study of privacy-sensitive features for SND in multiparty conversations. Our study investigates three different approaches to privacy-sensitive features. These approaches are based on: 1) simple, instantaneous feature extraction methods; 2) excitation source information based methods; and 3) feature obfuscation methods such as local (within 130 ms) temporal averaging and randomization applied on excitation source information. To evaluate these approaches for SND, we use multiparty conversational meeting data of nearly 450 hours. On this dataset, we evaluate these features and benchmark them against standard spectral shape based features such as Mel frequency perceptual linear prediction (MFPLP). Fusion strategies\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Detecting emergent leaders in small groups using nonverbal behavior\n", "abstract": " This paper presents first an analysis on how an emergent leader is perceived in newly formed small-groups, and in its second part tackles the emergent leader estimation task, using automatically extracted nonverbal cues. The estimations use rank-based and collective approaches with the combination of acoustic and visual features extracted from a new corpus specifically collected to analyse the emergent leadership phenomena. Our results show that visual information augments acoustic information, and the emergent leader is perceived by his/her peers as an active and dominant person. For the automatic inference of emergent leadership, adding relational information to the nonverbal cues improves the inference of each participant\u2019s label in the group.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Mining human location-routines using a multi-level topic model\n", "abstract": " In this work we address the problem of modeling varying time duration sequences for large-scale human routine discovery from cellphone sensor data using a multi-level approach to probabilistic topic models. We use an unsupervised learning approach that discovers human routines of varying durations ranging from half-hourly to several hours. Our methodology can handle large sequence lengths based on a principled procedure to deal with potentially large routine-vocabulary sizes, and can be applied to rather naive initial vocabularies to discover meaningful location-routines. We successfully apply the model to a large, real-life dataset, consisting of 97 cellphone users and 16 months of their location patterns, to discover routines with varying time durations.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Multi-person tracking in meetings: a comparative study\n", "abstract": " In this paper, we present the findings of the Augmented Multiparty Interaction (AMI) project investigation on the localization and tracking of 2D head positions in meetings. The focus of the study was to test and evaluate various multi-person tracking methods developed in the project using a standardized data set and evaluation methodology.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Annotation Procedure for WP4-locate\n", "abstract": " This document describes the procedure to annotate the features defined by the WP4-locate subgroup. It is intended for non-expert annotators, and so it aims at being simple and unambiguous.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Algorithms for video structuring\n", "abstract": " Video structuring aims at automatically finding structure in a video sequence. Occupying a keyposition within video analysis, it is a fundamental step for quality indexing and browsing. As a low level video analysis, video structuring can be seen as a serial process which includes (i) shot boundary detection,(ii) video shot feature extraction and (iii) video shot clustering. The resulting analysis serves as the base for higher level processing such as content-based image retrieval or semantic indexing. In this study, the whole process is examined and implemented. Two shot boundary detectors based on motion estimation and color distribution analysis are designed. Based on recent advances in machine learning, a novel technique for video shot clustering is presented. Typical approaches for segmenting and clustering shots use graph analysis, with split and merge algorithms for finding subgraphs corresponding to different scenes. In this work, the clustering algorithm is based on a spectral method which has proven its efficiency in still-image segmentation. This technique clusters points (in our case features extracted from video shots) using eigenvectors of matrices derived from data. Relevant data depends of the quality of feature extraction. After stating the main problems of video structuring, solutions are proposed defining an heuristical distance metric for similarity between shots. We combine color visual features with time constraints. The entire process of video structuring is tested on a ten hours home video database.", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Semiautomatic video object generation using multivalued watershed and partition lattice operators\n", "abstract": " We conceive the problem of multiple semantic wide object (SVO) extraction as an issue of designing extensive operators on the lattice of partitions. As a result, we propose a framework based on spatial partition generation and application of optimal operators on the generated partitions. The first stage is obtained with a multivalued morphological spatial segmentation method that incorporates an edge-driven marker extraction algorithm and a growing method which integrates both color and edge information. For the second stage, we propose a new spatio-temporal regional maximum likelihood partition operator for extraction purposes. Subjective and objective evaluation of the experimental results obtained with our approach on several MPEG-4 test video sequences show that it accurately tracks multiple SVOs in several scenarios, while improving the SVO extraction precision compared to traditional watershed\u00a0\u2026", "num_citations": "4\n", "authors": ["1189"]}
{"title": "Protecting mobile food diaries from getting too personal\n", "abstract": " Smartphone applications that use passive sensing to support human health and well-being primarily rely on:(a) generating low-dimensional representations from high-dimensional data streams;(b) making inferences regarding user behavior; and (c) using those inferences to benefit application users. Meanwhile, sometimes these datasets are shared with third parties as well. Human-centered ubiquitous systems need to ensure that sensitive attributes of users are protected when applications provide utility to people based on such behavioral inferences. In this paper, we demonstrate that inferences of sensitive attributes of users (gender, body mass index category) are possible using low-dimensional and sparse data coming from mobile food diaries (a combination of sensor data and self-reports). After exposing this potential risk, we demonstrate how deep learning techniques can be used for feature transformation\u00a0\u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Social Multimedia, Diversity, and Global South Cities: A Double Blind Side\n", "abstract": " Social media provides opportunities to examine urban phenomena at scale, and we believe that studying cities in the Global South through citizen-contributed data and AI-driven analytics should be a priority of multimedia research. However, little work has been done in our community, and we argue that this contributes to a double blind side problem. We exemplify this situation by studying Ma3Route, a mobile social media channel to crowdsource and broadcast transit reports in Nairobi, Kenya. Using multimedia data from its Twitter stream, we first conduct a descriptive analysis that shows an active community generating rich traffic-related reports, and then discover latent topics that identify both regular and ephemeral thematic clusters of reports involving accidents, traffic conditions, and attitudes of citizens towards authorities. In the second place, we conduct a deep learning-based analysis of Ma3Route images to\u00a0\u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Discovering eating routines in context with a smartphone app\n", "abstract": " In everyday life, eating follows patterns and occurs in context. We present an approach to discover daily eating routines of a population following a multidimensional representation of eating episodes, using data collected with the Bites'n'Bits smartphone app. Our approach integrates multiple contextual cues provided in-situ (food type, time, location, social context, concurrent activities, and motivations) with probabilistic topic models, which discover representative patterns across these contextual dimensions. We show that this approach, when applied on eating episode data for over 120 people and 1200 days, allows describing the main eating routines of the population in meaningful ways. This approach, resulting from a collaboration between ubiquitous computing and nutrition science, can support interdisciplinary work on contextual analytics for promotion of healthy eating.", "num_citations": "3\n", "authors": ["1189"]}
{"title": "A tale of two interactions: Inferring performance in hospitality encounters from cross-situation social sensing\n", "abstract": " People behave differently in different situations. With the advances in ubiquitous sensing technologies, it is now easier to capture human behavior across multiple situations automatically and unobtrusively. We investigate human behavior across two situations that are ubiquitous in hospitality (job interview and reception desk) with the objective of inferring performance on the job. Utilizing a dataset of 338 dyadic interactions, played by students from a hospitality management school, we first study the connections between automatically extracted nonverbal cues, linguistic content, and various perceived variables of soft skills and performance in these two situations. A correlation analysis reveals connection between perceived variables and nonverbal cues displayed during job interviews, and perceived performance on the job. We then propose a computational framework, with nonverbal cues and linguistic style from\u00a0\u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Shape Representations for Maya Codical Glyphs: Knowledge-driven or Deep?\n", "abstract": " This paper investigates two-types of shape representations for individual Maya codical glyphs: traditional bag-of-words built on knowledge-driven local shape descriptors (HOOSC), and Convolutional Neural Networks (CNN) based representations, learned from data. For CNN representations, first, we evaluate the activations of typical CNNs that are pretrained on large-scale image datasets; second, we train a CNN from scratch with all the available individual segments. One of the main challenges while training CNNs is the limited amount of available data (and handling data imbalance issue). Here, we attempt to solve this imbalance issue by introducing class-weights into the loss computation during training. Another possibility is oversampling the minority class samples during batch selection. We show that deep representations outperform the other, but CNN training requires special care for small-scale\u00a0\u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Transferring neural representations for low-dimensional indexing of Maya hieroglyphic art\n", "abstract": " We analyze the performance of deep neural architectures for extracting shape representations of binary images, and for generating low-dimensional representations of them. In particular, we focus on indexing binary images exhibiting compounds of Maya hieroglyphic signs, referred to as glyph-blocks, which constitute a very challenging dataset of arts given their visual complexity and large stylistic variety. More precisely, we demonstrate empirically that intermediate outputs of convolutional neural networks can be used as representations for complex shapes, even when their parameters are trained on gray-scale images, and that these representations can be more robust than traditional handcrafted features. We also show that it is possible to compress such representations up\u00a0to only three dimensions without harming much of their discriminative structure, such that effective visualization of Maya\u00a0\u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Ancient Maya writings as high-dimensional data: a visualization approach\n", "abstract": " The ancient Maya civilization flourished from around 2000 BC to 1600 AD and left a great amount of cultural heritage materials, in the shape of stone monument inscriptions, folded codex pages, or personal ceramic items. All these materials contain hieroglyphs (in short glyphs) written on them. The Maya writing system is visually complex (Fig. 1) and new glyphs are still being discovered. This brings the necessity of better digital preservation systems. Interpretation of a small amount of glyphs is still open to discussion due to both visual differences and semantic analysis. Some glyphs are damaged, or have many variations due to artistic reasons and the evolving nature of language. Signs following ancient Mesoamerican representational conventions end up being classified according to their appearance, which leads to potential confusions as the iconic origin of many signs and their transformations through time are not well understood. For instance, a sign thought to fall within the category of \u201cbody-part\u201d can later be proven to actually correspond to a vegetable element (a different semantic domain). Similarly, several signs classified as \u201cabstract\u201d,\u201csquare\u201d or \u201cround\u201d could actually be pars-pro-toto representations of a larger whole.", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Special issue on the mobile data challenge\n", "abstract": " Special Issue on the Mobile Data Challenge - Infoscience English Fran\u00e7ais login Home > Special Issue on the Mobile Data Challenge Infoscience Information Usage statistics Files Special Issue on the Mobile Data Challenge Gatica-Perez, Daniel ; Laurila, Juha K. ; Blom, Jan Published in: Pervasive And Mobile Computing, 9, 6, 751-751 Year: 2013 Publisher: Amsterdam, Elsevier Science Bv ISSN: 1574-1192 DOI: 10.1016/j.pmcj..Other identifiers: View record in Web of Science Laboratories: IEL Record appears in: Scientific production and competences > STI - School of Engineering > IEL - Institute of Electrical Engineering > UNATTRIBUTED-IEL - IEL - Unattributed publications Peer-reviewed publications Work produced at EPFL Journal Articles Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record \u2026", "num_citations": "3\n", "authors": ["1189"]}
{"title": "LP Residual Features for Robust, Privacy-Sensitive Speaker Diarization\n", "abstract": " We present a comprehensive study of linear prediction residual for speaker diarization on single and multiple distant microphone conditions in privacy-sensitive settings, a requirement to analyze a wide range of spontaneous conversations. Two representations of the residual are compared, namely real-cepstrum and MFCC, with the latter performing better. Experiments on RT06eval show that residual with subband information from 2.5 kHz to 3.5 kHz and spectral slope yields a performance close to traditional MFCC features. As a way to objectively evaluate privacy in terms of linguistic information, we perform phoneme recognition. Residual features yield low phoneme accuracies compared to traditional MFCC features.", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Mining human location-routines using a multi-level approach to topic modeling\n", "abstract": " In this work we address the problem of modeling varying time duration sequences for large-scale human routine discovery from cellphone sensor data using a multi-level approach to probabilistic topic models. We use an unsupervised learning approach that discovers human routines of varying durations ranging from half-hourly to several hours. Our methodology can handle large sequence lengths based on a principled procedure to deal with potentially large routine-vocabulary sizes, and can be applied to rather naive initial vocabularies to discover meaningful location-routines. We successfully apply the model to a large, real-life dataset, consisting of 97 cellphone users and 16 months of their location patterns, to discover routines with varying time durations.", "num_citations": "3\n", "authors": ["1189"]}
{"title": "Contactless Sleep Monitoring for Early Detection of Health Deteriorations in Community-Dwelling Older Adults: Exploratory Study\n", "abstract": " Background: Population aging is posing multiple social and economic challenges to society. One such challenge is the social and economic burden related to increased health care expenditure caused by early institutionalizations. The use of modern pervasive computing technology makes it possible to continuously monitor the health status of community-dwelling older adults at home. Early detection of health issues through these technologies may allow for reduced treatment costs and initiation of targeted preventive measures leading to better health outcomes. Sleep is a key factor when it comes to overall health and many health issues manifest themselves with associated sleep deteriorations. Sleep quality and sleep disorders such as sleep apnea syndrome have been extensively studied using various wearable devices at home or in the setting of sleep laboratories. However, little research has been conducted evaluating the potential of contactless and continuous sleep monitoring in detecting early signs of health problems in community-dwelling older adults.Objective: In this work we aim to evaluate which contactlessly measurable sleep parameter is best suited to monitor perceived and actual health status changes in older adults.Methods: We analyzed real-world longitudinal (up to 1 year) data from 37 community-dwelling older adults including more than 6000 nights of measured sleep. Sleep parameters were recorded by a pressure sensor placed beneath the mattress, and corresponding health status information was acquired through weekly questionnaires and reports by health care personnel. A total of 20 sleep parameters were\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "The Theory, Practice, and Ethical Challenges of Designing a Diversity-Aware Platform for Social Relations\n", "abstract": " Diversity-aware platform design is a paradigm that responds to the ethical challenges of existing social media platforms. Available platforms have been criticized for minimizing users\u2019 autonomy, marginalizing minorities, and exploiting users\u2019 data for profit maximization. This paper presents a design solution that centers the well-being of users. It presents the theory and practice of designing a diversity-aware platform for social relations. In this approach, the diversity of users is leveraged in a way that allows like-minded individuals to pursue similar interests or diverse individuals to complement each other in a complex activity. The end users of the envisioned platform are students, who participate in the design process. Diversity-aware platform design involves numerous steps, of which two are highlighted in this paper: 1) defining a framework and operationalizing the\" diversity\" of students, 2) collecting\" diversity\" data to build diversity-aware algorithms. The paper further reflects on the ethical challenges encountered during the design of a diversity-aware platform.", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Evaluation of 1-year in-home monitoring technology by home-dwelling older adults, family caregivers, and nurses\n", "abstract": " Introduction: Population ageing is increasing the healthcare needs and costs. Both frailty and chronic diseases affecting older people reduce their ability to live independently. However, most older people prefer to age in their own homes. New development of in-home monitoring can play a role in staying independent, active, and healthy for older people. This 12-month observational study aimed to evaluate a new in-home monitoring system among home-dwelling older adults (OA), their family caregivers (FC), and nurses for the support of home care. Methods: In-home monitoring system evaluated in this study continuously monitored OA\u2019s daily activities (e.g. mobility, sleep habits, fridge visits, door events) by ambient sensor system (DomoCare\u00ae) and health-related events by wearable sensors (Activity tracker, ECG). In case of deviations in daily activities, alerts were transmitted to nurses via email. Using specific questionnaires, the opinion of 13 OA, 13 FC, and 20 nurses were collected at the end of 12-months follow-up focusing on user experience and the impact of in-home monitoring on home care services. Results: The majority of OA, FC, and nurses considered that in-home sensors can help staying at home, improving home care and quality of life, preventing domestic accidents, and reducing family stress. The opinion tended to be more frequently favorable toward ambient sensors (76%; 95% CI: 61% - 87%) than toward wearable sensors (Activity tracker: 65%; 95% CI: 50% - 79%); ECG: 60%; 95% CI: 45% - 75%). On average, OA (74%; 95% CI: 46% - 95%) and FC (70%; 95% CI: 39% - 91%) tended to be more enthusiastic than nurses (60\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Analyzing and visualizing ancient Maya hieroglyphics using shape: From computer vision to Digital Humanities\n", "abstract": " Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing existing catalogs to identify individual glyphs. Automatic Maya glyph analysis provides an efficient way to assist scholars\u2019 daily work. We introduce the Histogram of Orientation Shape Context (HOOSC) shape descriptor to the Digital Humanities community. We discuss key issues for practitioners and study the effect that certain parameters have on the performance of the descriptor. Different HOOSC parameters are tested in an automatic ancient Maya hieroglyph retrieval system with two different settings, namely, when shape alone is considered and when glyph co-occurrence information is incorporated. Additionally, we developed a graph-based glyph visualization interface to facilitate efficient exploration and analysis of hieroglyphs. Specifically, a force-directed graph prototype is applied to visualize Maya glyphs\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities\n", "abstract": " Technological advances in digitization, automatic image analysis and information management are enabling the possibility to analyze, organize and visualize large cultural datasets. As one of the key visual cues, shape feature has been used in various image analysis tasks such as handwritten character recognition [1, 5], sketch analysis [4], etc. We assess a shape descriptor, within the application domain of Maya hieroglyphic analysis. Our aim is to introduce this descriptor to the wider Digital Humanities (DH) community, as a shape analysis tool for DH-related applications. The Maya civilization is one of the major cultural developments in ancient Mesoamerica. The ancient Maya language infused art with uniquely pictorial forms of hieroglyphic writing, which represents an exceptionally rich legacy [10]. Most Maya texts were written during the Classic period (AD 250-900) of the Maya civilization on various media types, including stone monuments. A special class of Maya texts was written on bark cloths as folding books from the Post-Classic period (AD 1000-1519). Only three such books (namely the Dresden, Madrid and Paris codices) are known to have survived the Spanish Conquest. A typical Maya codex page contains icons, main sign glyph blocks, captions, calendric signs, etc. Fig. 1 illustrates an example page segmented into main elements [6]. In this paper, we are interested in the main signs. Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Capturing upper body motion in conversation: An appearance quasi-invariant approach\n", "abstract": " We address the problem of body communication retrieval and measuring in seated conversations by means of markerless motion capture. In psychological studies, the use of automatic methods is key to reduce the subjectivity present in manual behavioral coding used to extract these cues. These studies usually involve hundreds of subjects with different clothing, non-acted poses, or different distances to the camera in uncalibrated, RGB-only video. However, range cameras are not yet common in psychology research, especially in existing recordings. Therefore, it becomes highly relevant to develop a fast method that is able to work in these conditions. Given the known relationship between depth and motion estimates, we propose to robustly integrate highly appearance-invariant image motion features in a machine learning approach, complemented with an effective tracking scheme. We evaluate the method's\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "The Workshop on Computational Personality Recognition\n", "abstract": " The Workshop on Computational Personality Recognition aims to define the state-of-the-art in the field and to provide tools for future standard evaluations in personality recognition tasks. In the WCPR14 we released two different datasets: one of Youtube Vlogs and one of Mobile Phone interactions. We structured the workshop in two tracks: an open shared task, where participants can do any kind of experiment, and a competition. We also distinguished two tasks: A) personality recognition from multimedia data, and B) personality recognition from text only. In this paper we discuss the results of the workshop.", "num_citations": "2\n", "authors": ["1189"]}
{"title": "New world, new worlds: Visual analysis of pre-columbian pictorial collections\n", "abstract": " We present an overview of the CODICES project, an interdisciplinary approach for analysis of pre-Columbian collections of pictorial materials \u2013 more specifically, of Maya hieroglyphics. We discuss some of the main scientific and technical challenges that we have found in our work, and present a summary of our current technical achievements. This overview stresses the importance of thinking globally and acting both locally and globally with respect to developing approaches for cultural heritage preservation, research, and education.", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Flickr groups: Multimedia communities for multimedia analysis\n", "abstract": " We present in this chapter a review of current work that leverages on large online social networks\u2019 meta-information, in particular Flickr Groups. We briefly present this hugely successful feature in Flickr and discuss the various ways in which metadata stemming from users\u2019 interactions with and within groups has been exploited by researchers to improve on state-of-the-art search and browsing algorithms. We then review recent works that have already made use of Flickr Groups, either as a data source, as a way of filtering content, or as a way to reach users for automatic analysis or user studies, and conclude by pointing out to potential directions of future research.", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Meeting data collection specifications\n", "abstract": " This follows from an earlier data collection effort that resulted in a corpus of 60 scripted meetings (30 train, 30 test), each of 5 minutes duration (now available at mmm. idiap. ch). The current data collection effort aims to address some of the limitations of the previous corpus, as well as to cater for a richer variety of research tasks. Some specific motivations for collecting a new corpus of meetings include the need for:", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Partition lattice operators for extraction of semantic video objects\n", "abstract": " We conceive the problem of multiple semantic video object (SVO) extraction as an issue of designing operators on a complete lattice of partitions. In this paper, we propose a framework based on accurate spatial partition generation and application of optimal extraction operators on the generated partitions. Under the framework, we introduce a spatio-temporal regional maximum likelihood operator for extraction purposes. Some theoretical properties of the operators are established. Experimental results show that our scheme is capable of successfully handling multiple SVOs in a variety of scenarios.", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Generating video objects by multiple-view extensive partition lattice operators\n", "abstract": " Occlusion/disocclusion is one of the fundamental problems for semantic video object (SVO) extraction because the tracking degradation from occlusion/disocclusion creates difficulty for the pixel-wise accuracy requirement and adds to the amount of user interaction required in off-line video editing applications. We present an approach based on the application of a new extensive operator in a lattice of partitions, that exploits information from various views of the scene to address the disocclusion problem by using a regional Bayesian formulation. Our multiview operator builds on the application of the maximum a posteriori (MAP) principle, by integrating a single view region classification stage and a multiview stage that solves for disoccluded regions labeled as uncertain. Results on several real sequences show that this approach improves the quality of the extracted SVOs and reduces the total amount of user\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Segmentation of moving human body parts by a modified MAP-MRF algorithm\n", "abstract": " Using a dense motion vector field as the main information the authors develop a region segmentation algorithm in which each region is matched to a four-parameter motion model. Based on Markov random fields the segmentation model detects moving parts of the human body with different apparent displacement such as the hands. The motion vector field has been estimated by a Baaziz pel-recursive method and considered together with others sources of information such as intensity contours, intensity values and non-compensated pixels as inputs of the Markov random field model. The maximum a posteriori criterion (MAP) is used for the optimization of the solution, and performed with a deterministic method: iterated conditional modes (ICM). Results on segmenting and classifying real sequences are shown and, based on a roughly defined directional dictionary, one application is pursuing the use of the\u00a0\u2026", "num_citations": "2\n", "authors": ["1189"]}
{"title": "Ten seconds of my nights: Exploring methods to measure brightness, loudness and attendance and their associations with alcohol use from video clips\n", "abstract": " Introduction Most evidence on associations between alcohol use behaviors and the characteristics of its social and physical context is based on self-reports from study participants and, thus, only account for their subjective impressions of the situation. This study explores the feasibility of obtaining alternative measures of loudness, brightness, and attendance (number of people) using 10-second video clips of real-life drinking occasions rated by human annotators and computer algorithms, and explores the associations of these measures with participants\u2019 choice to drink alcohol or not.   Methods Using a custom-built smartphone application, 215 16-25-year-olds documented characteristics of 2,380 weekend night drinking events using questionnaires and videos. Ratings of loudness, brightness, and attendance were obtained from three sources, namely in-situ participants\u2019 ratings, video-based annotator ratings, and video-based computer algorithm ratings. Bivariate statistics explored differences in ratings across sources. Multilevel logistic regressions assessed the associations of contextual characteristics with alcohol use. Finally, model fit indices and cross-validation were used to assess the ability of each set of contextual measures to predict participants\u2019 alcohol use.   Results Raw ratings of brightness, loudness and attendance differed slightly across sources, but were all correlated (r = .21 to .82, all p < .001). Participants rated bars/pubs as being louder (Cohen\u2019s d = 0.50 [95%-CI: 0.07\u20130.92]), and annotators rated private places as darker (d = 1.21 [95%-CI: 0.99\u20131.43]) when alcohol was consumed than when alcohol was not consumed\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Youth nightlife at home: towards a feminist conceptualisation of home\n", "abstract": " This paper explores home as a space of youth nightlife and drinking through a feminist lens. It draws on feminist geographical scholarship on home and 40 semi-structured interviews with young people aged 16\u201325 in Switzerland in the context of a larger interdisciplinary study. We find that the home figures as a central space of nightlife for young people beyond pre-drinking or home parties. At the same time, privacy and intimacy are important to young people when drinking alcohol outside of the home. We suggest that this preference of privacy when going out indicates an interweaving of private and public spheres in young people\u2019s nightlives. The paper argues that the home is both a central concrete space and an important symbolic notion in young people\u2019s nightlives. In so doing, it empirically complicates the public/private dualism and contributes to feminist geographical conceptualisations of home in the\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Shooting shots: Estimating alcoholic drink sizes in real life using event\u2010level reports and annotations of close\u2010up pictures\n", "abstract": " Introduction and Aims Drinks consumed in real life are diverse, in terms of beverage type, container size and alcohol by volume. To date, most ecological momentary assessment studies have assessed drinking amounts with \u2018standard\u2019 drinks, although their event\u2010level design allows for more advanced assessment schemes. The purpose of this empirical study is to compare participants' estimates of alcoholic drink characteristics, assessed using drink\u2010specific questions, with estimates generated by annotators based on pictures of the same drinks.   Design and Methods On weekend nights, 186 young adults took 1484 close\u2010up pictures of their drinks using a custom\u2010built smartphone application. Participants reported the beverage type, drink size and alcohol by volume. Annotators described the beverage type, container size and filling level. Correspondence between participants\u2019 and annotators' estimates was\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Understanding Applicants' Reactions to Asynchronous Video Interviews Through Self-reports and Nonverbal Cues\n", "abstract": " Asynchronous video interviews (AVIs) are increasingly used by organizations in their hiring process. In this mode of interviewing, the applicants are asked to record their responses to predefined interview questions using a webcam via an online platform. AVIs have increased usage due to employers' perceived benefits in terms of costs and scale. However, little research has been conducted regarding applicants' reactions to these new interview methods. In this work, we investigate applicants' reactions to an AVI platform using self-reported measures previously validated in psychology literature. We also investigate the connections of these measures with nonverbal behavior displayed during the interviews. We find that participants who found the platform creepy and had concerns about privacy reported lower interview performance compared to participants who did not have such concerns. We also observe weak\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Bites'n'Bits: Eating Behavior in Context from Mobile Data\n", "abstract": " Eating follows patterns and occurs in context. We present a mobile data collection study to capture food consumption in everyday life, and study connections between eating behaviors and context (time, location, social context, related activities and physical activity). Our contributions are three-fold. First, we developed a cross-platform smartphone application to collect geo-localized food photos, context survey data, and physical activity data from Fitbit devices. Second, we deployed a data collection campaign with 122 Swiss university students, resulting in 1208 days of food data, 3414 meal occasions, 1034 snacking occasions, 5097 photos, and 998 days of physical activity. We analyzed the collected data and report findings associated to the compliance and user experience, snacks vs. meals patterns, physical activity, and associations between contextual factors and eating occasions. Third, we addressed a novel ubicomp task, namely the classification of eating occasions (meals vs. snacks) in everyday life. We show that a machine learning method using time of day, time since last intake, and location is able to discriminate eating occasions with 84% precision, which significantly outperforms a baseline method based only on time.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "MAAYA: Multimedia Methods to Support Maya Epigraphic Analysis\n", "abstract": " This paper present an overview of a research project integrating epigraphy and computer science, which has developed a number of visual analysis methods for segmentation, classification, indexing, and retrieval of Maya glyphs in ancient codices. The paper summarizes the technical aspects of the methods developed for these tasks, including preparation of source materials, creation of a data repository, segmentation of glyph strokes, classification of single glyphs, and visualization of glyph collections. The developed methods, based on computer vision and machine learning, are generic and could be applicable to other sources of visual data in the digital humanities.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "SenseCityVity: Mobile Sensing, Urban Awareness, and Collective Action in Mexico\n", "abstract": " The SenseCityVity project aims to engage youth in Guanajuato City, Mexico, helping them investigate, document, and reflect on urban problems though mobile crowdsourcing. The Urban Data Challenge encourages them to collect geolocalized images, audio, and video for further analysis.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Introduction to the special section of best papers of ACM Multimedia 2011\n", "abstract": " ACM Multimedia (ACM MM) is the worldwide premier multimedia conference and a key event to share scientific achievements and industry innovations. In 2011, the conference was held in Scottsdale, AZ, USA, and a large number of high-quality papers were published and presented. As a novelty to this year\u2019s conference, the traditional four-track format was changed to a ten-area format in order to solicit papers from a wider range of timely multimedia-related topics. The areas defined for 2011 were as follows.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Learning semantics from multimedia web resources: an introduction to the special issue\n", "abstract": " The thirteen papers in this special issue focus on effective techniques for learning semantics from multimedia Web resources.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Modeling human behavior with mobile phones\n", "abstract": " In just a few years, mobile phones have emerged as the ultimate multimedia device. This is the summary of a proposed tutorial on Modeling Human Behavior with Mobile Phones, which aims to present the scientific and technological state-of-the-art in mobile phone-based modeling of large-scale human behavior from a coherent perspective, and hopes to motivate further work in this domain by the multimedia research community.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Estimating Dominance in Small Group Meetings with Audio-Visual Fusion of Nonverbal Cues\n", "abstract": " This paper addresses the multimodal nature of social dominance and presents multimodal fusion techniques to combine audio and visual nonverbal cues for dominance estimation in small group conversations. We combine the two modalities both at the feature extraction level and at the classifier level via score and rank level fusion. The classification is done by a simple rulebased estimator. We perform experiments on a new 10-hour dataset derived from the popular AMI corpus. We objectively evaluated the performance of each modality and each cue alone and in combination. Our results show that the combination of audio and visual cues is necessary to achieve the best performance.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Visual attention, speaking activity, and group conversational analysis in multi-sensor environments\n", "abstract": " Among the many possibilities of automation enabled by multi-sensor environments - several of which are discussed in this Handbook - one particularly relevant is the analysis of social interaction in the workplace, and more specifically, of conversational group interaction. Group conversations are ubiquitous, and represent a fundamental means through which ideas are discussed, progress is reported, and knowledge is created and disseminated.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Exploring contextual information in a layered framework for group action recognition\n", "abstract": " Contextual information is important for sequence modeling. Hidden Markov models (HMMs) and extensions, which have been widely used for sequence modeling, make simplifying, often unrealistic assumptions on the conditional independence of observations given the class labels, thus cannot accommodate overlapping features or long-term contextual information. In this paper, we introduce a principled layered framework with three implementation methods that take into account contextual information (as available in the whole or part of the sequence). The first two methods are based on state alpha and gamma posteriors (as usually referred to in the HMM formalism). The third method is based on conditional random fields (CRFs), a conditional model that relaxes the independent assumption on the observations required by HMMs for computational tractability. We illustrate our methods with the application of\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Motion likelihood and proposal modeling in Model-Based Stochastic Tracking\n", "abstract": " Particle filtering is now established as one of the most popular methods for visual tracking. Within this framework, there are two important considerations. The first one refers to the generic assumption that the observations are temporally independent given the sequence of object states. The second consideration, often made in the literature, uses the transition prior as proposal distribution. Thus, the current observations are not taken into account, requesting the noise process of this prior to be large enough to handle abrupt trajectory changes. As a result, many particles are either wasted in low likelihood regions of the state space, resulting in low sampling efficiency, or more importantly, propagated to distractor regions of the image, resulting in tracking failures. In this paper, we propose to handle both considerations using motion. We first argue that in general observations are conditionally correlated, and propose a new model to account for this correlation allowing for the natural introduction of implicit and/or explicit motion measurements in the likelihood term. Secondly, explicit motion measurements are used to drive the sampling process towards the most likely regions of the state space. Overall, the proposed model allows to handle abrupt motion changes and to filter out visual distractors when tracking objects with generic models based on shape or color distribution. Experimental results obtained on head tracking, using several sequences with moving camera involving large dynamics, and compared against the CONDENSATION algorithm, have demonstrated superior tracking performance of our approach.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "On Automatic Annotation of Images with Latent Space Models\n", "abstract": " Image auto-annotation, ie, the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Video object hyper-links for streaming applications\n", "abstract": " In video streaming applications, people usually rely on the traditional VCR functionalities to reach segments of interest. However, in many situations, the focus of the people are particular objects. Video object (VO) hyper-linking, i.e., the creation of non-sequential links between video segments where an object of interest appears, constitutes a highly desirable browsing feature that extends the traditional video structure representation. In this paper we present an approach for VO hyper-linking generation based on video structuring, definition of objects of interest, and automatic object localization in the video structure. We also discussed its use in a video streaming platform to provide objectbased VCR functionalities.", "num_citations": "1\n", "authors": ["1189"]}
{"title": "Segmentation and classification of hand gestures for man-machine communication\n", "abstract": " Man-machine communication in a natural way, it means without cumbersome gloves, is still an open problem. Keeping in mind the need to develop some friendly tools for helping people with disabilities to use the computer as a support tool for training into reinforced methods for learning to read or any other application. In this work we have addressed the problem of communication with a computer using some recognition of very basic hand gestures. From an engineering point of view our system is based on a video camera which captures image sequences and in a first time a segmentation of hand gestures is developed in order to provide information for its posterior classification and recognition. For classifying the segmented fields named e of gestures, for instance hand# 1 and hand# 2, see figures 5a and 6a, we have proceed first to obtain a binary version of these segmented fields comparing them with a threshold, so rendering the classification faster, then based on the Radon transform (Lim, 1990), a computation of the projected sum of the binary intensity of gestures has been done at directions 0 and 90, see figures 1 and 2. For reducing the number of data to be processed a wavelet decomposition of the projected sum of the binary intensity for each orientation (0 and 90) has been done using Daubechies filters: d4 (Daubechies, 1988). This projected and wavelet decomposed information has been used for classifying the gestures: training our system with our dictionary and computing the correlation coefficient between the wavelet coefficients corresponding to trained sequences and others captured and computed in continuous operation\u00a0\u2026", "num_citations": "1\n", "authors": ["1189"]}