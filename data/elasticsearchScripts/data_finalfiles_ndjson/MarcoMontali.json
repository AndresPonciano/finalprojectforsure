{"title": "Declarative specification and verification of service choreographiess\n", "abstract": " Service-oriented computing, an emerging paradigm for architecting and implementing business collaborations within and across organizational boundaries, is currently of interest to both software vendors and scientists. While the technologies for implementing and interconnecting basic services are reaching a good level of maturity, modeling service interaction from a global viewpoint, that is, representing service choreographies, is still an open challenge. The main problem is that, although declarativeness has been identified as a key feature, several proposed approaches specify choreographies by focusing on procedural aspects, leading to over-constrained and over-specified models. To overcome these limits, we propose to adopt DecSerFlow, a truly declarative language, to model choreographies. Thanks to its declarative nature, DecSerFlow semantics can be given in terms of logic-based languages. In\u00a0\u2026", "num_citations": "265\n", "authors": ["1890"]}
{"title": "Monitoring business constraints with linear temporal logic: An approach based on colored automata\n", "abstract": " Today\u2019s information systems record real-time information about business processes. This enables the monitoring of business constraints at runtime. In this paper, we present a novel runtime verification framework based on linear temporal logic and colored automata. The framework continuously verifies compliance with respect to a predefined constraint model. Our approach is able to provide meaningful diagnostics even after a constraint is violated. This is important as in reality people and organizations will deviate and in many situations it is not desirable or even impossible to circumvent constraint violations. As demonstrated in this paper, there are several approaches to recover after the first constraint violation. Traditional approaches that simply check constraints are unable to recover after the first violation and still foresee (inevitable) future violations. The framework has been implemented in the process\u00a0\u2026", "num_citations": "233\n", "authors": ["1890"]}
{"title": "Verification of relational data-centric dynamic systems with external services\n", "abstract": " Data-centric dynamic systems are systems where both the process controlling the dynamics and the manipulation of data are equally central. We study verification of (first-order) mu-calculus variants over relational data-centric dynamic systems, where data are maintained in a relational database, and the process is described in terms of atomic actions that evolve the database. Action execution may involve calls to external services, thus inserting fresh data into the system. As a result such systems are infinite-state. We show that verification is undecidable in general, and we isolate notable cases where decidability is achieved. Specifically we start by considering service calls that return values deterministically (depending only on passed parameters). We show that in a mu-calculus variant that preserves knowledge of objects appeared along a run we get decidability under the assumption that the fresh data introduced\u00a0\u2026", "num_citations": "193\n", "authors": ["1890"]}
{"title": "Compliance monitoring in business processes: Functionalities, application, and tool-support\n", "abstract": " In recent years, monitoring the compliance of business processes with relevant regulations, constraints, and rules during runtime has evolved as major concern in literature and practice. Monitoring not only refers to continuously observing possible compliance violations, but also includes the ability to provide fine-grained feedback and to predict possible compliance violations in the future. The body of literature on business process compliance is large and approaches specifically addressing process monitoring are hard to identify. Moreover, proper means for the systematic comparison of these approaches are missing. Hence, it is unclear which approaches are suitable for particular scenarios. The goal of this paper is to define a framework for Compliance Monitoring Functionalities (CMF) that enables the systematic comparison of existing and new approaches for monitoring compliance rules over business\u00a0\u2026", "num_citations": "176\n", "authors": ["1890"]}
{"title": "Specification and verification of declarative open interaction models: A logic-based approach\n", "abstract": " Many novel application scenarios and architectures in business process management or service composition are characterized by a distribution of activities and resources, and by complex interaction and coordination dynamics. In this book, Montali answers fundamental questions on open and declarative modeling abstractions via the integration and extension of quite diverse approaches into a computational logic-based comprehensive framework. This framework allows non IT experts to graphically specify interaction models that are then automatically transformed into a corresponding formal representation and a set of fully automated sound and complete verification facilities. The book constitutes a revised and extended version of the author\u2019s PhD thesis, which was honored with the 2009 \u201cMarco Cadoli\u201d prize, awarded by the Italian Association for Logic Programming for the most outstanding thesis focusing on computational logic, discussed between the years 2007 and 2009.", "num_citations": "176\n", "authors": ["1890"]}
{"title": "Foundations of data-aware process analysis: a database theory perspective\n", "abstract": " In this work we survey the research on foundations of data-aware (business) processes that has been carried out in the database theory community. We show that this community has indeed developed over the years a multi-faceted culture of merging data and processes. We argue that it is this community that should lay the foundations to solve, at least from the point of view of formal analysis, the dichotomy between data and processes still persisting in business process management.", "num_citations": "146\n", "authors": ["1890"]}
{"title": "Monitoring business constraints with the event calculus\n", "abstract": " Today, large business processes are composed of smaller, autonomous, interconnected subsystems, achieving modularity and robustness. Quite often, these large processes comprise software components as well as human actors, they face highly dynamic environments and their subsystems are updated and evolve independently of each other. Due to their dynamic nature and complexity, it might be difficult, if not impossible, to ensure at design-time that such systems will always exhibit the desired/expected behaviors. This, in turn, triggers the need for runtime verification and monitoring facilities. These are needed to check whether the actual behavior complies with expected business constraints, internal/external regulations and desired best practices. In this work, we present Mobucon EC, a novel monitoring framework that tracks streams of events and continuously determines the state of business constraints. In\u00a0\u2026", "num_citations": "124\n", "authors": ["1890"]}
{"title": "Reasoning on LTL on finite traces: Insensitivity to infiniteness\n", "abstract": " In this paper we study when an LTL formula on finite traces (LTLf formula) is insensitive to infiniteness, that is, it can be correctly handled as a formula on infinite traces under the assumption that at a certain point the infinite trace starts repeating an end event forever, trivializing all other propositions to false. This intuition has been put forward and (wrongly) assumed to hold in general in the literature. We define a necessary and sufficient condition to characterize whether an LTLf formula is insensitive to infiniteness, which can be automatically checked by any LTL reasoner. Then, we show that typical LTLf specification patterns used in process and service modeling in CS, as well as trajectory constraints in Planning and transition-based LTLf specifications of action domains in KR, are indeed very often insensitive to infiniteness. This may help to explain why the assumption of interpreting LTL on finite and on infinite traces has been (wrongly) blurred. Possibly because of this blurring, virtually all literature detours to Buechi automata for constructing the NFA that accepts the traces satisfying an LTLf formula. As a further contribution, we give a simple direct algorithm for computing such NFA.", "num_citations": "121\n", "authors": ["1890"]}
{"title": "Exploiting inductive logic programming techniques for declarative process mining\n", "abstract": " In the last few years, there has been a growing interest in the adoption of declarative paradigms for modeling and verifying process models. These paradigms provide an abstract and human understandable way of specifying constraints that must hold among activities executions rather than focusing on a specific procedural solution. Mining such declarative descriptions is still an open challenge. In this paper, we present a logic-based approach for tackling this problem. It relies on Inductive Logic Programming techniques and, in particular, on a modified version of the Inductive Constraint Logic algorithm. We investigate how, by properly tuning the learning algorithm, the approach can be adopted to mine models expressed in the ConDec notation, a graphical language for the declarative specification of business processes. Then, we sketch how such a mining framework has been concretely implemented as a\u00a0\u2026", "num_citations": "113\n", "authors": ["1890"]}
{"title": "Runtime verification of LTL-based declarative process models\n", "abstract": " Linear Temporal Logic (LTL) on finite traces has proven to be a good basis for the analysis and enactment of flexible constraint-based business processes. The Declare language and system benefit from this basis. Moreover, LTL-based languages like Declare can also be used for runtime verification. As there are often many interacting constraints, it is important to keep track of individual constraints and combinations of potentially conflicting constraints. In this paper, we operationalize the notion of conflicting constraints and demonstrate how innovative automata-based techniques can be applied to monitor running process instances. Conflicting constraints are detected immediately and our toolset (realized using Declare and ProM) provides meaningful diagnostics.", "num_citations": "95\n", "authors": ["1890"]}
{"title": "Representing and monitoring social commitments using the event calculus\n", "abstract": " Multiagent social commitments provide a principled basis for agent interactions, and serve as a natural tool to resolve design ambiguities. Indeed, they have been the subject of considerable research for more than a decade. However, the take-up of the social commitments paradigm is yet to come. To explain this negative result, we pinpoint a number of shortcomings, which this article aims to address. We extend current commitment modelling languages, thus leveraging expressive possibilities that were precluded by previous formalizations. We propose a novel axiomatization of commitment operations in a first order Event Calculus framework, that accommodates reasoning with data and metric time. Finally, we illustrate how publicly available  implementations can be exploited for commitment monitoring purposes.", "num_citations": "90\n", "authors": ["1890"]}
{"title": "Discovering data-aware declarative process models from event logs\n", "abstract": " A wealth of techniques are available to automatically discover business process models from event logs. However, the bulk of these techniques yield procedural process models that may be useful for detailed analysis, but do not necessarily provide a comprehensible picture of the process. Additionally, barring few exceptions, these techniques do not take into account data attributes associated to events in the log, which can otherwise provide valuable insights into the rules that govern the process. This paper contributes to filling these gaps by proposing a technique to automatically discover declarative process models that incorporate both control-flow dependencies and data conditions. The discovered models are conjunctions of first-order temporal logic expressions with an associated graphical representation (Declare notation). Importantly, the proposed technique discovers underspecified models\u00a0\u2026", "num_citations": "90\n", "authors": ["1890"]}
{"title": "Declarative process modeling in BPMN\n", "abstract": " Traditional business process modeling notations, including the standard Business Process Model and Notation (BPMN), rely on an imperative paradigm wherein the process model captures all allowed activity flows. In other words, every flow that is not specified is implicitly disallowed. In the past decade, several researchers have exposed the limitations of this paradigm in the context of business processes with high variability. As an alternative, declarative process modeling notations have been proposed (e.g., Declare). These notations allow modelers to capture constraints on the allowed activity flows, meaning that all flows are allowed provided that they do not violate the specified constraints. Recently, it has been recognized that the boundary between imperative and declarative process modeling is not crisp. Instead, mixtures of declarative and imperative process modeling styles are sometimes\u00a0\u2026", "num_citations": "80\n", "authors": ["1890"]}
{"title": "Inducing declarative logic-based models from labeled traces\n", "abstract": " In this work we propose an approach for the automatic discovery of logic-based models starting from a set of process execution traces. The approach is based on a modified Inductive Logic Programming algorithm, capable of learning a set of declarative rules.               The advantage of using a declarative description is twofold. First, the process is represented in an intuitive and easily readable way; second, a family of proof procedures associated to the chosen language can be used to support the monitoring and management of processes (conformance testing, properties verification and interoperability checking, in particular).               The approach consists in first learning integrity constraints expressed as logical formulas and then translating them into a declarative graphical language named DecSerFlow.               We demonstrate the viability of the approach by applying it to a real dataset from a health case\u00a0\u2026", "num_citations": "80\n", "authors": ["1890"]}
{"title": "Commitment tracking via the reactive event calculus\n", "abstract": " Runtime commitment verification is an important, open issue in multiagent research. To address it, we build on Yolum and Singh's formalization of commitment operations, on Chittaro and Montanari's cached event calculus, and on the SCIFF abductive logic programming proof-procedure. We propose a framework consisting of a declarative and compact language to express the domain knowledge, and a reactive and complete procedure to track the status of commitments effectively, producing provably sound and irrevocable answers.", "num_citations": "74\n", "authors": ["1890"]}
{"title": "Description logic knowledge and action bases\n", "abstract": " Description logic Knowledge and Action Bases (KAB) are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing new objects. We resort to a variant of DL-Lite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred. Actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the knowledge base (TBox and ABox), and effects are expressed in terms of new ABoxes. In this setting, we address verification of temporal properties expressed in a variant of first-order mu-calculus with quantification across states. Notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.", "num_citations": "71\n", "authors": ["1890"]}
{"title": "Towards data-aware constraints in declare\n", "abstract": " In recent years, declarative, constraint-based approaches have been proposed to model loosely-structured business processes, mediating between support and flexibility. A notable example is the Declare framework, equipped with a graphical declarative language whose semantics can be characterized with several logic-based formalisms. Up to now, Declare constraints have been mainly used to tackle control-flow aspects, abstracting away from data. In this work, we extend Declare so as to include task data and data-aware constraints. We show how the Event Calculus (EC) formalization of Declare can be improved to deal with such extensions, and to apply a reactive EC reasoner for monitoring data-aware constraints.", "num_citations": "69\n", "authors": ["1890"]}
{"title": "Resolving inconsistencies and redundancies in declarative process models\n", "abstract": " Declarative process models define the behaviour of business processes as a set of constraints. Declarative process discovery aims at inferring such constraints from event logs. Existing discovery techniques verify the satisfaction of candidate constraints over the log, but completely neglect their interactions. As a result, the inferred constraints can be mutually contradicting and their interplay may lead to an inconsistent process model that does not accept any trace. In such a case, the output turns out to be unusable for enactment, simulation or verification purposes. In addition, the discovered model contains, in general, redundancies that are due to complex interactions of several constraints and that cannot be cured using existing pruning approaches. We address these problems by proposing a technique that automatically resolves conflicts within the discovered models and is more powerful than existing pruning\u00a0\u2026", "num_citations": "67\n", "authors": ["1890"]}
{"title": "Ontology-driven extraction of event logs from relational databases\n", "abstract": " Process mining is an emerging discipline whose aim is to discover, monitor and improve real processes by extracting knowledge from event logs representing actual process executions in a given organizational setting. In this light, it can be applied only if faithful event logs, adhering to accepted standards (such as XES), are available. In many real-world settings, though, such event logs are not explicitly given, but are instead implicitly represented inside legacy information systems of organizations, which are typically managed through relational technology. In this work, we devise a novel framework that supports domain experts in the extraction of XES event log information from legacy relational databases, and consequently enables the application of standard process mining tools on such data. Differently from previous work, the extraction is driven by a conceptual representation of the domain of interest in\u00a0\u2026", "num_citations": "60\n", "authors": ["1890"]}
{"title": "Formal verification of wastewater treatment processes using events detected from continuous signals by means of artificial neural networks. Case study: SBR plant\n", "abstract": " This paper proposes a modular architecture for the analysis and the validation of wastewater treatment processes. An algorithm using neural networks is used to extract the relevant qualitative patterns, such as \u201capexes\u201d, \u201cknees\u201d and \u201csteps\u201d, from the signals acquired in the reaction tanks. These patterns, which show changes in the signals trend, are mapped to events in the process and logged using an appropriate XML format. The logs, in turn, are considered traces of the execution of a manufacturing process and validated using tools commonly applied for the Verification of Business Processes. The system has been applied to the data collected from a Sequencing Batch Reactor (SBR) for municipal wastewater treatment, equipped with probes for the on-line acquisition of signals such as pH, oxidation--reduction potential (ORP) and dissolved oxygen (DO). A SBR has turned out to be a suitable case study since the\u00a0\u2026", "num_citations": "58\n", "authors": ["1890"]}
{"title": "Expressing and verifying business contracts with abductive logic programming\n", "abstract": " SCIFF is a declarative language, based on abductive logic programming, that accommodates forward rules, predicate definitions, and constraints over finite domain variables. Its abductive declarative semantics can be related to that of deontic operators; its operational specification is the sound and complete SCIFF proof procedure, defined as a set of transition rules implemented and integrated into a reasoning and verification tool. A variation of the SCIFF proof procedure (g-SCIFF) can be used for static verification of contract properties. The use of SCIFF for business contract specification and verification is demonstrated in a concrete scenario. Encoding of SCIFF contract rules in RuleML accommodates integration of SCIFF with architectures for business contracts.", "num_citations": "58\n", "authors": ["1890"]}
{"title": "Monitoring business metaconstraints based on LTL and LDL for finite traces\n", "abstract": " Runtime monitoring is one of the central tasks to provide operational decision support to running business processes, and check on-the-fly whether they comply with constraints and rules. We study runtime monitoring of properties expressed in LTL on finite traces (LTF                   f                 ) and its extension LDF                   f                 . LDF                   f                  is a powerful logic that captures all monadic second order logic on finite traces, which is obtained by combining regular expressions with LTF                   f                 , adopting the syntax of propositional dynamic logic (PDL). Interestingly, in spite of its greater expressivity, LDF                   f                  has exactly the same computational complexity of LTF                   f                 . We show that LDF                   f                  is able to capture, in the logic itself, not only the constraints to be monitored, but also the de-facto standard RV-LTL monitors. This makes it possible to\u00a0\u2026", "num_citations": "55\n", "authors": ["1890"]}
{"title": "Verification of data-aware commitment-based multiagent system.\n", "abstract": " In this paper we investigate multiagent systems whose agent interaction is based on social commitments that evolve over time, in presence of (possibly incomplete) data. In particular, we are interested in modeling and verifying how data maintained by the agents impact on the dynamics of such systems, and on the evolution of their commitments. This requires to lift the commitment-related conditions studied in the literature, which are typically based on propositional logics, to a first-order setting. To this purpose, we propose a rich framework for modeling data-aware commitmentbased multiagent systems. In this framework, we study verification of rich temporal properties, establishing its decidability under the condition of \u201cstate-boundedness\u201d, ie, data items come from an infinite domain but, at every time point, each agent can store only a bounded number of them.", "num_citations": "55\n", "authors": ["1890"]}
{"title": "A framework for the systematic comparison and evaluation of compliance monitoring approaches\n", "abstract": " To support the whole business process compliance lifecycle, one also needs to monitor the actual processes and not just check their design. Recently, many approaches have been proposed that utilize a broad range of constraint languages and techniques to realize compliance monitoring solutions. Due to this diversity, the comparison of existing approaches is difficult and consequently hampers the evaluation of which approaches are suitable for which application scenarios. This paper provides a framework to compare and evaluate existing compliance monitoring approaches. The framework is based on ten typical Compliance Monitoring Functionalities (CMFs). These have been derived using a systematic literature review and five case studies from different domains. Existing approaches are evaluated based on the CMF framework, resulting in a list of open questions and a discussion of new challenges in this\u00a0\u2026", "num_citations": "55\n", "authors": ["1890"]}
{"title": "Social commitments in time: Satisfied or compensated\n", "abstract": " We define a framework based on computational logic technology and on a reactive axiomatization of the Event Calculus to formalize the evolution of commitments in time. We propose a new characterization of commitments with time that enables a rich modeling of the domain, various forms of reasoning, and run-time and static verification.", "num_citations": "53\n", "authors": ["1890"]}
{"title": "Db-nets: On the marriage of colored petri nets and relational databases\n", "abstract": " The integrated management of business processes and master data is being increasingly considered as a fundamental problem, by both the academia and the industry. In this position paper, we focus on the foundations of the problem, arguing that contemporary approaches struggle to find a suitable equilibrium between data- and process-related aspects. We then propose a new formal model, called db-nets, that balances such two pillars through the marriage of colored Petri nets and relational databases. We invite the research community to build on this new model, discussing in particular its potential in conceptual modeling, formal verification, and simulation.", "num_citations": "51\n", "authors": ["1890"]}
{"title": "Checking compliance of execution traces to business rules\n", "abstract": " Complex and flexible business processes are critical not only because they are difficult to handle, but also because they often tend to loose their intelligibility. Verifying compliance of complex and flexible processes becomes therefore a fundamental requirement. We propose a framework for performing compliance checking of process execution traces w.r.t.\u00a0expressive reactive business rules, tailored to the MXML meta-model. Rules are mapped to Logic Programming, using Prolog to classify execution traces as compliant/non-compliant. We show how different rule templates, inspired by the ConDec language, can be easily specified and then customized in the context of a real industrial case study. We finally describe how the proposed language and its underlying a-posteriori reasoning technique have been concretely implemented as a ProM analysis plug-in.", "num_citations": "51\n", "authors": ["1890"]}
{"title": "Add data into business process verification: Bridging the gap between theory and practice\n", "abstract": " The need to extend business process languages with the capability to model complex data objects along with the control flow perspective has lead to significant practical and theoretical advances in the field of Business Process Modeling (BPM). On the practical side, there are several suites for control flow and data modeling; nonetheless, when it comes to formal verification, the data perspective is abstracted away due to the intrinsic difficulty of handling unbounded data. On the theoretical side, there is significant literature providing decidability results for expressive data-aware processes. However, they struggle to produce a concrete impact as being far from real BPM architectures and, most of all, not providing actual verification tools. In this paper we aim at bridging such a gap: we provide a concrete framework which, on the one hand, being based on Petri Nets and relational models, is close to the widely used BPM suites, and on the other is grounded on solid formal basis which allow to perform formal verification tasks. Moreover, we show how to encode our framework in an action language so as to perform reachability analysis using virtually any state-of-the-art planner.", "num_citations": "50\n", "authors": ["1890"]}
{"title": "An operational decision support framework for monitoring business constraints\n", "abstract": " Only recently, process mining techniques emerged that can be used for Operational decision Support (OS), i.e., knowledge extracted from event logs is used to handle running process instances better. In the process mining tool ProM, a generic OS service has been developed that allows ProM to dynamically interact with an external information system, receiving streams of events and returning meaningful insights on the running process instances. In this paper, we present the implementation of a novel business constraints monitoring framework on top of the ProM OS service. We discuss the foundations of the monitoring framework considering two logic-based approaches, tailored to Linear Temporal Logic on finite traces and the Event Calculus.", "num_citations": "46\n", "authors": ["1890"]}
{"title": "Ontology-Based Data Access for Extracting Event Logs from Legacy Data: The onprom Tool\u00a0and Methodology\n", "abstract": " Process mining aims at discovering, monitoring, and improving business processes by extracting knowledge from event logs. In this respect, process mining can be applied only if there are proper event logs that are compatible with accepted standards, such as extensible event stream (XES). Unfortunately, in many real world set-ups, such event logs are not explicitly given, but instead are implicitly represented in legacy information systems. In this work, we exploit a framework and associated methodology for the extraction of XES event logs from relational data sources that we have recently introduced. Our approach is based on describing logs by means of suitable annotations of a conceptual model of the available data, and builds on the ontology-based data access (OBDA) paradigm for the actual log extraction. Making use of a real-world case study in the services domain, we compare our novel approach\u00a0\u2026", "num_citations": "41\n", "authors": ["1890"]}
{"title": "Ontology-based governance of data-aware processes\n", "abstract": " In this paper we show how one can use the technology developed recently for Ontology-Based Data Access (OBDA) to govern data-aware processes through ontologies. In particular, we consider processes executed over a relational database which issue calls to external services to acquire new information and update the data.We equip these processes with an OBDA system, in which an ontology modeling the domain of interest is connected through declarative mappings to the database, and that consequently allows one to understand and govern the manipulated information at the conceptual level. In this setting, we are interested in verifying first-order \u03bc-calculus formulae specifying temporal properties over the evolution of the information at the conceptual level. Specifically, we show how, building on first-order rewritability of queries over the system state that is typical of OBDA, we are able to reformulate\u00a0\u2026", "num_citations": "40\n", "authors": ["1890"]}
{"title": "A logic-based, reactive calculus of events\n", "abstract": " Since its introduction, the Event Calculus (\u2130\ud835\udc9e) has been recognized for being an excellent framework to reason about time and events, and it has been applied to a variety of domains. However, its formalization inside logic-based frameworks has been mainly based on backward, goal-oriented reasoning: given a narrative (also called execution trace) and a goal, logic-based formalizations of \u2130\ud835\udc9e focus on proving the goal, ie, establishing if a property (called fluent) holds. These approaches are therefore unsuitable in dynamic environments, where the narrative typically evolves over time: indeed, each occurrence of a new event requires to restart the reasoning process from scratch. Ad-hoc, procedural methods and implementations have been then proposed to overcome this issue. However, they lack a strong formal basis and cannot guarantee formal properties. As a consequence, the applicability of \u2130\ud835\udc9e has been\u00a0\u2026", "num_citations": "38\n", "authors": ["1890"]}
{"title": "Verification from declarative specifications using logic programming\n", "abstract": " In recent years, the declarative programming philosophy has had a visible impact on new emerging disciplines, such as heterogeneous multi-agent systems and flexible business processes. We address the problem of formal verification for systems specified using declarative languages, focusing in particular on the Business Process Management field. We propose a verification method based on the g-SCIFF abductive logic programming proof procedure and evaluate our method empirically, by comparing its performance with that of other verification frameworks.", "num_citations": "38\n", "authors": ["1890"]}
{"title": "Verification of choreographies during execution using the reactive event calculus\n", "abstract": " This article presents a run-time verification method of web service behaviour with respect to choreographies. We start from DecSerFlow as a graphical choreography description language. We select a core set of DecSerFlow elements and formalize them using a reactive version of the Event Calculus, based on the computational logic SCIFF framework. Our choice enables us to enrich DecSerFlow and the Event Calculus with quantitative time constraints and to model compensation actions.", "num_citations": "37\n", "authors": ["1890"]}
{"title": "An abductive framework for a-priori verification of web services\n", "abstract": " Although stemming from very different research areas, Multi-Agent Systems (MAS) and Service Oriented Computing (SOC) share common topics, problems and settings. One of the common problems is the need to formally verify the conformance of individuals (Agents or Web Services) to common rules and specifications (resp. Protocols/Choreographies), in order to provide a coherent behaviour and to reach the goals of the user. In previous publications, we developed a framework, SCIFF, for the automatic verification of compliance of agents to protocols. The framework includes a language based on abductive logic programming and on constraint logic programming for formally defining the social rules; suitable proof-procedures to check on-the-fly and a-priori the compliance of agents to protocols have been defined. Building on our experience in the MAS area, in this paper we make a first step towards the formal\u00a0\u2026", "num_citations": "37\n", "authors": ["1890"]}
{"title": "Multi-party business process compliance monitoring through IoT-enabled artifacts\n", "abstract": " Monitoring the compliance of the execution of multi-party business processes is a complex and challenging task: each actor only has the visibility of the portion of the process under its direct control, and the physical objects that belong to a party are often manipulated by other parties. Because of that, there is no guarantee that the process will be executed \u2014 and the objects be manipulated \u2014 as previously agreed by the parties.The problem is usually addressed through a centralized monitoring entity that collects information, sent by the involved parties, on when activities are executed and the artifacts are altered. This paper aims to tackle the problem in a different and innovative way: it proposes a decentralized solution based on the switch from control- to artifact-based monitoring, where the physical objects can monitor their own conditions and the activities in which they participate.To do so, the Internet of Things\u00a0\u2026", "num_citations": "36\n", "authors": ["1890"]}
{"title": "Verification of artifact-centric systems: Decidability and modeling issues\n", "abstract": " Artifact-centric business processes have recently emerged as an approach in which processes are centred around the evolution of business entities, called artifacts, giving equal importance to control-flow and data. The recent Guard-State-Milestone (GSM) framework provides means for specifying business artifacts lifecycles in a declarative manner, using constructs that match how executive-level stakeholders think about their business. However, it turns out that formal verification of GSM is undecidable even for very simple propositional temporal properties. We attack this challenging problem by translating GSM into a well-studied formal framework.We exploit this translation to isolate an interesting class of \u201cstate-bounded\u201d GSM models for which verification of sophisticated temporal properties is decidable. We then introduce some guidelines to turn an arbitrary GSM model into a state-bounded, verifiable\u00a0\u2026", "num_citations": "36\n", "authors": ["1890"]}
{"title": "Monitoring data-aware business constraints with finite state automata\n", "abstract": " Checking the compliance of a business process execution with respect to a set of regulations is an important issue in several settings. A common way of representing the expected behavior of a process is to describe it as a set of business constraints. Runtime verification and monitoring facilities allow us to continuously determine the state of constraints on the current process execution, and to promptly detect violations at runtime. A plethora of studies has demonstrated that in several settings business constraints can be formalized in terms of temporal logic rules. However, in virtually all existing works the process behavior is mainly modeled in terms of control-flow rules, neglecting the equally important data perspective. In this paper, we overcome this limitation by presenting a novel monitoring approach that tracks streams of process events (that possibly carry data) and verifies if the process execution is compliant\u00a0\u2026", "num_citations": "35\n", "authors": ["1890"]}
{"title": "Testing careflow process execution conformance by translating a graphical language to computational logic\n", "abstract": " Careflow systems implement workflow concepts in the clinical domain in order to administer, support and monitor the execution of health care services performed by different health care professionals and structures. In this work we focus on the monitoring aspects and propose a solution for the conformance verification of careflow process executions.               Given a careflow model, we have defined an algorithm capable of translating it to a formal language based on computational logic and abductive logic programming in particular. The main advantage of this formalism lies in its operational proof-theoretic counterpart, which is able to verify the conformance of a given careflow process execution (in the form of an event log) w.r.t. the model.               The feasibility of the approach has been tested on a case study related to the careflow process described in the cervical cancer screening protocol.", "num_citations": "33\n", "authors": ["1890"]}
{"title": "Conformance checking of executed clinical guidelines in presence of basic medical knowledge\n", "abstract": " Clinical Guidelines (CGs) capture medical evidence, but are not meant to deal with single patients\u2019 peculiarities and specific context limitations and/or constraints. In practice, the physician has to exploit basic medical knowledge (BMK) in order to adapt the general CG to the specific case at hand. The interplay between CG knowledge and BMK can be very complex. In this paper, we explore such interaction from the viewpoint of the conformance problem, intended as the adherence of an observed CG execution trace to both types of knowledge. We propose an approach based on the GLARE language to represent CGs, and on an homogeneous formalization of both CGs and BMK using Event Calculus (EC) and its Prolog-based implementation , focusing on \u201ca posteriori\u201d conformance evaluation.", "num_citations": "31\n", "authors": ["1890"]}
{"title": "Reactive event calculus for monitoring global computing applications\n", "abstract": " In 1986 Kowalski and Sergot proposed a logic-based formalism named Event Calculus (EC), for specifying in a declarative manner how the happening of events affects some representation (the state) of the world. Since its introduction, EC has been recognized for being an excellent framework to reason about time and events. Recently, with the advent of complex software systems decomposed into sets of autonomous, heterogeneous distributed entities, EC has drawn attention as a viable solution for monitoring them, where monitoring means to represent their state and how events dynamically affect such state.               In this work we present the fundamentals of a reactive and logic-based version of EC, named REC, for monitoring declarative properties, while maintaining a solid formal background. We present some results about its formal as well as practical aspects, and discuss how REC has been\u00a0\u2026", "num_citations": "28\n", "authors": ["1890"]}
{"title": "Abductive logic programming as an effective technology for the static verification of declarative business processes\n", "abstract": " We discuss the static verification of declarative Business Processes. We identify four desiderata about verifiers, and propose a concrete framework which satisfies them. The framework is based on the ConDec graphical notation for modeling Business Processes, and on Abductive Logic Programming technology for verification of properties. Empirical evidence shows that our verification method seems to perform and scale better, in most cases, than other state of the art techniques (model checkers, in particular). A detailed study of our framework\u2019s theoretical properties proves that our approach is sound and complete when applied to ConDec models that do not contain loops, and it is guaranteed to terminate when applied to models that contain loops.", "num_citations": "28\n", "authors": ["1890"]}
{"title": "Computational logic for run-time verification of web services choreographies: Exploiting the socs-si tool\n", "abstract": " In this work, we investigate the feasibility of using a framework based on computational logic, and mainly defined in the context of Multi-Agent Systems for Global Computing (SOCS UE Project), for modeling choreographies of Web Services with respect to the conversational aspect.               One of the fundamental motivations of using computational logic, beside its declarative and highly expressive nature, is given by its operational counterpart, that can provide a proof-theoretic framework able to verify the consistency of services designed in a cooperative and incremental manner.               In particular, in this paper we show that suitable \u201cSocial Integrity Constraints\u201d, introduced in the SOCS social model, can be used for specifying global protocols at the choreography level. In this way, we can use a suitable tool, derived from the proof-procedure defined in the context of the SOCS project, to check at run-time\u00a0\u2026", "num_citations": "28\n", "authors": ["1890"]}
{"title": "Formal modeling and SMT-based parameterized verification of data-aware BPMN\n", "abstract": " We propose DAB \u2013 a data-aware extension of BPMN where the process operates over case and persistent data (partitioned into a read-only database called catalog and a read-write database called repository). The model trades off between expressiveness and the possibility of supporting parameterized verification of safety properties on top of it. Specifically, taking inspiration from the literature on verification of artifact systems, we study verification problems where safety properties are checked irrespectively of the content of the read-only catalog, and accepting the potential presence of unboundedly many tuples in the catalog and repository. We tackle such problems using an array-based backward reachability procedure fully implemented in MCMT \u2013 a state-of-the-art array-based SMT model checker. Notably, we prove that the procedure is sound and complete for checking safety of DABs, and single out\u00a0\u2026", "num_citations": "27\n", "authors": ["1890"]}
{"title": "Verifiable UML artifact-centric business process models\n", "abstract": " Artifact-centric business process models have gained increasing momentum recently due to their ability to combine structural (ie, data related) with dynamical (ie, process related) aspects. In particular, two main lines of research have been pursued so far: one tailored to business artifact modeling languages and methodologies, the other focused on the foundations for their formal verification. In this paper, we merge these two lines of research, by showing how recent theoretical decidability results for verification can be fruitfully transferred to a concrete UML-based modeling methodology. In particular, we identify additional steps in the methodology that, in significant cases, guarantee the possibility of verifying the resulting models against rich first-order temporal properties. Notably, our results can be seamlessly transferred to different languages for the specification of the artifact lifecycles.", "num_citations": "27\n", "authors": ["1890"]}
{"title": "Ensuring model consistency in declarative process discovery\n", "abstract": " Declarative process models define the behaviour of business processes as a set of constraints. Declarative process discovery aims at inferring such constraints from event logs. Existing discovery techniques verify the satisfaction of candidate constraints over the log, but completely neglect their interactions. As a result, the inferred constraints can be mutually contradicting and their interplay may lead to an inconsistent process model that does not accept any trace. In such a case, the output turns out to be unusable for enactment, simulation or verification purposes. In addition, the discovered model contains, in general, redundancies that are due to complex interactions of several constraints and that cannot be solved using existing pruning approaches. We address these problems by proposing a technique that automatically resolves conflicts within the discovered models and is more powerful than existing\u00a0\u2026", "num_citations": "25\n", "authors": ["1890"]}
{"title": "Recency-bounded verification of dynamic database-driven systems\n", "abstract": " We propose a formalism to model database-driven systems, called database manipulating systems (DMS). The actions of a (DMS) modify the current instance of a relational database by adding new elements into the database, deleting tuples from the relations and adding tuples to the relations. The elements which are modified by an action are chosen by (full) first-order queries.(DMS) is a highly expressive model and can be thought of as a succinct representation of an infinite state relational transition system, in line with similar models proposed in the literature. We propose monadic second order logic (MSO-FO) to reason about sequences of database instances appearing along a run. Unsurprisingly, the linear-time model checking problem of (DMS) against (MSO-FO) is undecidable. Towards decidability, we propose under-approximate model checking of (DMS), where the under-approximation parameter is the\"\u00a0\u2026", "num_citations": "25\n", "authors": ["1890"]}
{"title": "Web service contracting: Specification and reasoning with SCIFF\n", "abstract": " The semantic web vision will facilitate automation of many tasks, including the location and dynamic reconfiguration of web services. In this article, we are concerned with a specific stage of web service location, called, by some authors, contracting. We address contracting both at the operational level and at the semantic level. We present a framework encompassing communication and reasoning, in which web services exchange and evaluate goals and policies. Policies represent behavioural interfaces. The reasoning procedure at the core of the framework is based on the abductive logic programming SCIFF proof-procedure. We describe the framework, show by examples how to formalise policies in the declarative language of SCIFF, and give the framework a model-theoretic and a sound proof-theoretic semantics.", "num_citations": "25\n", "authors": ["1890"]}
{"title": "An hybrid architecture integrating forward rules with fuzzy ontological reasoning\n", "abstract": " In recent years there has been a growing interest in the combination of rules and ontologies. Notably, many works have focused on the theoretical aspects of such integration, sometimes leading to concrete solutions. However, solutions proposed so far typically reason upon crisp concepts, while concrete domains require also fuzzy expressiveness.             In this work we combine mature technologies, namely the Drools business rule management system, the Pellet OWL Reasoner and the FuzzyDL system, to provide a unified framework for supporting fuzzy reasoning. After extending the Drools framework (language and engine) to support uncertainty reasoning upon rules, we have integrated it with custom operators that (i) exploit Pellet to perform ontological reasoning, and (ii) exploit FuzzyDL to support fuzzy ontological reasoning.             As a case study, we consider a decision-support system for the\u00a0\u2026", "num_citations": "24\n", "authors": ["1890"]}
{"title": "First-order \u03bc-calculus over generic transition systems and applications to the situation calculus\n", "abstract": " We consider \u03bc L, \u03bc L a, and \u03bc L p, three variants of the first-order \u03bc-calculus studied in verification of data-aware processes, that differ in the form of quantification on objects across states. Each of these three logics has a distinct notion of bisimulation. We show that the three notions collapse for generic dynamic systems, which include all state-based systems specified using a logical formalism, eg, the situation calculus. Hence, for such systems, \u03bc L, \u03bc L a, and \u03bc L p have the same expressive power. We also show that, when the dynamic system stores only a bounded number of objects in each state (eg, for bounded situation calculus action theories), a finite abstraction can be constructed that is faithful for \u03bc L (the most general variant), yielding decidability of verification. This contrasts with the undecidability for first-order ltl, and notably implies that first-order ltl cannot be captured by \u03bc L.", "num_citations": "23\n", "authors": ["1890"]}
{"title": "Object-centric behavioral constraints: integrating data and declarative process modelling\n", "abstract": " Object-centric behavioral constraints: integrating data and declarative process modelling \u2014 Eindhoven University of Technology research portal Skip to main navigation Skip to search Skip to main content Eindhoven University of Technology research portal Logo Help & FAQ English Nederlands Home Researchers Research output Organisational units Activities Projects Prizes Press / Media Facilities / Equipment Datasets Courses Research areas Student theses Search by expertise, name or affiliation Object-centric behavioral constraints: integrating data and declarative process modelling W. van der Aalst, A. Artale, M. Montali, S. Tritini Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution \u203a Academic \u203a peer-review 5 Citations (Scopus) 60 Downloads (Pure) Overview Original language English Title of host publication Proceedings of the 30th International Workshop on \u2026", "num_citations": "23\n", "authors": ["1890"]}
{"title": "Process mining manifesto\n", "abstract": " Process mining techniques are able to extract knowledge from event logs commonly available in today\u2019s information systems. These techniques provide new means to discover, monitor, and improve processes in a variety of application domains. There are two main drivers for the growing interest in process mining. On the one hand, more and more events are being recorded, thus, providing detailed information about the history of processes. On the other hand, there is a need to improve and support business processes in competitive and rapidly changing environments. This manifesto is created by the IEEE Task Force on Process Mining and aims to promote the topic of process mining. Moreover, by defining a set of guiding principles and listing important challenges, this manifesto hopes to serve as a guide for software developers, scientists, consultants, business managers, and end-users. The goal is to increase the maturity of process mining as a new tool to improve the (re) design, control, and support of operational business processes.", "num_citations": "23\n", "authors": ["1890"]}
{"title": "Compliance checking of cancer-screening careflows: an approach based on computational logic\n", "abstract": " Clinical guidelines and Careflow systems have been recently identified as a means to improve and standardize health care services. A number of ICT-based management solutions have been proposed, focussing on several aspects such as specification, process logs verification with respect to specification (compliance), enactment and administration of careflows.", "num_citations": "23\n", "authors": ["1890"]}
{"title": "A holistic approach for soundness verification of decision-aware process models\n", "abstract": " The last decade has witnessed an increasing transformation in the design, engineering, and mining of processes, moving from a pure control-flow perspective to more integrated models where also data and decisions are explicitly considered. This calls for methods and techniques able to ascertain the correctness of such integrated models. Differently from previous approaches, which mainly focused on the local interplay between decisions and their corresponding outgoing branches, we introduce a holistic approach to verify the end-to-end soundness of a Petri net-based process model, enriched with case data and decisions. In addition, we present an effective, implemented technique that verifies soundness by translating the input net into a colored Petri net with bounded color sets, on which standard state space analysis techniques are subsequently applied. Experiments on real life illustrate the\u00a0\u2026", "num_citations": "22\n", "authors": ["1890"]}
{"title": "Soundness of data-aware, case-centric processes\n", "abstract": " In recent years, a plethora of foundational results and corresponding techniques and tools has been developed to support the modeling, analysis, execution and improvement of business processes along their entire lifecycle. A major shortcoming of the analysis techniques is that they solely focus on the control-flow dimension of the process, omitting how business objects (i.e., cases) and their data affect and are manipulated by process instances and their tasks. In this work, we aim at filling this gap. We recast the classical notion of case-centric business process in a data-aware context. An emitter action is used to generate new cases, and while a case flows through the process control-flow, corresponding data are created, updated, and deleted by operating over a full-fledged relational database with constraints. To make our investigation concrete, we ground it on the recently introduced framework of data\u00a0\u2026", "num_citations": "21\n", "authors": ["1890"]}
{"title": "Verification and synthesis in description logic based dynamic systems\n", "abstract": " In this paper, we devise a general framework for formalizing Description Logic Based Dynamic Systems that is parametric w.r.t. the description logic knowledge base and the progression mechanism of interest. Using this framework we study verification and adversarial synthesis for specifications expressed in a variant of first-order \u03bc-calculus, with a controlled form of quantification across successive states. We provide key decidability results for both verification and synthesis, under a \u201cbounded-state\u201d assumption. We then study two instantiations of this general framework, where the description logic knowledge base is expressed in DL-Lite and , respectively", "num_citations": "21\n", "authors": ["1890"]}
{"title": "A hybrid approach to clinical guideline and to basic medical knowledge conformance\n", "abstract": " Several computer-based approaches to Clinical Guidelines have been developed in the last two decades. However, only recently the community has started to cope with the fact that Clinical Guidelines are just a part of the medical knowledge that physicians have to take into account when treating patients. The procedural knowledge in the guidelines have to be complemented by additional declarative medical knowledge. In this paper, we analyse such an interaction, by studying the conformance problem, defined as evaluating the adherence of a set of performed clinical actions w.r.t. the behaviour recommended by the guideline and by the medical knowledge.", "num_citations": "21\n", "authors": ["1890"]}
{"title": "Petri Nets with Parameterised Data\n", "abstract": " During the last decade, various approaches have been put forward to integrate business processes with different types of data. Each of these approaches reflects specific demands in the whole process-data integration spectrum. One particularly important point is the capability of these approaches to flexibly accommodate processes with multiple cases that need to co-evolve. In this work, we introduce and study an extension of coloured Petri nets, called catalog-nets, providing two key features to capture this type of processes. On the one hand, net transitions are equipped with guards that simultaneously inspect the content of tokens and query facts stored in a read-only, persistent database. On the other hand, such transitions can inject data into tokens by extracting relevant values from the database or by generating genuinely fresh ones. We systematically encode catalog-nets into one of the reference frameworks\u00a0\u2026", "num_citations": "20\n", "authors": ["1890"]}
{"title": "From model completeness to verification of data aware processes\n", "abstract": " Model Completeness is a classical topic in model-theoretic algebra, and its inspiration sources are areas like algebraic geometry and field theory. Yet, recently, there have been remarkable applications in computer science: these applications range from combined decision procedures for satisfiability and interpolation, to connections between temporal logic and monadic second order logic and to model-checking. In this paper we mostly concentrate on the last one: we study verification over a general model of so-called artifact-centric systems, which are used to capture business processes by giving equal important to the control-flow and data-related aspects. In particular, we are interested in assessing (parameterized) safety properties irrespectively of the initial database instance. We view such artifact systems as array-based systems, establishing a correspondence with model checking based on\u00a0\u2026", "num_citations": "20\n", "authors": ["1890"]}
{"title": "Semantic DMN: Formalizing decision models with domain knowledge\n", "abstract": " The Decision Model and Notation (DMN) is a recent OMG standard for the elicitation and representation of decision models. DMN builds on the notion of decision table, which consists of columns representing the inputs and outputs of a decision, and rows denoting rules. DMN models work under the assumption of complete information, and do not support integration with background domain knowledge. In this paper, we overcome these issues, by proposing decision knowledge bases (DKBs), where decisions are modeled in DMN, and domain knowledge is captured by means of first-order logic with datatypes. We provide a logic-based semantics for such an integration, and formalize how the different DMN reasoning tasks introduced in the literature can be lifted to DKBs. We then consider the case where background knowledge is expressed as an  description logic ontology equipped with datatypes\u00a0\u2026", "num_citations": "20\n", "authors": ["1890"]}
{"title": "Object-centric behavioral constraints\n", "abstract": " Today's process modeling languages often force the analyst or modeler to straightjacket real-life processes into simplistic or incomplete models that fail to capture the essential features of the domain under study. Conventional business process models only describe the lifecycles of individual instances (cases) in isolation. Although process models may include data elements (cf. BPMN), explicit connections to real data models (e.g., an entity relationship model or a UML class model) are rarely made. Therefore, we propose a novel approach that extends data models with a behavioral perspective. Data models can easily deal with many-to-many and one-to-many relationships. This is exploited to create process models that can also model complex interactions between different types of instances. Classical multiple-instance problems are circumvented by using the data model for event correlation. The declarative nature of the proposed language makes it possible to model behavioral constraints over activities like cardinality constraints in data models. The resulting object-centric behavioral constraint (OCBC) model is able to describe processes involving interacting instances and complex data dependencies. In this paper, we introduce the OCBC model and notation, providing a number of examples that give a flavour of the approach. We then define a set-theoretic semantics exploiting cardinality constraints within and across time points. We finally formalize conformance checking in our setting, arguing that evaluating conformance against OCBC models requires diagnostics that go beyond what is provided by contemporary conformance checking\u00a0\u2026", "num_citations": "20\n", "authors": ["1890"]}
{"title": "Verification of Description Logic Knowledge and Action Bases.\n", "abstract": " We introduce description logic (DL) Knowledge and Action Bases (KAB), a mechanism that provides both a semantically rich representation of the information on the domain of interest in terms of a DL KB and a set of actions to change such information over time, possibly introducing new objects. We resort to a variant of DL-Lite where UNA is not enforced and where equality between objects may be asserted and inferred. Actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the KB (TBox and ABox), and effects are expressed in terms of new ABoxes. We address the verification of temporal properties expressed in a variant of first-order \u03bc-calculus where a controlled form of quantification across states is allowed. Notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.", "num_citations": "20\n", "authors": ["1890"]}
{"title": "Model checking Petri nets with names using data-centric dynamic systems\n", "abstract": " Petri nets with name creation and management (-PNs) have been recently introduced as an expressive model for dynamic (distributed) systems, whose dynamics are determined not only by how tokens flow in the system, but also by the pure names they carry. On the one hand, this extension makes the resulting nets strictly more expressive than P/T nets: they can be exploited to capture a plethora of interesting systems, such as distributed systems enriched with channels and name passing, service interaction with correlation mechanisms, and resource-constrained workflow nets that explicitly account for process instances. On the other hand, fundamental properties like coverability, termination and boundedness are decidable for -PNs. In this work, we go one step beyond the verification of such general properties, and provide decidability and undecidability results of model checking -PNs against variants\u00a0\u2026", "num_citations": "19\n", "authors": ["1890"]}
{"title": "State-boundedness in data-aware dynamic systems\n", "abstract": " Verification of dynamic systems that manipulate data, stored in a database or ontology, has lately received increasing attention. A plethora of recent works has shown that verification of systems working over unboundedly many data is decidable even for very rich temporal properties, provided that the system is state-bounded. This condition requires the existence of an overall bound on the amount of data stored in each single state along the system evolution. In general, checking state-boundedness is undecidable. An open question is whether it is possible to isolate significant classes of dynamic systems for which state-boundedness is decidable. In this paper we provide a strong negative answer, by resorting to a novel connection with variants of Petri nets. In particular, we show undecidability for systems whose data component contains unary relations only, and whose action component queries and updates such relations in a very limited way. To contrast this result, we propose interesting relaxations of the sufficient conditions proposed in the concrete setting of Data-Centric Dynamic Systems, building on recent results on chase termination for tuple-generating dependencies.", "num_citations": "19\n", "authors": ["1890"]}
{"title": "SMT-based verification of data-aware processes: a model-theoretic approach\n", "abstract": " In recent times, satisfiability modulo theories (SMT) techniques gained increasing attention and obtained remarkable success in model-checking infinite-state systems. Still, we believe that whenever more expressivity is needed in order to specify the systems to be verified, more and more support is needed from mathematical logic and model theory. This is the case of the applications considered in this paper: we study verification over a general model of relational, data-aware processes, to assess (parameterized) safety properties irrespectively of the initial database (DB) instance. Toward this goal, we take inspiration from array-based systems and tackle safety algorithmically via backward reachability. To enable the adoption of this technique in our rich setting, we make use of the model-theoretic machinery of model completion, which surprisingly turns out to be an effective tool for verification of relational systems\u00a0\u2026", "num_citations": "18\n", "authors": ["1890"]}
{"title": "Model completeness, covers and superposition\n", "abstract": " In ESOP 2008, Gulwani and Musuvathi introduced a notion of cover and exploited it to handle infinite-state model checking problems. Motivated by applications to the verification of data-aware processes, we show how covers are strictly related to model completions, a well-known topic in model theory. We also investigate the computation of covers within the Superposition Calculus, by adopting a constrained version of the calculus, equipped with appropriate settings and reduction strategies.", "num_citations": "17\n", "authors": ["1890"]}
{"title": "OBDA for log extraction in process mining\n", "abstract": " Process mining is an emerging area that synergically combines model-based and data-oriented analysis techniques to obtain useful insights on how business processes are executed within an organization. Through process mining, decision makers can discover process models from data, compare expected and actual behaviors, and enrich models with key information about their actual execution. To be applicable, process mining techniques require the input data to be explicitly structured in the form of an event log, which lists when and by whom different case objects (i.e., process instances) have been subject to the execution of tasks. Unfortunately, in many real world set-ups, such event logs are not explicitly given, but are instead implicitly represented in legacy information systems. To apply process mining in this widespread setting, there is a pressing need for techniques able to support various process\u00a0\u2026", "num_citations": "17\n", "authors": ["1890"]}
{"title": "Modeling and reasoning over declarative data-aware processes with object-centric behavioral constraints\n", "abstract": " Existing process modeling notations ranging from Petri nets to BPMN have difficulties capturing the data manipulated by processes. Process models often focus on the control flow, lacking an explicit, conceptually well-founded integration with real data models, such as ER diagrams or UML class diagrams. To overcome this limitation, Object-Centric Behavioral Constraints (OCBC) models were recently proposed as a new notation that combines full-fledged data models with control-flow constraints inspired by declarative process modeling notations such as DECLARE and DCR Graphs. We propose a formalization of the OCBC model using temporal description logics. The obtained formalization allows us to lift all reasoning services defined for constraint-based process modeling notations without data, to the much more sophisticated scenario of OCBC. Furthermore, we show how reasoning over OCBC\u00a0\u2026", "num_citations": "16\n", "authors": ["1890"]}
{"title": "Verification of relational multiagent systems with data types\n", "abstract": " We study the extension of relational multiagent systems (RMASs), where agents manipulate full-fledged relational databases, with data types and facets equipped with domain-specific, rigid relations (such as total orders). Specifically, we focus on design-time verification of RMASs against rich first-order temporal properties expressed in a variant of first-order mu-calculus with quantification across states. We build on previous decidability results under the state-bounded assumption, ie, in each single state only a bounded number of data objects is stored in the agent databases, while unboundedly many can be encountered over time. We recast this condition, showing decidability in presence of dense, linear orders, and facets defined on top of them. Our approach is based on the construction of a finite-state, sound and complete abstraction of the original system, in which dense linear orders are reformulated as non-rigid relations working on the active domain of the system only. We also show undecidability when including a data type equipped with the successor relation.", "num_citations": "16\n", "authors": ["1890"]}
{"title": "Engineering and verifying agent-oriented requirements augmented by business constraints with  -Tropos\n", "abstract": " We propose -Tropos as a modeling framework to support agent-oriented systems engineering, from high-level requirements elicitation down to execution-level tasks. In particular, we show how -Tropos extends the Tropos methodology by means of declarative business constraints, inspired by the ConDec graphical language. We demonstrate the functioning of -Tropos using a running example inspired by a real-world industrial scenario, and we describe how -Tropos models can be automatically formalized in computational logic, discussing formal properties of the resulting framework and its verification capabilities.", "num_citations": "16\n", "authors": ["1890"]}
{"title": "-Tropos\n", "abstract": " The work presented in this paper stands at the intersection of three diverse research areas: agent-oriented early requirements engineering, business process requirements elicitation and specification, and computational logic-based specification and verification. The analysis of business requirements and the specification of business processes are fundamental steps in the development of information systems. The first part of this paper presents -Tropos as a way to combine business goals and requirements with the business process model. -Tropos enhances a well-known agent-oriented early requirements engineering framework with declarative business process-oriented constructs, inspired by the DecSerFlow and ConDec languages. In the second part of the paper, we show a mapping of -Tropos onto CIFF, a computational logic-based framework for properties and conformance verification.", "num_citations": "16\n", "authors": ["1890"]}
{"title": "Analysis of the GLARE and GPROVE Approaches to Clinical Guidelines\n", "abstract": " Clinical guidelines (GLs) play an important role in medical practice, and computerized support to GLs is now one of the most central areas of research in Artificial Intelligence in medicine. In recent years, many groups have developed different computer-assisted management systems of GL. Each approach has its own peculiarities and thus a comparison is necessary. Many possible aspects can be analyzed, but a first analysis has probably to consider the GL models, i.e. the representation formalisms provided. To this end, Peleg and al. [4] have analyzed and compared six different frameworks. In this paper, we analyse also GLARE and GPROVE on the basis of the same methodology. Moreover, we extend such analysis by considering the tools and the facilities that GLARE and GPROVE provide to support the use of GLs. The final goal of our analysis is to exploit the differences between these two systems\u00a0\u2026", "num_citations": "15\n", "authors": ["1890"]}
{"title": "A Computational Logic Application Framework for Service Discovery and Contracting\n", "abstract": " In Semantic Web technologies, searching for a service means identifying components that can potentially satisfy user needs in terms of inputs and outputs (discovery) and devise a fruitful interaction with the customer (contracting). In this paper, the authors present an application framework that encompasses both the discovery and the contracting steps in a unified search process. In particular, the authors accommodate service discovery by ontology-based reasoning and contracting by reasoning about behavioural interfaces, published in a formal language. To this purpose, the authors consider a formal approach grounded on Computational Logic. They define, illustrate, and evaluate a framework, called SCIFF Reasoning Engine (SRE), which can establish if a Semantic Web Service and a requester can fruitfully inter-operate, by computing a possible interaction plan based on the behavioural interfaces of both. The\u00a0\u2026", "num_citations": "14\n", "authors": ["1890"]}
{"title": "Integrating abductive logic programming and description logics in a dynamic contracting architecture\n", "abstract": " In semantic Web technologies, searching for a service means to identify components that can potentially satisfy the user needs in terms of outputs and effects (discovery), and that, when invoked by the customer, can fruitfully interact with her (contracting). In this paper, we present an application framework that encompasses both the discovery and the contracting steps, in a unified search process. In particular, we accommodate service discovery by ontology-based reasoning, and contracting by automated reasoning about policies published in a formal language. To this purpose, we consider a formal approach grounded on computational logic, and abductive logic programming in particular. We propose a framework, called SCIFF reasoning engine, able to establish, by ontological and abductive reasoning, if a semantic Web service and a requester can fruitfully inter-operate, taking as input the behavioral interfaces of\u00a0\u2026", "num_citations": "14\n", "authors": ["1890"]}
{"title": "Towards a decserflow declarative semantics based on computational logic\n", "abstract": " In this paper we exploit a computational logic-based framework, called SCIFF, for the formalization of DecSerFlow. DecSerFlow is a graphical, extendible high-level language for the declarative specification of service flows, and is grounded on LTL. SCIFF was originally developed in the context of the SOCS european project, where we addressed the issue of providing a formal language to define and verify interaction protocols in open environments. More specifically, in this work we show that SCIFF is concretely able to formalize the DecSerFlow core template formulae, tackling two complementary issues: on one hand, it is possible to specify SCIFF rules by using an intuitive and user-friendly graphical language; on the other hand, a DecSerFlow model may be grounded not only on LTL but also on an abductive framework, acquiring some new advantages and features. Finally, we propose to extend DecSerFlow by exploiting some useful features of the SCIFF framework, like for example the explicit notion of time, which could be used to specify temporal constraints and deadlines.", "num_citations": "14\n", "authors": ["1890"]}
{"title": "Formalizing application integration patterns\n", "abstract": " Enterprise Integration Patterns (EIPs) and their extensions denote the informally described building blocks of current Enterprise Application Integration (EAI) systems. Although a recent approach strives to provide an EIP formalization based on Coloured Petri Nets (CPNs), it does not completely consider EAI requirements, such as complex data, transacted resources and time. In the absence of a comprehensive formal definition, the patterns cannot be verified, and thus a formal foundation of EAI is missing. In this work, we leverage the novel db-net approach that finds a better balance between the data and process-related aspects than CPNs and we extend it according to the EAI requirements that we systematically collect on a pattern level. Then we discuss pattern realizations, and evaluate our approach for comprehensiveness, test correctness, and show its applicability.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Verification of data-aware processes via array-based systems (extended version)\n", "abstract": " We study verification over a general model of artifact-centric systems, to assess (parameterized) safety properties irrespectively of the initial database instance. We view such artifact systems as array-based systems, which allows us to check safety by adapting backward reachability, establishing for the first time a correspondence with model checking based on Satisfiability-Modulo-Theories (SMT). To do so, we make use of the model-theoretic machinery of model completion, which surprisingly turns out to be an effective tool for verification of relational systems, and represents the main original contribution of this paper. In this way, we pursue a twofold purpose. On the one hand, we reconstruct (restricted to safety) the essence of some important decidability results obtained in the literature for artifact-centric systems, and we devise a genuinely novel class of decidable cases. On the other, we are able to exploit SMT technology in implementations, building on the well-known MCMT model checker for array-based systems, and extending it to make all our foundational results fully operational.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Plan synthesis for knowledge and action bases\n", "abstract": " We study plan synthesis for a variant of Knowledge and Action Bases (KABs), a rich, dynamic framework, where states are description logic (DL) knowledge bases (KBs) whose extensional part is manipulated by actions that possibly introduce new objects from an infinite domain. We show that plan existence over KABs is undecidable even under severe restrictions. We then focus on state-bounded KABs, a class for which plan existence is decidable, and provide sound and complete plan synthesis algorithms, which combine techniques based on standard planning, DL query answering, and finitestate abstraction. All results hold for any DL with decidable query answering. We finally show that for lightweight DLs, plan synthesis can be compiled into standard ADL planning.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Verification of semantically-enhanced artifact systems\n", "abstract": " Artifact-Centric systems have emerged in the last years as a suitable framework to model business-relevant entities, by combining their static and dynamic aspects. In particular, the Guard-Stage-Milestone (GSM) approach has been recently proposed to model artifacts and their lifecycle in a declarative way. In this paper, we enhance GSM with a Semantic Layer, constituted by a fullfledged OWL 2 QL ontology linked to the artifact information models through mapping specifications. The ontology provides a conceptual view of the domain under study, and allows one to understand the evolution of the artifact system at a higher level of abstraction. In this setting, we present a technique to specify temporal properties expressed over the Semantic Layer, and verify them according to the evolution in the underlying GSM model. This technique has been implemented in a tool that exploits state-of-the-art ontology\u00a0\u2026", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Verification of inconsistency-aware knowledge and action bases\n", "abstract": " Description Logic Knowledge and Action Bases (KABs) have been recently introduced as a mechanism that provides a semantically rich representation of the information on the domain of interest in terms of a DL KB and a set of actions to change such information over time, possibly introducing new objects. In this setting, decidability of verification of sophisticated temporal properties over KABs, expressed in a variant of first-order mu-calculus, has been shown. However, the established framework treats inconsistency in a simplistic way, by rejecting inconsistent states produced through action execution. We address this problem by showing how inconsistency handling based on the notion of repairs can be integrated into KABs, resorting to inconsistency-tolerant semantics. In this setting, we establish decidability and complexity of verification.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Event condition expectation (ECE-) rules for monitoring observable systems\n", "abstract": " The standardization and broad adoption of Service Oriented Architectures, Web Services, and Cloud Computing is raising the complexity of ICT systems. Hence, assuring correct system behavior with regard to established design and business constraints is of the utmost importance. Run-time monitoring, where the outcomes of an observed system are continuously checked against what is expected of it, is one possible approach to providing the required oversight.               In this paper, we discuss this notion of rule expectations, their violation and/or fulfillment, and use these concepts to define the concept of an Event-Condition-Expectation (ECE-) rule, a variation of the traditional Event-Condition-Action rule pattern. To demonstrate these concepts, we present extensions to the syntax used by the production rule engine, Drools, and describe their use in a medical case study. The clinical decision support\u00a0\u2026", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Monitoring time-aware social commitments with reactive event calculus\n", "abstract": " Despite their dynamic nature, social commitments have been rarely used for monitoring purposes. Few attention has been paid to the relationship between commitments and the temporal dimension and to the corresponding run-time verification. Building on previous work, we present a declarative axiomatization of time-aware social commitments, extending their basic life cycle with timerelated transitions and with compensation mechanisms. The formalization is based on a reactive version of the Event Calculus, able to monitor the commitments evolution during a system\u2019s execution, checking if the interacting agents are honoring them or not.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "On the integration of declarative choreographies and Commitment-based agent societies into the SCIFF logic programming framework\n", "abstract": " The definition of choreography specification languages for Service Oriented Systems poses important challenges. Mainstream approaches tend to focus on procedural aspects, leading to over-constrained and over-specified models. Because of such a drawback, declarative languages are gaining popularity as a better way to model service choreographies. A similar issue was met in the Multi-Agent Systems domain, where declarative approaches based on social semantics have been used to capture the nature of agent interaction without over-constraining their behaviour.", "num_citations": "13\n", "authors": ["1890"]}
{"title": "Verifying the manipulation of data objects according to business process and data models\n", "abstract": " Business processes read and write data objects, usually stored in databases. Although data models and activity-oriented business process models originate from different paradigms, they need to work together properly. The data object states are transformed during each process instance by the activities of the process model. It is therefore necessary to verify whether the states of the data objects are correct according to the process model, and to discover the states of the stored data objects. This implies determining the relation between the data objects stored in the database, the data objects involved in the process, and the activities that within the business process that create the data objects and modify their states.  In order to verify the business process annotated with data states and to reduce the existing gap between data model and business process model, we propose a methodology that includes enlarging the\u00a0\u2026", "num_citations": "12\n", "authors": ["1890"]}
{"title": "Formal reasoning on natural language descriptions of processes\n", "abstract": " The existence of unstructured information that describes processes represents a challenge in organizations, mainly because this data cannot be directly referred into process-aware ecosystems due to ambiguities. Still, this information is important, since it encompasses aspects of a process that are left out when formalizing it on a particular modelling notation. This paper picks up this challenge and faces the problem of ambiguities by acknowledging its existence and mitigating it. Specifically, we propose a framework to partially automate the elicitation of a formal representation of a textual process description, via text annotation techniques on top of natural language processing. The result is the ATDP language, whose syntax and semantics are described in this paper. ATDP allows to explicitly cope with several interpretations of the same textual description of a process model. Moreover, we link the ATDP\u00a0\u2026", "num_citations": "12\n", "authors": ["1890"]}
{"title": "A framework for defining and verifying clinical guidelines: A case study on cancer screening\n", "abstract": " Medical guidelines are clinical behaviour recommendations used to help and support physicians in the definition of the most appropriate diagnosis and/or therapy within determinate clinical circumstances. Due to the intrinsic complexity of such guidelines, their application is not a trivial task; hence it is important to verify if health-care workers behave in a conform manner w.r.t. the intended model, and to evaluate how much their behaviour differs.               In this paper we present the GPROVE framework that we are developing within a regional project to describe medical guidelines in a visual way and to automatically perform the conformance verification.", "num_citations": "12\n", "authors": ["1890"]}
{"title": "Implementing and running data-centric dynamic systems\n", "abstract": " Data- and artifact-centric business processes are gaining momentum due to their ability of explicitly capturing the interplay between the process control-flow and the manipulated data. In this paper, we rely on the framework of Data-Centric Dynamic Systems (DCDSs), which has been recently introduced for the formal specification and verification of data-centric processes, and we discuss how it can be realized into a prototype system which is able to enact processes comprising human actors, services and data. This reference implementation exploits the natural correspondence between DCDSs and state-of-the-art rule engines, e.g., JBoss Drools, and present the interesting feature that the model used for analysis and verification is fully aligned with the one adopted for the execution.", "num_citations": "11\n", "authors": ["1890"]}
{"title": "Fuzzy conformance checking of observed behaviour with expectations\n", "abstract": " In some different research fields a research issue has been to establish if the external, observed behaviour of an entity is conformant to some rules/specifications/expectations. Research areas like Multi Agent Systems, Business Process, and Legal/Normative systems, have proposed different characterizations of the same problem, named as the conformance problem. Most of the available systems, however, provide only simple yes/no answers to the conformance issue.               In this paper we introduce the idea of a gradual conformance, expressed in fuzzy terms. To this end, we present a system based on a fuzzy extension of Drools, and exploit it to perform conformance tests. In particular, we consider two aspects: the first related to fuzzy ontological aspects, and the second about fuzzy time-related aspects. Moreover, we discuss how to conjugate the fuzzy contributions from these aspects to get a single\u00a0\u2026", "num_citations": "11\n", "authors": ["1890"]}
{"title": "Soundness verification of decision-aware process models with variable-to-variable conditions\n", "abstract": " In recent years, there has been an increasing interest in enriching the traditional control-flow perspective of processes with additional dimensions, the data perspective being the most prominent one. At the same time, variants of Petri nets with data have been extensively studied, giving raise to a plethora of formal models with different expressive power and computational guarantees. In this work, we focus on DPNs, a data-aware extension of P/T nets where the net is enriched with data variables of different types, and transitions are guarded by formulae that inspect and update such variables. Even though DPNs are less expressive than Petri nets where data are carried by tokens, they elegantly capture business processes operating over simple case data and taking complex decisions based on these data. Notably, various techniques have been implemented to discover DPNs from event data. However, such\u00a0\u2026", "num_citations": "10\n", "authors": ["1890"]}
{"title": "On the relevance of a business constraint to an event log\n", "abstract": " Declarative process modeling languages such as declare describe the behavior of processes by means of constraints. Such constraints exert rules on the execution of tasks upon the execution of other tasks called activations. The constraint is thus fulfilled both if it is activated and the consequent rule is respected, or if it is not activated at all. The latter case, named vacuous satisfaction, is clearly less interesting than the former. Such a distinction becomes of utmost importance in the context of declarative process mining techniques, where processes are analyzed based on the identification of the most relevant constraints valid in an event log. Unfortunately, this notion of relevance has never been formally defined, and all the proposals existing in the literature use ad-hoc definitions that are only applicable to a pre-defined set of constraint patterns. This makes existing declarative process mining techniques inapplicable\u00a0\u2026", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Semantical vacuity detection in declarative process mining\n", "abstract": " A large share of the literature on process mining based on declarative process modeling languages, like declare, relies on the notion of constraint activation to distinguish between the case in which a process execution recorded in event data \u201cvacuously\u201d satisfies a constraint, or satisfies the constraint in an \u201cinteresting way\u201d. This fine-grained indicator is then used to decide whether a candidate constraint supported by the analyzed event log is indeed relevant or not. Unfortunately, this notion of relevance has never been formally defined, and all the proposals existing in the literature use ad-hoc definitions that are only applicable to a pre-defined set of constraint patterns. This makes existing declarative process mining technique inapplicable when the target constraint language is extensible and may contain formulae that go beyond pre-defined patterns. In this paper, we tackle this hot, open challenge and\u00a0\u2026", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Verification of generalized inconsistency-aware knowledge and action bases\n", "abstract": " Knowledge and Action Bases (KABs) have been put forward as a semantically rich representation of a domain, using a DL KB to account for its static aspects, and actions to evolve its extensional part over time, possibly introducing new objects. Recently, KABs have been extended to manage inconsistency, with ad-hoc verification techniques geared towards specific semantics. This work provides a twofold contribution along this line of research. On the one hand, we enrich KABs with a high-level, compact action language inspired by Golog, obtaining so called Golog-KABs (GKABs). On the other hand, we introduce a parametric execution semantics for GKABs, so as to elegantly accomodate a plethora of inconsistency-aware semantics based on the notion of repair. We then provide several reductions for the verification of sophisticated first-order temporal properties over inconsistency-aware GKABs, and show that it can be addressed using known techniques, developed for standard KABs.", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Monitoring time-aware commitments within agent-based simulation environments\n", "abstract": " Despite their dynamic nature, social commitments have rarely been used for monitoring purposes. Little attention has been paid to the relationship between commitments and the temporal dimension and to the corresponding run-time verification. Building on previous work, we present a declarative axiomatization of time-aware social commitments, extending their basic life cycle with time-related transitions and compensation mechanisms. The formalization is based on a reactive version of the event calculus, able to monitor the commitment's evolution during a system's execution, to check whether the interacting agents are honoring them or not. The resulting monitoring framework can be used in the context of agent-based simulation, either to dynamically evaluate whether a running simulation is compliant with a commitment-based contract or to provide useful information to the interacting agents, helping them to\u00a0\u2026", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Policy-based reasoning for smart web service interaction\n", "abstract": " We present a vision of smart, goal-oriented web services that reason about other services\u2019 policies and evaluate the possibility of future interactions. To achieve our vision, we propose a proof theoretic approach. We assume web services whose interface behaviour is specified in terms of reactive rules. Such rules can be made public, in order for other web services to answer the following question:\u201cis it possible to inter-operate with a given web service and achieve a given goal?\u201d In this article we focus on the underlying reasoning process, and we propose a declarative and operational abductive logic programming-based framework, called WAVe.", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Protocol Specification and Verification by Using Computational Logic.\n", "abstract": " The aim of this paper is to report on some preliminary results obtained in the context of the MASSIVE research project (http://www. di. unito. it/massive/) relating the formal specification and verification of protocols in some different application field. A protocol is a way to express the right behavior of entities involved in a (possibly complex and distributed) process. The formalism to be used for protocol description should be as intuitive as possible, but it should be also formally defined, in order to allow formal checks both on the features of the protocol itself (eg termination), and also on the execution of it. To this purpose, we will show some results obtained by exploiting the SOCS\u2212 SI logic-based framework for the specification and the verification of protocols in various applicative fields such as electronic commerce, medicine and elearning. We will also present a new graphical notation to express medical guidelines, which could be automatically translated into the SOCS formalism.", "num_citations": "10\n", "authors": ["1890"]}
{"title": "Implementing data-centric dynamic systems over a relational DBMS\n", "abstract": " We base our work on a model called data-centric dynamic system (DCDS)[1], which can be seen as a framework for modeling and verification of systems where both the process controlling the dynamics and the manipulation of data are equally central. More specifically, a DCDS consists of a data layer and a process layer, interacting as follows: the data layer stores all the data of interest in a relational database, and the process layer modifies and evolves such data by executing actions under the control of a process, possibly injecting into the system external data retrieved through service calls. In this work, we propose an implementation of DCDSs in which all aspects concerning not only the data layer but also the process layer, are realized by means of functionalities provided by a relational DBMS. We present the architecture of our prototype system, describe its functionality, and discuss the next steps we intend to take towards realizing a full-fledged DCDS-based system that supports verification of rich temporal properties.", "num_citations": "9\n", "authors": ["1890"]}
{"title": "Semantic enrichment of gsm-based artifact-centric models\n", "abstract": " We provide a comprehensive framework for semantic GSM artifacts, discuss in detail its properties, and present main software engineering architectures it is able to capture. The distinguishing aspect of our framework is that it allows for expressing both the data and the lifecycle schema of GSM artifacts in terms of an ontology, i.e., a shared and formalized conceptualization of the domain of interest. To guide the modeling of data and lifecycle we provide an upper ontology, which is specialized in each artifact with specific lifecycle elements, relations, and business objects. The framework thus obtained allows to achieve several advantages. On the one hand, it makes the specification of conditions on data and artifact status attribute fully declarative and enables semantic reasoning over them. On the other, it fosters the monitoring of artifacts and the interoperation and cooperation among different artifact systems\u00a0\u2026", "num_citations": "9\n", "authors": ["1890"]}