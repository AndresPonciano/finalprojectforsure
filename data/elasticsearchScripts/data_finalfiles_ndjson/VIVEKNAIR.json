{"title": "Arrow: Low-level augmented bayesian optimization for finding the best cloud vm\n", "abstract": " With the advent of big data applications, which tend to have longer execution time, choosing the right cloud VM has significant performance and economic implications. For example, in our large-scale empirical study of 107 different workloads on three popular big data systems, we found that a wrong choice can lead to a 20 times slowdown or an increase in cost by 10 times. Bayesian optimization is a technique for optimizing expensive (black-box) functions. Previous work has only used instance-level information (such as core counts and memory size) which is not sufficient to represent the search space. In this work, we discover that this may lead to the fragility problem-either incurs high search cost or finds only the sub-optimal solution. The central insight of this paper is to use low-level performance information to augment the process of Bayesian Optimization. Our novel low-level augmented Bayesian Optimization\u00a0\u2026", "num_citations": "56\n", "authors": ["248"]}
{"title": "\u201cSampling\u201d as a baseline optimizer for search-based software engineering\n", "abstract": " Increasingly, Software Engineering (SE) researchers use search-based optimization techniques to solve SE problems with multiple conflicting objectives. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. An alternative approach, proposed in this paper, is to start with a very large population and sample down to just the better solutions. We call this method \u201cSway\u201d, short for \u201cthe sampling way\u201d. This paper compares Sway versus state-of-the-art search-based SE tools using seven models: five software product line models; and two other software process control models (concerned with project management, effort estimation, and selection of requirements) during incremental agile development. For these models, the experiments of this paper show that Sway is competitive with corresponding state-of-the-art evolutionary\u00a0\u2026", "num_citations": "42\n", "authors": ["248"]}
{"title": "Scout: An experienced guide to find the best cloud configuration\n", "abstract": " Finding the right cloud configuration for workloads is an essential step to ensure good performance and contain running costs. A poor choice of cloud configuration decreases application performance and increases running cost significantly. While Bayesian Optimization is effective and applicable to any workloads, it is fragile because performance and workload are hard to model (to predict). In this paper, we propose a novel method, SCOUT. The central insight of SCOUT is that using prior measurements, even those for different workloads, improves search performance and reduces search cost. At its core, SCOUT extracts search hints (inference of resource requirements) from low-level performance metrics. Such hints enable SCOUT to navigate through the search space more efficiently---only spotlight region will be searched. We evaluate SCOUT with 107 workloads on Apache Hadoop and Spark. The experimental results demonstrate that our approach finds better cloud configurations with a lower search cost than state of the art methods. Based on this work, we conclude that (i) low-level performance information is necessary for finding the right cloud configuration in an effective, efficient and reliable way, and (ii) a search method can be guided by historical data, thereby reducing cost and improving performance.", "num_citations": "31\n", "authors": ["248"]}
{"title": "Beyond Evolutionary Algorithms for Search-based Software Engineering\n", "abstract": " ContextEvolutionary algorithms typically require large number of evaluations (of solutions) to converge \u2013 which can be very slow and expensive to evaluate.ObjectiveTo solve search-based software engineering (SE) problems, using fewer evaluations than evolutionary methods.MethodInstead of mutating a small population, we build a very large initial population which is then culled using a recursive bi-clustering chop approach. We evaluate this approach on multiple SE models, unconstrained as well as constrained, and compare its performance with standard evolutionary algorithms.ResultsUsing just a few evaluations (under 100), we can obtain comparable results to state-of-the-art evolutionary algorithms.ConclusionJust because something works, and is widespread use, does not necessarily mean that there is no value in seeking methods to improve that method. Before undertaking search-based SE\u00a0\u2026", "num_citations": "25\n", "authors": ["248"]}
{"title": "Micky: A Cheaper Alternative for Selecting Cloud Instances\n", "abstract": " Most cloud computing optimizers explore and improve one workload at a time. When optimizing many workloads, the single-optimizer approach can be prohibitively expensive. Accordingly, we examine \"collective optimizer\" that concurrently explore and improve a set of workloads significantly reducing the measurement costs. Our large-scale empirical study shows that there is often a single cloud configuration which is surprisingly near-optimal for most workloads. Consequently, we create a collective-optimizer, MICKY, that reformulates the task of finding the near-optimal cloud configuration as a multi-armed bandit problem. MICKY efficiently balances exploration (of new cloud configurations) and exploitation (of known good cloud configuration). Our experiments show that MICKY can achieve on average 8.6 times reduction in measurement cost as compared to the state-of-the-art method while finding near-optimal\u00a0\u2026", "num_citations": "22\n", "authors": ["248"]}
{"title": "Whence to Learn? Transferring Knowledge in Configurable Systems using BEETLE\n", "abstract": " As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use Transfer Learning to \u201ctransfer\u201d old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer\u00a0\u2026", "num_citations": "18\n", "authors": ["248"]}
{"title": "An (Accidental) Exploration of Alternatives to Evolutionary Algorithms for SBSE\n", "abstract": " SBSE researchers often use an evolutionary algorithm to solve various software engineering problems. This paper explores an alternate approach of sampling. This approach is called SWAY (Samplying WAY) and finds the (near) optimal solutions to the problem by (i) creating a larger initial population and (ii) intelligently sampling the solution space to find the best subspace. Unlike evolutionary algorithms, SWAY does not use mutation or cross-over or multi-generational reasoning to find interesting subspaces but relies on the underlying dimensions of the solution space. Experiments with Software Engineering (SE) models shows that SWAY\u2019s performance improvement is competitive with standard MOEAs while, terminating over an order of magnitude faster.", "num_citations": "13\n", "authors": ["248"]}
{"title": "Is one hyperparameter optimizer enough?\n", "abstract": " Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics. To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be \u201cbest\u201d and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations.", "num_citations": "10\n", "authors": ["248"]}
{"title": "Uncoercibility in e-voting and eauctioning mechanisms using deniable encryption\n", "abstract": " The uncoercibility to prevent rigging in e-voting and e-auction have been studied in different literatures. It is realized that the notion of a virtual booth and untappable channel are required to prevent coerciveness. Virtual booth protects the candidates to cast their private values without being observed by the adversary/coercer. However the adversary can influence the candidates after their casting. Adversary used to acquire the encrypted votes/bids either from the colluded authorities (voting server, auctioneer) or by eavesdropping the communicating channel and coerces the candidates to disclose their private values with the private keys and verifies whether the ciphers are the encryption of the private values. In the prior literatures of e-voting and e-auctioning, threshold-encryption and receipt-free mechanism are used to prevent the coercion and collusion respectively. But they assumed untappable channel to restrict eavesdropping. However, practically untappable channel is difficult to achieve. It should be a dedicated trusted link or continuous fiber link to implement untappable channel. In this paper we present an alternative of untappable channel using deniable encryption. An encryption scheme is deniable if the sender can formulate \u2018fake random choice\u2019that will make the cipher text \u2018look like\u2019an encryption of a different plaintext, thus keeping the real plaintext private. Deniable encryption does not restrict the adversary to eavesdrop, but if the candidates are coerced, they are able to formulate a different value f v and can convince the adversary that the ciphers are the encryption of f v, without revealing the true private value r v. Therefore\u00a0\u2026", "num_citations": "10\n", "authors": ["248"]}
{"title": "Routing scheme for OBS networks\n", "abstract": " We observe that routing, in optical burst switching (OBS) networks, is basically motivated by reducing contention-induced loss and not by the objective of global resource optimization. Here, we propose a routing scheme for OBS networks that minimizes the burst loss probability of individual bursts and at the same time achieves the routing goal of maximization of the minimum unutilized bandwidth of links in the network. All OBS nodes periodically exchange their scheduler-state information, and hence all nodes learn the topology of the network along with the occupancy states of each link. Based on this information the end-to-end wavelength availability states of the routes are computed, and bursts are then routed through the path that is likely to give minimum loss. The scheme can be applied in both source-routing and hop-by-hop-routing modes with provision for deflection routing in each mode. We studied the\u00a0\u2026", "num_citations": "6\n", "authors": ["248"]}
{"title": "An integrated routing and offset-time adaptation scheme for OBS network\n", "abstract": " Optical burst switching (OBS) is considered as one of the most potential technology for implementation of transparent optical Internet in near future. In this paper we propose an integrated scheme for loss reduction and efficient resource utilization in OBS. We develop a framework for computation of estimated space-time loss surface in space-time plane for a burst over the network and utilise it to determine the most appropriate route and corresponding offset-time for the burst. The integrated routing and offset-time adaptation scheme proposed here, can simultaneously offer significantly lower burst loss rate and higher network-wide resource utilisation.", "num_citations": "3\n", "authors": ["248"]}
{"title": "A family of flexible offset-time based wavelength schedulers for obs edge-nodes\n", "abstract": " A family of variable offset-time based wavelength scheduling schemes for OBS edge-nodes, is proposed which performs 4-20 times better than ordinary LAUC-VF schedulers in terms of byte loss rate (ByLR). The schedulers accept a minimum and a maximum limit of offset-time along with the length of the burst as input and then return the appropriate wavelength and the exact offset-time, to be used for the burst. The basic principle is to align the burst either at the beginning or at the end of an existing void so that no new void is created after scheduling the burst unless mandated by the offset-time limits.", "num_citations": "3\n", "authors": ["248"]}
{"title": "Ontology based session management protocol for teleteaching domain\n", "abstract": " Teaching has always been a face to face interaction and hence requires physical presence of both teacher and the student. Teleteaching has been a revolution since it introduces teaching to geographic independence hence is a distributed system. Teleteaching as in any collaborative environment is very difficult to design. Before any implementation we need to model the system, here we have used ontology as to create a level of abstraction which broadly defines a model and the dependencies of activities, hence being useful for system to interact and helpful in the development process.", "num_citations": "2\n", "authors": ["248"]}
{"title": "Deniable encryption in replacement of untappable channel to prevent coercion\n", "abstract": " The incoerciblety to prevent rigging in e-voting and e-auction have been studied in different literatures. It is realized that the notion of a virtual booth and untappable channel are required to prevent coerciveness. Virtual booth protects the candidates to cast their private values without being observed by the adversary/coercer. However the adversary can influence the candidates after their casting. Adversary used to acquire the encrypted votes/bids either from the colluded authorities (voting server, auctioneer) or by eavesdropping the communicating channel. The adversary then coerces the candidates to disclose their private values with their private keys and verifies whether the ciphers are the encryption of the private values. In the prior literatures of e-voting and e-auctioning, threshold-encryption and receipt-free mechanism are used to prevent the coercion and collusion respectively. But they assumed\u00a0\u2026", "num_citations": "1\n", "authors": ["248"]}