{"title": "BitBlaze: A new approach to computer security via binary analysis\n", "abstract": " In this paper, we give an overview of the BitBlaze project, a new approach to computer security via binary analysis. In particular, BitBlaze focuses on building a unified binary analysis platform and using it to provide novel solutions to a broad spectrum of different security problems. The binary analysis platform is designed to enable accurate analysis, provide an extensible architecture, and combines static and dynamic analysis as well as program verification techniques to satisfy the common needs of security applications. By extracting security-related properties from binary programs directly, BitBlaze enables a principled, root-cause based approach to computer security, offering novel and effective solutions, as demonstrated with over a dozen different security applications.", "num_citations": "978\n", "authors": ["297"]}
{"title": "Polyglot: Automatic extraction of protocol message format using dynamic binary analysis\n", "abstract": " Protocol reverse engineering, the process of extracting the application-level protocol used by an implementation, without access to the protocol specification, is important for many network security applications. Recent work [17] has proposed protocol reverse engineering by using clustering on network traces. That kind of approach is limited by the lack of semantic information on network traces. In this paper we propose a new approach using program binaries. Our approach, shadowing, uses dynamic analysis and is based on a unique intuition-the way that an implementation of the protocol processes the received application data reveals a wealth of information about the protocol message format. We have implemented our approach in a system called Polyglot and evaluated it extensively using real-world implementations of five different protocols: DNS, HTTP, IRC, Samba and ICQ. We compare our results with the\u00a0\u2026", "num_citations": "510\n", "authors": ["297"]}
{"title": "Measuring pay-per-install: The commoditization of malware distribution\n", "abstract": " Recent years have seen extensive diversification of the \u201cunderground economy\u201d associated with malware and the subversion of Internet-connected systems. This trend towards specialization has compelling forces driving it: miscreants readily apprehend that tackling the entire value-chain from malware creation to monetization in the presence of ever-evolving countermeasures poses a daunting task requiring highly developed skills and resources. As a result, entrepreneurial-minded miscreants have formed pay-per-install (PPI) services\u2014specialized organizations that focus on the infection of victims\u2019 systems.In this work we perform a measurement study of the PPI market by infiltrating four PPI services. We develop infrastructure that enables us to interact with PPI services and gather and classify the resulting malware executables distributed by the services. Using our infrastructure, we harvested over a million client executables using vantage points spread across 15 countries. We find that of the world\u2019s top 20 most prevalent families of malware, 12 employ PPI services to buy infections. In addition we analyze the targeting of specific countries by PPI clients, the repacking of executables to evade detection, and the duration of malware distribution.", "num_citations": "359\n", "authors": ["297"]}
{"title": "Dispatcher: Enabling active botnet infiltration using automatic protocol reverse-engineering\n", "abstract": " Automatic protocol reverse-engineering is important for many security applications, including the analysis and defense against botnets. Understanding the command-and-control (C&C) protocol used by a botnet is crucial for anticipating its repertoire of nefarious activity and to enable active botnet infiltration. Frequently, security analysts need to rewrite messages sent and received by a bot in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation. To enable such rewriting, we need detailed information about the intent and structure of the messages in both directions of the communication despite the fact that we generally only have access to the implementation of one endpoint, namely the bot binary. Current techniques cannot enable such rewriting. In this paper, we propose techniques to extract the format of protocol messages sent by an application\u00a0\u2026", "num_citations": "325\n", "authors": ["297"]}
{"title": "AVclass: A Tool for Massive Malware Labeling\n", "abstract": " Labeling a malicious executable as a variant of a known family is important for security applications such as triage, lineage, and for building reference datasets in turn used for evaluating malware clustering and training malware classification approaches. Oftentimes, such labeling is based on labels output by antivirus engines. While AV labels are well-known to be inconsistent, there is often no other information available for labeling, thus security analysts keep relying on them. However, current approaches for extracting family information from AV labels are manual and inaccurate. In this work, we describe AVclass, an automatic labeling tool that given the AV labels for a, potentially massive, number of samples outputs the most likely family names for each sample. AVclass implements novel automatic techniques to address 3 key challenges: normalization, removal of generic tokens, and alias detection. We\u00a0\u2026", "num_citations": "323\n", "authors": ["297"]}
{"title": "Manufacturing compromise: the emergence of exploit-as-a-service\n", "abstract": " We investigate the emergence of the exploit-as-a-service model for driveby browser compromise. In this regime, attackers pay for an exploit kit or service to do the\" dirty work\" of exploiting a victim's browser, decoupling the complexities of browser and plugin vulnerabilities from the challenges of generating traffic to a website under the attacker's control. Upon a successful exploit, these kits load and execute a binary provided by the attacker, effectively transferring control of a victim's machine to the attacker.", "num_citations": "267\n", "authors": ["297"]}
{"title": "Binary Code Extraction and Interface Identification for Security Applications\n", "abstract": " Binary code reutilization is the process of automatically identifying the interface and extracting the instructions and data dependencies of a code fragment from an executable program, so that it is self-contained and can be reused by external code. Binary code reutilization is useful for a number of security applications, including reusing the proprietary cryptographic or unpacking functions from a malware sample and for rewriting a network dialog. In this paper we conduct the first systematic study of automated binary code reutilization and its security applications. The main challenge in binary code reutilization is understanding the code fragments interface. We propose a novel technique to identify the prototype of an undocumented code fragment directly from the programs binary, without access to source code or symbol information. Further, we must also extract the code itself from the binary so that it is self-contained and can be easily reused in another program. We design and implement a tool that uses a combination of dynamic and static analysis to automatically identify the prototype and extract the instructions of an assembly function into a form that can be reused by other C code. The extracted function can be run independently of the rest of the programs functionality and shared with other users. We apply our approach to scenarios that include extracting the encryption and decryption routines from malware samples, and show that these routines can be reused by a network proxy to decrypt encrypted traffic on the network. This allows the network proxy to rewrite the malwares encrypted traffic by combining the extracted encryption and\u00a0\u2026", "num_citations": "167\n", "authors": ["297"]}
{"title": "Towards automatic discovery of deviations in binary implementations with applications to error detection and fingerprint generation\n", "abstract": " Different implementations of the same protocol specification usually contain deviations, ie, differences in how they check and process some of their inputs. Deviations are commonly introduced as implementation errors or as different interpretations of the same specification. Automatic discovery of these deviations is important for several applications. In this paper, we focus on automatic discovery of deviations for two particular applications: error detection and fingerprint generation. We propose a novel approach for automatically detecting deviations in the way different implementations of the same specification check and process their input. Our approach has several advantages:(1) by automatically building symbolic formulas from the implementation, our approach is precisely faithful to the implementation;(2) by solving formulas created from two different implementations of the same specification, our approach significantly reduces the number of inputs needed to find deviations;(3) our approach works on binaries directly, without access to the source code. We have built a prototype implementation of our approach and have evaluated it using multiple implementations of two different protocols: HTTP and NTP. Our results show that our approach successfully finds deviations between different implementations, including errors in input checking, and differences in the interpretation of the specification, which can be used as fingerprints.", "num_citations": "154\n", "authors": ["297"]}
{"title": "Secure content sniffing for web browsers, or how to stop papers from reviewing themselves\n", "abstract": " Cross-site scripting defenses often focus on HTML documents, neglecting attacks involving the browser's content-sniffing algorithm, which can treat non-HTML content as HTML. Web applications, such as the one that manages this conference, must defend themselves against these attacks or risk authors uploading malicious papers that automatically submit stellar self-reviews. In this paper, we formulate content-sniffing XSS attacks and defenses. We study content-sniffing XSS attacks systematically by constructing high-fidelity models of the content-sniffing algorithms used by four major browsers. We compare these models with Web site content filtering policies to construct attacks. To defend against these attacks, we propose and implement a principled content-sniffing algorithm that provides security while maintaining compatibility. Our principles have been adopted, in part, by Internet Explorer 8 and, in full, by\u00a0\u2026", "num_citations": "137\n", "authors": ["297"]}
{"title": "The attack of the clones: A study of the impact of shared code on vulnerability patching\n", "abstract": " Vulnerability exploits remain an important mechanism for malware delivery, despite efforts to speed up the creation of patches and improvements in software updating mechanisms. Vulnerabilities in client applications (e.g., Browsers, multimedia players, document readers and editors) are often exploited in spear phishing attacks and are difficult to characterize using network vulnerability scanners. Analyzing their lifecycle requires observing the deployment of patches on hosts around the world. Using data collected over 5 years on 8.4 million hosts, available through Symantec's WINE platform, we present the first systematic study of patch deployment in client-side vulnerabilities. We analyze the patch deployment process of 1,593 vulnerabilities from 10 popular client applications, and we identify several new threats presented by multiple installations of the same program and by shared libraries distributed with\u00a0\u2026", "num_citations": "126\n", "authors": ["297"]}
{"title": "Firma: Malware clustering and network signature generation with mixed network behaviors\n", "abstract": " The ever-increasing number of malware families and polymorphic variants creates a pressing need for automatic tools to cluster the collected malware into families and generate behavioral signatures for their detection. Among these, network traffic is a powerful behavioral signature and network signatures are widely used by network administrators. In this paper we present FIRMA, a tool that given a large pool of network traffic obtained by executing unlabeled malware binaries, generates a clustering of the malware binaries into families and a set of network signatures for each family. Compared with prior tools, FIRMA produces network signatures for each of the network behaviors of a family, regardless of the type of traffic the malware uses (e.g., HTTP, IRC, SMTP, TCP, UDP). We have implemented FIRMA and evaluated it on two recent datasets comprising nearly 16,000 unique malware binaries. Our\u00a0\u2026", "num_citations": "124\n", "authors": ["297"]}
{"title": "Insights from the inside: A view of botnet management from infiltration\n", "abstract": " Recent work has leveraged botnet infiltration techniques to track the activities of bots over time, particularly with regard to spam campaigns. Building on our previous success in reverseengineering C&C protocols, we have conducted a 4-month infiltration of the MegaD botnet, beginning in October 2009. Our infiltration provides us with constant feeds on MegaD\u2019s complex and evolving C&C architecture as well as its spam operations, and provides an opportunity to analyze the botmasters\u2019 operations. In particular, we collect significant evidence on the MegaD infrastructure being managed by multiple botmasters. Further, FireEye\u2019s attempt to shutdown MegaD on Nov. 6, 2009, which occurred during our infiltration, allows us to gain an inside view on the takedown and how MegaD not only survived it but bounced back with significantly greater vigor.In addition, we present new techniques for mining information about botnet C&C architecture:\u201cGoogle hacking\u201d to dig out MegaD C&C servers and \u201cmilking\u201d C&C servers to extract not only the spectrum of commands sent to bots but the C&C\u2019s overall structure. The resulting overall picture then gives us insight into MegaD\u2019s management structure, its complex and evolving C&C architecture, and its ability to withstand takedown.", "num_citations": "117\n", "authors": ["297"]}
{"title": "Driving in the cloud: An analysis of drive-by download operations and abuse reporting\n", "abstract": " Drive-by downloads are the preferred distribution vector for many malware families. In the drive-by ecosystem many exploit servers run the same exploit kit and it is a challenge understanding whether the exploit server is part of a larger operation. In this paper we propose a technique to identify exploit servers managed by the same organization. We collect over time how exploit servers are configured and what malware they distribute, grouping servers with similar configurations into operations. Our operational analysis reveals that although individual exploit servers have a median lifetime of 16 hours, long-lived operations exist that operate for several months. To sustain long-lived operations miscreants are turning to the cloud, with 60% of the exploit servers hosted by specialized cloud hosting services. We also observe operations that distribute multiple malware families and that pay-per-install affiliate\u00a0\u2026", "num_citations": "106\n", "authors": ["297"]}
{"title": "Differential slicing: Identifying causal execution differences for security applications\n", "abstract": " A security analyst often needs to understand two runs of the same program that exhibit a difference in program state or output. This is important, for example, for vulnerability analysis, as well as for analyzing a malware program that features different behaviors when run in different environments. In this paper we propose a differential slicing approach that automates the analysis of such execution differences. Differential slicing outputs a causal difference graph that captures the input differences that triggered the observed difference and the causal path of differences that led from those input differences to the observed difference. The analyst uses the graph to quickly understand the observed difference. We implement differential slicing and evaluate it on the analysis of 11 real-world vulnerabilities and 2 malware samples with environment-dependent behaviors. We also evaluate it in an informal user study with two\u00a0\u2026", "num_citations": "98\n", "authors": ["297"]}
{"title": "Automatic protocol reverse-engineering: Message format extraction and field semantics inference\n", "abstract": " Understanding the command-and-control (C&C) protocol used by a botnet is crucial for anticipating its repertoire of nefarious activity. However, the C&C protocols of botnets, similar to many other application layer protocols, are undocumented. Automatic protocol reverse-engineering techniques enable understanding undocumented protocols and are important for many security applications, including the analysis and defense against botnets. For example, they enable active botnet infiltration, where a security analyst rewrites messages sent and received by a bot in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation.In this work, we propose a novel approach to automatic protocol reverse engineering based on dynamic program binary analysis. Compared to previous work that examines the network traffic, we leverage the availability of a program\u00a0\u2026", "num_citations": "91\n", "authors": ["297"]}
{"title": "Input generation via decomposition and re-stitching: Finding bugs in malware\n", "abstract": " Attackers often take advantage of vulnerabilities in benign software, and the authors of benign software must search their code for bugs in hopes of finding vulnerabilities before they are exploited. But there has been little research on the converse question of whether defenders can turn the tables by finding vulnerabilities in malware. We provide a first affirmative answer to that question. We introduce a new technique, stitched dynamic symbolic execution, that makes it possible to use exploration techniques based on symbolic execution in the presence of functionalities that are common in malware and otherwise hard to analyze, such as decryption and checksums. The technique is based on decomposing the constraints induced by a program, solving only a subset, and then re-stitching the constraint solution into a complete input. We implement the approach in a system for x86 binaries, and apply it to 4 prevalent\u00a0\u2026", "num_citations": "78\n", "authors": ["297"]}
{"title": "A lustrum of malware network communication: Evolution and insights\n", "abstract": " Both the operational and academic security communities have used dynamic analysis sandboxes to execute malware samples for roughly a decade. Network information derived from dynamic analysis is frequently used for threat detection, network policy, and incident response. Despite these common and important use cases, the efficacy of the network detection signal derived from such analysis has yet to be studied in depth. This paper seeks to address this gap by analyzing the network communications of 26.8 million samples that were collected over a period of five years. Using several malware and network datasets, our large scale study makes three core contributions. (1) We show that dynamic analysis traces should be carefully curated and provide a rigorous methodology that analysts can use to remove potential noise from such traces. (2) We show that Internet miscreants are increasingly using potentially\u00a0\u2026", "num_citations": "72\n", "authors": ["297"]}
{"title": "FiG: Automatic fingerprint generation\n", "abstract": " Fingerprinting is a widely used technique among the networking and security communities for identifying different implementations of the same piece of networking software running on a remote host. A fingerprint is essentially a set of queries and a classification function that can be applied on the responses to the queries in order to classify the software into classes. So far, identifying fingerprints remains largely an arduous and manual process. This paper proposes a novel approach for automatic fingerprint generation, that automatically explores a set of candidate queries and applies machine learning techniques to identify the set of valid queries and to learn an adequate classification function. Our results show that such an automatic process can generate accurate fingerprints that classify each piece of software into its proper class and that the search space for query exploration remains largely unexploited, with many new such queries awaiting discovery. With a preliminary exploration, we are able to identify new queries not previously used for fingerprinting.", "num_citations": "72\n", "authors": ["297"]}
{"title": "Measuring {PUP} prevalence and {PUP} distribution through pay-per-install services\n", "abstract": " Potentially unwanted programs (PUP) such as adware and rogueware, while not outright malicious, exhibit intrusive behavior that generates user complaints and makes security vendors flag them as undesirable. PUP has been little studied in the research literature despite recent indications that its prevalence may have surpassed that of malware.", "num_citations": "67\n", "authors": ["297"]}
{"title": "The MALICIA dataset: identification and analysis of drive-by download operations\n", "abstract": " Drive-by downloads are the preferred distribution vector for many malware families. In the drive-by ecosystem, many exploit servers run the same exploit kit and it is a challenge understanding whether the exploit server is part of a larger operation. In this paper, we propose a technique to identify exploit servers managed by the same organization. We collect over time how exploit servers are configured, which exploits they use, and what malware they distribute, grouping servers with similar configurations into operations. Our operational analysis reveals that although individual exploit servers have a median lifetime of 16\u00a0h, long-lived operations exist that operate for several months. To sustain long-lived operations, miscreants are turning to the cloud, with 60\u00a0% of the exploit servers hosted by specialized cloud hosting services. We also observe operations that distribute multiple malware families and that pay\u00a0\u2026", "num_citations": "67\n", "authors": ["297"]}
{"title": "Type inference on executables\n", "abstract": " In many applications, source code and debugging symbols of a target program are not available, and the only thing that we can access is the program executable. A fundamental challenge with executables is that, during compilation, critical information such as variables and types is lost. Given that typed variables provide fundamental semantics of a program, for the last 16 years, a large amount of research has been carried out on binary code type inference, a challenging task that aims to infer typed variables from executables (also referred to as binary code). In this article, we systematize the area of binary code type inference according to its most important dimensions: the applications that motivate its importance, the approaches used, the types that those approaches infer, the implementation of those approaches, and how the inference results are evaluated. We also discuss limitations, underdeveloped problems\u00a0\u2026", "num_citations": "56\n", "authors": ["297"]}
{"title": "Certified PUP: abuse in authenticode code signing\n", "abstract": " Code signing is a solution to verify the integrity of software and its publisher's identity, but it can be abused by malware and potentially unwanted programs (PUP) to look benign. This work performs a systematic analysis of Windows Authenticode code signing abuse, evaluating the effectiveness of existing defenses by certification authorities. We identify a problematic scenario in Authenticode where timestamped signed malware successfully validates even after the revocation of their code signing certificate. We propose hard revocations as a solution. We build an infrastructure that automatically analyzes potentially malicious executables, selects those signed, clusters them into operations, determines if they are PUP or malware, and produces a certificate blacklist. We use our infrastructure to evaluate 356 K samples from 2006-2015. Our analysis shows that most signed samples are PUP (88%-95%) and that\u00a0\u2026", "num_citations": "49\n", "authors": ["297"]}
{"title": "Coming of age: A longitudinal study of tls deployment\n", "abstract": " The Transport Layer Security (TLS) protocol is the de-facto standard for encrypted communication on the Internet. However, it has been plagued by a number of different attacks and security issues over the last years. Addressing these attacks requires changes to the protocol, to server-or client-software, or to all of them. In this paper we conduct the first large-scale longitudinal study examining the evolution of the TLS ecosystem over the last six years. We place a special focus on the ecosystem's evolution in response to high-profile attacks.", "num_citations": "48\n", "authors": ["297"]}
{"title": "Cyberprobe: Towards internet-scale active detection of malicious servers\n", "abstract": " Cybercriminals use different types of geographically distributed servers to run their operations such as C&C servers for managing their malware, exploit servers to distribute the malware, payment servers for monetization, and redirectors for anonymity. Identifying the server infrastructure used by a cybercrime operation is fundamental for defenders, as it enables take-downs that can disrupt the operation and is a critical step towards identifying the criminals behind it. In this paper, we propose a novel active probing approach for detecting malicious servers and compromised hosts that listen for (and react to) incoming network requests. Our approach sends probes to remote hosts and examines their responses, determining whether the remote hosts are malicious or not. It identifies different malicious server types as well as malware that listens for incoming traffic such as P2P bots. Compared with existing defenses, our active probing approach is fast, cheap, easy to deploy, and achieves Internet scale. We have implemented our active probing approach in a tool called CyberProbe. We have used CyberProbe to identify 151 malicious servers and 7,881 P2P bots through 24 localized and Internet-wide scans. Of those servers 75% are unknown to publicly available databases of malicious servers, indicating that CyberProbe can achieve up to 4 times better coverage than existing techniques. Our results reveal an important provider locality property: operations hosts an average of 3.2 servers on the same hosting provider to amortize the cost of setting up a relationship with the provider.", "num_citations": "48\n", "authors": ["297"]}
{"title": "Would Diversity Really Increase the Robustness of the Routing Infrastructure against Software Defects?\n", "abstract": " Today\u2019s routing infrastructure exhibits high homogeneity. This constitutes a serious threat to the resilience of the network, since a bug or security vulnerability in an implementation could make all routers running that implementation to become simultaneously unusable. This situation could arise as a defective software upgrade or a denial-ofservice attack.Diversity has been proposed as a solution to increase the resilience to software defects, but the benefits have not been clearly studied. In this paper, we use a graph theoretic approach to study those benefits, addressing three fundamental questions: 1) how to measure the robustness of the network to such failures; 2) how much diversity would be needed for a certain degree of robustness; and 3) how to best use the available diversity.", "num_citations": "47\n", "authors": ["297"]}
{"title": "Distributed evasive scan techniques and countermeasures\n", "abstract": " Scan detection and suppression methods are an important means for preventing the disclosure of network information to attackers. However, despite the importance of limiting the information obtained by the attacker, and the wide availability of such scan detection methods, there has been very little research on evasive scan techniques, which can potentially be used by attackers to avoid detection. In this paper, we first present a novel classification of scan detection methods based on their amnesty policy, since attackers can take advantage of such policies to evade detection. Then we propose two novel metrics to measure the resources that an attacker needs to complete a scan without being detected. Next, we introduce z-Scan, a novel evasive scan technique that uses distributed scanning, and show that it is extremely effective against TRW, one of the state-of-the-art scan detection methods. Finally, we\u00a0\u2026", "num_citations": "44\n", "authors": ["297"]}
{"title": "Autoprobe: Towards automatic active malicious server probing using dynamic binary analysis\n", "abstract": " Malware continues to be one of the major threats to Internet security. In the battle against cybercriminals, accurately identifying the underlying malicious server infrastructure (eg, C&C servers for botnet command and control) is of vital importance. Most existing passive monitoring approaches cannot keep up with the highly dynamic, ever-evolving malware server infrastructure. As an effective complementary technique, active probing has recently attracted attention due to its high accuracy, efficiency, and scalability (even to the Internet level). In this paper, we propose Autoprobe, a novel system to automatically generate effective and efficient fingerprints of remote malicious servers. Autoprobe addresses two fundamental limitations of existing active probing approaches: it supports pull-based C&C protocols, used by the majority of malware, and it generates fingerprints even in the common case when C&C servers are\u00a0\u2026", "num_citations": "41\n", "authors": ["297"]}
{"title": "Crash analysis with BitBlaze\n", "abstract": " The underlying problem behind many security vulnerabilities of today is memory corruption bugs. There are a variety of different techniques available to try to find such vulnerabilities. One of the most common techniques is fuzzing. Of the different approaches to fuzzing, one of the simplest and also most successful is mutation based fuzzing, which amounts to making random changes to an input and testing it within the application. One of the major limitations with this type of approach is not that it doesn\u02bct find problems, but rather it can find too many problems. The limiting factor in finding and exploiting (or fixing) security vulnerabilities in this case is not finding the vulnerabilities, but rather prioritizing the ones found and determining their root cause. This is a problem for two sets of people. One group is security researchers and exploit developers who don\u02bct know which crashes to look at and have the time consuming task of trying to determine the root cause of the problem from an invalid input and a crash. The other group are developers who are also interested in the underlying cause of the crash, but instead of trying to exploit it, they are trying to fix it. Understanding the cause of the vulnerability is important to fixing it correctly, otherwise the fix may correct one aspect of the vulnerability or code path to the bug, but not all code paths. For developers, even with source code in hand, this is not always an easy task.This whitepaper attempts to alleviate this problem by introducing a solution using BitBlaze, a binary analysis tool. This toolset can help to quickly determine whether a particular crash, found by mutation based fuzzing, is exploitable and also to\u00a0\u2026", "num_citations": "38\n", "authors": ["297"]}
{"title": "Towards generating high coverage vulnerability-based signatures with protocol-level constraint-guided exploration\n", "abstract": " Signature-based input filtering is an important and widely deployed defense. But current signature generation methods have limited coverage and the generated signatures often can be easily evaded by an attacker with small variations of the exploit message. In this paper, we propose protocol-level constraint-guided exploration, a new approach towards generating high coverage vulnerability-based signatures. In particular, our approach generates high coverage, yet compact, vulnerability point reachability predicates, which capture many paths to the vulnerability point. In our experimental results, our tool, Elcano, generates compact, high coverage signatures for real-world vulnerabilities.", "num_citations": "34\n", "authors": ["297"]}
{"title": "Caronte: Detecting location leaks for deanonymizing tor hidden services\n", "abstract": " Anonymity networks such as Tor are a critical privacy-enabling technology. Tor's hidden services provide both client and server anonymity. They protect the location of the server hosting the service and provide encryption at every hop from a client to the hidden service. This paper presents Caronte, a tool to automatically identify location leaks in hidden services, ie, sensitive information in the content served by the hidden service or its configuration that discloses the server's IP address. Compared to prior techniques that deanonymize hidden services Caronte implements a novel approach that does not rely on flaws on the Tor protocol and assumes an open-world, ie, it does not require a short list of candidate servers known in advance. Caronte visits the hidden service, extracts Internet endpoints and looks up unique strings from the hidden service's content, and examines the hidden service's certificate chain to\u00a0\u2026", "num_citations": "32\n", "authors": ["297"]}
{"title": "A look into 30 years of malware development from a software metrics perspective\n", "abstract": " During the last decades, the problem of malicious and unwanted software (malware) has surged in numbers and sophistication. Malware plays a key role in most of today\u2019s cyber attacks and has consolidated as a commodity in the underground economy. In this work, we analyze the evolution of malware since the early 1980s to date from a software engineering perspective. We analyze the source code of 151 malware samples and obtain measures of their size, code quality, and estimates of the development costs (effort, time, and number of people). Our results suggest an exponential increment of nearly one order of magnitude per decade in aspects such as size and estimated effort, with code quality metrics similar to those of regular software. Overall, this supports otherwise confirmed claims about the increasing complexity of malware and its production progressively becoming an industry.", "num_citations": "31\n", "authors": ["297"]}
{"title": "Whowas: A platform for measuring web deployments on iaas clouds\n", "abstract": " Public infrastructure-as-a-service (IaaS) clouds such as Amazon EC2 and Microsoft Azure host an increasing number of web services. The dynamic, pay-as-you-go nature of modern IaaS systems enable web services to scale up or down with demand, and only pay for the resources they need. We are unaware, however, of any studies reporting on measurements of the patterns of usage over time in IaaS clouds as seen in practice. We fill this gap, offering a measurement platform that we call WhoWas. Using active, but lightweight, probing, it enables associating web content to public IP addresses on a day-by-day basis. We exercise WhoWas to provide the first measurement study of churn rates in EC2 and Azure, the efficacy of IP blacklists for malicious activity in clouds, the rate of adoption of new web software by public cloud customers, and more.", "num_citations": "29\n", "authors": ["297"]}
{"title": "K-hunt: Pinpointing insecure cryptographic keys from execution traces\n", "abstract": " The only secrets in modern cryptography (crypto for short) are the crypto keys. Understanding how crypto keys are used in a program and discovering insecure keys is paramount for crypto security. This paper presents K-Hunt, a system for identifying insecure keys in binary executables. K-Hunt leverages the properties of crypto operations for identifying the memory buffers where crypto keys are stored. And, it tracks their origin and propagation to identify insecure keys such as deterministically generated keys, insecurely negotiated keys, and recoverable keys. K-Hunt does not use signatures to identify crypto operations, and thus can be used to identify insecure keys in unknown crypto algorithms and proprietary crypto implementations. We have implemented K-Hunt and evaluated it with 10 cryptographic libraries and 15 applications that contain crypto operations. Our evaluation results demonstrate that K-Hunt\u00a0\u2026", "num_citations": "26\n", "authors": ["297"]}
{"title": "The malsource dataset: Quantifying complexity and code reuse in malware development\n", "abstract": " During the last decades, the problem of malicious and unwanted software (malware) has surged in numbers and sophistication. Malware plays a key role in most of today's cyberattacks and has consolidated as a commodity in the underground economy. In this paper, we analyze the evolution of malware from 1975 to date from a software engineering perspective. We analyze the source code of 456 samples from 428 unique families and obtain measures of their size, code quality, and estimates of the development costs (effort, time, and number of people). Our results suggest an exponential increment of nearly one order of magnitude per decade in aspects such as size and estimated effort, with code quality metrics similar to those of benign software. We also study the extent to which code reuse is present in our dataset. We detect a significant number of code clones across malware families and report which features\u00a0\u2026", "num_citations": "23\n", "authors": ["297"]}
{"title": "SigPath: A Memory Graph Based Approach for Program Data Introspection and Modification\n", "abstract": " Examining and modifying data of interest in the memory of a target program is an important capability for security applications such as memory forensics, rootkit detection, game hacking, and virtual machine introspection. In this paper we present a novel memory graph based approach for program data introspection and modification, which does not require source code, debugging symbols, or any API in the target program. It takes as input a sequence of memory snapshots taken while the program executes, and produces a path signature, which can be used in different executions of the program to efficiently locate and traverse the in-memory data structures where the data of interest is stored. We have implemented our approach in a tool called SigPath. We have applied SigPath to game hacking, building cheats for 10 popular real-time and turn-based games, and for memory forensics, recovering from\u00a0\u2026", "num_citations": "20\n", "authors": ["297"]}
{"title": "Extracting models of security-sensitive operations using string-enhanced white-box exploration on binaries\n", "abstract": " Models of security-sensitive code enable reasoning about the security implications of code. In this paper we present an approach for extracting models of security-sensitive operations directly from program binaries, which lets third-party analysts reason about a program when the source code is not available. Our approach is based on string-enhanced white-box exploration, a new technique that improves the effectiveness of current white-box exploration techniques on programs that use strings, by reasoning directly about string operations, rather than about the individual byte-level operations that comprise them. We implement our approach and use it to extract models of the closed-source content sniffing algorithms of two popular browsers Internet Explorer 7 and Safari 3.1. We use the generated models to automatically find recently studied content-sniffing XSS attacks, and show the benefits of string-enhanced white-box exploration over current byte-level exploration techniques.Descriptors:", "num_citations": "18\n", "authors": ["297"]}
{"title": "Rosetta: Extracting Protocol Semantics using Binary Analysis with Applications to Protocol Replay and NAT Rewriting\n", "abstract": " Rewriting a previously seen dialog between two entities, so that it is accepted by another entity, is important for many applications including: the protocol replay problem and the NAT rewriting problem. Both problems are instances of a larger problem that we call the dialog rewriting problem. The challenge in dialog rewriting is that the dynamic fields, eg, hostnames, IP addresses, session identifiers or timestamps, in the original dialog need to be rewritten for the modified dialog to succeed. This is particularly difficult because the protocol used in the original dialog might be unknown.In this paper, our goal is to generate a transformation function that can be used to rewrite the values of the dynamic fields. For this, we propose binary analysis techniques to solve the main two challenges: 1) how to automatically identify the dynamic fields, and 2) how to automatically rewrite the values in the dynamic fields.", "num_citations": "18\n", "authors": ["297"]}
{"title": "Mind Your Own Business: A Longitudinal Study of Threats and Vulnerabilities in Enterprises.\n", "abstract": " Enterprises own a significant fraction of the hosts connected to the Internet and possess valuable assets, such as financial data and intellectual property, which may be targeted by attackers. They suffer attacks that exploit unpatched hosts and install malware, resulting in breaches that may cost millions in damages. Despite the scale of this phenomenon, the threat and vulnerability landscape of enterprises remains under-studied. The security posture of enterprises remains unclear, and it\u2019s unknown whether enterprises are indeed more secure than consumer hosts. To address these questions, we perform the largest and longest enterprise security study so far. Our data covers nearly 3 years and is collected from 28K enterprises, belonging to 67 industries, which own 82M client hosts and 73M public-facing servers.Our measurements comprise of two parts: an analysis of the threat landscape and an analysis of the enterprise vulnerability patching behavior. The threat landscape analysis studies the encounter rate of malware and PUP in enterprise client hosts. It measures, among others, that 91%\u201397% of the enterprises, 13%\u201341% of the hosts, encountered at least one malware or PUP file over the length of our study; that enterprises encounter malware much more often than PUP; and that some industries like banks and consumer finances achieve significantly lower malware and PUP encounter rates than the most-affected industries. The vulnerability analysis examines the patching of 12 client-side and 112 server-side applications in enterprise client hosts and servers. It measures, among others, that it takes over 6 months on average to patch 90\u00a0\u2026", "num_citations": "17\n", "authors": ["297"]}
{"title": "A Survey of Binary Code Similarity\n", "abstract": " Binary code similarity approaches compare two or more pieces of binary code to identify their similarities and differences. The ability to compare binary code enables many real-world applications on scenarios where source code may not be available such as patch analysis, bug search, and malware detection and analysis. Over the past 20 years numerous binary code similarity approaches have been proposed, but the research area has not yet been systematically analyzed. This paper presents a first survey of binary code similarity. It analyzes 61 binary code similarity approaches, which are systematized on four aspects: (1) the applications they enable, (2) their approach characteristics, (3) how the approaches are implemented, and (4) the benchmarks and methodologies used to evaluate them. In addition, the survey discusses the scope and origins of the area, its evolution over the past two decades, and the challenges that lie ahead.", "num_citations": "16\n", "authors": ["297"]}
{"title": "Dissecting Tor Bridges: a Security Evaluation of Their Private and Public Infrastructures\n", "abstract": " Bridges are onion routers in the Tor Network whose IP addresses are not public. So far, no global security analysis of Tor bridges has been performed. Leveraging public data sources, and two known Tor issues, we perform the first systematic study on the security of the Tor bridges infrastructure. Our study covers both the public infrastructure available to all Tor users, and the previously unreported private infrastructure, comprising private nodes for the exclusive use of those who know their existence.Our analysis of the public infrastructure is twofold. First, we examine the security implications of the public data in the CollecTor service, identifying several pieces of data that may be detrimental for the security of bridges. Then, we measure security relevant properties of public bridges. Our results show that the 55% of public bridges that carry clients are vulnerable to aggressive blocking; that 90% of bridge clients use default bridges that are trivial to identify; that the concurrent deployment of Pluggable Transports in bridges reduces the security of the most secure transports; and that running non-Tor services in the same host as a bridge may harm its anonymity.", "num_citations": "14\n", "authors": ["297"]}
{"title": "Network dialog minimization and network dialog diffing: two novel primitives for network security applications\n", "abstract": " In this work, we present two fundamental primitives for network security: network dialog minimization and network dialog diffing. Network dialog minimization (NDM) simplifies an original dialog with respect to a goal, so that the minimized dialog when replayed still achieves the goal, but requires minimal network communication, achieving significant time and bandwidth savings. We present network delta debugging, the first technique to solve NDM. Network dialog diffing compares two dialogs, aligns them, and identifies their common and different parts. We propose a novel dialog diffing technique that aligns two dialogs by finding a mapping that maximizes similarity.", "num_citations": "14\n", "authors": ["297"]}
{"title": "Black box anomaly detection: is it utopian?\n", "abstract": " Automatic identification of anomalies on network data is a problem of fundamental interest to ISPs to diagnose incipient problems in their networks. ISPs gather diverse data sources from the network for monitoring, diagnostics or provisioning tasks. Finding anomalies in this data is a huge challenge due to the volume of the data collected, the number and diversity of data sources and the diversity of anomalies to be detected.In this paper we introduce a framework for anomaly detection that allows the construction of a black box anomaly detector. This anomaly detector can be used for automatically finding anomalies with minimal human intervention. Our framework also allows us to deal with the different types of data sources collected from the network. We have developed a prototype of this framework, TrafficComber, and we are in the process of evaluating it using the data in the warehouse of a tier-1 ISP.", "num_citations": "13\n", "authors": ["297"]}
{"title": "Avclass2: Massive malware tag extraction from av labels\n", "abstract": " Tags can be used by malware repositories and analysis services to enable searches for samples of interest across different dimensions. Automatically extracting tags from AV labels is an efficient approach to categorize and index massive amounts of samples. Recent tools like AVclass and Euphony have demonstrated that, despite their noisy nature, it is possible to extract family names from AV labels. However, beyond the family name, AV labels contain much valuable information such as malware classes, file properties, and behaviors.", "num_citations": "10\n", "authors": ["297"]}
{"title": "Costly freeware: a systematic analysis of abuse in download portals\n", "abstract": " Freeware is proprietary software that can be used free of charge. A popular vector for distributing freeware is  download portals , i.e. websites that index, categorise, and host programs. Download portals can be abused to distribute potentially unwanted programs (PUP) and malware. The abuse can be due to PUP and malware authors uploading their ware, by benign freeware authors joining as affiliate publishers of pay-per-install (PPI) services and other affiliate programs, or by malicious download portal owners. The authors perform a systematic study of abuse in download portals. They build a platform to crawl download portals and apply it to download 191 K Windows freeware installers from 20 download portals. They analyse the collected installers and execute them in a sandbox to monitor their installation. They measure an overall ratio of PUP and malware between 8% (conservative estimate) and 26% (lax\u00a0\u2026", "num_citations": "9\n", "authors": ["297"]}
{"title": "Cross-origin state inference (COSI) attacks: Leaking web site states through xs-leaks\n", "abstract": " In a Cross-Origin State Inference (COSI) attack, an attacker convinces a victim into visiting an attack web page, which leverages the cross-origin interaction features of the victim's web browser to infer the victim's state at a target web site. Multiple instances of COSI attacks have been found in the past under different names such as login detection or access detection attacks. But, those attacks only consider two states (e.g., logged in or not) and focus on a specific browser leak method (or XS-Leak). This work shows that mounting more complex COSI attacks such as deanonymizing the owner of an account, determining if the victim owns sensitive content, and determining the victim's account type often requires considering more than two states. Furthermore, robust attacks require supporting a variety of browsers since the victim's browser cannot be predicted apriori. To address these issues, we present a novel approach to identify and build complex COSI attacks that differentiate more than two states and support multiple browsers by combining multiple attack vectors, possibly using different XS-Leaks. To enable our approach, we introduce the concept of a COSI attack class. We propose two novel techniques to generalize existing COSI attack instances into COSI attack classes and to discover new COSI attack classes. We systematically apply our techniques to existing attacks, identifying 40 COSI attack classes. As part of this process, we discover a novel XS-Leak based on window.postMessage. We implement our approach into Basta-COSI, a tool to find COSI attacks in a target web site. We apply Basta-COSI to test four stand-alone web applications\u00a0\u2026", "num_citations": "8\n", "authors": ["297"]}
{"title": "Bidirectional protocol reverse engineering: Message format extraction and field semantics inference\n", "abstract": " Automatic protocol reverse-engineering is important for many security applications, including the analysis and defense against botnets. Understanding such C&C protocols is crucial for anticipating a botnet\u2019s repertoire of nefarious activity and to enable active botnet infiltration. Frequently, messages sent and received by a bot have to be rewritten in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation. To enable such rewriting, we need detailed information about the intent and structure of the messages in both directions of the communication despite the fact that we generally only have access to the implementation of one endpoint, namely the bot binary. Current techniques cannot enable such rewriting. In this paper, we propose techniques to extract the format of the protocol messages sent by an application that implements a protocol specification, and to infer the field semantics for messages both sent and received by the application. Our techniques enable applications such as rewriting the C&C messages for active botnet infiltration. We implement our techniques into Dispatcher, a tool to extract the message format and field semantics of both received and sent messages. We use Dispatcher to analyze MegaD, a prevalent spam botnet employing a hitherto undocumented C&C protocol, and show that the protocol information extracted by Dispatcher can be used to rewrite the messages sent upstream to the botmaster.", "num_citations": "8\n", "authors": ["297"]}
{"title": "Towards attribution in mobile markets: identifying developer account polymorphism\n", "abstract": " Malicious developers may succeed at publishing their apps in mobile markets, including the official ones. If reported, the apps will be taken down and the developer accounts possibly be banned. Unfortunately, such take-downs do not prevent the attackers to use other developer accounts to publish variations of their malicious apps. This work presents a novel approach for identifying developer accounts, and other indicators of compromise (IOCs) in mobile markets, that belong to the same operation, ie, to the same owners. Given a set of seed IOCs, our approach explores app and version metadata to identify new IOCs that belong to the same operation. It outputs an attribution graph, which details the attribution inferences, so that they can be reviewed. We have implemented our approach into Retriever, a tool that supports multiple mobile markets including the official GooglePlay and AppleStore. We have evaluated\u00a0\u2026", "num_citations": "7\n", "authors": ["297"]}
{"title": "Ayudante: Identifying undesired variable interactions\n", "abstract": " A common programming mistake is for incompatible variables to interact, eg, storing euros in a variable that should hold dollars, or using an array index with the wrong array. This paper proposes a novel approach for identifying undesired interactions between program variables. Our approach uses two different mechanisms to identify related variables. Natural language processing (NLP) identifies variables with related names that may have related semantics. Abstract type inference (ATI) identifies variables that interact with each other. Any discrepancies between these two mechanisms may indicate a programming error. We have implemented our approach in a tool called Ayudante. We evaluated Ayudante using two open-source programs: the Exim mail server and grep. Although these programs have been extensively tested and in deployment for years, Ayudante\u2019s first report for grep revealed a programming\u00a0\u2026", "num_citations": "7\n", "authors": ["297"]}
{"title": "Poster: Cross-platform malware: write once, infect everywhere\n", "abstract": " In this ongoing work we perform the first systematic investigation of cross-platform (X-platform) malware. As a first step, this paper presents an exploration into existing X-platform malware families and X-platform vulnerabilities used to distribute them. Our exploration shows that X-platform malware uses a wealth of methods to achieve portability. It also shows that exploits for X-platform vulnerabilities are X-platform indeed and readily available in commercial exploit kits, making them an inexpensive distribution vector for X-platform malware.", "num_citations": "7\n", "authors": ["297"]}
{"title": "How did that get in my phone? unwanted app distribution on android devices\n", "abstract": " Android is the most popular operating system with billions of active devices. Unfortunately, its popularity and openness makes it attractive for unwanted apps, i.e., malware and potentially unwanted programs (PUP). In Android, app installations typically happen via the official and alternative markets, but also via other smaller and less understood alternative distribution vectors such as Web downloads, pay-per-install (PPI) services, backup restoration, bloatware, and IM tools. This work performs a thorough investigation on unwanted app distribution by quantifying and comparing distribution through different vectors. At the core of our measurements are reputation logs of a large security vendor, which include 7.9M apps observed in 12M devices between June and September 2019. As a first step, we measure that between 10% and 24% of users devices encounter at least one unwanted app, and compare the\u00a0\u2026", "num_citations": "6\n", "authors": ["297"]}
{"title": "Bcd: Decomposing binary code into components using graph-based clustering\n", "abstract": " Complex software is built by composing components implementing largely independent blocks of functionality. However, once the sources are compiled into an executable, that modularity is lost. This is unfortunate for code recipients, for whom knowing the components has many potential benefits, such as improved program understanding for reverse-engineering, identifying shared code across different programs, binary code reuse, and authorship attribution. A novel approach for decomposing such source-free program executables into components is here proposed. Given an executable, the approach first statically builds a decomposition graph, where nodes are functions and edges capture three types of relationships: code locality, data references, and function calls. It then applies a graph-theoretic approach to partition the functions into disjoint components. A prototype implementation, BCD, demonstrates the\u00a0\u2026", "num_citations": "6\n", "authors": ["297"]}
{"title": "An analysis of pay-per-install economics using entity graphs\n", "abstract": " Potentially unwanted programs (PUP) are a category of undesirable software which includes adware and rogueware. PUP is often distributed through commercial pay-per-install (PPI) services. In this work we perform what we believe is the first analysis of the economics of commercial PPI services. To enable the economic analysis, we propose a novel attribution approach using entity graphs that capture the network of companies and persons behind a PUP operation, eg, a commercial PPI service or a set of PUP. We analyze 3 Spain-based operations. Each operation runs a commercial PPI service, develops PUP, and manages download portals. For each operation, we collect financial statements submitted by the companies and audit reports when available. This data allows us to analyze not only the operation revenues, but also their profits (and losses), which can widely differ from revenues depending on operational costs.Our analysis answers 6 main questions.(1) How profitable are the commercial PPI services and the operations running them? We measure that the three operations have a total revenue of 202.5 M\u20ac, net income (ie, profits) of 23M\u20ac, and EBITDA of 24.7 M\u20ac. Overall, expenses are high and margins low.(2) What are the revenue sources of the operations? The largest source of revenue is the PPI service, which provides up to 90% of an operation\u2019s revenue. But, we also observe the operations to draw revenue from advertising, download portals, PUP, and streaming services.(3) How has the PPI business evolved over time? Peak revenue and net income happened in 2013 and there was a sharp decrease starting mid-2014 when\u00a0\u2026", "num_citations": "5\n", "authors": ["297"]}
{"title": "Experimental Study of a Network Access Server for a public WLAN access network\n", "abstract": " Wireless access networks have gained popularity due to the flexibility they allow to the users, who is able to move away from his or her desk while still being able to access information. Among the different Wireless LAN standards, the most widespread, by far, is IEEE 802.11.Public WLAN access networks are being set up in hotspots, ie areas expected to have high demand for bandwidth. Access to the Internet and to corporate networks is provided at these hotspots with limited coverage but high available bandwidth. Airports and hotels have often been the first targeted locations for these hotspots, but conference centres, cafes, and train stations will then follow. In the near future, any person who owns at least one access point and has a connection to the Internet can become a small operator and offer access to the Internet using these resources.", "num_citations": "5\n", "authors": ["297"]}
{"title": "Botnet infiltration: Finding bugs in botnet command and control\n", "abstract": " Botnet empires are built on the bugs and vulnerabilities of innocent users. Yet, no one knows whether bugs and vulnerabilities exist in botnet Command and Control (C&C) systems. Further, little is known about the true structure of C&C systems, their resilience mechanisms, and the operations of the Bot Master behind the scenes. In this paper, we turn the tables around to provide the first attempt to find bugs in botnet Command and Control. Our results on the MegaD C&C indicate that it is buggy, with evidence of it being vulnerable to memory reads out of bounds. We also find that the MegaD C&C is surprisingly far more susceptible to DDoS attacks than the average web server.As a result of bypassing MegaD's cookie-check authentication mechanism, we have been actively'milking'spam templates from MegaD C&C since 27 Oct 2009. Just like a video recorder, the spam templates provide us with constant feeds on\u00a0\u2026", "num_citations": "5\n", "authors": ["297"]}
{"title": "RevProbe: detecting silent reverse proxies in malicious server infrastructures\n", "abstract": " Web service operators set up reverse proxies to interpose the communication between clients and origin servers for load-balancing traffic across servers, caching content, and filtering attacks. Silent reverse proxies, which do not reveal their proxy role to the client, are of particular interest since malicious infrastructures can use them to hide the existence of the origin servers, adding an indirection layer that helps protecting origin servers from identification and take-downs.", "num_citations": "4\n", "authors": ["297"]}
{"title": "Understanding the role of malware in cybercrime\n", "abstract": " At the core of most cybercrime operations is the attacker's ability to install malware on Internetconnected computers without the owner's informed consent. The goal of the MALICIA project is to study the crucial role of malware in cybercrime and the rise in recent years of an \u201cunderground economy\u201d associated with malware and the subversion of Internet-connected computers.", "num_citations": "2\n", "authors": ["297"]}
{"title": "How things Work and Fail\n", "abstract": " This chapter contains sections titled:   Online Advertising: With Secret Security   Web Security Remediation Efforts   Content\u2010Sniffing XSS Attacks: XSS with Non\u2010HTML Content   Our Internet Infrastructure at Risk   Social Spam   Understanding CAPTCHAs and Their Weaknesses   Security Questions   Folk Models of Home Computer Security   Detecting and Defeating Interception Attacks Against SSL", "num_citations": "1\n", "authors": ["297"]}