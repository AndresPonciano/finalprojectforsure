{"title": "Design and evaluation of a wide-area event notification service\n", "abstract": " The components of a loosely coupled system are typically designed to operate by generating and responding to asynchronous events. An event notification service is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. The two primary services that should be provided to components by the infrastructure are notification selection (i. e., determining which notifications match which subscriptions) and notification delivery (i.e., routing matching notifications from publishers to subscribers). Numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event  notifications. Therefore, they suffer from an inherent inability to scale to\u00a0\u2026", "num_citations": "2224\n", "authors": ["306"]}
{"title": "Achieving scalability and expressiveness in an internet-scale event notification service\n", "abstract": " This paper describes the design of SIENA, an Internet-scale event notification middleware service for distributed event-based applications deployed over wide-area networks. SIENA is responsible for selecting the notifications that are of interest to clients (as expressed in client subscriptions) and then delivering those notifications to the clients via access points. The key design challenge for SIENA is maximizing expressiveness in the selection mechanism without sacrificing scalability of the delivery mechanism. This paper focuses on those aspects of the design of SIENA that fundamentally impact scalability and expressiveness. In particular, we describe SIENA's data model for notifications, the covering relations that formally define the semantics of the data model, the distributed architectures we have studied for SIENA's implementation, and the processing strategies we developed to exploit the covering relations for\u00a0\u2026", "num_citations": "632\n", "authors": ["306"]}
{"title": "From action to activity: sensor-based activity recognition\n", "abstract": " As compared to actions, activities are much more complex, but semantically they are more representative of a human\u05f3s real life. Techniques for action recognition from sensor-generated data are mature. However, few efforts have targeted sensor-based activity recognition. In this paper, we present an efficient algorithm to identify temporal patterns among actions and utilize the identified patterns to represent activities for automated recognition. Experiments on a real-world dataset demonstrated that our approach is able to recognize activities with high accuracy from temporal patterns, and that temporal patterns can be used effectively as a mid-level feature for activity representation.", "num_citations": "453\n", "authors": ["306"]}
{"title": "A practical approach to programming with assertions\n", "abstract": " Embedded assertions have been recognized as a potentially powerful tool for automatic runtime detection of software faults during debugging, testing, maintenance and even production versions of software systems. Yet despite the richness of the notations and the maturity of the techniques and tools that have been developed for programming with assertions, assertions are a development tool that has seen little widespread use in practice. The main reasons seem to be that (1) previous assertion processing tools did not integrate easily with existing programming environments, and (2) it is not well understood what kinds of assertions are most effective at detecting software faults. This paper describes experience using an assertion processing tool that was built to address the concerns of ease-of-use and effectiveness. The tool is called APP, an Annotation PreProcessor for C programs developed in UNIX-based\u00a0\u2026", "num_citations": "453\n", "authors": ["306"]}
{"title": "TestTube: A system for selective regression testing\n", "abstract": " The paper describes a system called TESTTUBE that combines static and dynamic analysis to perform selective retesting of software systems written in C. TESTTUBE first identifies which functions, types, variables and macros are covered by each test unit in a test suite. Each time the system under test is modified, TESTTUBE identifies which entities were changed to create the new version. Using the coverage and change information, TESTTUBE selects only those test units that cover the changed entities for testing the new version. We have applied TESTTUBE to selective retesting of two software systems, an I/O library and a source code analyzer. Additionally, we are adapting TESTTUBE for selective retesting of nondeterministic systems, where the main drawback is the unsuitability of dynamic analysis for identification of covered entities. Our experience with TESTTUBE has been quite encouraging, with an\u00a0\u2026", "num_citations": "418\n", "authors": ["306"]}
{"title": "A design framework for Internet-scale event observation and notification\n", "abstract": " There is increasing interest in having software systems execute and interoperate over the Internet. Execution and interoperation at this scale imply a degree of loose coupling and heterogeneity among the components from which such systems will be built. One common architectural style for distributed; loosely-coupled, heterogeneous software systems is a structure based on event generation, observation and notification. The technology to support this approach is well-developed for local area networks, but it is illsuited to networks on the scale of the Internet. Hence, new technologies are needed to support the construction of large-scale, event-based software systems for the Internet. We have begun to design a new facility for event observation and notification that better serves the needs of Internet-scale applications. In this paper we present results from our first step in this design process, in which we defined a\u00a0\u2026", "num_citations": "359\n", "authors": ["306"]}
{"title": "Action2Activity: recognizing complex activities from sensor data\n", "abstract": " As compared to simple actions, activities are much more complex, but semantically consistent with a human's real life. Techniques for action recognition from sensor generated data are mature. However, there has been relatively little work on bridging the gap between actions and activities. To this end, this paper presents a novel approach for complex activity recognition comprising of two components. The first component is temporal pattern mining, which provides a mid-level feature representation for activities, encodes temporal relatedness among actions, and captures the intrinsic properties of activities. The second component is adaptive Multi-Task Learning, which captures relatedness among activities and selects discriminant features. Extensive experiments on a real-world dataset demonstrate the effectiveness of our work.", "num_citations": "351\n", "authors": ["306"]}
{"title": "Fortune teller: predicting your career path\n", "abstract": " People go to fortune tellers in hopes of learning things about their future. A future career path is one of the topics most frequently discussed. But rather than rely on\" black arts\" to make predictions, in this work we scientifically and systematically study the feasibility of career path prediction from social network data. In particular, we seamlessly fuse information from multiple social networks to comprehensively describe a user and characterize progressive properties of his or her career path. This is accomplished via a multi-source learning framework with fused lasso penalty, which jointly regularizes the source and career-stage relatedness. Extensive experiments on real-world data confirm the accuracy of our model.", "num_citations": "242\n", "authors": ["306"]}
{"title": "Context-aware mobile music recommendation for daily activities\n", "abstract": " Existing music recommendation systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we present in this paper a novel approach to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. We present a probabilistic model to integrate contextual information with music content analysis to offer music recommendation for daily activities, and we present a prototype implementation of the model. Finally, we present evaluation results demonstrating good accuracy and usability of the model and prototype.", "num_citations": "239\n", "authors": ["306"]}
{"title": "Recognizing complex activities by a probabilistic interval-based model\n", "abstract": " A key challenge in complex activity recognition is the fact that a complex activity can often be performed in several different ways, with each consisting of its own configuration of atomic actions and their temporal dependencies. This leads us to define an atomic activity-based probabilistic framework that employs Allen's interval relations to represent local temporal dependencies. The framework introduces a latent variable from the Chinese Restaurant Process to explicitly characterize these unique internal configurations of a particular complex activity as a variable number of tables. It can be analytically shown that the resulting interval network satisfies the transitivity property, and as a result, all local temporal dependencies can be retained and are globally consistent. Empirical evaluations on benchmark datasets suggest our approach significantly outperforms the state-of-the-art methods.", "num_citations": "197\n", "authors": ["306"]}
{"title": "Urban water quality prediction based on multi-task multi-view learning\n", "abstract": " Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. In this work, we forecast the water quality of a station over the next few hours, using a multitask multi-view learning method to fuse multiple datasets from different domains. In particular, our learning model comprises two alignments. The first alignment is the spaio-temporal view alignment, which combines local spatial and temporal information of each station. The second alignment is the prediction alignment among stations, which captures their spatial correlations and performs copredictions by incorporating these correlations. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach.", "num_citations": "194\n", "authors": ["306"]}
{"title": "Component metadata for software engineering tasks\n", "abstract": " This paper presents a framework that lets a component de- veloper provide a component user with different kinds of information, depending on the specific context and needs. The framework is based on presenting this information in the form of metadata. Metadata describe static and dynamic aspects of the component, can be accessed by the user, and can be used for different tasks throughout the software engi- neering lifecycle. The framework is defined in a general way, so that the metadata can be easily extended if new types of data have to be provided. In our approach, we define a unique format and a unique tag for each kind of metadata provided. The tag lets the user of the component both treat the information provided as metadata in the correct way and query for a specific piece of information. We motivate the untapped po- tential of component metadata by showing the need for metadata in the\u00a0\u2026", "num_citations": "193\n", "authors": ["306"]}
{"title": "Using component metacontent to support the regression testing of component-based software\n", "abstract": " Component based software technologies are viewed as essential for creating the software systems of the future. However, the use of externally-provided components has serious drawbacks for a wide range of software engineering activities, often because of a lack of information about the components. Previously (A. Orso et al., 2000), we proposed the use of component metacontents: additional data and methods provided with a component, to support software engineering tasks. The authors present two new metacontent based techniques that address the problem of regression test selection for component based applications: a code based approach and a specification based approach. First, we illustrate the two techniques. Then, we present a case study that applies the code based technique to a real component based system. On the system studied, on average, 26% of the overall testing effort was saved over\u00a0\u2026", "num_citations": "191\n", "authors": ["306"]}
{"title": "Yeast: A general purpose event-action system\n", "abstract": " Distributed networks of personal workstations are becoming the dominant computing environment for software development organizations. Many cooperative activities that are carried out in such environments are particularly well suited for automated support. Taking the point of view that such activities are modeled most naturally as the occurrence of events requiring actions to be performed, we developed a system called Yeast (Yet another Event Action Specification Tool). Yeast is a client server system in which distributed clients register event action specifications with a centralized server, which performs event detection and specification management. Each specification submitted by a client defines a pattern of events that is of interest to the client's application plus an action that is to be executed in response to an occurrence of the event pattern; the server triggers the action of a specification once it has detected\u00a0\u2026", "num_citations": "187\n", "authors": ["306"]}
{"title": "Exploiting ADLs to specify architectural styles induced by middleware infrastructures\n", "abstract": " ABSTRACT Architecture Dejnition Languages (ADLs) enable the formalization of the architecture of software systems and the execution of preliminary analyses on them. These analyses aim at supporting the identification and solution of design problems in the early stages of software development. We have used ADLs to describe middleware-induced architectural styles. These styles describe the assumptions and constraints that middleware infrastructures impose on the architecture of systems. Our work originates from the belief that the explicit representation of these styles at the architectural level can guide designers in the definition of an architecture compliant with a pre-selected middleware infrastructure, or, conversely can support designers in the identification of the most suitable middleware infrastructure for a specific architecture.In this paper we provide an evaluation of ADLs as to their suitability for defining\u00a0\u2026", "num_citations": "156\n", "authors": ["306"]}
{"title": "A comparative study of coarse-and fine-grained safe regression test-selection techniques\n", "abstract": " Regression test-selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of some of these techniques have suggested that they can indeed benefit testers, but so far, few studies have empirically compared different techniques. In this paper, we present the results of a comparative empirical study of two safe regression test-selection techniques. The techniques we studied have been implemented as the tools DejaVu and TestTube; we compared these tools in terms of a cost model incorporating precision (ability to eliminate  unnecessary test cases), analysis cost, and test execution cost. Our results indicate, that in many instances, despite its relative lack of precision, TestTube can reduce the time\u00a0\u2026", "num_citations": "155\n", "authors": ["306"]}
{"title": "A historical perspective on runtime assertion checking in software development\n", "abstract": " This report presents initial results in the area of software testing and analysis produced as part of the Software Engineering Impact Project. The report describes the historical development of runtime assertion checking, including a description of the origins of and significant features associated with assertion checking mechanisms, and initial findings about current industrial use. A future report will provide a more comprehensive assessment of development practice, for which we invite readers of this report to contribute information.", "num_citations": "146\n", "authors": ["306"]}
{"title": "Method and apparatus for propagating content filters for a publish-subscribe network\n", "abstract": " A method and apparatus provide for propagating filters in a publish-subscribe network. A plurality of filters relating to subscriptions to content in the network are received. A number of the filters is reduced based upon particular criteria, and the reduced number of filters are propagated for use in satisfying the subscriptions. A filter receiving module, a filter reduction module, and a filter propagation module may execute these steps.", "num_citations": "141\n", "authors": ["306"]}
{"title": "System and method for selecting test units to be re-run in software regression testing\n", "abstract": " A system and method for selective regression testing of a software system that determines which test units of a test suite must be re-run after a modification to the software system. The entities which are covered by each test unit are identified. When the software system is modified the entities which were changed by the modification are identified. The test units which need to be re-run are determined by analyzing the change information and the coverage information to select those test units that cover changed entities.", "num_citations": "138\n", "authors": ["306"]}
{"title": "Automated generation of context-aware tests\n", "abstract": " The incorporation of context-awareness capabilities into pervasive applications allows them to leverage contextual information to provide additional services while maintaining an acceptable quality of service. These added capabilities, however, introduce a distinct input space that can affect the behavior of these applications at any point during their execution, making their validation quite challenging. In this paper, we introduce an approach to improve the test suite of a context-aware application by identifying context-aware program points where context changes may affect the application's behavior, and by systematically manipulating the context data fed into the application to increase its exposure to potentially valuable context variations. Preliminary results indicate that the approach is more powerful than existing testing approaches used on this type of application.", "num_citations": "137\n", "authors": ["306"]}
{"title": "Content-based addressing and routing: A general model and its application\n", "abstract": " The designers of communication networks are being challenged by the emergence of a new class of addressing and routing scheme referred to as content-based addressing and routing. This new approach differs from traditional unicast and multicast schemes in that it performs routing based on the data being transported in a message rather than on any specialized addressing and routing information attached to, or otherwise associated with, the message. An example of an application for content-based addressing and routing is an event notification service, which is a general-purpose facility for asynchronously and implicitly conveying information from generators of events to any and all parties expressing interest in those events. In order to implement content-based addressing and routing, we can adopt well-known and successful network architectures and protocols, provided that we understand how to map the core concepts and functionalities of content-based addressing and routing onto this established infrastructure. Toward that end, we have formulated a general, yet powerful model of addressing and routing that allows us to formalize the crucial aspects of content-based addressing and routing in a surprisingly simple manner. Furthermore, it allows us to treat traditional unicast and multicast addressing and routing uniformly as instances of this more general model. This paper presents our model and demonstrates its utility by showing its application to the design of an existing event notification service.", "num_citations": "136\n", "authors": ["306"]}
{"title": "Using scenarios to predict the reliability of concurrent component-based software systems\n", "abstract": " Scenarios are a popular means for capturing behavioural requirements of software systems early in the lifecycle. Scenarios show how components interact to provide system level functionality. If component reliability information is available, scenarios can be used to perform early system reliability assessment. In this paper we present a novel automated approach for predicting software system reliability. The approach involves extending a scenario specification to model (1)\u00a0the probability of component failure, and (2)\u00a0scenario transition probabilities derived from an operational profile of the system. From the extended scenario specification, probabilistic behaviour models are synthesized for each component and are then composed in parallel into a model for the system. Finally, a user-oriented reliability model described by Cheung is used to compute a reliability prediction from the system behaviour model\u00a0\u2026", "num_citations": "130\n", "authors": ["306"]}
{"title": "Towards a method of programming with assertions\n", "abstract": " Embedded assertions have long been recognized as a pot entially powerful tool for automatic runtime detection of software faults during debugging, testing and maintenance, Yet despite the richness of the notations and the maturity of the techniques and tools that have been develop ed for programming with assertions, assertions are a development tool that has seen little widespread use in practice. The main reasons seem to be that(1) previous assertion processing tools did not integrate easily with existing programming environments, and (2) it is not well understood what kinds of assertions are most effective at detecting software faults. This paper describes experience using an assertion processing tool that was built to address the concerns of ease-of-use and effectiveness. The tool is called APP, an Annotation Preprocessor for C programs developed in UNIX-based development environments. APP has been\u00a0\u2026", "num_citations": "126\n", "authors": ["306"]}
{"title": "Enabling confidentiality in content-based publish/subscribe infrastructures\n", "abstract": " Content-based publish/subscribe (CBPS) is an interaction model where the interests of subscribers are stored in a content-based forwarding infrastructure to guide routing of notifications to interested parties. In this paper, we focus on answering the following question: can we implement content-based publish/subscribe while keeping subscriptions and notifications confidential from the forwarding brokers? Our contributions include a systematic analysis of the problem, providing a formal security model and showing that the maximum level of attainable security in this setting is restricted. We focus on enabling provable confidentiality for commonly used applications and subscription languages in CBPS and present a series of practical provably secure protocols, some of which are novel and others adapted from existing work. We have implemented these protocols in Siena, a popular CBPS system. Evaluation results\u00a0\u2026", "num_citations": "124\n", "authors": ["306"]}
{"title": "Context-aware adaptive applications: Fault patterns and their automated identification\n", "abstract": " Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of\u00a0\u2026", "num_citations": "123\n", "authors": ["306"]}
{"title": "Impact analysis of database schema changes\n", "abstract": " We propose static program analysis techniques for identifying the impact of relational database schema changes upon object-oriented applications. We use dataflow analysis to extract all possible database interactions that an application may make. We then use this information to predict the effects of schema change. We evaluate our approach with a case-study of a commercially available content management system, where we investigated 62 versions of between 70k-127k LoC and a schema size of up to 101 tables and 568 stored procedures. We demonstrate that the program analysis must be more precise, in terms of context-sensitivity than related work. However, increasing the precision of this analysis increases the computational cost. We use program slicing to reduce the size of the program that needs to be analyzed. Using this approach, we are able to analyse the case study in under 2 minutes on a\u00a0\u2026", "num_citations": "120\n", "authors": ["306"]}
{"title": "Reducing congestion effects in wireless networks by multipath routing\n", "abstract": " We propose a solution to improve fairness and increase throughput in wireless networks with location information. Our approach consists of a multipath routing protocol, biased geographical routing (BGR), and two congestion control algorithms, in-network packet scatter (IPS) and end-to-end packet scatter (EPS), which leverage BGR to avoid the congested areas of the network. BGR achieves good performance while incurring a communication overhead of just 1 byte per data packet, and has a computational complexity similar to greedy geographic routing. IPS alleviates transient congestion by splitting traffic immediately before the congested areas. In contrast, EPS alleviates long term congestion by splitting the flow at the source, and performing rate control. EPS selects the paths dynamically, and uses a less aggressive congestion control mechanism on non-greedy paths to improve energy efficiency. Simulation\u00a0\u2026", "num_citations": "118\n", "authors": ["306"]}
{"title": "A study in software process data capture and analysis\n", "abstract": " A model of the software process that is based on the notion of events characterizing identifiable, instantaneous milestones in a process was developed, along with capture and analysis techniques suited to that model. A study was undertaken to gain experience with both the model and the capture and analysis techniques. Event data on several enactments of the build process of a large, complex software project were captured and entered into a database, and several queries were run against the data. The queries implement a variety of analyses on the event data by examining relationships among events, such as dependencies and time intervals. The output of the queries is statistical data that can be used to guide the design of process improvements. While the data collected in the study are incomplete, the initial results demonstrate the viability of this approach to capture and analysis.< >", "num_citations": "109\n", "authors": ["306"]}
{"title": "Model-based fault detection in context-aware adaptive applications\n", "abstract": " Applications running on mobile devices are heavily context-aware and adaptive, leading to new analysis and testing challenges as streams of context values drive these applications to undesired configurations that are not easily exposed by existing validation techniques. We address this challenge by employing a finite-state model of adaptive behavior to enable the detection of faults caused by (1) erroneous adaptation logic, and (2) asynchronous updating of context information, which leads to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors that we detect automatically by analyzing the system's adaptation model. We illustrate our approach on a simple but realistic application in which a cellphone's configuration profile is changed automatically based on the user's\u00a0\u2026", "num_citations": "104\n", "authors": ["306"]}
{"title": "Issues in supporting event-based architectural styles\n", "abstract": " The development of complex software systems is demanding well established approaches that guarantee robustness of products, economy of the development process, and rapid time to market. This need is becoming more and more relevant as the requirements of customers and the potential of computer telecommunication networks grow.To address this issue, researchers in the field of software architecture are defining a number of languages and tools that support the definition and validation of the architecture of systems. Also, a number of architectural styles are being formalized. Each of them defines \u201ca set of design rules that identify the kinds of components and connectors that may be used to compose a system or a subsystem, together with local or global constraints on the way the composition is done\u201d[5]. The formalization of styles helps the understanding and categorization of existing architectures and\u00a0\u2026", "num_citations": "103\n", "authors": ["306"]}
{"title": "Design of a scalable event notification service: Interface and architecture\n", "abstract": " Event-based distributed systems are programmed to operate in response to events. An event notification service is an application-independent infrastructure that supports the construction of event-based systems. While numerous technologies have been developed for supporting event-based interactions over local-area networks, these technologies do not scale well to wide-area networks such as the Internet. Wide-area networks pose new challenges that have to be attacked with solutions that specially address issues of scalability. This paper presents Siena, a scalable event notification service that is based on a distributed architecture of event servers. We first present a formally interface that is based on an extension to the publishsubscribe protocol. We then describe and compare several different server topologies and routing algorithms. We conclude by brie briefly discussing related work, our experience with an initial implementation of Siena, and a framework for evaluating the scalability of event notification services such as Siena.Descriptors:", "num_citations": "102\n", "authors": ["306"]}
{"title": "Adequate testing of component-based software\n", "abstract": " People have long advocated a component-based approach to software construction as a way of simplifying the design and maintenance of large software systems, increasing the opportunities for reuse, and increasing software development productivity. Although the technology for constructing component-based software is relatively advanced, we lack a sufficient theoretical basis for testing component-based software. This paper initiates the development of such a theory. The main result is a formal definition of the concept C-adequate-for-for adequate unit testing of a component and the concept C-adequateon-M for adequate integration testing of a component-based system. The paper uses these concepts to discuss practical considerations in adequate testing of component-based software.", "num_citations": "98\n", "authors": ["306"]}
{"title": "A framework for characterization and analysis of software system scalability\n", "abstract": " The term scalability appears frequently in computing literature, but it is a term that is poorly defined and poorly understood. The lack of a clear, consistent and systematic treatment of scalability makes it difficult to evaluate claims of scalability and to compare claims from different sources. This paper presents a framework for precisely characterizing and analyzing the scalability of a software system. The framework treats scalability as a multi-criteria optimization problem and captures the dependency relationships that underlie typical notions of scalability. The paper presents the results of a case study in which the framework and analysis method were applied to a real-world system, demonstrating that it is possible to develop a precise, systematic characterization of scalability and to use the characterization to compare the scalability of alternative system designs.", "num_citations": "96\n", "authors": ["306"]}
{"title": "Method and apparatus for implementing query-response interactions in a publish-subscribe network\n", "abstract": " A method and apparatus provide for implementing query-response interactions on a publish-subscribe network. An advertisement relating to a data set and a query representing a logical expression are received. The advertisement is mapped to a corresponding subscription. The query is mapped to a corresponding notification. The subscription and the notification are used for implementing of the advertisement and the query in the network.", "num_citations": "92\n", "authors": ["306"]}
{"title": "Interfaces and algorithms for a wide-area event notification service\n", "abstract": " The components of a loosely-coupled system are typically designed to operate by generating and responding to asynchronous events. An event notification service is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. The two primary services that should be provided to components by the infrastructure are notification selection (ie, determining which notifications match which subscriptions) and notification delivery (ie, routing matching notifications from publishers to subscribers). Numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event notifications. Therefore, they suffer from an inherent inability to scale to wide-area networks, such as the Internet, where the number and physical distribution of the service\u2019s clients can quickly overwhelm a centralized solution. The critical challenge in the setting of a wide-area network is to maximize the expressiveness in the selection mechanism without sacrificing scalability in the delivery mechanism. This paper presents SIENA, an event notification service that we have designed to exhibit both expressiveness and scalability. We describe the service\u2019s interface to applications, the algorithms used by networks of servers to select and deliver event notifications, and the strategies used to optimize performance. We present results of simulation studies that examine the scalability and performance of the service\u00a0\u2026", "num_citations": "83\n", "authors": ["306"]}
{"title": "Model checking service compositions under resource constraints\n", "abstract": " When enacting a web service orchestration defined using the Business Process Execution Language (BPEL) we observed various safety property violations. This surprised us considerably as we had previously established that the orchestration was free of such property violations using existing BPEL model checking techniques. In this paper, we describe the origins of these violations. They result from a combination of design and deployment decisions, which include the distribution of services across hosts, the choice of synchronisation primitives in the process and the threading configuration of the servlet container that hosts the orchestrated web services. This leads us to conclude that model checking approaches that ignore resource constraints of the deployment environment are insufficient to establish safety and liveness properties of service orchestrations specifically, and distributed systems more generally\u00a0\u2026", "num_citations": "80\n", "authors": ["306"]}
{"title": "Challenges for distributed event services: Scalability vs. expressiveness\n", "abstract": " The event-based style is a very promising approach for the development and integration of distributed objects. An event notification service (or event service) is the glue that ties together distributed components in an event-based architecture. An event service implements what is commonly known as the publish/subscribe protocol: components publish events to inform other components of a change in their internal state or to request services from other components; the event service registers the interest of components expressed by means of subscriptions and consequently dispatches event notifications. In practice, the event service mediates and facilitates the interaction among applications by filtering, aggregating, and delivering events on their behalf. Because of this decoupling, an event service is particularly suitable for supporting heterogeneous distributed objects.The functionality of an event service is characterized by two conflicting requirements: scalability and expressiveness. Scalability means that the service must be available over a wide-area network populated by numerous components each one producing and consuming many events. Expressiveness demands a rich subscription language that gives applications a flexible and fine-grained selection mechanism to describe precisely those events or combinations of events in which they are interested. This tension between scalability and expressiveness is evident in all the recently proposed technologies. The ones that provide an event service facility (eg, the CORBA Event Service [4], the JavaTM Distributed Event Specification [8], iBus [7], JEDI [2], Keryx [10], Elvin [6], and TIBCO\u2019s TIB\u00a0\u2026", "num_citations": "80\n", "authors": ["306"]}
{"title": "QoS-aware service composition in Dino\n", "abstract": " A major advantage offered by Web services technologies is the ability to dynamically discover and invoke services. This ability is particularly important for operations of many applications executing in open dynamic environments. The QoS properties of the required and provided services play a significant role in dynamic discovery and invocation of services in open dynamic environments. In this paper, we discuss our approach to QoS specification and service provider selection, in the context of our work on the Dino project. The service provider selection algorithm used in Dino takes into account the relative benefit offered by a provider with respect to the requester-specified QoS criteria, and the trustworthiness of the provider. We explain our approach using an example from the automotive domain.", "num_citations": "72\n", "authors": ["306"]}
{"title": "Volare: context-aware adaptive cloud service discovery for mobile systems\n", "abstract": " With the recent widespread use of smart mobile devices, as well as the increasing availability of fast and reliable wireless Internet connections for mobile devices, there is increased interest in mobile applications where the majority of the processing occurs on the server side. The flexibility, stability and scalability offered by cloud services make them an ideal architecture to use in client applications in a resource limited mobile environment. This is because mobile application usage patterns tend to be uneven, with various usage spikes according to time and location. However, the mobile setting presents a set of new challenges that cloud service discovery methods developed for non-mobile environments cannot address. The requirements a mobile client device will have from a cloud service may change due to changes in the context of the device, which may include hardware resources, environmental variables or\u00a0\u2026", "num_citations": "71\n", "authors": ["306"]}
{"title": "WREN---an environment for component-based development\n", "abstract": " Prior research in software environments focused on three important problems---tool integration, artifact management, and process guidance. The context for that research, and hence the orientation of the resulting environments, was a traditional model of development in which an application is developed completely from scratch by a single organization. A notable characteristic of component-based development is its emphasis on integrating independently developed components produced by multiple organizations. Thus, while component-based development can benefit from the capabilities of previous generations of environments, its special nature induces requirements for new capabilities not found in previous environments. This paper is concerned with the design of component-based development environments, or CBDEs. We identify seven important requirements for CBDEs and discuss their rationale, and we\u00a0\u2026", "num_citations": "68\n", "authors": ["306"]}
{"title": "Specifying concurrent systems with TSL\n", "abstract": " The Task Sequencing Language (TSL), which lets programmers specify constraints on the behavior of concurrent programs, is described. The constraints, specified by high-level annotations to Ada programs, are monitored at runtime; when a violation is detected, control is returned to the user, along with information about the nature of the violation. It is argued that such a tool is a necessary crutch for human reasoning capabilities when dealing with the complexities of concurrent process interaction. The first large-scale application of TSL, the specification and validation of a distributed tasking supervisor for Ada, is described.< >", "num_citations": "68\n", "authors": ["306"]}
{"title": "Using component metadata to regression test component\u2010based software\n", "abstract": " Increasingly, modern\u2010day software systems are being built by combining externally\u2010developed software components with application\u2010specific code. For such systems, existing program\u2010analysis\u2010based software engineering techniques may not directly apply, due to lack of information about components. To address this problem, the use of component metadata has been proposed. Component metadata are metadata and metamethods provided with components, that retrieve or calculate information about those components. In particular, two component\u2010metadata\u2010based approaches for regression test selection are described: one using code\u2010based component metadata and the other using specification\u2010based component metadata. The results of empirical studies that illustrate the potential of these techniques to provide savings in re\u2010testing effort are provided. Copyright \u00a9 2006 John Wiley & Sons, Ltd.", "num_citations": "64\n", "authors": ["306"]}
{"title": "A framework for modelling and analysis of software systems scalability\n", "abstract": " Scalability is a widely-used term in scientific papers, technical magazines and software descriptions. Its use in the most varied contexts contribute to a general confusion about what the term really means. This lack of consensus is a potential source of problems, as assumptions are made in the face of a scalability claim. A clearer and widely-accepted understanding of scalability is required to restore the usefulness of the term. This research investigates commonly found definitions of scalability and attempts to capture its essence in a systematic framework. Its expected contribution is in assisting software developers to reason, characterize, communicate and adjust the scalability of software systems.", "num_citations": "63\n", "authors": ["306"]}
{"title": "Representing Semantically Analyzed C++ Code with Reprise.\n", "abstract": " A prominent stumbling block in the spread of the C++ programming language has been a lack of programming and analysis tools to aid development and maintenance of C++ systems. One way to make the job of tool developers easier and to increase the quality of the tools they create is to factor out the common components of tools and provide the components as easily (re) used building blocks. Those building blocks include lexical, syntactic, and semantic analyzers, tailored database derivers, code annotators and instrumentors, and code generators. From these building blocks, tools such as structure browsers, data-ow analyzers, program/speci cation veri ers, metrics collectors, compilers, interpreters, and the like can be built more easily and cheaply. We believe that for C++ programming and analysis tools the most primitive building blocks are centered around a common representation of semantically analyzed C++ code.In this paper we describe such a representation, called Reprise (REPResentation Including SEmantics). The conceptual model underlying Reprise is based on the use of expressions to capture all semantic information about both the C++ language and code written in C++. The expressions can be viewed as forming a directed graph, where there is an explicit connection from each use of an entity to the declaration giving the semantics of that entity. We elaborate on this model, illustrate how various features of C++ are represented, discuss some categories of tools that would create and manipulate Reprise representations, and brie y describe our current implementation. This paper is not intended to provide a complete de\u00a0\u2026", "num_citations": "60\n", "authors": ["306"]}
{"title": "Reliability prediction in model-driven development\n", "abstract": " Evaluating the implications of an architecture design early in the software development lifecycle is important in order to reduce costs of development. Reliability is an important concern with regard to the correct delivery of software system service. Recently, the UML Profile for Modeling Quality of Service has defined a set of UML extensions to represent dependability concerns (including reliability) and other non-functional requirements in early stages of the software development lifecycle. Our research has shown that these extensions are not comprehensive enough to support reliability analysis for model-driven software engineering, because the description of reliability characteristics in this profile lacks support for certain dynamic aspects that are essential in modeling reliability. In this work, we define a profile for reliability analysis by extending the UML 2.0 specification to support reliability prediction based\u00a0\u2026", "num_citations": "56\n", "authors": ["306"]}
{"title": "Urbanfm: Inferring fine-grained urban flows\n", "abstract": " Urban flow monitoring systems play important roles in smart city efforts around the world. However, the ubiquitous deployment of monitoring devices, such as CCTVs, induces a long-lasting and enormous cost for maintenance and operation. This suggests the need for a technology that can reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity. In this paper, we aim to infer the real-time and fine-grained crowd flows throughout a city based on coarse-grained observations. This task is challenging due to the two essential reasons: the spatial correlations between coarse-and fine-grained urban flows, and the complexities of external impacts. To tackle these issues, we develop a method entitled UrbanFM based on deep neural networks. Our model consists of two major parts: 1) an inference network to generate fine-grained flow distributions from coarse-grained\u00a0\u2026", "num_citations": "53\n", "authors": ["306"]}
{"title": "Learning structures of interval-based Bayesian networks in probabilistic generative model for human complex activity recognition\n", "abstract": " Complex activity recognition is challenging due to the inherent uncertainty and diversity of performing a complex activity. Normally, each instance of a complex activity has its own configuration of atomic actions and their temporal dependencies. In our previous work, we proposed an atomic action-based Bayesian model that constructs Allen\u2019s interval relation networks to characterize complex activities in a probabilistic generative way: By introducing latent variables from the Chinese restaurant process, our approach is able to capture all possible styles of a particular complex activity as a unique set of distributions over atomic actions and relations. However, a major limitation of our previous models is their fixed network structures, which may lead to an overtrained or undertrained model owing to unnecessary or missing links in a network. In this work, we present an improved model that network structures can be\u00a0\u2026", "num_citations": "53\n", "authors": ["306"]}
{"title": "Known unknowns: Testing in the presence of uncertainty\n", "abstract": " Uncertainty is becoming more prevalent in the software systems we build, introducing challenges in the way we develop software, especially in software testing. In this work we explore how uncertainty affects software testing, how it is managed currently, and how it could be treated more effectively.", "num_citations": "45\n", "authors": ["306"]}
{"title": "Supporting architectural concerns in component-interoperability standards\n", "abstract": " There has been considerable work in industry on the development of component-interoperability models, such as COM, CORBA and JavaBeans. These models are intended to reduce the complexity of software development and to facilitate reuse of off-the-shelf components. The focus of these models is syntactic interface specification, component packaging, intercomponent communication, and bindings to a runtime environment. What these models lack is a consideration of architectural concerns: specifying systems of communicating components, explicitly representing loci of component interaction, and exploiting architectural styles that provide well understood global design solutions. The work described involves introducing support for architectural concerns in component models, particularly studying techniques to support notions of architectural style and explicit connectors. The JavaBeans component model\u00a0\u2026", "num_citations": "42\n", "authors": ["306"]}
{"title": "Method for storing Boolean functions to enable evaluation, modification, reuse, and delivery over a network\n", "abstract": " Data structures for storing subscription predicates for transmission in a publish-subscribe network. Subscriptions include Boolean-valued predicates defining content desired by a subscriber. An agent application converts the predicates into a suitable form for storing them in a common data structure. The predicates are used to generate filters to encapsulate the subscriptions, and the filters are specified in the data structure in addition to the Boolean relationships of the predicates. Routers in a network core use the data structure for content-based routing, which involves applying attributes in received packets to the filters.", "num_citations": "41\n", "authors": ["306"]}
{"title": "Predicting urban water quality with ubiquitous data\n", "abstract": " Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. However, predicting the urban water quality is a challenging task since the water quality varies in urban spaces non-linearly and depends on multiple factors, such as meteorology, water usage patterns, and land uses. In this work, we forecast the water quality of a station over the next few hours from a data-driven perspective, using the water quality data and water hydraulic data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, pipe networks, structure of road networks, and point of interests (POIs). First, we identify the influential factors that affect the urban water quality via extensive experiments. Second, we present a multi-task multi-view learning method to fuse those multiple datasets from different domains into an unified learning model. We evaluate our method with real-world datasets, and the extensive experiments verify the advantages of our method over other baselines and demonstrate the effectiveness of our approach.", "num_citations": "40\n", "authors": ["306"]}
{"title": "A Non-Parametric Generative Model for Human Trajectories.\n", "abstract": " Modeling human mobility and generating synthetic yet realistic location trajectories play a fundamental role in many (privacy-aware) analysis and design processes that operate on location data. In this paper, we propose a non-parametric generative model for location trajectories that can capture high-order geographic and semantic features of human mobility. We design a simple and intuitive yet effective embedding for locations traces, and use generative adversarial networks to produce data points in this space, which will finally be transformed back to a sequential location trajectory form. We evaluate our method on realistic location trajectories and compare our synthetic traces with multiple existing methods on how they preserve geographic and semantic features of real traces at both aggregated and individual levels. Our empirical results prove the capability of our generative model in preserving various useful properties of real data.", "num_citations": "39\n", "authors": ["306"]}
{"title": "Formal methods and testing: why the state-of-the art is not the state-of-the practice\n", "abstract": " Richard Denney noted that if one is discussing formal methods in testing and analysis in a commercial setting, their role in quality assurance (QA) must be considered. As to why formal methods are not\" state-of-the-practice\" for QA, he suggested a lack of focus by the formal methods community on QA groups as a consumer of formal methods. The use of formal methods in commercial QA presents a different set of needs than those of developers or researchers. He made a number of suggestions as to contributions formal methods could make in the QA area. The suggestions stem from two facts-of-life in QA groups: Most QA groups work with development groups that do not use formal methods, and most of the work involves analysis of other people's (often very informal) work. He suggested a focus on making formal methods a viable analysis tool for technical peer reviews (eg, inspections). Also, there is a need for\u00a0\u2026", "num_citations": "35\n", "authors": ["306"]}
{"title": "Packet routing via payload inspection for digital content delivery\n", "abstract": " Packet routing via payload inspection at routers in a core of a distributed network for use in distributing digital content such as video, music, and software. Packets include subjects and attributes in addition to routing information. The subjects correspond with particular types of content for subscriptions, and the attributes encapsulate the data or content, which can include video, music, or software such as software updates. The routers store filters corresponding with subscriptions to content. Upon receiving a packet, a router inspects the payload section of the packet containing the attributes in order to retrieve the attributes and apply them to the filters for the subscriptions to the digital content. If an attribute satisfies a filter, the packet is routed to the next link. If the attributes do not satisfy the filters, the router discards the packet. These routing decisions are distributed among routers in the network core.", "num_citations": "34\n", "authors": ["306"]}
{"title": "Challenges in exploiting architectural models for software testing\n", "abstract": " Software architectural modeling offers a natural framework for designing and analyzing modern large-scale software systems and for composing systems from reusable off-theshelf components. However, the nature of componentbased software presents particularly unique challenges for testing component-based systems. To date there have been relatively few attempts to establish a sound theoretical basis for testing component-based software.This paper discusses challenges in exploiting architectural models for software testing. The discussion is framed in terms of the author\u2019s recent work on defining a formal model of test adequacy for component-based software, and how this model can be enhanced to exploit formal architectural models.", "num_citations": "34\n", "authors": ["306"]}
{"title": "Method and apparatus for content-based packet routing using compact filter storage and off-line pre-computation\n", "abstract": " A method and apparatus provide for content-based routing of packets in a publish-subscribe network. A packet is received via the network. A map specifying computed filter coverage in an attribute space is accessed. Content of the packet is inspected for a routing decision for the packet. The map is used for the routing decision for the packet. The packet is routed based upon the inspected content of the packet and the map. A router for content-based routing of packets in a publish-subscribe network, comprising modules for performing this method is provided. A network including a plurality of such routers is provided. A computer-readable medium including instructions for performing this method is provided.", "num_citations": "33\n", "authors": ["306"]}
{"title": "A model driven approach for software systems reliability\n", "abstract": " The main contribution of this research is to provide platform-independent means to support reliability design following the principles of a model driven approach. The contribution aims to systematically address dependability concerns from the early to the late stages of software development. MDA appears to be a suitable framework to assess these concerns and, therefore, semantically integrate analysis and design models into one environment.", "num_citations": "33\n", "authors": ["306"]}
{"title": "MMKG: multi-modal knowledge graphs\n", "abstract": " We present Mmkg, a collection of three knowledge graphs that contain both numerical features and (links to) images for all entities as well as entity alignments between pairs of KGs. Therefore, multi-relational link prediction and entity matching communities can benefit from this resource. We believe this data set has the potential to facilitate the development of novel multi-modal learning approaches for knowledge graphs. We validate the utility of Mmkg in the  link prediction task with an extensive set of experiments. These experiments show that the task at hand benefits from learning of multiple feature types.", "num_citations": "32\n", "authors": ["306"]}
{"title": "Multi-layer faults in the architectures of mobile, context-aware adaptive applications\n", "abstract": " Modern hand-held devices are equipped with multiple context sensors exploited by increasingly sophisticated software applications, called Context-Aware Adaptive Applications (CAAAs), that adapt automatically to changes in the surrounding environment, such as by responding to the location and speed of the user. The architecture of CAAAs is typically layered and incorporates a context-awareness component to support processing of context values and triggering of adaptive changes. While this layered architecture is very natural for the design and implementation of CAAAs, it exhibits new kinds of failures that arise as a result of faults that are specific to the choice of technology for specific layers. In this paper we investigate the occurrence of such faults and failures that manifest across architectural layers, and we describe samples of such failures in four CAAAs.", "num_citations": "30\n", "authors": ["306"]}
{"title": "An event-based model of software configuration management\n", "abstract": " Many aspects of software configuration management can be modeled as the occurrence of events that require actions to be performed. For instance, the upgrade of a library module is an event whose occurrence requires several actions to be taken, including the following: q rebuilding all systems of which the library is a component; q testing the rebuilt systems; and q tracking of, and response to, the success or failure of the tests.", "num_citations": "27\n", "authors": ["306"]}
{"title": "An implementation of Anna\n", "abstract": " Anna is a language extension of Ada to include facilities for formally specifying the intended behavior of Ada programs. It augments Ada with precise machine-processable annotations so that well established formal methods of specification and documentation can be applied to Ada programs.This paper describes an implementation of a subset of Anna. The implementation is a transformer that accepts as input an Anna parse tree and produces as output an equivalent Ada parse tree that contains the necessary executable runtime checks for the Anna specifications. An approach called the Checking Function Approach is used. This involves the generation of a function for each annotation and generating calls to these functions at appropriate places. The transformer has to take care of various details like hiding, overloading, nesting, etc.It is hoped that the transformer will eventually cover most of Ann and have various\u00a0\u2026", "num_citations": "27\n", "authors": ["306"]}
{"title": "The evolution of software evolvability\n", "abstract": " We analyze two trends that have influenced the evolvability of component-based applications: increase of component exchangeability and increase of component distance. Exchangeability mechanisms can be classified either as code reuse or as service reuse. Component distance can vary from file scope to Internet scope. We discuss the various stages of evolvability in these dimensions, describe the state of the art, and speculate on future developments.", "num_citations": "26\n", "authors": ["306"]}
{"title": "Mining performance specifications\n", "abstract": " Functional testing is widespread and supported by a multitude of tools, including tools to mine functional specifications. In contrast, non-functional attributes like performance are often less well understood and tested. While many profiling tools are available to gather raw performance data, interpreting this raw data requires expert knowledge and a thorough understanding of the underlying software and hardware infrastructure. In this work we present an approach that mines performance specifications from running systems autonomously. The tool creates performance models during runtime. The mined models are analyzed further to create compact and comprehensive performance assertions. The resulting assertions can be used as an evidence-based performance specification for performance regression testing, performance monitoring, or as a foundation for more formal performance specifications.", "num_citations": "24\n", "authors": ["306"]}
{"title": "Reliability of run-time quality-of-service evaluation using parametric model checking\n", "abstract": " Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems. Complex behavioral performance metrics (PMs) are useful but often difficult to monitor or measure. Probabilistic model checking, especially parametric model checking, can support the computation of aggre- gate functions for a broad range of those PMs. In practice, those PMs may be defined with parameters determined by run-time data. In this paper, we address the reliability of QoS evaluation using parametric model checking. Due to the imprecision with the instantiation of parameters, an evaluation outcome may mislead the judgment about requirement violations. Based on a general assumption of run-time data distribution, we present a novel framework that contains light-weight statistical inference methods to analyze the re- liability of a parametric model checking output with respect to an intuitive criterion. We also present case\u00a0\u2026", "num_citations": "24\n", "authors": ["306"]}
{"title": "An iterative decision-making scheme for markov decision processes and its application to self-adaptive systems\n", "abstract": " Software is often governed by and thus adapts to phenomena that occur at runtime. Unlike traditional decision problems, where a decision-making model is determined for reasoning, the adaptation logic of such software is concerned with empirical data and is subject to practical constraints. We present an Iterative Decision-Making Scheme (IDMS) that infers both point and interval estimates for the undetermined transition probabilities in a Markov Decision Process (MDP) based on sampled data, and iteratively computes a confidently optimal scheduler from a given finite subset of schedulers. The most important feature of IDMS is the flexibility for adjusting the criterion of confident optimality and the sample size within the iteration, leading to a tradeoff between accuracy, data usage and computational overhead. We apply IDMS to an existing self-adaptation framework Rainbow and conduct a case study\u00a0\u2026", "num_citations": "24\n", "authors": ["306"]}
{"title": "Roar: increasing the flexibility and performance of distributed search\n", "abstract": " To search the web quickly, search engines partition the web index over many machines, and consult every partition when answering a query. To increase throughput, replicas are added for each of these machines. The key parameter of these algorithms is the trade-off between replication and partitioning: increasing the partitioning level improves query completion time since more servers handle the query, but may incur non-negligible startup costs for each sub-query. Finding the right operating point and adapting to it can significantly improve performance and reduce costs.", "num_citations": "24\n", "authors": ["306"]}
{"title": "Revisiting content-based publish/subscribe\n", "abstract": " Publish-subscribe is a powerful paradigm for distributed communication based on decoupled producers and consumers of information. Its event-driven nature makes it very appealing for large-scale data dissemination infrastructures. Various architectures were proposed in recent years that provide very diverse features. However, there are few well-defined metrics in the publish-subscribe area that would allow their evaluation and comparison. In this paper, we provide a broad overview of relevant quality-of-service metrics and describe their specific meaning in the context of distributed and decentralized publish-subscribe systems. Our goal is to provide a common base for future evaluations of emerging systems and for the design of qualityof- service aware publish-subscribe infrastructures.", "num_citations": "24\n", "authors": ["306"]}
{"title": "A bandit approach for intelligent IoT service composition across heterogeneous smart spaces\n", "abstract": " The number of connected devices and services available across the Internet of Things (IoT) is rapidly expanding. In this paper, we present a novel mechanism that improves the ability of mobile clients to dynamically discover potentially unknown IoT services across the IoT's increasingly fragmented protocol landscape. Our approach leverages the detected usage patterns of crowds of mobile users interacting with various IoT services in-situ. We model these usage patterns as a contextual bandit problem that incorporates arbitrarily complex contextual cues, such as device settings, users, environment, activities, etc. We present a modified LinUCB-hybrid algorithm that regulates the exploitation of known services based on contextual cues and makes use of exploration at appropriate times to discover contextually relevant IoT services for mobile users who may not have sufficient prior knowledge. These recommended\u00a0\u2026", "num_citations": "23\n", "authors": ["306"]}
{"title": "Fusing social networks with deep learning for volunteerism tendency prediction\n", "abstract": " Social networks contain a wealth of useful information. In this paper, we study a challenging task for integrating users' information from multiple heterogeneous social networks to gain a comprehensive understanding of users' interests and behaviors. Although much effort has been dedicated to study this problem, most existing approaches adopt linear or shallow models to fuse information from multiple sources. Such approaches cannot properly capture the complex nature of and relationships among different social networks. Adopting deep learning approaches to learning a joint representation can better capture the complexity, but this neglects measuring the level of confidence in each source and the consistency among different sources. In this paper, we present a framework for multiple social network learning, whose core is a novel model that fuses social networks using deep learning with source confidence and consistency regularization. To evaluate the model, we apply it to predict individuals' tendency to volunteerism. With extensive experimental evaluations, we demonstrate the effectiveness of our model, which outperforms several state-of-the-art approaches in terms of precision, recall and F1-score.", "num_citations": "23\n", "authors": ["306"]}
{"title": "Process-Centered Environment (Only) Support Environment-Centered Processes.\n", "abstract": " The software process research community has concentrated a large measure of its e ort on the problem of how to\\computerize\" the software process. In particular, the notion of a process-centered environment has emerged, dominating software environment research for the past several years. Among the many bene ts touted for process-centered environments are the ability to automate various aspects of a process and the ability to monitor the progress of a process in order to guide, enforce, or measure that process. This approach has shown great promise and indeed has even shown some early successes. Unfortunately, this emphasis on computerization in general, and on process-centered environments in particular, tends to focus attention on exactly those aspects of process that can be computerized, while giving short shrift to those aspects not amenable to computerization. This issue became clear during the past year as we studied a large, mature software process in use at AT&T. We performed the study as part of an e ort to develop process data capture and analysis techniques that could support the critical task of process improvement 4]. Our approach was to focus on the dynamic aspects of the process, such as the order of, and time taken by, each step in the process, as opposed to, say, the static roles and responsibilities assigned to project personnel or the static relationships among tools and product components. We took this approach in part because the process's dynamic aspects were the least well understood. In addition, we hypothesized that process problems ultimately lead to wasted intervals of time and that those problems can\u00a0\u2026", "num_citations": "23\n", "authors": ["306"]}
{"title": "Asymptotic bounds for quantitative verification of perturbed probabilistic systems\n", "abstract": " The majority of existing probabilistic model checking case studies are based on well understood theoretical models and distributions. However, real-life probabilistic systems usually involve distribution parameters whose values are obtained by empirical measurements and thus are subject to small perturbations. In this paper, we consider perturbation analysis of reachability in the parametric models of these systems (i.e., parametric Markov chains) equipped with the norm of absolute distance. Our main contribution is a method to compute the asymptotic bounds in the form of condition numbers for constrained reachability probabilities against perturbations of the distribution parameters of the system. The adequacy of the method is demonstrated through experiments with the Zeroconf protocol and the hopping frog problem.", "num_citations": "20\n", "authors": ["306"]}
{"title": "A model-driven approach to dynamic and adaptive service brokering using modes\n", "abstract": " Industry and academia are exploring ways to exploit the services paradigm to assist in the challenges of software self-management. In this paper we present a novel approach which aims to bring these two fields closer by specifying the requirements and capabilities within a UML2 model architecture style and illustrating how these model elements are used to generate specifications for dynamic runtime service brokering given different modes of a software system. The approach is implemented in a tool suite integrated into the Eclipse IDE with a prototype runtime service broker engine.", "num_citations": "20\n", "authors": ["306"]}
{"title": "Sensitivity analysis for a scenario-based reliability prediction model\n", "abstract": " As a popular means for capturing behavioural requirements, scenarios show how components interact to provide system-level functionality. If component reliability information is available, scenarios can be used to perform early system reliability assessment. In previous work we presented an automated approach for predicting software system reliability that extends a scenario specification to model (1) the probability of component failure, and (2) scenario transition probabilities. Probabilistic behaviour models of the system are then synthesized from the extended scenario specification. From the system behaviour model, reliability prediction can be computed. This paper complements our previous work and presents a sensitivity analysis that supports reasoning about how component reliability and usage profiles impact on the overall system reliability. For this purpose, we present how the system reliability varies as a\u00a0\u2026", "num_citations": "20\n", "authors": ["306"]}
{"title": "A methodology for the design of Ada transformation tools in a DIANA environment\n", "abstract": " Implementing and testing transformations incrementally makes DIANA-based Ada software tools easier to debug and more efficient.", "num_citations": "20\n", "authors": ["306"]}
{"title": "Perturbation analysis in verification of discrete-time Markov chains\n", "abstract": " Perturbation analysis in probabilistic verification addresses the robustness and sensitivity problem for verification of stochastic models against qualitative and quantitative properties. We identify two types of perturbation bounds, namely non-asymptotic bounds and asymptotic bounds. Non-asymptotic bounds are exact, pointwise bounds that quantify the upper and lower bounds of the verification result subject to a given perturbation of the model, whereas asymptotic bounds are closed-form bounds that approximate non-asymptotic bounds by assuming that the given perturbation is sufficiently small. We perform perturbation analysis in the setting of Discrete-time Markov Chains. We consider three basic matrix norms to capture the perturbation distance, and focus on the computational aspect. Our main contributions include algorithms and tight complexity bounds for calculating both non-asymptotic bounds and\u00a0\u2026", "num_citations": "18\n", "authors": ["306"]}
{"title": "Automated construction of testing and analysis tools\n", "abstract": " Many software testing and analyse's tools manipulate graph representations of programs, such as abstract syntax trees or abstract semantics graphs. Hand-crafting such tools in conventional programming languages can be difficult, error prone, and time consuming. Our approach is to use application generators targeted for the domain of graph-representation-based testing and analysis tools. Moreover, we generate the generators themselves, so that the development of tools based on different languages and/or representations can also be supported better. In this paper we report on our experiences in developing a system called Aria that generates testing and analysis tools based on an abstract semantics graph representation for C and C++ cabled Reprise. Aria itself was generated by the Genoa system. We demonstrate the utility of Aria and, thereby, the pourer of our approach, by showing Aria's use in the\u00a0\u2026", "num_citations": "18\n", "authors": ["306"]}
{"title": "An environment for Ada software development based on formal specification\n", "abstract": " This report gives an overview of the current status and plans to construct a prototype environment of advanced tools for software and hardware development based on the use of wide-spectrum languages. The wide-spectrum languages include Anna (ANNotated Ada), and TSL (Task Sequencing Language). The tools described here provide interactive aid at all stages in the system development process. Special emphasis is placed on distributed computing, both in providing tools for handling parallelism in the subject system, and in designing tools that utilize parallelism in the programming environment. Applications of these tools include requirements analysis, formal specification, rapid prototyping, testing, formal verfication and construction of self-testing Ada software for multi-processor systems.The report describes an existing environment of prototype tools supporting applications of Anna and TSL to formal\u00a0\u2026", "num_citations": "18\n", "authors": ["306"]}
{"title": "Fine-grained urban flow inference\n", "abstract": " Spatially fine-grained urban flow data is critical for smart city efforts. Though fine-grained information is desirable for applications, it demands much more resources for the underlying storage system compared to coarse-grained data. To bridge the gap between storage efficiency and data utility, in this paper, we aim to infer fine-grained flows throughout a city from their coarse-grained counterparts. This task exhibits two challenges: the spatial correlations between coarse- and fine-grained urban flows, and the complexities of external impacts. To tackle these issues, we develop a model entitled UrbanFM which consists of two major parts: 1) an inference network to generate fine-grained flow distributions from coarse-grained inputs that uses a feature extraction module and a novel distributional upsampling module; 2) a general fusion subnet to further boost the performance by considering the influence of different\u00a0\u2026", "num_citations": "17\n", "authors": ["306"]}
{"title": "Asymptotic perturbation bounds for probabilistic model checking with empirically determined probability parameters\n", "abstract": " Probabilistic model checking is a verification technique that has been the focus of intensive research for over a decade. One important issue with probabilistic model checking, which is crucial for its practical significance but is overlooked by the state-of-the-art largely, is the potential discrepancy between a stochastic model and the real-world system it represents when the model is built from statistical data. In the worst case, a tiny but nontrivial change to some model quantities might lead to misleading or even invalid verification results. To address this issue, in this paper, we present a mathematical characterization of the consequences of model perturbations on the verification distance. The formal model that we adopt is a parametric variant of discrete-time Markov chains equipped with a vector norm to measure the perturbation. Our main technical contributions include a closed-form formulation of asymptotic\u00a0\u2026", "num_citations": "17\n", "authors": ["306"]}
{"title": "Merging component models and architectural styles\n", "abstract": " Components have increasingly become the unit of development of software. In industry, there has been considerable work in the development of component interoperability models, such as ActiveX, CORBA and JavaBeans. In academia, there has been intensive research in developing a notion of software architecture. Our research involves studying how standard component models can be extended to accommodate important issues of architecture, including a notion of architectural style and support for explicit connectors. In this paper, we discuss issues arising from our initial effort in this research, where we have extended the JavaBeans component model to support component composition according to the C2 architectural style.", "num_citations": "17\n", "authors": ["306"]}
{"title": "Directed graph convolutional network\n", "abstract": " Graph Convolutional Networks (GCNs) have been widely used due to their outstanding performance in processing graph-structured data. However, the undirected graphs limit their application scope. In this paper, we extend spectral-based graph convolution to directed graphs by using first- and second-order proximity, which can not only retain the connection properties of the directed graph, but also expand the receptive field of the convolution operation. A new GCN model, called DGCN, is then designed to learn representations on the directed graph, leveraging both the first- and second-order proximity information. We empirically show the fact that GCNs working only with DGCNs can encode more useful information from graph and help achieve better performance when generalized to other models. Moreover, extensive experiments on citation networks and co-purchase datasets demonstrate the superiority of our model against the state-of-the-art methods.", "num_citations": "16\n", "authors": ["306"]}
{"title": "Translation-based sequential recommendation for complex users on sparse data\n", "abstract": " Sequential recommendation is one of the main tasks in recommender systems, where the next action (e.g., purchase, visit, and click) of the user is predicted based on his/her past sequence of actions. Translating Embeddings is a knowledge graph completion approach which was recently adapted to a translation-based sequential recommendation (TransRec) method. We observe a flaw of TransRec when handling complex translations, which hinders it from generating accurate suggestions. In view of this, we propose a  translation-based recommender for complex users  (CTransRec), which utilizes  category-specific projection  and  temporal dynamic relaxation . Using our proposed  Margin-based Pairwise Bayesian Personalized Ranking  and  Time-Aware Negative Sampling , CTransRec outperforms state-of-the-art methods for sequential recommendation on extremely sparse data. The superiority of CTransRec\u00a0\u2026", "num_citations": "16\n", "authors": ["306"]}
{"title": "A model to design and verify context-aware adaptive service composition\n", "abstract": " The introduction of mobile clients and context-aware behaviors into Web Service compositions may generate faults and inconsistencies. We introduce an extension of a composition model where context-awareness is made explicit and a number of correctness properties are verifiable. In particular, our extended model enables the verification of properties commonly used to validate context dependent applications. We also propose a set of algorithms to verify these properties efficiently.", "num_citations": "16\n", "authors": ["306"]}
{"title": "Integrating C2 with the unified modeling language\n", "abstract": " Architecture-based software development is an approach to designing software in which developers focus on one or more high-level models of the software system rather than program source code. Choosing which aspects to model and how to evaluate them are two decisions that frame software architecture research. Some software architecture researchers have proposed special-purpose notations that have a great deal of expressive power but are not well integrated with common development methods. Others have used general-purpose notations that are accessible to developers, but lack details needed for extensive analysis. In this paper we describe an approach to combining the advantages offered by these two different kinds of notation. In particular, we extend UML, an emerging standard notation for object-oriented design, with semantics specific to C2, an architectural style for userinterface intensive systems. Doing so suggests a practical strategy for bringing architectural modeling into the mainstream of software development and achieving partial integration of architectural models as needed.", "num_citations": "16\n", "authors": ["306"]}
{"title": "Perturbation analysis of stochastic systems with empirical distribution parameters\n", "abstract": " Probabilistic model checking is a quantitative verification technology for computer systems and has been the focus of intense research for over a decade. While in many circumstances of probabilistic model checking it is reasonable to anticipate a possible discrepancy between a stochastic model and a real-world system it represents, the state-of-the-art provides little account for the effects of this discrepancy on verification results. To address this problem, we present a perturbation approach in which quantities such as transition probabilities in the stochastic model are allowed to be perturbed from their measured values. We present a rigorous mathematical characterization for variations that can occur to verification results in the presence of model perturbations. The formal treatment is based on the analysis of a parametric variant of discrete-time Markov chains, called parametric Markov chains (PMCs), which are\u00a0\u2026", "num_citations": "15\n", "authors": ["306"]}
{"title": "Concurrent runtime checking of Annotated Ada programs\n", "abstract": " Anna is a language for writing machine-processable annotations of Ada programs. One of the main applications of Anna is the runtime checking of an Ada program for consistency with its formal specifications written in Anna. On single-processor systems, Anna runtime checks are used during testing and debugging of software.             This paper describes strategies for distributing Anna runtime checks so that they are executed in parallel with the Ada program. Concurrent checking of an annotated program can offer a substantial computational speedup over a sequentially checked version of the same program. Concurrent checking of Anna is therefore a crucial step in producing a self-checking program by allowing runtime checks for annotations to reside permanently in production versions of the program. Parallel checking will not always be useful in self-checking code, but certain kinds of annotations require\u00a0\u2026", "num_citations": "15\n", "authors": ["306"]}
{"title": "A case study in modeling a human-intensive, corporate software process\n", "abstract": " Describes a case study whose objective was to determine the feasibility, utility and limitations of using a process support tool to model and analyze a real process in active use by a large software development organization. The subject of our study was a process used in the maintenance of the AT&T 5ESS switching system software. The process was interesting to study primarily because it is a human-intensive process, which makes it less amenable to automation than build-like processes. Marvel 3.1 was used to create an executable model of the process. A key feature of our model is the separation of the informational aspects of the process from its operational aspects. The Marvel environment can be used for simulation, guidance, and tracking and querying the state of the process as it is performed. One of the key lessons learned from the study was that a complete process model must include an information\u00a0\u2026", "num_citations": "14\n", "authors": ["306"]}
{"title": "UML component diagrams and software architecture-experiences from the WREN project\n", "abstract": " In the course of building Wren, a component-based development environment, we encountered several difficulties when we tried to use UML component diagrams for modeling software architectures. One, the semantic interpretation of interfaces and dependencies is not clear in all cases. Two, component diagrams lack the expressive power to model unmet requirements.", "num_citations": "13\n", "authors": ["306"]}
{"title": "The complete transformation methodology for sequential runtime checking of an Anna subset\n", "abstract": " We present in this report a complete description of a methodology for transformation of Anna (Annot. ated Ada) programs to executable self-checking Ada programs. The methodology covers a subset of Anna which allows annotation of scalar types and objects. The allowed annotations include subtype annotations, subprogram annotations, result annotations, object annotations, out annotations and statement annotations. Except for package state expressions and quantified expressions, the full expression language of Anna is allowed in the subset. The transformation of annot, ations to executable checking functions is thoroughly illustrated through informal textual description, universal checking function templates and several transformation examples. We also describe the transformer and related software tools used to transform Anna programs. In conclusion, we describe validation of the transformer and some methods of making the transformation and runtime checking processes more efficient.", "num_citations": "13\n", "authors": ["306"]}
{"title": "On the role of style in selecting middleware and underwear\n", "abstract": " Middleware infrastructures are becoming a pervasive part of many distributed software systems. Wileden and Kaplan argue that middleware, like underwear, should not be the center of attention but should instead be kept hidden from public view, and it should never constrain or dictate what is publicly visible. These are admirable goals, yet the architects of distributed software systems must nevertheless recognize and account for the intimate relationship between middleware and the systems that use them. In particular, it is useful to view middleware infrastructures as inducing architectural styles, in the sense that they embody structural and behavioral constraints imposed on the systems that use them. Defining these styles and identifying the important relationships between them will allow architects to exploit these styles in a way that helps them defer, as long as possible, those architectural decisions that limit middleware choices, and to develop architectures that can accommodate the widest range of middleware. Otherwise, unattractive middleware choices may creep up on an architect in an annoying way.", "num_citations": "12\n", "authors": ["306"]}
{"title": "ProEva: runtime proactive performance evaluation based on continuous-time markov chains\n", "abstract": " Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which\u00a0\u2026", "num_citations": "11\n", "authors": ["306"]}
{"title": "Method for sending and receiving a Boolean function over a network\n", "abstract": " Conversion of subscription predicates for transmission in a publish-subscribe network. Subscriptions include Boolean-valued predicates defining content desired by a subscriber. An agent application converts the predicates into a suitable form for transmission to routers in a network core. The routers process the predicates into filter tables or data structures for use in content-based routing, which involves applying attributes in received packets to the filters. The agent also receives content corresponding with subscriptions and calls applications for presenting the content to a subscriber.", "num_citations": "11\n", "authors": ["306"]}
{"title": "Cascading verification: an integrated method for domain-specific model checking\n", "abstract": " Model checking is an established method for verifying behavioral properties of system models. But model checkers tend to support low-level modeling languages that require intricate models to represent even the simplest systems. Modeling complexity arises in part from the need to encode domain knowledge at relatively low levels of abstraction.", "num_citations": "10\n", "authors": ["306"]}
{"title": "Detecting problematic message sequences and frequencies in distributed systems\n", "abstract": " Testing the components of a distributed system is challenging as it requires consideration of not just the state of a component, but also the sequence of messages it may receive from the rest of the system or the environment. Such messages may vary in type and content, and more particularly, in the frequency at which they are generated. All of these factors, in the right combination, may lead to faulty behavior. In this paper we present an approach to address these challenges by systematically analyzing a component in a distributed system to identify specific message sequences and frequencies at which a failure can occur. At the core of the analysis is the generation of a test driver that defines the space of message sequences to be generated, the exploration of that space through the use of dynamic symbolic execution, and the timing and analysis of the generated tests to identify problematic frequencies. We\u00a0\u2026", "num_citations": "10\n", "authors": ["306"]}
{"title": "Runtime support for dynamic and adaptive service composition\n", "abstract": " The ability to dynamically compose autonomous services for optimally satisfying the requirements of different applications is one of the major advantages offered by the service-oriented computing (SOC) paradigm. A dynamic service composition implies that services requesters can be dynamically bound to most appropriate service providers that are available at runtime, in order to optimally satisfy the service requirements. At the same time, the autonomy of services involved in a composition means that the resulting composition may need to be adapted in response to changes in the service capabilities or requirements. Naturally, the infrastructure and technologies for providing runtime support for dynamic and adaptive composition of services form the backbone of the above process. In this chapter, we describe the Dino approach for providing the runtime support for dynamic and adaptive service\u00a0\u2026", "num_citations": "9\n", "authors": ["306"]}
{"title": "Design and verification of distributed tasking supervisors for concurrent programming languages\n", "abstract": " A tasking supervisor implements the concurrency constructs of a concurrent programming language. This thesis addresses two fundamental issues in constructing distributed implementations of a concurrent language:(1) Principles for designing a tasking supervisor for the language, and (2) Practical techniques for verifying that the supervisor correctly implements the semantics of the language. Previous research in concurrent languages has focused on the design of constructs for expressing concurrency, while ignoring these two important implementation issues.", "num_citations": "9\n", "authors": ["306"]}
{"title": "Verifying the long-run behavior of probabilistic system models in the presence of uncertainty\n", "abstract": " Verifying that a stochastic system is in a certain state when it has reached equilibrium has important applications. For instance, the probabilistic verification of the long-run behavior of a safety-critical system enables assessors to check whether it accepts a human abort-command at any time with a probability that is sufficiently high. The stochastic system is represented as probabilistic model, a long-run property is asserted and a probabilistic verifier checks the model against the property.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Probabilistic model checking of perturbed mdps with applications to cloud computing\n", "abstract": " Probabilistic model checking is a formal verification technique that has been applied successfully in a variety of domains, providing identification of system errors through quantitative verification of stochastic system models. One domain that can benefit from probabilistic model checking is cloud computing, which must provide highly reliable and secure computational and storage services to large numbers of mission-critical software systems.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Ambient flow: A visual approach for remixing the Internet of Things\n", "abstract": " The number of networked \u201csmart devices\u201d available in everyday environments is rapidly increasing; however, most adopt mutually incompatible networks, protocols, and application programming interfaces. In previous work, we introduced a variety of adaptive middleware techniques that enables a user's commodity mobile device (e.g., a smartphone) to serve as an adaptive gateway between mutually incompatible devices - providing service adaptation and protocol translation services through plug-ins that can be installed on-the-fly. In this paper, we present a complementary set of novel smart space design tools, which enable non-programmers to visually \u201cremix\u201d their ambient environments in new, playful and potentially unforeseen ways using an intuitive flow-graph model. Visual designs can be sent over the network to the user's mobile device, which instantiates them during runtime. This paper presents a\u00a0\u2026", "num_citations": "8\n", "authors": ["306"]}
{"title": "Nested reachability approximation for discrete-time Markov chains with univariate parameters\n", "abstract": " As models of real-world stochastic systems usually contain inaccurate information, probabilistic model checking for models with open or undetermined parameters has recently aroused research attention. In this paper, we study a kind of parametric variant of Discrete-time Markov Chains with uncertain transition probabilities, namely Parametric Markov Chains (PMCs), and probabilistic reachability properties with nested PCTL probabilistic operators. Such properties for a PMC with a univariate parameter define univariate real functions, called reachability functions, that map the parameter to reachability probabilities. An interesting application of these functions is sensitivity and robustness analysis of probabilistic model checking. However, a pitfall of computing the closed-form expression of a reachability function is the possible dynamism of its constraint set and target set. We pursue interval approximations for\u00a0\u2026", "num_citations": "8\n", "authors": ["306"]}
{"title": "A daily, activity-aware, mobile music recommender system\n", "abstract": " Existing music recommender systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we demonstrate in this demo a novel system to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. In our system, contextual information is integrated with music content analysis to offer recommendation for daily activities.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Multi-layer faults in the architectures of mobile, context-aware adaptive applications: a position paper\n", "abstract": " Five cellphones are sold every second, and there are four times more cellphones than computers, meaning there are some billions of mobile handheld devices in existence. Modern cellphones are equipped with multiple context sensors used by increasingly sophisticated software applications that exploit the sensors, allowing the applications to adapt automatically to changes in the surrounding environment, such as by responding to the location and speed of the user. The architecture of such applications is typically layered and incorporates a context-awareness middleware to support processing of context values. While this layered architecture is very natural for the design and implementation of applications, it gives rise to new kinds of faults and faulty behavior modes, which are difficult to detect using existing validation techniques. In this paper we provide scenarios illustrating such faults and exploring how they\u00a0\u2026", "num_citations": "8\n", "authors": ["306"]}
{"title": "Dino: Dynamic and adaptive composition of autonomous services\n", "abstract": " Service-oriented computing (SOC) offers a promising solution for dealing with coordination complexity in distributed software systems. Naturally, the infrastructure and technologies for composing services form the backbone of SOC. We argue that SOC has immense potential in enabling collaborations between distributed autonomous services in open dynamic environments, in addition to the restricted business environments that have been the main focus of the work done in SOC so far. We discuss some of the important issues and challenges involved in composing services in open dynamic environments, and give an overview of the Dino approach that we have been developing with an aim to meet these challenges effectively. Dino provides a runtime infrastructure for comprehensively supporting all stages of service composition, namely: service discovery, selection, binding, delivery, monitoring and adaptation. We conclude with a discussion on some of the ongoing and future work on Dino.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Reliability analysis of concurrent systems using LTSA\n", "abstract": " In this paper, we present an extension of LTSA that facilitates our approach to reliability prediction. LTSA allows the use of behavioural models for distributed systems as prototypes to explore system behaviour, and automated checking of model compliance to properties (i.e., model checking). To support reliability prediction, our new extensions allow annotating a scenario specification with probabilities and using LTSA to process the resulting scenarios. In this context, scenarios are partial descriptions of how components interact to provide system functionality. A scenario specification is formed by composing multiple scenarios possibly from different stakeholders.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Reducing congestion effects by multipath routing in wireless networks\n", "abstract": " We propose a solution to improve fairness and increasethroughput in wireless networks with location information.Our approach consists of a multipath routing protocol, BiasedGeographical Routing (BGR), and two congestion controlalgorithms, In-Network Packet Scatter (IPS) and End-to-EndPacket Scatter (EPS), which leverage BGR to avoid the congestedareas of the network. BGR achieves good performancewhile incurring a communication overhead of just 1 byte perdata packet, and has a computational complexity similar togreedy geographic routing. IPS alleviates transient congestion bysplitting traffic immediately before the congested areas. In contrast,EPS alleviates long term congestion by splitting the flow atthe source, and performing rate control. EPS selects the pathsdynamically, and uses a less aggressive congestion controlmechanism on non-greedy paths to improve energy efficiency.Simulation and experimental results show that our solutionachieves its objectives. Extensive ns-2 simulations show that oursolution improves both fairness and throughput as compared tosingle path greedy routing. Our solution reduces the variance ofthroughput across all flows by 35%, reduction which is mainlyachieved by increasing throughput of long-range flows witharound 70%. Furthermore, overall network throughput increasesby approximately 10%. Experimental results on a 50-node testbed are consistent with our simulation results, suggestingthat BGR is effective in practice.", "num_citations": "8\n", "authors": ["306"]}
{"title": "Critical considerations and designs for internet-scale, event-based compositional architectures\n", "abstract": " A common architectural style for distributed, loosely-coupled, heterogeneous software systems is a structure based on event generation, observation and notification. A notable characteristic of an architecture based on events is that interaction among architectural components occurs asynchronously, thereby simplifying the composition of autonomous, independently-executing components that may be written in different programming languages and executing on varied hardware platforms.There is increasing interest in deploying these kinds of distributed systems across wide-area networks such as the Internet. For instance, workflow systems for multi-national corporations, multi-site/multi-organization software development, and real-time investment analysis across world financial markets are just a few of the many applications that lend themselves to deployment on an Internet scale. However, deployment of such systems at the scale of the Internet imposes new challenges that are not met by existing technology. In particular, the technology to support an event-based architectural style is well-developed for local-area networks (eg, Field\u2019s Msg [7], SoftBench\u2019s BMS [1], ToolTalk [3] and Yeast [4]), but not for wide-area networks. One of these systems, Yeast, was built and studied by the first author while he was on the research staff at AT&T Bell Laboratories. Yeast is a general-purpose platform for building distributed applications in an eventbased architectural style, and it supports event-based interaction quite naturally within local-area networks. However, its centralized-server architecture limits its scalability to wide-area networks, as does its limited\u00a0\u2026", "num_citations": "8\n", "authors": ["306"]}
{"title": "Testing the correctness of tasking supervisors with TSL specifications\n", "abstract": " This paper describes the application of behavior specifications to the testing of tasking supervisors, an important component of an implementation of a concurrent programming language. The goal of such testing is to determine whether or not a tasking supervisor correctly implements the semantics of its associated language. We have tested a distributed tasking supervisor for the Ada programming language by monitoring the execution behavior of Ada tasking programs that have been compiled and linked with the supervisor. This behavior is checked for consistency with an event-based formalization of the Ada tasking semantics expressed in the TSL specification language. The TSL Runtime System automatically performs all monitoring and consistency checking at runtime. Our approach improves upon other approaches to testing tasking supervisors, particularly the Ada Compiler Validation Capability (ACVC), and\u00a0\u2026", "num_citations": "8\n", "authors": ["306"]}
{"title": "Learning Multi-Objective Rewards and User Utility Function in Contextual Bandits for Personalized Ranking.\n", "abstract": " This paper tackles the problem of providing users with ranked lists of relevant search results, by incorporating contextual features of the users and search results, and learning how a user values multiple objectives. For example, to recommend a ranked list of hotels, an algorithm must learn which hotels are the right price for users, as well as how users vary in their weighting of price against the location. In our paper, we formulate the contextaware, multi-objective, ranking problem as a Multi-Objective Contextual Ranked Bandit (MOCR-B). To solve the MOCR-B problem, we present a novel algorithm, named Multi-Objective Utility-Upper Confidence Bound (MOU-UCB). The goal of MOU-UCB is to learn how to generate a ranked list of resources that maximizes the rewards in multiple objectives to give relevant search results. Our algorithm learns to predict rewards in multiple objectives based on contextual information (combining the Upper Confidence Bound algorithm for multi-armed contextual bandits with neural network embeddings), as well as learns how a user weights the multiple objectives. Our empirical results reveal that the ranked lists generated by MOUUCB lead to better click-through rates, compared to approaches that do not learn the utility function over multiple reward objectives.", "num_citations": "7\n", "authors": ["306"]}
{"title": "The pros and cons of the'PACM'proposal: counterpoint\n", "abstract": " SIG Notices. ISI indexes SIG Notices, so some authors prefer them. This duplication is a historical accident due to SIGs that made conference proceedings issues of their SIG Notices as a member benefit, producing duplicate, ambiguous, and non-branded bibliographic entries. Worse, some conference names were not standardized from year to year. I strongly recommend that ACM work with the SIGS to develop citation formats that ensure the bibliographic record avoids confusion, the new citation clearly features conference brand names, and consistently apply this citation format retrospectively and thenceforth.", "num_citations": "7\n", "authors": ["306"]}
{"title": "Roundtable: the future of software engineering for internet computing\n", "abstract": " Seven research leaders in software engineering for Internet computing present their viewpoints on important issues that will shape this field's future. They discuss opportunities and challenges for the shifting software paradigm; stepping outside the comfort zone to revisit issues such as software correctness; improving Internet software dependability and programmability; addressing software engineering issues for the Internet of Things; exploring relationships among the Internet of Things, people, and software services; supporting a participatory culture of software development; and rethinking logging in online services.", "num_citations": "7\n", "authors": ["306"]}
{"title": "VOLARE: Adaptive web service discovery middleware for mobile systems\n", "abstract": " With the recent advent and widespread use of smart mobile devices, the flexibility and versatility offered by Service Oriented Architecture\u2019s (SOA) makes it an ideal approach to use in the rapidly changing mobile environment. However, the mobile setting presents a set of new challenges that service discovery methods developed for nonmobile environments cannot address. The requirements a mobile client device will have from a Web service may change due to changes in the context or the resources of the client device. In a similar manner, a mobile device that acts as a Web service provider will have different capabilities depending on its status, which may also change dramatically during runtime. This paper introduces VOLARE, a middleware-based solution that will monitor the resources and context of the device, and adapt service requests accordingly. The same method will be used to adapt the Quality of Service (QoS) levels advertised by service providers, to realistically reflect each provider\u2019s capabilities at any given moment. This approach will allow for more resource-efficient and accurate service discovery in mobile systems and will enable more reliable provider functionality in mobile devices.", "num_citations": "7\n", "authors": ["306"]}
{"title": "The certification of software tools with respect to software standards\n", "abstract": " Software development standards such as the UML provide complex modeling languages for specifying, visualizing, constructing, and documenting the artifacts of software systems. Software tools support the production of these artifacts according to the model elements, relationships, well-formedness rules and semantics defined in the standards. Due to the complexities of both standards and software tools, it is difficult to establish the compliance of the software tools to the standards. It has been suggested that many existing tools that advertise standard compliance fail to lift up to their claims. The objective of this work is to propose a framework for developing systematic, disciplined, and quantifiable certification schemes to assess the compliance of these tools to standards and to diagnose the causes of non-compliance.", "num_citations": "7\n", "authors": ["306"]}
{"title": "Exploit hijacking: Side effects of smart defenses\n", "abstract": " Recent advances in the defense of networked computers use instrumented binaries to track tainted data and can detect attempted break-ins automatically. These techniques identify how the transfer of execution to the attacker takes place, allowing the automatic generation of defenses. However, as with many technologies, these same techniques can also be used by the attackers: the information provided by detectors is accurate enough to allow an attacker to create a new worm using the same vulnerability, hijacking the exploit. Hijacking changes the threat landscape by pushing attacks to extremes (targeting selectively or creating a rapidly spreading worm), and increasing the requirements for automatic worm containment mechanisms. In this paper, we show that hijacking is feasible for two categories of attackers: those running detectors and those using Self-Certifying Alerts, a novel mechanism proposed by\u00a0\u2026", "num_citations": "7\n", "authors": ["306"]}
{"title": "A Comparative Study of Coarse-and Fine-Grained Safe Regression Test Selection\n", "abstract": " Regression test selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite for reveri cation of a modi ed program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of a few of these techniques have shown that they can be bene cial, but so far, few studies have compared techniques by performing the analysis for di erent methods on the same set of subjects. This paper describes a comparative evaluation of di erent regression test-selection techniques on the same experimental subjects. In particular, the two methods DejaVu and TestTube, developed by the authors, are evaluated and compared in terms of a cost model that considers their relative precision and e ciency. The data reveal that in some instances TestTube's relative lack of precision does not prevent it from competing with the more precise DejaVu, yet in other instances, DejaVu's superior precision gives it a clear advantage over TestTube. Since TestTube requires less critical-phase time cost, such variations in performance can complicate the tester's choice of which technique to use. As a result, a hybrid technique is suggested, which combines the gains in e ciency from TestTube with the increased precision of DejaVu.", "num_citations": "7\n", "authors": ["306"]}
{"title": "Digraph inception convolutional networks\n", "abstract": " Graph Convolutional Networks (GCNs) have shown promising results in modeling graph-structured data. However, they have difficulty with processing digraphs because of two reasons: 1) transforming directed to undirected graph to guarantee the symmetry of graph Laplacian is not reasonable since it not only misleads message passing scheme to aggregate incorrect weights but also deprives the unique characteristics of digraph structure; 2) due to the fixed receptive field in each layer, GCNs fail to obtain multi-scale features that can boost their performance. In this paper, we theoretically extend spectral-based graph convolution to digraphs and derive a simplified form using personalized PageRank. Specifically, we present the Digraph Inception Convolutional Networks (DiGCN) which utilizes digraph convolution and kth-order proximity to achieve larger receptive fields and learn multi-scale features in digraphs. We empirically show that DiGCN can encode more structural information from digraphs than GCNs and help achieve better performance when generalized to other models. Moreover, experiments on various benchmarks demonstrate its superiority against the state-of-the-art methods.", "num_citations": "6\n", "authors": ["306"]}
{"title": "Correction to'A Practical Approach to Programming with Assertions'\n", "abstract": " IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 21, NO. 3, MARCH 1995 265 the points we raise, and it may well be that the information we seek has become a casualty of the pressure on space in academic journals. Nevertheless, it is vitally important to precisely specify the mapping from a language-independent set of metrics to specific programming languages and sets of observations. Such precision is particularly important since the source code used in published work is generally not publicly available. The usefulness of the proposed metrics (and others) will be limited until their application to specific languages is clearly specified. Failure to resolve such issues in the near future may impede the development and validation of effective 00 software metrics. We believe that this is as important as the establishment of a sound theoretical basis for the metrics.", "num_citations": "6\n", "authors": ["306"]}
{"title": "Revisiting convolutional neural networks for citywide crowd flow analytics\n", "abstract": " Citywide crowd flow analytics is of great importance to smart city efforts. It aims to model the crowd flow (e.g., inflow and outflow) of each region in a city based on historical observations. Nowadays, Convolutional Neural Networks (CNNs) have been widely adopted in raster-based crowd flow analytics by virtue of their capability in capturing spatial dependencies. After revisiting CNN-based methods for different analytics tasks, we expose two common critical drawbacks in the existing uses: 1) inefficiency in learning global spatial dependencies, and 2) overlooking latent region functions. To tackle these challenges, in this paper we present a novel framework entitled DeepLGR that can be easily generalized to address various citywide crowd flow analytics problems. This framework consists of three parts: 1) a local feature extraction module to learn representations for each region; 2) a global context module to extract global contextual priors and upsample them to generate the global features; and 3) a region-specific predictor based on tensor decomposition to provide customized predictions for each region, which is very parameter-efficient compared to previous methods. Extensive experiments on two typical crowd flow analytics tasks demonstrate the effectiveness, stability, and generality of our framework.", "num_citations": "5\n", "authors": ["306"]}
{"title": "Extending Component Interoperability Standards to Support Architecture-Based Development\n", "abstract": " Components have increasingly become the unit of development of software. In industry, there has been considerable work in the development of standard component interoperability models, such as ActiveX, CORBA and JavaBeans. In academia, there has been intensive research in developing a notion of software architecture. Both of these efforts use software components as the basic building blocks, and both address concerns of structure and reuse. With component interoperability models, the focus is on specifying interfaces, binding mechanisms, packaging, intercomponent communication protocols, and expectations regarding the runtime environment. With software architecture, the focus is on specifying systems of communicating components, analyzing system properties, and generating\" glue\" code that binds system components. Our research involves studying how standard component models can be extended to accommodate important issues of architecture, including a notion of architectural style and support for explicit connectors. For our initial effort in this work, we have enhanced the JavaBeans component model to support component composition according to the C2 architectural style. Our approach enables the design and development of applications in the C2 style using off-theshelf Java components or \u201cbeans\" that are available to the designer. In this paper, we describe the techniques underlying our approach, and we identify the important issues that surface when attempting this type of extension.", "num_citations": "5\n", "authors": ["306"]}
{"title": "Fine-Grained Urban Flow Prediction\n", "abstract": " Urban flow prediction benefits smart cities in many aspects, such as traffic management and risk assessment. However, a critical prerequisite for these benefits is having fine-grained knowledge of the city. Thus, unlike previous works that are limited to coarse-grained data, we extend the horizon of urban flow prediction to fine granularity which raises specific challenges: 1) the predominance of inter-grid transitions observed in fine-grained data makes it more complicated to capture the spatial dependencies among grid cells at a global scale; 2) it is very challenging to learn the impact of external factors (eg, weather) on a large number of grid cells separately. To address these two challenges, we present a Spatio-Temporal Relation Network (STRN) to predict fine-grained urban flows. First, a backbone network is used to learn high-level representations for each cell. Second, we present a Global Relation Module\u00a0\u2026", "num_citations": "4\n", "authors": ["306"]}
{"title": "Intertool connections\n", "abstract": " Intertool connections | Practical reusable UNIX software ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksPractical reusable UNIX softwareIntertool connections chapter Intertool connections Share on Authors: Yih-Farn Chen Search about this author , Glenn Stephen Fowler profile image Glenn Fowler View Profile , David G Korn profile image David Korn View Profile , Eleftherios E Koutsofios profile image Eleftherios Koutsofios View Profile , Stephen S North profile image Stephen North View Profile , David S. Rosenblum profile image David Rosenblum View Profile , Kiemphong Vo profile image Kiem-Phong Vo View Profile Info & : \u2026", "num_citations": "4\n", "authors": ["306"]}
{"title": "A secure protocol for content-based publish/subscribe systems\n", "abstract": " Publish/subscribe systems with content-based routing have emerged in the past years as a scalable solution for wide area event notification. A drawback of this rich interaction model is the apparently inherent lack of security: a router must be trusted with subscription and notification information, in order to implement contentbased routing. We clearly define content confidentiality in content-based routing and evaluate several existing cryptographic protocols to solve this problem, pointing out the tradeoff between security and complexity they achieve. We present the first fully featured, secure protocol for content-based pub/sub presented in the literature and prove that it is computationally infeasible to break. Our protocol is an extension of the Private Simultaneous Messages model (PSM). We describe provably sound ways to reduce forms of attribute matching such as number relational tests or regular expression to the\u00a0\u2026", "num_citations": "4\n", "authors": ["306"]}
{"title": "Efficient decentralized LTL monitoring framework using tableau technique\n", "abstract": " This paper presents a novel framework for decentralized monitoring of Linear Temporal Logic (LTL) formulas, under the situation where processes are synchronous and the formula is represented as a tableau. The tableau technique allows one to construct a semantic tree for the input LTL formula, which can be used to optimize the decentralized monitoring of LTL in various ways. Given a system P and an LTL formula \u03c6, we construct a tableau T\u03c6. The tableau T\u03c6 is used for two purposes: (a) to synthesize an efficient round-robin communication policy for processes, and (b) to find the minimal ways to decompose the formula and communicate observations of processes in an efficient way. In our framework, processes can propagate truth values of both atomic and compound formulas (non-atomic formulas) depending on the syntactic structure of the input LTL formula and the observation power of processes. We\u00a0\u2026", "num_citations": "3\n", "authors": ["306"]}
{"title": "Evaluating recommender system stability with influence-guided fuzzing\n", "abstract": " Recommender systems help users to find products or services they may like when lacking personal experience or facing an overwhelming set of choices. Since unstable recommendations can lead to distrust, loss of profits, and a poor user experience, it is important to test recommender system stability. In this work, we present an approach based on inferred models of influence that underlie recommender systems to guide the generation of dataset modifications to assess a recommender\u2019s stability. We implement our approach and evaluate it on several recommender algorithms using the MovieLens dataset. We find that influence-guided fuzzing can effectively find small sets of modifications that cause significantly more instability than random approaches.", "num_citations": "3\n", "authors": ["306"]}
{"title": "Differencing graphical user interfaces\n", "abstract": " Graphical User Interface ( GUI )- based APplications  ( GAPs ) are ubiquitous and provide a wealth of sophisticated services. Nontrivial GAPs evolve through many versions, and understanding how GUIs of different versions of GAPs differ is crucial for various software quality tasks such as testing, cross-platform UI comparison and project effort estimation. Yet despite the criticality of automating GUI differencing, it is a manual, tedious, and laborious task. We offer a novel approach for differencing GUIs that combines tree edit distance measure algorithms with accessibility technologies for obtaining GUI models in a non-intrusive, platform and language-independent way, and it does not require the source code of GAPs. We developed a tool called  GUI DifferEntiator  ( GUIDE ) that allows users to difference GUIs of running GAPs. To evaluate GUIDE, we created an experimental platform that generates random GUIs with\u00a0\u2026", "num_citations": "3\n", "authors": ["306"]}
{"title": "Specification and analysis of dynamically-reconfigurable service architectures\n", "abstract": " A Service-Oriented Computing (SoC) architecture consists of a number of collaborating services to achieve one or more goals. Traditionally, the focus of developing services (as software components) has been on the static binding of these services within a single context and constrained in an individual manner. However, service architectures should be dynamic, where service binding and context changes with environmental changes. The task of designing and analysing such architectures becomes very complex. In this chapter we discuss a specification profile and analysis framework for service modes. A service mode provides an encapsulation of both specification and adaptation in different service scenarios. The approach is implemented as a tool suite and integrated into the Eclipse IDE.", "num_citations": "3\n", "authors": ["306"]}
{"title": "Software system scalability: concepts and techniques\n", "abstract": " With physical limits approaching in hardware, with demands on large-scale software systems continually increasing, and with the use of resource-poor wireless devices rapidly growing, issues of scale are becoming paramount in software engineering.", "num_citations": "3\n", "authors": ["306"]}
{"title": "Algorithms for efficient symbolic detection of faults in context-aware applications\n", "abstract": " Context-aware and adaptive applications running on mobile devices pose new challenges for the verification community. Current verification techniques are tailored for different domains (mostly hardware) and the kind of faults that are typical of applications running on mobile devices are difficult (or impossible) to encode using the patterns of ldquotraditionalrdquo verification domains. In this paper we present how techniques similar to the ones used in symbolic model checking can be applied to the verification of context-aware and adaptive applications. More in detail, we show how a model of a context-aware application can be encoded by means of ordered binary decision diagrams and we introduce symbolic algorithms for the verification of a number of properties.", "num_citations": "3\n", "authors": ["306"]}
{"title": "Distributed online filtering\n", "abstract": " Web search is a reactive process: search engines index the information published on many millions of servers, and users send queries to find sites that contain particular information. While this approach is adequate for static content, it fails to capture the temporal nature of online information. Sports scores, networking research preprints, stormy blog discussions, and indeed news are all examples of information whose value is strictly related to its timely delivery to users. Additionally, it is a bit awkward that the very numerous web servers publish information and basically hope that the (tens of) thousands of machines owned by search companies will make their content available to the world. Perhaps the web servers, having spare capacity most of the time (except for the rare case of flash crowds), can eliminate the middle-man and deliver information to interested users directly?This work focuses on online filtering of documents by the web servers themselves, organized in an overlay. Users express their long term interests as queries and register them with the web servers. Content published by the web servers is matched against users\u2019 interests and a decision is made to decide whether a given document should be delivered to the user. This approach is proactive, as decisions are taken online based on content. It does not come to replace search, but to complement it by enabling timely information delivery to the users who have expressed interest in that information. Requirements and Metrics. Our main metric is throughput, measured as the number of documents processed per second by the whole network. We require the system should be scalable\u00a0\u2026", "num_citations": "3\n", "authors": ["306"]}
{"title": "Revisiting convolutional neural networks for urban flow analytics\n", "abstract": " Citywide crowd flow prediction is of great importance to smart city efforts. It aims to model the crowd flow (e.g., inflow and outflow) of each region in a city based on historical observations. Nowadays, Convolutional Neural Networks (CNNs) have been widely adopted in raster-based crowd flow analytics by virtue of their capability in capturing spatial dependencies. After revisiting CNN-based methods for different analytics tasks, we expose two common critical drawbacks in the existing uses: 1) inefficiency in learning global spatial dependencies, and 2) overlooking latent region functions. To tackle these challenges, in this paper we present a novel framework entitled DeepLGR that can be easily generalized to address various citywide crowd flow analytics problems. This framework consists of three parts:  1) a local feature extraction module to learn representations for each region;  2) a global context module to extract global contextual priors and up sample them to generate the global features; and  3) a region-specific predictor based on tensor decomposition to provide customized predictions for each region, which is very parameter-efficient compared to previous methods. Extensive experiments on two typical crowd flow analytics tasks demonstrate the effectiveness, stability, and generality of our framework.", "num_citations": "2\n", "authors": ["306"]}
{"title": "Predicting Urban Water Quality with Ubiquitous Data-A Data-driven Approach\n", "abstract": " Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. However, predicting the urban water quality is a challenging task since the water quality varies in urban spaces non-linearly and depends on multiple factors, such as meteorology, water usage patterns, and land uses. In this work, we forecast the water quality of a station over the next few hours from a data-driven perspective, using the water quality data and water hydraulic data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, pipe networks, structure of road networks, and point of interests (POIs). First, we identify the influential factors that affect the urban water quality via extensive experiments. Second, we present a multi-task multi-view learning method to fuse those multiple datasets from different domains\u00a0\u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "Using branch frequency spectra to evaluate operational coverage\n", "abstract": " Coverage metrics try to quantify how well a software artifact is tested. High coverage numbers instill confidence in the software and might even be necessary to obtain certification. Unfortunately, achieving high coverage numbers does not imply high quality of the test suite. One shortcoming is that coverage metrics do not measure how well test suites cover systems in production. We look at coverage from an operational perspective. We evaluate test suite quality by comparing runs executed during testing with runs executed in production. Branch frequency spectra are employed to capture the behavior during runtime. Differences in the branch frequency spectra between field executions and testing runs indicate test suite deficiencies. This post-release test suite quality assurance mechanism can be used to (1) build confidence by pooling coverage information from many execution sites and (2) guide test suite\u00a0\u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "An Interval-Based Bayesian Generative Model for Human Complex Activity Recognition\n", "abstract": " Complex activity recognition is challenging due to the inherent uncertainty and diversity of performing a complex activity. Normally, each instance of a complex activity has its own configuration of atomic actions and their temporal dependencies. We propose in this paper an atomic action-based Bayesian model that constructs Allen's interval relation networks to characterize complex activities with structural varieties in a probabilistic generative way: By introducing latent variables from the Chinese restaurant process, our approach is able to capture all possible styles of a particular complex activity as a unique set of distributions over atomic actions and relations. We also show that local temporal dependencies can be retained and are globally consistent in the resulting interval network. Moreover, network structure can be learned from empirical data. A new dataset of complex hand activities has been constructed and made publicly available, which is much larger in size than any existing datasets. Empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate the competitiveness of our approach.", "num_citations": "2\n", "authors": ["306"]}
{"title": "The power of probabilistic thinking\n", "abstract": " Traditionally, software engineering has dealt in absolutes. For instance, we talk about a system being\" correct\" or\" incorrect\", with the shades of grey in between occasionally acknowledged but rarely dealt with explicitly. And we typically employ logical, algebraic, relational and other representations and techniques that help us reason about software in such absolute terms. There of course have been notable exceptions to this, such as the use of statistical techniques in testing and debugging. But by and large, both researchers and practitioners have favored the relative comfort of an absolutist viewpoint in all aspects of development. In this talk, I will argue the benefits of taking a more thoroughly probabilistic approach in software engineering. Software engineering is rife with stochastic phenomena, and the vast majority of software systems operate in an environment of uncertain, random behavior, which suits an\u00a0\u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "Fundamental Approaches to Software Engineering\n", "abstract": " This volume contains the papers accepted for presentation at FASE 2010, the 13th International Conference on Fundamental Approaches to Software Engineering, which was held in Paphos, Cyprus, in March 2010 as part of the annual European Joint Conference on Theory and Practice of Software (ETAPS). As with previous editions of FASE, this year\u2019s papers present foundational contributions and results on a broad range of topics in software engineering, including requirements engineering, software architectures, model-based and model-driven development, program analysis, testing, debugging, verification, and evolution. This year we received 103 submissions, of which 25 were accepted by the Program Committee for presentation at the conference, resulting in an acceptance rate of 24.3%. The submissions comprise 96 research papers and 7 tool demonstration papers, and the Program Committee\u00a0\u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "Using Software Patents To Support The Business Model Of Software Components\n", "abstract": " Even though off-the-shelf software components are considered to be the technology of the future, to date their success in the market is rather low. While many attempts have been undertaken to overcome technological obstacles such as component interoperability, the business model has been given comparably fewer thoughts. This paper discusses how inter-company reuse of software components could be facilitated by software patents. In particular, it explains how patents can help strengthen the weak business model of software components by slowing down today's proliferation of standards and reducing other obstacles on the way to make software component vending economically more viable.", "num_citations": "2\n", "authors": ["306"]}
{"title": "Automated Monitoring of Component Integrity in Distributed Object Systems\n", "abstract": " The marriage of object-oriented component programming languages such as C++ and Java with distributed object infrastructures such as CORBA has made the dream of component-based software development and assembly a reality. Yet while distributed object computing may ease the design and construction of software systems, it also introduces significant challenges in ensuring the integrity of component collaborations and the overall reliability of system execution.\u201cIntegrity\u201d here refers not only to the functional correctness of a distributed object system and its constituent components, but to other attributes as well, such as quality of service and system security. Given the immaturity of distributed object computing, the full scope of this problem has yet to be encountered in development practice or studied by researchers.", "num_citations": "2\n", "authors": ["306"]}
{"title": "Surveyors' forum: runtime checking and debugging of formally specified programs\n", "abstract": " For the past seven years, the Program Analysis and Verification Group at Stanford University, headed by David C. Luckham, one of the primary designers of the Anna specification language, has been working on developing practical algorithms for implementing annotation checks. The group\u2019s work on Anna has culminated in three doctoral theses [Neff 1989; Rosenblum 1988; Sankar 1989a] and several publications(listed below). Several tools, including a runtime annotation checker and debugger, have been distributed to mnorethan 50 sites worldwide, We have had much positive feedback from these sites. Based on this experience, we feel the need to address the remark quoted above (and other similar ones) made by Abbott. Abbott\u2019s remark is certainly true for arbitrary annotations. In fact, Sankar [1989al demonstrates that checking the runtime behavior of a program with respect to arbitrary algebraic\u00a0\u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "Siena: A Wide-Area Event Notification Service\n", "abstract": " SIENA: Wide-Area Event Notification Service Page 1 SIENA: Wide-Area Event Notification Service Antonio Carzaniga David S. Rosenblum Alexander L. Wolf Page 2 Event Services Are Not New! \u2666Event models \u2013 simple (UNIX signals) \u2013 sophisticated (database triggers) \u2666Event notification services \u2013 LAN message servers (eg, SUN ToolTalk, DEC Fuse, HP SoftBench) \u2013 USENET News (NNTP) \u2666Basic communication infrastructure \u2013 point-to-point (TCP/IP, RPC, CORBA, RMI) \u2013 multicast (MBONE) Page 3 Challenges \u2666Scalability \u2013 many-to-many communications \u2013 large number of objects and events \u2013 decentralized control \u2013 heterogeneous platforms and protocols \u2666Expressiveness \u2013 rich event notification model \u2013 expressive subscription language (an IP multicast address just doesn\u2019t cut it) \u2666Tradeoffs between scalability and expressiveness Page 4 SIENA Event Service financial institution NASDAQ advertise publish = * \u2026", "num_citations": "2\n", "authors": ["306"]}
{"title": "A Novel Decentralized LTL Monitoring Framework Using Formula Progression Table\n", "abstract": " This paper presents a new technique for optimizing formal analysis of Boolean formulas and Linear Temporal Logic (LTL) formulas, namely the formula simplification tables. A formula simplification table is a mathematical table that shows all possible simplifications of the formula under different truth assignments of its variables. The simplification table is constructed using a three-valued logic: besides true and false, the variable can take an unknown value. The advantages of constructing a simplification table of a formula are two-fold. First, it can be used to compute the logical influence weight of each variable in the formula, which is a quantitative score that shows the importance of the variable in affecting the outcome of the formula. Second, it can be used to identify variables that have the highest logical influences on the outcome of the formula. We demonstrate the effectiveness of formula simplification table\u00a0\u2026", "num_citations": "1\n", "authors": ["306"]}
{"title": "Editorial Journal-First Publication for the Software Engineering Community\n", "abstract": " Editorial Journal-First Publication for the Software Engineering Community Page 1 1 Editorial Journal-First Publication for the Software Engineering Community IEEE Transactions on Software Engineering (TSE) and ACM Transactions on Software Engineering and Methodology (TOSEM) are jointly initiating a uniform approach to \u201cjournal-first\u201d publication for the software-engineering research community. The goal is to provide a mechanism whereby research results that may be a better fit for publication via the journal review process can be broadly disseminated to the community through presentation in the technical research tracks of ACM- and IEEE-sponsored conferences. As in other communities that provide a journal-first publication route (such as the VLDB conference in the database community), authors choosing this route benefit from the absence of page limits and annual submission deadlines and the of their \u201c\u2026", "num_citations": "1\n", "authors": ["306"]}
{"title": "A visual design toolset for drag-and-drop smart space configuration\n", "abstract": " The number of networked smart devices available in everyday Internet of Things (IoT) environments is rapidly increasing; however, many current devices adopt mutually incompatible networks, protocols, and application programming interfaces (APIs). For example, devices like the Sphero Robotic ball and Parrot AR Drone helicopter both provide dedicated controller apps, but remain inherently incompatible. The Sphero supports sensor data streaming of its inertial measurement unit (IMU) over a Bluetooth connection, which can be used to determine the current orientation of the robot using a proprietary API. The AR Drone supports data connectivity over WiFi and accepts flight control commands that can be used to remotely pilot the drone. A grasped Sphero device could theoretically be used as an intuitive flight controller for the drone (by feeding its streaming IMU data into the drone\u2019s flight control API); however, such interactions are not inherently supported by the devices.", "num_citations": "1\n", "authors": ["306"]}
{"title": "In memoriam: David Notkin (1955--2013)\n", "abstract": " It is with great sadness that I write to mourn the death of David Notkin, who passed away on April 22, 2013. His passing is a great loss to the software engineering community and to the discipline of computer science as a whole. There already have been many tributes written about David and his numerous achievements and qualities, so the best I can do here is to offer my own personal tribute and those of the TOSEM Editorial Board.David was my immediate predecessor as Editor-in-Chief of TOSEM and, in that role, he greatly improved TOSEM\u2019s visibility, number of submissions, and turnaround time, and thus its timeliness, all without sacrificing the high standards of quality that have been a hallmark of TOSEM since its founding. In this regard he is a clear role model for me and for the Editorial Board as we carry TOSEM forward in the coming years. But beyond TOSEM, David has long been a mentor for me both as\u00a0\u2026", "num_citations": "1\n", "authors": ["306"]}
{"title": "Fundamental Approaches to Software Engineering: 13th International Conference, FASE 2010, Held as Part of the Joint European Conferences on Theory and Practice of Software\u00a0\u2026\n", "abstract": " This book constitutes the refereed proceedings of the 13th International Conference on Fundamental Approaches to Software Engineering, FASE 2010, held in Paphos, Cyprus, in March 2010, as part of ETAPS 2010, the European Joint Conferences on Theory and Practice of Software. The 25 papers presented were carefully reviewed and selected from 103 submissions. The volume also contains one invited talk. The topics covered are model transformation, software evolution, graph transformation, modeling concepts, verification, program analysis, testing and debugging, and performance modeling and analysis.", "num_citations": "1\n", "authors": ["306"]}
{"title": "Engage: Engineering Service Modes with WS-Engineer and Dino\n", "abstract": " In this demonstration we present an approach to engineering service brokering requirements and capabilities using the concepts of Service Modes. The demonstration illustrates building service modes in UML2 with Rational Software Modeller, transforming modes in WS-Engineer and generating artefacts for runtime service brokering.", "num_citations": "1\n", "authors": ["306"]}
{"title": "Impact Analysis of Relational Schema Changes on Native Language Queries\n", "abstract": " We present a technique for analyzing the impact that relational schema changes have on applications that use objectrelational mappings and native language queries. We present a meta model that identifies the data that needs to be obtained using static analysis from database, objectrelational mapping and objectoriented application programs. We discuss a number of static analysis algorithms that we use to extract potentially relevant data from these sources. We show how to specify schema impact using the Object Constraint Language. We discuss an implementation of these techniques in the SUITE environment. SUITE relies on the Soot framework for static analysis and CrocoPat for efficient execution of relational programs. We evaluate both quality and performance of the SUITE environment using versions of a database schema and an application from the bioinformatics domain.", "num_citations": "1\n", "authors": ["306"]}
{"title": "CAREER: mechanisms for ensuring the integrity of distributed object systems\n", "abstract": " Distributed object computing is an increasingly important computational paradigm in which independent software components interact over a distributed computing network. A fundamental problem in distributed object systems is ensuring the integrity of interactions between components and the reliability of system execution. The objective of this project is to investigate integrity-checking mechanisms based on welldefined architectural models, which provide a basis for specifying and validating integrity constraints. The architectural basis for this research is the C2 Architectural Style and related technologies (created at UC Irvine by Prof. Richard Taylor's group), while the JavaBeans component model provides the implementation-level context. The project has produced significant results in a number of areas.First, we have developed a formal theory of heterogeneous typing that allows components in architectural\u00a0\u2026", "num_citations": "1\n", "authors": ["306"]}
{"title": "Generalized event-action handling\n", "abstract": " Generalized event-action handling | Practical reusable UNIX software ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksPractical reusable UNIX softwareGeneralized event-action handling chapter Generalized event-action handling Share on Authors: David S. Rosenblum profile image David Rosenblum View Profile , Balachander Krishnamurthy profile image Balachander Krishnamurthy View Profile Authors Info & Affiliations Publication: Practical reusable UNIX softwareMarch 1995 Pages 247\u2013274 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation ! to.\u2026", "num_citations": "1\n", "authors": ["306"]}