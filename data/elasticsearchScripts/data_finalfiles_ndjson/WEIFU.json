{"title": "What is wrong with topic modeling? And how to fix it using search-based software engineering\n", "abstract": " ContextTopic modeling finds human-readable structures in unstructured textual data. A widely used topic modeling technique is Latent Dirichlet allocation. When running on different datasets, LDA suffers from \u201corder effects\u201d, i.e., different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results; specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results.ObjectiveTo provide a method in which distributions generated by LDA are more stable and can be used for further analysis.MethodWe use LDADE, a search-based software engineering tool which uses Differential Evolution (DE) to tune the LDA\u2019s parameters. LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of Software Engineering (SE\u00a0\u2026", "num_citations": "140\n", "authors": ["237"]}
{"title": "Easy over Hard: A Case Study on Deep Learning\n", "abstract": " While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~ a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~ other researchers to repeat, improve, or even refute that original work.", "num_citations": "124\n", "authors": ["237"]}
{"title": "Revisiting Unsupervised Learning for Defect Prediction\n", "abstract": " Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore\" unsupervised\" approaches to quality prediction that does not require labelled data. An alternate technique is to use\" supervised\" approaches that learn models from project data labelled with, say,\" defective\" or\" not-defective\". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects.", "num_citations": "84\n", "authors": ["237"]}
{"title": "Too much automation? The bellwether effect and its implications for transfer learning\n", "abstract": " Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing--at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether\u00a0\u2026", "num_citations": "51\n", "authors": ["237"]}
{"title": "How to\" DODGE\" Complex Software Analytics?\n", "abstract": " Machine learning techniques applied to software engineering tasks can be improved by hyperparameter optimization, i.e., automatic tools that find good settings for a learner's control parameters. We show that such hyperparameter optimization can be unnecessarily slow, particularly when the optimizers waste time exploring \"redundant tunings\", i.e., pairs of tunings which lead to indistinguishable results. By ignoring redundant tunings, DODGE, a tuning tool, runs orders of magnitude faster, while also generating learners with more accurate predictions than seen in prior state-of-the-art approaches.", "num_citations": "32\n", "authors": ["237"]}
{"title": "Applications of psychological science for actionable analytics\n", "abstract": " According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of\" heuristic\" s (ie lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics.", "num_citations": "23\n", "authors": ["237"]}
{"title": "Robust null-space based interference avoiding scheme for D2D communication underlaying cellular networks\n", "abstract": " In this paper, we design a null-space based robust interference avoiding strategy for the Device-to-Device (D2D) communication underlaying network. Thanks to the coordination between D2D user and the regular user, the interfering channel state information (CSI) among the base station (BS), cellular user equipment (CUE) and the D2D user equipments (DUEs) can be estimated from the training approach. Then, the null-space based transmit and receive beam-formings are designed at appropriate terminals to mitigate the interference caused in the future data transmission. To make the design practical, we also characterize the null-space uncertainty that is resulted from the imperfect channel estimation. Moreover, we derive the optimal transmission strategy that can achieve the best training-throughput tradeoff. Simulation results are provided to corroborate the proposed studies.", "num_citations": "15\n", "authors": ["237"]}
{"title": "Predicting project health for open source projects (using the DECART hyperparameter optimizer)\n", "abstract": " Software developed on public platforms is a source of data that can be used to make predictions about those projects. While the activity of a single developer may be random and hard to predict, when large groups of developers work together on software projects, the resulting behavior can be predicted with good accuracy. To demonstrate this, we use 78,455 months of data from 1,628 GitHub projects to make various predictions about the current status of those projects (as of April 2020). We find that traditional estimation algorithms make many mistakes. Algorithms like k-nearest neighbors (KNN), support vector regression (SVR), random forest (RFT), linear regression (LNR), and regression trees (CART) have high error rates (usually more than 50% wrong, sometimes over 130% wrong, median values). But that error rate can be greatly reduced using the DECART hyperparameter optimization. DECART is a differential evolution (DE) algorithm that tunes the CART data mining system to the particular details of a specific project. To the best of our knowledge, this is the largest study yet conducted, using the most recent data, for predicting multiple health indicators of open-source projects. Further, due to our use of hyperparameter optimization, the median predicting error of our predictions usually less than 10% which is much smaller than the errors seen in related work. Our results are a compelling argument for open-sourced development. Companies that only build in-house proprietary products may be cutting themselves off from the information needed to reason about those projects.", "num_citations": "2\n", "authors": ["237"]}