{"title": "QoS-aware middleware for web services composition\n", "abstract": " The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are\u00a0\u2026", "num_citations": "3968\n", "authors": ["2067"]}
{"title": "Quality driven web services composition\n", "abstract": " The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. The tasks of this process model are essentially invocations to functionalities offered by the underlying component services. Usually, several component services are able to execute a given task, although with different levels of pricing and quality. In this paper, we advocate that the selection of component services should be carried out during the execution of a composite service, rather than at design-time. In addition, this selection should consider multiple criteria (eg, price, duration, reliability), and it should take into account global constraints and preferences set by the user (eg, budget constraints). Accordingly\u00a0\u2026", "num_citations": "1729\n", "authors": ["2067"]}
{"title": "A petri net-based model for web service composition\n", "abstract": " The Internet is going through several major changes. It has become a vehicle of Web services rather than just a repository of information. Many organizations are putting their core business competencies on the Internet as a collection of Web services. An important challenge is to integrate them to create new value-added Web services in ways that could never be foreseen forming what is known as Business-to-Business (B2B) services. Therefore, there is a need for modeling techniques and tools for reliable Web service composition. In this paper, we propose a Petri net-based algebra, used to model control flows, as a necessary constituent of reliable Web service composition process. This algebra is expressive enough to capture the semantics of complex Web service combinations.", "num_citations": "1103\n", "authors": ["2067"]}
{"title": "Declarative composition and peer-to-peer provisioning of dynamic web services\n", "abstract": " The development of new services through the integration of existing ones has gained a considerable momentum as a means to create and streamline business-to-business collaborations. Unfortunately, as Web services are often autonomous and heterogeneous entities, connecting and coordinating them in order to build integrated services is a delicate and time-consuming task. In this paper, we describe the design and implementation of a system through which existing Web services can be declaratively composed, and the resulting composite services can be executed following a peer-to-peer paradigm, within a dynamic environment. This system provides tools for specifying composite services through. statecharts, data conversion rules, and provider selection, policies. These specifications are then translated into XML documents that can be interpreted by peer-to-peer inter-connected software components, in\u00a0\u2026", "num_citations": "632\n", "authors": ["2067"]}
{"title": "Business-to-business interactions: issues and enabling technologies\n", "abstract": " Business-to-Business (B2B) technologies pre-date the Web. They have existed for at least as long as the Internet. B2B applications were among the first to take advantage of advances in computer networking. The Electronic Data Interchange (EDI) business standard is an illustration of such an early adoption of the advances in computer networking. The ubiquity and the affordability of the Web has made it possible for the masses of businesses to automate their B2B interactions. However, several issues related to scale, content exchange, autonomy, heterogeneity, and other issues still need to be addressed. In this paper, we survey the main techniques, systems, products, and standards for B2B interactions. We propose a set of criteria for assessing the different B2B interaction techniques, standards, and products.", "num_citations": "620\n", "authors": ["2067"]}
{"title": "Quality control in crowdsourcing systems: Issues and directions\n", "abstract": " As a new distributed computing model, crowdsourcing lets people leverage the crowd's intelligence and wisdom toward solving problems. This article proposes a framework for characterizing various dimensions of quality control in crowdsourcing systems, a critical issue. The authors briefly review existing quality-control approaches, identify open issues, and look to future research directions. In the Web extra, the authors discuss both design-time and runtime approaches in more detail.", "num_citations": "473\n", "authors": ["2067"]}
{"title": "ContextUML: a UML-based modeling language for model-driven development of context-aware web services\n", "abstract": " Context-aware Web services are emerging as a promising technology for the electronic businesses in mobile and pervasive environments. Unfortunately, complex context-aware services are still hard to build. In this paper, we present a modeling language for the model-driven development of context-aware Web services based on the Unified Modeling Language (UML). Specifically, we show how UML can be used to specify information related to the design of context-aware services. We present the abstract syntax and notation of the language and illustrate its usage using an example service. Our language offers significant design flexibility that considerably simplifies the development of context-aware Web services.", "num_citations": "377\n", "authors": ["2067"]}
{"title": "On automating web services discovery\n", "abstract": " One of the challenging problems that Web service technology faces is the ability to effectively discover services based on their capabilities. We present an approach to tackling this problem in the context of description logics (DLs). We formalize service discovery as a new instance of the problem of rewriting concepts using terminologies. We call this new instance the best covering problem. We provide a formalization of the best covering problem in the framework of DL-based ontologies and propose a hypergraph-based algorithm to effectively compute best covers of a given request. We propose a novel matchmaking algorithm that takes as input a service request (or query) Q and an ontology  of services and finds a set of services called a \u201cbest cover\u201d of Q whose descriptions contain as much common information with Q as possible and as little extra information with respect to Q as possible. We have\u00a0\u2026", "num_citations": "350\n", "authors": ["2067"]}
{"title": "Self-serv: A platform for rapid composition of web services in a peer-to-peer environment\n", "abstract": " Publisher SummarySELF-SERV distinguishes three types of services: elementary services, composite services, and service communities. An elementary service is an individual Web accessible application (for example, a Java program) that does not explicitly rely on another Web service. A composite service aggregates multiple Web services that are referred to as its components. SELF-SERV relies on a declarative language for composing services based on state charts: a widely used formalism in the area of reactive systems that is emerging as a standard for process modeling following its integration into the Unified Modeling Language (UML). An operation of a composite service can be seen as having input parameters, output parameters, consumed and produced events, and a state chart gluing these elements together. The technology to compose Web services in appropriate time frames has not kept pace with\u00a0\u2026", "num_citations": "262\n", "authors": ["2067"]}
{"title": "Data integration in mashups\n", "abstract": " Mashup is a new application development approach that allows users to aggregate multiple services to create a service for a new purpose. Even if the Mashup approach opens new and broader opportunities for data/service consumers, the development process still requires the users to know not only how to write code using programming languages, but also how to use the different Web APIs from different services. In order to solve this problem, there is increasing effort put into developing tools which are designed to support users with little programming knowledge in Mashup applications development. The objective of this study is to analyze the richnesses and weaknesses of the Mashup tools with respect to the data integration aspect.", "num_citations": "216\n", "authors": ["2067"]}
{"title": "Facilitating the rapid development and scalable orchestration of composite web services\n", "abstract": " The development of new Web services through the composition of existing ones has gained a considerable momentum as a means to realise business-to-business collaborations. Unfortunately, given that services are often developed in an ad hoc fashion using manifold technologies and standards, connecting and coordinating them in order to build composite services is a delicate and time-consuming task. In this paper, we describe the design and implementation of a system in which services are composed using a model-driven approach, and the resulting composite services are orchestrated following a peer-to-peer paradigm. The system provides tools for specifying composite services through statecharts, data conversion rules, and multi-attribute provider selection policies. These specifications are interpreted by software components that interact in a peer-to-peer way to coordinate the execution of the\u00a0\u2026", "num_citations": "211\n", "authors": ["2067"]}
{"title": "Cloud computing: methodology, systems, and applications\n", "abstract": " Cloud computing has created a shift from the use of physical hardware and locally managed software-enabled platforms to that of virtualized cloud-hosted services. Cloud assembles large networks of virtual services, including hardware (CPU, storage, and network) and software resources (databases, message queuing systems, monitoring systems, and load-balancers). As Cloud continues to revolutionize applications in academia, industry, government, and many other fields, the transition to this efficient and flexible platform presents serious challenges at both theoretical and practical levels\u2014ones that will often require new approaches and practices in all areas. Comprehensive and timely, Cloud Computing: Methodology, Systems, and Applications summarizes progress in state-of-the-art research and offers step-by-step instruction on how to implement it. Summarizes Cloud Developments, Identifies Research Challenges, and Outlines Future Directions Ideal for a broad audience that includes researchers, engineers, IT professionals, and graduate students, this book is designed in three sections: Fundamentals of Cloud Computing: Concept, Methodology, and Overview Cloud Computing Functionalities and Provisioning Case Studies, Applications, and Future Directions It addresses the obvious technical aspects of using Cloud but goes beyond, exploring the cultural/social and regulatory/legal challenges that are quickly coming to the forefront of discussion. Properly applied as part of an overall IT strategy, Cloud can help small and medium business enterprises (SMEs) and governments in optimizing expenditure on application-hosting\u00a0\u2026", "num_citations": "187\n", "authors": ["2067"]}
{"title": "Request rewriting-based web service discovery\n", "abstract": " One of the challenging problems that Web service technology faces is the ability to effectively discover services based on their capabilities. We present an approach to tackle this problem in the context of DAML-S ontologies of services. The proposed approach enables to select the combinations of Web services that best match a given request Q and effectively computes the extra information with respect to Q (e.g., the information required by a service request but not provided by any existing service). We study the reasoning problem associated with such a matching process and propose an algorithm derived from hypergraphs theory.", "num_citations": "162\n", "authors": ["2067"]}
{"title": "Protocol-aware matching of web service interfaces for adapter development\n", "abstract": " With the rapid growth in the number of online Web services, the problem of service adaptation has received significant attention. In matching and adaptation, the functional description of services including interface and data as well as behavioral descriptions are important. Existing work on matching and adaptation focuses only on one aspect.", "num_citations": "127\n", "authors": ["2067"]}
{"title": "Dynamic composition and optimization of web services\n", "abstract": " Process-based composition of Web services has recently gained significant momentum for the implementation of inter-organizational business collaborations. In this approach, individual Web services are choreographed into composite Web services whose integration logics are expressed as composition schema. In this paper, we present a goal-directed composition framework to support on-demand business processes. Composition schemas are generated incrementally by a rule inference mechanism based on a set of domain-specific business rules enriched with contextual information. In situations where multiple composition schemas can achieve the same goal, we must first select the best composition schema, wherein the best schema is selected based on the combination of its estimated execution quality and schema quality. By coupling the dynamic schema creation and quality-driven selection\u00a0\u2026", "num_citations": "123\n", "authors": ["2067"]}
{"title": "Interconnecting heterogeneous information systems\n", "abstract": " Information systems are the backbone of many of today's computerized applications. Distributed databases and the infrastructure needed to support them have been well studied. However, this book is the first to address distributed database interoperability by examining the successes and failures, various approaches, infrastructures, and trends of the field. A gap exists in the way that these systems have been investigated by real practitioners. This gap is more pronounced than usual, partly because of the way businesses operate, the systems they have, and the difficulties created by systems' autonomy and heterogeneity. Telecommunications firms, for example, must deal with an increased demand for automation while at the same time continuing to function at their current level. While academics are focusing on investigating differences between distributed databases, federated databases, heterogeneous databases, and, more generally, among loosely connected and tightly coupled systems, those who have to deal with real problems right away know that the only relevant research is the one that will ensure that their system works to produce reasonably correct results. Interconnecting Heterogeneous Information Systems covers the underlying principles and infrastructures needed to realize truly global information systems. The book discusses technologies related to middleware, the Web, workflows, transactions, and data warehousing. It also overviews architectures with a discussion of critical issues. The book gives an overview of systems that can be viewed as learning platforms. While these systems do not translate to successful commercial\u00a0\u2026", "num_citations": "120\n", "authors": ["2067"]}
{"title": "Configurable composition and adaptive provisioning of web services\n", "abstract": " Web services composition has been an active research area over the last few years. However, the technology is still not mature yet and several research issues need to be addressed. In this paper, we describe the design of CCAP, a system that provides tools for adaptive service composition and provisioning. We introduce a composition model where service context and exceptions are configurable to accommodate needs of different users. This allows for reusability of a service in different contexts and achieves a level of adaptiveness and contextualization without recoding and recompiling of the overall composed services. The execution semantics of the adaptive composite service is provided by an event-driven model. This execution model is based on Linda Tuple Spaces and supports real-time and asynchronous communication between services. Three core services, coordination service, context service, and\u00a0\u2026", "num_citations": "111\n", "authors": ["2067"]}
{"title": "A top-down petri net-based approach for dynamic workflow modeling\n", "abstract": " A top-down approach for workflow design is proposed in the framework of Petri net theory. Simple but powerful refinement rules are proposed that guarantee soundness of the resulting workflow nets. The refinement process supports the definition of regions, which are parts of the workflow that correspond to logistically related items. Exception handlers can be associated to regions. Defining regions helps determining the impact areas of the unexpected events during workflow execution.", "num_citations": "108\n", "authors": ["2067"]}
{"title": "Enabling personalized composition and adaptive provisioning of web services\n", "abstract": " The proliferation of interconnected computing devices is fostering the emergence of environments where Web services made available to mobile users are a commodity. Unfortunately, inherent limitations of mobile devices still hinder the seamless access to Web services, and their use in supporting complex user activities. In this paper, we describe the design and implementation of a distributed, adaptive, and context-aware framework for personalized service composition and provisioning adapted to mobile users. Users specify their preferences by annotating existing process templates, leading to personalized service-based processes. To cater for the possibility of low bandwidth communication channels and frequent disconnections, an execution model is proposed whereby the responsibility of orchestrating personalized processes is spread across the participating services and user agents. In addition\u00a0\u2026", "num_citations": "104\n", "authors": ["2067"]}
{"title": "Cloud resource orchestration programming: overview, issues, and directions\n", "abstract": " Cloud computing provides on-demand access to affordable hardware (such as multicore CPUs, GPUs, disk drives, and networking equipment) and software (databases, application servers, load-balancers, data processors, and frameworks). The pervasiveness and power of cloud computing alleviates some of the problems that application administrators face in their existing hardware and locally managed software environments. However, the rapid increase in scale, dynamicity, heterogeneity, and diversity of cloud resources necessitates having expert knowledge about programming complex orchestration operations (for example, selection, deployment, monitoring, and runtime control) on those resources to achieve the desired quality of service. This article provides an overview of the key cloud resource types and resource orchestration operations, with special focus on research issues involved in programming\u00a0\u2026", "num_citations": "103\n", "authors": ["2067"]}
{"title": "A query language for analyzing business processes execution\n", "abstract": " The execution of a business process (BP) in today\u2019s enterprises may involve a workflow and multiple IT systems and services. Often no complete, up-to-date documentation of the model or correlation information of process events exist. Understanding the execution of a BP in terms of its scope and details is challenging specially as it is subjective: depends on the perspective of the person looking at BP execution. We present a framework, simple abstractions and a language for the explorative querying and understanding of BP execution from various user perspectives. We propose a query language for analyzing event logs of process-related systems based on the two concepts of folders and paths, which enable an analyst to group related events in the logs or find paths among events. Folders and paths can be stored to be used in follow-on analysis. We have implemented the proposed techniques and the\u00a0\u2026", "num_citations": "100\n", "authors": ["2067"]}
{"title": "Flexible composition of enterprise web services\n", "abstract": " The process-based composition of Web services is emerging as a promising approach to automate business process within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. Business process automation technology such as workflow management systems (WFMSs) can be used to choreograph the component services. However, one of the fundamental assumptions of most WFMSs is that workflow schemas are static and predefined. Such an assumption is impractical for business processes that have an explosive number of options, or dynamic business processes that must be generated and altered on the fly to meet rapid changing business conditions. In this paper, we describe a rule inference framework called DY flow , where end users declaratively define their business objectives\u00a0\u2026", "num_citations": "99\n", "authors": ["2067"]}
{"title": "On composite web services provisioning in an environment of fixed and mobile computing resources\n", "abstract": " We present a framework for Web services provisioning in a hybrid environment of fixed and mobile computing resources. Several obstacles still hinder the seamless provisioning of Web services in mobile environments. Examples of such obstacles are: throughput and connectivity of wireless networks, limited computing resources of mobile devices, and risks of communication channel disconnections. In the proposed framework, software agents represent users, providers of services, and providers of resources. The business logic of composite services is expressed as a process model using statecharts formalism. Among other things, the use of agents provides an infrastructure that has the ability to handle disconnections during service preparation for execution. The framework also integrates a service execution planning approach to optimally select computing resources (fixed or mobile) on top of which\u00a0\u2026", "num_citations": "93\n", "authors": ["2067"]}
{"title": "Policy-driven exception-management for composite web services\n", "abstract": " Process-based composition of Web services has recently gained significant momentum in the implementation of business processes. A critical and time-consuming part of business process development is the detection and handling of exceptions that may occur during process execution. In this paper, we introduce a novel, policy-driven approach to exception management, which substantially simplifies business process development. We advocate that exception management should be implemented in the system infrastructure. Using our exception management framework, developers define exception policies in a declarative manner. Before a business process is executed, the service composition middleware integrates the exception policies with normal business logic to generate an exception-aware process schema. We argue that our policy-driven approach significantly reduces the development time of business\u00a0\u2026", "num_citations": "91\n", "authors": ["2067"]}
{"title": "An agent-based approach for supporting cross-enterprise workflows\n", "abstract": " In order to support global competitiveness and rapid market responsiveness, virtual enterprises need to efficiently integrate different organization's workflows to provide customized services. Currently, most of the integrations are case based which have high setup cost and involve time consuming low level programming. Cross-enterprise workflow that is able to streamline and coordinate business processes across organizations in dynamic Web environment provides a low cost and flexible solution. We develop an agent based cross-enterprise workflow management system (WFMS) architecture which can dynamically integrate the workflows and compose a workflow execution community customized to different workflow specifications.", "num_citations": "90\n", "authors": ["2067"]}
{"title": "Interleaving web services composition and execution using software agents and delegation\n", "abstract": " The paper presents a software agent-based approach that supports the interleaving of Web services composition and execution. A Web service is an accessible application that other applications and humans as well can automatically discover and invoke. Interleaving stands for carrying out the composition and execution of Web services in parallel. This allows handling the execution context of the Web services. Dynamic information that change overtime can feature such a context and require thus, a certain form of adjustment of the Web services. In this paper, the deployment operations of Web services composition and execution are entrusted to software agents that can delegate their work to each other if needed.", "num_citations": "84\n", "authors": ["2067"]}
{"title": "A taxonomy and survey of cloud resource orchestration techniques\n", "abstract": " Cloud services and applications prove indispensable amid today\u2019s modern utility-based computing. The cloud has displayed a disruptive and growing impact on everyday computing tasks. However, facilitating the orchestration of cloud resources to build such cloud services and applications is yet to unleash its entire magnitude of power. Accordingly, it is paramount to devise a unified and comprehensive analysis framework to accelerate fundamental understanding of cloud resource orchestration in terms of concepts, paradigms, languages, models, and tools. This framework is essential to empower effective research, comprehension, comparison, and selection of cloud resource orchestration models, languages, platforms, and tools. This article provides such a comprehensive framework while analyzing the relevant state of the art in cloud resource orchestration from a novel and holistic viewpoint.", "num_citations": "81\n", "authors": ["2067"]}
{"title": "A taxonomy and survey on autonomic management of applications in grid computing environments\n", "abstract": " In Grid computing environments, the availability, performance, and state of resources, applications, services, and data undergo continuous changes during the life cycle of an application. Uncertainty is a fact in Grid environments, which is triggered by multiple factors, including: (1) failures, (2) dynamism, (3) incomplete global knowledge, and (4) heterogeneity. Unfortunately, the existing Grid management methods, tools, and application composition techniques are inadequate to handle these resource, application and environment behaviors. The aforementioned characteristics impose serious requirements on the Grid programming and runtime systems if they wish to deliver efficient performance to scientific and commercial applications. To overcome the above challenges, the Grid programming and runtime systems must become autonomic or self\u2010managing in accordance with the high\u2010level behavior specified by\u00a0\u2026", "num_citations": "79\n", "authors": ["2067"]}
{"title": "Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface\n", "abstract": " Brain-Computer Interface (BCI) is a system empowering humans to communicate with or control the outside world with exclusively brain intentions. Electroencephalography (EEG) based BCIs are promising solutions due to their convenient and portable instruments. Despite the extensive research of EEG in recent years, it is still challenging to interpret EEG signals effectively due to the massive noises in EEG signals (eg, low signal-noise ratio and incomplete EEG signals), and difficulties in capturing the inconspicuous relationships between EEG signals and certain brain activities. Most existing works either only consider EEG as chain-like sequences neglecting complex dependencies between adjacent signals or requiring pre-processing such as transforming EEG waves into images. In this paper, we introduce both cascade and parallel convolutional recurrent neural network models for precisely identifying human intended movements and instructions effectively learning the compositional spatio-temporal representations of raw EEG streams. Extensive experiments on a large scale movement intention EEG dataset (108 subjects, 3,145,160 EEG records) have demonstrated that both models achieve high accuracy near 98.3% and outperform a set of baseline methods and most recent deep learning based EEG recognition models, yielding a significant accuracy increase of 18% in the cross-subject validation scenario. The developed models are further evaluated with a real-world BCI and achieve a recognition accuracy of 93% over five instruction intentions. This suggests the proposed models are able to generalize over different kinds of intentions\u00a0\u2026", "num_citations": "76\n", "authors": ["2067"]}
{"title": "Coredb: a data lake service\n", "abstract": " The continuous improvement in connectivity, storage and data processing capabilities allow access to a data deluge from sensors, social-media, news, user-generated, government and private data sources. Accordingly, in a modern data-oriented landscape, with the advent of various data capture and management technologies, organizations are rapidly shifting to datafication of their processes. In such an environment, analysts may need to deal with a collection of datasets, from relational to NoSQL, that holds a vast amount of data gathered from various private/open data islands, ie Data Lake. Organizing, indexing and querying the growing volume of internal data and metadata, in a data lake, is challenging and requires various skills and experiences to deal with dozens of new databases and indexing technologies: How to store information items? What technology to use for persisting the data? How to deal with\u00a0\u2026", "num_citations": "70\n", "authors": ["2067"]}
{"title": "Reputation management in crowdsourcing systems\n", "abstract": " Worker selection is a significant and challenging issue in crowdsourcing systems. Such selection is usually based on an assessment of the reputation of the individual workers participating in such systems. However, assessing the credibility and adequacy of such calculated reputation is a real challenge. In this paper, we propose a reputation management model which leverages the values of the tasks completed, the credibility of the evaluators of the results of the tasks and time of evaluation of the results of these tasks in order to calculate more dependable quality metrics for workers and evaluators. The model has been implemented and experimentally validated.", "num_citations": "70\n", "authors": ["2067"]}
{"title": "Handling transactional properties in web service composition\n", "abstract": " The development of new services by composition of existing ones has gained considerable momentum as a means of integrating heterogeneous applications and realising business collaborations. Services that enter into compositions with other services may have transactional properties, especially those in the broad area of resource management (e.g. booking services). These transactional properties may be exploited in order to derive composite services which themselves exhibit certain transactional properties. This paper presents a model for composing services that expose transactional properties and more specifically, services that support tentative holds and/or atomic execution. The proposed model is based on a high-level service composition operator that produces composite services that satisfy specified atomicity constraints. The model supports the possibility of selecting the services that enter\u00a0\u2026", "num_citations": "70\n", "authors": ["2067"]}
{"title": "Collusion detection in online rating systems\n", "abstract": " Online rating systems are subject to unfair evaluations. Users may try to individually or collaboratively promote or demote a product. Collaborative unfair rating, i.e., collusion, is more damaging than individual unfair rating. Detecting massive collusive attacks as well as honest looking intelligent attacks is still a real challenge for collusion detection systems. In this paper, we study impact of collusion in online rating systems and asses their susceptibility to collusion attacks. The proposed model uses frequent itemset mining technique to detect candidate collusion groups and sub-groups. Then, several indicators are used for identifying collusion groups and to estimate how damaging such colluding groups might be. The model has been implemented and we present results of experimental evaluation of our methodology.", "num_citations": "69\n", "authors": ["2067"]}
{"title": "Service Chart Diagrams-Description & Application.\n", "abstract": " This paper presents an approach for the design and development of service-driven applications. These applications rely on the collaboration of multiple services that businesses offer to the external community. To ensure that the collaboration of services takes place effectively, service chart diagrams are proposed as a specification technique. These diagrams leverage the traditional state chart diagrams of UML. Furthermore, in service chart diagrams it is advocated that services do not invoke each other. However, they engage conversations before committing themselves to a composition process of services.", "num_citations": "68\n", "authors": ["2067"]}
{"title": "Scalable graph-based OLAP analytics over process execution data\n", "abstract": " In today\u2019s knowledge-, service-, and cloud-based economy, businesses accumulate massive amounts of data from a variety of sources. In order to understand businesses one may need to perform considerable analytics over large hybrid collections of heterogeneous and partially unstructured data that is captured related to the process execution. This data, usually modeled as graphs, increasingly come to show all the typical properties of big data: wide physical distribution, diversity of formats, non-standard data models, independently-managed and heterogeneous semantics. We use the term big process graph to refer to such large hybrid collections of heterogeneous and partially unstructured process related execution data. Online analytical processing (OLAP) of big process graph is challenging as the extension of existing OLAP techniques to analysis of graphs is not straightforward. Moreover, process\u00a0\u2026", "num_citations": "67\n", "authors": ["2067"]}
{"title": "Recovery nets: Towards self-adaptive workflow systems\n", "abstract": " A workflow management system (WfMS) provides a central control point for defining business processes and orchestrating their execution. A major limitation of current WfMSs is their lack of support for dynamic workflow adaptations. This functionality is an important requirement in order to provide sufficient flexibility to cope with expected but unusual situations and failures. In this paper, we propose Self-Adaptive Recovery Net (SARN), an extended Petri net model for specifying exceptional behavior in workflow systems at design time. SARN can adapt the structure of the underlying Petri net at run time to handle exceptions while keeping the Petri net design simple and easy. The proposed framework also caters for high-level recovery policies that are incorporated either with a single task or a set of tasks, called a recovery region.", "num_citations": "64\n", "authors": ["2067"]}
{"title": "CoreKG: a knowledge lake service\n", "abstract": " With Data Science continuing to emerge as a powerful differentiator across industries, organisations are now focused on transforming their data into actionable insights. This task is challenging as in today's knowledge-, service-, and cloud-based economy, businesses accumulate massive amounts of raw data from a variety of sources. Data Lakes introduced as a storage repository to organize this raw data in its native format (supporting from relational to NoSQL DBs) until it is needed. The rationale behind a Data Lake is to store raw data and let the data analyst decide how to cook/curate them later. In this paper, we present the notion of Knowledge Lake, i.e. a contextualized Data Lake. The Knowledge Lake will provide the foundation for big data analytics by automatically curating the raw data in the Data Lake and to prepare them for deriving insights. We present CoreKG-an open source Data and Knowledge Lake\u00a0\u2026", "num_citations": "63\n", "authors": ["2067"]}
{"title": "Process analytics: concepts and techniques for querying and analyzing process data\n", "abstract": " This book starts with an introduction to process modeling and process paradigms, then explains how to query and analyze process models, and how to analyze the process execution data. In this way, readers receive a comprehensive overview of what is needed to identify, understand and improve business processes. The book chiefly focuses on concepts, techniques and methods. It covers a large body of knowledge on process analytics\u2013including process data querying, analysis, matching and correlating process data and models\u2013to help practitioners and researchers understand the underlying concepts, problems, methods, tools and techniques involved in modern process analytics. Following an introduction to basic business process and process analytics concepts, it describes the state of the art in this area before examining different analytics techniques in detail. In this regard, the book covers analytics over different levels of process abstractions, from process execution data and methods for linking and correlating process execution data, to inferring process models, querying process execution data and process models, and scalable process data analytics methods. In addition, it provides a review of commercial process analytics tools and their practical applications. The book is intended for a broad readership interested in business process management and process analytics. It provides researchers with an introduction to these fields by comprehensively classifying the current state of research, by describing in-depth techniques and methods, and by highlighting future research directions. Lecturers will find a wealth of material to choose\u00a0\u2026", "num_citations": "61\n", "authors": ["2067"]}
{"title": "Self-adapting recovery nets for policy-driven exception handling in business processes\n", "abstract": " In this paper, we propose Self-Adapting Recovery Net (SARN), an extended Petri net model, for specifying exceptional behavior in business processes. SARN adapts the structure of the underlying Petri net at run time to handle exceptions while keeping the Petri net design easy. The proposed framework caters for the specification of high-level recovery policies that are incorporated either with a single task or a set of tasks, called a Recovery Region. These recovery policies are generic directives that model exceptions at design time together with a set of primitive operations used at run time to handle the occurrence of exceptions. We identified a set of recovery policies that are useful and commonly needed in many practical situations. A tool has been developed to illustrate the viability of the proposed exception handling technique.", "num_citations": "60\n", "authors": ["2067"]}
{"title": "Supporting dynamic interactions among web-based information sources\n", "abstract": " The ubiquity of the World Wide Web offers an ideal opportunity for the deployment of highly distributed applications. Now that connectivity is no longer an issue, attention has turned to providing a middleware infrastructure that will sustain data sharing among Web-accessible databases. We present a dynamic architecture and system for describing, locating, and accessing data from Web-accessible databases. We propose the use of flexible organizational constructs service links and coalitions to facilitate data organization, discovery, and sharing among Internet-accessible databases. A language is also proposed to support the definition and manipulation of these constructs. The implementation combines Java, CORBA, database API (JDBC), agent, and database technologies to support a scalable and portable architecture interconnecting large networks of heterogeneous and autonomous databases. We report on\u00a0\u2026", "num_citations": "60\n", "authors": ["2067"]}
{"title": "Adapting a spreadsheet for use with a complex object\n", "abstract": " A method for use with a spreadsheet includes storing a cell object, where the cell object includes a location in the spreadsheet of a cell to which the cell object relates and a process associated with the cell, and performing the process on a complex object to produce a result, where the complex object includes a construct comprised of data and code. A display is generated for the cell that is based on the result.", "num_citations": "57\n", "authors": ["2067"]}
{"title": "A framework and a language for on-line analytical processing on graphs\n", "abstract": " Graphs are essential modeling and analytical objects for representing information networks. Existing approaches, in on-line analytical processing on graphs, took the first step by supporting multi-level and multi-dimensional queries on graphs, but they do not provide a semantic-driven framework and a language to support n-dimensional computations, which are frequent in OLAP environments. The major challenge here is how to extend decision support on multidimensional networks considering both data objects and the relationships among them. Moreover, one of the critical deficiencies of graph query languages, e.g. SPARQL, is the lack of support for n-dimensional computations. In this paper, we propose a graph data model, GOLAP, for online analytical processing on graphs. This data model enables extending decision support on multidimensional networks considering both data objects and the\u00a0\u2026", "num_citations": "57\n", "authors": ["2067"]}
{"title": "Definition and execution of composite web services: The self-serv project\n", "abstract": " Web services composition is emerging as a promising technology for the effective automation of businessto-business collaborations. It allows organizations to form alliances by connecting their applications, databases, and systems, in order to offer \u201cone-stops shops\u201d for their customers. The SELF-SERV project aims at providing tool support and middleware infrastructure for the definition and execution of composite Web services. A major outcome of the project has been a prototype system in which Web services are declaratively composed, and the resulting composite services can be orchestrated either in a peerto-peer or in a centralized way within a dynamic environment. Work is underway to extend this system in order to enable user-driven composition of Web services, and their execution in a mobile environment.", "num_citations": "57\n", "authors": ["2067"]}
{"title": "A systematic review and comparative analysis of cross-document coreference resolution methods and tools\n", "abstract": " Information extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. Among various IE tasks, extracting actionable intelligence from an ever-increasing amount of data depends critically upon cross-document coreference resolution (CDCR) - the task of identifying entity mentions across information sources that refer to the same underlying entity. CDCR is the basis of knowledge acquisition and is at the heart of Web search, recommendations, and analytics. Real time processing of CDCR processes is very important and have various applications in discovering must-know information in real-time for clients in finance, public sector, news, and crisis management. Being an emerging area of research and practice, the reported literature on CDCR challenges and solutions is growing fast but is scattered due to the large space\u00a0\u2026", "num_citations": "55\n", "authors": ["2067"]}
{"title": "Composing and Maintaining Web-based Virtual Enterprises.\n", "abstract": " Electronic commerce (E-commerce) is evolving from the simple notion of electronic catalogs to the notion of virtual enterprises (VEs). Existing enterprises form alliances, joining their services in order to share their costs, skills and resources in offering a value-added service. Ad-hoc and proprietary solutions on the one hand, and lack of a canonical model for composing and managing VEs on the other hand, have largely hampered a faster pace in deploying Web-based VEs. In this paper, we propose a generic framework for creating and maintaining VEs. We introduce a language, WebBI\u0116-\u0116DL, to cater for the definition and maintenance of VEs. To provide an early feasibility of the proposed framework, we have implemented a prototype that allows easy definition and maintenance of VEs. The implementation architecture is based on CORBA and proven Web technologies including Java and database APIs.", "num_citations": "53\n", "authors": ["2067"]}
{"title": "Peer-to-peer traced execution of composite services\n", "abstract": " The connectivity generated by the Internet is opening unprecedented opportunities of automating business-to-business collaborations. As a result, organisations of all sizes are forming online alliances in order to deliver integrated value-added services. Unfortunately, due to a lack of tools and methodologies offering an adequate level of abstraction, the development of these integrated services is currently ad hoc and requires a considerable effort of low-level programming, especially when dealing with coordination, communication, and execution tracing issues. In this paper, we present a framework through which business services can be declaratively composed, and the resulting composite services can be executed in a fully traceable manner. The traces of a composite service executions are collected incrementally through peer-to-peer interactions between the involved providers. Once collected, these\u00a0\u2026", "num_citations": "52\n", "authors": ["2067"]}
{"title": "Service oriented architecture: Overview and directions\n", "abstract": " The push toward business automation, motivated by opportunities in terms of cost savings and higher quality, more reliable executions, has generated the need for integrating the different applications. Integration has been one of the main drivers in the software market during the late nineties and into the new millennium. It has led to a large body of research and development in areas such as data integration [26], software components integration, enterprise information integration (EII), enterprise applications integration (EAI), and recently service integration and composition [2,11,16,12].               Service oriented architectures (SOAs) provide an architectural paradigm and abstractions that allow to simplify integration [2,21]. There a number of technologies available to realize SOA. Among them, Web services and the set of related specifications (referred to as WS-* family), and also services that are built\u00a0\u2026", "num_citations": "49\n", "authors": ["2067"]}
{"title": "Towards semantic-driven, flexible and scalable framework for peering and querying e-catalog communities\n", "abstract": " Given that e-catalogs are often autonomous and heterogeneous, effectively integrating and querying them is a delicate and time-consuming task. More importantly, the number of e-catalogs to be integrated and queried may be large and continuously changing. Conventional approaches where the development of an integrated e-catalog requires the understanding of each of the underlying catalog are inappropriate. In this paper, we use the concept of e-catalog communities and peer relationships among them to facilitate the querying of a potentially large number of dynamic e-catalogs. E-catalog communities are essentially containers of related e-catalogs. We propose a flexible and user-centric query matching algorithm that exploits both community descriptions and peer relationships to find e-catalogs that best match a user query. The user query is formulated using a description of a given community.", "num_citations": "47\n", "authors": ["2067"]}
{"title": "DARec: Deep domain adaptation for cross-domain recommendation via transferring rating patterns\n", "abstract": " Cross-domain recommendation has long been one of the major topics in recommender systems. Recently, various deep models have been proposed to transfer the learned knowledge across domains, but most of them focus on extracting abstract transferable features from auxilliary contents, e.g., images and review texts, and the patterns in the rating matrix itself is rarely touched. In this work, inspired by the concept of domain adaptation, we proposed a deep domain adaptation model (DARec) that is capable of extracting and transferring patterns from rating matrices {\\em only} without relying on any auxillary information. We empirically demonstrate on public datasets that our method achieves the best performance among several state-of-the-art alternative cross-domain recommendation models.", "num_citations": "45\n", "authors": ["2067"]}
{"title": "An integrated service architecture for managing capital market systems\n", "abstract": " This article studies current developments and trends in the area of capital market systems. In particular, it defines the trading lifecycle and the activities associated with it. The article then investigates opportunities for the integration of legacy systems and existing communication protocols through distributed integrated services that correspond to established business processes. These integrated services link to basic services such as an exchange, a settlement, or a registry service. Examples of such integrated services include pre-trade services (e.g., analytics) or post-trade services (e.g., surveillance). The article then presents the various levels of integration in capital market systems and discusses the standards in place. It establishes that most interactions occur at a low level of abstraction such as the network (e.g., TCP/IP), data format (e.g., FIX, XML), and middleware levels (e.g., CORBA). Finally, the article\u00a0\u2026", "num_citations": "44\n", "authors": ["2067"]}
{"title": "A survey on expert recommendation in community question answering\n", "abstract": " Community question answering (CQA) represents the type of Web applications where people can exchange knowledge via asking and answering questions. One significant challenge of most real-world CQA systems is the lack of effective matching between questions and the potential good answerers, which adversely affects the efficient knowledge acquisition and circulation. On the one hand, a requester might experience many low-quality answers without receiving a quality response in a brief time; on the other hand, an answerer might face numerous new questions without being able to identify the questions of interest quickly. Under this situation, expert recommendation emerges as a promising technique to address the above issues. Instead of passively waiting for users to browse and find their questions of interest, an expert recommendation method raises the attention of users to the appropriate\u00a0\u2026", "num_citations": "43\n", "authors": ["2067"]}
{"title": "Formal consistency verification between BPEL process and privacy policy\n", "abstract": " Despite the increased privacy concerns in the Internet, not much attention has been paid into enforcing privacy policies of organisations who collect and consume personal data using automatic means (eg, Web services). In this paper, we propose a graph-transformation based framework to check whether an internal business process (implemented using a standard Web service composition language such as BPEL) adheres to the organisation's privacy policies. The graph-based specification formalism combines the advantages of an intuitive visual framework with rigorous semantical foundation that allows consistency checking between a business process and privacy policy. The privacy consistency verification framework is defined by a set of rules to build the system state and sets of constraints (positive and negative) to specify the wanted and unwanted substates.", "num_citations": "43\n", "authors": ["2067"]}
{"title": "On automating basic data curation tasks\n", "abstract": " Big data analytics is firmly recognized as a strategic priority for modern enterprises. At the heart of big data analytics lies the data curation process, consists of tasks that transform raw data (unstructured, semi-structured and structured data sources) into curated data, ie contextualized data and knowledge that is maintained and made available for use by end-users and applications. To achieve this, the data curation process may involve techniques and algorithms for extracting, classifying, linking, merging, enriching, sampling, and the summarization of data and knowledge. To facilitate the data curation process and enhance the productivity of researchers and developers, we identify and implement a set of basic data curation APIs and make them available as services to researchers and developers to assist them in transforming their raw data into curated data. The curation APIs enable developers to easily add\u00a0\u2026", "num_citations": "42\n", "authors": ["2067"]}
{"title": "Opinion fraud detection via neural autoencoder decision forest\n", "abstract": " Online reviews play an important role in influencing buyers\u2019 daily purchase decisions. However, fake and meaningless reviews, which cannot reflect users\u2019 genuine purchase experience and opinions, widely exist on the Web and pose great challenges for users to make right choices. Therefore, it is desirable to build a fair model that evaluates the quality of products by distinguishing spamming reviews. We present an end-to-end trainable unified model to leverage the appealing properties from Autoencoder and random forest. A stochastic decision tree model is implemented to guide the global parameter learning process. Extensive experiments were conducted on a large Amazon review dataset. The proposed model consistently outperforms a series of compared methods.", "num_citations": "40\n", "authors": ["2067"]}
{"title": "Datasynapse: A social data curation foundry\n", "abstract": " Social data analytics have become a vital asset for organizations and governments. For example, over the last few years, governments started to extract knowledge and derive insights from vastly growing open data to personalize the advertisements in elections, improve government services, predict intelligence activities, as well as to improve national security and public health. A key challenge in analyzing social data is to transform the raw data generated by social actors into curated data, i.e., contextualized data and knowledge that is maintained and made available for use by end-users and applications. To address this challenge, we present the notion of knowledge lake, i.e., a contextualized Data Lake, to provide the foundation for big data analytics by automatically curating the raw social data and to prepare them for deriving insights. We present a social data curation foundry, namely DataSynapse, to\u00a0\u2026", "num_citations": "40\n", "authors": ["2067"]}
{"title": "WITS: an IoT-endowed computational framework for activity recognition in personalized smart homes\n", "abstract": " Over the past few years, activity recognition techniques have attracted unprecedented attentions. Along with the recent prevalence of pervasive e-Health in various applications such as smart homes, automatic activity recognition is being implemented increasingly for rehabilitation systems, chronic disease management, and monitoring the elderly for their personal well-being. In this paper, we present WITS, an end-to-end web-based in-home monitoring system for convenient and efficient care delivery. The system unifies the data- and knowledge-driven techniques to enable a real-time multi-level activity monitoring in a personalized smart home. The core components consist of a novel shared-structure dictionary learning approach combined with rule-based reasoning for continuous daily activity tracking and abnormal activities detection. WITS also exploits an Internet of Things middleware for the scalable\u00a0\u2026", "num_citations": "40\n", "authors": ["2067"]}
{"title": "Mashup recommendation by regularizing matrix factorization with API co-invocations\n", "abstract": " Mashups are a dominant approach for building data-centric applications, especially mobile applications, in recent years. Since mashups are predominantly based on public data sources and existing APIs, it requires no sophisticated programming knowledge of people to develop mashup applications. The recent prevalence of open APIs and open data sources in the Big Data era has provided new opportunities for mashup development, but at the same time increase the difficulty of selecting the right services for a given mashup task. The API recommendation for mashup differs from traditional service recommendation tasks in lacking the specific QoS information and formal semantic specification of the APIs, which limits the adoption of many existing methods. Although there are a significant number of service recommendation approaches, most of them focus on improving the recommendation accuracy and work\u00a0\u2026", "num_citations": "40\n", "authors": ["2067"]}
{"title": "Enabling the analysis of cross-cutting aspects in ad-hoc processes\n", "abstract": " Processes in case management applications are flexible, knowledge-intensive and people-driven, and often used as guides for workers in processing of artifacts. An important fact is the evolution of process artifacts over time as they are touched by different people in the context of a knowledge-intensive process. This highlights the need for tracking process artifacts in order to find out their history (artifact versioning) and also provenance (where they come from, and who touched and did what on them). We present a framework, simple abstractions and a language for analyzing cross-cutting aspects (in particular versioning and provenance) over process artifacts. We introduce two concepts of timed-folders to represent evolution of artifacts over time, and activity-paths to represent the process which led to artifacts. The introduced approaches have been implemented on top of FPSPARQL, Folder-Path enabled\u00a0\u2026", "num_citations": "40\n", "authors": ["2067"]}
{"title": "An overview of multidatabase systems: Past and present\n", "abstract": " Organizations all over the world rely on a wide variety of databases to conduct their everyday business. Large organizations use databases on a variety of platforms, including mainframes, workstations, and servers configured for a corporate intranet. Historically, the databases are usually designed from scratch if none is found to meet the organization's requirements. This has led to a proliferation of databases obeying different sets of requirements for modeling identical or similar world objects. In many instances, and because of a lack of an organized conglomeration of databases, users typically create their own pieces of information that may be present in existing databases. As organizations have become more sophisticated, pressure to provide information sharing across dissimilar platforms has mounted. At the same time, advances in distributed computing and networking, combined with the affordable high level of connectivity, are helping to make information sharing across databases a reality. Databases are typically managed by different database management systems (DBMSs) running on heterogeneous computing platforms. The challenge is to give users the sense that they are accessing a single database that contains almost everything they need while preserving the integrity and investments of the preexisting environment [BS95]. In such systems, it is necessary not only to provide uniform access to all data and resources but also to allow databases to cooperate by exchanging data and synchronizing their execution seamlessly. This emerging need to provide organization-wide access to data and software resources is creating a demand\u00a0\u2026", "num_citations": "40\n", "authors": ["2067"]}
{"title": "Systematic approaches for designing B2B applications\n", "abstract": " The advent of the Internet has encouraged trading partners to collaborate electronically by leveraging the type of distributed applications usually referred to as business-to-business (B2B) applications. A review of the literature on B2B integration shows that designing B2B solutions is inherently complex because of the need to make integration design decisions at different levels of abstraction. For every decision, there may be several possible solutions offering different levels of quality. These alternatives may be offered in the form of integration approaches, patterns, models, technologies, standards, or protocols. In addition, integration design decisions and alternatives are often highly interdependent. Therefore, new approaches are needed to alleviate the design complexity of B2B applications. A framework is discussed that would help designers navigate design decisions and alternatives from early business\u00a0\u2026", "num_citations": "39\n", "authors": ["2067"]}
{"title": "Using mapreduce to scale events correlation discovery for business processes mining\n", "abstract": " In this paper, we present a scalable data analysis technique to support efficient event correlation for mining business processes. We propose a two-stages approach to compute correlation conditions and their entailed process instances from event logs using MapReduce framework. The experimental results show that the algorithm scales well to large datasets.", "num_citations": "38\n", "authors": ["2067"]}
{"title": "A collaborative approach for caching dynamic data in portal applications\n", "abstract": " Portals are one of the rapidly growing applications on the Web, providing a single interface to access different sources (providers). Providing fast response time is one of the critical issues in such applications. Dissatisfaction of users dramatically increases with increasing response time, resulting in abandonment of Web sites, which in turn could result in loss of revenue by businesses. In this paper we address the performance of such applications through caching techniques. We discuss the limitations of existing solutions and introduce a caching strategy based on collaboration between the portal and its providers. Providers trace their logs, extract information to identify good candidates for caching and notify the portal. Caching at the portal is mainly decided based on scores calculated by providers and associated with objects. We evaluate the performance of the collaborative caching strategy using simulation data. We also address the issue of heterogeneous scoring policies by different providers and introduce mechanisms to regulate caching scores.", "num_citations": "38\n", "authors": ["2067"]}
{"title": "Managing the web of things: linking the real world to the web\n", "abstract": " Managing the Web of Things: Linking the Real World to the Web presents a consolidated and holistic coverage of engineering, management, and analytics of the Internet of Things. The web has gone through many transformations, from traditional linking and sharing of computers and documents (ie, Web of Data), to the current connection of people (ie, Web of People), and to the emerging connection of billions of physical objects (ie, Web of Things). With increasing numbers of electronic devices and systems providing different services to people, Web of Things applications present numerous challenges to research institutions, companies, governments, international organizations, and others. This book compiles the newest developments and advances in the area of the Web of Things, ranging from modeling, searching, and data analytics, to software building, applications, and social impact. Its coverage will enable effective exploration, understanding, assessment, comparison, and the selection of WoT models, languages, techniques, platforms, and tools. Readers will gain an up-to-date understanding of the Web of Things systems that accelerates their research. Offers a comprehensive and systematic presentation of the methodologies, technologies, and applications that enable efficient and effective management of the Internet of Things Provides an in-depth analysis on the state-of-the-art Web of Things modeling and searching technologies, including how to collect, clean, and analyze data generated by the Web of Things Covers system design and software building principles, with discussions and explorations of social impact for the Web of\u00a0\u2026", "num_citations": "37\n", "authors": ["2067"]}
{"title": "Spreadsheet-based complex data transformation\n", "abstract": " Spreadsheets are used by millions of users as a routine all-purpose data management tool. It is now increasingly necessary for external applications and services to consume spreadsheet data. In this paper, we investigate the problem of transforming spreadsheet data to structured formats required by these applications and services. Unlike prior methods, we propose a novel approach in which transformation logic is embedded into a familiar and expressive spreadsheet-like formula mapping language. Popular transformation patterns provided by transformation languages and mapping tools, that are relevant to spreadsheet-based data transformation, are supported in the language via formulas. Consequently, the language avoids cluttering the source spreadsheets with transformations and turns out to be helpful when multiple schemas are targeted. We implemented a prototype and evaluated the benefits of our\u00a0\u2026", "num_citations": "35\n", "authors": ["2067"]}
{"title": "An analysis of spreadsheet-based services mashup\n", "abstract": " Spreadsheets, a popular productivity tool, has gained attention as a potential mashup development environment targeted towards end-users. In this paper, we present a general architecture of mashup tools for spreadsheets. We also present an analysis of the state-of-the art on spreadsheet-based mashup tools. The analysis result is used to guide our research in developing a lightweight semi-automatic mashup tool using spreadsheet paradigm.", "num_citations": "35\n", "authors": ["2067"]}
{"title": "Robust evaluation of products and reviewers in social rating systems\n", "abstract": " Social rating systems are widely used to harvest user feedback and to support making decisions by users on the Web. Web users may try to exploit such systems by posting unfair or false evaluations for fame or profit reasons. Detecting the real rating scores of products as well as the trustworthiness of reviewers is an important and a very challenging problem. Existing approaches use majority-based methods along with temporal analysis and clustering techniques to tackle this problem, but they are vulnerable to massive intelligent collaborative attacks. In this paper, we propose a set of novel algorithms for robust computation of product rating scores and reviewer trust ranks. We introduce a supporting framework consisting of three main components responsible for calculating a robust rating score for product, behavior analysis of reviewers and trust computation for reviewers. We propose a novel algorithm for\u00a0\u2026", "num_citations": "34\n", "authors": ["2067"]}
{"title": "A Review on Crowdsourcing for Education: State of the Art of Literature and Practice.\n", "abstract": " This paper systematically reviews the state of the art of the literature and practices of enhancing learning and teaching through crowdsourcing. We refer to these emerging phenomena as \u201ccrowdsourcing for education\u201d(CfE). Based on 51 relevant initiatives in practice, which we identified, we develop a definition and a taxonomy of CfE, and analysed CfE following the structure of what, who, why and how. We find that crowdsourcing has been used and benefits education in four ways: creating educational contents, providing practical experience, facilitating the exchange of complementary knowledge, and augmenting feedback. Eight principal motivations are reported to have led people to participate in CfE systems according to existing studies. This paper supports future research by developing a framework for CfE and providing a research agenda.", "num_citations": "31\n", "authors": ["2067"]}
{"title": "ProcessAtlas: A scalable and extensible platform for business process analytics\n", "abstract": " In today's knowledge\u2010, service\u2010, and cloud\u2010based economy, an overwhelming amount of business\u2010related data are being generated at a fast rate daily from a wide range of sources. These data increasingly show all the typical properties of big data: wide physical distribution, diversity of formats, nonstandard data models, and independently managed and heterogeneous semantics. In this context, there is a need for new scalable and process\u2010aware services for querying, exploration, and analysis of process data in the enterprise because (1) process data analysis services should be capable of processing and querying large amount of data effectively and efficiently and, therefore, have to be able to scale well with the infrastructure's scale and (2) the querying services need to enable users to express their data analysis and querying needs using process\u2010aware abstractions rather than other lower\u2010level abstractions\u00a0\u2026", "num_citations": "31\n", "authors": ["2067"]}
{"title": "Event correlation analytics: scaling process mining using mapreduce-aware event correlation discovery techniques\n", "abstract": " This paper introduces a scalable process event analysis approach, including parallel algorithms, to support efficient event correlation for big process data. It proposes a two-stages approach for finding potential event relationships, and their verification over big event datasets using MapReduce framework. We report on the experimental results, which show the scalability of the proposed methods, and also on the comparative analysis of the approach with traditional non-parallel approaches in terms of time and cost complexity.", "num_citations": "31\n", "authors": ["2067"]}
{"title": "Web service implementation and composition techniques\n", "abstract": " Web-accessible services, referred to as Web services, are an integral part of modern information technology, from mobile devices to cloud-and crowd-computing. The Internet of Things (IoT), Big Data, Web 2.0, and social networks all rely on Web-based interfaces to allow connectivity over distributed components, thereby enabling us to deliver innovative and disruptive solutions in every industry in the global market. Long gone are the days when developers had to code each specific service\u2014which often was a highly tedious, manual, and time-consuming task. Instead, Web services are driving the rapid creation of software. Today, with a few lines of code, you can tap into some remarkable resources, whether it is a payment network like MasterCard, a mapping service like ESRI or the machine learning engine that powers IBM\u2019s Watson [31]. Web services are created by businesses to empower businesses. This has\u00a0\u2026", "num_citations": "30\n", "authors": ["2067"]}
{"title": "Representation and querying of unfair evaluations in social rating systems\n", "abstract": " Social rating systems are subject to unfair evaluations. Users may try to individually or collaboratively promote or demote a product. Detecting unfair evaluations, mainly massive collusive attacks as well as honest looking intelligent attacks, is still a real challenge for collusion detection systems. In this paper, we study the impact of unfair evaluations in online rating systems. First, we study the individual unfair evaluations and their impact on the reputation of people calculated by social rating systems. We then propose a method for detecting collaborative unfair evaluations, also known as collusion. The proposed model uses frequent itemset mining technique to detect the candidate collusion groups and sub-groups. We use several indicators to identify collusion groups and to estimate how destructive such colluding groups can be. The approaches presented in this paper have been implemented in prototype tools, and\u00a0\u2026", "num_citations": "29\n", "authors": ["2067"]}
{"title": "Conceptual modeling of privacy-aware web service protocols\n", "abstract": " Internet users are becoming increasingly concerned about their personal information being collected and used by Web service providers. Since the privacy policies are mainly developed and maintained separately from the business process that collects and manipulates data, it is hard to perform analysis and management of the processes in terms of privacy policies. We propose a formal technique with which Web service providers describe the use and storage of requesters\u2019 personal data. The description is integrated with a Web service protocol using an extended state machine model. Having such a conceptual model will enable model-driven development and management of Web service protocols with respect to their privacy aspects such as collection, disclosure, and obligation.", "num_citations": "29\n", "authors": ["2067"]}
{"title": "Dynamic restructuring of e-catalog communities based on user interaction patterns\n", "abstract": " Since e-catalogs are dynamic, autonomous, and heterogeneous, the integration of a potentially large number of dynamic e-catalogs is a delicate and time-consuming task. In this paper, we describe the design and the implementation of a system through which existing on-line product catalogs can be integrated, and the resulting integrated catalogs can be continuously adapted and personalized within a dynamic environment. The integration framework originates from a previous project on integration of Web data, called WebFINDIT. Using the framework, we propose a methodology for adaptation of integrated catalogs based on the observation of customers' interaction patterns.", "num_citations": "29\n", "authors": ["2067"]}
{"title": "Ontological approach for information discovery in internet databases\n", "abstract": " The Internet has solved the age-old problem of network connectivity and thus enabling the potential access to, and data sharing among large numbers of databases. However, enabling users to discover useful information requires an adequate metadata infrastructure that must scale with the diversity and dynamism of both users' interests and Internet accessible databases. In this paper, we present a model that partitions the information space into a distributed, highly specialized domain ontologies. We also introduce inter-ontology relationships to cater for user-based interests across ontologies defined over Internet databases. We also describe an architecture that implements these two fundamental constructs over Internet databases. The aim of the proposed model and architecture is to eventually facilitate data discovery and sharing for Internet databases.", "num_citations": "29\n", "authors": ["2067"]}
{"title": "Adversarial collaborative neural network for robust recommendation\n", "abstract": " Most of recent neural network (NN)-based recommendation techniques mainly focus on improving the overall performance, such as hit ratio for top-N recommendation, where the users' feedbacks are considered as the ground-truth. In real-world applications, those feedbacks are possibly contaminated by imperfect user behaviours, posing challenges on the design of robust recommendation methods. Some methods apply man-made noises on the input data to train the networks more effectively (eg the collaborative denoising auto-encoder). In this work, we propose a general adversarial training framework for NN-based recommendation models, improving both the model robustness and the overall performance. We apply our approach on the collaborative auto-encoder model, and show that the combination of adversarial training and NN-based models outperforms highly competitive state-of-the-art\u00a0\u2026", "num_citations": "27\n", "authors": ["2067"]}
{"title": "Big data and cross-document coreference resolution: Current state and future opportunities\n", "abstract": " Information Extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. Among various IE tasks, extracting actionable intelligence from ever-increasing amount of data depends critically upon Cross-Document Coreference Resolution (CDCR) - the task of identifying entity mentions across multiple documents that refer to the same underlying entity. Recently, document datasets of the order of peta-/tera-bytes has raised many challenges for performing effective CDCR such as scaling to large numbers of mentions and limited representational power. The problem of analysing such datasets is called \"big data\". The aim of this paper is to provide readers with an understanding of the central concepts, subtasks, and the current state-of-the-art in CDCR process. We provide assessment of existing tools/techniques for CDCR subtasks and highlight big data challenges in each of them to help readers identify important and outstanding issues for further investigation. Finally, we provide concluding remarks and discuss possible directions for future work.", "num_citations": "27\n", "authors": ["2067"]}
{"title": "Programming cloud resource orchestration framework: operations and research challenges\n", "abstract": " The emergence of cloud computing over the past five years is potentially one of the breakthrough advances in the history of computing. It delivers hardware and software resources as virtualization-enabled services and in which administrators are free from the burden of worrying about the low level implementation or system administration details. Although cloud computing offers considerable opportunities for the users (e.g. application developers, governments, new startups, administrators, consultants, scientists, business analyst, etc.) such as no up-front investment, lowering operating cost, and infinite scalability, it has many unique research challenges that need to be carefully addressed in the future. In this paper, we present a survey on key cloud computing concepts, resource abstractions, and programming operations for orchestrating resources and associated research challenges, wherever applicable.", "num_citations": "27\n", "authors": ["2067"]}
{"title": "Toward self-organizing service communities\n", "abstract": " This paper discusses a framework in which catalog service communities are built, linked for interaction, and constantly monitored and adapted over time. A catalog service community (represented as a peer node in a peer-to-peer network) in our system can be viewed as domain specific data integration mediators representing the domain knowledge and the registry information. The query routing among communities is performed to identify a set of data sources that are relevant to answering a given query. The system monitors the interactions between the communities to discover patterns that may lead to restructuring of the network (e.g., irrelevant peers removed, new relationships created, etc.).", "num_citations": "27\n", "authors": ["2067"]}
{"title": "Introduction to the special issue on m-services\n", "abstract": " The GROWTH of Internet technologies has enabled a wave of innovations that are having an important impact on the way businesses deal with their partners as well as their customers. To remain competitive, traditional businesses need to take advantage of the information revolution that the Internet and Web have both brought about. Most businesses are moving their operations to the Web for more automation, efficient business processes, personalization, and for more global visibility.", "num_citations": "27\n", "authors": ["2067"]}
{"title": "Agflow: Agent-based cross-enterprise workflow management system\n", "abstract": " AgFlow: Agent-based Cross-Enterprise Workflow Management System Page 1 !#\"%$'&( ) wsedwf evqd g seh( vjil kmsi pg nobe xw pq b sh hqpwbeswxutvsi wxudx y7zt{U|q eve}u riP}v p dwdwv R}# |qsi w`cb' 5vq| pwdgsi sife` kv}up bedwfi evDbe |qsi w`cbg}u|qse 7 ja v e m f ( e f m ! \" f $#% & '# $# f d ( ) f 0# 1#f '23 d $#4 #f &6 f 7 ! 8 d @9BADC5Eg ( f ! e F # d ' G 3 d H P f # Q# R d f # ! & f ! S2( e G x d T9j U AB f ! UC1 f 7 5 E4 f V # 7W Xj 9 t 8r \" Y`23 G 8g $ab e f c d e d e c d 5V # t fW geh rtsvudwvxDy TiDwv \"r rtwvp 6dBf'g hqi 7 tjg k lnmSoD (p rqts1m sa 3mvu w tjg j )x Iy z yd U{! yd | }~ f tjg j | | ` f | | Page 2 ) tm g tjg | Hjg m Iy ! | q | Dh f 7 Hx % q P 7 7 } tjg q P q } w # Car Rental logging ECA Rule Base ECA Rule Engine Log Database logging logging Process Agent Workflow Flight Booking Discovery Agent Meta-data Repository \u2026", "num_citations": "27\n", "authors": ["2067"]}
{"title": "A unified framework for supporting dynamic schema evolution in object databases\n", "abstract": " This paper addresses the design of an integrated framework for managing schema evolution. This framework is based on the adaptation and extension of two main schema evolution approaches, namely schema modification and schema versioning. The proposed framework provides an integrated environment to support different database evolution techniques (such as, modification and versions at the schema level, conversion, object versioning, and screening at the instance level). We introduce the concept of class/schema version pertinence enabling the database administrator to judge the pertinence of versions with regard to database applications. Finally, we propose a declarative language based on OQL, the ODMG query language, that the user can use to guide objects adaptation process when dealing with complex or application specific schema updates.", "num_citations": "27\n", "authors": ["2067"]}
{"title": "WebBIS: An infrastructure for agile integration of web services\n", "abstract": " The Web is changing the way organizations are conducting their business. Businesses are rushing to provide modular applications, called Web services, that can be programmatically accessed through the Web. Despite the tremendous developments achieved so far, one of the most important, yet untapped potential, is the use of Web services as facilitators for inter-organizational cooperation. This promising concept, known as Web service composition, is gaining momentum as the potential silver bullet for the envisioned Semantic Web. The development of such integrated services has so far been ad hoc, time-consuming, and requires extensive low-level programming efforts. In this paper, we present WebBIS (Web Base of Internet-accessible Services), a generic framework for composing and managing Web services. We combine the object-oriented and active rules paradigms for such a task. We also provide a\u00a0\u2026", "num_citations": "25\n", "authors": ["2067"]}
{"title": "Galaxy: a platform for explorative analysis of open data sources\n", "abstract": " A large volume of Open Data is being generated on a continuous basis. Examples of this are the case of social, natural, and information systems such as World Wide Web and social networks. Most entities and objects in the Open Data are interconnected, forming a complex, semi-structured, and information-rich networks. In this sense, Linked Open Data has the potential to be similar to a federated database. Since Linked Open Data is based on W3C standards, it is possible to implement a federation infrastructure, however, the current SPARQL standard makes it challenging to analyze the Open Data in an explorative manner. Consequently, it will be hard to discover the hidden knowledge in the relationships among entities in Open Data sources. In this paper, we present Galaxy, a platform for explorative analysis of Open Data Sources. Galaxy facilitates the analysis of Open Data graphs based on simple abstractions, ie folders and paths, which enable an analyst to group related entities in the graph or find paths among entities. Galaxy uses Hadoop data processing platforms to store and retrieve large numbers of RDF triples and to support cost-effective and Web-scale processing of Semantic Web data through a Folder-Path enabled extension of SPARQL.", "num_citations": "24\n", "authors": ["2067"]}
{"title": "iSheets: a spreadsheet-based machine learning development platform for data-driven process analytics\n", "abstract": " In the era of big data, the quality of services any organization provides largely depends on the quality of their data-driven processes. In this context, the goal of process data science, is to enable innovative forms of information processing that enable enhanced insight and decision making. For example, consider the data-driven and knowledge-intensive processes in Australian government\u2019s office of the e-Safety commissioner, where the goal is to empowering all citizens to have safer, more positive experiences online. An example process, is to analyze the large amount of data generated every second on social networks to understand patterns of suicidal thoughts, online bullying and criminal/exterimist behaviour. Current processes leverage machine learning systems, e.g., to perform automatic mental-health-disorders detection from social networks. This approach is challenging for knowledge workers (end\u00a0\u2026", "num_citations": "23\n", "authors": ["2067"]}
{"title": "Crowdcorrect: A curation pipeline for social data cleansing and curation\n", "abstract": " Process and data are equally important for business process management. Data-driven approaches in process analytics aims to value decisions that can be backed up with verifiable private and open data. Over the last few years, data-driven analysis of how knowledge workers and customers interact in social contexts, often with data obtained from social networking services such as Twitter and Facebook, have become a vital asset for organizations. For example, governments started to extract knowledge and derive insights from vastly growing open data to improve their services. A key challenge in analyzing social data is to understand the raw data generated by social actors and prepare it for analytic tasks. In this context, it is important to transform the raw data into a contextualized data and knowledge. This task, known as data curation, involves identifying relevant data sources, extracting data and\u00a0\u2026", "num_citations": "23\n", "authors": ["2067"]}
{"title": "Temporal provenance model (TPM): model and query language\n", "abstract": " Provenance refers to the documentation of an object's lifecycle. This documentation (often represented as a graph) should include all the information necessary to reproduce a certain piece of data or the process that led to it. In a dynamic world, as data changes, it is important to be able to get a piece of data as it was, and its provenance graph, at a certain point in time. Supporting time-aware provenance querying is challenging and requires: (i) explicitly representing the time information in the provenance graphs, and (ii) providing abstractions and efficient mechanisms for time-aware querying of provenance graphs over an ever growing volume of data. The existing provenance models treat time as a second class citizen (i.e. as an optional annotation). This makes time-aware querying of provenance data inefficient and sometimes inaccessible. We introduce an extended provenance graph model to explicitly represent time as an additional dimension of provenance data. We also provide a query language, novel abstractions and efficient mechanisms to query and analyze timed provenance graphs. The main contributions of the paper include: (i) proposing a Temporal Provenance Model (TPM) as a timed provenance model; and (ii) introducing two concepts of timed folder, as a container of related set of objects and their provenance relationship over time, and timed paths, to represent the evolution of objects tracing information over time, for analyzing and querying TPM graphs. We have implemented the approach on top of FPSPARQL, a query engine for large graphs, and have evaluated for querying TPM models. The evaluation shows the viability\u00a0\u2026", "num_citations": "23\n", "authors": ["2067"]}
{"title": "User-centric services provisioning in wireless environments\n", "abstract": " contributed articles november 2008| vol. 51| no. 11| communications of the acm 131 provisioning, which takes into account the needs of mobile users. The foundation of our approach is to enable an effective access to integrated services by combining technologies such as Web services, multi-agent systems, and publish/subscribe systems. In this article, we review the design principles, the architecture, and the implementation of the prototype system. design PrinciplesLeveraging Web services and software agents in combination with publish/subscribe systems provides the foundation to enable effective access to integrated services in wireless environments. Web Services. Web services provide the pillars for evolving the Internet into a service-oriented integration platform of unprecedented scale and agility. The foundation of this platform lies in the modularization and virtualization of system functions and resources\u00a0\u2026", "num_citations": "23\n", "authors": ["2067"]}
{"title": "On demand business-to-business integration\n", "abstract": " We develop an agent-based cross-enterprise Workflow Management System (WFMS) which can integrate business processes on user\u2019s demand. In our approach, business processes are wrapped by service agents. Based on users\u2019 requirements, the integration agent contacts the discovery agent to locate appropriate service agents, then negotiates with the service agents about task executions. Ac ost model is proposed which allows the integration agent to update execution plan and integrate service agents dynamically.", "num_citations": "22\n", "authors": ["2067"]}
{"title": "Truth discovery via exploiting implications from multi-source data\n", "abstract": " Data veracity is a grand challenge for various tasks on the Web. Since the web data sources are inherently unreliable and may provide conflicting information about the same real-world entities, truth discovery is emerging as a countermeasure of resolving the conflicts by discovering the truth, which conforms to the reality, from the multi-source data. A major challenge related to truth discovery is that different data items may have varying numbers of true values (or multi-truth), which counters the assumption of existing truth discovery methods that each data item should have exactly one true value. In this paper, we address this challenge by exploiting and leveraging the implications from multi-source data. In particular, we exploit three types of implications, namely the implicit negative claims, the distribution of positive/negative claims, and the co-occurrence of values in sources' claims, to facilitate multi-truth discovery\u00a0\u2026", "num_citations": "21\n", "authors": ["2067"]}
{"title": "Message correlation and business protocol discovery in service interaction logs\n", "abstract": " The problem of discovering protocols and business processes based on the analysis of log files is a real challenge. The behavior of a Web service can be specified using a Business Protocol, hence the importance of this discovery. The construction of the Business Protocol begins by correlating the logged messages into their conversations (i.e. instances of the business protocol). The accomplishment of this task is easy if we assume that the logs contain the right identifiers, which would allow us to associate every message to a conversation. But in real-world situations, this kind of information rarely exists inside the log files.               Our work consists in correlating the messages present in Web service logs into the conversations they belong to, and then generating automatically the Business Protocol that reflects the messaging behavior perceived in the log. Contrary to other approaches, we do not assume\u00a0\u2026", "num_citations": "21\n", "authors": ["2067"]}
{"title": "Selection of Web Services for Composition Using Location of Provider Hosts Criterion.\n", "abstract": " We present a Web service composition approach that relies on three selection criteria: execution cost, execution time, and location of provider hosts. A Web service is an accessible application that can be automatically discovered and invoked by other applications and humans. Web services can be composed into high level business-processes that users trigger in order to satisfy their needs. Because providers can have Web services in common, criteria are needed to select which Web services will be considered for composition. Location of provider hosts is among these criteria and aims for example at reducing the number of remote interactions between provider hosts.", "num_citations": "21\n", "authors": ["2067"]}
{"title": "istory: Intelligent storytelling with social data\n", "abstract": " The production of knowledge from ever increasing amount of social data is seen by many organizations as an increasingly important capability that can complement the traditional analytics sources. Examples include extracting knowledge and deriving insights from social data to improve government services, predict intelligence activities, personalize the advertisements in elections and improve national security and public health. Understanding social data can be challenging as the analysis goal can be subjective. In this context, storytelling is considered as an appropriate metaphor as it facilitates understanding and surfacing insights which is embedded within the data. In this paper, we focus on the research problem of \u2018understanding the social data\u2019in general and more particularly the curation, summarization and presentation of large amounts of social data. The goal is to enable intelligent narrative construction\u00a0\u2026", "num_citations": "20\n", "authors": ["2067"]}
{"title": "Intelligent knowledge lakes: the age of artificial intelligence and big data\n", "abstract": " The continuous improvement in connectivity, storage and data processing capabilities allow access to a data deluge from the big data generated on open, private, social and IoT (Internet of Things) data islands. Data Lakes introduced as a storage repository to organize this raw data in its native format until it is needed. The rationale behind a Data Lake is to store raw data and let the data analyst decide how to curate them later. Previously, we introduced the novel notion of Knowledge Lake, i.e., a contextualized Data Lake, and proposed algorithms to turn the raw data (stored in Data Lakes) into contextualized data and knowledge using extraction, enrichment, annotation, linking and summarization techniques. In this tutorial, we introduce Intelligent Knowledge Lakes to facilitate linking Artificial Intelligence (AI) and Data Analytics. This will enable AI applications to learn from contextualized data and use them\u00a0\u2026", "num_citations": "20\n", "authors": ["2067"]}
{"title": "Business process analytics and big data systems: A roadmap to bridge the gap\n", "abstract": " Business processes represent a cornerstone to the operation of any enterprise. They are the operational means for such organizations to fulfill their goals. Nowadays, enterprises are able to gather massive amounts of event data. These are generated as business processes are executed and stored in transaction logs, databases, e-mail correspondences, free form text on (enterprise) social media, and so on. Taping into these data, enterprises would like to weave data analytic techniques into their decision making capabilities. In recent years, the IT industry has witnessed significant advancements in the domain of Big Data analytics. Unfortunately, the business process management (BPM) community has not kept up to speed with such developments and often rely merely on traditional modeling-based approaches. New ways of effectively exploiting such data are not sufficiently used. In this paper, we advocate that a\u00a0\u2026", "num_citations": "20\n", "authors": ["2067"]}
{"title": "Dual: A deep unified attention model with latent relation representations for fake news detection\n", "abstract": " The prevalence of online social media has enabled news to spread wider and faster than traditional publication channels. The easiness of creating and spreading the news, however, has also facilitated the massive generation and dissemination of fake news. It, therefore, becomes especially important to detect fake news so as to minimize its adverse impact such as misleading people. Despite active efforts to address this issue, most existing works focus on mining news\u2019 content or context information from individuals but neglect the use of clues from multiple resources. In this paper, we consider clues from both news\u2019 content and side information and propose a hybrid attention model to leverage these clues. In particular, we use an attention-based bi-directional Gated Recurrent Units (GRU) to extract features from news content and a deep model to extract hidden representations of the side information. We\u00a0\u2026", "num_citations": "20\n", "authors": ["2067"]}
{"title": "A case study in developing web services for capital markets\n", "abstract": " The area of finance has always evolved along side with the development of new technology. For instance, utilizing new technologies in capital markets trading automation is one of the major factors for the market efficiency and competitiveness as time has a huge impact on the costs incurred by financial institutions. Considering capital markets as our case study, we investigate the usability of these technologies in implementing business processes that span across a number of legacy applications. We describe Web services as emerging technologies that facilitate the composition and execution of distributed business processes. We also present an overview of a service-oriented architecture for capital market systems (CMSs). This architecture is meant to integrate existing legacy applications and facilitate the automation of trading-related business processes.", "num_citations": "20\n", "authors": ["2067"]}
{"title": "Adversarial collaborative auto-encoder for top-n recommendation\n", "abstract": " Recently, deep learning-based recommendation models have been proved to have state-of-the-art recommendation accuracy. However, most of the existing work assume that user feedbacks are noise-free, on which the neural networks (NN) are trained. Although some methods apply man-made noises on the input data to train the networks more effectively (e.g. the collaborative denoising auto-encoder), the noises are randomly generated. To gain further improvements, we focus on boosting the overall recommendation performance through adversarial noises. We propose a general framework to adversarially train a NN-based item recommendation model. In particular, we select the collaborative auto-encoder model as an example and test our method on three public datasets. We show that our approach enhances both overall robustness and performance which outperforms competitive state-of-the-art item\u00a0\u2026", "num_citations": "19\n", "authors": ["2067"]}
{"title": "Expert as a service: Software expert recommendation via knowledge domain embeddings in stack overflow\n", "abstract": " Question answering (Q&A) communities have gained momentum recently as an effective means of knowledge sharing over the crowds, where many users are experts in the real-world and can make quality contributions in certain domains or technologies. Although the massive user-generated Q&A data present a valuable source of human knowledge, a related challenging issue is how to find those expert users effectively. In this paper, we propose a framework for finding such experts in a collaborative network. Accredited with recent works on distributed word representations, we are able to summarize text chunks from the semantics perspective and infer knowledge domains by clustering pre-trained word vectors. In particular, we exploit a graph-based clustering method for knowledge domain extraction and discern the shared latent factors using matrix factorization techniques. The proposed clustering method\u00a0\u2026", "num_citations": "19\n", "authors": ["2067"]}
{"title": "Time sequence summarization to scale up chronology-dependent applications\n", "abstract": " In this paper, we present the concept of Time Sequence Summarization to support chronology-dependent applications on massive data sources. Time sequence summarization takes as input a time sequence of events that are chronologically ordered. Each event is described by a set of descriptors. Time sequence summarization produces a concise time sequence that can be substituted for the original time sequence in chronology-dependent applications. We propose an algorithm that achieves time sequence summarization based on a generalization, grouping and concept formation process. Generalization expresses event descriptors at higher levels of abstraction using taxonomies while grouping gathers similar events. Concept formation is responsible for reducing the size of the input time sequence of events by representing each group created by one concept. The process is performed in a way such that the\u00a0\u2026", "num_citations": "19\n", "authors": ["2067"]}
{"title": "On-board RSVP: An extension of RSVP to support real-time services in on-board IP networks\n", "abstract": " The extension of Internet services to public transport passengers is slowly becoming inevitable. To this end, it is envisaged that high-speed local area networks will be deployed on-board public transport vehicles (e.g., buses, trains, ships and planes). The on-board LAN will be connected to the Internet via an on-board mobile router (MR). The passengers simply connect their devices to the on-board LAN and start enjoying Internet services. The mobility of the entire on-board network including the passenger devices is managed transparently by the MR. However, the mobility of the router (and the entire IP subnet) gives rise to several unique challenges for achieving end-to-end resource reservation. In this paper we propose a novel extension for RSVP, which addresses these issues. The proposed On-Board RSVP protocol can effectively, transparently, and scalably support end-to-end resource reservation in\u00a0\u2026", "num_citations": "19\n", "authors": ["2067"]}
{"title": "Extending web services technologies: the use of multi-agent approaches\n", "abstract": " Extending Web Services Technologies addresses the rapidly growing impact of Multi-Agent Systems on web services tools and techniques. In particular, the book addresses the potential for MAS techniques to impact the difficult challenges that must be tackled for web services technology to realize its promises. The area of web services offers the multi-agent community exciting research possibilities, including similarities in system architectures, powerful tools, and a focus on issues such as trust and reliability. Likewise, techniques developed in the multi-agent research community promise to have a strong impact on this fast growing technology. The contents contain contributions by leading international researchers and professionals from both the web services and Multi-Agent Systems community. Topics include semantic web services and associated standards, architectures integrating agents and services, transactions, authorization, and service composition.", "num_citations": "19\n", "authors": ["2067"]}
{"title": "Self-adapting cloud services orchestration for fulfilling intensive sensory data-driven IoT workflows\n", "abstract": " Cloud computing has been adopted to support among others the storage and processing of complex Internet of Things (IoT) workflows handling sensory streamed time-series data. IoT workflow is often composed following a set of procedures which makes it hard to self-adapt, self-configure to react to runtime environment changes. Therefore, declarative data-driven workflow composition will provision self-learning and self-configurable workflows such as those of IoT. This paper proposes a comprehensive architecture to support end-to-end workflow management processes including declarative specification and composition, configuration deployment, orchestration, execution, adaptation, and quality enforcement. The later provision runtime intelligence for IoT workflow orchestration; this is achieved through the automated monitoring and analysis of runtime cloud resource orchestration, the monitoring of workflows\u00a0\u2026", "num_citations": "17\n", "authors": ["2067"]}
{"title": "Detecting, representing and querying collusion in online rating systems\n", "abstract": " Online rating systems are subject to malicious behaviors mainly by posting unfair rating scores. Users may try to individually or collaboratively promote or demote a product. Collaborating unfair rating 'collusion' is more damaging than individual unfair rating. Although collusion detection in general has been widely studied, identifying collusion groups in online rating systems is less studied and needs more investigation. In this paper, we study impact of collusion in online rating systems and asses their susceptibility to collusion attacks. The proposed model uses a frequent itemset mining algorithm to detect candidate collusion groups. Then, several indicators are used for identifying collusion groups and for estimating how damaging such colluding groups might be. Also, we propose an algorithm for finding possible collusive subgroup inside larger groups which are not identified as collusive. The model has been implemented and we present results of experimental evaluation of our methodology.", "num_citations": "17\n", "authors": ["2067"]}
{"title": "An incremental knowledge acquisition method for improving duplicate invoices detection\n", "abstract": " Duplicate records are a major problem and duplicate invoices are a specific example of this. The detection of duplicate invoices is a critical issue for business since duplicate invoices can result in a company paying more than once for goods or services ordered. Past experience has shown that generic duplicate record detection techniques are not very useful when applied to invoices: the rate of false positives can be so high that invoice clerks are discouraged from using the system. This is because such approaches do not take the business context into account, e.g. what types of good were ordered as well as the past relationship with that vendor. In this paper, we discuss applying ripple down rules (RDR), an approach for incremental and end-user-centred knowledge acquisition, to the problem of classifying pairs of potential duplicate invoices. We describe how we built a prototype on top of the SAP ERP product\u00a0\u2026", "num_citations": "17\n", "authors": ["2067"]}
{"title": "Conceptmap: A conceptual approach for formulating user preferences in large information spaces\n", "abstract": " In a large information space a user needs to iteratively investigate the data to formulate her preferences for IR systems. In recent years several visualization techniques have been proposed to help a user to better formulate her preferences. However, using these solutions a user needs to explicitly specify her preferences for IR systems in forms of keywords or phrases. In this paper we present ConceptMap, a system that takes the advantage of deep learning and a knowledge lake to provide a conceptual summary of the information space. ConceptMap allows a user to specify her preferences implicitly as a set of concepts without the need to iteratively investigate the information space. It provides a 2D Radial Map of concepts where a user can rank items relevant to her preferences through dragging and dropping. Our experiment results shows that ConceptMap can help users to better formulate their preferences\u00a0\u2026", "num_citations": "16\n", "authors": ["2067"]}
{"title": "Expert2vec: Experts representation in community question answering for question routing\n", "abstract": " Communities of Question Answering (CQAs) are rapidly growing communities for exchanging information in the form of questions and answers. They rely on the contributions of users (i.e., members of the community) who have appropriate domain knowledge and can provide helpful answers. In order to deliver the most appropriate and valuable answers, identification of such users (experts) is critically important. However, a common problem faced in CQAs is that of poor expertise matching, i.e., routing of questions to inappropriate users. In this paper, we focus on Stack Overflow (a programming CQA) and address this problem by proposing an embedding based approach that integrates users\u2019 textual content obtained from the community (e.g., answers) and community feedback in a unified framework. Our embedding-based approach is used to find the best relevant users for a given question by computing\u00a0\u2026", "num_citations": "16\n", "authors": ["2067"]}
{"title": "Semantic service mediation\n", "abstract": " The service mediation that decouples service interactions is a key component in supporting the implementation of SOA solutions cross enterprises. The decoupling is achieved by having the consumers and providers to interact via an intermediary. The earliest service mediations are keyword and value-based, which require both service providers and consumers to adhere same data formats in defining service interfaces and requests. This requirement makes it inadequate for supporting interactions among services in heterogeneous and dynamic environments. In order to overcome this limitation, semantics are introduced into service mediations, for more flexible service matchings. In this paper, we proposed a novel semantic service mediation. Different from existing semantic service mediations, our system uses ontologies not only for one-to-one service matchings, but also for one-to-multiple service\u00a0\u2026", "num_citations": "16\n", "authors": ["2067"]}
{"title": "Servicemosaic project: modeling, analysis and management of web services interactions\n", "abstract": " This paper provides an overview of ServiceMosaic, which is a platform for model-driven analysis and management of service interactions. In particular, in this paper, we focus on business protocols modelling and analysis by providing operators for compatibility and replaceability checking of business protocols, and model-driven adapter development for business protocols..", "num_citations": "16\n", "authors": ["2067"]}
{"title": "A three-level specification approach for an environment of software agents and Web services\n", "abstract": " This paper presents an approach for the specification of a software agent-based and Web service-oriented environment. A software agent is an autonomous entity that acts on user\u2019s behalf. Whereas a Web service is an accessible application that other applications and humans can discover and trigger. Users in collaboration with their agents compose Web services into high-level business processes denoted by composite services. The participation of Web services in a composite service is based on several selection criteria such as the execution cost of a Web service and the location of the resources on which a Web service will be performed. Prior to that selection, the specification approach puts forwards three levels: intrinsic, organizational/functional, and behavior. Besides the specification approach, the composition of Web services is illustrated in this paper with service chart diagrams.", "num_citations": "16\n", "authors": ["2067"]}
{"title": "Towards a composition framework for e-/m-services\n", "abstract": " We present a framework that enables the composition of services for the benefit of users. Two types of services exist: E-services and M-services. Moreover, two types of users exist: static and mobile. The composition framework, software agents and workflows are used.", "num_citations": "16\n", "authors": ["2067"]}
{"title": "WEBFINDIT: An architecture and system for querying web databases\n", "abstract": " The Web offers a single user interface to data sharing across heterogeneous and autonomous databases, but it was not designed to handle the rigid DBMS protocols and data formats used by relational and object-oriented databases. WebFindIt is an ongoing project to develop the database equivalent of the World Wide Web-namely, a World Wide Database-through a middleware infrastructure for describing, locating, and accessing data from any kind of Web-accessible database. A special-purpose language, Web-Tassili, supports the definition and manipulation of middleware constructs for organizing the information space. An implementation of WebFindIt combines Java, CORBA and database technologies.", "num_citations": "16\n", "authors": ["2067"]}
{"title": "Industry 4.0, how to integrate legacy devices: a cloud IoT approach\n", "abstract": " The Industry 4.0 approach purpose the massive transformation of the traditional plants into smart factories, however the current industrial shop floor, is composed of a large number of dumb devices (legacy devices). This work describes an approach for integrating legacy devices with a cloud-based IoT platform. This proposal uses the following steps: Virtualize the real equipment, connect the virtual equipment to the cloud, make all the necessary tests with the virtual equipment, and after testing it connects the real equipment with the cloud. We opted for the initial use of a virtual equipment (virtual Modular Production System-CIROS\u00ae), as this creates a totally free and secure development and a testing environment without having to stop the real equipment in the shop floor for testing. In this approach were used the same tools that the companies use, such as OPC UA, UaGateway, CODESYS\u00ae, and the integration was\u00a0\u2026", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Calling for response: Automatically distinguishing situation-aware tweets during crises\n", "abstract": " Recent years have witnessed the prevalence and use of social media during crises, such as Twitter, which has been becoming a valuable information source for offering better responses to crisis and emergency situations by the authorities. However, the sheer amount of information of tweets can\u2019t be directly used. In such context, distinguishing the most important and informative tweets is crucial to enhance emergency situation awareness. In this paper, we design a convolutional neural network based model to automatically detect crisis-related tweets. We explore the twitter-specific linguistic, sentimental and emotional analysis along with statistical topic modeling to identify a set of quality features. We then incorporate them to into a convolutional neural network model to identify crisis-related tweets. Experiments on real-world Twitter dataset demonstrate the effectiveness of our proposed model.", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Detecting cloud (anti) patterns: OCCI perspective\n", "abstract": " Open Cloud Computing Interface (OCCI) follows a set of guidelines (i.e. best practices) to create interoperable APIs over Cloud resources. In this paper, we identify a set of patterns that must be followed and anti-patterns that should be avoided to comply with the OCCI guidelines. To automatically detect (anti)patterns, we propose a Semantic-based approach, relying on SWRL (Semantic Web Rule Language) rules and in SQWRL (Semantic Query-Enhanced Web Rule Language) queries to describe the (anti)patterns symptoms. An evaluation, conducted on real world Cloud service APIs, shows the feasibility of the proposed approach by assessing their compliance to OCCI standard.", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Scalable SaaS-Based Process Customization with CaseWalls\n", "abstract": " The rising popularity of SaaS allows individuals and enterprises to leverage various services (e.g. Dropbox, Github, GDrive and Yammer) for everyday processes. However, these disparate services do not in general communicate with each other, rather used in an ad-hoc manner with little or no customizable process support. This inevitably leads to \u201cshadow processes\u201d, often only informally managed by e-mail or the like. In this paper, we propose a framework to simplify the integration of disparate services and effectively build customized processes. The implementation of the proposed techniques includes an agile services integration platform, called: CaseWalls. We provide a knowledge-based event-bus for unified interactions between disparate services, while allowing process participants to interact and collaborate on relevant cases.", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Web service adaptation: Mismatch patterns and semi-automated approach to mismatch identification and adapter development\n", "abstract": " The rapid growth of online Web services has led to the proliferation of functionality-wise equivalent services with differences in their descriptions and behaviors, and therefore has given rise to the need for service adaptation. In this chapter, we discuss key challenges for Web service interoperability and adaptation. We present a consolidated framework including a methodology, methods and tools for identifying and tackling service adaptation challenges by characterizing service adaptation issues, their semi-automated identification and resolution for adapter development. The innovative contributions of the our work consist in (i) a taxonomy of common mismatches at the service interfaces and business protocols whose definitions and resolutions are captured in mismatch patterns, (ii) a business protocol-aware matching of service specifications, and (iii) methods and tools for instantiating mismatch patterns\u00a0\u2026", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Middleware technologies for b2b integration\n", "abstract": " Several middleware technologies and frameworks have emerged with the aim to support interoperability between distributed systems and therefore facilitating their integration process. Within the context of Business-to-Business (B2B) e-commerce, this paper reviews the major emerging technologies that support different levels of interoperability that facilitate rapid application development for e-businesses. These technologies are illustrated with a case study in global financial industry with a view to see the efficacy of existing middleware to solve the integration problems between systems within a specific domain.", "num_citations": "15\n", "authors": ["2067"]}
{"title": "Extending SPARQL to support entity grouping and path queries\n", "abstract": " The ability to efficiently find relevant subgraphs and paths in a large graph to a given query is important in many applications including scientific data analysis, social networks, and business intelligence. Currently, there is little support and no efficient approaches for expressing and executing such queries. This paper proposes a data model and a query language to address this problem. The contributions include supporting the construction and selection of: (i) folder nodes, representing a set of related entities, and (ii) path nodes, representing a set of paths in which a path is the transitive relationship of two or more entities in the graph. Folders and paths can be stored and used for future queries. We introduce FPSPARQL which is an extension of the SPARQL supporting folder and path nodes. We have implemented a query engine that supports FPSPARQL and the evaluation results shows its viability and efficiency for querying large graph datasets.", "num_citations": "14\n", "authors": ["2067"]}
{"title": "An analytic approach to people evaluation in crowdsourcing systems\n", "abstract": " Worker selection is a significant and challenging issue in crowdsourcing systems. Such selection is usually based on an assessment of the reputation of the individual workers participating in such systems. However, assessing the credibility and adequacy of such calculated reputation is a real challenge. In this paper, we propose an analytic model which leverages the values of the tasks completed, the credibility of the evaluators of the results of the tasks and time of evaluation of the results of these tasks in order to calculate an accurate and credible reputation rank of participating workers and fairness rank for evaluators. The model has been implemented and experimentally validated.", "num_citations": "14\n", "authors": ["2067"]}
{"title": "Data services in your spreadsheet!\n", "abstract": " End-user programmers---the 45 million of them, as estimated for 2001 in US alone [7]---routinely use spreadsheet to visualize, manipulate, and analyze data. Thanks to this environment, they can build applications that solve their daily problems. Even building a report can be seen as programming an application that takes corporate data as input and outputs a presentation. To build this application, spreadsheet users have to import data and place them in spreadsheet cells, highlight the important pieces, compute maybe some aggregates, add a chart or two. If well done, this application will be used each time data are updated to effortlessly produce a fresh report.", "num_citations": "14\n", "authors": ["2067"]}
{"title": "Towards a conversation-driven composition of web services\n", "abstract": " We outline the role and benefits that conversations could bring to Web services in general and their composition in particular. A Web service is an accessible application that other applications and humans as well can discover and trigger to satisfy multiple needs (eg, accommodation booking). While much of the work on Web services to date has focussed on low-level standards for publishing, discovering, and invoking Web services, it is deemed appropriate to start leveraging the Web services to the level of active components. These components would be able to engage in conversations, make decisions, and adjust their behavior according to the situations in which they participate. A conversation is a consistent exchange of messages between participants involved in joint operations and thus, have common interests.", "num_citations": "14\n", "authors": ["2067"]}
{"title": "Using Java and CORBA for implementing Internet databases\n", "abstract": " We describe an architecture called WebFINDIT that allows dynamic couplings of Web accessible databases based on their content and interest. We propose an implementation using WWW, Java, JDBC, and CORBA's ORBs that communicate via the CORBA's IIOP protocol. The combination of these technologies offers a compelling middleware infrastructure to implement fluid-area enterprise applications. In addition to a discussion of WebFINDIT's core concepts and implementation architecture, we also discuss an experience of exiting WebFINDIT in a healthcare application.", "num_citations": "14\n", "authors": ["2067"]}
{"title": "Rating prediction via generative convolutional neural networks based regression\n", "abstract": " Ratings are an essential criterion for evaluating the quality of movies and a critical indicator of whether a customer would watch a movie. Therefore, an important related research challenge is to predict the rating of a movie before it is released in cinema or even before it is produced. Many existing approaches fail to address this challenge because they predict movie ratings based on post-production factors such as review comments from social media. Consequently, they are generally inapplicable until a movie has been released for a certain period of time when a sufficient number of review comments have become available. In this paper, we propose a regression model based on generative convolutional neural networks for movie rating prediction. Instead of post-production factors widely used by previous work, this model learns from movies\u2019 intrinsic pillars such as genres, budget, cast, director and plot\u00a0\u2026", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Similarity-aware deep attentive model for clickbait detection\n", "abstract": " \u00a9 Springer Nature Switzerland AG 2019. Clickbait is a type of web content advertisements designed to entice readers into clicking accompanying links. Usually, such links will lead to articles that are either misleading or non-informative, making the detection of clickbait essential for our daily lives. Automated clickbait detection is a relatively new research topic. Most recent work handles the clickbait detection problem with deep learning approaches to extract features from the meta-data of content. However, little attention has been paid to the relationship between the misleading titles and the target content, which we found to be an important clue for enhancing clickbait detection. In this work, we propose a deep similarity-aware attentive model to capture and represent such similarities with better expressiveness. In particular, we present the ways of either using similarity only or integrating it with other available quality features for the clickbait detection. We evaluate our model on two benchmark datasets, and the experimental results demonstrate the effectiveness of our approach by outperforming a series of competitive state-of-the-arts and baseline methods.", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Empowering truth discovery with multi-truth prediction\n", "abstract": " Truth discovery is the problem of detecting true values from the conflicting data provided by multiple sources on the same data items. Since sources' reliability is unknown a priori, a truth discovery method usually estimates sources' reliability along with the truth discovery process. A major limitation of existing truth discovery methods is that they commonly assume exactly one true value on each data item and therefore cannot deal with the more general case that a data item may have multiple true values (or multi-truth). Since the number of true values may vary from data item to data item, this requires truth discovery methods being able to detect varying numbers of truth values from the multi-source data. In this paper, we propose a multi-truth discovery approach, which addresses the above challenges by providing a generic framework for enhancing existing truth discovery methods. In particular, we redeem the\u00a0\u2026", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Dimensions for evaluating cloud resource orchestration frameworks\n", "abstract": " Despite the proliferation of cloud resource orchestration frameworks (CROFs), DevOps managers and application developers still have no systematic tool for evaluating their features against desired criteria. The authors present generic technical dimensions for analyzing CROF capabilities and understanding prominent research to refine them.", "num_citations": "13\n", "authors": ["2067"]}
{"title": "ProcessBase: A Hybrid Process Management Platform\n", "abstract": " Traditional structured process-support systems increasingly prove too rigid amidst today\u2019s fast-paced and knowledge-intensive environments. Commonly described as \u201cunstructured\u201d or \u201csemi-structured\u201d processes, they cannot be pre-planned and likely to be dependent upon the interpretation of human-workers during process execution. On the other hand, there has been a plethora of Social and Web 2.0 services to support workers with enhanced collaboration, however these tools are often used ad-hoc with little or no customisable process support. In order to address these challenges, we thus present: \u201cProcessBase\u201d, an innovative Hybrid-Processes platform that holistically combines structured, semi-structured and unstructured activities. Our task-model proposed encapsulates a spectrum of process specificity, including: structured to ad-hoc Web-service tasks, automated rule-tasks, human-tasks as well\u00a0\u2026", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Ws-catalognet: An infrastructure for creating, peering, and querying e-catalog communities\n", "abstract": " E-catalog portals, such as Expedia. com and Amazon. com, are becoming more and more prominent feature of the Web. They aim to offer one-stop shopping experience for the users. However, the users still need to access a number of portals separately, or to use search engines in order to get complete information they are looking for. It is clearly useful to provide a unified interface to access multiple e-catalog portals. The issue here is that the technology to create, organise, integrate and search these portals has not kept pace with the rapid growth of the available information space. Most existing approaches for providing access to integrated e-catalogs as a portal are based on (i) creating centralised product data repository collected from participating e-catalog providers,(ii) statically linking manually (ad-hoc) identified ecatalogs to the portal. Surely, these are not scalable approaches. First, we cannot expect the integrators to understand underlying schemas of thousands of online e-catalogs and produce a single schema. Second, considering the dynamic nature of the Web, the underlying schema of any online e-catalog may change any time and change frequently, which has to be reflected to the central schema.", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Service oriented computing: Opportunities and challenges\n", "abstract": " Service oriented architectures (SOAs) are emerging as the technologies and architectures of choice for implementing distributed systems. Recent advances and standardization efforts in SOAs provide necessary building blocks for supporting the automated development and interoperability of services. Although, standardization is crucial by no means is sufficient. Wide spread adoption of service technologies requires high level framework and methodology and identification of appropriate abstractions and notations for specifying service requirements and characteristics to support automated development and interoperability. In this paper, we identify interoperability layers of SOAs, review major approaches for service development and highlight some research directions.", "num_citations": "13\n", "authors": ["2067"]}
{"title": "Hiword: A petri net-based hierarchical workflow designer\n", "abstract": " Much work is being conducted in the area of business process modeling using workflow technology. HiWorD is a hierarchical workflow modeling prototype with simulation capability. It models business processes using Petri nets in a hierarchical manner and implements recovery transitions as a technique to recover from exceptions. The workflow hierarchy is created by refining places and transitions using predefined patterns. By using these patterns, it is proven that the resulting workflow will be sound.", "num_citations": "13\n", "authors": ["2067"]}
{"title": "FISA: feature-based instance selection for imbalanced text classification\n", "abstract": " Support Vector Machines (SVM) classifiers are widely used in text classification tasks and these tasks often involve imbalanced training. In this paper, we specifically address the cases where negative training documents significantly outnumber the positive ones. A generic algorithm known as FISA (Feature-based Instance Selection Algorithm), is proposed to select only a subset of negative training documents for training a SVM classifier. With a smaller carefully selected training set, a SVM classifier can be more efficiently trained while delivering comparable or better classification accuracy. In our experiments on the 20-Newsgroups dataset, using only 35% negative training examples and 60% learning time, methods based on FISA delivered much better classification accuracy than those methods using all negative training documents.", "num_citations": "12\n", "authors": ["2067"]}
{"title": "Protocol discovery from imperfect service interaction data\n", "abstract": " This paper studies the problem of discovering protocol definitions from read-world service interaction data, which often are imperfect in various ways. It first describes the challenges in protocol discovery in such a context and the different aspects that must be considered by a protocol discovery solution. Next, it reports the current progress by presenting a discovery algorithm that is robust to log imperfection and widely applicable. Following, it shows our interactive protocol refinement approach that is intended to correct possible imprecisions introduced in the discovered protocol due to log imperfection. Finally, the experimental results on both real and synthetic data is presented.", "num_citations": "12\n", "authors": ["2067"]}
{"title": "Discovering e-services using UDDI in SELF-SERV\n", "abstract": " One of the key needs for businesses today over the Internet is the ability to dynamically discover services that they need, and compose them in an interoperable manner to carry out their business processes. The Universal Description, Discovery and Integration (UDDI) is a new technology that offers a standard way for businesses to build a registry, discover each other and describe how to interact over the Internet. In this paper, we discuss how UDDI is integrated into a Web service composition framework.", "num_citations": "12\n", "authors": ["2067"]}
{"title": "World Wide Database\u2014integrating the Web, CORBA and databases\n", "abstract": " While most of the data published on the Web is either semi-structured (eg, HTML documents) or unstructured (eg, text files, images), the Web also offers \u201chooks\u201d to ac. cess non-Web centric structured data (eg, relational databases). CGI scripts are usually used to access back-end databases. The Web has so far been incongruous with databases. The reason that the Web is database unfriendly is that it has been developed for open data sources. Databases are closed in nature in that communication with them is through a rigid protocol (DBMS). One needs to know the schema of a database to access or modify its state. This is fundamentally different from the openness and freeform type of Web data. 1Veb protocols and search engines have been developed for this kind of requirements and environments. Therefore, it is important to note that information retrieval and search techniques could not be applied because of\u00a0\u2026", "num_citations": "12\n", "authors": ["2067"]}
{"title": "Adaptive rule adaptation in unstructured and dynamic environments\n", "abstract": " Rule-based systems have been used to augment machine learning based algorithms for annotating data in unstructured and dynamic environments. Rules can alleviate many of shortcomings inherent in pure algorithmic approaches. Rule adaptation is a challenging and error-prone task: in a rule-based system, there is a need for an analyst to adapt rules in order to keep them applicable and precise. In this paper, we present an approach for adapting data annotation rules in unstructured and constantly changing environments. Our approach offloads analysts from adapting rules and autonomically identifies the optimal modification for rules using a Bayesian multi-armed-bandit algorithm. We conduct experiments on different curation domains and compare the performance of our approach with systems relying on analysts. The experimental results show a comparative performance of our approach compared to\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "A study of incorrect paraphrases in crowdsourced user utterances\n", "abstract": " Developing bots demands highquality training samples, typically in the form of user utterances and their associated intents. Given the fuzzy nature of human language, such datasets ideally must cover all possible utterances of each single intent. Crowdsourcing has widely been used to collect such inclusive datasets by paraphrasing an initial utterance. However, the quality of this approach often suffers from various issues, particularly language errors produced by unqualified crowd workers. More so, since workers are tasked to write open-ended text, it is very challenging to automatically asses the quality of paraphrased utterances. In this paper, we investigate common crowdsourced paraphrasing issues, and propose an annotated dataset called Para-Quality, for detecting the quality issues. We also investigate existing tools and services to provide baselines for detecting each category of issues. In all, this work presents a data-driven view of incorrect paraphrases during the bot development process, and we pave the way towards automatic detection of unqualified paraphrases.", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Friendly Hackers to the Rescue: How Organizations Perceive Crowdsourced Vulnerability Discovery.\n", "abstract": " Over the past years, crowdsourcing has increasingly been used for the discovery of vulnerabilities in software. While some organizations have extensively used crowdsourced vulnerability discovery, other organizations have been very hesitant in embracing this method. In this paper, we report the results of a qualitative study that reveals organizational concerns and fears in relation to crowdsourced vulnerability discovery. The study is based on 36 key informant interviews with various organizations. The study reveals a set of pre-adoption fears (ie, lacking managerial expertise, low quality submissions, distrust in security professionals, cost escalation, lack of motivation of security professionals) as well as the post-adoption issues actually experienced. The study also identifies countermeasures that adopting organizations have used to mitigate fears and minimize issues. Implications for research and practice are discussed.", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Service Component Architecture (SCA)\n", "abstract": " In this chapter, we introduce a framework known as Service Component Architecture (SCA) that provides a technology-agnostic capability for composing applications from distributed services. Building a successful SOA solution in practice can be complex, due to the lack of standards and specifications, especially when integrating many different technology environments. This chapter explores techniques for adopting a consensus on how to describe an assembly of services, and how to implement and access them regardless of the technology.", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Context as a service: realizing internet of things-aware processes for the independent living of the elderly\n", "abstract": " The Internet of Things (IoT) embodies the evolution from systems that link digital documents to systems that relate digital information with real-world physical items. It provides the infrastructure to transparently and seamlessly glue heterogeneous resources and services together by accessing sensors and actuators over the Internet. By connecting the physical world and the digital world, IoT creates numerous novel opportunities for many applications such as smart homes, smart cities, and industrial automation. However, on the other hand, IoT poses challenges to business process development, which unfortunately, have rarely been studied in the literature. In this paper, we present WITSCare, a research prototype of Web-based Internet of Things Smart home systems, with the aims of helping older people live in their own homes independently longer and safer. WITSCare exploits the heterogeneous\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "A value-added approach to design BI applications\n", "abstract": " Big Data Era has largely contributed in accelerating the development of large, high quality and valuable Knowledge Bases () by academicians (e.g., Cyc, DBpedia, Freebase, and YAGO) and industrials (e.g., Knowledge Graph). On the other hand, serious studies have identified the crucial role of  for analytical tasks, by offering analysts more entities (people, places, products, etc.). The availability of a huge, high quality and valuable  may contribute on designing value-added approaches for business intelligence applications. In this paper, we first propose a novel approach for semantic  design that considers  in the life cycle. Secondly, based on graph formalization adapted to , we produce conceptual multidimensional design and a semantic ETL process that orchestrates the graph data flows from data sources to the  storage. Finally, all steps of our approach are illustrated using the\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Harnessing implicit teamwork knowledge to improve quality in crowdsourcing processes\n", "abstract": " Workers in online crowd sourcing systems have different levels of expertise, trustworthiness, incentives and motivations. Therefore, recruiting sufficient number of well-suited workers is always a challenge. Existing methods usually recruit workers through open calls, friendships relations, matching their profiles with task requirements or recruiting teams of workers. But there are still challenges that need more investigations, mainly all existing recruitment methods are highly vulnerable to collaborating misbehaviour, i.e., Collusion. %These groups are highly vulnerable to collusion attacks. In this paper, we propose a recruitment method which takes into account individual and social attributes of workers to find suitable workers. The method discovers indirect collaborations between workers to harness implicit teamwork knowledge in order to increase the quality of crowd sourcing tasks' outcome and in the same time\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Servicebase: A programming knowledge-base for service oriented development\n", "abstract": " In recent times we have witnessed several advances in modern web-technology that has transformed the Internet into a global deployment and development platform. Such advances include Web 2.0 for large-scale collaboration; Social-computing for increased awareness; as well as Cloud-computing, which have helped virtualized resources over the Internet. As a result, this new computing environment has thus presented developers with ubiquitous access to countless web-services, along with computing resources, data-resources and tools. However, while these web-services enable tremendous automation and re-use opportunities, new productivity challenges have also emerged: The same repetitive, error-prone and time consuming integration work needs to get done each time a developer integrates a new API. To address these challenges we have developed ServiceBase, a \"programming\"\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Formulating the architectural design of enterprise applications as a search problem\n", "abstract": " Software architecture design is widely recognized to be a complex task. This is especially true when designing enterprise applications that require deciding about a number of architectural design issues, often involving selecting among various design alternatives that impact differently on a set of quality attributes. In order to facilitate the selection process, earlier research efforts have already investigated the use of quantitative decision-making methods for scoring and ranking design alternatives. These methods, however, treat individual architectural decisions independently without considering their synergistic interrelationships. We argue that many architectural decisions are highly interdependent with each other, and thus need to be treated jointly in the selection process. To support this claim, we have identified two types of dependencies that can occur among different design decisions. We show that in particular\u00a0\u2026", "num_citations": "11\n", "authors": ["2067"]}
{"title": "E-commerce enabling technologies\n", "abstract": " This work is covered by copyright. Unless the document is being made available under a Creative Commons Licence, you must assume that re-use is limited to personal use and that permission from the copyright owner must be obtained for all other uses. If the document is available under a Creative Commons License (or other specified license) then refer to the Licence for details of permitted re-use. It is a condition of access that users recognise and abide by the legal requirements associated with these rights. If you believe that this work infringes copyright please provide details by email to qut. copyright@ qut. edu. au", "num_citations": "11\n", "authors": ["2067"]}
{"title": "Feature-based and adaptive rule adaptation in dynamic environments\n", "abstract": " Rule-based systems have been used increasingly to augment learning algorithms for annotating data. Rules alleviate many of the shortcomings inherent in pure algorithmic approaches, in cases algorithms are not working well or lack from enough training data. However, in dynamic curation environments where data are constantly changing, there is a need to craft and adapt rules to keep them applicable and precise. Rule adaptation has been proven to be painstakingly difficult and error-prone, as an analyst is needed for examining the precision of rules and applying different modifications to adapt the imprecise ones. In this paper, we present an autonomic and conceptual approach to adapt data annotation rules. Our approach offloads analysts from adapting rules; it boosts rules to annotate a larger number of items using a set of high-level conceptual features, e.g. topic. We utilize a Bayesian multi-armed-bandit\u00a0\u2026", "num_citations": "10\n", "authors": ["2067"]}
{"title": "Source-aware crisis-relevant tweet identification and key information summarization\n", "abstract": " Twitter is an important source of information that people frequently contribute to and rely on for emerging topics, public opinions, and event awareness. Crisis-relevant tweets can potentially avail a magnitude of applications such as helping authorities and governments become aware of situations and thus offer better responses. One major challenge toward crisis-awareness in Twitter is to identify those tweets that are relevant to unseen crises. In this article, we propose an automatic labeling approach to distinguishing crisis-relevant tweets while differentiating source types (e.g., government or personal accounts) simultaneously. We first analyze and identify tweet-specific linguistic, sentimental, and emotional features based on statistical topic modeling. Then, we design a novel correlative convolutional neural network which uses a shared hidden layer to learn effective representations of the multi-faceted features\u00a0\u2026", "num_citations": "10\n", "authors": ["2067"]}
{"title": "Fuzzy integral optimization with deep q-network for eeg-based intention recognition\n", "abstract": " Non-invasive brain-computer interface using electroencephalography (EEG) signals promises a convenient approach empowering humans to communicate with and even control the outside world only with intentions. Herein, we propose to analyze EEG signals using fuzzy integral with deep reinforcement learning optimization to aggregate two aspects of information contained within EEG signals, namely local spatio-temporal and global temporal information, and demonstrate its benefits in EEG-based human intention recognition tasks. The EEG signals are first transformed into a 3D format preserving both topological and temporal structures, followed by distinctive local spatio-temporal feature extraction by a 3D-CNN, as well as the global temporal feature extraction by an RNN. Next, a fuzzy integral with respect to the optimized fuzzy measures with deep reinforcement learning is utilized to integrate the\u00a0\u2026", "num_citations": "10\n", "authors": ["2067"]}
{"title": "On decidability of simulation in data-centeric business protocols\n", "abstract": " We consider the problem of analyzing specifications of data-centric services. Specifications of such services incorporate data in business protocols. We focus our study on the decidability of the problem of checking the simulation preorder in the framework of the Colombo model. Colombo is a data-centric service that appears, at a first glance, to have a limited expressivity. Our first result, presented in this paper, shows that even in this restricted framework, both simulation and state reachability problems are already undecidable. Even worse, these problems remain undecidable in the case of non-communicating, read-only services.", "num_citations": "10\n", "authors": ["2067"]}
{"title": "software architectures and application development environments for Cloud computing\n", "abstract": " Welcome to the special issue of Software: Practice and Experience (SPE) journal. This special issue compiles a number of excellent technical contributions that significantly advance the state-of-the-art of software architectures and application development environments for cloud computing.", "num_citations": "10\n", "authors": ["2067"]}
{"title": "An adaptive document version management scheme\n", "abstract": " This paper addresses the design and implementation of an adaptive document version management scheme. Existing schemes typically assume: (i) a priori expectations for how versions will be manipulated and (ii) fixed priorities between storage space usage and average access time. They are not appropriate for all possible applications. We introduce the concept of document version pertinence levels in order to select the best scheme for given requirements (e.g., access patterns, trade-offs between access time and storage space). Pertinence levels can be considered as heuristics to dynamically select the appropriate scheme to improve the effectiveness of version management. We present a testbed for evaluating XML version management schemes.", "num_citations": "10\n", "authors": ["2067"]}
{"title": "A multi-dimensional trust model for processing big data over competing clouds\n", "abstract": " Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a\u00a0\u2026", "num_citations": "9\n", "authors": ["2067"]}
{"title": "A model-driven framework for interoperable cloud resources management\n", "abstract": " The proliferation of cloud computing has enabled powerful virtualization capabilities and outsourcing strategies. Suitably, a vast variety of cloud resource configuration and management tools have emerged to meet this needs, whereby DevOps are empowered to design end-to-end and automated cloud management tasks that span across a selection of best-of-breed tools. However, inherent heterogeneities among resource description models and management capabilities of such tools pose fundamental limitations when managing complex and dynamic cloud resources. In this paper we thus propose the notion of \u201cDomain-specific Models\u201d \u2013 a higher-level model-driven approach for describing elementary and federated cloud resources as reusable knowledge artifacts over existing tools. We also propose a pluggable architecture to translate these artifacts into lower-level resource descriptions and\u00a0\u2026", "num_citations": "9\n", "authors": ["2067"]}
{"title": "Business Process Management Workshops: BPM 2007 International Workshops, BPI, BPD, CBP, ProHealth, RefMod, semantics4ws, Brisbane, Australia, September 24, 2007, Revised\u00a0\u2026\n", "abstract": " These proceedings contain the final versions of papers accepted for the workshops that were held in conjunction with the Fifth International Conference on Business Process Management (BPM 2007) that took place in Brisbane, Australia. Twenty workshop proposals were submitted for this conference of which seven were selected. Ultimately this resulted in six workshops that ran concurrently on September 24 2007. This was the third year running for BPM workshops, a testament to the continued success of the workshop program. The BPM community\u2019s ongoing strong interest in process modelling, design, measurement and analysis were well reflected in the \u201cBusiness Process Intelligence\u201d and \u201cBusiness Process Design\u201d workshops. This year\u2019s workshops also included two new emerging areas that have gained increased attention:\u201cCollaborative Business Processes\u201d\u2014a topic which explores the challenges in\u00a0\u2026", "num_citations": "9\n", "authors": ["2067"]}
{"title": "OCEAN: Scalable and adaptive infrastructure for on-board information access\n", "abstract": " The idea of providing seamless connectivity and information access to users on-board public transport vehicles has attracted increasing popularity in recent years, as is evidenced by several commercially available systems that have attempted to implement it. In this article, we overview the specific technological challenges, research issues, as well as opportunities, that arise in the context of providing communication and information access on public transport. We focus on both the networking perspective\u2014in particular, discussing extensions required to existing TCP/IP mechanisms to support the moving on-board networks\u2014and the data management perspective, eg personalization of information and caching/pre-fetching for a highly dynamic and heterogeneous population of users. We contend that, to offer the best performance and flexibility, the design of public transport information systems ought to take advantage of the synergy between these two areas.", "num_citations": "9\n", "authors": ["2067"]}
{"title": "Automatic web service disco\n", "abstract": " Archive ouverte HAL - Automatic web service disco Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00107342, version 1 Article dans une revue Automatic web service disco Boualem Benatallah 1 M. Hacid 1 Christophe Rey 1 Farouk Toumani 1 D\u00e9tails 1 LIMOS - Laboratoire d'Informatique, de Mod\u00e9lisation et d'optimisation des Syst\u00e8mes Type de document : Article dans une revue Domaine : Informatique [cs] / Base de donn\u00e9es [cs.DB] Liste compl\u00e8te des m\u00e9tadonn\u00e9es Voir https://hal.archives-ouvertes.fr/\u2026", "num_citations": "9\n", "authors": ["2067"]}
{"title": "Interoperability in semantic web services\n", "abstract": " Semantic Web services approach is emerging as a promising technology for the effective automation of services development and interoperability by providing richer descriptions of service properties, capabilities and behavior in form of metadata. In this short paper, we discuss interoperability issues in semantic Web services.", "num_citations": "9\n", "authors": ["2067"]}
{"title": "WS-CatalogNet: building peer-to-peer e-catalog\n", "abstract": " One of the key issues in product catalogs is how to efficiently integrate and query large, intricate, heterogeneous catalogs. We propose a framework for building a dynamic catalog portals using catalog communities and semantic peer relationships between them. The aim is to facilitate distributed, dynamic and scalable integration of e-catalogs. Our approach is based on Peer-to-Peer architecture. Peers in our system serve as data integration mediators having individual schema to support semantically rich queries. Connections between peers are established based on domains and the relationships they represent. Schema and relationships in peers are used for routing queries among peers.", "num_citations": "9\n", "authors": ["2067"]}
{"title": "Data sharing on the web\n", "abstract": " The Internet and the World Wide Web (WWW) have elicited the explosion of accessible data repositories. Because high connectivity is now a reality, the challenge has been to take advantage of it to enable data sharing in a cost-effective way. Any proposed architecture would have to allow dynamic couplings of heterogeneous databases based on their content and interest. The authors propose an implementation using CORBA and Web technologies as a distributed infrastructure and platform to support the dynamic interconnection of heterogeneous and autonomous databases on the Web.", "num_citations": "9\n", "authors": ["2067"]}
{"title": "Automatic generation of chatbots for conversational web browsing\n", "abstract": " In this paper, we describe the foundations for generating a chatbot out of a website equipped with simple, bot-specific HTML annotations. The approach is part of what we call conversational web browsing, i.e., a dialog-based, natural language interaction with websites. The goal is to enable users to use content and functionality accessible through rendered UIs by \u201ctalking to websites\u201d instead of by operating the graphical UI using keyboard and mouse. The chatbot mediates between the user and the website, operates its graphical UI on behalf of the user, and informs the user about the state of interaction. We describe the conceptual vocabulary and annotation format, the supporting conversational middleware and techniques, and the implementation of a demo able to deliver conversational web browsing experiences through Amazon Alexa.", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Model-driven orchestration for cloud resources\n", "abstract": " Several DevOps tools have emerged to orchestrate cloud resources. However, inherent heterogeneity and complex implementation within these tools make it hard for DevOps users to design required resource-related artifacts. Currently, the defacto standard for cloud resource modeling and orchestration is TOSCA. Nonetheless, TOSCA is usually bound to TOSCA-compliant orchestration tools. Moreover, the actual integration between TOSCA and DevOps tools is still performed using costly coding and in ad-hoc manner. To resolve this, we believe that mapping and translation mechanisms between TOSCA and DevOps tools should be provided. In this paper, we propose a new model-driven approach for cloud resource orchestration. Our approach (i) adopts TOSCA to design resource-related artifacts regardless of a specific DevOps tool; (ii) enables a new model-driven translation technique that serves to translate\u00a0\u2026", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Expert recommendation via tensor factorization with regularizing hierarchical topical relationships\n", "abstract": " Knowledge acquisition and exchange are generally crucial yet costly for both businesses and individuals, especially when the knowledge concerns various areas. Question Answering Communities offer an opportunity for sharing knowledge at a low cost, where communities users, many of whom are domain experts, can potentially provide high-quality solutions to a given problem. In this paper, we propose a framework for finding experts across multiple collaborative networks. We employ the recent techniques of tree-guided learning (via tensor decomposition), and matrix factorization to explore user expertise from past voted posts. Tensor decomposition enables to leverage the latent expertise of users, and the posts and related tags help identify the related areas. The final result is an expertise score for every user on every knowledge area. We experiment on Stack Exchange Networks, a set of question\u00a0\u2026", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Software security professionals: Expertise indicators\n", "abstract": " In crowd-sourcing, selecting the person with suitable expertise is very important; especially since the task requester is not always in direct contact with the worker. Recently, this has become increasingly important particularly when the crowd-sourced tasks are complex and require skillful workers (e.g. software development, software testing, vulnerability discovery, and open innovation). In this paper, we aim to identify indicators to determine the expertise of security professionals in a crowd-sourcing vulnerability discovery platform. We review literature and online contents, conduct interviews with domain experts, and survey security professionals involved in the task of vulnerability discovery. We discuss the indicators we have found, and we provide some recommendations to help improve the process of selecting security professionals to perform crowd tasks related to vulnerability discovery.", "num_citations": "8\n", "authors": ["2067"]}
{"title": "CloudMap: A Visual Notation for Representing and Managing Cloud Resources\n", "abstract": " With the vast proliferation of cloud computing technologies, DevOps are inevitably faced with managing large amounts of complex cloud resource configurations. This involves being able to proficiently understand and analyze cloud resource attributes and relationships, and make decisions on demand. However, a majority of cloud tools encode resource descriptions and monitoring and control scripts in tedious textual formats. This presents complex and overwhelming challenges for DevOps to manually read, and iteratively build a mental representation especially when it involves a large number of cloud resources. To alleviate these frustrations we propose a model-driven notation to visually represent, monitor and control cloud resource configurations; managed underneath by existing cloud resource orchestration tools such as Docker. We propose a mindmap-based interface and set of visualization\u00a0\u2026", "num_citations": "8\n", "authors": ["2067"]}
{"title": "A toolkit for simplified web-services programming\n", "abstract": " The Internet has truly transformed into a global deployment and development platform. For example, Web 2.0 inspires large-scale collaboration; Social-computing empowers increased awareness; as well as Cloud-computing for virtualization of resources. As a result, developers have thus been presented with ubiquitous access to countless web-services. However, while this enables tremendous automation and re-use opportunities, new productivity challenges have also emerged: The same repetitive, error-prone and time consuming integration work needs to get done each time a developer integrates a new API. In order to address these challenges, we designed and developed ServiceBase, a \u201cprogramming\u201d knowledge-base to abstract, organize, incrementally curate and thereby re-use service-related programming-knowledge. Empowered by this knowledge we then provide: (a) A set of APIs that expose\u00a0\u2026", "num_citations": "8\n", "authors": ["2067"]}
{"title": "WS-Advisor: A task memory for service composition frameworks\n", "abstract": " With the proliferation of Web services, it is becoming increasingly important to support the users in selecting the most appropriate compositions of services for a task. We propose a new service discovery and selection framework that utilises the concept of task memories and a social network of task memories. A task memory captures the service composition history and their meta-data such as associated context and user rating. A network of task memories is formed to realise an effective task memory sharing platform among the users.", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Web service computing: Overview and directions\n", "abstract": " Web Service is a new buzzword sweeping through the information systems infrastructure industry. With the advent of the Internet and the Web, the first generation of Web services was born, namely Business-to-Customer (B2C) Web services (eg, virtual malls, customized news delivery, traffic monitoring, and route planning). More recently, organizations started using the Internet and Web as means to automate relationships between their business processes, ie, creating Business-to-Business (B2B) Web services. These services allow organizations to form alliances by joining their applications, databases, and systems. The purpose is to share their costs, skills, and resources as well as to offer value-added services. Examples of B2B Web services include procurement, customer relationship management (CRM), finance, billing, traffic information services, accounting, human resources, supply chain, and manufacturing\u00a0\u2026", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Building adaptive e-catalog communities based on user interaction patterns\n", "abstract": " In recent years, integration of e-catalogs has gained considerable momentum because of the emergence of online shopping portals, the increased demand for information exchange between trading partners, the prevalence of mergers and acquisitions, and so on. An e-catalog is any product catalog that you can search and query online, such as at www. dell. com or Amazon. com. Most solutions to the problem of organizing and integrating e-catalogs use a category-based hierarchy to structure a product catalog in a \u201cone view fits all\u201d fashion. This hierarchy is often determined by a system designer, who usually has a priori expectations of how customers will explore catalogs. However, customers might have different expectations. To minimize the gap between these expectations, designers should consider how customers actually use the catalogs. For example, for a computer parts catalog, assume that many users always access the product category RAM right after the category CPU. If the administrator merges the two categories to form a new category CPU&RAM, users need to visit the new category only once for information on both products. We propose a user-centric technique for restructuring integrated e-catalogs. We\u2019ve implemented this technique in WebCatalogPers, 1 a system that integrates online product catalogs. WebCatalogPers aims to continuously improve the organization of catalogs by responding to the ways customers navigate them. The resulting integrated catalogs continuously adapt and can be restructured in a dynamic environment.", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Dealing with version pertinence to design an efficient schema evolution framework\n", "abstract": " The paper addresses the design of a schema evolution framework enabling an efficient management of object versions. This framework is based on the adaptation and extension of two main schema evolution approaches, that is the approaches based on schema modification and those based on schema versioning. The framework provides an integrated environment to support different levels of adaptation (such as, modification and versioning at the schema level, conversion, object versioning, and emulation at the instance level). In addition, the authors introduce the concept of class/schema version pertinence enabling the database administrator to judge the pertinence of versions with regard the application programs. Finally, they provide operations for immediate refreshing of a database to enable an efficient manipulation of versions by a large number of application programs.", "num_citations": "8\n", "authors": ["2067"]}
{"title": "Crowdsourcing planar facility location allocation problems\n", "abstract": " Facility location allocation is key to success of urban design, mainly in designing transport systems, finding locations for warehouse, fire stations and so on. The problem of determining locations of k facilities so that provides service to n customers, also known as p-median problems, is one of the well-known -hard problems. Several heuristics have been proposed to solve location allocation problems, each of which has several limitations such as accuracy, time and flexibility, besides their advantages. In this paper, we propose to solve the p-median problems using crowdsourcing and gamification techniques. We present a crowdsourced game, called SolveIt, which employs wisdom and intelligence of the crowd to solve location allocation problems. We have presented a data model for representing p-median problems, designed and implemented the game and tested it using gold standards generated\u00a0\u2026", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Model-driven elasticity for cloud resources\n", "abstract": " Elasticity is a key distinguishing feature of cloud services. It represents the power to dynamically reconfigure resources to adapt to varying resource requirements. However, the implementation of such feature has reached a level of complexity since various and non standard interfaces are provided to deal with cloud resources. To alleviate this, we believe that elasticity features should be provided at resource description level. In this paper, we propose a Cloud Resource Description Model (cRDM) based on State Machine formalism. This novel abstraction allows representing cloud resources while considering their elasticity behavior over the time. Our prototype implementation shows the feasibly and experiments illustrate the productivity and expressiveness of our cRDM model in comparison to traditional solutions.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Process-driven configuration of federated cloud resources\n", "abstract": " Existing cloud resource providers offer heterogeneous resource deployment services to describe and deploy resource configurations. Describing and deploying federated cloud resource configurations over such deployment services is challenging due to dynamic application requirements and complexity of cloud environments. While solutions exist to solve this problem, they offer limited facilities to cater for resource provisioning over federated cloud services. This paper presents a novel cloud resource deployment framework that leverages a unified configuration knowledge-base where process-based notation is used to describe complex configurations over federated cloud services. Based on these notations, a deployment engine generates deployment scripts that can be executed by external cloud resource deployment services such as Puppet and Chef. The paper describes the concepts, techniques\u00a0\u2026", "num_citations": "7\n", "authors": ["2067"]}
{"title": "DataSheets: A spreadsheet-based data-flow language\n", "abstract": " We are surrounded by data, a vast amount of data that has brought about an increasing need for combining and analyzing it in order to extract information and generate knowledge. A need not exclusive of big software companies with expert programmers; from scientists to bloggers, many end-user programmers currently demand data management tools to generate information according to their discretion. However, data is usually distributed among multiple sources, hence, it requires to be integrated, and unfortunately, this process is still available just for professional developers. In this paper we propose DataSheets, a novel approach to make the data-flow specification accessible and its representation comprehensible to end-user programmers. This approach consists of a spreadsheet-based data-flow language that has been tested and evaluated in a service-centric composition framework.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Service learning and teaching foundry: A virtual SOA/BPM learning and teaching community\n", "abstract": " With the growing presence of BPM and SOA in the IT industry, their impact on the IT education will be profound. Many institutions are becoming aware of the acute need of developing learning and teaching resource frameworks for the BPM and SOA. In this paper, we present part of such an effort from a team at the University of New South Wales, currently developing Service Learning and Teaching Foundry as a dedicated virtual teaching and learning space for BPM/SOA. We present the motivation, design and current implementation of the foundry, as well as a curriculum design of a Service Technologies module which is used to pilot the foundry system.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "A user-driven environment for financial market data analysis\n", "abstract": " This paper proposes a software development environment which facilitates the analysis of large financial datasets by end-users. This environment is based on an event-based data model that gives a coherent representation of market activities, particularly high-fequency market and news data. The model makes it possible to define software components and Web services to manipulate entities in the model. The paper also describes a prototype implementation which allows domain experts to compose components and services to build an application. This prototype uses the Triana scientific workflow system to define workflows of existing software components and Web Services. This approach is demonstrated on a realistic case study related to processing both news and financial market data.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Protocol discovery from imperfect service interaction logs\n", "abstract": " This paper studies the problem of discovering protocol definitions from read-world service interaction data, which often are imperfect in various ways. It first describes the challenges in protocol discovery in such a context and the different aspects that must be considered by a protocol discovery solution. Next, it reports the current progress by presenting a discovery algorithm that is robust to log imperfection and widely applicable. Following, it shows our interactive protocol refinement approach that is intended to correct possible imprecisions introduced in the discovered protocol due to log imperfection. Finally, the experimental results on both real and synthetic data is presented. 1 Introduction and", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Building and querying e-catalog networks using p2p and data summarisation techniques\n", "abstract": " One of the critical issues in Web-based e-commerce has been how to efficiently and effectively integrate and query heterogeneous, diverse e-catalogs. We propose an integration framework for building and querying catalogs. Our approach is based on a hybrid of peer-to-peer data sharing paradigm and Web-services architecture. Peers in our system serve as domain-specific data integration mediators. Links between peers are established based on the similarity of the domain they represent. The relationships are used for routing queries among peers. As the number of catalogs involved grow larger, the need for filtering irrelevant data sources will become increasingly high. We apply a summarisation technique to summarise the content of catalogs. The summaries are used to pre-selecting data sources that are relevant to a user query.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Caching dynamic data for e-business applications\n", "abstract": " This paper is concerned with business portals; one of the rapidly growing web applications. It investigates the problem of providing a fast response time in such applications, particularly through the use of caching techniques. It discusses issues related to caching providers\u2019 response messages at business portals and proposes a caching strategy based on the collaboration between the portal and providers.", "num_citations": "7\n", "authors": ["2067"]}
{"title": "Web Services\u2013REST or Restful Services\n", "abstract": " In this chapter, REST is introduced, an alternate Web service implementation technique. Unlike SOAP and WSDL with clearly defined standards, REST contains a set of generic Web service design principles and guidelines that can be interpreted and implemented differently. In this chapter, we present the fundamentals of the said principles, explaining the core properties that make a service \"RESTful\". As a practical exercise, we include activities to build a full REST-based service with all READ/UPDATE operations and its client application.", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Network Security Technologies: Design and Applications: Design and Applications\n", "abstract": " Recent advances in technologies have created a need for solving security problems in a systematic way. With this in mind, network security technologies have been produced in order to ensure the security of software and communication functionalities at basic, enhanced, and architectural levels. Network Security Technologies: Design and Applications presents theoretical frameworks and the latest research findings in network security technologies while analyzing malicious threats which can compromise network integrity. This book is an essential tool for researchers and professionals interested in improving their understanding of the strategic role of trust at different levels of information and knowledge society.", "num_citations": "6\n", "authors": ["2067"]}
{"title": "A cloud resource orchestration framework for simplifying the management of web applications\n", "abstract": " Cloud computing paradigm [1] has shifted the computing from physical hardware- and locally managed software-enabled platforms to virtualized cloud-hosted services. Cloud computing assembles large networks of virtualized services: hardware resources (CPU, storage, and network) and software resources (e.g., databases, load-balancers, monitoring systems, etc.). Key issue in exploiting the potential of cloud computing is \u201cResource Orchestration\u201d. Resource orchestration process spans across a range of operations from selection, assembly, and deployment of resources to monitoring their run-time performance statistics (e.g. load, availability, throughput, utilization, etc.). The process aims to ensure achievement of fault-tolerant and QoS fulfillment states by resources and applications through adaptive management.", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Similarity function recommender service using incremental user knowledge acquisition\n", "abstract": " Similar entity search is the task of identifying entities that most closely resemble a given entity (e.g., a person, a document, or an image). Although many techniques for estimating similarity have been proposed in the past, little work has been done on the question of which of the presented techniques are most suitable for a given similarity analysis task. Knowing the right similarity function is important as the task is highly domain- and data-dependent. In this paper, we propose a recommender service that suggests which similarity functions (e.g., edit distance or jaccard similarity) should be used for measuring the similarity between two entities. We introduce the notion of \u201csimilarity function recommendation rule\u201d that captures user knowledge about similarity functions and their usage contexts. We also present an incremental knowledge acquisition technique for building and maintaining a set of similarity\u00a0\u2026", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Task memories and task forums: A foundation for sharing service-based personal processes\n", "abstract": " The growing number of online accessible services call for effective techniques to support users in discovering, selecting, and aggregating services. We present WS-Advisor, a framework for enabling users to capture and share task memories. A task memory represents knowledge (e.g., context and user rating) about services selection history for a given task. WS-Advisor provides a declarative language that allows users to share task definitions and task memories with other users and communities. The service selection component of this framework enables a user agent to improve its service selection recommendations by leveraging task memories of other user agents with which the user share tasks in addition to the local task memories.", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Vers un mod\u00e8le de composition de services web avec propri\u00e9t\u00e9s transactionnelles.\n", "abstract": " Le d\u00e9veloppement et l\u2019adoption de la technologie associ\u00e9e aux services web permettent d\u2019implanter de nouvelles applications avec valeur ajout\u00e9e en composant des services existants. Certains des services web, qui pourraient intervenir dans de telles compositions, ont des propri\u00e9t\u00e9s transactionnelles inh\u00e9rentes. Ceci est le cas notamment des services associ\u00e9s \u00e0 la r\u00e9servation de ressources, comme par exemple la r\u00e9servation de chambres d\u2019h\u00f4tel, de services professionnels, etc. Les propri\u00e9t\u00e9s transactionnelles de ces services peuvent \u00eatre exploit\u00e9es lors de leur composition, pour r\u00e9pondre \u00e0 des contraintes et des pr\u00e9f\u00e9rences \u00e9tablies par le concepteur. Cet article pr\u00e9sente un mod\u00e8le de composition de services poss\u00e9dant certaines propri\u00e9t\u00e9s transactionnelles, plus sp\u00e9cifiquement des fonctionnalit\u00e9s de tentative de r\u00e9servation et d\u2019ex\u00e9cution atomique. Le mod\u00e8le est bas\u00e9 sur un op\u00e9rateur de haut niveau qui permet aux concepteurs de composer des services cens\u00e9s s\u2019 ex\u00e9cuter en parall\u00e8le, et de prendre en compte des contraintes li\u00e9es \u00e0 l\u2019atomicit\u00e9 au travers de param\u00e8tres et de fonctions de restriction et de pr\u00e9f\u00e9rence.ABSTRACT. The development of new services by composition of existing ones has gained considerable momentum as a means of integrating heterogeneous applications and realising business collaborations. Services that enter into compositions with other services may have transactional properties, especially those in the broad area of resource management (eg booking", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Peering and querying e-catalog communities\n", "abstract": " More and more suppliers are offering access to their product or information portals (also called e-catalogs) via the Web. The key issue is how to efficiently integrate and query large, intricate, heterogeneous information sources such as e-catalogs. Traditional data integration approach, where the development of an integrated schema requires the understanding of both structure and semantics of all schemas of sources to be integrated, is hardly applicable because of the dynamic nature and size of the Web. We present WS-CatalogNet: a Web services based data sharing middleware infrastructure whose aims is to enhance the potential of e-catalogs by focusing on scalability and flexible aspects of their sharing and access.", "num_citations": "6\n", "authors": ["2067"]}
{"title": "Learning word representation for the cyber security vulnerability domain\n", "abstract": " There have been ever-increasing amounts of security vulnerabilities discovered and reported in recent years. Much of the information related to these vulnerabilities is currently available to the public, in the form of rich, textual data (e.g. vulnerability reports). Many of the state-of-the-art techniques used today to process such textual data rely on so-called word embeddings. As of today, several pre-trained embeddings have been created, many of which rely on general-purpose training datasets such as Google News and Wikipedia. More recently, other domain-specific word embeddings have been created (e.g. in the context of software development) to cope with terminology and ambiguity limitations of existing general-purpose embeddings. The availability of word embeddings for specialised domains is critical for the effectiveness of domain-specific tasks that rely on this technique. In this paper, we propose a word\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "State machine based human-bot conversation model and services\n", "abstract": " Task-oriented virtual assistants (or simply chatbots) are in very high demand these days. They employ third-party APIs to serve end-users via natural language interactions. Chatbots are famed for their easy-to-use interface and gentle learning curve (it only requires one of humans\u2019 most innate ability, the use of natural language). Studies on human conversation patterns show, however, that day-to-day dialogues are of multi-turn and multi-intent nature, which pushes the need for chatbots that are more resilient and flexible to this style of conversations. In this paper, we propose the idea of leveraging Conversational State Machine to make it a core part of chatbots\u2019 conversation engine by formulating conversations as a sequence of states. Here, each state covers an intent and contains a nested state machine to help manage tasks associated to the conversation intent. Such enhanced conversation engine\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Security vulnerability information service with natural language query support\n", "abstract": " The huge data breaches and attacks reported in the past years (e.g., the cases of Yahoo and Equifax) have significantly raised the concerns on the security of software used and developed by companies for their day-to-day operations. In this context, becoming aware about existing security vulnerabilities and taking preventive actions is of paramount importance for security professionals to help keep software secure. The increasingly large number of vulnerabilities discovered every year and the scattered and heterogeneous nature of vulnerability-related information make this, however, a non-trivial task. This paper aims at mitigating this problem by making security vulnerability information timely available and easily searchable. We propose to enrich and index security vulnerability information collected from publicly available sources on the Web. To make this information easily queryable we propose a\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Predicting citywide passenger demand via reinforcement learning from spatio-temporal dynamics\n", "abstract": " The global urbanization imposes unprecedented pressure on urban infrastructure and public resources. The population explosion has made it challenging to satisfy the daily needs of urban residents.'Smart City'is a solution that utilizes different types of data collection sensors to help manage assets and resources intelligently and more efficiently. Under the Smart City umbrella, the primary research initiative in improving the efficiency of car-hailing services is to predict the citywide passenger demand to address the imbalance between the demand and supply. However, predicting the passenger demand requires analysis on various data such as historical passenger demand, crowd outflow, and weather information, and it remains challenging to discover the latent relationships among these data. To address this challenge, we propose to improve the passenger demand prediction via learning the salient spatial\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "GrCAN: gradient boost convolutional autoencoder with neural decision forest\n", "abstract": " Random forest and deep neural network are two schools of effective classification methods in machine learning. While the random forest is robust irrespective of the data domain, the deep neural network has advantages in handling high dimensional data. In view that a differentiable neural decision forest can be added to the neural network to fully exploit the benefits of both models, in our work, we further combine convolutional autoencoder with neural decision forest, where autoencoder has its advantages in finding the hidden representations of the input data. We develop a gradient boost module and embed it into the proposed convolutional autoencoder with neural decision forest to improve the performance. The idea of gradient boost is to learn and use the residual in the prediction. In addition, we design a structure to learn the parameters of the neural decision forest and gradient boost module at contiguous steps. The extensive experiments on several public datasets demonstrate that our proposed model achieves good efficiency and prediction performance compared with a series of baseline methods.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Web service composition: overview\n", "abstract": " In this chapter, we introduce the motivation behind Web service composition technologies \u2013 going from an atomic to a composite service. In doing so, we discuss the two main paradigms of multiple service interactions: Web service orchestration and Web service choreography. In the rest of the book, we will focus on Web service orchestration as the main paradigm behind Web service composition techniques.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Web services\u2013soap and WSDL\n", "abstract": " In this chapter, SOAP and WSDL are explained as important standards that lay the foundation for standardized descriptions of messages and operations of a Web service. We first describe the core elements of SOAP and WSDL standards with examples, then present how the two standards are fit together to form the common message communication styles, namely RPC and Document. The chapter concludes with a practical exercise covering activities that build a simple Web service and its client application.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Data curation apis\n", "abstract": " Understanding and analyzing big data is firmly recognized as a powerful and strategic priority. For deeper interpretation of and better intelligence with big data, it is important to transform raw data (unstructured, semi-structured and structured data sources, e.g., text, video, image data sets) into curated data: contextualized data and knowledge that is maintained and made available for use by end-users and applications. In particular, data curation acts as the glue between raw data and analytics, providing an abstraction layer that relieves users from time consuming, tedious and error prone curation tasks. In this context, the data curation process becomes a vital analytics asset for increasing added value and insights. In this paper, we identify and implement a set of curation APIs and make them available (on GitHub) to researchers and developers to assist them transforming their raw data into curated data. The curation APIs enable developers to easily add features - such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string, number, date and time data; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data - into their\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Unified representation and reuse of federated cloud resources configuration knowledge\n", "abstract": " The proliferation of tools for different aspects of cloud resource configuration processes encourages DevOps to design end-to-end and automated configuration processes that span across a selection of best-of-breed tools. But heterogeneities among configuration knowledge representation models of such tools pose vital limitations for acquisition, discovery and curation of configuration knowledge for federated cloud application and resource requirements. We propose an embryonic data-model for representing cloud resource configuration knowledge artifacts. We also propose a rule based recommender service, which empowers (1) incremental knowledge acquisition and curation, and (2) declarative context driven knowledge recommendation. The paper describes the concepts, techniques and current implementation of the proposed system. Experiments on 36 real-life cloud resources show efficient re-use of\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Decidability and complexity of simulation preorder for data-centric web services\n", "abstract": " This paper studies the problem of checking the simulation preorder for data-centric services. It focuses more specifically on the underlying decidability and complexity issues in the framework of the Colombo model [1]. We show that the simulation test is exptime-complete for Colombo services without any access to the database (noted ColomboDB\u2009=\u2009\u2205\u2009) and 2exptime-complete when only bounded databases are considered (the obtained model is noted Colombo                     bound                   ). This is a decidability border since we have shown in previous work that the simulation test for unbounded Colombo is undecidable. Moreover, as a side effect of this work, we establish a correspondance between ColomboDB\u2009=\u2009\u2205\u2009, restricted to equality, and Guarded Variable Automata (GVA) [2]. As a consequence, we derive EXPTIME-completeness of simulation for GVA.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Correlating time-related data sources with co-clustering\n", "abstract": " A huge amount of data is circulated and collected every day on a regular time basis. Given a pair of such datasets, it might be possible to reveal hidden dependencies between them since the presence of the one dataset elements may influence the elements of the other dataset and vice versa. Furthermore, the impact of these relations may last during a period instead of the time point of their co-occurrence. Mining such relations under those assumptions is a challenging problem. In this paper, we study two time-related datasets whose elements are bilaterally affected over time. We employ a co-clustering approach to identify groups of similar elements on the basis of two distinct criteria: the direction and duration of their impact. The proposed approach is evaluated using time-related news and stock\u2019s market real datasets.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "On embedding task memory in services composition frameworks\n", "abstract": " With the increasing availability of Web services and adoption of services oriented paradigm, there is a growing need to dynamically compose services for realizing complex user tasks. While service composition is itself an important problem, a key issue is also how to support users in selecting the most appropriate compositions of services to fulfill a task. In existing dynamic services selection approaches, combinations of services are repeatedly discovered (e.g., using ontology-based matching techniques) and selected by users whenever needed. To improve their effectiveness, we propose a new technique that provides an efficient access to what is named a \u201ctask memory\u201d. A task memory is used to provide users with a context-aware service selection by recommending combinations of services that are most appropriate in a given context. A task memory is formed using the service composition history and\u00a0\u2026", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Dynamic restructuring of recovery nets\n", "abstract": " A Self-Adaptive Recovery Net (SARN) is an extended Petri net model for specifying exceptional behavior in workflow systems. SARN caters for high-level recovery policies that are incorporated either with a single task or a set of tasks, called a recovery region. A recovery region delimits the part of the workflow from which the associated recovery policies take place. In this paper, we assume that SARN is initially partitioned into recovery regions by workflow designers who have a priori expectations for how exceptions will be handled. We propose a pattern-based approach to dynamically restructure SARN partition. The objective is to continuously restructure recovery regions within SARN partition to reflect the dynamic changes in handling exceptions. The restructuring of SARN partition is based on the observation of predefined recovery patterns.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Usage\u2014Centric Adaptation of Dynamic E\u2014Catalogs\n", "abstract": " Although research into the integration of e-catalogs has gained considerable momentum over the years, the needs for building adaptive catalogs have been largely ignored. Catalogs are designed by system designers who have a priori expectations for how catalogs will be explored by users. It is necessary to consider how users are using catalogs since they may have different expectations. In this paper, we describe the design and the implementation of a system through which integrated product catalogs are continuously adapted and restructured within a dynamic environment. The adaptation of integrated catalogs is based on the observation of customers\u2019 interaction patterns.", "num_citations": "5\n", "authors": ["2067"]}
{"title": "Exploring missing interactions: A convolutional generative adversarial network for collaborative filtering\n", "abstract": " Adversarial examples can be detrimental to a recommender, leading to a surging enthusiasm for applying adversarial learning to improve recommendation performance, eg raising model robustness, alleviating data sparsity, generating initial profiles for cold-start users or items, etc. Most existing adversarial example generation methods fall within three categories: attacking the user-item interactions or auxiliary contents, adding perturbations in latent space, sampling the latent space according to certain distribution. In this work, we focus on the semantic-rich user-item interactions in a recommender system and propose a novel generative adversarial network (GAN) named Convolutional Generative Collaborative Filtering (Conv-GCF). We develop an effective perturbation mechanism (adversarial noise layer) for convolutional neural networks (CNN), based on which we design a generator with residual blocks to\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Discovering activities from emails based on pattern discovery approach\n", "abstract": " Significant research work has been conducted in the area of process mining leading to mature solutions for discovering knowledge from structured process event logs analysis. Recently, there have been several initiatives to extend the scope of these analysis to consider heterogeneous and unstructured data sources. More precisely, email analysis has attracted much attention as emailing system is considered as one of the principal channel to support the execution of business processes (BP). However, given the unstructured nature of email logs data, process mining techniques could not be applied directly; thus it is necessary to generate structured event logs. Activities form a cornerstone component of BP that must be identified to obtain such structured logs. In this paper we propose to discover frequent activities from email logs. Existing approaches are usually supervised or require human intervention. In\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "A declarative language to support dynamic evolution of web service business protocols\n", "abstract": " We investigate the problem of web service instances migration in the context of business protocol evolution, i.e., how to convert active instances of web services from an old version of a business protocol into a new one? We propose a framework based on a declarative approach to support service providers in defining fine-grained migration strategies of active instances. While the existing approaches for instances migration force the migrated instances to reflect the original ones as accurately as possible, in our approach we give to service providers the ability to declaratively define the constraints that drive the instances migration process. A migration strategy is expressed as a set of instances migration rules which are specified using an instance mapping language made of a set of generic migration patterns. The proposed approach has been implemented in a software tool that provides useful\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Service graph base: A unified graph-based platform for representing and manipulating service artifacts\n", "abstract": " The number of Web-Services publicly accessible through APIs have rapidly grown in recent years. Although, while these services are key in providing access to data as well as a variety of functionality, often their full potential remains yet to be fully exploited. Due to the different standards used to implement and expose Web services, it is usually cumbersome for developers to reuse them within third-party applications. In this paper, we present Service Graph Base, a unified graph-based platform for representing and manipulating service artifacts. It allows professional developers to add service knowledge to a graph base, which can be done either automatically or manually. By adopting the language-neutral RDF-based Graph Model and developing a uniformly-interfaced service editor for service native artifacts extraction, services are represented as RDF graphs in Service Graph Base. Once they are in the graph\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Integrating feature analysis and background knowledge to recommend similarity functions\n", "abstract": " Existing approaches in similarity analysis is little concerned with the right choice of similarity functions. We present an approach for suggesting which similarity functions (e.g., edit distance) are most appropriate for a given similarity search task. We identify data features (e.g., misspellings) that are considerable when choosing similarity functions. We also introduce the concept of similarity function background knowledge that associates data features with similarity functions, and apply the knowledge to recommend suitable similarity functions.", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Using graph aggregation for service interaction message correlation\n", "abstract": " Discovering the behavior of services and their interactions in an enterprise requires the ability to correlate service interaction messages into process instances. The service interaction logic (or process model) is then discovered from the set of process instances that are the result of a given way of correlating messages. However, sometimes, the Correlation Conditions (CC) allowing to identify correlations of messages from a service interaction log are not known. In such cases, and with a large number of message\u2019s correlator attributes, we are facing a large space of possible ways messages may be correlated which makes identifying process instances difficult. In this paper, we propose an approach based on message indexation and aggregation to generate a size-efficient Aggregated Correlation Graph (ACG) that exhibits all the ways messages correlate in a service interaction log not only for disparate pairs\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Data messaging based approach for web service composition\n", "abstract": " One of the important aspects of Web services is the service composition that offers the possibility to compile value-added services from elementary ones. Due to autonomous development of Web services, their composition could fail because of broken messages flow. To tackle this problem, we propose to build a third party service, called mediator, that tries to produce the missing (non-existent) messages to fulfill the composition.", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Policy-based exception handling in business processes\n", "abstract": " A workflow management system (WfMS) provides a central control point for defining business processes and orchestrating their execution. A major limitation of current WfMSs is their lack of support for dynamic workflow adaptations. This functionality is an important requirement in order to provide sufficient flexibility to cope with expected but unusual situations and failures. In this report, we propose Self-Adaptive Recovery Net (SARN), an extended Petri net model for specifying exceptional behavior in workflow systems at design time. SARN can adapt the structure of the underlying Petri net at run time to handle exceptions while keeping the Petri net design simple and easy. The proposed framework also caters for high-level recovery policies that are incorporated either with a single task or a set of tasks, called a recovery region. These recovery policies are generic constructs that model exceptions at design time together with a set of primitive operations that can be used at run time to handle the occurrence of exceptions.", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Adaptive web-based database communities\n", "abstract": " The evolution into the global information infrastructure and the concomitant increase in the available information on the Web, is offering a powerful distribution vehicle for organizations that need to coordinate the use of multiple information sources. However, the technology to organize, search, integrate, and evolve these sources has not kept pace with the rapid growth of the available information space. In this chapter, we present our work in the WebFINDIT project. WebFINDIT aims to achieve the scalable integration and efficient querying of Web-accessible databases through the incremental data-driven discovery and formation of interrelationships between information sources. WebFINDIT uses an ontological organization of the information space to filter interactions and accelerate service searches. More precisely, the information space is organized as domain-specific groups. Each group forms a database\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Personalised organisation of dynamic e-Catalogs\n", "abstract": " Research into personalisation issues in product catalogs has mainly been focused on recommender systems and the needs for building adaptive catalogs have been largely ignored. Catalogs are designed by system designers who have a priori expectations for how catalogs will be explored by users. It is necessary to consider how users are using catalogs since they may have different expectations. WebCatalog                                            Pers                  proposed a design and an implementation of a system through which integrated product catalogs are continuously adapted and restructured within a dynamic environment. The adaptation of integrated catalogs is based on the observation of customers\u2019 interaction patterns. In this paper, we extend the idea further by introducing the notion of liked minded people, where the same design principle of WebCatalog                                            Pers                  is applied\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "Evolution du schema d'une base de donnees a objets: une approche par compromis\n", "abstract": " Dans cette th\u00e8se, nous int\u00e9ressons au probl\u00e8me de l'\u00e9volution des sch\u00e9mas pour les bases de donn\u00e9es \u00e0 objets. Nous consid\u00e9rons d'abord les solutions propos\u00e9es pour la gestion de l'\u00e9volution de sch\u00e9ma de bases de donn\u00e9es \u00e0 objets. Nous proposons une classification des approches existantes. Pour chacune de ces approches nous d\u00e9crivons son principe, les m\u00e9canismes d'\u00e9volution associ\u00e9s, ainsi que les produits et les prototypes qui l'implantent. Nous analysons ces travaux en soulignant les avantages et les inconv\u00e9nients de chaque approche. Nous pr\u00e9sentons ensuite notre approche. D'une part, cette approche propose un cadre qui permet de combiner les fonctionnalit\u00e9s de la modification et du versionnement pour une meilleure gestion de l'\u00e9volution de sch\u00e9ma. D'autre part, elle offre \u00e0 l'utilisateur un langage permettant de d\u00e9crire les liens entre les diff\u00e9rents \u00e9tats de la base de donn\u00e9es afin de traduire le plus fid\u00e8lement possible les \u00e9volutions du monde r\u00e9el. Le versionnement de sch\u00e9ma \u00e9vite la perte d'informations et assure que les anciens programmes d'applications continuent de fonctionner. Cependant, le nombre de versions peut devenir important; ce qui rend complexe leur gestion. Notre approche permet de limiter le nombre de versions:(1) l'\u00e9volution d'un sch\u00e9ma est traduite par sa modification si l'\u00e9volution est non-soustractive(ne provoque pas la suppression de propri\u00e9t\u00e9s) ou si l'utilisateur le d\u00e9cide,(2) La technique utilis\u00e9e pour adapter les instances au sch\u00e9ma apr\u00e8s l'\u00e9volution, est bas\u00e9e sur la caract\u00e9risation de l'importance de l'existence en tant que telle d'une version d'objet. Ainsi, le nombre de versions est limit\u00e9 \u00e0\u00a0\u2026", "num_citations": "4\n", "authors": ["2067"]}
{"title": "An Internet of Things Service Roadmap\n", "abstract": " We propose a roadmap for leveraging the tremendous opportunities the Internet of Things (IoT) has to offer. We argue that the combination of the recent advances in service computing and IoT technology provide a unique framework for innovations not yet envisaged, as well as the emergence of yet-to-be-developed IoT applications. This roadmap covers: emerging novel IoT services, articulation of major research directions, and suggestion of a roadmap to guide the IoT and service computing community to address key IoT service challenges.", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Toward higher-level abstractions based on state machine for cloud resources elasticity\n", "abstract": " With the dynamic nature of cloud applications and rapid change of their resource requirements, elasticity over cloud resources has to be effectively supported. It represents the ability to dynamically adjust cloud resources that applications use in order to adapt to their varying workloads, while maintaining the desired quality of service. However, implementing elasticity is still challenging task for cloud users as heterogeneous and low-level interfaces are provided to manage cloud resources. To alleviate this, we believe that elasticity features should be provided at resource description level. In this paper, we propose a new Cloud Resource Description Model called cRDM, which is based on State Machine formalism. Using this model, we aim at representing cloud resources while considering their elasticity behavior over the time without referring to any low level interfaces or cloud provider technical constraints. We also\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Dual-stream self-attentive random forest for false information detection\n", "abstract": " The prevalence of online social media facilitates massive knowledge acquisition and sharing throughout the Web. Meanwhile, it inevitably poses the risk of generating and disseminating false information by both benign and malicious users. Despite there has been considerable research on false information detection from both the opinion-based and fact-based perspectives, they mostly focus on tailored solutions for a particular domain and carry out limited work on leveraging multi-faceted clues such as textual cues, behavioral trails, and relational connection. We propose a novel dual-stream attentive random forest that is capable of selecting clues of discriminative information from individuals, collective information (e.g., texts), and correlations of entities (e.g., social interactions) adaptively. In particular, we use an interpretive attention model for learning textual contents. The model treats the important and\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Introduction to Service Oriented Architecture\n", "abstract": " In this chapter, we begin by understanding Service Oriented Architecture (SOA), its key values and goals too modern and evolving business ecosystems. We then describe the SOA architectural stack in reference to software application integration layers. This is followed by an introduction to service composition and data-flow techniques, including end-user mashups. This chapter also presents the overall goals, structure and organization of the rest of this book.", "num_citations": "3\n", "authors": ["2067"]}
{"title": "A resource provisioning strategy for elastic analytical workflows in the cloud\n", "abstract": " With the advent of big data era, the data analytical applications have sprung up which can be modeled using workflow. Such workflow applications are subject to continuously arriving requests and have a rigid requirement on response time. When running the analytical workflow in a cloud platform, one of the critical questions arising here is that how to provision resources in a way that the monetary cost can be reduced under the constraint of the response time. In this paper, we use queueing network theory to address this challenge. First, we present the performance analytic model for the elastic analytical workflows based on queueing network theory. Then, we design a resource provision strategy to determine the number of virtual machines for individual component of the applications with response time constraint. Simulation experiments using the real workload trace data show that our proposed approach\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Mine your own business, mine others' news!\n", "abstract": " Major media companies such as The Financial Times, the Wall Street Journal or Reuters generate huge amounts of textual news data on a daily basis. Mining frequent patterns in this mass of information is critical for knowledge workers such as financial analysts, stock traders or economists. Using existing frequent pattern mining (FPM) algorithms for the analysis of news data is difficult because of the size and lack of structuring of the free text news content. In this article, we demonstrate a comprehensive Streaming TEmporAlData (STEAD) analysis framework for mining frequent patterns in financial news. In this demonstration, we show how the mining task is supported by the use of a Time-Aware Content Summarization algorithm (TACS). This summary generates a concise representation of large volume of data by taking into account the expert's peculiar interest while preserving the news arrival temporal\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Introduction to special issue on semantic Web services\n", "abstract": " Service-oriented computing (SOC) is a new paradigm for distributed computing, slated to shape modern societies in vital areas such as health, government, science, engineering, and business [Spohrer and Riecken 2006; Yu et al. 2007; Medjahed et al. 2003]. It utilizes Web services as the building blocks for developing agile networks of collaborating applications. Web services are the direct answer to the explosion of heterogeneous Web applications that have been developed in the last decade or so. They are increasingly becoming the de facto means to deliver all kinds of functionalities on the Web for direct consumption by programs. Web services may wrap a wide range of Web-accessible resources such as software programs, sensors, databases, and storage devices. This unprecedented proliferation of Web services has been possible because of the intense activity aimed at standardizing the different aspects\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Reflective data sharing in managing Internet databases\n", "abstract": " The popularity of the Internet has generated an explosion in the number of accessible information sources. In addition, recent advances in wide-area networking have led to a push for a logically-unified, yet physically distributed, information repository accessible through the Internet. The architecture of data resources and sources should be scalable and able to accommodate hundreds and thousands of databases. These are called Internet databases. This has to be achieved against a backdrop of accommodating database autonomy and bridging their heterogeneity with reasonable overhead. In addition, manual and evolution management needs a careful design as ad hoc management is clearly intractable in a highly dynamic environment as the Internet. Therefore, there is a need for supporting a self-adjustable model of evolution. We propose an architecture that has the ability to reflect upon its own state and to\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Formal correctness procedures for object-oriented databases\n", "abstract": " One of the most di cult requirements of database support is to guarantee schema and instance update correctness. Engineering applications rely on this safeguard during product development. Thus it is essential that a DBMS dynamically support schema modi cations by allowing on-line access to the schema, and providing automatic consistency checking of objects as well as self-adaptive dynamic linking of new methods. Our paper addresses these issues by providing a set of formalized procedures for de ning and hence checking schema correctness. In that context, we propose a core model for schema correctness management in object-oriented databases.", "num_citations": "3\n", "authors": ["2067"]}
{"title": "Gestion d'un \u00e9volution du sch\u00e9ma d'une base de donn\u00e9es \u00e0 objets: une approche par compromis\n", "abstract": " Dans cette th\u00e8se, nous int\u00e9ressons au probl\u00e8me de l'\u00e9volution des sch\u00e9mas pour les bases de donn\u00e9es \u00e0 objets. Nous consid\u00e9rons d'abord les solutions propos\u00e9es pour la gestion de l'\u00e9volution de sch\u00e9ma de bases de donn\u00e9es \u00e0 objets. Nous proposons une classification des approches existantes. Pour chacune de ces approches nous d\u00e9crivons son principe, les m\u00e9canismes d'\u00e9volution associ\u00e9s, ainsi que les produits et les prototypes qui l'implantent. Nous analysons ces travaux en soulignant les avantages et les inconv\u00e9nients de chaque approche. Nous pr\u00e9sentons ensuite notre approche. D'une part, cette approche propose un cadre qui permet de combiner les fonctionnalit\u00e9s de la modification et du versionnement pour une meilleure gestion de l'\u00e9volution de sch\u00e9ma. D'autre part, elle offre \u00e0 l'utilisateur un langage permettant de d\u00e9crire les liens entre les diff\u00e9rents \u00e9tats de la base de donn\u00e9es afin de traduire le plus fid\u00e8lement possible les \u00e9volutions du monde r\u00e9el. Le versionnement de sch\u00e9ma \u00e9vite la perte d'informations et assure que les anciens programmes d'applications continuent de fonctionner. Cependant, le nombre de versions peut devenir important ; ce qui rend complexe leur gestion. Notre approche permet de limiter le nombre de versions: (1) l'\u00e9volution d'un sch\u00e9ma est traduite par sa modification si l'\u00e9volution est non-soustractive (ne provoque pas la suppression de propri\u00e9t\u00e9s) ou si l'utilisateur le d\u00e9cide, (2) La technique utilis\u00e9e pour adapter les instances au sch\u00e9ma apr\u00e8s l'\u00e9volution, est bas\u00e9e sur la caract\u00e9risation de l'importance de l'existence en tant que telle d'une version d'objet. Ainsi, le nombre de versions est limit\u00e9 \u00e0\u00a0\u2026", "num_citations": "3\n", "authors": ["2067"]}
{"title": "A Query Language for Summarizing and Analyzing Business Process Data\n", "abstract": " In modern enterprises, Business Processes (BPs) are realized over a mix of workflows, IT systems, Web services and direct collaborations of people. Accordingly, process data (i.e., BP execution data such as logs containing events, interaction messages and other process artifacts) is scattered across several systems and data sources, and increasingly show all typical properties of the Big Data. Understanding the execution of process data is challenging as key business insights remain hidden in the interactions among process entities: most objects are interconnected, forming complex, heterogeneous but often semi-structured networks. In the context of business processes, we consider the Big Data problem as a massive number of interconnected data islands from personal, shared and business data. We present a framework to model process data as graphs, i.e., Process Graph, and present abstractions to summarize the process graph and to discover concept hierarchies for entities based on both data objects and their interactions in process graphs. We present a language, namely BP-SPARQL, for the explorative querying and understanding of process graphs from various user perspectives. We have implemented a scalable architecture for querying, exploration and analysis of process graphs. We report on experiments performed on both synthetic and real-world datasets that show the viability and efficiency of the approach.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "API Topics Issues in Stack Overflow Q&As Posts: An Empirical Study\n", "abstract": " Application Programming Interfaces (APIs) have become one of the key assets within modern businesses, facilitating the linking and integration of intra- and inter-organizational data and systems in the context of complex and heterogeneous technology ecosystems. APIs allow organizations to monetize data, build profitable partnerships and foster innovation and growth. Understanding APIs and their usage are therefore key to building solutions for enabling successful business operations. This paper aims at understanding API topic issues posted on Stack Overflow (SO), a Community Question Answering (CQA) site for programmers. We conduct an empirical analysis on a sample of 400 randomly-selected Q&As threads to help identify API-related issues and their main topics. A thematic analysis performed on this sample reveals eight main topics related to APIs, among which API usage, debugging, API constraints\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Dynamic event type recognition and tagging for data-driven insights in law-enforcement\n", "abstract": " In law enforcement, investigators are typically tasked with analyzing large collections of evidences in order to identify and extract key information to support investigation cases.  In this context, events are key elements that help understanding and reconstructing what happened from the collection of evidence items. With the ever increasing amount of data (e.g., e-mails and content from social media) gathered today as part of investigation tasks (in most part done manually), managing such amount of data can be challenging and prone to missing important details that could be of high relevance to a case. In this paper, we aim to facilitate the work of investigators through a framework for deriving insights from data. We focus on the auto-recognition and dynamic tagging of event types (e.g., phone calls) from (textual) evidence items, and propose a framework to facilitate these tasks and provide support for insights\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Towards Swarm Intelligence Architectural Patterns: An IoT-Big Data-AI-Blockchain Convergence Perspective\n", "abstract": " The Internet of Things (IoT) is exploding. It is made up of billions of smart devices-from minuscule chips to mammoth machines-that use wireless technology to talk to each other (and to us). IoT infrastructures can vary from instrumented connected devices providing data externally to smart, and autonomous systems. To accompany data explosion resulting, among others, from IoT, Big data analytics processes examine large data sets to uncover hidden patterns, unknown correlations between collected events, either at a very technical level (incident/anomaly detection, predictive maintenance) or at business level (customer preferences, market trends, revenue opportunities) to provide improved operational efficiency, better customer service, competitive advantages over rival organizations, etc. In order to capitalize business value of the data generated by IoT sensors, IoT, Big Data Analytics/IA need to meet in the\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Data-augmented regression with generative convolutional network\n", "abstract": " Generative adversarial networks (GAN)-based approaches have been extensively investigated whereas GAN-inspired regression (i.e., numeric prediction) has rarely been studied in image and video processing domains. The lack of sufficient labeled data in many real-world cases poses great challenges to regression methods, which generally require sufficient labeled samples for their training. In this regard, we propose a unified framework that combines a robust autoencoder and a generative convolutional neural network (GCNN)-based regression model to address the regression problem. Our model is able to generate high-quality artificial samples via augmenting the size of a small number of training samples for better training effects. Extensive experiments are conducted on two real-world datasets and the results show that our proposed model consistently outperforms a set of advanced techniques\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Experts community memory for entity similarity functions recommendation\n", "abstract": " Similarity search (or similar entity search) is the process of finding all entities similar to a given entity (e.g., a person, a document, or an image). Although many techniques for similarity analysis have been proposed in the past, little work has been done on the question of which of the presented techniques are most suitable for a given similarity search task. Knowing the right similarity function is important as the task is highly domain- and data-dependent. In this article, we provide an approach for recommending which similarity functions (e.g., edit distance or jaccard similarity) should be used for measuring the similarity between two entities. The approach employs an incremental knowledge acquisition technique for capturing domain experts\u2019 knowledge about similarity functions and their usage contexts (e.g., entity class, attribute name and some keywords). In addition, for situations where domain experts have little or\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "On Automating Basic Data Curation Tasks.\n", "abstract": " Big data analytics is firmly recognized as a strategic priority for modern enterprises. At the heart of big data analytics lies the data curation process, consists of tasks that transform raw data (unstructured, semi-structured and structured data sources) into curated data, ie contextualized data and knowledge that is maintained and made available for use by end-users and applications. To achieve this, the data curation process may involve techniques and algorithms for extracting, classifying, linking, merging, enriching, sampling, and the summarization of data and knowledge. To facilitate the data curation process and enhance the productivity of researchers and developers, we identify and implement a set of basic data curation APIs and make them available as services to researchers and developers to assist them in transforming their raw data into curated data. The curation APIs enable developers to easily add features-such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string and numbers; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data-into their data applications. These services can be accessed via a REST API\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Web Service Composition: Data Flows\n", "abstract": " In this chapter, we examine the data-flow aspects of Web service composition, which specifies how data is exchanged between services. The data-flow description encapsulates the data movement from one service to another and the transformations applied on this data. We introduce two different paradigms based on the message passing style, namely, blackboard and explicit data flow. We conclude the chapter with a discussion of Mashup applications as a way to implement data-flow oriented service composition.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Web Services\u2013Data Services\n", "abstract": " In this chapter, we explore the concept of data services. After clarifying the main concepts, we introduce key enabling technologies for building data services, namely XSLT and XQuery. These two XML-based languages are used to transform and query potentially heterogeneous data into well-understood standard XML documents. The lab exercises included at the end of this chapter will guide you to learn the basic syntax and usage scenarios of XSLT and XQuery.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Unveiling Contextual Similarity of Things via Mining Human-Thing Interactions in the Internet of Things\n", "abstract": " With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations of ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based method for discovering underlying connections of things via mining the rich content embodied in human-thing interactions in terms of user, temporal and spatial information. We model these various information using two graphs, namely spatio-temporal graph and social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management. To demonstrate the utility, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Up in the air: When homes meet the web of things\n", "abstract": " The emerging Internet of Things (IoT) will comprise billions of Web-enabled objects (or \"things\") where such objects can sense, communicate, compute and potentially actuate. WoT is essentially the embodiment of the evolution from systems linking digital documents to systems relating digital information to real-world physical items. It is widely understood that significant technical challenges exist in developing applications in the WoT environment. In this paper, we report our practical experience in the design and development of a smart home system in a WoT environment. Our system provides a layered framework for managing and sharing the information produced by physical things as well as the residents. We particularly focus on a research prototype named WITS, that helps the elderly live independently and safely in their own homes, with minimal support from the decreasing number of individuals in the working-age population. WITS enables an unobtrusive monitoring of elderly people in a real-world, inhabituated home environment, by leveraging WoT technologies in building context-aware, personalized services.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Web Information Systems Engineering--WISE 2014: 15th International Conference, Thessaloniki, Greece, October 12-14, 2014, Proceedings, Part II\n", "abstract": " This book constitutes the proceedings of the 15th International Conference on Web Information Systems Engineering, WISE 2014, held in Thessaloniki, Greece, in October 2014. The 52 full papers, 16 short and 14 poster papers, presented in the two-volume proceedings LNCS 8786 and 8787 were carefully reviewed and selected from 196 submissions. They are organized in topical sections named: Web mining, modeling and classification; Web querying and searching; Web recommendation and personalization; semantic Web; social online networks; software architectures amd platforms; Web technologies and frameworks; Web innovation and applications; and challenge.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Time-aware content summarization of data streams\n", "abstract": " Major media companies such as The Financial Times, the Wall Street Journal or Reuters generate huge amounts of textual news data on a daily basis. Mining frequent patterns in this mass of information is critical for knowledge workers such as financial analysts, stock traders or economists. Using existing frequent pattern mining (FPM) algorithms for the analysis of news data is difficult because of the size and lack of structuring of the free text news content. In this article, we propose a Time-Aware Content Summarization algorithm to supports FPM in financial news data. The summary allows a concise representation of large volume of data by taking into account the expert's peculiar interest. The summary also preserves the news arrival time information which is essential for FPM algorithms. We experimented the proposed approach on Reuters news data and integrated it into the Streaming TEmporAl Data (STEAD) analysis framework for interactive discovery of frequent pattern.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Fast and scalable access to advance resource reservation data in future cellular networks\n", "abstract": " Increasing demand on the scarce radio spectrum indicates that availability of required radio resources (e.g. wireless bandwidth) at a given time and location will be less certain in the future. Resource uncertainty inhibits many future applications that require large amount of resources to be guaranteed at specific times and locations. In order to address the problem of resource uncertainty, these applications may need to reserve network resources in advance. However, in order to support percall advance reservation, the network must store, process, and access large volume of reservation data efficiently. In particular, the admission control functions must have fast and scalable access to these reservation data for making effective decisions regarding the acceptance and rejection of both immediate and future calls. In this paper, we propose a distributed reservation database architecture that features proactive\u00a0\u2026", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Towards a Unified Framework for Composing E-/M-Services\n", "abstract": " This paper discusses an approach to design a framework that enables the composition of services for the benet of users. Services are decomposed into two types: e-services and m-services. Similar to services, users are decomposed into two types: static and mobile. To set up the composition framework, the approach consists of the following steps: understand the features of wireless devices in term of computing, storage, and display capabilities; dene the characteristics of e-services vs. mservices; dene the principles of the framework that composes e-services and m-services; suggest selection criteria between an e-service and mservice; and nally implement the framework using a running example, eg travel planning.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "B2B E-Commerce: Issues and Enabling Technologies\n", "abstract": " \u2013Analysis on what an individual user has liked in the past.\u2013Text mining on the web page contents retrieved by the user in the past to find out major area of interests on news or products.\u2013Web contents that are related to the major area of interests are recommended to users.", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Modeling of dynamic internet transactional workflows\n", "abstract": " This work is covered by copyright. Unless the document is being made available under a Creative Commons Licence, you must assume that re-use is limited to personal use and that permission from the copyright owner must be obtained for all other uses. If the document is available under a Creative Commons License (or other specified license) then refer to the Licence for details of permitted re-use. It is a condition of access that users recognise and abide by the legal requirements associated with these rights. If you believe that this work infringes copyright please provide details by email to qut. copyright@ qut. edu. au", "num_citations": "2\n", "authors": ["2067"]}
{"title": "Evolution de sch\u00e9ma & Adaptation des instances\n", "abstract": " Pascal 001 Exact sciences and technology/001A Sciences and techniques of general use/001A01 Information science. Documentation/001A01F Information retrieval systems. Information and document management system/001A01F01 System design and modelling", "num_citations": "2\n", "authors": ["2067"]}
{"title": "An extensible and reusable pipeline for automated utterance paraphrases\n", "abstract": " In this demonstration paper we showcase an extensible and reusable pipeline for automatic paraphrase generation, i.e., reformulating sentences using different words. Capturing the nuances of human language is fundamental to the effectiveness of Conversational AI systems, as it allows them to deal with the different ways users can utter their requests in natural language. Traditional approaches to utterance paraphrasing acquisition, such as hiring experts or crowdsourcing, involve processes that are often costly or time consuming, and with their own trade-offs in terms of quality. Automatic paraphrasing is emerging as an attractive alternative that promises a fast, scalable and cost-effective process. In this paper we showcase how our extensible and reusable pipeline for automated utterance paraphrasing can support the development of Conversational AI systems by integrating and extending existing techniques under an unified and configurable framework.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Understanding How Early-Stage Researchers Perceive External Research Feedback\n", "abstract": " Feedback on research projects can be important for learning research skills, especially for Early-Stage Researchers (ESRs), who are typically PhD students [Wang and Li 2011]. However, most ESRs get limited feedback from a small circle of advisors, reviewers and peers [Zhang et al. 2017; Gafney 2005]. This is a growing challenge as the number of research students is increasing, while dedicated ondemand feedback is hardly scalable\u2013advisors have limited time and resources to provide in-depth feedback to multiple ESRs [Gafney 2005; Zhang et al. 2017]. Collective intelligence has shown its emerging capability in scaling feedback exchange in various contexts [Jiang et al. 2018; Zhang et al. 2017; Lebeuf et al. 2017; Campbell et al. 2016; Evans et al. 2017; Hui et al. 2019]. For example, researchers described how creative designers and artists seek feedback through online creative communities [Campbell et al. 2016; Evans et al. 2017]; how students get feedback from experts in industry [Silva et al. 2020; Trainer et al. 2017; Harburg et al. 2018; Chen et al. 2017]; and how online forums, such as Q&A platforms, afford on-demand feedback [Ford et al. 2017; Abdalkareem et al. 2017; Choi et al. 2014]. This represents a great opportunity for research community, a sector that is inherently drawing on high levels of good-will and volunteer feedback on research artefacts.Feedback on research projects and activities from large external communities has not yet been properly leveraged and catered for the specific needs of ESRs [Anonymous 2021]. Online crowds are helping some citizen science projects gathering and processing research data [Law et\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Software expert discovery via knowledge domain embeddings in a collaborative network\n", "abstract": " Community Question Answering (CQA) websites can be claimed as the most major venues for knowledge sharing, and the most effective way of exchanging knowledge at present. Considering that massive amount of users are participating online and generating huge amount data, management of knowledge here systematically can be challenging. Expert recommendation is one of the major challenges, as it highlights users in CQA with potential expertise, which may help match unresolved questions with existing high quality answers while at the same time may help external services like human resource systems as another reference to evaluate their candidates. In this paper, we in this work we propose to exploring experts in CQA websites. We take advantage of recent distributed word representation technology to help summarize text chunks, and in a semantic view exploiting the relationships between natural\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "A Unified Knowledge Representation and Context-aware Recommender System in Internet of Things\n", "abstract": " Within the rapidly developing Internet of Things (IoT), numerous and diverse physical devices, Edge devices, Cloud infrastructure, and their quality of service requirements (QoS), need to be represented within a unified specification in order to enable rapid IoT application development, monitoring, and dynamic reconfiguration. But heterogeneities among different configuration knowledge representation models pose limitations for acquisition, discovery and curation of configuration knowledge for coordinated IoT applications. This paper proposes a unified data model to represent IoT resource configuration knowledge artifacts. It also proposes IoT-CANE (Context-Aware recommendatioN systEm) to facilitate incremental knowledge acquisition and declarative context driven knowledge recommendation.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Web Service Composition: Control Flows\n", "abstract": " In this chapter, we present BPEL and BPMN as two main languages of Web service composition. Both BPEL and BPMN allow the codification of control flow logic of a composite service. We will introduce the core syntax elements of the two languages and their usage examples. The lab activities will show how to build a simple BPEL service by composing other services to implement a home loan-processing scenario.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Big Data Analytics Using Cloud and Crowd\n", "abstract": " The increasing application of social and human-enabled systems in people's daily life from one side and from the other side the fast growth of mobile and smart phones technologies have resulted in generating tremendous amount of data, also referred to as big data, and a need for analyzing these data, i.e., big data analytics. Recently a trend has emerged to incorporate human computing power into big data analytics to solve some shortcomings of existing big data analytics such as dealing with semi or unstructured data. Including crowd into big data analytics creates some new challenges such as security, privacy and availability issues. In this paper study hybrid human-machine big data analytics and propose a framework to study these systems from crowd involvement point of view. We identify some open issues in the area and propose a set of research directions for the future of big data analytics area.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Semantics-Based Approach for Dynamic Evolution of Trust Negotiation Protocols in Cloud Collaboration\n", "abstract": " Many techniques for addressing trust negotiation issues is little concerned with managing the dynamic evolution of trust negotiation protocols (policies), particularly in cases where there exist ongoing negotiations when a protocol has been changed. We propose an approach that automatically determines how consequences of changing a protocol affect ongoing negotiations. In particular, our approach allows to capture the semantics and intention of protocol changes, memorize and apply them in effectively analyzing the impact of protocol changes on negotiations.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Detecting, Representing and Querying Collusion in Online Rating Systems 2012-3\n", "abstract": " Online rating systems are subject to malicious behaviors mainly by posting unfair rating scores. Users may try to individually or collaboratively promote or demote a product. Collaborating unfair rating'collusion'is more damaging than individual unfair rating. Although collusion detection in general has been widely studied, identifying collusion groups in online rating systems is less studied and needs more investigation. In this paper, we study impact of collusion in online rating systems and asses their susceptibility to collusion attacks. The proposed model uses a frequent itemset mining algorithm to detect candidate collusion groups. Then, several indicators are used for identifying collusion groups and for estimating how damaging such colluding groups might be. Also, we propose an algorithm for finding possible collusive subgroup inside larger groups which are not identified as collusive. The model has been implemented and we present results of experimental evaluation of our methodology.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Querying e-catalogs using content summaries\n", "abstract": " With the rapid development of e-services on the Web, increasing number of e-catalogs are becoming accessible to users. A large number of e-catalogs provide information about similar type of products/services. To simplify users information searching effort, data integration systems have being developed to integrate e-catalogs providing similar type of information such that users can query those e-catalogs with a mediator through an uniform query interface. The conventional approach to answer a query received by a mediator is to select e-catalogs purely based on their query capabilities, i.e., query interface specifications. However, an e-catalog having the capability to answer a query does not mean it has relevant answers to the query. To remedy the wasted resources of querying catalogs that do not generate an answer, in this paper, we propose to use catalog content summary as a filter and select the\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Towards a scalable algorithm for query rewriting using views in presence of value constraints: Extended version\n", "abstract": " In this paper, we investigate the problem of query rewriting using views in a hybrid language allowing nominals (ie, individual names) to occur in intentional descriptions. Of particular interest, restricted form of nominals where individual names refer to simple values enable the specification of value constraints (ie, sets of allowed values for attributes). Such constraints are very useful in practice enabling, for example, fine-grained description of queries and views in integration systems and thus can be exploited to reduce the query processing cost. We use description logics to formalize the problem of query rewriting using views in presence of value constraints and show that the technique of query rewriting can be used to process queries under the certain answer semantics. We propose a sound and complete query rewriting Bucket-like algorithm that uses data mining techniques and hypergraph framework to favour\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Regulating call blocking of mobile routers in wireless cellular networks\n", "abstract": " Onboard mobile networking is a new communica-tion paradigm where in-vehicle mobile router (MR) connects a large number of passengers to the Internet via cellular (or satellite) networks. All communication to and from the public transport is transparently handled by the MR. In this communication model, a call blocking of the MR would immediately disrupt on-going sessions of potentially large number of onboard passengers. This observation motivates us to investigate techniques that can reduce, and possibly regulate, call blocking probability of MRs. In this paper, we investigate two different techniques to achieve this objective. The first technique, called capacity reservation (CR), reserves a portion of the total base station capacity for exclusive use by the MRs, thus giving them priority over other calls. In the second technique, called probabilistic admission control (PAC), no capacity is reserved, but other on-going\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Towards an approach for coordinating personalized composite services in an environment of mobile users\n", "abstract": " This paper presents an approach for coordinating personalized composite services, which are intended to be offered to mobile users. A composite service is an aggregation of several component services either primitive or composite services. By coordination, it is meant the mechanisms that specify the orchestration of the component services of a composite service. The orchestration concerns the execution chronology of the component services, the data that the component services exchange, the states that the component services take, and the actions that the component services perform. By personalization, it is meant the integration of preferences of users into the specification of the orchestration of the component services. Preferences concern when and where the component services need to be executed. This execution is outsourced to software agents, which consider the respective contexts\u00a0\u2026", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Technologies for E-Services: 4th International Workshop, TES 2003, Berlin, Germany, September 8, 2003, Proceedings\n", "abstract": " E-services, and in particular Web services, are emerging as a promising tech-logy for the e? ective automation of application integration across networks and organizations. The basic technological infrastructure for e-services is structured around three major standards: SOAP, WSDL, and UDDI. These standards p-vide building blocks for service description, discovery, and interaction. E-service technologies have clearly in? uenced positively the development of integrated-stems by providing programmatic access to e-services through SOAP, WSDL, and UDDI. E-services are evolving toward being able to solve critical integ-tion issues including security, transactions, collaborative processes management, semantic aspects, and seamless integration with existing middleware infrastr-tures. VLDB-TES 2003 was the fourth workshop in a successful series of annual workshops on technologies for E-services, held in conjunction with the VLDB conference. The objective of VLDB-TES 2003 was to bring together researchers, practitioners, anduserstoexchangenewideas, developments, andexperienceson issues related to E-services. VLDB-TES 2003 took place in Berlin, Germany. It featured the presentation of 16 regular papers. In addition to the presentation of research papers, the workshop included two invited talks and a panel discussion.", "num_citations": "1\n", "authors": ["2067"]}
{"title": "Suivi d\u2019ex\u00e9cution de services accessibles par l\u2019Internet\n", "abstract": " De nombreuses collaborations inter-organisationnelles s\u2019 appuient aujourd\u2019hui sur le d\u00e9veloppement de nouveaux services accessibles par l\u2019Internet,a partir d\u2019autres d\u00e9ja existants. Ces services composites ainsi obtenus, peuventa leur tour entrer dans d\u2019autres compositions. Les organisations offrant ce type de services, aussi bien que leurs clients ou partenaires, ont besoin de disposer de compte-rendus d\u2019ex\u00e9cutions de ces services pour mesurer leurs performances, expliquer d\u2019\u00e9ventuelles pannes et finalement fournir un support d\u2019aidea la d\u00e9cision. Cet article pr\u00e9sente une approche pour la gestion d\u2019un tel suivi. Les points abord\u00e9s concernent la mod\u00e9lisation et la collection des traces dans un environnement distribu\u00e9, ainsi que l\u2019\u00e9valuation des requ\u00eates portant sur ces traces.", "num_citations": "1\n", "authors": ["2067"]}