{"title": "Process discovery using integer linear programming\n", "abstract": " The research domain of process discovery aims at constructing a process model (e.g. a Petri net) which is an abstract representation of an execution log. Such a Petri net should (1) be able to reproduce the log under consideration and (2) be independent of the number of cases in the log. In this paper, we present a process discovery algorithm where we use concepts taken from the language-based theory of regions, a well-known Petri net research area. We identify a number of shortcomings of this theory from the process discovery perspective, and we provide solutions based on integer linear programming.", "num_citations": "248\n", "authors": ["110"]}
{"title": "Rewriting aggregate queries using views\n", "abstract": " We investigate the problem of rewriting queries with aggregate operators using views that may or may not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum.", "num_citations": "209\n", "authors": ["110"]}
{"title": "Process discovery using integer linear programming\n", "abstract": " The research domain of process discovery aims at constructing a process model (eg a Petri net) which is an abstract representation of an execution log. Such a model should (1) be able to reproduce the log under consideration and (2) be independent of the number of cases in the log. In this paper, we present a process discovery algorithm where we use concepts taken from the language-based theory of regions, a well-known Petri net research area. We identify a number of shortcomings of this theory from the process discovery perspective, and we provide solutions based on integer linear programming.", "num_citations": "186\n", "authors": ["110"]}
{"title": "Process mining software repositories\n", "abstract": " Software developers' activities are in general recorded in software repositories such as version control systems, bug trackers and mail archives. While abundant information is usually present in such repositories, successful information extraction is often challenged by the necessity to simultaneously analyze different repositories and to combine the information obtained. We propose to apply process mining techniques, originally developed for business process analysis, to address this challenge. However, in order for process mining to become applicable, different software repositories should be combined, and \u201crelated\u201d software development events should be matched: e.g., mails sent about a file, modifications of the file and bug reports that can be traced back to it. The combination and matching of events has been implemented in FRASR (Framework for Analyzing Software Repositories), augmenting the process\u00a0\u2026", "num_citations": "148\n", "authors": ["110"]}
{"title": "On negative results when using sentiment analysis tools for software engineering research\n", "abstract": " Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and\u00a0\u2026", "num_citations": "138\n", "authors": ["110"]}
{"title": "Why developers are slacking off: Understanding how software teams use slack\n", "abstract": " Slack is a modern communication platform for teams that is seeing wide and rapid adoption by software develop-ment teams. Slack not only facilitates team messaging and archiving, but it also supports a wide plethora of inte-grations to external services and bots. We have found that Slack and its integrations (ie, bots) are playing an increas-ingly significant role in software development, replacing email in some cases and disrupting software development processes. To understand how Slack impacts development team dynamics, we designed an exploratory study to inves-tigate how developers use Slack and how they benefit from it. We find that developers use Slack for personal, team-wide and community-wide purposes. Our research also reveals that developers use and create diverse integrations (called bots) to support their work. This study serves as the first step towards understanding the role of Slack in sup\u00a0\u2026", "num_citations": "105\n", "authors": ["110"]}
{"title": "Choosing Your Weapons: On Sentiment Analysis Tools for Software Engineering Research\n", "abstract": " Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack\u00a0\u2026", "num_citations": "97\n", "authors": ["110"]}
{"title": "A general framework for automatic termination analysis of logic programs\n", "abstract": " This paper describes a general framework for automatic termination analysis of logic programs, where we understand by \u201ctermination\u201d the finiteness of the LD-tree constructed for the program and a given query. A general property of mappings from a certain subset of the branches of an infinite LD-tree into a finite set is proved. From this result several termination theorems are derived, by using different finite sets. The first two are formulated for the predicate dependency and atom dependency graphs. Then a general result for the case of the query-mapping pairs relevant to a program is proved (cf. [29, 21]). The correctness of the TermiLog system described in [22] follows from it. In this system it is not possible to prove termination for programs involving arithmetic predicates, since the usual order for the integers is not well-founded. A new method, which can be easily incorporated in TermiLog or similar\u00a0\u2026", "num_citations": "86\n", "authors": ["110"]}
{"title": "Challenges for static analysis of java reflection-literature review and empirical study\n", "abstract": " The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research\u00a0\u2026", "num_citations": "85\n", "authors": ["110"]}
{"title": "Theil index for aggregation of software metrics values\n", "abstract": " We propose a new approach to aggregating software metrics from the micro-level of individual artifacts (e.g., methods, classes and packages) to the macro-level of the entire software system. The approach, Theil index, is a well-known econometric measure of inequality. The Theil index allows to study the impact of different categorizations of the artifacts, e.g., based on the development technology or developers' teams, on the inequality of the metrics values measured. We apply the Theil index in a series of experiments. We have observed that the Theil index and the related notions provide valuable insights in organization and evolution of software systems, as well as in sources of inequality.", "num_citations": "79\n", "authors": ["110"]}
{"title": "Nested nets for adaptive systems\n", "abstract": " We consider nested nets, i.e. Petri nets in which tokens can be Petri nets themselves. We study the value semantics of nested nets rather than the reference semantics, and apply nested nets to model adaptive workflow, i.e. flexible workflow that can be modified during the execution. A typical domain with a great need for this kind of workflow is health care, from which domain we choose the running example. To achieve the desired flexibility we allow transitions that create new nets out of the existing ones. Therefore, nets with completely new structure can be created at the run time. We show that by careful selection of basic operations on the nets we can obtain a powerful modeling formalism that enforces correctness of models. Moreover, the formalism can be implemented based on existing workflow engines.", "num_citations": "75\n", "authors": ["110"]}
{"title": "You Can\u2019t Control the Unfamiliar: A Study on the Relations Between Aggregation Techniques for Software Metrics\n", "abstract": " A popular approach to assessing software maintainability and predicting its evolution involves collecting and analyzing software metrics. However, metrics are usually defined on a micro-level (method, class, package), and should therefore be aggregated in order to provide insights in the evolution at the macro-level (system). In addition to traditional aggregation techniques such as the mean, median, or sum, recently econometric aggregation techniques, such as the Gini, Theil, Kolm, Atkinson, and Hoover inequality indices have been proposed and applied to software metrics. In this paper we present the results of an extensive correlation study of the most widely-used traditional and econometric aggregation techniques, applied to lifting SLOC values from class to package level in the 106 systems comprising the Qualitas Corpus. Moreover, we investigate the nature of this relation, and study its evolution on a subset\u00a0\u2026", "num_citations": "68\n", "authors": ["110"]}
{"title": "Soundness of resource-constrained workflow nets\n", "abstract": " We study concurrent processes modelled as workflow Petri nets extended with resource constraints. We define a behavioural correctness criterion called soundness: given a sufficient initial number of resources, all cases in the net are guaranteed to terminate successfully, no matter which schedule is used. We give a necessary and sufficient condition for soundness and an algorithm that checks it.", "num_citations": "66\n", "authors": ["110"]}
{"title": "EquiX\u2014Easy Querying in XML Databases\n", "abstract": " The Web is explored by many users, but only a few of them have experience in using query languages. Thus, one of the greatest challenges, provided by XML, is to create a query language simple enough for the naive user. In this paper, we present EquiX| a powerful and easy to use query language for XML. The main goal in designing EquiX is to strike the right balance between expressive power and simplicity.EquiX has a form-based GUI that is constructed automatically from the DTDs of XML documents. Query forms are built from well known HTML primitives. The result of a query in EquiX is a collection of XML documents, and it is automatically generated from the query without explicit speci cation of the format of the result. Knowledge of XML syntax is not required in order to use EquiX. Yet, EquiX is able to express rather complicated queries, containing quanti cation, negation and aggregation.", "num_citations": "65\n", "authors": ["110"]}
{"title": "Functional and cost-based automatic generator for hybrid vehicles topologies\n", "abstract": " The energy efficiency of a hybrid electric vehicle is dictated by the topology (coupling option of power sources/sinks), choice (technology), and control of components. The first design area among these, the topology, has the biggest flexibility of them all, yet, so far in the literature, the topology design is limited investigated due to its high complexity. In practice, a predefined small set of topologies is used to optimize their energy efficiency by varying the power specifications of the main components (sizing). By doing so, the complete design of the vehicle is, inherently and to a certain extent, suboptimal. Moreover, various complex topologies appear on the automotive market and no tool exists to optimally choose or evaluate them. To overcome this design limitation, in this paper, a novel framework is presented that deals with the automatic generation of possible topologies given a set of components (e.g., engine, electric\u00a0\u2026", "num_citations": "64\n", "authors": ["110"]}
{"title": "Code of conduct in open source projects\n", "abstract": " Open source projects rely on collaboration of members from all around the world using web technologies like GitHub and Gerrit. This mixture of people with a wide range of backgrounds including minorities like women, ethnic minorities, and people with disabilities may increase the risk of offensive and destroying behaviours in the community, potentially leading affected project members to leave towards a more welcoming and friendly environment. To counter these effects, open source projects increasingly are turning to codes of conduct, in an attempt to promote their expectations and standards of ethical behaviour. In this first of its kind empirical study of codes of conduct in open source software projects, we investigated the role, scope and influence of codes of conduct through a mixture of quantitative and qualitative analysis, supported by interviews with practitioners. We found that the top codes of conduct are\u00a0\u2026", "num_citations": "62\n", "authors": ["110"]}
{"title": "Survey of approaches for handling static analysis alarms\n", "abstract": " Static analysis tools have showcased their importance and usefulness in automated detection of code anomalies and defects. However, the large number of alarms reported and cost incurred in their manual inspections have been the major concerns with the usage of static analysis tools. Existing studies addressing these concerns differ greatly in their approaches to handle the alarms, varying from automatic postprocessing of alarms, supporting the tool-users during manual inspections of the alarms, to designing of light-weight static analysis tools. A comprehensive study of approaches for handling alarms is, however, not found. In this paper, we review 79 alarms handling studies collected through a systematic literature search and classify the approaches proposed into seven categories. The literature search is performed by combining the keywords-based database search and snowballing. Our review is intended\u00a0\u2026", "num_citations": "54\n", "authors": ["110"]}
{"title": "Algorithms for rewriting aggregate queries using views\n", "abstract": " Queries involving aggregation are typical in database applications. One of the main ideas to optimize the execution of an aggregate query is to reuse results of previously answered queries. This leads to the problem of rewriting aggregate queries using views. Due to a lack of theory, algorithms for this problem were rather ad-hoc. They were sound, but were not proven to be complete.               Recently we have given syntactic characterizations for the equivalence of aggregate queries and applied them to decide when there exist rewritings. However, these decision procedures do not lend themselves immediately to an implementation. In this paper, we present practical algorithms for rewriting queries with count and sum. Our algorithms are sound. They are also complete for important cases. Our techniques can be used to improve well-known procedures for rewriting non-aggregate queries. These procedures\u00a0\u2026", "num_citations": "47\n", "authors": ["110"]}
{"title": "TermiLog: A system for checking termination of queries to logic programs\n", "abstract": " TermiLog is a system implemented in SICStus Prolog for automatically checking termination of queries to logic programs. Given a program and query, the system either answers that the query terminates or that it cannot prove termination. The system can handle automatically 82% of the 120 programs we tested it on.", "num_citations": "45\n", "authors": ["110"]}
{"title": "Simulink models are also software: Modularity assessment\n", "abstract": " In automotive industry, more and more complex electronics and software systems are being developed to enable the innovation and to decrease costs. Besides the complex multimedia, comfort, and safety systems of conventional vehicles, automotive companies are required to develop more and more complex engine, aftertreatment, and energy management systems for their (hybrid) electric vehicles to reduce fuel consumption and harmful emissions. MATLAB/Simulink is one of the most popular graphical modeling languages and a simulation tool for validating and testing control software systems. Due to the increasing complexity and size of Simulink models of automotive software systems, it has become a necessity to maintain the Simulink models.", "num_citations": "42\n", "authors": ["110"]}
{"title": "Empirical analysis of the relationship between CC and SLOC in a large corpus of Java methods\n", "abstract": " Measuring the internal quality of source code is one of the traditional goals of making software development into an engineering discipline. Cyclomatic Complexity (CC) is an often used source code quality metric, next to Source Lines of Code (SLOC). However, the use of the CC metric is challenged by the repeated claim that CC is redundant with respect to SLOC due to strong linear correlation. We test this claim by studying a corpus of 17.8M methods in 13K open-source Java projects. Our results show that direct linear correlation between SLOC and CC is only moderate, as caused by high variance. We observe that aggregating CC and SLOC over larger units of code improves the correlation, which explains reported results of strong linear correlation in literature. We suggest that the primary cause of correlation is the aggregation. Our conclusion is that there is no strong linear correlation between CC and SLOC of\u00a0\u2026", "num_citations": "41\n", "authors": ["110"]}
{"title": "Recognizing gender of stack overflow users\n", "abstract": " Software development remains a predominantly male activity, despite coordinated efforts from research, industry, and policy makers. This gender imbalance is most visible in social programming, on platforms such as Stack Overflow.", "num_citations": "39\n", "authors": ["110"]}
{"title": "EquiX\u2014a search and query language for XML\n", "abstract": " EquiX is a search language for XML that combines the power of querying with the simplicity of searching. Requirements for such languages are discussed, and it is shown that EquiX meets the necessary criteria. Both a graph\u2010based abstract syntax and a formal concrete syntax are presented for EquiX queries. In addition, the semantics is defined and an evaluation algorithm is presented. The evaluation algorithm is polynomial under combined complexity. EquiX combines pattern matching, quantification, and logical expressions to query both the data and meta\u2010data of XML documents. The result of a query in EquiX is a set of XML documents. A DTD describing the result documents is derived automatically from the query.", "num_citations": "38\n", "authors": ["110"]}
{"title": "Mining student capstone projects with FRASR and ProM\n", "abstract": " Capstone projects are commonly carried out at the end of an undergraduate program of study in software engineering or computer science. While traditionally such projects solely focussed on the software product to be developed, in more recent work importance of the development process has been stressed. Currently process quality assessment techniques are limited to review of intermediary artifacts, self-and peer evaluations. We advocate augmenting the assessment by mining software repositories used by the students during the development. We present the assessment methodology and illustrate it by applying to a number of software engineering capstone projects.", "num_citations": "37\n", "authors": ["110"]}
{"title": "SQuAVisiT: A flexible tool for visual software analytics\n", "abstract": " We present the Software Quality Assessment and Visualization Toolset (SQuAVisiT), a flexible tool for visual software analytics. Visual software analytics supports analytical reasoning about software systems facilitated by interactive visual interfaces. In particular, SQuAVisiT assists software developers, maintainers and assessors in performing quality assurance and maintenance tasks. Flexibility of SQuAVisiT allows for integration of multiple programming languages and variety of analysis and visualization tools.SQuAVisiT has been successfully applied in a number of case studies, ranging from hundreds to thousands KLOC, from homogeneous to heterogeneous systems.", "num_citations": "29\n", "authors": ["110"]}
{"title": "Adaptive workflows for healthcare information systems\n", "abstract": " Current challenges in Healthcare Information Systems (HIS) include supplying patients with personalized medical information, creating means for efficient information flow between different healthcare providers in order to lower risks of medical errors and increase the quality of care. To address these challenges, the information about patient-related processes, such as currently executed medical protocols, should be made available for medical staff and patients. Existing HIS are mostly data-centered, and therefore cannot provide an adequate solution. To give processes a prominent role in HIS, we apply the adaptive workflow nets framework. This framework allows both healthcare providers and patients to get an insight into the past and current processes, but also foresee possible future developments. It also ensures quality and timing of data communication essential for efficient information flow.", "num_citations": "29\n", "authors": ["110"]}
{"title": "RTTool: A tool for extracting relative thresholds for source code metrics\n", "abstract": " Meaningful thresholds are essential for promoting source code metrics as an effective instrument to control the internal quality of software systems. Despite the increasing number of source code measurement tools, no publicly available tools support extraction of metric thresholds. Moreover, earlier studies suggest that in larger systems significant number of classes exceed recommended metric thresholds. Therefore, in our previous study we have introduced the notion of a relative threshold, i.e., a pair including an upper limit and a percentage of classes whose metric values should not exceed this limit. In this paper we propose RTTOOL, an open source tool for extracting relative thresholds from the measurement data of a benchmark of software systems. RTTOOL is publicly available at http://aserg.labsoft.dcc.ufmg.br/rttool.", "num_citations": "28\n", "authors": ["110"]}
{"title": "Checking properties of adaptive workflow nets\n", "abstract": " In this paper we consider adaptive workflow nets, a class of nested nets that allows more comfort and expressive power for modeling adaptability and exception handling in workflow nets. We define two important behavioural properties of adaptive workflow nets: soundness and circumspectness. Soundness means that a proper final marking (state) can be reached from any marking which is reachable from the initial marking, and no garbage will be left. Circumspectness means that the upper layer is always ready to handle any exception that can happen in a lower layer. We define a finite state abstraction for adaptive workflow nets and show that soundness and circumspectness can be verified on this abstraction.", "num_citations": "28\n", "authors": ["110"]}
{"title": "Empirical analysis of the relationship between CC and SLOC in a large corpus of Java methods and C functions\n", "abstract": " Measuring the internal quality of source code is one of the traditional goals of making software development into an engineering discipline. Cyclomatic complexity (CC) is an often used source code quality metric, next to source lines of code (SLOC). However, the use of the CC metric is challenged by the repeated claim that CC is redundant with respect to SLOC because of strong linear correlation. We conducted an extensive literature study of the CC/SLOC correlation results. Next, we tested correlation on large Java (17.6\u2002M methods) and C (6.3\u2002M functions) corpora. Our results show that linear correlation between SLOC and CC is only moderate as a result of increasingly high variance. We further observe that aggregating CC and SLOC as well as performing a power transform improves the correlation. Our conclusion is that the observed linear correlation between CC and SLOC of Java methods or C functions\u00a0\u2026", "num_citations": "27\n", "authors": ["110"]}
{"title": "Recurrence with affine level mappings is p-time decidable for clp(r)\n", "abstract": " In this paper we introduce a class of constraint logic programs such that their termination can be proved by using affine level mappings. We show that membership to this class is decidable in polynomial time.", "num_citations": "26\n", "authors": ["110"]}
{"title": "On termination of meta-programs\n", "abstract": " The term meta-programming refers to the ability of writing programs that have other programs as data and exploit their semantics. The aim of this paper is presenting a methodology allowing us to perform a correct termination analysis for a broad class of practical meta-interpreters, including negation and performing different tasks during the execution. It is based on combining the power of general orderings, used in proving termination of term-rewrite systems and programs, and on the well-known acceptability condition, used in proving termination of logic programs. The methodology establishes a relationship between the ordering needed to prove termination of the interpreted program and the ordering needed to prove termination of the meta-interpreter together with this interpreted program. If such a relationship is established, termination of one of those implies termination of the other one, i.e. the meta-interpreter\u00a0\u2026", "num_citations": "26\n", "authors": ["110"]}
{"title": "Assessing and improving quality of QVTo model transformations\n", "abstract": " We investigate quality improvement in QVT operational mappings (QVTo) model transformations, one of the languages defined in the OMG standard on model-to-model transformations. Two research questions are addressed. First, how can we assess quality of QVTo model transformations? Second, how can we develop higher-quality QVTo transformations? To address the first question, we utilize a bottom\u2013up approach, starting with a broad exploratory study including QVTo expert interviews, a review of existing material, and introspection. We then formalize QVTo transformation quality into a QVTo quality model. The quality model is validated through a survey of a broader group of QVTo developers. We find that although many quality properties recognized as important for QVTo do have counterparts in general purpose languages, a number of them are specific to QVTo or model transformation languages\u00a0\u2026", "num_citations": "24\n", "authors": ["110"]}
{"title": "How remote work can foster a more inclusive environment for transgender developers\n", "abstract": " In this position paper, we claim that remote work offers a mechanism of control for identity disclosure and empowerment of software developers from marginalized communities. By talking to several transgender software developers we identified three themes that resonate across the trans experience and intersect with the advantages to working in software development remotely: identity disclosure, high-impact technical work and the autonomy to disengage and re-engage. Based on these themes we identify several open questions that the research community should address.", "num_citations": "23\n", "authors": ["110"]}
{"title": "Traceability visualization in model transformations with tracevis\n", "abstract": " Model transformations are commonly used to transform models suited for one purpose (e.g., describing a solution in a particular domain) to models suited for a related but different purpose (e.g., simulation or execution). The disadvantage of a transformational approach, however, is that feedback acquired from analyzing transformed models is not reported on the level of the problem domain but on the level of the transformed model. Expressing the feedback on the level of the problem domain requires improving traceability in model transformations.               We propose to visualize traceability links in (chains of) model transformations, thus making traceability amenable for analysis.", "num_citations": "23\n", "authors": ["110"]}
{"title": "Beyond the code itself: how programmers really look at pull requests\n", "abstract": " Developers in open source projects must make decisions on contributions from other community members, such as whether or not to accept a pull request. However, secondary factors-beyond the code itself-can influence those decisions. For example, signals from GitHub profiles, such as a number of followers, activity, names, or gender can also be considered when developers make decisions. In this paper, we examine how developers use these signals (or not) when making decisions about code contributions. To evaluate this question, we evaluate how signals related to perceived gender identity and code quality influenced decisions on accepting pull requests. Unlike previous work, we analyze this decision process with data collected from an eye-tracker. We analyzed differences in what signals developers said are important for themselves versus what signals they actually used to make decisions about others\u00a0\u2026", "num_citations": "22\n", "authors": ["110"]}
{"title": "A Data Set of OCL Expressions on GitHub\n", "abstract": " In model driven engineering (MDE), meta-models are the central artifacts. As a complement, the Object Constraint Language (OCL) is a language used to express constraints and operations on meta-models. The Eclipse Modeling Framework (EMF) provides an implementation of OCL, enabling OCL-annotated meta-models. Existing empirical studies of the OCL have been conducted on small collections of data. To facilitate empirical research into the OCL on a larger scale, we present the first publicly available data set of OCL expressions. The data set contains 9188 OCL expressions originating from 504 EMF meta-models in 245 systematically selected GitHub repositories. Both the original meta-models and the generated abstract syntax trees are included, allowing for a variety of empirical studies of the OCL. To illustrate the applicability of this data set in practice, we performed three case studies.", "num_citations": "21\n", "authors": ["110"]}
{"title": "I2SD: Reverse Engineering Sequence Diagrams from Enterprise Java Beans with Interceptors\n", "abstract": " An Enterprise JavaBeans (EJB) interceptor is a software mechanism that provides for introducing behaviour implemented as separate code into the execution of a Java application. In this way, EJB interceptors provide a clear separation of the core functionality of the bean and other concerns, such as logging or performance analysis. Despite the beauty of the idea behind the i nterceptors, developing, testing and managing dependencies introduced by the interceptors are considered to be daunting tasks. For example, the developers can specify interceptors at multiple locations and by multiple means. However, different locations and specification means influence the order of the interceptor invocation, which is governed by more than 15 different intertwined rules defined in the EJB standard. To facilitate development of EJB applications, we have designed I2SD, Interceptors to Sequence Diagrams, a tool for reverse\u00a0\u2026", "num_citations": "21\n", "authors": ["110"]}
{"title": "How do scratch programmers name variables and procedures?\n", "abstract": " Research shows the importance of selecting good names to identifiers in software code: more meaningful names improve readability. In particular, several guidelines encourage long and descriptive variable names. A recent study analyzed the use of variable names in five programming languages, focusing on single-letter variable names, because of the apparent contradiction between their frequent use and the fact that these variables violate the aforementioned guidelines. In this paper, we analyze variables in Scratch, a popular block-based language aimed at children. We start by replicating the above single-letter study for Scratch. We augment this study by analyzing single-letter procedure names, and by investigating the use of Scratch specific naming patterns: spaces in variable names, numerics as variables and textual labels in procedure names. The results of our analysis show that Scratch programmers\u00a0\u2026", "num_citations": "20\n", "authors": ["110"]}
{"title": "Code generation with templates\n", "abstract": " Templates are used to generate all kinds of text, including computer code. The last decade, the use of templates gained a lot of popularity due to the increase of dynamic web applications. Templates are a tool for programmers, and implementations of template engines are most times based on practical experience rather than based on a theoretical background. This book reveals the mathematical background of templates and shows interesting findings for improving the practical use of templates. First, a framework to determine the necessary computational power for the template metalanguage is presented. The template metalanguage does not need to be Turing-complete to be useful. A non-Turing-complete metalanguage enforces separation of concerns between the view and model. Second, syntactical correctness of all languages of the templates and generated code is ensured. This includes the syntactical correctness of the template metalanguage and the output language. Third, case studies show that the achieved goals are applicable in practice. It is even shown that syntactical correctness helps to prevent cross-site scripting attacks in web applications. The target audience of this book is twofold. The first group exists of researcher interested in the mathematical background of templates. The second group exists of users of templates. This includes designers of template engines on one side and programmers and web designers using templates on the other side", "num_citations": "20\n", "authors": ["110"]}
{"title": "Dn-based architecture assessment of Java Open Source software systems\n", "abstract": " Since their introduction in 1994 the Martin's metrics became popular in assessing object-oriented software architectures. While one of the Martin metrics, normalised distance from the main sequence D n , has been originally designed with assessing individual packages, it has also been applied to assess quality of entire software architectures. The approach itself, however, has never been studied. In this paper we take the first step to formalising the D n -based architecture assessment of Java open source software. We present two aggregate measures: average normalised distance from the main sequence Dmacr n , and parameter of the fitted statistical model lambda. Applying these measures to a carefully selected collection of benchmarks we obtain a set of reference values that can be used to assess quality of a system architecture. Furthermore, we show that applying the same measures to different versions of\u00a0\u2026", "num_citations": "20\n", "authors": ["110"]}
{"title": "Acceptability with general orderings\n", "abstract": " We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but we apply these orderings directly to the logic program and not to the term-rewrite system obtained through some transformation. We define some variants of acceptability, based on general orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants.               The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple orderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.", "num_citations": "20\n", "authors": ["110"]}
{"title": "Improving prolog programs: Refactoring for prolog\n", "abstract": " Refactoring is an established technique from the object-oriented (OO) programming community to restructure code: it aims at improving software readability, maintainability, and extensibility. Although refactoring is not tied to the OO-paradigm in particular, its ideas have not been applied to logic programming until now. This paper applies the ideas of refactoring to Prolog programs. A catalogue is presented listing refactorings classified according to scope. Some of the refactorings have been adapted from the OO-paradigm, while others have been specifically designed for Prolog. The discrepancy between intended and operational semantics in Prolog is also addressed by some of the refactorings. In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and the experience with applying ViPReSS to a large Prolog legacy system is reported. The main conclusion is that refactoring is both a viable\u00a0\u2026", "num_citations": "19\n", "authors": ["110"]}
{"title": "Unfolding the mystery of mergesort\n", "abstract": " This paper outlines a fully automatic transformation for proving termination of queries to a class of logic programs, for which existing methods do not work directly. The transformation consists of creating adorned clauses and unfolding. In general, the transformation may improve termination behavior by pruning infinite branches from the LD-tree. Conditions are given under which the transformation preserves termination behavior. The work presented here has been done in the framework of the TermiLog system, and it complements the algorithms described in [12].", "num_citations": "18\n", "authors": ["110"]}
{"title": "Tailoring complexity metrics for simulink models\n", "abstract": " The size and complexity of Simulink models is constantly increasing, just as the systems which they represent. Therefore, it is beneficial to control them already at the design phase. In this paper we establish a set of complexity metrics for Simulink models to capture diverse aspects of complexity by proposing new and redefining existing metrics. To evaluate the applicability of our metrics, we compare them with the closed-source metric proposed by Mathworks. Moreover, through a case study from the automotive domain, we relate such metrics to quality attributes as determined by domain experts, and correlate them to known faults. Preliminary assessment suggests that complexity is closely related to analysability, understandability, and testability.", "num_citations": "17\n", "authors": ["110"]}
{"title": "An empirical perspective on security challenges in large-scale agile software development\n", "abstract": " Background Agile methods have been shown to have a negative impact on security. Several studies have investigated challenges in aligning security practices with agile methods, however, none of these have examined security challenges in the context of large-scale agile. Large-scale agile can present unique challenges, as large organizations often involve highly interdependent teams that need to align with other (non-agile) departments. Goal Our objective is to identify security challenges encountered in large-scale agile software development from the perspective of agile practitioners. Method Cooperative Method Development is applied to guide a qualitative case study at Rabobank, a Dutch multinational banking organization. A total of ten interviews is conducted with members in different agile roles from five different agile development teams. Data saturation has been obtained. By open card sorting we\u00a0\u2026", "num_citations": "16\n", "authors": ["110"]}
{"title": "STRESS: A Semi-Automated, Fully Replicable Approach for Project Selection\n", "abstract": " The mining of software repositories has provided significant advances in a multitude of software engineering fields, including defect prediction. Several studies show that the performance of a software engineering technology (e.g., prediction model) differs across different project repositories. Thus, it is important that the project selection is replicable. The aim of this paper is to present STRESS, a semi-automated and fully replicable approach that allows researchers to select projects by configuring the desired level of diversity, fit, and quality. STRESS records the rationale behind the researcher decisions and allows different users to re-run or modify such decisions. STRESS is open-source and it can be used used locally or even online (www.falessi.com/STRESS/). We perform a systematic mapping study that considers studies that analyzed projects managed with JIRA and Git to asses the project selection replicability\u00a0\u2026", "num_citations": "16\n", "authors": ["110"]}
{"title": "A complete operator library for DSL evolution specification\n", "abstract": " Domain-specific languages (DSLs) allow users to model systems using concepts from a specific domain. Evolution of DSLs triggers co-evolution of models developed in these languages. Manual co-evolution of the thousands of models is unfeasible, calling for an automated support. A prerequisite to automating model co-evolution with respect to DSL evolution is the ability to formally specify DSL evolution, e.g., using predefined evolution operators. Success or failure of the practical application of the operator-based approach therefore depends heavily on the operators offered by the operator library at hand. In this paper we evaluate the completeness of the state-of-the-art operator library claimed to be \"practically complete\" (which we denote as H) by using it to specify evolution of an ecosystem of 22 commercial DSLs over the period of four years. We observe that 11% of the changes cannot be specified. However\u00a0\u2026", "num_citations": "16\n", "authors": ["110"]}
{"title": "Automotive ADLs: a study on enforcing consistency through multiple architectural levels\n", "abstract": " Over the last decade, Architecture Description Languages (ADLs) are attracting considerable attention by automotive companies because they consider them as one of the key solutions to improve the quality of automotive electronic and software systems. Automotive ADLs like EAST-ADL, AADL, TADL, and AML are being defined to address not only the architectural description or the representation issues but also as a method to enable requirements traceability and early analysis of a system. Besides the automotive specific ADLs, SysML and MARTE are emerging as viable modeling approaches for automotive systems engineering domain as well. However, all these modeling approaches lack the capability of ensuring the architectural quality. This paper identifies an architectural inconsistency between the different architectural levels as one of the key issues regarding architectural quality of automotive systems\u00a0\u2026", "num_citations": "16\n", "authors": ["110"]}
{"title": "History-based joins: Semantics, soundness and implementation\n", "abstract": " In this paper we study the use of case history for control structures in workflow processes. In particular we introduce a history-dependent join. History dependent control offers much more modeling power than classical control structures and it solves several semantical problems of industrial modeling frameworks. We study the modeling power by means of workflow patterns. Since proper completion (i.e. the ability of any configuration reachable from the initial one to reach the final one) is always an important \u201dsanity check\u201d of process modeling, we introduce a modeling method that guarantees this property for the new control structures. Finally we consider an implementation of the proposed control structures on top of an existing workflow engine.", "num_citations": "16\n", "authors": ["110"]}
{"title": "Combining the power of searching and querying\n", "abstract": " EquiX is a search language for XML that combines the power of querying with the simplicity of searching. Requirements for search languages are discussed and it is shown that EquiX meets the necessary criteria. Both a graphical abstract syntax and a formal concrete syntax are presented for EquiX queries. In addition, the semantics is defined. It is shown that EquiX has an evaluation algorithm that is polynomial under combined complexity.               EquiX combines pattern matching, quantification and logical expressions to query both the data and meta-data of XML documents. The result of a query in EquiX is a set of XML documents. A DTD describing the result documents is derived automatically from the query.", "num_citations": "16\n", "authors": ["110"]}
{"title": "Automated Analyses of Model-Driven Artifacts\n", "abstract": " Over the past years, there has been an increase in the application of model driven engineering in industry. Similar to traditional software engineering, understanding how technologies are actually used in practice is essential for developing good tooling, and decision making processes. Unfortunately, obtaining and analyzing empirical data in a model-driven context is still tedious and time consuming, introducing large lead-times. In this paper we present a framework for the automated extraction, analysis, and visualization of data and metrics on model-driven artifacts. We subsequently present various examples of how the framework was successfully applied in a large industrial setting to answer a plethora of different questions with respect to decision making and tool development.", "num_citations": "15\n", "authors": ["110"]}
{"title": "Maintenance of specification models in industry using edapt\n", "abstract": " Domain specific languages (DSLs) ease the adoption of formal specification in industry. They allow developers to describe their specification models in concepts of their domain. However, DSLs evolve over time, causing specification models to have to co-evolve to reflect the evolution in the DSL. The maintenance overhead introduced by these, often manual, changes to specification models threatens to overshadow the advantages of DSL usage in industry. To this extent, many approaches have been proposed in the literature to facilitate DSL maintenance by automating model co-changes. In this paper, we evaluate the ability of a tool, Edapt, to support the change and co-change in twenty-two industrial DSLs and corresponding specification models over a maintenance period of four years. We observe that the tool is only able to automatically co-change specification models for 72% of the DSL changes. To address\u00a0\u2026", "num_citations": "15\n", "authors": ["110"]}
{"title": "Inference of termination conditions for numerical loops in Prolog\n", "abstract": " We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows overcoming these difficulties. Our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows us to perform a correct analysis of such computations automatically, by extending previous work on a constraint-based approach to termination. Finally, we discuss possible extensions of the technique, including incorporating general term orderings.", "num_citations": "15\n", "authors": ["110"]}
{"title": "Proving termination for logic programs by the query-mapping pairs approach\n", "abstract": " This paper describes a method for proving termination of queries to logic programs based on abstract interpretation. The method uses query-mapping pairs to abstract the relation between calls in the LD-tree associated with the program and query. Any well founded partial order for terms can be used to prove the termination. The ideas of the query-mapping pairs approach have been implemented in SICStus Prolog in a system called TermiLog, which is available on the web. Given a program and query pattern the system either answers that the query terminates or that there may be non-termination. The advantages of the method are its conceptual simplicity and the fact that it does not impose any restrictions on the programs.", "num_citations": "15\n", "authors": ["110"]}
{"title": "Improving model inference in industry by combining active and passive learning\n", "abstract": " Inferring behavioral models (e.g., state machines) of software systems is an important element of re-engineering activities. Model inference techniques can be categorized as active or passive learning, constructing models by (dynamically) interacting with systems or (statically) analyzing traces, respectively. Application of those techniques in the industry is, however, hindered by the trade-off between learning time and completeness achieved (active learning) or by incomplete input logs (passive learning). We investigate the learning time/completeness achieved trade-off of active learning with a pilot study at ASML, provider of lithography systems for the semiconductor industry. To resolve the trade-off we advocate extending active learning with execution logs and passive learning results. We apply the extended approach to eighteen components used in ASML TWINSCAN lithography machines. Compared to\u00a0\u2026", "num_citations": "14\n", "authors": ["110"]}
{"title": "DSL/Model Co-Evolution in Industrial EMF-Based MDSE Ecosystems\n", "abstract": " Model Driven Engineering and Domain Specific Languages (DSLs) are being used in industry to increase productivity, and enable novel techniques like virtual prototyping. Using DSLs, engineers can model a systems in terms of their domain, rather than encoding it in general purpose concepts, like those offered by UML. However, DSLs evolve over time, often in a non-backwards-compatible way with respect to their models. When this happens, models need to be coevolved to remain usable. Because the number of models in an industrial setting grows so large, manual co-evolution is becoming unfeasible calling for an automated approach. Many approaches for automated co-evolution of models with respect to their DSLs exist in literature, each operating in a highly specialized context. In this paper, we present a highlevel architecture that tries to capture the general process needed for automated co-evolution of models in response to DSL evolution, and assess which challenges are still open.", "num_citations": "14\n", "authors": ["110"]}
{"title": "Analysing the BKE-security protocol with \u03bcCRL\n", "abstract": " The \u03bcCRL language and its corresponding tool set form a powerful methodology for the verification of distributed systems. We demonstrate its use by applying it to the well-known Bilateral Key Exchange security protocol.", "num_citations": "14\n", "authors": ["110"]}
{"title": "Emotional Labor of Software Engineers.\n", "abstract": " The concept of emotional labor, introduced by Hochschild in 1983, refers to the \u201cprocess by which workers are expected to manage their feelings in accordance with organizationally defined rules and guidelines\u201d. For instance, judges are expected to appear impartial, nurses\u2014compassionate and police officers\u2014authoritative. While software development has been traditionally stereotyped as a nerdy \u201clone wolf\u201d job less likely to induce emotional labor, nowadays software developers become more and more social, on the one hand, and are subject to increasing amount of behavioral expectations, eg, formulated as codes of conduct.In this position paper we stress that software developers are subject to emotional labor, envision how emotional labor can be identified based on emotion detection techniques applied in software engineering, suggest possible antecedents and consequents of emotional labor and discuss interventions that can be designed to address the challenges of emotional labor.", "num_citations": "13\n", "authors": ["110"]}
{"title": "History-dependent Petri nets\n", "abstract": " Most information systems that are driven by process models (e.g., workflow management systems) record events in event logs, also known as transaction logs or audit trails. We consider processes that not only keep track of their history in a log, but also make decisions based on this log. To model such processes we extend the basic Petri net framework with the notion of history and add guards to transitions evaluated on the process history. We show that some classes of history-dependent nets can be automatically converted to classical Petri nets for analysis purposes. These classes are characterized by the form of the guards (e.g., LTL guards) and sometimes the additional requirement that the underlying classical Petri net is either bounded or has finite synchronization distances.", "num_citations": "13\n", "authors": ["110"]}
{"title": "Inference of termination conditions for numerical loops in Prolog\n", "abstract": " Numerical computations form an essential part of almost any real-world program. Clearly, in order for a termination analyser to be of practical use it should contain a mechanism for inferring termination of such computations. However, this topic attracted less attention of the research community. In this work we concentrate on automatic termination inference for logic programs depending on numerical computations. Dershowitz et al. [8] showed that termination of general numerical computations, for instance on floating point numbers, may be counter-intuitive, i.e., the observed behaviour does not necessarily coincide with the theoretically expected one. Thus, we restrict ourselves to integer computations only.", "num_citations": "13\n", "authors": ["110"]}
{"title": "Improving software maintenance using process mining and predictive analytics\n", "abstract": " This research focuses on analyzing and improving maintenance process by exploring novel applications of process mining and predictive analytics. We analyze the software maintenance process by applying process mining on software repositories, and address the identified inefficiencies using predictive analytics. To drive our research, we engage with practitioners from large, global IT companies and emphasize on the practical usability of the proposed solution approaches, which are evaluated by conducting a series of case studies on open source and commercial projects.", "num_citations": "12\n", "authors": ["110"]}
{"title": "Scheduling-free resource management\n", "abstract": " We investigate a resource management policy that allocates resources based on the number of available resources only. We formulate a condition on resource requesting processes, called solidity that guarantees successful termination. Processes that do not satisfy this condition can be modified to become solid. We investigate performance of the resource management policy proposed by comparing it to the theoretically found optimum for the special case. Our method can be applied in, for example, project management systems, orchestration software and workflow engines.", "num_citations": "12\n", "authors": ["110"]}
{"title": "Termination of floating-point computations\n", "abstract": " Numerical computations form an essential part of almost any real-world program. Traditional approaches to termination of logic programs are restricted to domains isomorphic to (\u2115,>); more recent works study termination of integer computations where the lack of well-foundedness of the integers has to be taken into account. Termination of computations involving floating-point numbers can be counterintuitive because of rounding errors and implementation conventions. We present a novel technique that allows us to prove termination of such computations. Our approach extends the previous work on termination of integer computations.", "num_citations": "12\n", "authors": ["110"]}
{"title": "Improving prolog programs: Refactoring for prolog\n", "abstract": " Refactoring is an established technique from the OO-community to restructure code: it aims at improving software readability, maintainability and extensibility. Although refactoring is not tied to the OO-paradigm in particular, its ideas have not been applied to Logic Programming until now.                 This paper applies the ideas of refactoring to Prolog programs. A catalogue is presented listing refactorings classified according to scope. Some of the refactorings have been adapted from the OO-paradigm, while others have been specifically designed for Prolog. Also the discrepancy between intended and operational semantics in Prolog is addressed by some of the refactorings.                 In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and the experience with applying ViPReSS to a large Prolog legacy system is reported. Our main conclusion is that refactoring is not only\u00a0\u2026", "num_citations": "12\n", "authors": ["110"]}
{"title": "Runtime monitoring in continuous deployment by differencing execution behavior model\n", "abstract": " Continuous deployment techniques support rapid deployment of new software versions. Usually a new version is deployed on a limited scale, its behavior is monitored and compared against the previously deployed version and either the deployment of the new version is broadened, or one reverts to the previous version. The existing monitoring approaches, however, do not capture the differences in the execution behavior between the new and the previously deployed versions.                 We propose an approach to automatically discover execution behavior models for the deployed and the new version using the execution logs. Differences between the two models are identified and enriched such that spurious differences, e.g., due to logging statement modifications, are mitigated. The remaining differences are visualized as cohesive diff regions within the discovered behavior model, allowing one to\u00a0\u2026", "num_citations": "11\n", "authors": ["110"]}
{"title": "Requirements certification for offshoring using LSPCM\n", "abstract": " Requirements hand-over is a common practice in software development off shoring. Cultural and geographical distance between the outsourcer and supplier, and the differences in development practices hinder the communication and lead to the misinterpretation of the original set of requirements. In this article we advocate requirements quality certification using LSPCM as a prerequisite for requirements hand-over. LSPCM stands for LaQuSo Software Product Certification Model that can be applied by non-experienced IT assessors to verify software artifacts in order to contribute to the successfulness of the project. To support our claim we have analyzed requirements of three off shoring projects using LSPCM. Application of LSPCM revealed severe flaws in one of the projects. The responsible project leader confirmed later that the development significantly exceeded time and budget. In the other project no major\u00a0\u2026", "num_citations": "11\n", "authors": ["110"]}
{"title": "Reverse engineering sequence diagrams for Enterprise JavaBeans with business method interceptors\n", "abstract": " Enterprise JavaBeans (EJB) is a component technology commonly used for enterprise application development. Recent EJB 3.0 specification involves interceptors, a mechanism providing means to dynamically introduce additional behavior into the execution of a bean. As multiple interceptors can be applied to the same bean, and the order of interceptor invocation can be affected by a variety of specification rules, complexity of interceptors invocation can easily become a burden for the developers or maintainers. In order to help the developers we propose an algorithm for reverse engineering UML sequence diagrams from EJB 3.0 programs.", "num_citations": "10\n", "authors": ["110"]}
{"title": "On termination of logic programs with floating point computations\n", "abstract": " Numerical computations form an essential part of almost any real-world program. Traditional approaches to termination of logic programs are restricted to domains isomorphic to  , more recent works study termination of integer computations. Termination of computations involving real numbers is cumbersome and counter-intuitive due to rounding errors and implementation conventions. We present a novel technique that allows us to prove termination of such computations. Our approach extends the previous work on termination of integer computations.", "num_citations": "10\n", "authors": ["110"]}
{"title": "Non-transformational termination analysis of logic programs, based on general term-orderings\n", "abstract": " We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general term-orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but that we apply these orderings directly to the logic program and not to the termrewrite system obtained through some transformation. We define some variants of acceptability, based on general term-orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants.               The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple termorderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.", "num_citations": "10\n", "authors": ["110"]}
{"title": "Reducing user input requests to improve IT support ticket resolution process\n", "abstract": " Management and maintenance of IT infrastructure resources such as hardware, software and network is an integral part of software development and maintenance projects. Service management ensures that the tickets submitted by users, i.e. software developers, are serviced within the agreed resolution times. Failure to meet those times induces penalty on the service provider. To prevent a spurious penalty on the service provider, non-working hours such as waiting for user inputs are not included in the measured resolution time, that is, a service level clock pauses its timing. Nevertheless, the user interactions slow down the resolution process, that is, add to user experienced resolution time and degrade user experience. Therefore, this work is motivated by the need to analyze and reduce user input requests in tickets\u2019 life cycle.               To address this problem, we analyze user input requests and\u00a0\u2026", "num_citations": "9\n", "authors": ["110"]}
{"title": "A bottom-up quality model for QVTO\n", "abstract": " We investigate the notion of quality in QVT Operational Mappings (QVTo), one of the languages defined in the OMG standard on model-to-model transformations. We utilize a bottom-up approach, starting with a broad exploratory study including QVTo expert interviews, a review of existing material, and introspection. We then formalize QVTo transformation quality into a QVTo quality model, consisting of high-level quality goals, quality properties, and evaluation procedures. We validate the quality model by conducting a survey in which a broader group of QVTo developers rate each property on its importance to QVTo code quality. We find that although many quality properties recognized as important for QVTo do have counterparts in traditional languages, a number are specific to QVTo or model transformation languages. Additionally, a selection of QVTo best practices discovered are presented. The primary\u00a0\u2026", "num_citations": "9\n", "authors": ["110"]}
{"title": "Hasta-La-Vista: Termination analyser for logic programs\n", "abstract": " Verifying termination is often considered as one of the most important aspects of program verification. In this paper we present Hasta-La-Vista\u2014an automatic tool for analysing termination of logic programs. To the best of our knowledge, Hasta-La-Vista is unique in being able to prove termination of programs depending on integer computations.", "num_citations": "9\n", "authors": ["110"]}
{"title": "Model management tools for models of different domains: a systematic literature review\n", "abstract": " Objective: The goal of this study is to present an overview of industrial and academic approaches to cross-domain model management. We aim at identifying industrial and academic tools for cross-domain model management and describing the inconsistency types addressed by them as well as strategies the users of the tools employ to keep consistency between models of different domains. Method: We conducted a systematic literature review. Using the keyword-based search on Google Scholar we analyzed 515 potentially relevant studies; after applying inclusion and exclusion criteria 88 papers were selected for further analysis. Results: The main findings/contributions are: (i) a list of available tools used to support model management; (ii) approximately 31% of the tools can provide consistency model checking on models of different domains and approximately 24% on the same domain; (iii) available strategies\u00a0\u2026", "num_citations": "8\n", "authors": ["110"]}
{"title": "Repositioning of static analysis alarms\n", "abstract": " The large number of alarms reported by static analysis tools is often recognized as one of the major obstacles to industrial adoption of such tools.", "num_citations": "8\n", "authors": ["110"]}
{"title": "Software metrics\n", "abstract": " Dia 0 Page 1 2IS55 Software Evolution Software metrics Alexander Serebrenik Page 2 Assignments \u2022 Assignment 3: deadline \u2013 March 17! \u2022 Assignment 4: testing \u2022 Assignment 5: \u2022 Do software evolution laws hold in practice? \u2022 Use software metrics to instrument and verify the claim \u2022 Preprocessed datasets / SET / W&I PAGE 1 13-3-2014 Page 3 Recall\u2026 \u2022 Metric: \u2022 \u201cA quantitative measure of the degree to which a system, component, or process possesses a given variable. \u201d --IEEE Standard 610.12-1990 \u2022 \u201cA software metric is any type of measurement which relates to a software system, process or related documentation.\u201d --- Ian Sommerville, Software Eng. 2006 \u2022 Short: mapping of software artefacts to a well-known domain / SET / W&I PAGE 2 13-3-2014 Page 4 Metrics and scales \u2022 What metrics have we seen so far? \u2022 Size: LOC, SLOC \u2022 Code duplication: POP, RNR, \u2026 \u2022 Requirements: Flesch-Kincaid grade level / SET -\u2026", "num_citations": "8\n", "authors": ["110"]}
{"title": "Detecting Modularity \"Smells\" in Dependencies Injected with Java Annotations\n", "abstract": " Dependency injection is a recent programming mechanism reducing dependencies among components by delegating them to an external entity, called a dependency injection framework. An increasingly popular approach to dependency injection implementation relies upon using Java annotations, a special form of syntactic metadata provided by the dependency injection frameworks. However, uncontrolled use of annotations may lead to potential violations of well-known modularity principles. In this paper we catalogue \u201cbad smells\u201d, i.e., modularity-violating annotations defined by the developer or originating from the popular dependency injection frameworks. For each violation we discuss potential implications and propose means of resolving it. By detecting modularity bad smells in Java annotations our approach closes the gap between the state-of-the-art programming practice and currently available analysis\u00a0\u2026", "num_citations": "8\n", "authors": ["110"]}
{"title": "A software framework for automated verification\n", "abstract": " This paper describes a software framework supporting the automated verification of models. The framework allows analyzing different kinds of behavioral models of software systems and business processes like UML activity diagrams and BPEL models. To extend the applicability of the verification tools, a variety of transformation tools have been integrated in the framework.", "num_citations": "8\n", "authors": ["110"]}
{"title": "On termination of binary CLP programs\n", "abstract": " Termination of binary CLP programs has recently become an important question in the termination analysis community. The reason for this is due to the fact that some of the recent approaches to termination of logic programs abstract the input program to a binary CLP program and conclude termination of the input program from termination of the abstracted program. In this paper we introduce a class of binary CLP programs such that their termination can be proved by using linear level mappings. We show that membership to this class is decidable and present a decision procedure. Further, we extend this class to programs such that their termination proofs require a combination of linear functions. In particular we consider as level mappings tuples of linear functions and piecewise linear functions.", "num_citations": "7\n", "authors": ["110"]}
{"title": "Refactoring prolog code\n", "abstract": " Refactoring is a popular technique from the OO-community to restructure code: it aims at improving software readability, maintainability and extensibility. In this paper we apply the ideas of refactoring to Prolog programs. We present a catalogue of refactorings adapted to or specifically developed for Prolog. We also discuss ViPReSS, our semi-automatic refactoring browser, and our experience with applying ViPReSS to a large Prolog legacy system. Our conclusion is that refactoring is clearly both viable and desirable in the context of Prolog.", "num_citations": "7\n", "authors": ["110"]}
{"title": "Termination analysis of logic programs\n", "abstract": " Termination is well-known to be one of the most intriguing aspects of program verification. Since logic programs are Turing-complete, it follows by the undecidability of the halting problem that there exists no algorithm which, given an arbitrary logic program, decides whether the program terminates. However, one can propose both conditions that are equivalent to termination and their approximations that imply termination and can be verified automatically. This paper briefly discusses these kinds of conditions that were studied in [2].", "num_citations": "7\n", "authors": ["110"]}
{"title": "Proving termination with adornments\n", "abstract": " Termination is well-known to be one of the important aspects of program correctness. Logic programming provides a framework with a strong theoretical basis for tackling this problem. However, due to the declarative formulation of programs, the danger of non-termination may increase. As a result, termination analysis received considerable attention in logic programming. Recently, the study of termination of numerical programs led to the emerging of the adorning technique\u00a0[7]. This technique implements the well-known \u201cdivide et impera\u201d strategy by distinguishing between different subsets of values for variables, and deriving termination proofs based on these subsets. In this paper we generalise this technique and discuss its applicability to the domain of terms (the Herbrand domain).", "num_citations": "7\n", "authors": ["110"]}
{"title": "J. Yang\n", "abstract": " Feature extraction for deep sleep detection \u2014 Eindhoven University of Technology research portal Skip to main navigation Skip to search Skip to main content Eindhoven University of Technology research portal Logo Help & FAQ English Nederlands Home Researchers Research output Organisational units Activities Projects Prizes Press / Media Facilities / Equipment Datasets Courses Research areas Student theses Search by expertise, name or affiliation Feature extraction for deep sleep detection J. Yang Mathematics and Computer Science Student thesis: Master Abstract Date of Award 31 Aug 2013 Original language English Supervisor Xi Long (Supervisor 1), Reinder Haakma (External coach), TGK Calders (Supervisor 2), TL Hoang (Supervisor 2) & Alexander Serebrenik (Supervisor 2) Cite this Standard Feature extraction for deep sleep detection Yang, J. (Author). 31 Aug 2013 Student thesis: Master full text : /'\u2026", "num_citations": "7\n", "authors": ["110"]}
{"title": "Exploring DSL Evolutionary Patterns in Practice\n", "abstract": " Model-driven engineering is used in the design of systems to (ao) enable analysis early in the design process. For instance, by using domain-specific languages, enabling engineers to model systems in terms of their domain, rather then encoding them into general purpose modeling languages. Domain-specific languages, like classical software, evolve over time. When domain languages evolve, they may trigger co-evolution of models, model-to-model transformations, editors (both graphical and textual), and other artifacts that depend on the domain-specific language. This co-evolution can be tedious and very costly. In literature, various approaches are proposed towards automated co-evolution. However, these approaches do not reach full automation. Several other studies have shown that there are theoretical limitations to the level of automation that can be achieved in certain scenarios. For several scenarios full automation can never be achieved. We wish to gain insight to which extent practically occurring scenarios can be automated. To gain this insight, in this paper, we investigate on a large-scale industrial repository, which (co-) evolutionary scenarios occur in practice, and compare them with the various scenarios and their theoretical automatability. We then assess whether practically occurring scenarios can be fully automated.", "num_citations": "6\n", "authors": ["110"]}
{"title": "Less is more: Unparser-completeness of metalanguages for template engines\n", "abstract": " A code generator is a program translating an input model into code. In this paper we focus on template-based code generators in the context of the model view controller architecture (MVC). The language in which the code generator is written is known as a metalanguage in the code generation parlance. The metalanguage should be, on the one side, expressive enough to be of practical value, and, on the other side, restricted enough to enforce the separation between the view and the model, according to the MVC. In this paper we advocate the notion of unparser-complete metalanguages as providing the right level of expressivity. An unparser-complete metalanguage is capable of expressing an unparser, a code generator that translates any legal abstract syntax tree into an equivalent sentence of the corresponding context-free language. A metalanguage not able to express an unparser will fail to produce all\u00a0\u2026", "num_citations": "6\n", "authors": ["110"]}
{"title": "Visualizing traceability in model transformation compositions\n", "abstract": " Analysis and execution of domain-specific models is typically performed by applying model transformations to transform them to formalisms suitable for that purpose. In this way, model transformations provide great flexibility. However, it is the transformed models that are analyzed and executed instead of the domain-specific models themselves. Consequently, analysis and execution results have to be traced back to the source models. This raises the issue of traceability in model transformations. This issue becomes even more apparent when compositions of multiple model transformations are considered. Therefore, we propose in this paper to visualize traceability in (compositions of) model transformations. In this visualization, the relation between the transformation functions that comprise a model transformation and the source and target metamodel of that transformation are made explicit.", "num_citations": "6\n", "authors": ["110"]}
{"title": "Formally specified type checkers for domain specific languages: experience report\n", "abstract": " An important part of the usability of a programming or specification language lies in the presence of supporting tools that are provided with the language, eg, type checkers, debuggers and simulators. Development of such tools for domain-specific languages imposes a number of specific evolvability requirements. Our contribution is twofold: First, we present an MSOS-based approach to automatic generation of formally specified type checkers for domain-specific languages. Second, we report on the application of our approach to Chi, a high level specification language for describing concurrent systems. The resulting type checker has been successfully integrated in the tool chain of the Chi language.", "num_citations": "6\n", "authors": ["110"]}
{"title": "History-based joins: Semantics, soundness and implementation\n", "abstract": " In this paper we study the use of case history for control structures in workflow processes. History-dependent control offers much more modeling power than classical control structures and it solves several semantical problems of industrial modeling frameworks. In particular, we introduce a history-dependent join. We study the modeling power by means of workflow patterns. Since proper completion (i.e. the ability of any configuration reachable from the initial one to reach the final one) is always an important \u201csanity check\u201d of process modeling, we introduce a modeling method that guarantees this property holds. Finally we consider an implementation of the proposed control structures on top of an existing workflow engine.", "num_citations": "6\n", "authors": ["110"]}
{"title": "LogLogics: A logic for history-dependent business processes\n", "abstract": " Choices in business processes are often based on the process history saved as a log-file listing events and their time stamps. In this paper we introduce LogLogics, a finite-path variant of the Timed Propositional Temporal Logic with Past, which can be in particular used for specifying guards in business process models. The novelty is due to the presence of boundary points corresponding to the starting and current observation points, which gives rise to a three-valued logic allowing us to distinguish between temporal formulas that hold for any log extended with some possible past and future (true), those that do not hold for any extended log (false) and those that hold for some but not all extended logs (unknown). We reduce the check of the truth value of a LogLogics\u00a0formula to a check on a finite abstraction and present an evaluation algorithm. We also define LogLogics\u00a0patterns for commonly occurring properties.", "num_citations": "6\n", "authors": ["110"]}
{"title": "Refactoring logic programs\n", "abstract": " Program changes take up a substantial part of the entire programming effort. Often a preliminary step of improving the design without altering the external behaviour can be recommended. This is the idea behind refactoring, a source-to-source program transformation that recently came to prominence in the OO-community [1]. Unlike the existing results on automated program transformation, refactoring does not aim at transforming the program entirely automatically. The decision on whether the transformation should be applied and how it should be done is left to the program developer. However, providing automated support for refactoring is useful and an important challenge.", "num_citations": "6\n", "authors": ["110"]}
{"title": "Refactoring Prolog programs\n", "abstract": " Refactoring is a technique to restructure code in a disciplined way originating from the OO-community. It aims to improve software readability, maintainability and extensibility. Unlike the existing results on program transformation refactoring can require user input to take certain decisions. In this paper we apply the ideas of refactoring to Prolog programs. We start by presenting a catalogue of refactorings. Then we discuss ViPReSS, our refactoring browser, and our experience with applying ViPReSS to a big Prolog legacy system.", "num_citations": "6\n", "authors": ["110"]}
{"title": "Token history Petri nets\n", "abstract": " State of the art information system commonly record events in log files, also known as audit trails. Moreover, business processes often go beyond the sole recording the events and base decisions on the events observed in the past. To model such processes we extend the basic Petri net framework with the notion of history by associating tokens with histories, adding guards evaluated on the history to the transitions and mapping arcs to expressions involving histories. Guards and arc expressions can involve data associated with the transitions.", "num_citations": "5\n", "authors": ["110"]}
{"title": "Modelling History-Dependent Business Processes.\n", "abstract": " Choices in business processes are often based on the process history saved as a log-file listing events and their time stamps. In this paper we introduce a finite-path variant of the timed propositional logics with past for specifying guards in business process models. The novelty is due to the introduction of boundary points start and now corresponding to the starting and current observation points. Reasoning in presence of boundary points requires three-valued logics as one needs to distinguish between temporal formulas that hold, those that do not hold and \u201cunknown\u201d ones corresponding to \u201copen cases\u201d. Finally, we extend a sub-language of the logics to take uncertainty into account.", "num_citations": "5\n", "authors": ["110"]}
{"title": "The diversity crisis in software development\n", "abstract": " This special issue of IEEE Software focuses on diversity and inclusion in software development, presenting research results and best practices for making the field equitable for all. It is well documented that the industry does not provide evenhanded participation conditions. Research has shown that implicit gender biases significantly impact hiring decisions,1 women disengage faster than men,2 Palestinian tech entrepreneurs do not have access to Internet-based distribution and payment platforms,3 software developers with a visual impairment lack tools to navigate code editors,4,5 and women are sometimes less likely to get their code accepted.6 Tools, processes, products, and education are not inclusive. Dimensions such as geography, gender, socioeconomic politics, age, ethnicity, and disability shape who can participate in creating technology.", "num_citations": "4\n", "authors": ["110"]}
{"title": "Action-based recommendation in pull-request development\n", "abstract": " Pull requests (PRs) selection is a challenging task faced by integrators in pull-based development (PbD), with hundreds of PRs submitted on a daily basis to large open-source projects. Managing these PRs manually consumes integrators' time and resources and may lead to delays in the acceptance, response, or rejection of PRs that can propose bug fixes or feature enhancements. On the one hand, well-known platforms for performing PbD, like GitHub, do not provide built-in recommendation mechanisms for facilitating the management of PRs. On the other hand, prior research on PRs recommendation has focused on the likelihood of either a PR being accepted or receive a response by the integrator. In this paper, we consider both those likelihoods, this to help integrators in the PRs selection process by suggesting to them the appropriate actions to undertake on each specific PR. To this aim, we propose an\u00a0\u2026", "num_citations": "4\n", "authors": ["110"]}
{"title": "Empowering OCL research: a large-scale corpus of open-source data from GitHub\n", "abstract": " Model-driven engineering (MDE) enables the rise in abstraction during development in software and system design. In particular, meta-models become a central artifact in the process, and are supported by various other artifacts such as editors and transformation. In order to define constraints, invariants, and queries on model-driven artifacts, a generic language has been developed: the Object Constraint Language (OCL). In literature, many studies into OCL have been performed on small collections of data, mostly originating from a single source (e.g., OMG standards). As such, generalization of results beyond the data studied is often mentioned as a threat to validity. Creation of a benchmark dataset has already been identified as a key enabler to address the generalization threat. To facilitate further empirical studies in the field of OCL, we present the first large-scale dataset of 103262 OCL expression\u00a0\u2026", "num_citations": "4\n", "authors": ["110"]}
{"title": "A Case of Industrial vs. Open-source OCL: Not So Different After All.\n", "abstract": " When studying model-driven engineering (MDE) in industry, generalization of studies is often hard, as most artifacts are proprietary and confidential in nature. A possible solution would be to study open-source artifacts. However, open-source artifacts are not necessarily representative for those found in the industry. As the first step towards investigating the viability of open-source MDE artifacts as an alternative to less accessible industrial ones, we use a large open-source dataset and several industrial meta-models to show that the complexity of OCL expressions in open-source and industry is similar.", "num_citations": "4\n", "authors": ["110"]}
{"title": "Udapt: Edapt extensions for industrial application\n", "abstract": " Domain specific languages (DSLs) allow modeling systems in terms of domain concepts. We discuss the shortcomings of existing DSL evolution approaches when applied in industry and propose extensions addressing these shortcomings.", "num_citations": "4\n", "authors": ["110"]}
{"title": "Predictive performance and discrimination in unbalanced classification\n", "abstract": " In this thesis, we investigate two types of problems in classifier construction. First, we focus on the problem of class imbalance. We acknowledge that common accuracy metrics such as accuracy and precision are not appropriate to evaluate performance of a classifier for an imbalanced dataset, and therefore consider existing solutions including the ROC-curve and Kappa statistic, and extend the notion of the ROC-curve to a new single-value performance identifier \u201cROC-distance\u201d. We employ several methods that reduce class imbalance, using variable support for itemsets, and synthetically oversampling the minority class, using techniques that preserve the original characteristics of a dataset, while creating an equal amount of samples for both classes. Secondly, we focus on the problem of fairness and discrimination in classifiers. Classifiers that learn from socially sensitive data, can learn certain decisions that may be considered discriminatory, due to biased data or even discriminatory choices that are recorded, which may lead to structural discrimination by the classifier. We show methods that quantify and remove discrimination from the classifier. The influence of discrimination removal on classifier performance is evaluated and we propose a different sorting precedence than proposed by CBA2 (Classification Based on Associations), which deals better with discrimination removal than the original CBA2 method. We develop an association rule mining classifier. During the development of this classifier, we used Bayesian Networks to visualize decision characteristics of the classifier and acknowledge that this is an effective way of analysing\u00a0\u2026", "num_citations": "4\n", "authors": ["110"]}
{"title": "Evolution specification evaluation in industrial mdse ecosystems\n", "abstract": " Domain-specific languages (DSLs) allow users to model systems using concepts from a specific domain. Evolution of DSLs triggers co-evolution of models developed in these languages. When the number of models that needs to co-evolve increases, so does the required effort to do so. This is called the co-evolution problem. We have investigated the extent of the co-evolution problem at ASML [1], provider of lithography equipment for the semiconductor industry. Here we have described the structure and evolution of a large-scale ecosystem of DSLs. We have observed that due to the large number of artifacts that require coevolutionary activity, manual solutions have become unfeasible, and an automated approach is required. A popular approach for automating co-evolution is the operator-based approach. In this paper we have evaluated the operator-based approach on a large-scale industrial case-study of twenty-two DSLs and 95 model-to-model transformations with a revision history of over three years, and have revealed deficiencies in existing operator libraries. To address these deficiencies we have presented a topdown methodology to derive a complete set of operators.", "num_citations": "4\n", "authors": ["110"]}
{"title": "Working with the past: Integrating history in petri nets\n", "abstract": " Most information systems that are driven by processmodels (eg, workflowmanagement systems) record events in event logs, also known as transaction logs or audit trails. We consider processes that not only keep track of their history in a log, but also make decisions based on this log. To model such processes we extend the basic Petri net framework with the notion of history and add guards to transitions evaluated on the process history. We show that some classes of historydependent nets can be automatically converted to classical Petri nets for analysis purposes. Some of these classes are characterized by the form of the guards (eg, LTL+ Past guards), while others by restrictions on the underlying classical Petri net.", "num_citations": "4\n", "authors": ["110"]}
{"title": "Automatic termination analysis of programs containing arithmetic predicates\n", "abstract": " For logic programs with arithmetic predicates, showing termination is not easy, since the usual order for the integers is not well-founded. A new method, easily incorporated in the TermiLog system for automatic termination analysis, is presented for showing termination in this case.The method consists of the following steps: First, a finite abstract domain for representing the range of integers is deduced automatically. Based on this abstraction, abstract interpretation is applied to the program. The result is a finite number of atoms abstracting answers to queries which are used to extend the technique of query-mapping pairs. For each query-mapping pair that is potentially non-terminating, a bounded (integer-valued) termination function is guessed. If traversing the pair decreases the value of the termination function, then termination is established. Simple functions often suffice for each query-mapping pair, and that gives\u00a0\u2026", "num_citations": "4\n", "authors": ["110"]}
{"title": "Corrigendum: Empirical analysis of the relationship between CC and SLOC in a large corpus of java methods and C functions published on 9 december 2015,\u201d\n", "abstract": " During the preparation of the corresponding chapter in Davy Landman's PhD thesis, some minor graphical and statistical discrepancies were found in the paper \u201cEmpirical analysis of the relationship between CC and SLOC in a large corpus of Java methods and C functions.\u201d 1", "num_citations": "3\n", "authors": ["110"]}
{"title": "A quality framework for evaluating automotive architecture\n", "abstract": " As the number and complexity of software systems increase in automobiles, it has become crucial to specify, measure, and evaluate automotive software quality. However, the existing quality methods focus on MATLAB/Simulink design models, and do not address architectural models (eg high-level functional or electrical/electronic (E/E) architectures). Therefore, our objective is to develop novel quality specification, measurement, and evaluation methods targeting both architectural and design models. We combine the methods proposed with recent insights in software quality modelling and create an integrated quality assessment framework for architectural and design models of automotive software. The quality framework comprises six quality characteristics, 16 sub-characteristics, 71 metrics, and three independent tools (parser, quality metrics calculator, and quality visualiser). By applying the framework to three subsequent releases of an architectural model and the corresponding design models, we have observed, for example, that addition of new functionality or bug fixing in design models often come at a price of increased complexity at the design level, and sometimes compromise modularity of the architectural model. Keywords: quality framework, architecture evaluation, quality model, quality metrics", "num_citations": "3\n", "authors": ["110"]}
{"title": "Evolution Mechanisms of Automotive Architecture Description Languages\n", "abstract": " \u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website.", "num_citations": "3\n", "authors": ["110"]}
{"title": "Dn-based design quality comparison of industrial Java applications\n", "abstract": " The normalized distance from the main sequence, denoted D n , is a popular object-oriented metric introduced by Martin in 1994. While the metric has been designed for assessment of individual packages it has also been applied in practice to quality assessment of entire software architectures. This gap between the industrial practice and theoretical understanding has been recently addressed for Java open-source systems. Based on study of a benchmarks collection the authors proposed a statistical model characterizing (a) the average value of D n , and (b) distribution of D n . Contribution of the current work is twofold. First, we show feasibility of application of the D n -based assessment above to commercial Java applications. Second, we validate the approach by showing that the results obtained are consistent with those obtained by means of a series of independent studies, such as layering, presence of cyclic\u00a0\u2026", "num_citations": "3\n", "authors": ["110"]}
{"title": "Type checking evolving languages with MSOS\n", "abstract": " Evolution of programming languages requires co-evolution of static analysis tools designed for these languages. Traditional approaches to static analysis, e.g., those based on Structural Operational Semantics (SOS), assume, however, that the syntax and the semantics of the programming language under consideration are fixed. Language modification is, therefore, likely to cause redevelopment of the analysis techniques and tools. Moreover, the redevelopment cost can discourage the language engineers from improving the language design.             To address the co-evolution problem we suggest to base static analyses on modular structural operational semantics (MSOS). By using an intrinsically modular formalism, type rules can be added, removed or modified easily. We illustrate our approach by developing an MSOS-based type analysis technique for Chi, a domain specific language for hybrid systems\u00a0\u2026", "num_citations": "3\n", "authors": ["110"]}
{"title": "Modelling with history-dependent petri nets\n", "abstract": " Most information systems that are driven by process models (e.g., workflow management systems) record events in event logs, also known as transaction logs or audit trails. We consider processes that not only keep track of their history in a log, but also make decisions based on this log. Extending our previous work on history-dependent Petri nets we propose and evaluate a methodology for modelling processes by such nets and show how history-dependent nets can combine modelling comfort with analysability.", "num_citations": "3\n", "authors": ["110"]}
{"title": "Termination analysis of logic programs using acceptability with general term orders\n", "abstract": " We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general term-orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but that we apply these orderings directly to the logic program and not to the term-rewrite system obtained through some transformation. We define some variants of acceptability, based on general term-orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants. The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple term-orderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.", "num_citations": "3\n", "authors": ["110"]}
{"title": "Analyzing Comments in Ticket Resolution to Capture Underlying Process Interactions\n", "abstract": " Activities in the ticket resolution process have comments and emails associated with them. Process mining uses structured logs and does not analyze the unstructured data such as comments for process discovery. However, comments can provide additional information for discovering models of process reality and identifying improvement opportunities efficiently. To address the problem, we propose to extract topical phrases (keyphrases) from the unstructured data using an unsupervised graph-based approach. These keyphrases are then integrated into the event log to derive enriched event logs. A process model is discovered using the enriched event logs wherein keyphrases are represented as activities, thereby capturing the flow relationship with other activities and the frequency of occurrence. This provides insights that can not be obtained solely from the structured data.", "num_citations": "2\n", "authors": ["110"]}
{"title": "Suitability of optical character recognition (ocr) for multi-domain model management\n", "abstract": " The development of systems following model-driven engineering can include models from different domains. For example, to develop a mechatronic component one might need to combine expertise about mechanics, electronics, and software. Although these models belong to different domains, the changes in one model can affect other models causing inconsistencies in the entire system. There are, however, a limited amount of tools that support management of models from different domains. These models are created using different modeling notations and it is not plausible to use a multitude of parsers geared towards each and every modeling notation. Therefore, to ensure maintenance of multi-domain systems, we need a uniform approach that would be independent from the peculiarities of the notation. Meaning that such a uniform approach can only be based on something which is present in all those\u00a0\u2026", "num_citations": "2\n", "authors": ["110"]}
{"title": "Reducing static analysis alarms based on non-impacting control dependencies\n", "abstract": " Static analysis tools help to detect programming errors but generate a large number of alarms. Repositioning of alarms is recently proposed technique to reduce the number of alarms by replacing a group of similar alarms with a small number of newly created representative alarms. However, the technique fails to replace a group of similar alarms with a fewer representative alarms mainly when the immediately enclosing conditional statements of the alarms are different and not nested. This limitation is due to conservative assumption that a conditional statement of an alarm may prevent the alarm from being an error. To address the limitation above, we introduce the notion of non-impacting control dependencies (NCDs). An NCD of an alarm is a transitive control dependency of the alarm\u2019s program point, that does not affect whether the alarm is an error. We approximate the computation of NCDs based on the alarms\u00a0\u2026", "num_citations": "2\n", "authors": ["110"]}
{"title": "Activity diagrams and state machines\n", "abstract": " \u2022 Main scenario: The user dials number \u201cB\u201d.\u2022 Alternative step: If the user does not dial a number for a certain amount of time, a permanent tone is emitted by the switch center, no further call will be accepted and the user has to replace the hook.", "num_citations": "2\n", "authors": ["110"]}
{"title": "Detecting dependencies in enterprise javabeans with squavisit\n", "abstract": " We present recent extensions to SQuAVisiT, Software Quality Assessment and Visualization Toolset. While SQuAVisiT has been designed with traditional software and traditional caller-callee dependencies in mind, recent popularity of Enterprise JavaBeans (EJB) required extensions that enable analysis of additional forms of dependencies: EJB dependency injections, object-relational (persistence) mappings and Web service mappings. In this paper we discuss the implementation of these extensions in SQuAVisiT and the application of SQuAVisiT to an open-source software system.", "num_citations": "2\n", "authors": ["110"]}
{"title": "Modularity analysis of automotive control software\n", "abstract": " A design language and tool like MATLAB/Simulink is used for the graphical modelling and simulation of automotive control software. As the functionality based on electronics and software systems increases in motor vehicles, it is becoming increasingly important for system/software architects and control engineers in the automotive industry to ensure the quality of the highly complex MATLAB/Simulink control software. For automotive software, modularity is recognized as being a crucial quality attribute; therefore at Eindhoven University of Technology in the Netherlands we have been carrying out industrial case studies on defining and validating the modularity of Simulink models. assessing quality of Simulink models are limited.For automotive software, modularity is recognized as being paramount since changing or reusing non-modular software is very costly. MathWorks provides quality related tools like \u201cModeling Metric Tool\u201d and \u201csldiagnostics\u201d to quantitatively measure the content of a Simulink model (Stateflow model as well), to improve the productivity and quality of model development, eg model size, complexity, and defect densities. Other quality metrics eg for measuring instability, abstractness, and complexity of Simulink models have been intro-", "num_citations": "2\n", "authors": ["110"]}
{"title": "SAW-BOT: Proposing Fixes for Static Analysis Warnings with GitHub Suggestions\n", "abstract": " In this experience report we present SAW-BOT, a bot proposing fixes for static analysis warnings. The bot has been evaluated with five professional software developers by means of a Wizard of Oz experiment, semi-structured interviews and the mTAM questionnaire. We have observed that developers prefer GitHub suggestions to two baseline operation modes. Our study indicates that GitHub suggestions are a viable mechanism for implementing bots proposing fixes for static analysis warnings.", "num_citations": "1\n", "authors": ["110"]}
{"title": "Techniques for Efficient Automated Elimination of False Positives\n", "abstract": " Static analysis tools are useful to detect common programming errors. However, they generate a large number of false positives. Postprocessing of these alarms using a model checker has been proposed to automatically eliminate false positives from them. To scale up the automated false positives elimination (AFPE), several techniques, e.g., program slicing, are used. However, these techniques increase the time taken by AFPE, and the increased time is a major concern during application of AFPE to alarms generated on large systems.To reduce the time taken by AFPE, we propose two techniques. The techniques achieve the reduction by identifying and skipping redundant calls to the slicer and model checker. The first technique is based on our observation that, (a) combination of application-level slicing, verification with incremental context, and the context-level slicing helps to eliminate more false positives; (b\u00a0\u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "A reflection on \"An exploratory study on exception handling bugs in java programs\"\n", "abstract": " Exception handling is a feature provided by most mainstream programming languages, and typically involves constructs to throw and handle error signals. On the one hand, early work has argued extensively about the benefits of exception handling, such as promoting modularity by defining how exception handlers can be implemented and maintained independently of the normal behavior of the system and easing but localization. On the other hand, some studies argue that exception handling can make the programming languages unnecessarily complex and promote the introduction of subtle bugs in programs. In 2015 we published a paper describing a study investigating the prevalence and nature of exception handling bugs in two large, widely adopted Java systems. This study also confronted its findings about real exception handling bugs with the perceptions of developers about those bugs, also accounting\u00a0\u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "Model management tools for models of different domains: a systematic literature review\n", "abstract": " Objective: The goal of this study is to present an overview of industrial and academic approaches to cross-domain model management. We aim at identifying industrial and academic tools for cross-domain model management and describing the inconsistency types addressed by them as well as strategies the users of the tools employ to keep consistency between models of different domains. Method: We conducted a systematic literature review. Using the keyword-based search on Google Scholar we analyzed 515 potentially relevant studies; after applying inclusion and exclusion criteria 88 papers were selected for further analysis. Results: The main findings/contributions are: (i) a list of available tools used to support model management; (ii) approximately 31% of the tools can provide consistency model checking on models of different domains and approximately 24% on the same domain; (iii) available strategies to keep the consistency between models of different domains are not mature enough; (iv) explicit modeling dependencies between models is not common in the industry. However, it is considered as a requirement by academia if one wishes to manage inconsistency between models of different domains. Conclusion: This study presents an overview of industrial practices and academic approaches about the cross-domain model management. The results presented in this study can be used as a starting point for future research on model management topics, and also for further improvement of actual model management tools.", "num_citations": "1\n", "authors": ["110"]}
{"title": "The relationship between CC and SLOC: a preliminary analysis on its evolution\n", "abstract": " Is it useful to measure both Cyclomatic Complexity (CC) and Source Lines of Code (SLOC)? In previous work [1] we have analyzed the reported linear relationship between CC and SLOC. In our large corpus of Java projects, we could not find such a linear relationship. Raising questions for future work. Object Oriented Programming (OOP) could cause the lack of a linear relationship between CC and SLOC. In an OO language, dynamic dispatch and polymorphism are used as an alternative to control flow statements. However, related work [2],[3] reported linear relationships for both C++ and Java. As identified in our earlier work, there is an open question of the evolution of this relationship. Therefore we explorer a possible evolutionary argument: are the Java programs of today using more OOP? And does this cause the decreased power of SLOC to predict CC?", "num_citations": "1\n", "authors": ["110"]}
{"title": "QVTo model transformations: Assessing and improving their quality\n", "abstract": " QVTo model transformations : assessing and improving their quality (2014) | www.narcis.nl KNAW KNAW Narcis Back to search results Eindhoven University of Technology Publication QVTo model transformations : assessing and improving their quality (2014) Open access . Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title QVTo model transformations : assessing and improving their quality Published in ERCIM News, 99, 32 - 33. ERCIM. ISSN 0926-4981. Author Gerpheide, CM; Schiffelers, RRH; Serebrenik, A. Publisher Department of Mathematics and Computer Science; Model Driven Software Engineering W&I; Sectie Softwaretechnologie; Software Engineering and Technology Date issued 2014 Access Open Access Language English Type Article Publisher ERCIM Publication https://research.tue.nl/nl/publications/qvto-model-transform... urn::nl\u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "Control Flow in the Wild A first look at 13K Java projects\n", "abstract": " II. EXPERIMENT We took the Sourcerer Corpus which contains 18K (13K non empty) Java projects. Using a Java grammar and RASCAL [3] we parsed all Java files. All methods were transformed [1] into CFPs. Acfp is an AST created by removing all statements not related to control flow. Table I contains a list of Java\u2019s language constructs kept. The last step is to change all expressions inside the arguments of the constructs into an empty string.", "num_citations": "1\n", "authors": ["110"]}
{"title": "Seeing the Forest for the trees with New Econometric Aggregation techniques\n", "abstract": " Maintaining a software system is like renovating a house: it usually takes longer and costs more than planned. Like a house owner preparing a condition report identifying potential problems before renovation, a software owner should assess maintainability of software before renovating or extending it. To measure software maintainability one often applies software metrics, associating software artifacts with numerical values. Unfortunately, advanced software metrics are commonly measured at a level of small artifacts, eg methods and classes, and fail to provide an adequate picture of the system maintainability. Continuing the analogy, the state-of-theart in software metrics corresponds to a condition report detailing the state of every brick and obscuring the general assessment in the multitude of details. Metrics visualization techniques provide a general overview of the measurements but have difficulties with presenting evolution of these measurements in time.To see the forest of a software system for the trees of individual measurements, aggregation techniques are used. Current aggregation techniques are, however, usually unreliable or involve human judgment. For instance, the mean is known to become unreliable in the presence of highly skewed distributions, which are typical for software metrics. Another approach, distribution fitting, consists of manually selecting a known family of distributions and fitting its parameters to approximate the observed", "num_citations": "1\n", "authors": ["110"]}
{"title": "Process mining software repositories: Do developers work as expected?\n", "abstract": " ERCIM News 88 Page 1 Process mining software repositories: do developers work as expected? Citation for published version (APA): Poncin, W., Serebrenik, A., & Brand, van den, MGJ (2012). Process mining software repositories: do developers work as expected? ERCIM News, 88, 16-17. Document status and date: Published: 01/01/2012 Document Version: Publisher\u2019s PDF, also known as Version of Record (includes final page, issue and volume numbers) Please check the document version of this publication: \u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are of the \u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "Managing the co-evolution of software artifacts\n", "abstract": " Software development projects are virtually always carried out under pressure. Planning and budgets are tight, room for errors is non-existent and the pressure to deliver is high. Natural questions for (test) managers arise, such as:\u201cWhen have we tested enough?\u201d and \u201cHow many tests do we have to redo for this new version?'\u201d. The naive answer would be:\u201cwhen we have convinced ourselves through testing that all requirements are satisfied\u201d. Unfortunately, attaining maximal confidence with minimal effort is not easy.In order to convince ourselves that the system does what it is supposed to do, tests are needed. Requirements, design and code change during the development of software. As a consequence, tests need to change as well. In the end we want to ensure that all requirements and risks are adequately addressed with tests. For this, tests at different levels of abstraction and for different software artifacts are required and need to be managed.", "num_citations": "1\n", "authors": ["110"]}
{"title": "Predicting service request rates for adaptive resource allocation in SOA\n", "abstract": " Service orientation is rapidly becoming the common practice in the IT world. A price one often has to pay for the advantages of service oriented architectures (SOA) is performance deterioration. SOA performance heavily depends on the allocation of computational resources to services. The needs of services in computational resources are however changing, depending eg on the environmental factors and changes in business processes (and hence service orchestrations). To ensure good performance results, the resource allocation should respond to the changes in the SOA environment. In this paper we focus on the detection of the changes in the environment and the prediction of the expected service requests rates. For this purpose we first discover a stochastic model of the service request rates. Then we monitor the system to detect changes in the environment behaviour and signal the necessity to reconsider\u00a0\u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "Fingerprinting logic programs\n", "abstract": " In this work we present work in progress on functionality duplication detection in logic programs. Eliminating duplicated functionality recently became prominent in context of refactoring. We describe a quantitative approach that allows to measure the ``similarity'' between two predicate definitions. Moreover, we show how to compute a so-called ``fingerprint'' for every predicate. Fingerprints capture those characteristics of the predicate that are significant when searching for duplicated functionality. Since reasoning on fingerprints is much easier than reasoning on predicate definitions, comparing the fingerprints is a promising direction in automated code duplication in logic programs.", "num_citations": "1\n", "authors": ["110"]}
{"title": "Proceedings of Verification and Validation of Software Systems (VVSS2007)\n", "abstract": " Proceedings of Verification and Validation of Software Systems (VVSS2007) (2007) | www.narcis.nl KNAW KNAW Narcis Back to search results Radboud University Nijmegen Publication Proceedings of Verification and Validation of Software Systems (VVSS2007) (2007) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Proceedings of Verification and Validation of Software Systems (VVSS2007) Published in TUE Computer Science Reports ; 07-04. Eindhoven : Technische Universiteit Eindhoven Author Groot, Perry; Serebrenik, Alexander; Eekelen, MCJD van Date issued 2007 Access Closed Access Type Book Publisher Eindhoven : Technische Universiteit Eindhoven Publication http://hdl.handle.net/2066/36357 Persistent Identifiers NBN urn:nbn:nl:ui:22-2066/36357 Handle 2066/36357 Url http://hdl.handle.net/2066/36357 Metadata XML Go to .\u2026", "num_citations": "1\n", "authors": ["110"]}
{"title": "The price of coordination in resource management\n", "abstract": " We propose a resource management policy that grants or refuses requests for resources based only on the request made and the number of free resources. Computations at runtime are independent of the number of active cases. The policy requires little coordination and is therefore easy to implement in workflow management systems. This policy has been shown to be successful in avoiding deadlocks. In this paper we investigate its performance characteristics.", "num_citations": "1\n", "authors": ["110"]}