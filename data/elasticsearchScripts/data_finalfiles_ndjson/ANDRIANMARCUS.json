{"title": "Automated severity assessment of software defect reports\n", "abstract": " In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue. The paper presents a new and automated method named SEVERIS (severity issue assessment), which assists the test engineer in assigning severity levels to defect reports. SEVERIS is based on standard text mining and machine learning techniques applied to existing sets of defect reports. A case study on using SEVERIS with data from NASApsilas Project and Issue Tracking System (PITS) is presented in the paper. The case study results indicate that SEVERIS is a good predictor for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "356\n", "authors": ["56"]}
{"title": "Combining formal concept analysis with information retrieval for concept location in source code\n", "abstract": " The paper addresses the problem of concept location in source code by presenting an approach which combines formal concept analysis (FCA) and latent semantic indexing (LSI). In the proposed approach, LSI is used to map the concepts expressed in queries written by the programmer to relevant parts of the source code, presented as a ranked list of search results. Given the ranked list of source code elements, our approach selects most relevant attributes from these documents and organizes the results in a concept lattice, generated via FCA. The approach is evaluated in a case study on concept location in the source code of eclipse, an industrial size integrated development environment. The results of the case study show that the proposed approach is effective in organizing different concepts and their relationships present in the subset of the search results. The proposed concept location method outperforms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "319\n", "authors": ["56"]}
{"title": "On the use of automated text summarization techniques for summarizing source code\n", "abstract": " During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities (such as, classes, methods, packages, etc.), so they can efficiently identify and then focus on the ones related to their task at hand. Sometimes reading just a method header or a class name does not tell enough about its purpose and meaning, while reading the entire implementation takes too long. We study a solution which mitigates the two approaches, i.e., short and accurate textual descriptions that illustrate the software entities without having to read the details of the implementation. We create such descriptions using techniques from automatic text summarization. The paper presents a study that investigates the suitability of various such techniques for generating source code summaries. The results indicate that a combination of text summarization techniques is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "282\n", "authors": ["56"]}
{"title": "Automatic generation of natural language summaries for java classes\n", "abstract": " Most software engineering tasks require developers to understand parts of the source code. When faced with unfamiliar code, developers often rely on (internal or external) documentation to gain an overall understanding of the code and determine whether it is relevant for the current task. Unfortunately, the documentation is often absent or outdated. This paper presents a technique to automatically generate human readable summaries for Java classes, assuming no documentation exists. The summaries allow developers to understand the main goal and structure of the class. The focus of the summaries is on the content and responsibilities of the classes, rather than their relationships with other classes. The summarization tool determines the class and method stereotypes and uses them, in conjunction with heuristics, to select the information to be included in the summaries. Then it generates the summaries using\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "280\n", "authors": ["56"]}
{"title": "Local versus global lessons for defect prediction and effort estimation\n", "abstract": " Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "222\n", "authors": ["56"]}
{"title": "Automatic query reformulations for text retrieval in software engineering\n", "abstract": " There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "211\n", "authors": ["56"]}
{"title": "Better cross company defect prediction\n", "abstract": " How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "198\n", "authors": ["56"]}
{"title": "The conceptual cohesion of classes\n", "abstract": " While often defined in informal ways, software cohesion reflects important properties of modules in a software system. Cohesion measurement has been used for quality assessment, fault proneness prediction, software modularization, etc. Existing approaches to cohesion measurement in object-oriented software are largely based on the structural information of the source code, such as attribute references in methods. These measures reflect particular interpretations of cohesion and try to capture different aspects of cohesion and no single cohesion metric or suite is accepted as standard measurement for cohesion. The paper proposes a new set of measures for the cohesion of individual classes within an OO software system, based on the analysis of the semantic information embedded in the source code, such as comments and identifiers. A case study on open source software is presented, which compares the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "181\n", "authors": ["56"]}
{"title": "How can I use this method?\n", "abstract": " Code examples are small source code fragments whose purpose is to illustrate how a programming language construct, an API, or a specific function/method works. Since code examples are not always available in the software documentation, researchers have proposed techniques to automatically extract them from existing software or to mine them from developer discussions. In this paper we propose MUSE (Method USage Examples), an approach for mining and ranking actual code examples that show how to use a specific method. MUSE combines static slicing (to simplify examples) with clone detection (to group similar examples), and uses heuristics to select and rank the best examples in terms of reusability, understandability, and popularity. MUSE has been empirically evaluated using examples mined from six libraries, by performing three studies involving a total of 140 developers to: (i) evaluate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["56"]}
{"title": "Concept location using formal concept analysis and information retrieval\n", "abstract": " The article addresses the problem of concept location in source code by proposing an approach that combines Formal Concept Analysis and Information Retrieval. In the proposed approach, Latent Semantic Indexing, an advanced Information Retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. Given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using Formal Concept Analysis. The approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. The empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "123\n", "authors": ["56"]}
{"title": "Information retrieval methods for automated traceability recovery\n", "abstract": " The potential benefits of traceability are well known and documented, as well as the impracticability of recovering and maintaining traceability links manually. Indeed, the manual management of traceability information is an error prone and time consuming task. Consequently, despite the advantages that can be gained, explicit traceability is rarely established unless there is a regulatory reason for doing so. Extensive efforts have been brought forth to improve the explicit connection of software artifacts in the software engineering community (both research and commercial). Promising results have been achieved using Information Retrieval (IR) techniques for traceability recovery. IR-based traceability recovery methods propose a list of candidate traceability links based on the similarity between the text contained in the software artifacts. Software artifacts have different structures and the common element\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "109\n", "authors": ["56"]}
{"title": "Automating extract class refactoring: an improved method and its evaluation\n", "abstract": " During software evolution the internal structure of the system undergoes continuous modifications. These continuous changes push away the source code from its original design, often reducing its quality, including class cohesion. In this paper we propose a method for automating the Extract Class refactoring. The proposed approach analyzes (structural and semantic) relationships between the methods in a class to identify chains of strongly related methods. The identified method chains are used to define new classes with higher cohesion than the original class, while preserving the overall coupling between the new classes and the classes interacting with the original class. The proposed approach has been first assessed in an artificial scenario in order to calibrate the parameters of the approach. The data was also used to compare the new approach with previous work. Then it has been empirically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["56"]}
{"title": "Mining source code descriptions from developer communications\n", "abstract": " Very often, source code lacks comments that adequately describe its behavior. In such situations developers need to infer knowledge from the source code itself or to search for source code descriptions in external artifacts. We argue that messages exchanged among contributors/developers, in the form of bug reports and emails, are a useful source of information to help understanding source code. However, such communications are unstructured and usually not explicitly meant to describe specific parts of the source code. Developers searching for code descriptions within communications face the challenge of filtering large amount of data to extract what pieces of information are important to them. We propose an approach to automatically extract method descriptions from communications in bug tracking systems and mailing lists. We have evaluated the approach on bug reports and mailing lists from two open\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "101\n", "authors": ["56"]}
{"title": "On the use of domain terms in source code\n", "abstract": " Information about the problem domain of the software and the solution it implements is often embedded by developers in comments and identifiers. When using software developed by others or when are new to a project, programmers know little about how domain information is reflected in the source code. Programmers often learn about the domain from external sources such as books, articles, etc. Hence, it is important to use in comments and identifiers terms that are commonly known in the domain literature, as it is likely that programmers will use such terms when searching the source code. The paper presents a case study that investigated how domain terms are used in comments and identifiers. The study focused on three research questions: (1) to what degree are domain terms found in the source code of software from a particular problem domain?; (2) which is the preponderant source of domain terms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "99\n", "authors": ["56"]}
{"title": "On the use of stack traces to improve text retrieval-based bug localization\n", "abstract": " Many bug localization techniques rely on Text Retrieval (TR) models. The most successful approaches have been proven to be the ones combining TR techniques with static analysis, dynamic analysis, and/or software repositories information. Dynamic software analysis and software repositories mining bring a significant overhead, as they require instrumenting and executing the software, and analyzing large amounts of data, respectively. We propose a new static technique, named Lobster (Locating Bugs using Stack Traces and text Retrieval), which is meant to improve TR-based bug localization without the overhead associated with dynamic analysis and repository mining. Specifically, we use the stack traces submitted in a bug report to compute the similarity between their code elements and the source code of a software system. We combine the stack trace based similarity and the textual similarity provided by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "96\n", "authors": ["56"]}
{"title": "Clustering support for static concept location in source code\n", "abstract": " One of the most common comprehension activities undertaken by developers is concept location in source code. In the context of software change, concept location means finding locations in source code where changes are to be made in response to a modification request. Static techniques for concept location usually rely on searching the source code using textual information or on navigating the dependencies among software elements. In this paper we propose a novel static concept location technique, which leverages both the textual information present in the code and the structural dependencies between source code elements. The technique employs a textual search in that source code, which is clustered using the Border Flow algorithm, based on combining both structural and textual data. We evaluated the technique against a text search based baseline approach using data on almost 200 changes from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["56"]}
{"title": "Detecting missing information in bug descriptions\n", "abstract": " Bug reports document unexpected software behaviors experienced by users. To be effective, they should allow bug triagers to easily understand and reproduce the potential reported bugs, by clearly describing the Observed Behavior (OB), the Steps to Reproduce (S2R), and the Expected Behavior (EB). Unfortunately, while considered extremely useful, reporters often miss such pieces of information in bug reports and, to date, there is no effective way to automatically check and enforce their presence. We manually analyzed nearly 3k bug reports to understand to what extent OB, EB, and S2R are reported in bug reports and what discourse patterns reporters use to describe such information. We found that (i) while most reports contain OB (ie, 93.5%), only 35.2% and 51.4% explicitly describe EB and S2R, respectively; and (ii) reporters recurrently use 154 discourse patterns to describe such content. Based on these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "86\n", "authors": ["56"]}
{"title": "Source code exploration with Google\n", "abstract": " The paper presents a new approach to source code exploration, which is the result of integrating the Google Desktop Search (GDS) engine into the Eclipse development environment. The resulting search engine, named Google Eclipse Search (GES), provides improved searching in Eclipse software projects. The paper advocates for a component-based approach that allows us to develop strong tools, which support various maintenance tasks, by leveraging the strengths of existing frameworks and components. The development effort for such tools is reduced, while customization and flexibility, to fully support user needs, is maintained. GES allows developers to search software projects in a manner similar to searching the Internet or their own desktops. The proposed approach takes advantages of the power of GDS for quick and accurate searching and of Eclipse's extensibility. The paper discusses usage\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["56"]}
{"title": "Using structural and semantic measures to improve software modularization\n", "abstract": " Changes during software evolution and poor design decisions often lead to packages that are hard to understand and maintain, because they usually group together classes with unrelated responsibilities. One way to improve such packages is to decompose them into smaller, more cohesive packages. The difficulty lies in the fact that most definitions and interpretations of cohesion are rather vague and the multitude of measures proposed by researchers usually capture only one aspect of cohesion. We propose a new technique for automatic re-modularization of packages, which uses structural and semantic measures to decompose a package into smaller, more cohesive ones. The paper presents the new approach as well as an empirical study, which evaluates the decompositions proposed by the new technique. The results of the evaluation indicate that the decomposed packages have better cohesion\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["56"]}
{"title": "Class level fault prediction using software clustering\n", "abstract": " Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system.", "num_citations": "67\n", "authors": ["56"]}
{"title": "ARENA: an approach for the automated generation of release notes\n", "abstract": " Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["56"]}
{"title": "Supporting the evolution of a software visualization tool through usability studies\n", "abstract": " The paper presents a usability study conducted with graduate and undergraduate computer science students, designed to evaluate the effectiveness of a software visualization tool named sv3D, and to provide necessary user data for the evolution of the system. Sv3D is a software visualization tool for comprehension of large software, capable of displaying source code and associated metrics in three dimensions. The participants in the study answered two types of questions: one set provided objective measurements to support the formulated hypotheses with respect to the accuracy and speed of the users answering questions using sv3D; the second set of questions provided subjective measurements that were used to support the evolution of sv3D. We formulated two null hypotheses with respect to accuracy and time respectively. The collected data supported one hypothesis and rejected the other.", "num_citations": "60\n", "authors": ["56"]}
{"title": "Software re-modularization based on structural and semantic metrics\n", "abstract": " The structure of a software system has a major impact on its maintainability. To improve maintainability, software systems are usually organized into subsystems using the constructs of packages or modules. However, during software evolution the structure of the system undergoes continuous modifications, drifting away from its original design, often reducing its quality. In this paper we propose an approach for helping maintainers to improve the quality of software modularization. The proposed approach analyzes the (structural and semantic) relationships between classes in a package identifying chains of strongly related classes. The identified chains are used to define new packages with higher cohesion than the original package. The proposed approach has been empirical evaluated through a case study. The context of the study is represented by an open source system, JHotDraw, and two software systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["56"]}
{"title": "Recommending refactoring operations in large software systems\n", "abstract": " During its lifecycle, the internal structure of a software system undergoes continuous modifications. These changes push away the source code from its original design, often reducing its quality. In such cases, refactoring techniques can be applied to improve the readability and reducing the complexity of source code, to improve the architecture and provide for better software extensibility. Despite its advantages, performing refactoring in large and nontrivial software systems might be very challenging. Thus, a lot of effort has been devoted to the definition of automatic or semi-automatic approaches to support developer during software refactoring. Many of the proposed techniques are for recommending refactoring operations. In this chapter, we present guidelines on how to build such recommendation systems and how to evaluate them. We also highlight some of the challenges that exist in the field, pointing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["56"]}
{"title": "On the impact of refactoring operations on code quality metrics\n", "abstract": " Refactorings are behavior-preserving source code transformations. While tool support exists for (semi) automatically identifying refactoring solutions, applying or not a recommended refactoring is usually up to the software developers, who have to assess the impact that the transformation will have on their system. Evaluating the pros (e.g., the bad smell removal) and cons (e.g., side effects of the change) of a refactoring is far from trivial. We present RIPE (Refactoring Impact Prediction), a technique that estimates the impact of refactoring operations on source code quality metrics. RIPE supports 12 refactoring operations and 11 metrics and it can be used together with any refactoring recommendation tool. RIPE was used to estimate the impact on 8,103 metric values, for 504 refactorings from 15 open source systems. 38% of the estimates are correct, whereas the median deviation of the estimates from the actual\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["56"]}
{"title": "Visualization of CVS repository information\n", "abstract": " Mining software repositories is an important activity during software evolution, as the extracted data is used to support a variety of software maintenance tasks. The key information extracted from these repositories gives a picture of the changes on the software system. To have a complete picture, tailored to the needs of the developer, the extracted data needs to be filtered, aggregated, and presented to the users. In this paper we propose a new visualization for such data, which relies on an existing software visualization front-end, SourceViewer3D (sv3D). The new visualization allows users to define multiple views of the change history data, each view helps answer a set of questions relevant to specific maintenance tasks. Data can be viewed at different granularity (e.g., file, line of text, method, class) and comprehensive views can be defined, which display to the user multiple data types at the same time. Complex\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["56"]}
{"title": "Using observed behavior to reformulate queries during text retrieval-based bug localization\n", "abstract": " Text Retrieval (TR)-based approaches for bug localization rely on formulating an initial query based on a bug report. Often, the query does not return the buggy software artifacts at or near the top of the list (i.e., it is a low-quality query). In such cases, the query needs reformulation. Existing research on supporting developers in the reformulation of queries focuses mostly on leveraging relevance feedback from the user or expanding the original query with additional information (e.g., adding synonyms). In many cases, the problem with such lowquality queries is the presence of irrelevant terms (i.e., noise) and previous research has shown that removing such terms from the queries leads to substantial improvement in code retrieval. Unfortunately, the current state of research lacks methods to identify the irrelevant terms. Our research aims at addressing this problem and our conjecture is that reducing a low-quality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["56"]}
{"title": "Query-based configuration of text retrieval solutions for software engineering tasks\n", "abstract": " Text Retrieval (TR) approaches have been used to leverage the textual information contained in software artifacts to address a multitude of software engineering (SE) tasks. However, TR approaches need to be configured properly in order to lead to good results. Current approaches for automatic TR configuration in SE configure a single TR approach and then use it for all possible queries. In this paper, we show that such a configuration strategy leads to suboptimal results, and propose QUEST, the first approach bringing TR configuration selection to the query level. QUEST recommends the best TR configuration for a given query, based on a supervised learning approach that determines the TR configuration that performs the best for each query according to its properties. We evaluated QUEST in the context of feature and bug localization, using a data set with more than 1,000 queries. We found that QUEST is able\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["56"]}
{"title": "The effect of lexicon bad smells on concept location in source code\n", "abstract": " Experienced programmers choose identifier names carefully, in the attempt to convey information about the role and behavior of the labeled code entity in a concise and expressive way. In fact, during program understanding the names given to code entities represent one of the major sources of information used by developers. We conjecture that lexicon bad smells, such as, extreme contractions, inconsistent term use, odd grammatical structure, etc., can hinder the execution of maintenance tasks which rely on program understanding. We propose an approach to determine the extent of this impact and instantiate it on the task of concept location. In particular, we conducted a study on two open source software systems where we investigated how lexicon bad smells affect Information Retrieval-based concept location. In this study, the classes changed in response to past modification requests are located before and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["56"]}
{"title": "Using code ownership to improve ir-based traceability link recovery\n", "abstract": " Information Retrieval (IR) techniques have gained wide-spread acceptance as a method for automating traceability recovery. These techniques recover links between software artifacts based on their textual similarity, i.e., the higher the similarity, the higher the likelihood that there is a link between the two artifacts. A common problem with all IR-based techniques is filtering out noise from the list of candidate links, in order to improve the recovery accuracy. Indeed, software artifacts may be related in many ways and the textual information captures only one aspect of their relationships. In this paper we propose to leverage code ownership information to capture relationships between source code artifacts for improving the recovery of traceability links between documentation and source code. Specifically, we extract the author of each source code component and for each author we identify the \u0393\u00c7\u00a3context\u0393\u00c7\u00a5 she worked on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["56"]}
{"title": "Evaluating the specificity of text retrieval queries to support software engineering tasks\n", "abstract": " Text retrieval approaches have been used to address many software engineering tasks. In most cases, their use involves issuing a textual query to retrieve a set of relevant software artifacts from the system. The performance of all these approaches depends on the quality of the given query (i.e., its ability to describe the information need in such a way that the relevant software artifacts are retrieved during the search). Currently, the only way to tell that a query failed to lead to the expected software artifacts is by investing time and effort in analyzing the search results. In addition, it is often very difficult to ascertain what part of the query leads to poor results. We propose a novel pre-retrieval metric, which reflects the quality of a query by measuring the specificity of its terms. We exemplify the use of the new specificity metric on the task of concept location in source code. A preliminary empirical study shows that our metric\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["56"]}
{"title": "Link analysis algorithms for static concept location: an empirical assessment\n", "abstract": " During software evolution, one of the most important comprehension activities is concept location in source code, as it identifies the places in the code where changes are to be made in response to a modification request. Change requests (such as, bug fixing or new feature requests) are usually formulated in natural language, while the source code also includes large amounts of text. In consequence, many of the existing concept location techniques are based on text search or text retrieval. Such approaches reformulate concept location as a document retrieval problem. We refine and improve such solutions by leveraging dependencies between source code elements. Dependency information is used by a link analysis algorithm to rank the document space and to improve concept location based on text retrieval. We implemented our solution to concept location using the PageRank algorithm, used in web\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["56"]}
{"title": "Supporting extract class refactoring in eclipse: The aries project\n", "abstract": " During software evolution changes are inevitable. These changes may lead to design erosion and the introduction of inadequate design solutions, such as design antipatterns. Several empirical studies provide evidence that the presence of antipatterns is generally associated with lower productivity, greater rework, and more significant design efforts for developers. In order to improve the quality and remove antipatterns, refactoring operations are needed. In this demo, we present the Extract class features of ARIES (Automated Refactoring In EclipSe), an Eclipse plug-in that supports the software engineer in removing the \u0393\u00c7\u00a3Blob\u0393\u00c7\u00a5 antipattern.", "num_citations": "37\n", "authors": ["56"]}
{"title": "Semantic-driven program analysis\n", "abstract": " The tasks of maintenance and reengineering of an existing software system require a great deal of effort to be spent on understanding the source code to determine the behavior, organization, and architecture of the software. Different types of information (eg, static, dynamic, source code, documentation, etc.) will describe different features of the software system. There are at least two key aspects of the system that the user needs to understand:(1) what problem is the software solving and (2) how is the software achieving the solution.", "num_citations": "36\n", "authors": ["56"]}
{"title": "Predicting query quality for applications of text retrieval to software engineering tasks\n", "abstract": " Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["56"]}
{"title": "Software is data too\n", "abstract": " Software systems are designed and engineered to process data. However, software is data too. The size and variety of today's software artifacts and the multitude of stakeholder activities result in so much data that individuals can no longer reason about all of it. We argue in this position paper that data mining, statistical analysis, machine learning, information retrieval, data integration, etc., are necessary solutions to deal with software data. New research is needed to adapt existing algorithms and tools for software engineering data and processes, and new ones will have to be created.", "num_citations": "34\n", "authors": ["56"]}
{"title": "Semantic driven program analysis\n", "abstract": " The paper presents an approach to extract and to analyze the semantic content (i.e., problem and solution domain semantics) of existing software systems to support program understanding and software various maintenance tasks, such as: recovery of traceability links between documentation and source code, identification of abstract data types in legacy code, and identification of high-level concept clones in software. The semantic information is derived from the comments, documentation, and identifier names associated with the source code using information retrieval methods. The paper advocates for the use of latent semantic indexing as the underlying support for the semantic driven analysis. The presented results are based on the author's doctoral dissertation (Marcus, 2003).", "num_citations": "32\n", "authors": ["56"]}
{"title": "Utilizing association rules for the identification of errors in data\n", "abstract": " The paper analyzes the application of association rules to the problem of data cleansing and automatically identifying potential errors in data sets. Association rules are a fundamental class of patterns that exist in data. These patterns have been widely utilized (eg, market basket analysis) and extensive studies exist to find efficient association rule mining algorithms. Special attention is given in literature to the extension of binary association rules (eg, ratio, quantitative, generalized, multiple-level, constrained-based, distance-based, composite association rules). A new extension of the boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. These rules are used to identify outliers in data. An algorithm that finds these rules and identifies potential errors in data is proposed. A prototype tool is described and the results of applying it to a real-world data set are given. The tool is designed to use as little domain knowledge as possible and constitutes the first part in a proposed framework for automated data cleansing. Other approaches to data cleansing are described and compared.", "num_citations": "29\n", "authors": ["56"]}
{"title": "Reformulating queries for duplicate bug report detection\n", "abstract": " When bugs are reported, one important task is to check if they are new or if they were reported before. Many approaches have been proposed to partially automate duplicate bug report detection, and most of them rely on text retrieval techniques, using the bug reports as queries. Some of them include additional bug information and use complex retrieval- or learning-based methods. In the end, even the most sophisticated approaches fail to retrieve duplicate bug reports in many cases, leaving the bug triagers to their own devices. We argue that these duplicate bug retrieval tools should be used interactively, allowing the users to reformulate the queries to refine the retrieval. With that in mind, we are proposing three query reformulation strategies that require the users to simply select from the bug report the description of the software's observed behavior and/or the bug title, and combine them to issue a new query. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["56"]}
{"title": "An Algorithm for the Discovery of Arbitrary Length Ordinal Association Rules.\n", "abstract": " Association rule mining techniques are used to search attribute-value pairs that occur frequently together in a data set. Ordinal association rules are a particular type of association rules that describe orderings between attributes that commonly occur over a data set [9]. Although ordinal association rules are defined between any number of the attributes, only discovery algorithms of binary ordinal association rules (ie, rules between two attributes) exist. In this paper, we introduce the DOAR algorithm that efficiently finds all ordinal association rules of interest to the user, of any length, which hold over a data set. We present a theoretical validation of the algorithm and experimental results obtained by applying this algorithm on a real data set.", "num_citations": "22\n", "authors": ["56"]}
{"title": "Using bug descriptions to reformulate queries during text-retrieval-based bug localization\n", "abstract": " Text Retrieval (TR)-based approaches for bug localization rely on formulating an initial query based on the full text of a bug report. When the query fails to retrieve the buggy code artifacts, developers can reformulate the query and retrieve more candidate code documents. Existing research on query reformulation focuses mostly on leveraging relevance feedback from the user or on expanding the original query with additional information. We hypothesize that the title of the bug reports, the observed behavior, expected behavior, steps to reproduce, and code snippets provided by the users in bug descriptions, contain the most relevant information for retrieving the buggy code artifacts, and that other parts of the descriptions contain more irrelevant terms, which hinder retrieval. This paper proposes and evaluates a set of query reformulation strategies based on the selection of existing information in bug\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["56"]}
{"title": "Improving traceability link recovery methods through software artifact summarization\n", "abstract": " Analyzing candidate traceability links is a difficult, time consuming and error prone task, as it usually requires a detailed study of a long list of software artifacts of various kinds. One option to alleviate this problem is to select the most important features of the software artifacts that the developers would investigate. We discuss in this position paper how text summarization techniques could be used to address this problem. The potential gains in using summaries are both in terms of time and correctness of the traceability link recovery process.", "num_citations": "21\n", "authors": ["56"]}
{"title": "Automatic software summarization: the state of the art.\n", "abstract": " Automatic text summarization has been widely studied for more than fifty years. In software engineering, automatic summarization is an emerging area that shows great potential and poses new and exciting research challenges. This technical briefing provides an introduction to the state of the art and maps future research directions in automatic software summarization.", "num_citations": "20\n", "authors": ["56"]}
{"title": "Evolving a project-based software engineering course: A case study\n", "abstract": " This paper presents the evolution of a project-based course in Software Engineering for undergraduate students at the Universidad Nacional de Colombia. We describe and explain the changes we have done over six semesters. In addition, we investigate the effects of the changes on the students' grades and their project activities, by analyzing the software project repositories and the student feedback. Most of the changes had positive and expected results, while some had unexpected consequences. We distill a set of lessons regarding the class evolution, which will guide the future improvement of the course and which could be useful for other educators developing a similar course.", "num_citations": "20\n", "authors": ["56"]}
{"title": "Translating video recordings of mobile app usages into replayable scenarios\n", "abstract": " Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (eg, bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["56"]}
{"title": "we must keep going i guess\n", "abstract": " CFB: A Call for Benchmarks-for Software Visualization. The paper argues for the need of a benchmark, or suite of benchmarks, to exercise and evaluate software visualization methods, tools, and research. The intent of the benchmark (s) must be to further and motivate research in the field of using visualization methods to support understanding and analysis of real world and/or large scale software systems undergoing development or evolution. The paper points to other software engineering sub-fields that have recently benefited from benchmarks and explains how these examples can assist in the development of a benchmark for software visualization.", "num_citations": "15\n", "authors": ["56"]}
{"title": "On the vocabulary agreement in software issue descriptions\n", "abstract": " Many software comprehension tasks depend on how stakeholders textually describe their problems. These textual descriptions are leveraged by Text Retrieval (TR)-based solutions to more than 20 software engineering tasks, such as duplicate issue detection. The common assumption of such methods is that text describing the same issue in multiple places will have a common vocabulary. This paper presents an empirical study aimed at verifying this assumption and discusses the impact of the common vocabulary on duplicate issue detection. The study investigated 13K+ pairs of duplicate bug reports and Stack Overflow (SO) questions. We found that on average, more than 12.2% of the duplicate pairs do not have common terms. The other duplicate issue descriptions share, on average, 30% of their vocabulary. The good news is that these duplicates have significantly more terms in common than the non\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["56"]}
{"title": "Panel: Identifications of Concepts, Features, and Concerns in Source Code\n", "abstract": " Software development process transforms requirements into source code. These requirements are formulated as concepts from the problem domain or the solution domain; among them, features are the concepts that describe user selectable behavior. During software evolution, existing concepts and features of the system are changed, deleted, or new ones are added. In order to solve any of these change tasks, the existing concepts need to be precisely identified in the source code. This identification process was initially defined as the concept assignment (location) problem [2]. It is of no surprise that searching and browsing the software artifacts with the goal to identify parts of the source code that implement a concept from the software domain is one of the most common activities during software evolution. The existing methods of concept and feature location fall into two broad categories, based on the information\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["56"]}
{"title": "Generating query-specific class API summaries\n", "abstract": " Source code summaries are concise representations, in form of text and/or code, of complex code elements and are meant to help developers gain a quick understanding that in turns help them perform specific tasks. Generation of summaries that are task-specific is still a challenge in the automatic code summarization field. We propose an approach for generating on-demand, extrinsic hybrid summaries for API classes, relevant to a programming task, formulated as a natural language query. The summaries include the most relevant sentences extracted from the API reference documentation and the most relevant methods.", "num_citations": "11\n", "authors": ["56"]}
{"title": "A measure to assess the behavior of method stereotypes in object-oriented software\n", "abstract": " The implementation of software systems should ideally follow the design intentions of the system. However, this is not always the case - the design and implementation of software systems may diverge during software evolution. In this paper we propose a measure based on run time information to assess the consistency between the design and the implementation of OO methods. The measure is based on the analysis of the runtime behavior of methods and considers the frequency of fan-in and fan-out method calls. We analyze this measure with respect to the design intent of methods, reflected by their stereotype. We apply the proposed approach to data from three open source software systems and analyze the behavior of method stereotypes across the systems and within each system. The analysis shows that most methods behave as expected based on their stereotypes and it also detects cases that may need\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["56"]}
{"title": "Adapting to online teaching in software engineering courses\n", "abstract": " The COVID-19 worldwide pandemic caused sudden and unexpected changes in how we teach software engineering and other university courses. This paper presents an empirical study that aims to improve our understanding on how the assessment of student learning changed, in response to the transition from in-class to online courses. A questionnaire was distributed to instructors across the globe. The results indicate that the evaluation methodologies for most reported learning objectives have changed. Not surprising, in-class oral presentations and in-class exams are no longer used by the instructors for evaluations. We observed a trend of having fewer exams and more project-related evaluations after the transition. Not all instructors changed the way they evaluated student learning after the transition, however the majority reported their effort in student learning assessment increased after the transition\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["56"]}
{"title": "Empirical studies in software and systems traceability\n", "abstract": " Software and systems traceability is a critical element of any rigorous software development process and it is a required component of the approval and certification process in most safetyand security-critical systems. With the growing importance of these systems in our societies, traceability became a heavily studied research topic. Though important, traceability is also an element of software development processes that is most elusive. Manufacturers struggle in finding the right degree of traceability for their needs and in establishing accurate sets of traceability links. The cost, effort, and discipline needed to create and maintain trace links in a rapidly evolving software system can be extremely high. Moreover, its benefits often go unrealized in practice, either due to ill-defined and ad-hoc traceability processes, poor user training, or a lack of effective tool support. However, at the same time, traceability has been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["56"]}
{"title": "Extract package refactoring in aries\n", "abstract": " Software evolution often leads to the degradation of software design quality. In Object-Oriented (OO) systems, this often results in packages that are hard to understand and maintain, as they group together heterogeneous classes with unrelated responsibilities. In such cases, state-of-the-art re-modularization tools solve the problem by proposing a new organization of the existing classes into packages. However, as indicated by recent empirical studies, such approaches require changing thousands of lines of code to implement the new recommended modularization. In this demo, we present the implementation of an Extract Package refactoring approach in ARIES (Automated Refactoring In EclipSe), a tool supporting refactoring operations in Eclipse. Unlike state-of-the-art approaches, ARIES automatically identifies and removes single low-cohesive packages from software systems, which represent localized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["56"]}
{"title": "Using traceability links to identifying potentially erroneous artifacts during regulatory reviews\n", "abstract": " Safety critical systems emphasize the high quality of both hardware and software of a product since the safety of the product to the public tops all the other considerations. In these domains, regulatory agencies are entitled to conduct reviews on the entire range of artifacts produced from system design to system performance/maintenance. Regulatory review is comprised of the pre-market review and the post-market review. Each aspect of the regulatory review is time-consuming and laborious. In this paper we target situations when errors are identified in the reviewed software, either during pre- or post-market review. We propose an automated mechanism, which utilizes traceability information to identify lists of related software artifacts that are similar to those involved in the reported errors. With the tool recommendations, regulators can quickly investigate these suspicious locations and avoid the occurrence of future\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["56"]}
{"title": "Searching stackoverflow questions with multi-faceted categorization\n", "abstract": " StackOverflow provides answers for a huge number of software development questions that are frequently encountered by developers. However, searching relevant questions in StackOverflow is not always easy using the keyword based search engine provided by StackOverflow. A software development question can be characterized by multiple attributes, such as, its concern (eg, configuration problem, error handling, sample code, etc.), programming language, operating system, and involved middleware, framework, library and software technology. We propose a multi-faceted and interactive approach for searching StackOverflow questions (called MFISSO), which leverages these attributes of the questions. Our approach starts with an initial keyword-based query and extracts a multifaceted categorization from all the candidate questions using natural language processing and data mining. It then allows\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["56"]}
{"title": "On the quality of identifiers in test code\n", "abstract": " Meaningful, expressive identifiers in source code can enhance the readability and reduce comprehension efforts. Over the past years, researchers have devoted considerable effort to understanding and improving the naming quality of identifiers in source code. However, little attention has been given to test code, an important resource during program comprehension activities. To better grasp identifier quality in test code, we conducted a survey involving manually written and automatically generated test cases from ten open source software projects. The survey results indicate that test cases contain low quality identifiers, including the manually written ones, and that the quality of identifiers is lower in test code than in production code. We also investigated the use of three state-of-the-art rename refactoring recommenders for improving test code identifiers. The analysis highlights their limitations when applied to test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["56"]}
{"title": "Sv3D meets eclipse\n", "abstract": " Source Viewer 3D (sv3D) is a software visualization tool, based on a 3D extension of the file map metaphor. Sv3D has been developed as an independent visualization front end that represents source code and associated metrics as 3D metaphors, namely containers and poly cylinders. We developed a new version of sv3D as an Eclipse plug-in. Most of the functionality of the original tool is preserved, while new features are added. This tool demonstration will focus on the new features as well as on the benefits that the integration with Eclipse brings, such as, improved usability and integration with other plug-ins.", "num_citations": "3\n", "authors": ["56"]}
{"title": "Towards a Benchmark and Automatic Calibration for IR-Based Concept Location\n", "abstract": " There has been a great deal of research into the use of Information Retrieval (IR)-based techniques to support concept location in source code. Much of this research has been focused on determining how to use various IR techniques to support concept location. Very little attention has been given to the effect of different configurations of corpus building and indexing on query results. In this paper, we propose a tool designed to support large-scale studies of IR techniques in varying configurations of parameters with the intention of automatically calibrating these parameters. We also discuss preliminary efforts to create the benchmark data such studies require.", "num_citations": "1\n", "authors": ["56"]}
{"title": "Guest editor\u0393\u00c7\u00d6s introduction to the special section on the 2009 international conference on program comprehension (ICPC 2009)\n", "abstract": " Program comprehension is a vital blend of software engineering activities that support reuse, inspection, maintenance, evolution, migration, reverse engineering, and reengineering of existing software systems. The International Conference on Program Comprehension (ICPC) is the principal venue for work in the area of program comprehension as well as a leading venue for work in the areas of software analysis, reverse engineering, software evolution, and software visualization. ICPC 2009 took place during May 17\u0393\u00c7\u00f419, 2009, in Vancouver, British Columbia, Canada, co-located with the International Conference on Software Engineering (ICSE \u0393\u00c7\u00ff09). ICPC 2009 received a record number of technical paper submissions (74), which allowed us to assemble an excellent program that continues ICPC\u0393\u00c7\u00d6s tradition of providing a high-quality venue for sharing the latest advances in program comprehension. The program\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["56"]}