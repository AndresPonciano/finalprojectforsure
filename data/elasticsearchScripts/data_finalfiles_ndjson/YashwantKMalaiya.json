{"title": "Using neural networks in reliability prediction\n", "abstract": " It is shown that neural network reliability growth models have a significant advantage over analytic models in that they require only failure history as input and not assumptions about either the development environment or external parameters. Using the failure history, the neural-network model automatically develops its own internal model of the failure process and predicts future failures. Because it adjusts model complexity to match the complexity of the failure history, it can be more accurate than some commonly used analytic models. Results with actual testing and debugging data which suggest that neural-network models are better at endpoint predictions than analytic models are presented.< >", "num_citations": "350\n", "authors": ["455"]}
{"title": "Prediction of software reliability using connectionist models\n", "abstract": " A number of analytical models have been proposed in recent years for modeling software reliability growth trends. However, different models have different predictive capabilities at different phases of testing, and there is no single model that can be relied on for accurate predictions in all circumstances. The applicability of the connectionist approach is explored using various network models, training regimes, and data representation methods. An empirical comparison is made between this new approach and 5 well-known software reliability growth models using actual data sets from several different software projects. The results suggest that connectionist models may adapt well across different data sets and exhibit a better predictive accuracy. The analysis shows that the connectionist approach is capable of developing models of varying complexity.", "num_citations": "290\n", "authors": ["455"]}
{"title": "A new fault model and testing techniques for CMOS devices\n", "abstract": " CiNii \u8ad6\u6587 - A new fault model and testing techniques for CMOS devices CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 A new fault model and testing techniques for CMOS devices MALAIYA YK \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MALAIYA YK \u53ce\u9332\u520a\u884c\u7269 Proc. IEEE Int. Test Conference Proc. IEEE Int. Test Conference, 25-34, 1982 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 On Concurrent Error Detection of Asynchronous Circuits Using Mixed-Signal Approach KISHORE B. Ravi , NANYA Takashi IEICE transactions on information and systems 80(3), 351-362, 1997-03-25 \u53c2\u8003\u6587\u732e22\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10007221706 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 \u2026", "num_citations": "244\n", "authors": ["455"]}
{"title": "Antirandom testing: Getting the most out of black-box testing\n", "abstract": " Random testing is a well known concept that requires that each test is selected randomly regardless of the test previously applied. The paper introduces the concept of antirandom testing. In this testing strategy each test applied is chosen such that its total distance from all previous tests is maximum. Two distance measures are defined. Procedures to construct antirandom sequences are developed. A checkpoint encoding scheme is introduced that allows automatic generation of efficient test cases. Further developments and studies needed are identified.", "num_citations": "180\n", "authors": ["455"]}
{"title": "A detailed examination of bridging faults\n", "abstract": " CiNii \u8ad6\u6587 - A detailed examination of bridging faults CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf [\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 A detailed examination of bridging faults MALAIYA YK \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MALAIYA YK \u53ce\u9332\u520a\u884c\u7269 Proc. Int. Conf. on Computer Design Proc. Int. Conf. on Computer Design, 78-81, 1986 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 CMOS\u8ad6\u7406\u56de\u8def\u306e\u767a\u632f\u3092\u751f\u3058\u308bIC\u30d4\u30f3\u77ed\u7d61\u6545\u969c\u691c\u51fa \u56de\u8def \u4e00\u5bae \u6b63\u535a , \u6a4b\u722a \u6b63\u6a39 , \u56db\u67f3 \u6d69\u4e4b , \u70ba\u8c9e \u5efa\u81e3 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u8ad6\u6587\u8a8c. DI, \u60c5\u5831\u30fb\u30b7\u30b9\u30c6\u30e0, I-\u60c5\u5831\u51e6\u7406 = The transactions of the Institute of Electronics, Information and Communication Engineers. DI 86(6), 402-411, 2003-06-01 \u53c2\u8003\u6587\u732e16\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) /\u2026", "num_citations": "123\n", "authors": ["455"]}
{"title": "Testing for timing faults in synchronous sequential integrated circuits\n", "abstract": " CiNii \u8ad6\u6587 - Testing for timing faults in synchronous sequential integrated circuits CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec\u7248 \u306e\u516c\u958b\u306b\u3064\u3044\u3066 Testing for timing faults in synchronous sequential integrated circuits MALAIYA YK \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MALAIYA YK \u53ce\u9332\u520a\u884c\u7269 Proc. Int. Test Conf., 1983 Proc. Int. Test Conf., 1983, 1983 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Equivalence of Sequential Transition Test Generation and Constrained Combinational Stuck-at Test Generation Iwagaki Tsuyoshi , Ohtake Satoshi , Fujiwara Hideo \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. DC, \u30c7\u30a3\u30da\u30f3\u30c0\u30d6\u30eb\u30a3\u30f3\u30b0 \u2026", "num_citations": "113\n", "authors": ["455"]}
{"title": "Predictability of software-reliability models\n", "abstract": " A two-component predictability measure that characterizes the long-term predictive capability of a model is presented. One component, average error, measures how well a model predicts throughout the testing phase. The other component, average bias, measures the general tendency to overestimate or underestimate the number of faults. Data sets for both large and small projects from diverse sources with various initial fault density ranges have been analyzed. The results show that: (i) the logarithmic model seems to predict well in most data sets, (ii) the inverse polynomial model can be used as the next alternative, and (iii) the delayed S-shaped model, which in some data sets fit well generally performed poorly. The statistical analysis shows that these models have appreciably different predictive capabilities.< >", "num_citations": "104\n", "authors": ["455"]}
{"title": "Prediction of software reliability using neural networks\n", "abstract": " Software reliability growth models have achieved considerable importance in estimating reliability of software products. This paper explores the use of feed-forward neural networks as a model for software reliability growth prediction. To empirically evaluate the predictive capability of this new approach data sets from di erent software projects are used. The neural networks approach exhibits a consistent behavior in prediction and the predictive performance is comparable to that of parametric models.", "num_citations": "98\n", "authors": ["455"]}
{"title": "Requirements volatility and defect density\n", "abstract": " Ideally the requirements for a software system should be completely and unambiguously determined before design, coding and testing rake place. In practice, often there are changes in the requirements, causing software components to be redesigned, deleted or added. This requirements volatility causes the software to have a higher defect density. In this paper we analytically examine the influence of requirement changes taking place during different times by examining the consequences of software additions, removals and modifications. We take into account interface defects which arise due to errors at the interfaces among software sections. We compare the resulting defect density in the presence of requirement volatility, with the defect density that would have resulted had requirements not changed. The results show that if the requirement changes take place close to the release date, there is a greater impact\u00a0\u2026", "num_citations": "96\n", "authors": ["455"]}
{"title": "The coverage problem for random testing\n", "abstract": " Random testing is frequently an attractive alternative to deterministic test generation. How to estimate the coverage obtained by random testing is an important problem. This paper considers a possible technique for combinational circuits. Random testing properties of several combinational circuits are examined.", "num_citations": "93\n", "authors": ["455"]}
{"title": "Defining and assessing quantitative security risk measures using vulnerability lifecycle and cvss metrics\n", "abstract": " Known vulnerabilities which have been discovered but not patched represents a security risk which can lead to considerable financial damage or loss of reputation. They include vulnerabilities that have either no patches available or for which patches are applied after some delay. Exploitation is even possible before public disclosure of a vulnerability. This paper formally defines risk measures and examines possible approaches for assessing risk using actual data. We explore the use of CVSS vulnerability metrics which are publically available and are being used for ranking vulnerabilities. Then, a general stochastic risk evaluation approach is proposed which considers the vulnerability lifecycle starting with discovery. A conditional risk measure and assessment approach is also presented when only known vulnerabilities are considered. The proposed approach bridges formal risk theory with industrial approaches currently being used, allowing IT risk assessment in an organization, and a comparison of potential alternatives for optimizing remediation. These actual data driven methods will assist managers with software selection and patch application decisions in quantitative manner.", "num_citations": "85\n", "authors": ["455"]}
{"title": "A continuous-parameter Markov model and detection procedures for intermittent faults\n", "abstract": " A continuous-parameter Markov model for intermittent faults in digital systems is presented. This continuous model is more realistic than discrete-parameter models previously presented by other authors. The results obtained using the proposed model can be reduced to those obtained from the previous models, by using appropriate approximations.", "num_citations": "83\n", "authors": ["455"]}
{"title": "An examination of fault exposure ratio\n", "abstract": " The fault exposure ratio, K, is an important factor that controls the per-fault hazard rate, and hence, the effectiveness of the testing of software. The authors examine the variations of K with fault density, which declines with testing time. Because faults become harder to find, K should decline if testing is strictly random. However, it is shown that at lower fault densities K tends to increase. This is explained using the hypothesis that real testing is more efficient than strictly random testing especially at the end of the test phase. Data sets from several different projects (in USA and Japan) are analyzed. When the two factors, e.g., shift in the detectability profile and the nonrandomness of testing, are combined the analysis leads to the logarithmic model that is known to have superior predictive capability.< >", "num_citations": "79\n", "authors": ["455"]}
{"title": "Modeling and testing for timing faults in synchronous sequential circuits\n", "abstract": " Even with proper design, integrated circuits and systems can have timing problems because of physical faults or variation of parameters. The authors introduce a fault model that takes into account timing related failures in both the combinational logic and the storage elements. Using their fault model and the system's requirements for proper operation, the authors propose ways to handle flipflop-to-flipflop delay, path selection, initialization, error propagation, race-around, and anomalous behavior. They discuss the advantages of scan designs like LSSD and the effectiveness of random delay testing.", "num_citations": "76\n", "authors": ["455"]}
{"title": "What do the software reliability growth model parameters represent?\n", "abstract": " Here we investigate the underlying basis connecting the software reliability growth models to the software testing and debugging process. This is important for several reasons. First, if the parameters have an interpretation, then they constitute a metric for the software test process and the software under test. Secondly, it may be possible to estimate the parameters even before testing begins. These a priori values can serve as a check for the values computed at the beginning of testing, when the test-data is dominated by short term noise. They can also serve as initial estimates when iterative computations are used. Among the two-parameter models, the exponential model is characterized by its simplicity. Both its parameters have a simple interpretation. However, in some studies it has been found that the logarithmic Poisson model has superior predictive capability. Here we present a new interpretation for the\u00a0\u2026", "num_citations": "73\n", "authors": ["455"]}
{"title": "Module size distribution and defect density\n", "abstract": " Data from several projects show a significant relationship between the size of a module and its defect density. We address implications of this observation. Does the overall defect density of a software project vary with its module size distribution? Even more interesting is the question can we exploit this dependence to reduce the total number of defects? We examine the available data sets and propose a model relating module size and defect density. It takes into account defects that arise due to the interconnections among the modules as well as defects that occur due to the complexity of individual modules. Model parameters are estimated using actual data. We then present a key observation that allows use of this model for not just estimation of the defect density, but also potentially optimizing a design to minimize defects. This observation, supported by several data sets examined, is that the module sizes often\u00a0\u2026", "num_citations": "69\n", "authors": ["455"]}
{"title": "Software Reliability Models: Developments, Evaluation and Applications\n", "abstract": " Software Reliability Models | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Reliability Models: Developments, Evaluation and Applications ABSTRACT No abstract available. Index Terms (auto-classified) 1.Software Reliability Models 1.Computing methodologies 1.Computer graphics 1.Graphics systems and interfaces 1.Virtual reality 2.Modeling and simulation 1.Model development and analysis 1.Modeling methodologies 2.General and reference 1.Cross-computing tools and techniques 1.Reliability 3.Human-centered computing 1.Human computer interaction (HCI) 1.Interaction paradigms 1.Mixed / 4.its \u2026", "num_citations": "69\n", "authors": ["455"]}
{"title": "Pass-transistor logic design\n", "abstract": " Logic functions implemented using CMOS transmission gates provide a moderate improvement in area and speed over logic gate implementations. Several techniques for the implementation of pass transistor logic are presented. These techniques use only nMOS transistors in the pass network. The output logic level is restored using additional circuitry. The proposed designs require less silicon area, less power dissipation, and operate at higher speeds compared with the conventional CMOS pass-transistor networks. The speed of operation depends mainly on the circuitry used to restore the output signal of the pass network. The different techniques are compared with respect to the layout area and operating speed.", "num_citations": "65\n", "authors": ["455"]}
{"title": "Automatic Test Generation using Checkpoint Encoding and Antirandom Testing\n", "abstract": " The implementation of an efficient automatic test generation scheme for black box testing is discussed. It uses checkpoint encoding and antirandom testing schemes. Checkpoint encoding converts test generation to a binary problem. The checkpoints are selected as the boundary and illegal cases in addition to valid cases to probe the input space. Antirandom testing selects each test case such that it is as different as possible from all the previous tests. The implementation is illustrated using benchmark examples that have been used in the literature. Use of random testing both with checkpoint encoding and without is also reported. Comparison and evaluation of the effectiveness of these methods is also presented. Implications of the observations for larger software systems are noted. Overall, antirandom testing gives higher code coverage than encoding random testing, which gives higher code coverage than pure\u00a0\u2026", "num_citations": "64\n", "authors": ["455"]}
{"title": "Software vulnerability markets: Discoverers and buyers\n", "abstract": " Some of the key aspects of vulnerability\u2014discovery, dissemination, and disclosure\u2014have received some attention recently. However, the role of interaction among the vulnerability discoverers and vulnerability acquirers has not yet been adequately addressed. Our study suggests that a major percentage of discoverers, a majority in some cases, are unaffiliated with the software developers and thus are free to disseminate the vulnerabilities they discover in any way they like. As a result, multiple vulnerability markets have emerged. In some of these markets, the exchange is regulated, but in others, there is little or no regulation. In recent vulnerability discovery literature, the vulnerability discoverers have remained anonymous individuals. Although there has been an attempt to model the level of their efforts, information regarding their identities, modes of operation, and what they are doing with the discovered vulnerabilities has not been explored. Reports of buying and selling of the vulnerabilities are now appearing in the press; however, the existence of such markets requires validation, and the natures of the markets need to be analyzed. To address this need, we have attempted to collect detailed information. We have identified the most prolific vulnerability discoverers throughout the past decade and examined their motivation and methods. A large percentage of these discoverers are located in Eastern and Western Europe and in the Far East. We have contacted several of them in order to collect firsthand information regarding their techniques, motivations, and involvement in the vulnerability markets. We examine why many of the discoverers\u00a0\u2026", "num_citations": "63\n", "authors": ["455"]}
{"title": "Assessing vulnerability exploitability risk using software properties\n", "abstract": " Attacks on computer systems are now attracting increased attention. While the current trends in software vulnerability discovery indicate that the number of newly discovered vulnerabilities continues to be significant, the time between the public disclosure of vulnerabilities and the release of an automated exploit is shrinking. Thus, assessing the vulnerability exploitability risk is critical because this allows decision-makers to prioritize among vulnerabilities, allocate resources to patch and protect systems from these vulnerabilities, and choose between alternatives. Common vulnerability scoring system (CVSS) metrics have become the de facto standard for assessing the severity of vulnerabilities. However, the CVSS exploitability measures assign subjective values based on the views of experts. Two of the factors in CVSS, Access Vector and Authentication, are the same for almost all vulnerabilities. CVSS does\u00a0\u2026", "num_citations": "62\n", "authors": ["455"]}
{"title": "Vulnerability discovery modeling using Weibull distribution\n", "abstract": " A vulnerability discovery model describes the variation in the vulnerability discovery rate during the lifetime of a software system and can be used to assess risk and to evaluate possible mitigation approaches. A few vulnerability discovery models have recently been proposed. The AML Logistic model has been found to provide the best fit in several cases. Weibull distribution, which can model an asymmetric pdf, is often used for reliability evaluation in some fields but has not been used for modeling vulnerability discovery. Here we propose a new Weibull distribution based on vulnerability discovery model and compare it with the existing AML Model. The results show that the new model performs well in many cases, and may be considered as an alternative to the AML model.", "num_citations": "56\n", "authors": ["455"]}
{"title": "Using attack surface entry points and reachability analysis to assess the risk of software vulnerability exploitability\n", "abstract": " An unpatched vulnerability can lead to security breaches. When a new vulnerability is discovered, it needs to be assessed so that it can be prioritized. A major challenge in software security is the assessment of the potential risk due to vulnerability exploitability. CVSS metrics have become a de facto standard that is commonly used to assess the severity of a vulnerability. The CVSS Base Score measures severity based on exploitability and impact measures. CVSS exploitability is measured based on three metrics: Access Vector, Authentication, and Access Complexity. However, CVSS exploitability measures assign subjective numbers based on the views of experts. Two of its factors, Access Vector and Authentication, are the same for almost all vulnerabilities. CVSS does not specify how the third factor, Access Complexity, is measured, and hence we do not know if it considers software properties as a factor. In this\u00a0\u2026", "num_citations": "52\n", "authors": ["455"]}
{"title": "The scaling problem in neural networks for software reliability prediction.\n", "abstract": " Recently neural networks have been applied for software reliability growth prediction. Although the predictive capability of the neural network models are better than some of the well known analytic models, the scaling problem has not been completely addressed yet. With the present neural network models, it is necessary to scale the cumulative faults over a 0.0 to 1.0 range. So the user has to estimate in advance a maximum value for the total number of faults to be detected at the end of the test phase. In practice, such an estimate may not be accurate. Use of an inaccurate value for scaling the cumulative faults can severely affect the predictive capability of neural network models. This paper presents a solution to the scaling problem in terms of a clipped linear unit in the output layer. With a clipped linear output unit, the neural networks can predict positive values in any unbounded range. We demonstrate the applicability of the proposed network structure with three data sets and compare its predictive accuracy with that of our earlier models. Expressions for the failure rate process represented by the models of the proposed network structure are also derived.", "num_citations": "52\n", "authors": ["455"]}
{"title": "On accuracy of switch-level modeling of bridging faults in complex gates\n", "abstract": " Bridging faults have been shown to be a major failure mode in VLSI devices. This study examines nMOS and CMOS complex gates in detail for bridging faults. Analysis is carried out using both switch and circuit level models for comparison. It is shown that in most cases, the switch level analysis predicts the correct behavior. A set of conditions are presented, under which the switch level analysis may fail to predict the correct behavior. These conditions can be used for accurate switch level test generation and simulation.", "num_citations": "52\n", "authors": ["455"]}
{"title": "To fear or not to fear that is the question: Code characteristics of a vulnerable functionwith an existing exploit\n", "abstract": " Not all vulnerabilities are equal. Some recent studies have shown that only a small fraction of vulnerabilities that have been reported has actually been exploited. Since finding and addressing potential vulnerabilities in a program can take considerable time and effort, recently effort has been made to identify code that is more likely to be vulnerable. This paper tries to identify the attributes of the code containing a vulnerability that makes the code more likely to be exploited. We examine 183 vulnerabilities from the National Vulnerability Database for Linux Kernel and Apache HTTP server. These include eighty-two vulnerabilities that have been found to have an exploit according to the Exploit Database. We characterize the vulnerable functions that have no exploit and the ones that have an exploit using eight metrics. The results show that the difference between a vulnerability that has no exploit and the one that has\u00a0\u2026", "num_citations": "46\n", "authors": ["455"]}
{"title": "A framework for software security risk evaluation using the vulnerability lifecycle and cvss metrics\n", "abstract": " A vulnerability that has been discovered but is unpatched represents a security risk to a system. During the lifetime of a software system, new vulnerabilities are discovered over time. There are two opposing actors, the patch developers and the potential exploiters. An exploit can happen immediately after a disclosure, perhaps even before the disclosure if the discovery is made by a black-hat finder. Here, a framework for software risk evaluation with respect to the vulnerability lifecycle is proposed. Risk can be evaluated using the likelihood of a security breach and the impact of that adverse event on the system. The proposed approach models the vulnerability lifecycle as a stochastic process. Some of the CVSS metrics can be used to evaluate the impact of the breach. The model uses the information about the transition rates with the related distributions and can lead to simplified as well as detailed modeling methods. It allows a comparison of software systems in terms of the risk and potential approaches for optimization of remediation.", "num_citations": "46\n", "authors": ["455"]}
{"title": "Test generation for delay faults using stuck-at-fault test set\n", "abstract": " Delay faults are referred to the out-ofspecification path delays which result in unstabilized or incorrect circuit behaviors. The abnormal delay in cOl! lponentscan cause intermIttent (transIent) faults which are responsible for the most faults in the dIgital equipment in the field. When one uses design-for-testability discipline, such as the level-sensitive scan design (LSSD), testing for the timing failures is required mainly for delay faults which exceed the maximum allowable propagation time along signal paths in the combinational network part of a digital network. The idea of path sensitization is used for finding tests for delay faults. In this paper, we analyze the relationships between stuck-at-fault tests and delay tests (tests for delay faults). It is shown that with only a minor modification, many stuck-atfault tests can be adopted as delay tests and such delay tests may simultaneously detect stuck-atfaults and delay faults along more than one path. Consequently, we propose to start delay-test generation with a given stuck-at-fault test set which is usually available. A selection process is applied to select tests according to certain criteria. Tests for the uncovered delay faults are then generated by using currently available methods. An experimental selector has been implemented in PL/I on an Itel AS-6 computer. The test~ atterns generated for an off-the-shelf 4-bit ALU chip shows a good coverage is obtained by using only a small number of tests.", "num_citations": "46\n", "authors": ["455"]}
{"title": "Faulty behavior of storage elements and its effects on sequential circuits\n", "abstract": " It is often assumed that the faults in storage elements (SEs) can be modeled as output/input stuck-at faults of the element. They are implicitly considered equivalent to the stuck-at faults in the combinational logic surrounding the SE cells. Transistor-level faults in common SEs are examined here. A more accurate higher level fault model for elementary SEs that better represents the physical failures is presented. It is shown that a minimal (stuck-at) model may be adequate if only modest fault coverage is desired. The enhanced model includes some common fault behaviors of SEs that are not covered by the minimal fault model. These include data-feedthrough and clock-feedthrough behaviors, as well as problems with logic level retention. Fault models for complex SE cells can be obtained without a significant loss of information about the structure of the circuit. The detectability of feedthrough faults is considered.< >", "num_citations": "45\n", "authors": ["455"]}
{"title": "On input profile selection for software testing\n", "abstract": " Analyzes the effect of input profile selection on software testing using the concept of a fault detectability profile. The optimality of the input profile during testing depends on factors such as the planned testing effort and the fault detectability profile. To achieve ultra-reliable software, it is preferable to select the test input uniformly among the different input domains. On the other hand, if the testing effort is limited due to cost or schedule constraints, one should test only the highly used input domains. Use of an operational profile is also needed for the accurate determination of operational reliability.< >", "num_citations": "44\n", "authors": ["455"]}
{"title": "Enhancement of resolution in supply current based testing for large ICs\n", "abstract": " Current drawn by a static CMOS VLSI integrated circuit during quiescent periods is extremely small and is normally of the order of nanoamperes. However, it is remarkably susceptible to a number of failure modes. Many faults present in such ICs cause the quiescent power-supply cur-rent (IDDQ) to increase by several orders of magnitude. Some of these faults may not manifest as logical faults, and would not be detected by traditional IC test techniques. In large ICs, it may be hard to distinguish between larger IDDQ due to defects and elevated IDDQ due to normal parameter variations. A statistical characterization of the problem is presented. This can be used to determine the optimal size of partitions. A new information compression scheme is presented which can significantly enhance resolu-tion.", "num_citations": "40\n", "authors": ["455"]}
{"title": "Predictability measures for software reliability models\n", "abstract": " It is critical to be able to achieve an acceptable quality level before a software package is released. It is often important to meet a target release date. To be able to estimate the testing efforts required, it is necessary to use a software reliability growth model. While several different software reliability growth models have been proposed, there exist no clear guidelines about which model should be used. Here a twwcomponent predictability measure is presented that characterizes the long term predictability of a model. The first component, average predictability, measures how well a model predicts throughout the testing phase. The sec-ond component, average bias, is a measure of the general tendency to overestimate or underestimate the number of faults. Data sets for both large and small projects from diverse sources have been analyzed. Results presented here indicate that some models perform better than others in most cases.", "num_citations": "40\n", "authors": ["455"]}
{"title": "Neural networks for software reliability engineering\n", "abstract": " Artificial neural networks (or simply neural networks) are a computa-tional metaphor inspired by studies of the brain and nervous systems in biological organisms. They are highly idealized mathematical mod-els of the essence of our present understanding of how simple nervous systems work. Neural networks operate on the principle of learning from examples; no model is speci\ufb01ed a priori. Neural networks are likened to nonparametric models in the statistical literature. Recent development in neural networks has shown that they can be applied in a variety of problem domains. For example, neural networks are used to solve complex nonlinear function approximation problems, difficult linearly inseparable pattern classi\ufb01cation problems, speech recogni-tion and control problems, and complex time-series modeling problems. Though the neural network technology has been applied in various \ufb01elds, its utility in\u00a0\u2026", "num_citations": "37\n", "authors": ["455"]}
{"title": "Reliability measure of hardware redundancy fault-tolerant digital systems with intermittent faults\n", "abstract": " While significant results are available which allow estimation of reliability measure for systems with permanent faults, no generally applicable results are available for intermittent (transient) faults. Methods are presented here which allow reliability evaluation for systems with both intermittent and permanent faults. Two reliability measures, instantaneous and durational reliabilities, are defined and methods to compute them are given. Computed results for the durational reliability for various redundancy schemes are compared.", "num_citations": "36\n", "authors": ["455"]}
{"title": "Antirandom Testing: A Distance-Based Approach.\n", "abstract": " Random testing is a form of black-box testing which does not require knowledge of the circuit under test. It avoids the problem of deterministic test generation using structural information about the circuit under test. Available evidence suggests that pseudorandom testing may be a reasonable choice for obtaining a moderate degree of confidence, however it becomes inefficient when only hard-to-test faults remain [1]. Generally in BIST environment, pseudorandom test vectors are used for testing digital circuits. In pseudorandom testing, each test vector is chosen with equal probability out of a pool that initially contains M different vectors without replacement [2]. A common approach is to generate them using an autonomous linear feedback shift register (ALFSR) based on a primitive polynomial [3]. An alternative approach is to use a cellular automata-based generator [4, 5]. The use of Galois LFSRs (GLFSRs) as test pattern generators for BIST schemes employing multiple scan chains is investigated in [6, 7]. The effectiveness of such techniques can be enhanced by using some information about the circuit under test and tailoring the generator hardware to achieve higher coverage faster [8]. Weighted random pattern techniques adjust the probability generating 1\u2019s and 0\u2019s at each input, rather than using equal probabilities. Such an approach has been shown to increase efficiency of testing, especially for modules such as PLAs [9]. In some cases test generation for self-testing may be implemented in software [10]. Pseudorandom testing does not exploit some information that is available in black-box testing environment. This information consists of the\u00a0\u2026", "num_citations": "35\n", "authors": ["455"]}
{"title": "Design of CMOS circuits for stuck-open fault testability\n", "abstract": " A CMOS design that offers highly testable CMOS circuits is presented. The design requires a minimal amount of extra hardware for testing. The test phase for the proposed design is simple and uses a single test vector to detect a fault. The design offers the detection of transistor stuck-open faults deterministically. In this design, the tests are not invalidated due to timing skews/delays, glitches, or charge redistribution among the internal nodes.< >", "num_citations": "35\n", "authors": ["455"]}
{"title": "Modeling skewness in vulnerability discovery\n", "abstract": " A vulnerability discovery model attempts to model the rate at which the vulnerabilities are discovered in a software product. Recent studies have shown that the S\u2010shaped Alhazmi\u2013Malaiya Logistic (AML) vulnerability discovery model often fits better than other models and demonstrates superior prediction capabilities for several major software systems. However, the AML model is based on the logistic distribution, which assumes a symmetrical discovery process with a peak in the center. Hence, it can be expected that when the discovery process does not follow a symmetrical pattern, an asymmetrical distribution based discovery model might perform better. Here, the relationship between performance of S\u2010shaped vulnerability discovery models and the skewness in target vulnerability datasets is examined. To study the possible dependence on the skew, alternative S\u2010shaped models based on the Weibull, Beta\u00a0\u2026", "num_citations": "32\n", "authors": ["455"]}
{"title": "A survey of methods for intermittent fault analysis*\n", "abstract": " As digital circuits grow in complexity, and as they are used in more and more critical applications, their reliability becomes an important consideration. This directs the attention of researchers involved in this field to two basic questions- first, how to design more reliable systems; second, how can something be said about the reliability of a particular system?", "num_citations": "32\n", "authors": ["455"]}
{"title": "Modeling learningless vulnerability discovery using a folded distribution\n", "abstract": " A vulnerability discovery model describes the vulnerability discovery rate in a software system, and predicts the future behavior. It can allow the IT managers and developers to allocate their resources optimally by timely development and application of patches. Such models also allow the end-users to assess security risk in their systems. Recently, researchers have proposed a few vulnerability discovery models. The models are based on different assumptions, and thus differ in their accuracy and prediction capabilities. Among these models, the AML model has been found to have performed better in many cases in terms of model fitting and prediction capabilities. The AML model assumes that the discovery rate is symmetric. However, it has been noted that there are cases when the discovery trend is asymmetric. In this paper, we investigate the applicability of using a new vulnerability discovery model called Folded model, based on the Folded normal distribution, and compare it with the AML model. Results show that Folded model performs better than the AML model in general for both model fitting and prediction capabilities in cases when the learning phase is not present.", "num_citations": "31\n", "authors": ["455"]}
{"title": "Estimating the number of residual defects [in software]\n", "abstract": " The number of residual defects is one of the most important factors that allows one to decide if a piece of software is ready to be released. In theory, one can find all the defects and count them. However, it is impossible to find all the defects within a reasonable amount of time. Estimating the defect density can become difficult for high-reliability software, since the remaining defects can be extremely hard to test for. One possible way is to apply the exponential software reliablility growth model (SRGM), and thus estimate the total number of defects present at the beginning of testing. In this paper, we show the problems with this approach and present a new approach based on software test coverage. Test coverage directly measures the thoroughness of testing, avoiding the problem of variations of test effectiveness. We apply this model to actual test data in order to project the residual number of defects. This method\u00a0\u2026", "num_citations": "31\n", "authors": ["455"]}
{"title": "Comparing and evaluating CVSS base metrics and microsoft rating system\n", "abstract": " Evaluating the accuracy of vulnerability security risk metrics is important because incorrectly assessing a vulnerability to be more critical could lead to a waste of limited resources available and ignoring a vulnerability incorrectly assessed as not critical could lead to a breach with a high impact. In this paper, we compare and evaluate the performance of the CVSS Base metrics and Microsoft Rating system. The CVSS Base metrics are the de facto standard that is currently used to measure the severity of individual vulnerabilities. The Microsoft Rating system developed by Microsoft has been used for some of the most widely used systems. Microsoft software vulnerabilities have been assessed by both the Microsoft metrics and the CVSS Base metrics which makes their comparison feasible. The two approaches, the technical analysis approach (Microsoft) and the expert opinions approach (CVSS) differ significantly. To\u00a0\u2026", "num_citations": "29\n", "authors": ["455"]}
{"title": "Estimating the number of defects: A simple and intuitive approach\n", "abstract": " The number of defects is an important measure of software quality which is widely used in industry. Unfortunately, accurate estimation of defect density can be a difficult task. Sampling techniques generally assume that the faults found are a representative sample of the all existing faults, which results in inaccurate estimates. Other existing techniques provide little information in addition to the number of faults already found. Software test coverage tools can easily and accurately measure the extent to which the software has been exercised. Both testing time and test coverage can be used as measures to model the defect finding process. However test coverage is a more direct measure of test effectiveness and can be expected to correlate better with the number of defects found.Here we describe a simple and intuitive procedure which can be used to estimate the total number of residual defects, once a suitable coverage level has been achieved. The technique is consistent with common testing approaches used. The method will be illustrated using actual data and is compared with existing approaches. Our results show that the method yields consistent estimates. An enhanced version of this approach is being implemented in a GUI tool.", "num_citations": "28\n", "authors": ["455"]}
{"title": "ROBUST: a next generation software reliability engineering tool\n", "abstract": " In this paper, we propose an approach for linking the isolated research results together and describe a tool supporting the incorporation of the various existing modeling techniques. The new tool, named ROBUST, is based on recent research results validated using actual data. It employs knowledge of static software complexity metrics, dynamic failure data and test coverages for software reliability estimation and prediction at different software development phases.", "num_citations": "25\n", "authors": ["455"]}
{"title": "Self-diagnosis of non-homogeneous distributed systems\n", "abstract": " This paper presents some theoretical results on self-diagnosis of non-homogeneous distributed systems, based on a graphtheoretical model. A processing unit in a system can be either a testing unit or a non-testing unit. The former tests at least one other unit while the latter is only tested by others. Self-diagnosis is achieved in three stages:(1) diagnosis is made for individual units via test links,(2) testing units exchange information with one another so that each completes its diagnosis for the whole system, and (3) testing units broadcast information to nontesting units and let them achieve their diagnosis. Selfdiagnosability measures allowing non-homogenity in a system are discussed.", "num_citations": "25\n", "authors": ["455"]}
{"title": "Testable design for BiCMOS stuck-open fault detection\n", "abstract": " BiCMOS devices exhibit sequential behavior under transistor stuck-open (s-open) faults. In addition to the sequential behavior, delay faults are also present. Detection of s-open faults exhibiting sequential behavior need two or multipattern sequences, and delay faults are all the more difficult to detect. A new design for testability scheme is presented for single BJT BiCMOS logic gates which uses only two extra transistors to improve the circuit testability regardless of timing skews/delays, glitches or charge sharing among internal nodes. It requires only a single vector instead of the two or multipattern sequences. The testable design scheme presented also avoids the requirement of generating tests for delay faults.< >", "num_citations": "24\n", "authors": ["455"]}
{"title": "Fault modelling of ECL devices\n", "abstract": " Logic behaviour of an ECL OR/NOR gate under different physical faults is examined. It is shown that the conventional stuck-at fault modelling may be inadequate for obtaining a sufficiently high fault coverage. A new augmented stuck-at fault model is presented which provides a better coverage of physical failures.< >", "num_citations": "24\n", "authors": ["455"]}
{"title": "Seasonal variation in the vulnerability discovery process\n", "abstract": " Vulnerability discovery rates need to be taken into account for evaluating security risks. Accurate projection of these rates is required to estimate the effort needed to develop patches for handling vulnerabilities discovered. Seasonal behaviors of the vulnerability discovery process for a multi-year life-cycle of software products are examined. A careful inspection of the data for several major operating systems, web servers and web browsers suggests presence of a seasonal behavior that is not considered by the vulnerability discovery models. This paper examines the statistical significance of the annual seasonal pattern in the vulnerability discovery rates using the seasonal index approach. The autocorrelation function is used to identify the periodicity. A time series analysis that combines thelonger term trends with cycles caused by seasonality may predict the future pattern more accurately. The analysis of the\u00a0\u2026", "num_citations": "23\n", "authors": ["455"]}
{"title": "Dynamic power minimization during combinational circuit testing as a traveling salesman problem\n", "abstract": " Testing of VLSI circuits can cause generation of excessive heat which can damage the chips under test. In the random testing environment, high-performance CMOS circuits consume significant dynamic power during testing because of enhanced switching activity in the internal nodes. Our work focuses on the fact that power minimization is a traveling salesman problem (TSP). We explore application of local search and genetic algorithms to test set reordering and perform a quantitative comparison to previously used deterministic techniques. We also consider reduction of the original test set as a dual-objective optimization problem, where switching activity and fault coverage are the two objective functions.", "num_citations": "23\n", "authors": ["455"]}
{"title": "Fault exposure ratio estimation and applications\n", "abstract": " One of the most important parameters that control reliability growth is the fault exposure ratio (FER) identified by J.D. Musa et al. (1991). It represents the average detectability of the faults in software. Other parameters that control reliability growth are software size and execution speed of the processor which are both easily evaluated. The fault exposure ratio thus presents a key challenge in our quest towards understanding the software testing process and characterizing it analytically. It has been suggested that the fault exposure ratio may depend on the program structure, however the structuredness as measured by decision density may average out and may not vary with program size. In addition FER should be independent of program size. The available data sets suggest that FER varies as testing progresses. This has been attributed partly to the non-randomness of testing. We relate defect density to FER and\u00a0\u2026", "num_citations": "23\n", "authors": ["455"]}
{"title": "Enhancing accuracy of software reliability prediction\n", "abstract": " The measurement and prediction of software reliability require the use of the software reliability growth models (SRGMs). The predictive quality can be measured by the average end-point projection error. In this paper, the effects of two orthogonal classes of approaches to improve prediction capability of a SRM have been examined using a large number of data sets. The first approach is preprocessing of data to filter out short term noise. The second is to overcome the bias inherent in the model. The results show that proper application of these two approaches can be more important than the selection of the model.", "num_citations": "23\n", "authors": ["455"]}
{"title": "On fault modeling and testing of content-addressable memories\n", "abstract": " Associative or content addressable memories can be used for many computing applications. This paper discusses fault modeling for the content addressable memory (CAM) chips. Detailed examination of a single CAM cell is presented. A functional fault model for a CAM architecture executing exact match derived from the single cell model is presented. An efficient testing strategy can be derived using the proposed fault model.< >", "num_citations": "22\n", "authors": ["455"]}
{"title": "Redundant design in interdependent networks\n", "abstract": " Modern infrastructure networks are often coupled together and thus could be modeled as interdependent networks. Overload and interdependent effect make interdependent networks more fragile when suffering from attacks. Existing research has primarily concentrated on the cascading failure process of interdependent networks without load, or the robustness of isolated network with load. Only limited research has been done on the cascading failure process caused by overload in interdependent networks. Redundant design is a primary approach to enhance the reliability and robustness of the system. In this paper, we propose two redundant methods, node back-up and dependency redundancy, and the experiment results indicate that two measures are effective and costless. Two detailed models about redundant design are introduced based on the non-linear load-capacity model. Based on the attributes and historical failure distribution of nodes, we introduce three static selecting strategies-Random-based, Degree-based, Initial load-based and a dynamic strategy-HFD (historical failure distribution) to identify which nodes could have a back-up with priority. In addition, we consider the cost and efficiency of different redundant proportions to determine the best proportion with maximal enhancement and minimal cost. Experiments on interdependent networks demonstrate that the combination of HFD and dependency redundancy is an effective and preferred measure to implement redundant design on interdependent networks. The results suggest that the redundant design proposed in this paper can permit construction of highly robust\u00a0\u2026", "num_citations": "21\n", "authors": ["455"]}
{"title": "Using software structure to predict vulnerability exploitation potential\n", "abstract": " Most of the attacks on computer systems are due to the presence of vulnerabilities in software. Recent trends show that number of newly discovered vulnerabilities still continue to be significant. Studies have also shown that the time gap between the vulnerability public disclosure and the release of an automated exploit is getting smaller. Therefore, assessing vulnerabilities exploitability risk is critical as it aids decision-makers prioritize among vulnerabilities, allocate resources, and choose between alternatives. Several methods have recently been proposed in the literature to deal with this challenge. However, these methods are either subjective, requires human involvement in assessing exploitability, or do not scale. In this research, our aim is to first identify vulnerability exploitation risk problem. Then, we introduce a novel vulnerability exploitability metric based on software structure properties viz.: attack entry\u00a0\u2026", "num_citations": "21\n", "authors": ["455"]}
{"title": "Estimating defect density using test coverage\n", "abstract": " Defect density is one of the most important factors that allow one to decide if a piece of software is ready to be released. In theory, one can find all the defects and count them, however it is impossible to find all the defects within any reasonable amount of time. Estimating defect density can become difficult for high reliability software, since the remaining defects can be extremely hard to test. Defect seeding will work only if the distribution of seeded defects is similar to the existing defects. One possible way is to apply the exponential SRGM and thus estimate the total number of defects present at the beginning of testing. Here we show the problems with this approach and present a new approach based on software test coverage. Software test coverage directly measures the thoroughness of testing avoiding the problem of variations of test effectiveness. Here we present interpretations of the parameters of the coverage-defect-density model presented by Malaiya et al. We apply this model to actual test data to project the residual defect density. The results show that this method results in estimates that are more stable than the existing methods. This method is easier to understand and the convergence to the estimate can be visually observed.", "num_citations": "21\n", "authors": ["455"]}
{"title": "Behavior of faulty single BJT BiCMOS logic gates\n", "abstract": " The logic behavior of single BJT BiCMOS devices under transistor level shorts and opens is examined. In addition to delay faults, faults that cause the gate to exhibit sequential behavior were observed. Several faults can be detected only by monitoring the current. The faulty behaviour of bipolar (TTL) and CMOS logic families is compared with BiCMOS. Effects of bridging faults in BiCMOS devices has been examined for both hard short as well as bridging with a significant resistance.< >", "num_citations": "21\n", "authors": ["455"]}
{"title": "CMOS stuck-open fault detection using single test patterns\n", "abstract": " CMOS combinational circuits exhibit sequential behavior in the presence of open faults, thus making it necessary to use two pattern tests. Two or multi-pattern sequences may fail to detect CMOS stuck-open faults in the presence of glitches. The available methods for augmenting CMOS gates to test CMOS stuck-open faults, are found to be inadequate in the presence of glitches. A new CMOS testable design is presented. The scheme uses two additional MOSFETs, which convert a CMOS gate to either pseudo nMos or pseudo pMOS gate during testing. The proposed design ensures the detection of stuck-open faults using a single vector during testing", "num_citations": "20\n", "authors": ["455"]}
{"title": "A consolidated approach for estimation of data security breach costs\n", "abstract": " Many security breaches have been reported in the past few years impacting both large and small organizations. There has often been considerable disagreement about the overall cost of such breaches. No significant formal studies have yet addressed this issue, though some proprietary approaches exist. A few computational models for evaluating partial data breach costs have been implemented, but these approaches have not been formally compared and have not been systematically optimized. There is a need to develop a more complete and formal model that will minimize redundancy among the factors considered and will confirm with the available data regarding the costs of data breaches. Existing approaches also need to be validated using the data for some of the well documented breaches. It is noted that the existing models yield widely different estimates. The reasons for this variation are examined, and\u00a0\u2026", "num_citations": "19\n", "authors": ["455"]}
{"title": "Limitations of switch level analysis for bridging faults\n", "abstract": " Switch-level models are widely used for fault analysis of MOS digital circuits. Switch-level analysis (SLA) provides significantly more accurate results compared to gate-level models, and also avoids the complexities of circuit-level analysis. The accuracy of SLA is critically examined, and conditions under which SLA may generate incorrect results are specified. Such conditions may occur when the bulk of a transistor is connected to its source. These conditions are especially applicable under certain types of bridging faults. A simple technique is suggested for accurate switch-level modeling under such conditions.< >", "num_citations": "19\n", "authors": ["455"]}
{"title": "Linearly correlated intermittent failures\n", "abstract": " In reliability calculation, the general assumption is to regard the behavior of redundant systems as (statistically) s-independent. This considerably simplifies the mathematics involved, but limits the usefulness of the results as s-dependence can appreciably impact the system reliability. This paper introduces two measures, using linear correlation, to describe the s-dependence of intermittent failures. One measure relates to the linear correlation for the entire period during which faults are active in different modules; the other measure relates to the closeness in time of the instants faults become active in different modules. Characteristics and their relationship with reliability and fault processes of these measures are considered. Irreversible processes corresponding to permanent failures are examined.", "num_citations": "19\n", "authors": ["455"]}
{"title": "Evaluating CVSS base score using vulnerability rewards programs\n", "abstract": " CVSS Base Score and the underlying metrics have been widely used. Recently there have been attempts to validate them. Some of the researchers have questioned the CVSS metrics based on a lack of correlation with the reported exploits and attacks. In this research, we use the independent scales used by the vulnerability reward programs (VRPs) to see if they correlate with the CVSS Base Score. We examine 1559 vulnerabilities of Mozilla Firefox and Google Chrome browsers. The results show that there is a significant correlation between the VRPs severity ratings and CVSS scores, when three level rankings are used. For both approaches, the sets of vulnerabilities identified as Critical or High severity vulnerabilities include a large number of shared vulnerabilities, again suggesting mutual conformation. The results suggest that the CVSS Base Score may be a useful metric for prioritizing vulnerabilities\u00a0\u2026", "num_citations": "18\n", "authors": ["455"]}
{"title": "Antirandom vs. pseudorandom testing\n", "abstract": " This paper introduces the concept of antirandom testing where each test applied is chosen such that its total distance from all previous tests is maximum. This spans the test vector space to the maximum extent possible for a given number of vectors. This strategy results in a higher fault coverage when the number of vectors that are applied is limited. Results on several ISCAS benchmarks show this strategy to be very effective when a high fault coverage needs to be achieved with a limited number of test vectors. The superiority of the antirandom testing approach is even more significant for testing bridging faults.", "num_citations": "18\n", "authors": ["455"]}
{"title": "CMOS open-fault detection in the presence of glitches and timing skews\n", "abstract": " A testable CMOS design technique in which some extra transistors are used in such a way that the CMOS gate is converted to a pseudo-nMOS/pMOS gate during testing is discussed. With the proposed design technique, CMOS open faults can be detected regardless of timing skews/delays, glitches, or charge sharing among the internal nodes. The major advantage of the proposed testable design technique is that it allows the use of a single test vector to detect a stuck-open fault. This significantly reduces the complexity of test generation and the time consumed for testing. The design procedure is simple and all the classical algorithms and automatic test-pattern-generating programs can be used to generate tests for circuits designed according to this technique. Even random testing techniques can be used efficiently to detect the open faults in these CMOS circuits.< >", "num_citations": "18\n", "authors": ["455"]}
{"title": "On the need for simulation for better characterization of software reliability\n", "abstract": " Software reliability engineering must develop beyond statistical analysis of data and analytic models which frequently require unrealistic assumptions. We must develop a viable discipline of simulation to aid experimental and industrial application of software reliability engineering. This will require developing standard modeling components, connections, tools, and a body of knowledge to interpret the phenomena that are being modeled. This paper explains how and why simulation models of software reliability can help to characterize the testing and debugging process more accurately. Examples are given to illustrate possible application of software reliability simulation.", "num_citations": "17\n", "authors": ["455"]}
{"title": "Most successful vulnerability discoverers: Motivation and methods\n", "abstract": " In this paper, we investigate the factors that motivate and enable successful vulnerability discovery and the role of vulnerability markets. This is done by studying the career, motivation and methods of the most successful vulnerability discoverers. Vulnerability discovery takes considerable expertise. Some vulnerabilities, if exploited, can cause enormous damage to an organization, a segment of the economy, or even national security. Software developers, security organizations and government agencies are continuously engaged in efforts to prevent improper disclosure of vulnerabilities that can lead to zero-day exploitations. We observe that a major percentage of vulnerabilities are discovered by individuals external to software development organizations. We identify the top vulnerability discoverers throughout the past 12 years, and examine their motivation and methods. We observe that financial reward is a major motivation, especially to discoverers in Eastern Europe.. The paper studies the actual vulnerability market, rather than the hypothetical markets often studied in recent literature.", "num_citations": "16\n", "authors": ["455"]}
{"title": "Reliability allocation\n", "abstract": " A system is generally designed as an assembly of subsystems, each with its own reliability attributes. The overall system reliability is a function of the subsystem reliability metrics. The cost of the system is the sum of the costs for all the subsystems. This article examines possible approaches to allocate the reliability values such that the total cost is minimized. The problem is very general and is applicable for mechanical systems, electrical systems, computer hardware and software. The problem involves expressing the system reliability in terms of the subsystem reliability values and specifying the cost function for each subsystem which allows setting up an optimization problem. Software reliability allocation is examined as a detailed example. The article also examines some preliminary apportionment approaches that have been proposed. In some cases, it is possible to use exact optimization methods. In general, a complex case will require use of iterative approaches.", "num_citations": "16\n", "authors": ["455"]}
{"title": "IDDQ testing of integrated circuits\n", "abstract": " An integrated circuit includes a circuit architecture that enhances the I DDQ testability of circuitry such as random access memories. Increased accuracy and test speed are achieved by partitioning the circuit array into multiple partitions. Pairs of partitions connected to a voltage source node and having substantially identical ground line capacitances are subdivided into respective blocks. Each block in a pair of the partitions includes a corresponding block in the other partition. Each of the corresponding blocks in a pair has a substantially equal ground line capacitance, and preferably each of the blocks has a substantially equal ground line capacitance. Pairs of corresponding blocks are coupled to respective built-in current comparators. Each block is preferably configured to include portions of non-contiguous, interleaved bit line segments and portions of non-contiguous, interleaved word lines. Simultaneous test\u00a0\u2026", "num_citations": "15\n", "authors": ["455"]}
{"title": "The effect of built-in current sensors (BICS) on operational and test performance/spl lsqb/CMOS ICs/spl rsqb\n", "abstract": " Effects of built-in current sensors on I/sub DDQ/ measurement as well as on the performance of the circuit under test are considered. Most of the Built-in Current Sensor designs transform the ground terminal of the circuit under test to a virtual ground. This causes increase in both propagation delay as well as I/sub DDQ/ sampling time with increase in the number of gates, affecting both test as well as operational performance. The effect that the current sensor has on the operational and test performance is considered. Circuit partitioning may be used for overcoming the effects of BICS on I/sub DDQ/ measurements as well as on the performance of the circuit under test.< >", "num_citations": "15\n", "authors": ["455"]}
{"title": "The nature of fault exposure ratio\n", "abstract": " The fault exposure ratio K is an important factor that controls the per-fault hazard rate, and hence the effectiveness of software testing. The paper examines the variations of K with fault density which declines with testing time. Because faults get harder to find, K should decline if testing is strictly random. However, it is shown that at lower fault densities K tends to increase, suggesting that real testing is more efficient than random testing. Data sets from several different projects are analyzed. Models for the two factors controlling K are suggested, which jointly lead to the logarithmic model.< >", "num_citations": "15\n", "authors": ["455"]}
{"title": "Relationship between attack surface and vulnerability density: A case study on apache HTTP server\n", "abstract": " Software Security metrics are quantitative measures related to a software system's level of trustworthiness. They can be used to aid in resource allocation, program planning, risk assessment, and product and service selection. Recently researchers have proposed several software security metrics. Among these are attack surface and vulnerability density. The attack surface measure has been used by a few major software companies, such as Microsoft, Hewlett-Packard, and SAP. The vulnerability density measure has been applied by some researchers to Windows and Linux family of operating systems, in addition to some web servers and browsers. Despite their promise, establishing the validity of software security metrics remains a key challenge. A single security metric may be unable to measure all aspects of security and hence the use of multiple metrics may be needed in some situations. To assess the\u00a0\u2026", "num_citations": "14\n", "authors": ["455"]}
{"title": "CMOS stuck-open fault testability\n", "abstract": " CMOS combinational circuits exhibit sequential behavior in the presence of open faults, thus making it necessary to use two-pattern tests. An example which illustrates how such tests can be invalidated by glitches induced by circuit delays is presented. It is also shown that some results recently reported by researchers may not be valid in the presence of glitches.< >", "num_citations": "14\n", "authors": ["455"]}
{"title": "Analysis of an important class of non-Markov systems\n", "abstract": " Probabilistic modeling of many types of systems generally assumes Markov behavior. However, some important practical systems exhibit memory. For example, in digital computer systems, the probability of occurrence of a transient failure is related to the time period the system has been operating correctly. Analytic methods do not yet exist that allow accurate modeling of such systems for the purpose of reliability analysis and fault-tolerant design. Methods are presented here to analyze an important class of non-Markov systems. In this class, the transition-probability-rate of an out-ward transition from a state is related to the duration the system has continuously been in that state. To analyze such systems, concept of memory profile has been introduced. Methods are first presented which enable computation of steady-state probabilities for both discrete-time and continuous-time processes with two states. These are\u00a0\u2026", "num_citations": "14\n", "authors": ["455"]}
{"title": "Periodicity in software vulnerability discovery, patching and exploitation\n", "abstract": " Periodicity in key processes related to software vulnerabilities need to be taken into account for assessing security at a given time. Here, we examine the actual multi-year field datasets for some of the most used software systems (operating systems and Web-related software) for potential annual variations in vulnerability discovery processes. We also examine weekly periodicity in the patching and exploitation of the vulnerabilities. Accurate projections of the vulnerability discovery process are required to optimally allocate the effort needed to develop patches for handling discovered vulnerabilities. A time series analysis that combines the periodic pattern and longer-term trends allows the developers to predict future needs more accurately. We analyze eighteen datasets of software systems for annual seasonality in their vulnerability discovery processes. This analysis shows that there are indeed repetitive\u00a0\u2026", "num_citations": "13\n", "authors": ["455"]}
{"title": "Resolution enhancement in IDDQ testing for large ICs\n", "abstract": " Current drawn by a static CMOS VLSI integrated circuit during quiescent periods is extremely small and is normally of the order of nanoamperes. However, it is remarkably susceptible to a number of failure modes. Many faults present in such ICs cause the quiescent power-supply current (IDDQ) to increase by several orders of magnitude. Some of these faults may not manifest themselves as logical faults, and would not be detected by traditional IC test techniques.In large ICs, it may be hard to distinguish between larger IDDQ due to defects and elevated IDDQ due to normal parameter variations. A statistical characterization of the problem is presented. This can be used to determine the optimal size of partitions. A new information compression scheme is presented which can significantly enhance resolution.", "num_citations": "13\n", "authors": ["455"]}
{"title": "X-IDDQ: a novel defect detection technique using IDDQ data\n", "abstract": " A statistical technique X-IDDQ for extracting defect information from IDDQ data is presented that is effective for detection of defects in ICs. The technique treats the IDDQ measurements in a holistic manner to come up with a statistic X that is highly correlated to the presence of defects. X-IDDQ facilitates binning of ICs and enhances the test process by early identification of faults. The transformation metrics, for evaluating X statistic from IDDQ measurements, obtained using one batch works extremely well for different batches, facilitating its use with manufacturing-line testing.", "num_citations": "12\n", "authors": ["455"]}
{"title": "Data size reduction for clustering-based binning of ICs using principal component analysis (PCA)\n", "abstract": " Accurate binning of ICs using analog characteristics such as I DDQ  requires using data from a number of vectors. From this data, information needs to be extracted using a method that will yield sufficiently high resolution. Using a large volume of data can require significant computation time. If n analog measurements are made for each chip, the data has n dimensions. However the measured I DDQ  values for a chip can he highly correlated. We examine an approach based on principal component analysis (PCA) for reducing the data size while preserving almost all of the information. PCA transforms the data by extracting statistically independent components and arranging them in the order of relative significance. Using industrial I DDQ  data we found that often n-dimensional data can be reduced to a single dimension with no substantial change in the clusters identified", "num_citations": "12\n", "authors": ["455"]}
{"title": "The Effect of Correlated Faults on Software Reliability\n", "abstract": " The reliability models often assume random testing and statistical independence of faults to keep the analysis tractable. In practice, these assumptions do not hold. This paper presents a reliability modeling approach that considers nonrandom testing. This approach is used to calculate the fault exposure ratio, which characterizes the testing process. The analysis of the experimental data suggests that the fault exposure ratio varies differently in the early and the later stages of testing. The analysis here presents an explanation of this behavior.", "num_citations": "11\n", "authors": ["455"]}
{"title": "Structurally guided black box testing\n", "abstract": " Black-box testing [1, 13, 11] can be easily automated and involves less processing than white box testing because it does not use information about the program structure. However it is very hard to achieve high coverage with black-box testing. Some branches can be very hard to reach. These branches influence the testability of the code they encapsulate and the module in turn. A technique which can help black-box testing to cover these hard to test branches easily will significantly enhance test effectiveness. In this paper, we propose a simple guided approach which makes black-box testing more effective using some easily available structural information. In this paper, we propose a new dynamic measure termed potential of a branch. During testing, we extract useful structural information and combine it with changing coverage information to evaluate the current potential of a branch. We use this measure to guide the black-box testing techniques so that the new tests generated are more likely to exercise branches which are so far not covered. We also present an instrumentation approach called magnifying branches which may enhance the effectiveness of the new approach. These magnifying branches help the guided approach to increase the focus on these harder branches. The coverage results of a black-box testing technique (random testing) with and without guiding are then compared. Further we compare these coverage results with those obtained after instrumenting the programs with the magnifying branches. The results show that this simple guided technique significantly improves coverage, especially for programs with complex\u00a0\u2026", "num_citations": "10\n", "authors": ["455"]}
{"title": "Modelling and analysis of bridging faults in emitter-coupled logic (ECL) circuits\n", "abstract": " With the recent achievement of lower power and higher densities, bipolar ECL technology is expected to be used widely in high performance digital circuits. Recent investigations have revealed that bridging faults can be a major failure mode in ICs. The paper presents a detailed analysis of bridging faults in ECL. Certain bridging faults manifest as stuck-at faults. Effects of bridging faults between logical units without feedback and logical units with feedback in ECL are presented. An analytical approach is presented for computation of logic levels at ECL outputs under varying unknown bridging resistances. Effects of bridging faults and bridging resistances on output logic levels in ECL have been examined along with their effects on noise immunity.< >", "num_citations": "10\n", "authors": ["455"]}
{"title": "Fault modeling and testable design of 2-level complex ECL gates\n", "abstract": " Bipolar emitter coupled logic (ECL) devices can now be fabricated at very high densities and much lower power consumption. Behaviour of 2-level complex ECL gates is examined in the presence of physical faults. It is shown that the conventional stuck-at fault model cannot represent a majority of circuit level faults. A new augmented stuck-at fault model is presented which provides a significantly higher coverage of physical failures. A testable design approach is presented for on-line detection of certain error conditions occurring in gates with true and complementary outputs which is a normal implementation for ECL devices.<>", "num_citations": "10\n", "authors": ["455"]}
{"title": "Relationship between test effectiveness and Coverage\n", "abstract": " A model has recently been developed that relates test coverage and defect coverage. It has been shown that this model can be used to estimate the residual defect density. Defect finding capability of sets of tests with different coverage values has recently been studied by Frankl and Iakounenko. They have defined measure test effectiveness, and have evaluated it for programs containing a single fault. Here we explore the applicability of Malaiya et al\u2019s model to the test data reported by Frankl and Iakounenko.", "num_citations": "9\n", "authors": ["455"]}
{"title": "Limitations of built-in current sensors (BICS) for I/sub DDQ/testing\n", "abstract": " Quiescent current (I/sub DDQ/) drawn by a static CMOS device is extremely small and is of the order of nanoamperes. Under many faults, (I/sub DDQ/) can increase by several orders of magnitude. Either an external or an on-chip current sensor can be used to detect enhanced static current drawn by a static CMOS device. An on-chip sensor, termed a BICS (Built-In Current Sensor) can be significantly faster. Implementation of BICS has received a lot of interest in the recent years. Some limitations posed by BICS on I/sub DDQ/ measurement caused due to increase in I/sub DDQ/ settling time as well as propagation delay is considered. Results indicate that careful attention needs to be given to circuit partitioning for implementing BICS. Some of the considerations that need to be taken into account while designing new BICS are presented.< >", "num_citations": "9\n", "authors": ["455"]}
{"title": "Analysis of detection capability of parallel signature analyzers\n", "abstract": " All signature analyzers are prone to aliasing errors. Here a rigorous mathematical analysis is presented to identify error conditions under which aliasing can occur for several common types of serial signature analyzers (SSA's) and parallel signature analyzers (PSA's). The PSA's are faster and require less hardware than the SSA's; however, for PSA's some double errors are special cause of concern. Such aliasing errors are analyzed and it is shown that PSA pairs can be identified for which these errors are disjoint. New reconfigurable PSA designs are presented which use a two-signature scheme to detect all double errors.", "num_citations": "9\n", "authors": ["455"]}
{"title": "An API development model for digital twins\n", "abstract": " Digital twins are software implementations of their physical counterparts. These software representations act through application program interfaces (APIs) to the physical devices they monitor, engage with and possibly control. Here a development model is proposed to ensure the desired reliability and performance of the API which is critical to the overall success of the Digital Twin.", "num_citations": "8\n", "authors": ["455"]}
{"title": "An outlier detection based approach for PCB testing\n", "abstract": " Capacitive Leadframe testing is an effective approach for detecting faults in printed circuit boards. Capacitance measurements, however, are affected by mechanical variations during testing and by tolerances of electrical parameters of components, making it difficult to use threshold based techniques for defect detection. A novel approach is presented for identifying boards that are likely to be outliers. Based on Principal Components Analysis (PCA), this approach treats the set of capacitance measurements of individual connectors or sockets in a holistic manner to overcome the measurement and component parameter variations inherent in test data. The effectiveness of the method is evaluated using measurements on three different boards. Enhancements to the technique to increase the resolution of the method are presented and evaluated.", "num_citations": "8\n", "authors": ["455"]}
{"title": "Seasonality in vulnerability discovery in major software systems\n", "abstract": " Prediction of vulnerability discovery rates can be used to assess security risks and to determine the resources needed to develop patches quickly to handle vulnerabilities discovered. An examination of the vulnerability data suggests a seasonal behavior that has not been modeled by the recently proposed vulnerability discovery models. This seasonality has not been identified or examined so far. This study examines whether vulnerability discovery rates for Windows NT, IIS Server and the Internet Explorer exhibit a significant annual seasonal pattern. Actual data has been analyzed using seasonal index and auto correlation function approaches to identify seasonality and to evaluate its statistical significance. The results for the three software systems show that there is indeed a significant annual seasonal pattern.", "num_citations": "8\n", "authors": ["455"]}
{"title": "A bipartite, differential I/sub DDQ/testable static RAM design\n", "abstract": " I/sub DDQ/ (Defect Detection by Quiescent power supply current measurement), or current testing, has emerged in the last few years as an effective technique for detecting certain classes of faults in high-density ICs. In this paper, a testable design that enhances the I/sub DDQ/ testability of static random access memories (SRAMs) for off-line testing as proposed. To achieve high accuracy and a test speed approaching the system operational speed, the memory is partitioned for comparison of I/sub DDQ/ values. Parallel write/read operations are used to activate possible faults, while quiescent power supply currents from two blocks are compared.", "num_citations": "8\n", "authors": ["455"]}
{"title": "Modeling of intra-cell defects in CMOS SRAM\n", "abstract": " The effect of defects within a single cell of a static random access memory (SRAM) is examined. All major types of faults, including bridging, transistor stuck-open and stuck-on, are examined. A significant fraction of all faults cause high IDDQ values to be observed. Faults leading to inter-cell coupling are identified.< >", "num_citations": "8\n", "authors": ["455"]}
{"title": "Bridging faults in BiCMOS circuits\n", "abstract": " Combining the advantages of CMOS and bipolar, BiCMOS is emerging as a major technology for many high performance digital and mixed signal applications. Recent investigations revealed that bridging faults can be a major failure mode in IC's. Effects of bridging faults in BiCMOS circuits are presented. Bridging faults between logical units without feedback and logical units with feedback are considered. Several bridging faults can be detected by monitoring the power supply current (I (sub DDQ) monitoring). Effects of bridging faults and bridging resistance on output logic levels were examined along with their effects on noise immunity.", "num_citations": "8\n", "authors": ["455"]}
{"title": "Use of storage elements as primitives for modeling faults in synchronous sequential circuits\n", "abstract": " While fauli models for various implementaiions of combinaiional logic have been examined in deiail, only limited attention has been paid io storage eletuenis. In many designs, siorage elements (SEs) represent a significant fraction of the electrical nodes. Mosi iesiable designs are geared towards testing of combinational logic, and generally depend on an increase in nuinber of SEs. The stuck-at fault model assuiiies that an output (input) of a SE can be stuck-ai-0 or 1. This minimal fauli model may not model some comiiion fauli behaviors for SEs. These include feed-through faulls which cause ilre cell to erhibii data or clockfeed-through behavior, and ihe inabiliiy io laic11 H or L signals. In a master-slave cell, or when two-phase clocking is used, many of these faults may appear as output. duck-at faults. The miniiiral fault model may be an efficient fault model when only modest coverage may be required. The\u00a0\u2026", "num_citations": "8\n", "authors": ["455"]}
{"title": "Behavior of Faulty double BJT BiCmos logic gates\n", "abstract": " Logic Behavior of a Double BJT BiCMOS device under transistor level shorts and opens is examined. In addition to delay faults, faults that cause the gate to exhibit sequential behavior were observed. Several faults can be detected only by monitoring the current. The faulty behavior of Bipolar (TTL) and CMOS logic families is compared with BiCMOS, to bring out the testability differences.", "num_citations": "8\n", "authors": ["455"]}
{"title": "On Bridging Faults in ECL Circuits\n", "abstract": " With the recent achievement of lower power and higher densities, ECL technology is txpected to be used widely in high performance digital circuits. Recent investigations have revealed that bridging faults can be a major failure mode in ICs. This paper presents a detailed analysis of bridging faults in ECL. Eflects of bridging faults between logical units without feedback and logical units with feedback in ECL are presented. An Analytical approach is presented for computation of logic levels at ECL outputs under varying unknown bridging resistances.", "num_citations": "8\n", "authors": ["455"]}
{"title": "Early characterization of the defect removal process\n", "abstract": " In the early phase of testing, the defect detection rate often shows an erratic behavior. The delayed S-shaped model is robust in this situation and is now widely used by the Japanese. Recent investigations suggests that the logarithmic model has much better overall predictive capability; however, it is often unstable at the beginning. Here a technique is presented that makes the logarithnfic model much more robust in the early phases.", "num_citations": "8\n", "authors": ["455"]}
{"title": "Adaptive testing based on moment estimation\n", "abstract": " Adaptive testing (AT) is a software testing approach that uses a feedback mechanism to enhance test effectiveness. Its testing strategy can be adjusted online by using the testing data collected during the software testing process. However, it requires complex parameter estimation which results in excessive computational overhead that may hinder the applicability of AT. In this paper, we propose an approach called AT based on moment estimation (AT-ME) to address this problem. The proposed approach uses moment estimation to serve as the algorithm of parameter estimation, which reduces the complexity of AT-ME. In addition, a dynamic length for testing action is set to limit the number of decisions without influencing the test effectiveness. The proposed approach has been validated on the Siemens test suite, which includes seven real programs. The experiments show that AT-ME can reduce the computational\u00a0\u2026", "num_citations": "7\n", "authors": ["455"]}
{"title": "Optimal Reliability Allocation\n", "abstract": " The overall reliability of a complex system, designed as an assembly of subsystems, depends on the subsystem reliability metrics. The cost of each subsystem is a function of its reliability. This article considers the problem of allocating the reliability values to minimize the total cost while achieving the reliability target. The reliability allocation problem is applicable to mechanical and electrical systems as well as computer hardware and software. The approach involves expressing the system reliability in terms of the subsystem reliability values and specifying the cost function for each subsystem, which allows setting up an optimization problem. Software reliability allocation is examined as a detailed example and some preliminary apportionment approaches are presented. While in many cases, it is possible to use exact optimization methods, a complex case may require use of iterative approaches.", "num_citations": "7\n", "authors": ["455"]}
{"title": "Software reliability and security\n", "abstract": " Software problems are the main causes of system failures today. There are many well-known cases of the tragic consequences of software failures. In critical systems, very high reliability is naturally expected. Software packages used everyday also need to be highly reliable, because the enormous investment of the software developer is at stake. Studies have shown that reliability is regarded as the most important attribute by potential customers. All software developed will have a significant number of defects. All programs must be tested and debugged, until sufficiently high reliability is achieved. It is not possible to ensure that all the defects in a software system have been found and removed; however, the number of remaining bugs must be very small. As software must be released within a reasonable time, to avoid loss of revenue and market share, the developer must take a calculated risk and must have a strategy for achieving the required reliability by the target release date. For software systems, quantitative methods for achieving and measuring reliability are coming in use because of the emergence of well-understood and validated approaches. Enough industrial and experimental data are available to develop and validate methods for achieving high reliability. The minimum acceptable standards for software reliability have gradually risen in recent years.This entry presents an overview of the essential concepts and techniques in the software reliability field. We examine factors that impact reliability during development as well as during testing. First, we discuss the reliability approaches taken during different phases of software development\u00a0\u2026", "num_citations": "7\n", "authors": ["455"]}
{"title": "Gate level representation of ECL circuits for fault modeling\n", "abstract": " Bipolar emitter coupled logic (ECL) devices can now be fabricated at high densities and lower power consumption. With the achievement of low power and high densities, ECL technology is expected to be used widely in high performance digital circuits. This necessitates the need for obtaining optimum gate level models for ECL circuits. A simple technique to obtain a gate level model of an ECL circuit is presented. The gate level models obtained for 1-level and 2-level ECL using the transformation rules presented are the same as the fault models that provide higher coverage of physical failures.<>", "num_citations": "7\n", "authors": ["455"]}
{"title": "Automatic test software\n", "abstract": " Even when software is developed using a rigorous discipline, it will contain a significant number of bugs. At one time, it was believed that the use of formal methods would eventually allow probable correct programs to be written, thus completely eliminating the need for testing. Today, we know that while the number of defects in a program written will be lower under certain development environments, they will still add up to a large number in a reasonably large program. There has to be testing and debugging, which can consume up to 60% of the total effort.A study by McGibbon presents a perspective [mcg97]. He compares traditional development approach with two formal methods, VDM and Z. For a program with 30,000 source lines of code (SLOC), the traditional methods will be able to deliver software with 34 defects with an estimated life-cycle cost of $2.5 million. Using Z, the total cost would be reduced by $2.2 million, but still about 8 defects would be left. Additional cost savings can be achieved by using VDM; however, it will result in 24 defects in the delivered product. In all three cases, a substantial part of the cost is due to testing and debugging.", "num_citations": "6\n", "authors": ["455"]}
{"title": "Principal component analysis-based compensation for measurement errors due to mechanical misalignments in PCB testing\n", "abstract": " Capacitive Leadframe Testing is capable of detecting open solder defects in Printed Circuit Boards (PCB). Principal Component Analysis (PCA)-based approach has been shown to be effective in identifying outlier devices using Capacitive Leadframe Testing measurements. In practice, when a sense plate orientation is shifted or tilted, the resulting measurement variation makes detecting outliers harder. Approaches are introduced to compensate for the `abnormal' measurements due to sense-plate variations. A PCA based technique is developed to estimate the relative amount of tilt and shift in sense plates. Such estimates can be used to compensate for mechanical misalignments. It can also isolate the misalignment related information from the defect related information in the data. The effectiveness of this technique in the presence of the two common forms of mechanical variations is illustrated using experimental\u00a0\u2026", "num_citations": "6\n", "authors": ["455"]}
{"title": "Antirandom Testing: Beyond Random Testing,\"\n", "abstract": " Random testing is a well known concept that requires that each test is selected randomly regardless of the test previously applied. In actual practice it takes the form of pseudo-random testing, where each test pattern is a shifted version of the previous one with one new bit added. This paper introduces the concept of antirandom testing. In this testing strategy each test applied is chosen such that its total distance from all previous tests is maximum. This spans the test vector space to the maximum extent possible for a given number of vectors. This strategy results in a higher fault coverage when the number of vectors that are applied is limited. Algorithm for generating antirandom tests is presented. A Reed-Solomon code based test set is also introduced that results in test vectors with antirandom characteristics. Results comparing the different test strategies on ISCAS benchmarks show these strategies to be very effective when a high fault coverage needs to be achieved with a limited number of test vectors. The superiority of the antirandom testing approach is even more significant for testing bridging faults.", "num_citations": "6\n", "authors": ["455"]}
{"title": "Detection of feed-through faults in CMOS storage elements\n", "abstract": " In testing sequential circuits, internal faults in the storage elements (SE's) are sometimes modeled as stuck-at faults in the combinational circuits surrounding the SE. The detection of some transistor-level faults that cannot be modeled as stuck-at are considered. These feed-through faults cause the cell to become either data-feed-through, which makes the cell combinational, or clock-feed-through, which causes the clock signal or its complement to appear at the output. Under such faults, the cell does not function as a memory element. Here it is shown that such faults may or may not be detected depending on delays involved. Conditions under which race-ahead occurs are identified.", "num_citations": "6\n", "authors": ["455"]}
{"title": "Test-experiments for detection and location of intermittent faults in sequential circuits\n", "abstract": " Practical solutions have not been obtained from the previous papers addressing the problem of testing intermittent faults in sequential circuits. Existing methods are only suitable for small sequential circuits. This correspondence presents a new technique to design test-experiments for intermittent faults which can conveniently be used for relatively more complex synchronous sequential circuits.", "num_citations": "6\n", "authors": ["455"]}
{"title": "Modeling skewness in vulnerability discovery models in major operating systems\n", "abstract": " A few vulnerability discovery models have been proposed recently. Studies have shown that the S-shaped AML vulnerability discovery model generally performs better than other models. The AML model assumes a symmetrical Logistic discovery pattern. This work examines the cases when discovery pattern is not symmetrical; thus potentially an asymmetrical discovery model might perform better. Here, new vulnerability discovery models based on asymmetrical S-shaped distributions are proposed, and their fit and prediction capabilities are compared. The results show that all the right skewed datasets are represented better with the Gamma distribution based model.", "num_citations": "5\n", "authors": ["455"]}
{"title": "Vulnerability discovery in multiple version software systems: open source and commercial software systems\n", "abstract": " The vulnerability discovery process for a program describes the rate at which the vulnerabilities are discovered. A model of the discovery process can be used to estimate the number of vulnerabilities likely to be discovered in the near future. Past studies have considered vulnerability discovery only for individual software versions, without considering the impact of shared code among successive versions and the evolution of source code. These affecting factors in vulnerability discovery process need to be taken into account estimate the future software vulnerability discovery trend more accurately. This thesis examines possible approaches for taking these factors into account in the previous works. We implemented these factors on vulnerability discovery process. We examine a new approach for quantitatively vulnerability discovery process, based on shared source code measurements among multiple version software system. The applicability of the approach is examined using Apache HTTP Web server and Mysql DataBase Management System (DBMS). The result of this approach shows better goodness of fit than fitting result in the previous researches. Using this revised software vulnerability discovery process, the superposition effect which is an unexpected vulnerability discovery in the previous researches could be determined by software discovery model. The multiple software vulnerability discovery model (MVDM) shows that vulnerability discovery rate is different with single vulnerability discovery model's (SVDM) discovery rate because of newly considered factors. From these result, we create and applied new SVDM for open source and\u00a0\u2026", "num_citations": "5\n", "authors": ["455"]}
{"title": "Test generation for BiCMOS circuits\n", "abstract": " Stuck-ON faults in BiCMOS devices result in an enhanced I/sub DDQ/. Stuck-OPEN faults exhibit both sequential behavior and delay faults. Test generation is considered for stuck-ON and stuck-OPEN faults in BiCMOS circuits. A procedure for obtaining test vectors/sequences for testing of faults manifesting as enhanced I/sub DDQ/ delay faults and sequential behavior is presented. The procedure involves activating a conduction path to obtain test vectors/sequences. The resulting test vectors obtained for single stuck-ON faults are also applicable to multiple stuck-ON faults. The scheme provides robust test sequences with unity Hamming distance for testing of delay faults and sequential behavior in BiCMOS circuits.< >", "num_citations": "5\n", "authors": ["455"]}
{"title": "An analysis and testing of operation induced faults in MOS VLSI\n", "abstract": " The operation induced faults in CMOS circuits are discussed. The significance of these faults for high density, small geometry circuits is pointed out. For modeling purposes the effects of these faults are correlated with classical fault models. A conductance fault model is presented to incorporate these faults. A test scheme to detect these faults is suggested which is based on the measurement of supply current. A scheme to generate test patterns for these faults is also outlined.<>", "num_citations": "5\n", "authors": ["455"]}
{"title": "Reprogrammable FPLA with universal test set\n", "abstract": " A field programmable logic array is presented which can be programmed. This FPLA uses one-transistor reprogrammable switches instead of fuses. The FPLA design presented here is also easily testable. In this design, the PLA is partitioned into two parts, which are tested independently. The delay is kept to a minimum for each test vector. Furthermore, parallelism is employed during testing, and thus minimal test time is obtained. It employs a universal test set of minimal length to detect all single crosspoint faults, stuck faults and bridging faults. This universal test set also covers the majority of multiple faults. The test set is simple and avoids test generation complexity. A user can reprogram and test the proposed PLA.", "num_citations": "5\n", "authors": ["455"]}
{"title": "Optimal clustering and statistical identification of defective ICs using I/sub DDQ/testing\n", "abstract": " Instead of relying on setting an arbitrary threshold current value as in traditional loop testing, clustering based test technique relies on the characteristics of an IC with respect to all the other ICs in a lot to make a test decision. An improvement in the cluster analysis technique for I/sub DDQ/ testing is presented. Results of applying this technique to data collected on a high volume graphics chip are presented. The results are also compared against a newer more innovative form of I/sub DDQ/ testing.", "num_citations": "4\n", "authors": ["455"]}
{"title": "Guest Editors' Introduction: Steps to Practical Reliability Measurement\n", "abstract": " The need for reliability in critical defense, aerospace, and real-time military systems has received much attention, but surveys show that civilian customers also rate reliability as one of software\u2019s most important attributes. Developers who must ensure a level of quality and reliability before product release obviously need techniques to evaluate reliability. Yet most have little knowledge of reliability models or their applicability and are unaware of their validation results or limitations. Reliability modeling is still in a very early stage compared with other engineering disciplines, but interest is growing. One book on the subject by John Musa, Anthony Iannino, and &zuhira Okumoto (see the box on p. 12), is so popular it has been translated into Japanese. And the IEEE Computer Society\u2019s Technical Committee on Software Engineering has formed a reliablity subcommittee to organize an annual international symposium. The\u00a0\u2026", "num_citations": "4\n", "authors": ["455"]}
{"title": "On designing robust testable CMOS combinational circuits\n", "abstract": " The potential invalidation of two-pattern tests for detecting stuck-open faults in CMOS combinational circuits has been studied from a functional point of view. Two methods of testable realisations avoiding the problem are presented. A new type of two-level testable realisation is proposed in which both stuck-open and stuck-short faults can be detected. In both the methods, valid test patterns are applied at the inputs and the logic responses are observed at the output. Finally, multilevel realisations of combinational functions have been considered in which all stuck-short faults are three-pattern testable and all stuck-open faults are two-pattern testable.", "num_citations": "4\n", "authors": ["455"]}
{"title": "Testing of complex gates\n", "abstract": " A systematic scheme for testing NMOS complex gates is presented. A minimal complete test set for all single and multiple detectable s-open, s-on and bridging faults is obtained. The scheme can easily be extended to test any general NMOS complex gate.", "num_citations": "4\n", "authors": ["455"]}
{"title": "Option in Control Implementation\n", "abstract": " A large range of options is available to implement the control part of a digital systems. Some common conceptions about them are examined here. A formal terminology is introduced. Several options with their variations are discussed. Similarity of PLAs with random logic is suggested. It is suggested that a microprogrammed design is less testable compared to random logic design.", "num_citations": "4\n", "authors": ["455"]}
{"title": "Software Reliability: A Quantitative Approach\n", "abstract": " This chapter takes the the view that there exist several components and aspects of software reliability and they need to be integrated into a systematic framework. It presents the essential concepts and approaches needed for predicting, evaluating, and managing the reliability of software. The chapter also presents the type of tools available for assisting in achieving and evaluating reliability. Software controls everyday life of individuals involving commerce or social interactions, in addition to critical applications such as aviation or banking. Reliability growth in software is generally described using a software reliability growth model. As bugs are encountered and removed during testing, the reliability of the software improves. In hardware systems, the reliability gradually declines because the possibility of a permanent failure increases. During debugging, the objective of software testing is to find faults as quickly as\u00a0\u2026", "num_citations": "3\n", "authors": ["455"]}
{"title": "Fault Modeling of ECL for high fault coverage of physical defects\n", "abstract": " Bipolar Emitter Coupled Logic (ECL) devices can now be fabricated at higher densities and consumes much lower power. Behaviour of simple and complex ECL gates are examined in the presence of physical faults. The effectiveness of the classical stuck-at model in representing physical failures in ECL gates is examined. It is shown that the conventional stuck-at fault model cannot represent a majority of circuit level faults. A new augmented stuck-at fault model is presented which provides a significantly higher coverage of physical failures. The model may be applicable to other logic families that use logic gates with both true and complementary outputs. A design for testability approach is suggested for on-line detection of certain error conditions occurring in gates with true and complementary outputs which is a normal implementation for ECL devices.", "num_citations": "3\n", "authors": ["455"]}
{"title": "Modeling of faulty behavior of ECL storage elements\n", "abstract": " Bipolar emitter coupled logic (ECL) devices can now be fabricated at very high densities and much lower power consumption. Behavior of two different ECL storage element implementations are examined in the presence of physical faults. While fault models for some implementations of CMOS storage elements have been examined, not much attention has been paid to ECL storage elements. The conventional stuck-at fault model termed minimal fault model assumes that an input (output) of a storage element can be stuck-at-1 or 0. The minimal fault model may not model the behavior under certain physical failures in a storage element. The enhanced fault model providing higher coverage of physical failures is presented.< >", "num_citations": "3\n", "authors": ["455"]}
{"title": "On the testing of microprogrammed processor\n", "abstract": " A testing procedure is given for a microprogrammed processor. The conventional procedure to generate test vectors is used instead of C-testability for the bit-slice microprocessor. The minimal complete test sequences are calculated for the micro-sequencer and the ALU, and stored in the micro-memory. Micro-memory is implemented by an electrically erasable PLA which has the capability to test itself using a universal test set. A one-bit wide processor section is assumed. However, the method can be extended to a processor of any word length. The tests for the microsequencer and the ALU are applied from the micromemory. The response is compared against precalculated signature stored in the system memory. A parity bit is generated to indicate a fault.< >", "num_citations": "3\n", "authors": ["455"]}
{"title": "Quality model for testing augmented reality applications\n", "abstract": " Augmented Reality applications have the capability of merging virtual objects into physical setting, or alternatively they can wrap physical objects within a virtual scene. Augmented reality applications are similar to virtual reality applications in that aspects of the visualizations are computer generated, but augmented reality apps also must contain a view of the physical world. Augmented reality applications are being utilized in service, manufacturing, product areas, as well as gaming. Mobile devices are becoming common runtime environments for augmented reality applications and the mobile device proliferation is enabling a wave of AR applications. Due to the combined nature of digital and physical objects, as well as the environmental and contextual constraints, a traditional test plan is not sufficient. A new quality model is proposed that takes these issues into account, and an example of how machine learning\u00a0\u2026", "num_citations": "2\n", "authors": ["455"]}
{"title": "Software Reliability Management\n", "abstract": " This entry identifies the factors that control software reliability and the approaches that are needed to achieve desired reliability targets. Common reliability measures are defined. The factors that impact defect density and defect finding rates are discussed and software reliability growth modeling is introduced. Both test-time-and test-coverage-based models are introduced. Modeling for security vulnerability discovery process is also presented. Reliability of multicomponent software systems is discussed followed by optimal allocation of test resources to achieve a target reliability level. Finally some of the applicable software tools available today are mentioned.", "num_citations": "2\n", "authors": ["455"]}
{"title": "Short-term periodicity in security vulnerability activity\n", "abstract": " Some of the major computer security organizations monitor a global pool of systems for presence of vulnerabilities and worms. The extensive amount of data generated provides important insights into the vulnerability activity and the risk they represent. An examination of the data published suggests weekly periodical behavior. This paper identifies the periodicity and examines its statistical significance for some of the data series published. The results shows that the seven-day periodicity in presence of unpatched vulnerabilities as well as the exploitation pattern. The behavior during the weekdays itself is found to vary. This behavior should be used to optimize resource allocations and for determination of risk.", "num_citations": "2\n", "authors": ["455"]}
{"title": "Software Reliability\n", "abstract": " For hardware systems, quantitative methods for achiev-ing and measuring reliability have been in universal use for a long time. Similar techniques for software are coming into use due to the emergence of well understood and validated approaches. Enough industrial and experimental data have become available to develop and evaluate meth-ods for achieving high reliability. The minimum accept-able standards for software reliability have risen gradually in recent years. Developing reliable software has become an engineering discipline rather than an art. This article presents an overview of the basic concepts and techniques in the software reliability field. First we discuss the reliability approaches taken during different phases of software development. Commonly used soft-ware reliability measures are defined next. We discuss what factors control software defect density. Key ideas in test methodologies are presented. Use of a software re-liability growth model is discussed and illustrated using industrial data. Use of such models allows one to estimate the testing effort needed to reach a reliability goal. We will also see how reliability of a system can be evaluated if we know the failure rates of the components. Finally the article presents the type of tools that are available to assist in achieving and evaluating reliability.", "num_citations": "2\n", "authors": ["455"]}
{"title": "A Neural Network based Approach for Testing Analog Circuits with Frequency Domain Classification and Time Domain Testing\n", "abstract": " A new analog circuit testing technique based on neural networks is proposed. It is based on utilizing the domain knowledge in order to design a training set for a neural network which characterizes the circuit\u2019s response under faulty and fault free conditions. The training set is a general set representation of the entire response space of the circuit under both the faulty and fault free conditions. The results show that the neural network approach is capable of detecting response variation due to acceptable parameter variations from that due to faults. Effectiveness of the approach in detecting hard faults (shorts and opens) and soft faults (out-ofspecification) for both single and multiple faults are presented.", "num_citations": "2\n", "authors": ["455"]}
{"title": "Differential IDDQ Testable Static RAM Architecture\n", "abstract": " A testable design that enhances the IDDQ testability of random access memories (SRAMs) for o-line testing is proposed. Increased accuracy and test speed can be achieved by memory array partitioning. Comaparision of IDDQ values from two blocks is performed during parallel write/read operations to memory locations of the two blocks. Simultaneous write/read operations to all locations within physically interleaved block can signi cantly enhance the test speed as well as fault activation.", "num_citations": "2\n", "authors": ["455"]}
{"title": "A novel high-speed BiCMOS domino logic family\n", "abstract": " A new BiCMOS dynamic logic family is presented. The logic gates provide a significant speed-up over existing logic families, such as, CMOS, BiCMOS and dynamic CMOS for the same feature size. The proposed logic family provides high drive capability to drive large loads at higher speeds compared to CMOS domino logic family; there are power advantages due to dynamic operation compared to the conventional fully complementary CMOS and BiCMOS devices. It also has area advantage compared to that of the conventional BiCMOS gates. The proposed high-speed BiCMOS domino family provides speed improvement with existing feature size, using the existing fabrication technology.", "num_citations": "2\n", "authors": ["455"]}
{"title": "Data-feedthrough faults in circuits using unclocked storage elements\n", "abstract": " Some faults in storage elements (SEs) do not manifest as stuck-at-0/1 faults. These include data-feedthrough faults that cause the SE cell to exhibit combinational behaviour. The authors investigate the implications of such faults on the behaviour of circuits using unclocked SEs. It is shown that effects of data-feedthrough faults at the behavioural level are different from those due to stuck-at faults, and therefore tests generated for the latter may be inadequate.< >", "num_citations": "2\n", "authors": ["455"]}
{"title": "Input pattern classification for transistor level testing of BiCMOS circuits\n", "abstract": " In BiCMOS, transistor stuck-OPEN faults exhibit delay faults in addition to sequential behavior. Stuck-ON faults cause enhanced I/sub DDQ/. The faulty behavior of Bipolar (TTL) and CMOS logic families is compared with BiCMOS. The faults in BiCMOS devices cause one or more parts (p-part or n-parts) of the circuit to exhibit a different state (conducting or nonconducting) from the fault-free circuit. An input pattern classification scheme is presented for different faults. These classes of patterns are then used to obtain test sets.< >", "num_citations": "2\n", "authors": ["455"]}
{"title": "Empirical Estimation of Fault Exposure Ratio\n", "abstract": " The fault exposure ratio K represents the average detectability of faults in a software. Knowing the value of K for a software system allows the use of the exponential model, and probably also the logarithmic model, even before software testing phase begins. It can also be used to enhance the software reliability growth models (SRGM) at the early stage of testing when the software failure data can only provides limited accuracy. This paper presents a model which relates the fault exposure ratio K to the initial defect density D0. Empirical analysis indicates that this model can describe K fairly well.", "num_citations": "2\n", "authors": ["455"]}
{"title": "A new testable design of field programmable logic arrays\n", "abstract": " A field programmable logic array (FPLA) design is presented which is easily testable. The programmable logic array (PLA) is partitioned into two parts, which are tested independently. The delay is kept to a minimum for each test vector. Parallelism is employed during testing, and thus minimal test time is obtained. It employs a universal test set of minimal length to detect all single crosspoint faults, stuck faults, and bridging faults. This universal test set also covers the majority of multiple faults. The test set is simple and avoids test generation complexity. A user can reprogram and test the proposed PLA.< >", "num_citations": "2\n", "authors": ["455"]}
{"title": "Design of a testable RISC-to-CISC control architecture\n", "abstract": " A new control architecture is introduced. It is highly testable, and it will support both simple and complex instruction sets. The flexibility is achieved by use of allocable-storage (AS) blocks. An AS block may be assigned to the control part to hold the microcode, or it may be assigned to the data part to hold data or instructions.", "num_citations": "2\n", "authors": ["455"]}
{"title": "Quantitative Assessment of Cybersecurity Risks for Mitigating Data Breaches in Business Systems\n", "abstract": " The evaluation of data breaches and cybersecurity risks has not yet been formally addressed in modern business systems. There has been a tremendous increase in the generation, usage and consumption of industrial and business data as a result of smart and computational intensive software systems. This has resulted in an increase in the attack surface of these cyber systems. Consequently, there has been a consequent increase in the associated cybersecurity risks. However, no significant studies have been conducted that examine, compare, and evaluate the approaches used by the risk calculators to investigate the data breaches. The development of an efficient cybersecurity solution allows us to mitigate the data breaches threatened by the cybersecurity risks such as cyber-attacks against database storage, processing and management. In this paper, we develop a comprehensive, formal model that estimates the two components of security risks: breach cost and the likelihood of a data breach within 12 months. The data used in this model are taken from the industrial business report, which provides the necessary information collected and the calculators developed by the major organizations in the field. This model integrated with the cybersecurity solution uses consolidated factors that have a significant impact on the data breach risk. We propose mathematical models of how the factors impact the cost and the likelihood. These models allow us to conclude that results obtained through the models mitigate the data breaches in the potential and future business system dynamically. View Full-Text", "num_citations": "1\n", "authors": ["455"]}
{"title": "Quantitative Cyber-Security\n", "abstract": " }. It is possible that some of them will get selected more often in a non-optimal manner.\u2013Code within a sub-partition may be correlated relative to the probability of exercising some faults. Thus the effectiveness of testing may be diluted if the same sub-partition frequently gets chosen.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Assessing Software Reliability Enhancement Achievable through Testing\n", "abstract": " For achieving high reliability in software, one must consider the nature of actual defects and the defect finding process, which is necessary to make testing efficient and to ensure that the reliability growth modeling is realistic. This chapter presents a perspective based on the nature of actual defects as described by the detectability profile. The real defects vary significantly in testability and the hard-to-test faults are likely to be found later during testing. The chapter considers software partitioning and how it impacts testing effectiveness. The defect-finding process is initially modeled using the common assumptions of a fixed fault exposure ratio and then refined using actual reliability growth data. The two approaches result in the exponential and the logarithmic Poisson software reliability growth models. The chapter also considers the relationship between test coverage and defect density. It discusses the extent to which\u00a0\u2026", "num_citations": "1\n", "authors": ["455"]}
{"title": "Fault Tolerant Computing\n", "abstract": " \u2022 The inputs to a system can represent different types of operations. The input mix called \u201cProfile\u201d can impact effectiveness of testing.\u2022 For example a Search program can be tested for text data, numerical data, data already sorted etc. If most testing a done using numerical data, more bugs related to text data may remain unfound.4/9/2008 template 6", "num_citations": "1\n", "authors": ["455"]}
{"title": "Fault Tolerant Autonomic Computing in Web Services\n", "abstract": " Web services enable networked systems, typically based upon the World Wide Web (WWW), and their related services, to interoperate. Moreover, Service-Oriented Architecture (SOA), Simple Object Access Protocol (SOAP), Extensible Markup Language, Remote Procedure Call (XMLRPC), and Representational State Transfer (REST) are architectural styles of networked systems, upon which Web Services are implemented. Given the growing dependence and criticality of these services for users, fault tolerance plays an increasingly important role in providing usable and reliable services. With the advent of autonomic computing, or selfmanaging computer systems, research groups have been pursuing methods of combining fault tolerant autonomic approaches with Web services. This paper reviews research and progress to date of those autonomic methods and researches Web services architectural style support of autonomic methods.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Early Applicability of the Coverage/Defect Model\n", "abstract": " Test coverage is closely related to the defect detection efficiency. A model to relate test coverage and defects found has recently been proposed and tested using actual data sets. Most of the data sets used so far have been obtained by testing software that has already been debugged to some extent. Here we examine the question of the applicability of the MLBKS model very early when little or no prior debugging has been performed. Data collected from an actual project is analyzed. The results demonstrate that the model successfully describes the early relationship between the test coverage and the defects found. The results confirm earlier analysis that suggests that the position of the knee of the curve describing the model should depend on the initial defect density.", "num_citations": "1\n", "authors": ["455"]}
{"title": "A visualization system for sliding windows protocols\n", "abstract": " This paper shows how algorithm visualization can be used to teach sliding windows protocols. In the described approach, the student creates and visually manipulates traffic between an abstract sender/receiver pair. This work contributes to algorithm visualization by describing a tool built specifically to teach sliding windows protocols. The sliding windows visualization system can be seen at http://www.burgoyne.com//spl sim/henryd/master/spl I.bar/project. The interaction levels available in the software are described. Finally, we make the claim that sliding windows protocols lend themselves especially to visualization and that algorithm visualization is an important \"missing link\" in the teaching of sliding windows protocols.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Clustering-based production-line binning of ICs based on IDDQ\n", "abstract": " A clustering-based technique is proposed for production line testing and real time binning of ICs. This paper presents a two-phase approach. The first phase involves off-line clustering and cluster characterization based on prior data. In the second phase, each device is sorted based on its quality attributes, into bins associated with specific quality and cost parameters. This allows fast real-time sorting of ICs on production line use of IDDQ test methodology. Use of clustering algorithm provides a high-resolution technique for identifying groups of devices with similar characteristics. Proposed technique is tested on production test data and results are presented.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Input pattern classification for detection of stuck-ON and bridging faults using I/sub DDQ/testing in BiCMOS and CMOS circuits\n", "abstract": " Quiescent power supply current monitoring (I/sub DDQ/) has been shown to be effective for testing CMOS devices. BiCMOS is emerging as a major technology for high speed, high performance, digital and mixed signal applications. Stuck-ON faults as well as bridging faults in BiCMOS circuits cause enhanced I/sub DDQ/. An input pattern classification scheme is presented for detection of stuck-ON/bridging faults causing enhanced I/sub DDQ/. This technique can also be used for detecting I/sub DDQ/ related faults in CMOS circuits.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Operational and Test Performance in the Presence of Built-in Current Sensors\n", "abstract": " The effects of Built-In Current Sensors (BICS) on   measurements as well as on the performance of the circuit under test are considered. Most of the Built-In Current Sensor designs transform the ground terminal of the circuit under test into a virtual ground. This causes increases in both propagation delay and   sampling time with the increase in the number of gates, affecting both test as well as operational performance. The effects that current sensors have on the operational and test performance of a circuit are considered. Circuit partitioning may be used for overcoming the effects of BICS on   measurements as well as on the performance of the circuit under test.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Input pattern classification for transistor level testing of bridging faults in BiCMOS circuits\n", "abstract": " Combining the advantages of bipolar and CMOS, BiCMOS is emerging as a major technology for high speed, high performance, digital and mixed signal applications. Recent investigations have revealed that bridging faults can be a major failure mode in ICs. This paper presents the effects of bridging faults affecting p- or n-parts and input bridging faults of logical nodes affecting p- and n-parts. It is shown that bridging faults can be detected by I/sub DDQ/ monitoring in BiCMOS devices. An input pattern classification scheme is presented for bridging faults. These classes of input patterns are then used to obtain test sets for bridging fault detection.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Testable design of BiCMOS circuits for stuck-open fault detection using single patterns\n", "abstract": " Single BJT BiCMOS devices exhibit sequential behavior under transistor stuck-OPEN (s-OPEN) faults. In addition to the sequential behavior, delay faults are also present. Detection of s-OPEN faults exhibiting sequential behavior needs two-pattern or multipattern sequences, and delay faults are all the more difficult to detect. A new design for testability scheme is presented that uses only two extra transistors to improve the circuit testability regardless of timing skews/delays, glitches, or charge sharing among internal nodes. With this design, only a single vector is required to test for a fault instead of the two-pattern or multipattern sequences. The testable design scheme presented also avoids the requirement of generating tests for delay faults.< >", "num_citations": "1\n", "authors": ["455"]}
{"title": "Manifestations of faults in single-and double-BJT BiCMOS logic gates\n", "abstract": " Combining the inherent advantages of bipolar and CMOS, BiCMOS is emerging as a major technology for high speed, high performance, digital and mixed signal applications. Logic behaviour of single- and double-BJT BiCMOS devices under transistor level shorts and opens is examined. In addition to sequential behaviour, some stuck open faults exhibit increased delay. While most stuck on faults can be detected by logic level testing, some of them can only be detected by monitoring the power supply current (IDDQ monitoring). A stuck open fault in double-BJT BiCMOS device manifesting as enhanced dynamic IDD current is shown. The faulty behaviour of bipolar (TTL) and CMOS logic families is compared with BiCMOS. Testability of both single- and double-BJT BiCMOS devices are discussed, along with a design for testability approach for detecting stuck open faults in S-BJT BiCMOS devices.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Faulty behavior of asynchronous storage elements\n", "abstract": " It is often assumed that the faults in storage elements (SEs) can be modeled as output/input stuck-at-faults of the element. They are implicitly considered equivalent to the stuck-at faults in the combinational logic surrounding the SEcells. A more accurate higher level fault model for elementarySEs usedinasynchronous circuits is presented. This model o ers better representation of the physical failures. It is shown that the stuck-at model may be adequate if only modest fault coverage is desired. The enhanced model includes some commonfault behaviors of SEs that are not coveredbythe the stuck-at model. These include data-feed-t hrough behaviors that cause the SEto be combinational. Fault models for complex SEcells can be obtained without a signi cant loss of information about the structure of the circuit.", "num_citations": "1\n", "authors": ["455"]}
{"title": "An Introduction to Software Reliability Models.\n", "abstract": " It is now recognized that a software product must have achieved a certain degree ofreliability before it can be released. Use ofa software reliability model allows a manager to project the time and personnel needed to achieve the desired reliability by the target release date. The recent book Software Reliability Models [1] presents recent developments in theory, evaluation and applications of major models.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Optimization of test parallelism with limited hardware overhead\n", "abstract": " The main considerations for built-in self-test (BIST) for complex circuits are fault coverage, test time, and hardware overhead. In the BIST technique, exhaustive or pseudo-exhaustive testing is used to test the combinational logic in a register sandwich. If register sandwiches can be identified in a complex digitial system, it is possible to test several of them in parallel using the built-in logic block observation (BILBO) technique. Concurrent built-in logic block observation (CBILBO) technique can further improve the test time, but it requires significant hardware overhead. A systematic scheduling technique is suggested to optimize parallel tests of register sandwiches. Techniques are proposed to deal with shared registers for parallel testing. The proposed method attempts to reduce further the test time while only modestly increasing the hardware overhead.", "num_citations": "1\n", "authors": ["455"]}
{"title": "MESHNET: a new fault-tolerant LAN for industrial environment\n", "abstract": " The design and protocol for a fault-tolerant high-performance local area network called MESHNET is proposed. It is a token passing scheme with multiple buses. It provides multiple paths between any pair of nodes and thus the fault-tolerance feature. Under fault-free conditions, it can support a much faster rate of traffic generation and a much higher number of nodes compared to a single bus architecture and gives a better performance in terms of channel utilization, throughput, and average delay per data packet. It maintains connectivity even when multiple faults have occurred. The topology, architecture, and other basic features of the proposed network are described, and a preliminary reliability analysis is presented.< >", "num_citations": "1\n", "authors": ["455"]}
{"title": "On inherent untestability of unaugmented microprogrammed control\n", "abstract": " Effective and efficient testing of the control part of a processor has remained a difficult problem. While several approaches have been proposed in the literature for handling unaugmented control parts, they involve questionable assumptions, and the results have not been encouraging. Here it is shown that unless some DFT (Design for Testability) approaches are taken, microprogrammed control is inherently a poorly testable structure. The considerations include lack of an elegant fault model, presence of components with low random testability, the length of a checking sequence and information-theoretic considerations. The design approaches must therefore include DFT augmentations and/or removal of sub-functional logic.", "num_citations": "1\n", "authors": ["455"]}
{"title": "Analyzing data for CMOS leakage faults\n", "abstract": " Stuck-on and leakage faults are among the major failure modes for CMOS ICS. The only definite way to test for these faults is to monitor the supply current under different test vectors. To evaluate the effectiveness of this test technique, distribution of normal and abnormal conductance values has to be obtained. This paper presents two procedures which can be used to extract the required information from measurable data. Experimental data for a number of CMOS chips has been analyzed and the results are presented.", "num_citations": "1\n", "authors": ["455"]}