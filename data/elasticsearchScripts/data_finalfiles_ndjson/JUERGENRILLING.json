{"title": "Application of dynamic slicing in program debugging\n", "abstract": " A dynamic program slice is an executable part of a program whose behavior is identical, for the same program input, to that of the original program with respect to a variable (s) of interest at some execution position. In the existing dynamic slicing tools dynamic slices are represented in a textual form, ie, a dynamic slice is displayed to programmers in the form of highlighted statements or in the form of a subprogram. Although dynamic slicing does narrow the size of the program, it is still up to the programmer to analyze the text of a dynamic slice and identify a faulty part in the program. The textual representation of a dynamic slice does not provide much guidance in program debugging and understanding of program behavior, which frequently is a major factor in efficient debugging. During dynamic slice computation different types of information are computed and then discarded after computation of the dynamic slice. In this paper we propose new dynamic slicing related features that exploit this information to improve the process of program debugging. These features were implemented in our dynamic slicing tool that can be used for program debugging.", "num_citations": "112\n", "authors": ["125"]}
{"title": "Empowering software maintainers with semantic web technologies\n", "abstract": " Software maintainers routinely have to deal with a multitude of artifacts, like source code or documents, which often end up disconnected, due to their different representations and the size and complexity of legacy systems. One of the main challenges in software maintenance is to establish and maintain the semantic connections among all the different artifacts. In this paper, we show how Semantic Web technologies can deliver a unified representation to explore, query and reason about a multitude of software artifacts. A novel feature is the automatic integration of two important types of software maintenance artifacts, source code and documents, by populating their corresponding sub-ontologies through code analysis and text mining. We demonstrate how the resulting \u201cSoftware Semantic Web\u201d can support typical maintenance tasks through ontology queries and Description Logic reasoning, such as\u00a0\u2026", "num_citations": "106\n", "authors": ["125"]}
{"title": "Dynamic program slicing methods\n", "abstract": " A dynamic program slice is that part of a program that \u201caffects\u201d the computation of a variable of interest during program execution on a specific program input. Dynamic program slicing refers to a collection of program slicing methods that are based on program execution and may significantly reduce the size of a program slice because run-time information, collected during program execution, is used to compute program slices. Dynamic program slicing was originally proposed only for program debugging, but its application has been extended to program comprehension, software testing, and software maintenance. Different types of dynamic program slices, together with algorithms to compute them, have been proposed in the literature. In this paper we present a classification of existing dynamic slicing methods and discuss the algorithms to compute dynamic slices. In the second part of the paper, we compare the\u00a0\u2026", "num_citations": "103\n", "authors": ["125"]}
{"title": "Automatic quality assessment of source code comments: the JavadocMiner\n", "abstract": " An important software engineering artefact used by developers and maintainers to assist in software comprehension and maintenance is source code documentation. It provides insights that help software engineers to effectively perform their tasks, and therefore ensuring the quality of the documentation is extremely important. Inline documentation is at the forefront of explaining a programmer\u2019s original intentions for a given implementation. Since this documentation is written in natural language, ensuring its quality needs to be performed manually. In this paper, we present an effective and automated approach for assessing the quality of inline documentation using a set of heuristics, targeting both quality of language and consistency between source code and its comments. We apply our tool to the different modules of two open source applications (ArgoUML and Eclipse), and correlate the results returned\u00a0\u2026", "num_citations": "94\n", "authors": ["125"]}
{"title": "Program slicing in understanding of large programs\n", "abstract": " Program slicing transforms a large program into a smaller one that contains only statements relevant to the computation of a given function. It has been shown that program slicing can be useful in program understanding. Traditionally, program slices are represented in the textual form. Although slicing does narrow the size of the program, the textual representation of a slice does not provide much guidance in the understanding of large programs. In this paper we present program slicing concepts on the module level that allow for better understanding of program slices of large programs and their executions. These concepts have been developed for static and dynamic program slicing and are combined with different methods of visualization to guide programmers in the process of program understanding. The presented concepts have been implemented in the slicing tool that is used to investigate the usefulness of\u00a0\u2026", "num_citations": "84\n", "authors": ["125"]}
{"title": "Change impact analysis for requirement evolution using use case maps\n", "abstract": " Changing customer needs and computer technology are the driving factors influencing software evolution. There is a need to assess the impact of these changes on existing software systems. Requirement specification is gaining increasingly attention as a critical phase of software systems development process. In particular for larger systems, it quickly becomes difficult to comprehend what impact a requirement change might have on the overall system or parts of the system. Thus, the development of techniques and tools to support the evolution of requirement specifications becomes an important issue. In this paper we present a novel approach to change impact analysis at the requirement level. We apply both slicing and dependency analysis at the use case map specification level to identify the potential impact of requirement changes on the overall system. We illustrate our approach and its applicability with a\u00a0\u2026", "num_citations": "77\n", "authors": ["125"]}
{"title": "Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics\n", "abstract": " Achieving and maintaining high software quality is most dependent on how easily the software engineer least familiar with the system can understand the system's code. Understanding attributes of cognitive processes can lead to new software metrics that allow the prediction of human performance in software development and for assessing and improving the understandability of text and code. In this research we present novel metrics based on current understanding of short-term memory performance to predict the location of high frequencies of errors and to evaluate the quality of a software system. We further enhance these metrics by applying static and dynamic program slicing to provide programmers with additional guidance during software inspection and maintenance efforts.", "num_citations": "77\n", "authors": ["125"]}
{"title": "Dynamic program slicing in understanding of program execution\n", "abstract": " A dynamic program slice is an executable part of a program whose behavior is identical, for the same program input, to that of an original program with respect to a variable(s) of interest at some execution position. In the existing dynamic slicing tools, dynamic slices are represented in the textual form, i.e., a dynamic slice is displayed to programmers as a subprogram or as highlighted statements in the original program. Although dynamic slicing does narrow the size of the program, the textual representation of a dynamic slice does not provide much guidance in the understanding of program execution. During dynamic slice computation different types of information are computed and then discarded. In this paper we propose new dynamic-slicing related features that exploit this information for the purpose of understanding program execution. These features were implemented in the dynamic slicing tool that is used to\u00a0\u2026", "num_citations": "76\n", "authors": ["125"]}
{"title": "Flexible Ontology Population from Text: The OwlExporter.\n", "abstract": " Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.", "num_citations": "74\n", "authors": ["125"]}
{"title": "Ontological approach for the semantic recovery of traceability links between software artefacts\n", "abstract": " Traceability links provide support for software engineers in understanding relations and dependencies among software artefacts created during the software development process. The authors focus on re-establishing traceability links between existing source code and documentation to support software maintenance. They present a novel approach that addresses this issue by creating formal ontological representations for both documentation and source code artefacts. Their approach recovers traceability links at the semantic level, utilising structural and semantic information found in various software artefacts. These linked ontologies are supported by ontology reasoners to allow the inference of implicit relations among these software artefacts.", "num_citations": "73\n", "authors": ["125"]}
{"title": "Software clustering using dynamic analysis and static dependencies\n", "abstract": " Decomposing a software system into smaller, more manageable clusters is a common approach to support the comprehension of large systems. In recent years, researchers have focused on clustering techniques to perform such architectural decomposition, with the most predominant clustering techniques relying on the static analysis of source code. We argue that these static structural relationships are not sufficient for software clustering due to the increased complexity and behavioral aspects found in software systems. In this paper, we present a novel software clustering approach that combines dynamic and static analysis to identify component clusters. We introduce a two-phase clustering technique that combines software features to build a core skeleton decomposition with structural information to further refine these clusters. A case study is presented to evaluate the applicability and effectiveness of our\u00a0\u2026", "num_citations": "58\n", "authors": ["125"]}
{"title": "An ontology-based approach for traceability recovery\n", "abstract": " Traceability links provide support for software engineers in understanding the relations and dependencies among software artifacts created during the software development process. In this research, we focus on re-establishing traceability links between existing source code and documentation to support reverse engineering. We present a novel approach that addresses this issue by creating formal ontological representations for both the documentation and source code artifacts. These representations are then aligned to establish traceability links at the semantic level. Our approach recovers traceability links by utilizing the structural and semantic information in various software artifacts and the linked ontologies are also supported by ontology reasoners to infer implicit relations among these software artifacts.", "num_citations": "57\n", "authors": ["125"]}
{"title": "Text mining and software engineering: an integrated source code and document analysis approach\n", "abstract": " Documents written in natural languages constitute a major part of the artefacts produced during the software engineering life cycle. Especially during software maintenance or reverse engineering, semantic information conveyed in these documents can provide important knowledge for the software engineer. A text mining system capable of populating a software ontology with information detected in documents is presented. A particular novelty is the integration of results from automated source code analysis into a natural language processing pipeline, allowing to crosslink software artefacts represented in code and natural language on a semantic level.", "num_citations": "56\n", "authors": ["125"]}
{"title": "An approach for mapping features to code based on static and dynamic analysis\n", "abstract": " System evolution depends greatly on the ability of a maintainer to locate source code that is specific to feature implementation. Existing feature location techniques require either exercising several features of the system, or rely heavily on domain experts to guide the feature location process. In this paper, we present a novel approach for feature location that combines static and dynamic analysis techniques. An execution trace is generated by exercising the feature under study (dynamic analysis). A component dependency graph (static analysis) is used to rank the components invoked in the trace according to their relevance to the feature. Our ranking technique is based on the impact of a component modification on the rest of the system. The proposed approach is automatic to a large extent relieving users from any decision that would otherwise require extensive domain knowledge of the system. A case study is\u00a0\u2026", "num_citations": "50\n", "authors": ["125"]}
{"title": "A cognitive complexity metric based on category learning\n", "abstract": " Software development is driven by software comprehension. Controlling a software development process is dependent on controlling software comprehension. Measures of factors that influence software comprehension are required in order to achieve control. The use of high-level languages results in many different kinds of lines of code that require different levels of comprehension effort. As the reader learns the set of arrangements of operators, attributes and labels particular to an application, comprehension is eased as familiar arrangements are repeated. Elements of cognition that describe the mechanics of text comprehension serve as a guide to assessing comprehension demands in the understanding of programs written in high level languages. A new metric, Kinds of Lines of Code Identifier Density is introduced and a case study demonstrates its application and importance. Related work is discussed.", "num_citations": "44\n", "authors": ["125"]}
{"title": "3D visualization techniques to support slicing-based program comprehension\n", "abstract": " Graphic visuals derived from reverse engineered source code have long been recognized for their impact on improving the comprehensibility of structural and behavioral aspects of large software systems and their source code. A number of visualization techniques, primarily graph-based, do not scale. Some other proposed techniques based on 3D metaphors tend to obscure important structural relationships in the program. Multiple views displayed in overlapping windows are suggested as a possible solution, which more often than not results in problems of information overload and cognitive discontinuity. In this paper, we first present a comprehensive survey of related work in program comprehension and software visualization, and follow it up with a detailed description of our research which uses program slicing for deriving program structure-based attributes and 3D-metaball-based rendering techniques to help\u00a0\u2026", "num_citations": "38\n", "authors": ["125"]}
{"title": "An ontology-based approach to software comprehension-reasoning about security concerns\n", "abstract": " There exists a large variety of techniques to detect and correct software security vulnerabilities at the source code level, including human code reviews, testing, and static analysis. In this article, we present a static analysis approach that supports both the identification of security flaws and the reasoning about security concerns. We introduce an ontology-based program representation that lets security experts and programmers specify their security concerns as part of the ontology. Within our tool implementation, we support complex queries on the underlying program model using either predefined or user-defined concepts and relations. Queries regarding security concerns, such as exception handling, object accessibility etc. are demonstrated in order to show the applicability and flexibility of our approach", "num_citations": "34\n", "authors": ["125"]}
{"title": "An ontological software comprehension process model\n", "abstract": " Comprehension is an essential part of software maintenance. Only software that is well understood can evolve in a controlled manner. In this paper, we present a formal process model to support the comprehension of software systems by using Ontology and Description Logic. This formal representation supports the use of reasoning services across different knowledge resources and therefore, enables us to provide users with guidance during the comprehension process that is context sensitive to their particular comprehension task.", "num_citations": "32\n", "authors": ["125"]}
{"title": "On the use of metaballs to visually map source code structures and analysis results onto 3d space\n", "abstract": " Many reverse-engineering tools have been developed to derive abstract representations from existing source code. Graphic visuals derived from reverse engineered source code have long been recognized for their impact on improving the comprehensibility of the structural and behavioral aspects of software systems and their source code. As programs become more complex and larger, the sheer volume of information to be comprehended by developers becomes daunting. In this paper, we combine dynamic source analysis to selectively identify source code that is relevant at any point and combine it with 3D visualization techniques to reverse engineer and analyze source code, program executions, and program structures. For this research, we focus particularly on the use of metaballs, a 3D modeling technique that has already found extensive use representing complex organic shapes and structural\u00a0\u2026", "num_citations": "30\n", "authors": ["125"]}
{"title": "Mining bug repositories--a quality assessment\n", "abstract": " The process of evaluating, classifying, and assigning bugs to programmers is a difficult and time consuming task which greatly depends on the quality of the bug report itself. It has been shown that the quality of reports originating from bug trackers or ticketing systems can vary significantly. In this research, we apply information retrieval (IR) and natural language processing (NLP) techniques for mining bug repositories. We focus particularly on measuring the quality of the free form descriptions submitted as part of bug reports used by open source bug trackers. Properties of natural language influencing the report quality are automatically identified and applied as part of a classification task. The results from the automated quality assessment are used to populate and enrich our existing software engineering ontology to support a further analysis of the quality and maturity of bug trackers.", "num_citations": "27\n", "authors": ["125"]}
{"title": "Modeling comprehension processes in software development\n", "abstract": " As programs become more complex and larger, the sheer volume of information to be comprehended by developers becomes daunting. Software development is fraught with complexity that is difficult to identify a priori. Complexity is relative to the task, the developer's experience and the resources available. In this research, we identify comprehension processes applied in software development, and the cognitive loads associated with these processes. We present an abstraction of the cognitive environment of the software developer, and introduce techniques to minimize the cognitive effort in the short-term and the long-term.", "num_citations": "26\n", "authors": ["125"]}
{"title": "Investigating the relationship between usability and conceptual gaps for human-centric CASE tools\n", "abstract": " Several interviews that we conducted highlight that many of the ease-of-use (usability) problems of CASE tools are instances of \"conceptual gaps\". A conceptual gap arises because of some difference between the software developer's mental model of the integrated development environment (IDE) and the way it can be used. Filling these gaps is the first step towards human-centric IDE. In this article, we begin by motivating our investigations with a survey highlighting common usability problems in the most popular Java IDEs. We then discuss how the developer's experiences with the complicity of cognitive studies can minimize these conceptual gaps while making the IDE more human-centered. We close our discussion with recommendations for establishing a rigorous scientific investigation for filling these conceptual gaps, as well as for developing and evaluating the ease of use of IDEs.", "num_citations": "26\n", "authors": ["125"]}
{"title": "Beyond information silos\u2014An omnipresent approach to software evolution\n", "abstract": " Nowadays, software development and maintenance are highly distributed processes that involve a multitude of supporting tools and resources. Knowledge relevant for a particular software maintenance task is typically dispersed over a wide range of artifacts in different representational formats and at different abstraction levels, resulting in isolated 'information silos'. An increasing number of task-specific software tools aim to support developers, but this often results in additional challenges, as not every project member can be familiar with every tool and its applicability for a given problem. Furthermore, historical knowledge about successfully performed modifications is lost, since only the result is recorded in versioning systems, but not how a developer arrived at the solution. In this research, we introduce conceptual models for the software domain that go beyond existing program and tool models, by including\u00a0\u2026", "num_citations": "25\n", "authors": ["125"]}
{"title": "A unified ontology-based process model for software maintenance and comprehension\n", "abstract": " In this paper, we present a formal process model to support the comprehension and maintenance of software systems. The model provides a formal ontological representation that supports the use of reasoning services across different knowledge resources. In the presented approach, we employ our Description Logic knowledge base to support the maintenance process management, as well as detailed analyses among resources, e.g., the traceability between various software artifacts. The resulting unified process model provides users with active guidance in selecting and utilizing these resources that are context-sensitive to a particular comprehension task. We illustrate both, the technical foundation based on our existing SOUND environment, as well as the general objectives and goals of our process model.", "num_citations": "24\n", "authors": ["125"]}
{"title": "Granularity-driven dynamic predicate slicing algorithms for message passing systems\n", "abstract": " Program Slicing is a well-known decomposition technique that transforms a large program into a smaller one that contains only statements relevant to the computation of a selected function. In this paper, we present two novel predicate-based dynamic slicing algorithms for message passing programs. Unlike more traditional slicing criteria that focus only on parts of the program that influence a variable of interest at a specific position in the program, a predicate focuses on those parts of the program that influence the predicate. The dynamic predicate slices capture some global requirements or suspected error properties of a distributed program and computes all statements that are relevant. The presented algorithms differ from each other in their computational approaches (forward versus backward) and in the granularity of information they provide. A proof of correctness of these algorithms is provided\u00a0\u2026", "num_citations": "24\n", "authors": ["125"]}
{"title": "A hybrid program slicing framework\n", "abstract": " Program slicing is a decomposition technique that transforms a large program into a smaller one that contains only statements relevant to the computation of a selected function. Applications of program slicing can be found in software testing, debugging, and maintenance by reducing the amount of data that has to be analyzed in order to comprehend a program or parts of its functionality. In this paper, we present a general dynamic and static slicing algorithm. Both algorithms are based on the notion of removable blocks and compute executable slices for object-oriented programs. In the second part of the paper we present our hybrid-slicing framework that was designed to take advantage of static and dynamic slicing algorithms that share the common notion of removable blocks, to enhance traditional slicing techniques. The hybrid-slicing framework is an integrated part of our existing MOOSE software\u00a0\u2026", "num_citations": "23\n", "authors": ["125"]}
{"title": "A requirement level modification analysis support framework\n", "abstract": " Modification analysis is an essential phase of most software maintenance processes, requiring decision makers to perform and predict potential change impacts, feasibility and costs associated with a potential modification request. The majority of existing techniques and tools supporting modification analysis focusing on source code level analysis and require an understanding of the system and its implementation. In this research, we present a novel approach to support the identification of potential modification and re-testing efforts associated with a modification request, without the need for analyzing or understanding the system source code. We combine Use Case Maps with Formal Concept Analysis to provide a unique modification analysis framework that can assist decision makers during modification analysis at the requirements level. We demonstrate the applicability of our approach on a telephony system\u00a0\u2026", "num_citations": "22\n", "authors": ["125"]}
{"title": "Abstract operational semantics for use case maps\n", "abstract": " Scenario-driven requirement specifications are widely used to capture and represent functional requirements. Use Case Maps (UCM) is being standardized as part of the User Requirements Notation (URN), the most recent addition to ITU\u2013T\u2019s family of languages. UCM models allow the description of functional requirements and high-level designs at early stages of the development process. Recognizing the importance of having a well defined semantic, we propose, in this paper, a concise and rigorous formal semantics for Use Case Maps (UCM). The proposed formal semantics addresses UCM\u2019s operational semantics and covers the key language functional constructs. These semantics are defined in terms of Multi-Agent Abstract State Machines that describes how UCM specifications are executed and eliminates ambiguities hidden in the informal language definition. The resulting operational semantics\u00a0\u2026", "num_citations": "21\n", "authors": ["125"]}
{"title": "A light-weight proactive software change impact analysis using use case maps\n", "abstract": " Changing customer needs and technology are driving factors influencing software evolution. Consequently, there is a need to assess the impact of these changes on existing software systems. For many users, technology is no longer the main problem, and it is likely to become a progressively smaller problem as standard solutions are provided by technology vendors. Instead, research will focus on the interface of the software with business practices. There exists a need to raise the level of abstraction further by analyzing and predicting the impact of changes at the specification level. In this research, we present a lightweight approach to identify the impact of requirement changes at the specification level. We use specification information included in use case maps to analyze the potential impact of requirement changes on a system.", "num_citations": "20\n", "authors": ["125"]}
{"title": "Ontological text mining of software documents\n", "abstract": " Documents written in natural languages constitute a major part of the software engineering lifecycle artifacts. Especially during software maintenance or reverse engineering, semantic information conveyed in these documents can provide important knowledge for the software engineer. In this paper, we present a text mining system capable of populating a software ontology with information detected in documents.", "num_citations": "19\n", "authors": ["125"]}
{"title": "Timed use case maps\n", "abstract": " Scenario-driven requirement specifications are widely used to capture and represent functional requirements. Use Case Maps are being standardized as part of the User Requirements Notation (URN), the most recent addition to ITU-T\u2019s family of languages. UCM models focus on the description of functional requirements and high-level designs at early stages of the development process. How a system is executed over time and how this may affect its correctness and performance, however, are introduced later in the development process which may require considerable changes in design or even worse at the requirement analysis level. We believe that timing aspects must be integrated into the system model, and this must be done already at an early stage of development. This paper introduces an approach to describe timing constraints in Use Case Maps specifications. We present a formal semantics of\u00a0\u2026", "num_citations": "19\n", "authors": ["125"]}
{"title": "Use Case Maps as a property specification language\n", "abstract": " Although a significant body of research in the area of formal verification and model checking tools of software and hardware systems exists, the acceptance of these tools by industry and end-users is rather limited. Beside the technical problem of state space explosion, one of the main reasons for this limited acceptance is the unfamiliarity of users with the required specification notation. Requirements have to be typically expressed as temporal logic formalisms and notations. Property specification patterns were successfully introduced to bridge this gap between users and model checking tools. They also enable non-experts to write formal specifications that can be used for automatic model checking. In this paper, we propose an abstract high level pattern-based approach to the description of property specifications based on Use Case Maps (UCM). We present a set of commonly used properties with their\u00a0\u2026", "num_citations": "18\n", "authors": ["125"]}
{"title": "A quality perspective of software evolvability using semantic analysis\n", "abstract": " Software development and maintenance are highly distributed processes that involve a multitude of supporting tools and resources. Knowledge relevant to these resources is typically dispersed over a wide range of artifacts, representation formats, and abstraction levels. In order to stay competitive, organizations are often required to assess and provide evidence that their software meets the expected requirements. In our research, we focus on assessing non-functional quality requirements, specifically evolvability, through semantic modeling of relevant software artifacts. We introduce our SE-Advisor that supports the integration of knowledge resources typically found in software ecosystems by providing a unified ontological representation. We further illustrate how our SE-Advisor takes advantage of this unified representation to support the analysis and assessment of different types of quality attributes related to the\u00a0\u2026", "num_citations": "17\n", "authors": ["125"]}
{"title": "Story-driven approach to software evolution\n", "abstract": " From a maintenance perspective, only software that is well understood can evolve in a controlled and high-quality manner. Software evolution itself is a knowledge-driven process that requires the use and integration of different knowledge resources. The authors present a formal representation of an existing process model to support the evolution of software systems by representing knowledge resources and the process model using a common representation based on ontologies and description logics. This formal representation supports the use of reasoning services across different knowledge resources, allowing for the inference of explicit and implicit relations among them. Furthermore, an interactive story metaphor is introduced to guide maintainers during their software evolution activities and to model the interactions between the users, knowledge resources and process model.", "num_citations": "17\n", "authors": ["125"]}
{"title": "An ASM operational semantics for use case maps\n", "abstract": " Scenario-driven requirement specifications are widely used to capture and represent functional requirements. Use case maps (UCM) is being standardized as part of the user requirements notation (URN), an addition to ITU-T's family of languages. UCM models allow the description of functional requirements and high-level designs at early stages of the development process. Recognizing the importance of having a well defined semantic, we propose, in this paper, a concise and rigorous formal semantics for use case maps, defined in terms of multi-agent abstract state machines. The proposed formal semantics addresses UCM's operational semantics and provides a sound basis for executing UCM specifications using simulation tools and supporting formal verification.", "num_citations": "17\n", "authors": ["125"]}
{"title": "Applying reduction techniques to software functional requirement specifications\n", "abstract": " Requirement Specification is gaining increasingly attention as a critical phase of software systems development. As requirement descriptions evolve, they quickly become error-prone and difficult to understand. Therefore, the development of techniques and tools to support requirement specification development, understanding, testing, maintenance and reuse becomes an important issue. This paper extends the well-known technique of program slicing to Functional Requirement Specification based on the Use Case Map notation. This new application of slicing, called UCM Requirement Slicing is useful to aid requirement comprehension and maintenance. In contrast to traditional program slicing, requirement slicing is designed to operate on the requirement specification of a system, rather than the source code of a program. The resulting requirement slice provides knowledge about high-level\u00a0\u2026", "num_citations": "16\n", "authors": ["125"]}
{"title": "An evaluation of timed scenario notations\n", "abstract": " There is a general consensus on the importance of good Requirements Engineering (RE) for achieving high quality software. The modeling and analysis of requirements have been the main challenges during the development of complex systems. Although semi-formal, scenario driven approaches have raised the awareness and use of requirement engineering techniques, mostly because of their intuitive representation. Scenarios are a well established approach to describe functional requirements, uncovering hidden requirements and trade-offs, as well as validating and verifying requirements.The ability to perform quantitative analysis at the requirements level supports the detection of design errors during the early stages of a software development life cycle, and helps reduce the cost of later redesign activities. In order to achieve this goal, non-functional aspects and in particular time-related aspects have to be\u00a0\u2026", "num_citations": "15\n", "authors": ["125"]}
{"title": "Formal verification of use case maps with real time extensions\n", "abstract": " Scenario-driven requirement specifications are widely used to capture and represent functional requirement. More recently, the Use Case Maps language (UCM), being standardized by ITU-T as part of the User Requirements Notation (URN) has gained on popularity within the software requirements community. UCM models focus on the description of functional and behavioral requirements as well as high-level designs at the early stages of system development processes. However, timing issues are often overlooked during the initial system design and treated as non-related behavioral issues and described therefore in separate models. We believe that timing aspects must be integrated into the system model during early development stages. In this paper, we present a novel approach to describe timing constraints in UCM specifications. We describe a formal operational semantics of Timed UCM in terms\u00a0\u2026", "num_citations": "15\n", "authors": ["125"]}
{"title": "Automatic traceability recovery: An ontological approach\n", "abstract": " Software maintainers routinely have to deal with a multitude of artifacts, like source code or documents. These artifacts often end up disconnected from each other, due to their different representations and levels of abstractions. One of the main challenges in software maintenance therefore is to recover and maintain the semantic connections among these artifacts. In this research, we present a novel approach that addresses this traceability issue by creating formal ontological representations for both software documentation and source code artifacts. The resulting representations are then aligned to establish traceability links at semantic level. Ontological queries and reasoning can be applied on these representations to infer and establish additional traceability links to support specific maintenance tasks.", "num_citations": "15\n", "authors": ["125"]}
{"title": "Predicate-based dynamic slicing of message passing programs\n", "abstract": " Program slicing is a well-known decomposition technique that transforms a large program into a smaller one that contains only statements relevant to the computation of a selected function. We present a novel predicate-based dynamic slicing algorithm for message passing programs. Unlike the more traditional slicing criteria that focus only on the parts of the program that influence a variable of interest at a specific position in the program, a predicate focuses on those parts of the program that influence the predicate. The dynamic predicate slice captures some global requirements or suspected error properties of a distributed program and computes all statements that are relevant. We present an algorithm and a sample computation to illustrate how the predicate slice can be computed. Additionally, we introduce a predicate trace to classify the relevance of statement executions based on the predicate slice. A\u00a0\u2026", "num_citations": "15\n", "authors": ["125"]}
{"title": "Quantifying developer experiences via heuristic and psychometric evaluation\n", "abstract": " A previous report identified several usability and learnability problems with integrated development environments (IDE) for Java. That report also cast these problems as examples of a conceptual gap between developer mental models and how programs are represented in IDEs. This present study extends the previous work through heuristic and psychometric assessment of problems reported by both experienced and inexperienced developers in their use of an IDE for C++. The results indicate that both groups identified similar kinds of ease-of-use problems, especially concerning program learnability and visibility (e.g., the usefulness of error and help messages). These findings are discussed in relation to other research results about developers' experiences with CASE tools and conceptual gaps between the tools and their users.", "num_citations": "15\n", "authors": ["125"]}
{"title": "Approach for solving the feature location problem by measuring the component modification impact\n", "abstract": " Maintaining a large software system is an inherently difficult task that often involves locating and comprehending system features prior to performing the actual maintenance task at hand. Feature location techniques were introduced to locate the source code components implementing specific software features. Common to these approaches is that they rely either on exercising several features of a system, and/or domain experts to guide the feature location process. In this study, the authors present a novel hybrid feature location approach that combines static and dynamic analysis techniques. Our approach uses a component dependency graph of the system to provide a ranking of the components according to their feature relevance. The ranking itself is based on the impact of a component modification on the remaining parts of a system. Our approach can almost be completely automated without requiring an\u00a0\u2026", "num_citations": "14\n", "authors": ["125"]}
{"title": "Feature interaction analysis: a maintenance perspective\n", "abstract": " Software systems have become more complex, with myriad features and multiple functionalities. A major challenge in developing and maintaining such complex software is to identify potential conflicts among its features. Feature interaction analysis becomes progressively more difficult as software's feature combinations and available scenarios increase. Software maintainers need to identify and analyze conflicts that can arise from feature modification requests. Our approach combines Use Case Maps with Formal Concept Analysis to assist maintainers in identifying feature modification impacts at the requirements level, without the need to examine the source code. We demonstrate the applicability of this approach using a teleommunication case study", "num_citations": "14\n", "authors": ["125"]}
{"title": "Traceability in software engineering\u2013past, present and future\n", "abstract": " Many changes have occurred in software engineering research and practice since 1968, when software engineering as a research domain was established. One of these research areas is traceability, a key aspect of any engineering discipline, enables engineers to understand the relations and dependencies among various artifacts in a system.", "num_citations": "14\n", "authors": ["125"]}
{"title": "Feature location using crowd-based screencasts\n", "abstract": " Crowd-based multi-media documents such as screencasts have emerged as a source for documenting requirements of agile software projects. For example, screencasts can describe buggy scenarios of a software product, or present new features in an upcoming release. Unfortunately, the binary format of videos makes traceability between the video content and other related software artifacts (eg, source code, bug reports) difficult. In this paper, we propose an LDA-based feature location approach that takes as input a set of screencasts (ie, the GUI text and/or spoken words) to establish traceability link between the features described in the screencasts and source code fragments implementing them. We report on a case study conducted on 10 WordPress screencasts, to evaluate the applicability of our approach in linking these screencasts to their relevant source code artifacts. We find that the approach is able to\u00a0\u2026", "num_citations": "13\n", "authors": ["125"]}
{"title": "Intelligent software development environments: integrating natural language processing with the eclipse platform\n", "abstract": " Software engineers need to be able to create, modify, and analyze knowledge stored in software artifacts. A significant amount of these artifacts contain natural language, like version control commit messages, source code comments, or bug reports. Integrated software development environments (IDEs) are widely used, but they are only concerned with structured software artifacts \u2013 they do not offer support for analyzing unstructured natural language and relating this knowledge with the source code. We present an integration of natural language processing capabilities into the Eclipse framework, a widely used software IDE. It allows to execute NLP analysis pipelines through the Semantic Assistants framework, a service-oriented architecture for brokering NLP services based on GATE. We demonstrate a number of semantic analysis services helpful in software engineering tasks, and evaluate one task in\u00a0\u2026", "num_citations": "13\n", "authors": ["125"]}
{"title": "Ontology-based program comprehension tool supporting website architectural evolution\n", "abstract": " A challenge of existing program comprehension approaches is to provide consistent and flexible representations for software systems. Maintainers have to match their mental models with the different representations these tools provide. In this paper, we present a novel approach that addresses this issue by providing a consistent ontological representation for both source code and documentation. The ontological representation unifies information from various sources, and therefore reduces the maintainers' comprehension efforts. In addition, representing software artifacts in a formal ontology enables maintainers to formulate hypotheses about various properties of software systems. These hypotheses can be validated through an iterative exploration of information derived by our ontology inference engine. The implementation of our approach is presented in detail, and a case study is provided to demonstrate the\u00a0\u2026", "num_citations": "12\n", "authors": ["125"]}
{"title": "An ontology-based approach for the recovery of traceability links\n", "abstract": " CiNii \u8ad6\u6587 - An Ontology-based Approach for the Recovery of Traceability Links CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec \u7248\u306e\u516c\u958b\u306b\u3064\u3044\u3066 An Ontology-based Approach for the Recovery of Traceability Links ZHANG Y. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 ZHANG Y. \u53ce\u9332\u520a\u884c\u7269 Proc. 3rd Int, Workshop on Metamodels, Schemas, Grammars, and Ontologies for Reverse Engineering, 2006 Proc. 3rd Int, Workshop on Metamodels, Schemas, Grammars, and Ontologies for Reverse Engineering, 2006, 2006 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u4ed5\u69d8\u66f8\u3068 Java \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u69cb\u9020\u306e\u985e\u4f3c\u6027\u306b\u57fa\u3065\u304f\u5bfe\u5fdc\u4ed8\u3051 , , \u2026", "num_citations": "12\n", "authors": ["125"]}
{"title": "Maximizing functional cohesion of comprehension environments by integrating user and task knowledge\n", "abstract": " Program comprehension tools should facilitate the comprehension strategies used by programmers to achieve specific tasks. Many reverse engineering tools have been developed to derive abstract representations from existing source code and to apply a variety of analysis techniques. Yet, most of these software programs fail to provide users with the necessary guidance in choosing the appropriate methods, tools, abstraction levels and analysis techniques, and they frequently expose the user to unrelated information. The author presents a task and user-centered comprehension environment that maximizes the functional cohesion among the tools and comprehension techniques by focusing on a particular user task and its appropriate comprehension strategy. At the same time, we try to minimize the data coupling for the selected task by providing only the necessary task specific information, therefore reducing the\u00a0\u2026", "num_citations": "12\n", "authors": ["125"]}
{"title": "The CONCEPT project-applying source code analysis to reduce information complexity of static and dynamic visualization techniques\n", "abstract": " The goal of software visualization is to acquire sufficient knowledge about a software system by identifying program artifacts and understanding their relationships. Graphical representations have long been recognized as having an important impact in improving the comprehension of source code. In this paper, we present several visualization techniques that we combine with analytical source code analysis to reduce the amount and, therefore, the complexity of data that has to be displayed. In particular, we focus on static and dynamic program slicing and apply this source code analysis technique on tree maps, hyperbolic trees, and UML based visualization techniques to support programmers in creating better mental models of the source code. We also introduce our CONCEPT prototype and describe how the presented approaches can be applied to reduce the information complexity for particular source code\u00a0\u2026", "num_citations": "11\n", "authors": ["125"]}
{"title": "SE-EQUAM-an evolvable quality metamodel\n", "abstract": " Quality has become a key assessment factor for organizations to determine if their software ecosystems are capable to meet constantly changing environmental factors and requirements. Many quality models exist to assess the evolvability or maintainability of software systems. Common to these models is that they, contrary to the software ecosystems they are assessing, are not evolvable or reusable. In this research, we introduce SE-EQUAM a novel ontology-based quality assessment metamodel that was designed from ground up to support model reuse and evolvability. SE-EQUAM takes advantage of Semantic Web technologies such as support for the open world assumption, incremental knowledge population, and knowledge inference. We present a case study that illustrates the reusability and evolvability of our SE-EQUAM approach.", "num_citations": "10\n", "authors": ["125"]}
{"title": "OntEQAM: A methodology for assessing evolvability as a quality factor in software ecosystems\n", "abstract": " Software development and evolution are highly distributed processes that involve a multitude of supporting tools and resources. Knowledge relevant to these resources is typically dispersed over a wide range of artifacts, representation formats, and abstraction levels. In order to stay competitive, organizations are often required to assess and provide evidence that their software meets expected quality requirements. Similarly, the quality assessment of open source or third-party components is a crucial factor in software development. In our research, we focus specifically on modeling and assessing evolvability as a quality factor. We introduce our OntEQAM methodology that supports the integration and semantic analysis of knowledge resources typically found in software ecosystems. We further illustrate how our OntEQAM methodology can be applied to support the automated analysis and assessment of different types of quality attributes related to the evolvability of software ecosystems.", "num_citations": "10\n", "authors": ["125"]}
{"title": "Enriching SE ontologies with bug report quality\n", "abstract": " Semantic web technologies have previously been applied to reduce both the abstraction and semantic gap existing among software engineering artifacts such as source code and bug reporting systems. In this research we extend the use semantic web technologies to assess the quality of bug reports stored and managed by bug trackers, such as Bugzilla or JIRA that are commonly used in both open source and commercial software development. The quality of free form bug reports has been shown to vary significantly, making the process of evaluating, classifying, and assigning bugs to programmers a difficult and time consuming task. In this research, we apply Natural Language Processing techniques to automatically assess the quality of free form bug reports and use this assessment to enrich our existing software engineering ontology to provide maintainers with semantic rich queries.", "num_citations": "10\n", "authors": ["125"]}
{"title": "Software Visualization-A Process Perspective\n", "abstract": " Software visualization is one of the enabling techniques to provide support during software maintenance activities. Software maintenance is a multidimensional problem domain which involves the integration, abstraction and analysis of different knowledge resources and artifacts. Maintainers are typically left with no guidance on how these existing artifacts, tools and knowledge should be utilized to complete a particular task. In this research, we present a novel visualization approach that integrates these artifacts in the software maintenance process chain.", "num_citations": "10\n", "authors": ["125"]}
{"title": "Assessing the quality factors found in in-line documentation written in natural language: The JavadocMiner\n", "abstract": " An important software engineering artifact used by developers and maintainers to assist in software comprehension and maintenance is source code documentation. It provides the insight needed by software engineers when performing a task, and therefore ensuring the quality of this documentation is extremely important. In-line documentation is at the forefront of explaining a programmer's original intentions for a given implementation. Since this documentation is written in natural language, ensuring its quality so far needed to be performed manually. In this paper, we present an effective and automated approach for assessing the quality of in-line documentation using a set of heuristics, targeting both the quality of language and consistency between the source code and its comments. Our evaluation is made up of two parts: We first apply the JavadocMiner tool to the different modules of two open source\u00a0\u2026", "num_citations": "9\n", "authors": ["125"]}
{"title": "An ontology-based approach to automate tagging of software artifacts\n", "abstract": " Context: Software engineering repositories contain a wealth of textual information such as source code comments, developers' discussions, commit messages and bug reports. These free form text descriptions can contain both direct and implicit references to security concerns. Goal: Derive an approach to extract security concerns from textual information that can yield several benefits, such as bug management (e.g., prioritization), bug triage or capturing zero-day attack. Method: Propose a fully automated classification and tagging approach that can extract security tags from these texts without the need for manual training data. Results: We introduce an ontology based Software Security Tagger Framework that can automatically identify and classify cybersecurity-related entities, and concepts in text of software artifacts. Conclusion: Our preliminary results indicate that the framework can successfully extract and\u00a0\u2026", "num_citations": "8\n", "authors": ["125"]}
{"title": "On mining crowd-based speech documentation\n", "abstract": " Despite the globalization of software development, relevant documentation of a project, such as requirements and design documents, often still is missing, incomplete or outdated. However, parts of that documentation can be found outside the project, where it is fragmented across hundreds of textual web documents like blog posts, email messages and forum posts, as well as multimedia documents such as screencasts and podcasts. Since dissecting and filtering multimedia information based on its relevancy to a given project is an inherently difficult task, it is necessary to provide an automated approach for mining this crowd-based documentation. In this paper, we are interested in mining the speech part of YouTube screencasts, since this part typically contains the rationale and insights of a screencast. We introduce a methodology that transcribes and analyzes the transcribed text using various Information\u00a0\u2026", "num_citations": "8\n", "authors": ["125"]}
{"title": "Beyond generated software documentation\u2014A web 2.0 perspective\n", "abstract": " Over the last decades, software engineering processes have constantly evolved to reflect cultural, social, technological, and organizational changes, which are often a direct result of the Internet. The introduction of the Web 2.0 resulted in further changes creating an interactive, community driven platform. However, these ongoing changes have yet to be reflected in the way we document software systems. Documentation generators, like Doxygen and its derivatives (Javadoc, Natural Docs, etc.) have become the de-facto industry standards for creating external technical software documentation from source code. However, the inter-woven representation of source code and documentation within a source code editor limits the ability of these approaches to provide rich media, internationalization, and interactive content. In this paper, we combine the functionality of a Web browser with a source code editor to provide\u00a0\u2026", "num_citations": "8\n", "authors": ["125"]}
{"title": "Feature location based on impact analysis\n", "abstract": " Feature location has long been recognized as an important reverse engineering activity to identify the implementation of a given system functionality in the source code. In this paper, we present a simple yet powerful approach for solving the feature location problem based on impact analysis. The presented approach combines two different sources of information: an execution trace that corresponds to the software feature under study and a static component dependency graph (CDG). Using the CDG, we rank the components invoked in the trace by measuring the impact of a component modification on the rest of the system. Our hypothesis is that the smaller the impact of a component modification, the more likely it is that the component is specific to the feature under study. A case study is presented to support the applicability of our approach", "num_citations": "8\n", "authors": ["125"]}
{"title": "Generating an NLP corpus from java source code: The SSL Javadoc Doclet\n", "abstract": " Source code contains a large amount of natural language text, particularly in the form of comments, which makes it an emerging target of text analysis techniques. Due to the mix with program code, it is difficult to process source code comments directly within NLP frameworks such as GATE. Within this work we present an effective means for generating a corpus using information found in source code and in-line documentation, by developing a custom doclet for the Javadoc tool. The generated corpus uses a schema that is easily processed by NLP applications, which allows language engineers to focus their efforts on text analysis tasks, like automatic quality control of source code comments. The SSLDoclet is available as open source software.", "num_citations": "7\n", "authors": ["125"]}
{"title": "Semantic technologies in system maintenance (stsm 2008)\n", "abstract": " This paper gives a brief overview of the international workshop on semantic technologies in system maintenance. It describes a number of semantic technologies (e.g., ontologies, text mining, and knowledge integration techniques) and identifies diverse tasks in software maintenance where the use of semantic technologies can be beneficial, such as traceability, system comprehension, software artifact analysis, and information integration.", "num_citations": "7\n", "authors": ["125"]}
{"title": "Benchmarking usability of early designs using predictive metrics\n", "abstract": " This paper introduces a set of novel metrics-based prediction models that allow stakeholders and user interface designers to assess, compare and choose among alternative designs in early development stages. Most of the existing usability evaluation methods require a fully functional prototype. Tests are also mostly conducted after the development and deployment of the software. Tests also require a specific costly lab which may result in significant maintenance costs. The proposed models in this paper reply on the correlations that we discovered between predictive usability metrics and the results of usability tests performed by users. Our empirical investigations show that predictive metrics and user-oriented tests conducted by users provide similar scores regarding the overall usability as well as other parameters such as learnability and efficiency of alternative designs before their development and deployment.", "num_citations": "7\n", "authors": ["125"]}
{"title": "MOOSE-A task-driven program comprehension environment\n", "abstract": " Many tools have been developed to derive abstract representations from existing source code. Yet, most of these tools provide only little help in providing an encompassing picture of the system under examination. Graphical visualization techniques derived from reverse engineered source code have long been recognized for their impact on improving the comprehensibility of software systems and their source code. In this paper, we present a task-oriented approach to software comprehension by introducing our MOOSE (Montreal Object-Oriented Slicing Environment) environment that provides a task-driven wizard approach that supports a cognitive comprehension model combined with reverse engineering techniques, algorithmic and visualization support. We close our discussion with a brief overview of typical software comprehension tasks and how the MOOSE environment will benefit users during these\u00a0\u2026", "num_citations": "7\n", "authors": ["125"]}
{"title": "Investigation of dynamic slicing and its application in program comprehension\n", "abstract": " The objective of this thesis is to investigate dynamic slicing and its application in program comprehension. The process of program comprehension can become aggravating in software maintenance and debugging because programmers usually debug someone else's programs and often, they only poorly or partially understand these programs. As part of the dynamic slice computation, different types of information are determined and usually discarded after the slice computation.", "num_citations": "7\n", "authors": ["125"]}
{"title": "A context-driven software comprehension process model\n", "abstract": " Comprehension is an essential part of software evolution. Only software that is well understood can evolve in a controlled manner. In this paper, we present a formal process model to support the comprehension of software systems by using ontology and description logic. This formal representation supports the use of reasoning services across different knowledge resources and therefore, enables us to provide users with guidance during the comprehension process that is context sensitive to their particular comprehension task. As part of the process model, we also adopt a new interactive story metaphor, to represent the interactions between users and the comprehension process", "num_citations": "6\n", "authors": ["125"]}
{"title": "Software clustering based on behavioural features\n", "abstract": " Understanding a large software system can be made easier if the system is decomposed into smaller and more manageable clusters; software engineers can focus on analyzing only the subsystems needed to solve the maintenance task at hand. There exist several software clustering techniques, among which the most predominant ones are based on the analysis of the source code. However, due to the increasing complexity of software, we argue that this structural clustering is no longer sufficient. In this paper, we present a novel clustering approach based on dynamic analysis. The technique is based on measuring the similarity between the components of the system under study according to the number of software features they implement. A discussion on why software features can be a good candidate clustering criterion is presented. We also present a preliminary case study that supports the applicability of the\u00a0\u2026", "num_citations": "5\n", "authors": ["125"]}
{"title": "Context driven slicing based coupling measures\n", "abstract": " We present a framework of program slicing based coupling measurements to evaluate software quality. The proposed framework combines the well-known coupling measurement, CBO (coupling between object classes), RFC (response from classes), and MPC (message passing coupling), with slicing based source code analysis. The proposed measures are implemented in our CONCEPT tool, and an initial experimental analysis has been performed to illustrate the applicability of our measurements.", "num_citations": "5\n", "authors": ["125"]}
{"title": "Consolidating the ISO usability models\n", "abstract": " In recent years software usability has become a major research theme in the software engineering community. Up to now, only a few software quality models address the usability evaluation and measurement in a detailed and structured way. The International Organization for Standardization (ISO) developed a variety of models to measure software usability, but none of these models cover all usability aspects. Furthermore, they are not part of the current software engineering practices and no tool exists to support it.", "num_citations": "5\n", "authors": ["125"]}
{"title": "Analyzing and predicting software quality trends using financial patterns\n", "abstract": " The financial community assesses and analyzes fundamental qualities of stocks to predict their future performance. During the analysis different external and internal factors are considered which can affect the stock price. Financial analysts use indicators and analysis patterns, such as such as Moving Averages, Crossover patterns, and M-Top/W-Bottom patterns to determine stock price trends and potential trading opportunities. Similar to the stock market, also qualities of software systems are part of larger ecosystems which are affected by internal and external factors. Our research provides a cross disciplinary approach which takes advantages of these financial indicators and analysis patterns and re-applies them for the analysis and prediction of evolvability qualities in software system. We conducted several case studies to illustrate the applicability of our approach.", "num_citations": "4\n", "authors": ["125"]}
{"title": "Oasis: Opening-up architectures of software-intensive systems\n", "abstract": " Opening-up architectures of software-intensive systems includes, as a key element, reverse engineering software, up to a simple component level. This paper introduces the Oasis project, which aims at decreasing comprehension time of existing systems to be used in a system-of-systems. The problem to be addressed and the vision are presented and current tool deficiencies are described.Descriptors:", "num_citations": "4\n", "authors": ["125"]}
{"title": "When open source turns cold on innovation\u2014The challenges of navigating licensing complexities in new research domains\n", "abstract": " In this poster, we review the limitations open source licences introduce to the application of Linked Data in Software Engineering. We investigate whether open source licences support special requirements to publish source code as Linked Data on the Internet.", "num_citations": "3\n", "authors": ["125"]}
{"title": "Reasoning about global clones: Scalable semantic clone detection\n", "abstract": " The Semantic Web is slowly transforming the Web as we know it into a machine understandable pool of information that can be consumed and reasoned about by various clients. Source code is no exception to this trend and various communities have proposed standards to share code as linked data. With the availability of large amounts of open source code published in publicly accessible repositories, the introduction of massive horizontal scaling frameworks, and cloud computing infrastructures, a new era of software mining across information silos is reshaping the software engineering landscape. Given these technological advances, analyzing code at a global scale, across systems, projects and organizational boundaries, becomes feasible. In this paper, we introduce a clone detection algorithm and its implementation that can scale to such large global datasets, by modeling clones using description logic and\u00a0\u2026", "num_citations": "3\n", "authors": ["125"]}
{"title": "Exploring the correlation between predictive usability measures and user tests\n", "abstract": " For software products to succeed they have to satisfy customer expectations, including usability aspects of the software. Early during the development cycle important design decisions are made for projects that are typically on restricted schedules and budgets. There is a need to support this decision making process, by empowering the decision makers with better tools and techniques. In this research we present a prediction approach that allows potential stakeholders to examine and choose among different user interfaces based on our usability prediction model. The usability predication model establishes correlations between predictive usability metrics and the results of usability tests performed by users. Based on our case studies we applied a multiple regression method to find relationships between response usability quality attributes and explanatory usability metrics.", "num_citations": "3\n", "authors": ["125"]}
{"title": "A feature location approach for mapping application features extracted from crowd-based screencasts to source code\n", "abstract": " Crowd-based multimedia documents such as screencasts have emerged as a source for documenting requirements, the workflow and implementation issues of open source and agile software projects. For example, users can show and narrate how they manipulate an application\u2019s GUI to perform a certain functionality, or a bug reporter could visually explain how to trigger a bug or a security vulnerability. Unfortunately, the streaming nature of programming screencasts and their binary format limit how developers can interact with a screencast\u2019s content. In this research, we present an automated approach for mining and linking the multimedia content found in screencasts to their relevant software artifacts and, more specifically, to source code. We apply LDA-based mining approaches that take as input a set of screencast artifacts, such as GUI text and spoken word, to make the screencast content accessible and\u00a0\u2026", "num_citations": "2\n", "authors": ["125"]}
{"title": "Semantic modeling approach for software vulnerabilities data sources\n", "abstract": " Data sources describing software security vulnerabilities are commonly used by software engineers not only increase the security of software systems but also enhance software productivity and reduce maintenance costs. However, with the constantly growing amount of available security vulnerability information and this information being spread across heterogeneous resources, software developers are struggling in taking full advantage of these resources. The Semantic Web and its supporting technology stack have been widely promoted to support the modeling, reuse and interoperability among heterogeneous data sources. In our research we present a Semantic Web enabled knowledge model which provides a formal and semi-automated approach for unifying vulnerability information resources. As part of this knowledge modeling approach, we also take advantage of Formal Concept Analysis (FCA) to identify\u00a0\u2026", "num_citations": "2\n", "authors": ["125"]}
{"title": "Modification analysis support at the requirements level\n", "abstract": " Modification analysis is part of most maintenance processes and includes among other activities, early prediction of potential change impacts, feasibility studies, cost estimation, etc. Existing impact analysis and regression testing techniques being source code based require at least some understanding of the system implementation. In this research we present a novel approach that combines UCM with FCA to assist decision makers in supporting modification analysis at the requirements level. Our approach provides support for determining the potential modification and re-testing effort associated with a change without the need to analyze or comprehend source code. We demonstrate the applicability of our approach on a telephony system case study.", "num_citations": "2\n", "authors": ["125"]}
{"title": "Applying code analysis and 3D design pattern grouping to facilitate program comprehension\n", "abstract": " The increasing size and complexity of software systems introduces new challenges in comprehending the overall structure of programs. Modeling languages and notations were introduced to provide abstractions from existing source code during forward engineering. However, these same modeling techniques and notations fail during source code reverse engineering due to: (1) Information overload; and (2) the existence of a conceptual gap between the abstractions derived during forward and reverse engineered. Our tool uses a 3D representation for UML in combination with source code analysis to facilitate the comprehension process. We also address issues of crosscutting, navigation, and the use of animation to visualize design patterns", "num_citations": "2\n", "authors": ["125"]}
{"title": "Position paper: challenges in visualizing and reconstructing architectural views\n", "abstract": " A common approach to cope with software architecture comprehension is to provide higher levels of abstraction of lower level system information. Architectural recovery tools provide such high-level views by extracting and abstracting a subset of the software entities. In this research we are focusing on challenges in visualizing and reconstructing architectural views. In particular we are looking into issues related to the applicability of current visualization representations generated by architectural recovery tools to support views and products specified by the C4ISR architecture framework.", "num_citations": "2\n", "authors": ["125"]}
{"title": "A contextual guidance approach to software security\n", "abstract": " With the ongoing trend towards the globalization of software systems and their development, components in these systems might not only work together, but may end up evolving independently from each other. Modern IDEs have started to incorporate support for these highly distributed environments, by adding new collaborative features. As a result, assessing and controlling system quality (e.g. security concerns) during system evolution in these highly distributed systems become a major challenge. In this research, we introduce a unified ontological representation that integrates best security practices in a context-aware tool implementation. As part of our approach, we integrate information from traditional static source code analysis with semantic rich structural information in a unified ontological representation. We illustrate through several use cases how our approach can support the evolvability of software\u00a0\u2026", "num_citations": "1\n", "authors": ["125"]}
{"title": "State of the art report on component substitution\n", "abstract": " The problem of managing the evolution of complex and large software systems is well known. Organizations are under tremendous pressure to evolve their existing systems to better respond to marketplace needs and rapidly changing technologies. This constant pressure to evolve these systems is driven by every growing expectations of the customer for new enterprise standards, new products and system features, and improved performance. Evolution is therefore required to cope with new software releases and manage hardware and to avoid software obsolescence.Component substitution can be seen as a specific instance of a software maintenance activity. It typically involves the comprehension of an existing system, the identification of the scope of the modification, conflict identification, component adaptation, and implementation of connections between components. The challenges for a maintainer start with the identification of the scope of a component. This is particularly true in the context of a large system and/or a system the maintainer is not familiar with", "num_citations": "1\n", "authors": ["125"]}