{"title": "Effect of test set minimization on fault detection effectiveness\n", "abstract": " Given a test set T to test a program P, there are at least two attributes of T that determine its fault detection effectiveness. One attribute is the size of T measured as the number of test cases in T. Another attribute is the code coverage measured when P is executed on all elements of T. The fault detection effectiveness of T is the ratio of the number of faults guaranteed to result in program failure when P is executed on T to the total number of faults present in P. An empirical study was conducted to determine the relative importance of the size and coverage attributes in affecting the fault detection effectiveness of a randomly selected test set for some program P. Results from this study indicate that as the size of a test set is reduced, while the code coverage is kept constant, there is little or no reduction in the fault detection effectiveness of the new test set so generated. For the study reported, of the two attributes mentioned\u00a0\u2026", "num_citations": "510\n", "authors": ["309"]}
{"title": "A survey of malware detection techniques\n", "abstract": " Malware is a worldwide epidemic. Studies suggest that the impact of malware is getting worse. Malware detectors are the primary tools in defense against malware. The quality of such a detector is determined by the techniques it uses. It is therefore imperative that we study malware detection techniques and understand their strengths and limitations. This survey examines 45 malware detection techniques and offers an opportunity to compare them against one another aiding in the decision making process involved with developing a secure application/system. The survey also provides a comprehensive bibliography as an aid to researchers in malware detection.", "num_citations": "509\n", "authors": ["309"]}
{"title": "Design of mutant operators for the C programming language\n", "abstract": " This report documents the classi cation, de nition, rationale, and semantics of the mutant operators designed for the (proposed) ANSI C programming language Kern88]. The design of mutant operators was carried out by the authors of this report over a six month period starting August, 1988.Throughout the report, the following conventions are used:(a) keywords in C are emboldened,(ii) non-terminal symbols from the C grammar are italicized,(iii) mutant operators are emboldened and appear in upper case letters,(iv) mutant operator category names are emboldened and begin with an upper case letter followed by three lower case letters, and (v) P denotes the program under test and hence the program to be mutated. In general, P will consist of several functions that could be mutated individually or as a group depending on how much of P is being tested. This report assumes that the reader is familiar with the underlying theory and techniques of mutation analysis. However, the next section provides an overview of mutation analysis. DeMi79] and DeMi87] contain the in depth background material. The remainder of the report is organized into 13 sections. The next section provides an overview of mutation analysis. Section 3 enumerates the reasons responsible for the mutant operators reported here. Section 4 presents a classi cation of the mutant operators. We expect such a classi cation to be useful for the tool designer1 and for the software tester. Section 5 consolidates the naming conventions used while naming the large number of mutant operators. The conventions are designed to simplify the expansion of the four letter mnemonics for all\u00a0\u2026", "num_citations": "378\n", "authors": ["309"]}
{"title": "Interface mutation: An approach for integration testing\n", "abstract": " The need for test adequacy criteria is widely recognized. Several criteria have been proposed for the assessment of adequacy of tests at the unit level. However, there remains a lack of criteria for the assessment of the adequacy of tests generated during integration testing. We present a mutation based interprocedural criterion, named Interface Mutation (IM), suitable for use during integration testing. A case study to evaluate the proposed criterion is reported. In the study, the UNIX sort utility was seeded with errors and Interface Mutation evaluated by measuring the cost of its application and its error revealing effectiveness. Alternative IM criteria using different sets of Interface Mutation operators were also evaluated. While comparing the error revealing effectiveness of these Interface Mutation-based test sets with same size randomly generated test sets, we observed that in most cases Interface Mutation based test\u00a0\u2026", "num_citations": "342\n", "authors": ["309"]}
{"title": "On the estimation of reliability of a software system using reliabilities of its components\n", "abstract": " We report an experiment to evaluate a method, known as component based reliability estimation (CBRE), for the estimation of reliability of a software system using reliabilities of its components. CBRE involves computing path reliability estimates based on the sequence of components executed for each test input. Path reliability estimates are averaged over all test runs to obtain an estimate of the system reliability. In the experiment reported, three components of a Unix utility were seeded with errors and the reliability of each component was measured. The faulty components were then introduced systematically into the utility, in various combinations, to produce several faulty versions of the utility. For each faulty version, test cases were drawn from an operational profile to measure the component-based reliability. The true reliability of the faulty version was estimated using the frequency count approach. The\u00a0\u2026", "num_citations": "287\n", "authors": ["309"]}
{"title": "Reducing the cost of mutation testing: An empirical study\n", "abstract": " Of the various testing strategies, mutation testing has been empirically found to be effective in detecting faults. However, mutation often imposes unacceptable demands on computing and human resources because of the large number of mutants that need to be compiled and executed on one or more test cases. In addition, the tester needs to examine many mutants and analyze these for possible equivalence with the program under test. For these reasons, mutation is generally regarded as too expensive to use. Because one significant component of the cost of mutation is the execution of mutants against test cases, we believe that this cost can be reduced dramatically by reducing the number of mutants that need to be examined. We report results from a case study designed to investigate two alternatives for reducing the cost of mutation. The alternatives considered are randomly selected x% mutation and\u00a0\u2026", "num_citations": "287\n", "authors": ["309"]}
{"title": "Proteum-A tool for the assessment of test adequacy for C programs: User\u2019s guide\n", "abstract": " This technical report presents the main features of Proteum (Program Testing Using Mutants), a testing tool that supports Mutation Analysis criterion. Proteum can be configured for testing programs in many procedural programming languages. This guide reports the version 1.1-C that works with the C language on SUN workstations, under OPENWINDOWS environment. Proteum has been developed at University of S\u00e3o Paulo (USP), S\u00e3o Carlos, SP, Brazil [DEL93] and used in teaching and researching activities at USP and at SERC/Purdue University.", "num_citations": "281\n", "authors": ["309"]}
{"title": "Automated test data generation using an iterative relaxation method\n", "abstract": " An important problem that arises in path oriented testing is the generation of test data that causes a program to follow a given path. In this paper, we present a novel program execution based approach using an iterative relaxation method to address the above problem. In this method, test data generation is initiated with an arbitrarily chosen input from a given domain. This input is then iteratively refined to obtain an input on which all the branch predicates on the given path evaluate to the desired outcome. In each iteration the program statements relevant to the evaluation of each branch predicate on the path are executed, and a set of linear constraints is derived. The constraints are then solved to obtain the increments for the input. These increments are added to the current input to obtain the input for the next iteration. The relaxation technique used in deriving the constraints provides feedback on the amount by\u00a0\u2026", "num_citations": "270\n", "authors": ["309"]}
{"title": "Test set size minimization and fault detection effectiveness: A case study in a space application\n", "abstract": " An important question in software testing is whether it is reasonable to apply coverage based criteria as a filter to reduce the size of a test set. An empirical study was conducted using a test set minimization technique to explore the effect of reducing the size of a test set, while keeping block coverage constant, on the fault detection strength of the resulting minimized test set. Two types of test sets were examined. For those with respect to a fixed size, no test case screening was conducted during the generation, whereas for those with respect to a fixed coverage, each subsequent test case had to improve the overall coverage in order to be included. The study reveals that no matter how a test set is generated (with or without any test case screening) block minimized test sets have a size/effectiveness advantage, in terms of a significant reduction in test set size but with almost the same fault detection effectiveness, over\u00a0\u2026", "num_citations": "233\n", "authors": ["309"]}
{"title": "A dataset to support research in the design of secure water treatment systems\n", "abstract": " This paper presents a dataset to support research in the design of secure Cyber Physical Systems (CPS). The data collection process was implemented on a six-stage Secure Water Treatment (SWaT) testbed. SWaT represents a scaled down version of a real-world industrial water treatment plant producing 5\u00a0gallons per minute of water filtered via membrane based ultrafiltration and reverse osmosis units. This plant allowed data collection under two behavioral modes: normal and attacked. SWaT was run non-stop from its \u201cempty\u201d state to fully operational state for a total of 11-days. During this period, the first seven days the system operated normally i.e. without any attacks or faults. During the remaining days certain cyber and physical attacks were launched on SWaT while data collection continued. The dataset reported here contains the physical properties related to the plant and the water treatment process\u00a0\u2026", "num_citations": "208\n", "authors": ["309"]}
{"title": "Performance, effectiveness, and reliability issues in software testing\n", "abstract": " The author has identified two problems that need to be overcome in order that some of the powerful testing techniques be used in practice: performance and effectiveness. The testing methods referred to are dataflow and mutation testing.<>", "num_citations": "206\n", "authors": ["309"]}
{"title": "SWaT: A water treatment testbed for research and training on ICS security\n", "abstract": " This paper presents the SWaT testbed, a modern industrial control system (ICS) for security research and training. SWaT is currently in use to (a) understand the impact of cyber and physical attacks on a water treatment system, (b) assess the effectiveness of attack detection algorithms, (c) assess the effectiveness of defense mechanisms when the system is under attack, and (d) understand the cascading effects of failures in one ICS on another dependent ICS. SWaT consists of a 6-stage water treatment process, each stage is autonomously controlled by a local PLC. The local fieldbus communications between sensors, actuators, and PLCs is realized through alternative wired and wireless channels. While the experience with the testbed indicates its value in conducting research in an active and realistic environment, it also points to design limitations that make it difficult for system identification and attack detection in\u00a0\u2026", "num_citations": "193\n", "authors": ["309"]}
{"title": "Generating test data for branch coverage\n", "abstract": " Branch coverage is an important criteria used during the structural testing of programs. We present a new program execution based approach to generate input data that exercises a selected branch in a program. The test data generation is initiated with an arbitrarily chosen input from the input domain of the program. A new input is derived from the initial input in an attempt to force execution through any of the paths through the selected branch. The method dynamically switches among the paths that reach the branch by refining the input. Using a numerical iterative technique that attempts to generate an input to exercise the branch, it dynamically selects a path that offers less resistance. We have implemented the technique and present experimental results of its performance for some programs. Our results show that our method is feasible and practical.", "num_citations": "172\n", "authors": ["309"]}
{"title": "Effect of test set size and block coverage on the fault detection effectiveness\n", "abstract": " Size and code coverage are two important attributes that characterize a set of tests. When a program P is executed on elements of a test set T, we can observe the fault-detecting capacity of T for P. We can also observe the degree to which T induces code coverage on P according to some coverage criterion. We would like to know whether it is the size of T or the coverage of T on P which determines the fault detection effectiveness (FDE) of T for P. We found that there is little or no reduction in the FDE of a test set when its size is reduced while the all-uses coverage is kept constant. These data suggest, indirectly, that coverage is more correlated than the size with the FDE. To further investigate this suggestion, we report an empirical study to compare the statistical correlation between (1) FDE and coverage, and (2) FDE and the size. Results from our experiments indicate that the correlation between FDE and block\u00a0\u2026", "num_citations": "126\n", "authors": ["309"]}
{"title": "An overview of software cybernetics\n", "abstract": " Software cybernetics explores the interplay between software and control and is motivated by the fundamental question whether or not and how software behavior can be controlled. In this paper, we formulate the underlying motivations and ideas of software cybernetics and review various existing research topics in this emerging area, including feedback mechanisms in software processes, bisimulation and controllability, adaptive software, software synthesis, software test process control, and adaptive testing. We identify software rejuvenation and performance control, software fault-tolerance, logical foundation for control systems, and communication complexity for control systems as potential research topics. Several on-going research projects are also summarized.", "num_citations": "125\n", "authors": ["309"]}
{"title": "A formal model of the software test process\n", "abstract": " A novel approach to model the system test phase of the software life cycle is presented. This approach is based on concepts and techniques from control theory and is useful in computing the effort required to reduce the number of errors and the schedule slippage under a changing process environment. Results from these computations are used, and possibly revised, at specific checkpoints in a feedback-control structure to meet the schedule and quality objectives. Two case studies were conducted to study the behavior of the proposed model. One study reported here uses data from a commercial project. The outcome from these two studies suggests that the proposed model might well be the first significant milestone along the road to a formal and practical theory of software process control.", "num_citations": "124\n", "authors": ["309"]}
{"title": "Testing for software vulnerability using environment perturbation\n", "abstract": " We describe a methodology for testing a software system for possible security flaws. Based on the observation that most security flaws are caused by the program's inappropriate interactions with the environment, and are triggered by a user's malicious perturbation on the environment (which we call an environment fault), we view the security testing problem as the problem of testing for the fault\u2010tolerance properties of a software system. We consider each environment perturbation as a fault and the resulting security compromise a failure in the toleration of such faults. Our approach is based on the well\u2010known technique of fault injection. Environment faults are injected into the system under test and system behavior observed. The failure to tolerate faults is an indicator of a potential security flaw in the system. An Environment\u2010Application Interaction (EAI) fault model is proposed which guides us to decide what faults\u00a0\u2026", "num_citations": "104\n", "authors": ["309"]}
{"title": "Issues in testing distributed component-based systems\n", "abstract": " Issues in testing distributed component-based systems are discussed. Di erences in testing such systems and other systems are identi ed. Several limitations and shortcomings of the existing test methodologies are also identi ed and a new methodology proposed.", "num_citations": "100\n", "authors": ["309"]}
{"title": "Discriminative models of integrating document evidence and document-candidate associations for expert search\n", "abstract": " Generative models such as statistical language modeling have been widely studied in the task of expert search to model the relationship between experts and their expertise indicated in supporting documents. On the other hand, discriminative models have received little attention in expert search research, although they have been shown to outperform generative models in many other information retrieval and machine learning applications. In this paper, we propose a principled relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Compared with the state-of-the-art language models for expert search, the proposed research can naturally integrate various document evidence and document-candidate associations into a single model without extra modeling assumptions or effort. An extensive set of experiments have been conducted on\u00a0\u2026", "num_citations": "97\n", "authors": ["309"]}
{"title": "Compiler-integrated program mutation\n", "abstract": " A method for integrating support for program mutation into a compiler is presented. The method is both efficient and sufficiently powerful to support program mutation software testing. Moreover, existing research suggests that this approach appears to be essential for the cost-effective application of program mutation to testing large commercial software systems. It is believed that a compiler-integrated approach will provide a significant increase in the efficiency of several existing testing tools and allow program mutation to be effectively used to test commercial software systems.<>", "num_citations": "95\n", "authors": ["309"]}
{"title": "Software testing and reliability\n", "abstract": " It is believed that there is an important relationship between the estimation of reliability of a program, its structure, and the amount oftesting it has been subjected to. Though one can imagine several ways of quantifying the amount of testing, we consider one or more measures of code coverage as possible quantifiers. Statement coverage, decision coverage, and data flow coverage are some of the code coverage measures. These measures are based on the structure, often detailed, of the software. Several software reliability theorists observe that the structure of the software should be closely followed in the analysis of reliability.[Broc90] suggests:\u201cAt some future time it may be possible to match a reliability model to a program via the characteristics of that program, or even of the software development methodology used.\u201d[Musa87] has suggested a similar possibility. The importance of distributing testing according to a\u00a0\u2026", "num_citations": "92\n", "authors": ["309"]}
{"title": "Mutation versus all-uses: An empirical evaluation of cost, strength and effectiveness\n", "abstract": " Although mutation has been empirically found to be effective in detecting faults, it often imposes unacceptable demands on computing and human resources because of the large number of mutants that need to be compiled and executed on one or more test cases. We designed a case study to explore two alternatives of mutation to reduce its cost without significantly deteriorating its strength and effectiveness. The alternatives are (1) constrained abs/ror mutation which examines only the abs and ror mutants and ignores the others, and (2) randomly selected 10% mutation which examines only 10% of the randomly selected mutants of each mutation operator in Mothra. Data collected during experimentation have shown that both alternatives provide a significant cost reduction in terms of the number of test cases required to satisfy an adequacy criterion and the number of mutants to be examined. Such gain is\u00a0\u2026", "num_citations": "91\n", "authors": ["309"]}
{"title": "The Mothra tool set (software testing)\n", "abstract": " Mothra is a software test environment that supports mutation-based testing of software systems. Mutation analysis is a powerful software testing technique that evaluates the adequacy of test data based on its ability to differentiate between the program under test and its mutants, where mutants are constructed by inserting single, simple errors into the program under test. This evaluation process also provides guidance in the creation of new test cases to provide more adequate testing. Mothra consists of a collection of individual tools, each of which implements a separate, independent function for the testing system. The initial Mothra tool set, for the most part, duplicates functionality existing in previous mutation analysis systems. Current efforts are concentrated on extending this basic tool set to include capabilities previously unavailable to the software testing community. The authors describe Mothra tool set and\u00a0\u2026", "num_citations": "88\n", "authors": ["309"]}
{"title": "WADI: a water distribution testbed for research in the design of secure cyber physical systems\n", "abstract": " The architecture of a water distribution testbed (WADI), and on-going research in the design of secure water distribution system is presented. WADI consists of three stages controlled by Programmable Logic Controllers (PLCs) and two stages controlled via Remote Terminal Units (RTUs). Each PLC and RTU uses sensors to estimate the system state and the actuators to effect control. WADI is currently used to (a) conduct security analysis for water distribution networks,(b) experimentally assess detection mechanisms for potential cyber and physical attacks, and (c) understand how the impact of an attack on one CPS could cascade to other connected CPSs. The cascading effects of attacks can be studied in WADI through its connection to two other testbeds, namely for water treatment and power generation and distribution.", "num_citations": "85\n", "authors": ["309"]}
{"title": "High-performance mutation testing\n", "abstract": " Testing a large software program is a time consuming operation. In addition to the time spent by the tester in identifying, locating, and correcting bugs, a significant amount of time is spent in the execution of the program under test and its instrumented or fault-induced variants, also known as mutants. When using mutation testing to achieve high reliability, there can be many such mutants. In this article, we show how a multiple instruction multiple data (MIMD) architecture can be exploited to obtain significant reductions in the total execution time of the mutants. We describe the architecture of the PM othra system, which is designed to provide the tester with a transparent interface to a parallel machine. Experimental results obtained on the Ncube/7 hypercube are presented. The near-linear speedups show the perfect match that exists between the software testing application and a local memory MIMD architecture\u00a0\u2026", "num_citations": "85\n", "authors": ["309"]}
{"title": "High performance software testing on SIMD machines\n", "abstract": " Mutant unification, a new method for high-performance software testing, supports program mutation on parallel machines based on the Single Instruction Multiple Data (SIMD) stream paradigm. Parameters that affect the performance of unification are identified, and their effect on the time to completion of a mutation test cycle and speedup are studied. Program mutation analysis provides an effective means for determining the reliability of large software systems. It also provides a systematic method for measuring the adequacy of test data. Testing large software systems with mutation on traditional sequential machines is computation bound and prohibitive. The proposed unification method offers a practical alternative to current approaches, which are unacceptably slow and only suitable for testing relatively small programs. It also opens up a new application domain for SIMD machines.", "num_citations": "84\n", "authors": ["309"]}
{"title": "On the correlation between code coverage and software reliability\n", "abstract": " We report experiments conducted to investigate the correlation between code coverage and software reliability. Black-, decision-, and all-use-coverage measures were used. Reliability was estimated to be the probability of no failure over the given input domain defined by an operational profile. Four of the five programs were selected from a set of Unix utilities. These utilities range in size from 121 to 8857 lines of code, artificial faults were seeded manually using a fault seeding algorithm. Test data was generated randomly using a variety of operational profiles for each program. One program was selected from a suite of outer space applications. Faults seeded into this program were obtained from the faults discovered during the integration testing phase of the application. Test cases were generated randomly using the operational profile for the space application. Data obtained was graphed and analyzed to observe\u00a0\u2026", "num_citations": "83\n", "authors": ["309"]}
{"title": "An investigation into the response of a water treatment system to cyber attacks\n", "abstract": " An experimental investigation was undertaken to understand the impact of single-point cyber attacks on a Secure Water Treatment (SWaT) system. Cyber attacks were launched on SWaT through its SCADA server that connects to the Programmable Logic Controllers (PLCs) that in turn are connected to sensors and actuators. Attacks were designed to meet attacker objectives selected from a novel attacker model. Outcome of the experiments led to a better understanding of (a) the propagation of an attack across the system measured in terms of the number of components affected and (b) the behavior of the water treatment process in SWaT in response to the attacks. The observed response to various attacks was then used to propose attack detection mechanisms based on various physical properties measured during the treatment process.", "num_citations": "78\n", "authors": ["309"]}
{"title": "Fault detection effectiveness of mutation and data flow testing\n", "abstract": " We report results from an experiment to compare the fault detection effectiveness of mutation, its variants and the all-uses data flow criteria. Adequate test sets were generated randomly, as opposed to by human testers as in some previous studies. We view our results in the light of those from earlier studies comparing mutation with path-oriented testing strategies. We identify and discuss factors that one might consider while evaluating an adequacy criterion for use in practice. Results from our experiments strengthen a hypothesis that an adequacy criterion based on one of the two variants of mutation has superior fault detection effectiveness than that of the all-uses criterion.", "num_citations": "77\n", "authors": ["309"]}
{"title": "Assessing testing tools in research and education\n", "abstract": " An evaluation of three software engineering tools based on their use in research and educational environments is presented. The three testing tools are Mothra, a mutation-testing tool, Asset, a dataflow testing tool, and ATAC, a dataflow testing tool. Asset, ATAC, and Mothra were used in research projects that examined relative and general fault-detection effectiveness of testing methods, how good a test set is after functional testing based on program specification, how reliability estimates from existing models vary with the testing method used, and how improved coverage affects reliability. Students used ATAC and Mothra by treating the tools as artifacts and studying them from the point of view of documentation, coding style, and possible enhancements, solving simple problems given during testing lectures, and conducting experiments that supported ongoing research in software testing and reliability. The\u00a0\u2026", "num_citations": "76\n", "authors": ["309"]}
{"title": "Distributed detection of single-stage multipoint cyber attacks in a water treatment plant\n", "abstract": " A distributed detection method is proposed to detect single stage multi-point (SSMP) attacks on a Cyber Physical System (CPS). Such attacks aim at compromising two or more sensors or actuators at any one stage of a CPS and could totally compromise a controller and prevent it from detecting the attack. However, as demonstrated in this work, using the flow properties of water from one stage to the other, a neighboring controller was found effective in detecting such attacks. The method is based on physical invariants derived for each stage of the CPS from its design. The attack detection effectiveness of the method was evaluated experimentally against an operational water treatment testbed containing 42 sensors and actuators. Results from the experiments point to high effectiveness of the method in detecting a variety of SSMP attacks but also point to its limitations. Distributing the attack detection code among\u00a0\u2026", "num_citations": "75\n", "authors": ["309"]}
{"title": "Interface mutation\n", "abstract": " Applications that utilize a broker\u2010based architecture are often composed of components that need to be tested individually and in combination. Furthermore, adequacy assessment of tests of components is useful in that it assists testers in identifying weaknesses in the tests generated so far and in offering hints on what the new tests must be. Traditional test adequacy criteria have limitations for commercial use, especially when tests for large components are to be assessed for their adequacy. This paper describes a test adequacy criterion based on interface mutation and a method, based on the criterion, to test components. This method requires the mutation of elements only from within a component's interface and not from within the code that implements the interface. The adequacy criterion based on interface mutation was evaluated empirically and compared with coverage criteria based on control flow for its\u00a0\u2026", "num_citations": "75\n", "authors": ["309"]}
{"title": "TABOR: A graphical model-based approach for anomaly detection in industrial control systems\n", "abstract": " Industrial Control Systems (ICS) such as water and power are critical to any society. Process anomaly detection mechanisms have been proposed to protect such systems to minimize the risk of damage or loss of resources. In this paper, a graphical model-based approach is proposed for profiling normal operational behavior of an operational ICS referred to as SWaT (Secure Water Treatment). Timed automata are learned as a model of regular behaviors shown in sensors signal like fluctuations of water level in tanks. Bayesian networks are learned to discover dependencies between sensors and actuators. The models are used as a one-class classifier for process anomaly detection, recognizing irregular behavioral patterns and dependencies. The detection results can be interpreted and the abnormal sensors or actuators localized due to the interpretability of the graphical models. This approach is applied to a\u00a0\u2026", "num_citations": "74\n", "authors": ["309"]}
{"title": "Aligning cyber-physical system safety and security\n", "abstract": " Safety and security are two key properties of Cyber-Physical Systems (CPS). Safety is aimed at protecting the systems from accidental failures in order to avoid hazards, while security is focused on protecting the systems from intentional attacks. They share identical goals - protecting CPS from failing. When aligned within a CPS, safety and security work well together in providing a solid foundation of an invincible CPS, while weak alignment may produce inefficient development and partially-protected systems. The need of such alignment has been recognized by the research community, the industry, as well as the International Society of Automation (ISA), which identified a need of alignment between safety and security standards ISA84 (IEC 61511) and ISA99 (IEC 62443). We propose an approach for aligning CPS safety and security at early development phases by synchronizing safety and security\u00a0\u2026", "num_citations": "72\n", "authors": ["309"]}
{"title": "Categorization of software errors that led to security breaches\n", "abstract": " A set of errors known to have led to security breaches in computer systems was analyzed. The analysis led to a categorization of these errors. After examining several proposed schemes for the categorization of software errors a new scheme was developed and used. This scheme classi es errors by their cause, the nature of their impact, and the type of change, or x, made to remove the error. The errors considered in this work are found in a database maintained by the COAST laboratory. The categorization is the rst step in the investigation of the e ectiveness of various measures of code coverage in revealing software errors that might lead to security breaches.", "num_citations": "72\n", "authors": ["309"]}
{"title": "A grammar based fault classification scheme and its application to the classification of the errors of TEX\n", "abstract": " We present a novel scheme for categorizing coding faults. Our grammar based scheme uses the notion of syntactic transformers and is automatable. The classi cation that results from our scheme can be used by researchers investigating the e ectiveness of software testing techniques. In these respects our scheme is signi cantly di erent from several proposed in the past by other researchers. We have used it to categorize the ten year log of errors of TEX reported by Knuth. For each fault classi ed, we also provide, wherever possible, the precise substring that constitutes the fault. The entire error log and the associated program is in public domain and hence our categorization can be veri ed. We also provide a fault classi cation algorithm that uses a top-down strategy to nd the di erences between two parse trees, annotated with syntactic transformers, to classify various faults. We claim that such an algorithm can be integrated within a software development environment and used as a low cost mechanism for monitoring and classifying faults.", "num_citations": "71\n", "authors": ["309"]}
{"title": "Noise matters: Using sensor and process noise fingerprint to detect stealthy cyber attacks and authenticate sensors in cps\n", "abstract": " A novel scheme is proposed to authenticate sensors and detect data integrity attacks in a Cyber Physical System (CPS). The proposed technique uses the hardware characteristics of a sensor and physics of a process to create unique patterns (herein termed as fingerprints) for each sensor. The sensor fingerprint is a function of sensor and process noise embedded in sensor measurements. Uniqueness in the noise appears due to manufacturing imperfections of a sensor and due to unique features of a physical process. To create a sensor's fingerprint a system-model based approach is used. A noise-based fingerprint is created during the normal operation of the system. It is shown that under data injection attacks on sensors, noise pattern deviations from the fingerprinted pattern enable the proposed scheme to detect attacks. Experiments are performed on a dataset from a real-world water treatment (SWaT) facility\u00a0\u2026", "num_citations": "64\n", "authors": ["309"]}
{"title": "Using process invariants to detect cyber attacks on a water treatment system\n", "abstract": " An experimental investigation was undertaken to assess the effectiveness of process invariants in detecting cyber-attacks on an Industrial Control System (ICS). An invariant was derived from one selected sub-process and coded into the corresponding controller. Experiments were performed each with an attack selected from a set of three stealthy attack types and launched in different states of the system to cause tank overflow and degrade system productivity. The impact of power failure, possibly due to an attack on the power source, was also studied. The effectiveness of the detection method was investigated against several design parameters. Despite the apparent simplicity of the experiment, results point to challenges in implementing invariant-based attack detection in an operational Industrial Control System.", "num_citations": "63\n", "authors": ["309"]}
{"title": "Generalized attacker and attack models for cyber physical systems\n", "abstract": " An attacker model is proposed for Cyber Physical Systems (CPS). The attack models derived from the attacker model are used to generate parameterized attack procedures and functions that target a specific CPS. The proposed models capture both physical and cyber attacks and unify a number of existing attack models into a common framework useful for researchers in the experimental assessment of attack detection techniques. The generality of the models is shown by mapping a broad variety of existing attack models to the models proposed here, as well as generating attacks that are not found in the CPS design literature. The models have been used extensively in understanding the impact of cyber attacks on a water treatment system and in the design and assessment of detection mechanisms.", "num_citations": "61\n", "authors": ["309"]}
{"title": "On the use of software artifacts to evaluate the effectiveness of mutation analysis for detecting errors in production software\n", "abstract": " We show how mutation testing can be used to detect simple and complex errors that are often found in production software. We present a classification of the errors of TEX reported by Knuth. Using this classification we show that indeed the simple errors that mutation models do form a significant percentage of the errors found in production software. We introduce the notion of an error revealing mutant and show how such mutants, created by simple alterations of the program under test, can expose complex errors. We use the data provided by Knuth to obtain the types of complex errors used in our examples.", "num_citations": "60\n", "authors": ["309"]}
{"title": "Model-based security analysis of a water treatment system\n", "abstract": " An approach to analyzing the security of a cyber-physical system (CPS) is proposed, where the behavior of a physical plant and its controller are captured in approximate models, and their interaction is rigorously checked to discover potential attacks that involve a varying number of compromised sensors and actuators. As a preliminary study, this approach has been applied to a fully functional water treatment testbed constructed at the Singapore University of Technology and Design. The analysis revealed previously unknown attacks that were confirmed to pose serious threats to the safety of the testbed, and suggests a number of research challenges and opportunities for applying a similar type of formal analysis to cyber-physical security.", "num_citations": "58\n", "authors": ["309"]}
{"title": "Vulnerability testing of software system using fault injection\n", "abstract": " We describe an approach for testing a software system for possible security flaws. Traditionally, security testing is done using penetration analysis and formal methods. Based on the observation that most security flaws are triggered due to a flawed interaction with the environment, we view the security testing problem as the problem of testing for the fault-tolerance properties of a software system. We consider each environment perturbation as a fault and the resulting security compromise a failure in the toleration of such faults. Our approach is based on the well known technique of fault-injection. Environment faults are injected into the system under test and system behavior observed. The failure to tolerate faults is an indicator of a potential security flaw in the system. An Environment-Application Interaction (EAI) fault model is proposed. EAI allows us to decide what faults to inject. Based on EAI, we present a security-flaw classification scheme. This scheme was used to classify 142 security flaws in a vulnerability database. This classification revealed that 91 of the security flaws in the database are covered by the EAI model.", "num_citations": "57\n", "authors": ["309"]}
{"title": "Distributed attack detection in a water treatment plant: Method and case study\n", "abstract": " The rise in attempted and successful attacks on critical infrastructure, such as power grid and water treatment plants, has led to an urgent need for the creation and adoption of methods for detecting such attacks often launched either by insiders or state actors. This paper focuses on one such method that aims at the detection of attacks that compromise one or more actuators and sensors in a plant either through successful intrusion in the plant's communication network or directly through the plant computers. The method, labelled as Distributed Attack Detection (DAD), detects attacks in real-time by identifying anomalies in the behavior of the physical process in the plant. Anomalies are identified by using monitors that are implementations of invariants derived from the plant design. Each invariant must hold either throughout the plant operation, or when the plant is in a given state. The effectiveness of DAD was\u00a0\u2026", "num_citations": "54\n", "authors": ["309"]}
{"title": "Keylekh: a keyboard for text entry in indic scripts\n", "abstract": " Typing in an Indian language is currently not an easy task. Significant training is required before one can achieve an acceptable speed and only professional typists make the investment. Part of the complexity arises due to the structure of Indic scripts and large number of characters in each script. Solutions to input text in Indic languages have been around for a while, but none of these are usable enough to emerge as the de-facto standard. Here we describe the design of a new keyboard based on the structure of the Indic alphabet. The project went through cycles of design, prototyping and user evaluation. The evaluation was done by multiple techniques-usability tests, informal demonstrations, road shows and a typing competition. We particularly found the road shows and the competition useful for gathering feedback for this type of products.", "num_citations": "53\n", "authors": ["309"]}
{"title": "UNA based iterative test data generation and its evaluation\n", "abstract": " A number of approaches have been proposed to automatically generate test data to traverse a given path in a program. We present a program execution based approach to generate test data for a given path. The technique derives a desired input for a test path by iteratively refining an arbitrarily chosen input. A set of linear constraints on the increments to the input are derived to refine the input. We solve this constraint set using a Unified Numerical Approach (UNA) developed in this paper. Our technique can generate both integer and floating point inputs as well as handle arrays and loops. We determine a basis set of paths for a program and use our technique to generate test data for this set. We implemented and experimentally evaluated our technique. We present results of generating input for scientific programs. The experimental results show that the technique is effective in that it generates input for most of the\u00a0\u2026", "num_citations": "53\n", "authors": ["309"]}
{"title": "Using sensitivity analysis to validate a state variable model of the software test process\n", "abstract": " We report on the sensitivity analysis of a state variable model (Model S) proposed earlier. Model S captures the dominant behavior of the system test phase of the software test process. Sensitivity analysis is a mathematical methodology to compute changes in the system behavior due to changes in system parameters or variables. This is particularly important when parameters are calibrated using noisy or small data sets. Nevertheless, by mathematically quantifying the effects of parameter variations on the behavior of the model, and thereby the STP, one can easily and quickly evaluate the effect of such variations on the process performance without having to perform extensive simulations. In all cases studied, model S behaved according to empirical observations which serves to validate the model. It is also shown that sensitivity analysis can suggest structural improvements in a model when the model does not\u00a0\u2026", "num_citations": "51\n", "authors": ["309"]}
{"title": "Modeling mutation and a vector processor\n", "abstract": " Mutation analysis is a software testing methodology designed to substantiate the correctness of a program Phi. The mutation approach is to induce syntactically correct changes in Phi, thereby creating a set of mutant programs. The goal of a tester is to construct a set of test data T that distinguishes the output of Phi (T) from that of all mutant programs. Test data sensitive enough to distinguish all mutant programs is deemed adequate to infer the probable correctness of Phi. An algorithm is proposed which was designed to exploit the architecture of a vector processor like the Cyber 205 or Cray X/MP. The algorithm manages the simultaneous execution of multiple mutant Fortran 77 programs. This is accomplished by viewing the execution of these mutants as a sequence of vector instructions. The algorithm promises potential to greatly increase the performance of a mutation-based testing system, as well as points\u00a0\u2026", "num_citations": "50\n", "authors": ["309"]}
{"title": "Scalable and effective test generation for role-based access control systems\n", "abstract": " Conformance testing procedures for generating tests from the finite state model representation of Role-Based Access Control (RBAC) policies are proposed and evaluated. A test suite generated using one of these procedures has excellent fault detection ability but is astronomically large. Two approaches to reduce the size of the generated test suite were investigated. One is based on a set of six heuristics and the other directly generates a test suite from the finite state model using random selection of paths in the policy model. Empirical studies revealed that the second approach to test suite generation, combined with one or more heuristics, is most effective in the detection of both first-order mutation and malicious faults and generates a significantly smaller test suite than the one generated directly from the finite state models.", "num_citations": "49\n", "authors": ["309"]}
{"title": "Evolution support mechanisms for software product line process\n", "abstract": " Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies \u2013 change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes \u2013 architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.", "num_citations": "48\n", "authors": ["309"]}
{"title": "A Systematic Framework to Generate Invariants for Anomaly Detection in Industrial Control Systems.\n", "abstract": " A common method: build a predictive model, eg, AR, LDS, RNN models: x (t)= f (x {t\u2212 p: t\u2212 1}, u {t\u2212 p: t\u2212 1}; \u03b8)\u25ba x {t\u2212 p: t\u2212 1} the sensor measurements from time t\u2212 p to t\u2212 1\u25ba u {t\u2212 p: t\u2212 1} the actuator states from time t\u2212 p to t\u2212 1\u25ba x (t) the predicted sensor measurements at time t An alarm will be raised when the residual error x (t)\u2212 x (t)> \u03c4", "num_citations": "47\n", "authors": ["309"]}
{"title": "Measuring erythrocyte deformability with fluorescence, fluid forces, and optical trapping\n", "abstract": " A laser-based method has been developed for experimentally probing single red blood cell (RBC) buckling and determining RBC membrane rigidity. Our method combines a liquid flow cell, fluorescence microscopy, and an optical-trap to facilitate simple measurements of the shear modulus and buckling properties of single RBCs, under physiological conditions. The efficacy of the method is illustrated by studying buckling behavior of normal and Plasmodium-infected RBCs, and the effect of Plasmodium falciparum\u2013conditioned medium on normal, uninfected cells. Our simple method, which quantifies single-RBC deformability, may ease detection of RBC hematological disorders.", "num_citations": "47\n", "authors": ["309"]}
{"title": "High performance testing on SIMD machines\n", "abstract": " The authors describe how software testing using mutation analysis can be performed efficiently on an SIMD machine. They develop a technique that permits unified scheduling of multiple mutant programs on a very large SIMD machine. They believe that supercomputers with novel architectures can be used to enhance software productivity by using techniques like the one proposed.<>", "num_citations": "46\n", "authors": ["309"]}
{"title": "NoisePrint Attack Detection Using Sensor and Process Noise Fingerprint in Cyber Physical Systems\n", "abstract": " An attack detection scheme is proposed to detect data integrity attacks on sensors in Cyber-Physical Systems (CPSs). A combined fingerprint for sensor and process noise is created during the normal operation of the system. Under sensor spoofing attack, noise pattern deviates from the fingerprinted pattern enabling the proposed scheme to detect attacks. To extract the noise (difference between expected and observed value) a representative model of the system is derived. A Kalman filter is used for the purpose of state estimation. By subtracting the state estimates from the real system states, a residual vector is obtained. It is shown that in steady state the residual vector is a function of process and sensor noise. A set of time domain and frequency domain features is extracted from the residual vector. Feature set is provided to a machine learning algorithm to identify the sensor and process. Experiments are\u00a0\u2026", "num_citations": "45\n", "authors": ["309"]}
{"title": "Conformance testing of temporal role-based access control systems\n", "abstract": " We propose an approach for conformance testing of implementations required to enforce access control policies specified using the Temporal Role-Based Access Control (TRBAC) model. The proposed approach uses Timed Input-Output Automata (TIOA) to model the behavior specified by a TRBAC policy. The TIOA model is transformed to a deterministic se-FSA model that captures any temporal constraint by using two special events Set and Exp. The modified W-method and integer-programming-based approach are used to construct a conformance test suite from the transformed model. The conformance test suite so generated provides complete fault coverage with respect to the proposed fault model for TRBAC specifications.", "num_citations": "45\n", "authors": ["309"]}
{"title": "PMothra: Scheduling mutants for execution on a hypercube\n", "abstract": " Reliable software testing is a time consuming operation. In addition to the time spent by the tester in identifying, locating, and correcting bugs, a significant time is spent in the execution of the program under test and its instrumented or fault induced variants. When using mutation based testing to achieve high reliability, the number of such variants can be very large. In this paper we describe the architecture of a tool named PMothra that is designed to provide an architecture-transparent interface to a tester. In its current version, PMothra exploits the hypercube architecture by scheduling the execution of mutants on a 128-node Ncube/7 hypercube.", "num_citations": "42\n", "authors": ["309"]}
{"title": "Epic: An electric power testbed for research and training in cyber physical systems security\n", "abstract": " Testbeds that realistically mimic the operation of critical infrastructure are of significant value to researchers. One such testbed, named Electrical Power and Intelligent Control (EPIC), is described in this paper together with examples of its use for research in the design of secure smart-grids. EPIC includes generation, transmission, smart home, and micro-grid. EPIC enables researchers to conduct research in an active and realistic environment. It can also be used to understand the cascading effects of failures in one Industrial Control System (ICS) on another, and to assess the effectiveness of novel attack detection algorithms. Four feasible attack scenarios on EPIC are described. Two of these scenarios, demonstrated on EPIC, namely a power supply interruption attack and a physical damage attack, and possible mitigation, are also described.", "num_citations": "38\n", "authors": ["309"]}
{"title": "Enhancing Software Reliability Estimates using Modified Adaptive Testing\n", "abstract": " ContextMost software reliability models are based on a binary notion of correctness, i.e. \u201csuccessful\u201d or \u201cfailed.\u201d However, in several instances, it is important to account of failure severity to obtain more descriptive and accurate estimates of the reliability of the software.ObjectiveIn this paper, we develop a set of extended metrics based on the Nelson\u2019s software reliability model to account for information gained from a user\u2019s point of view regarding the severity of the observed failures. Model formulation based on multi-granularity failure severity is provided, and the proposed metrics are proved to be backward compatible.MethodIn order to estimate the software reliability through testing, an extended adaptive testing strategy, namely Modified Adaptive Testing (MAT) is proposed. The use of test history information allows the resulting test process to be adaptive in the selection of tests under limited test budget. Simulations\u00a0\u2026", "num_citations": "38\n", "authors": ["309"]}
{"title": "Listen: A tool to investigate the use of sound for the analysis of program behavior\n", "abstract": " We describe the architecture and use of a tool named LISTEN. This is a general purpose tool to instrument computer programs so that during program execution aspects of program behavior are mapped to audible sound. Ongoing research aimed at investigating the usefulness of sound in various programming-related tasks and a lack of supporting tools led to the development of LISTEN. This tool is expected to find use in tasks such as program testing and debugging, software-development environments for the visually handicapped, and data analysis using aural cues. We also report our initial experience gathered during exploratory use of LISTEN and provide a summary of ongoing research using this tool.", "num_citations": "38\n", "authors": ["309"]}
{"title": "Introduction to microprocessors\n", "abstract": " Presents architectural, programming, and interfacing concepts and techniques using the Intel 8085 as the primary microprocessor. This book illustrates programming concepts using several examples from both the 8085 and Z80. It describes commonly used memory types and chips such as the static RAM, EPROM, and EEPROM.", "num_citations": "37\n", "authors": ["309"]}
{"title": "Monitoring the software test process using statistical process control: a logarithmic approach\n", "abstract": " Statistical Process Control (SPC) is a powerful tool to control the quality of processes. It assists management personnel in the identification of problems and actions to be taken to bring a process into a stable state. SPC has been applied in various fields, including the Software Development Process. However, some processes are better characterized by factors that exhibit an exponential behavior. The use of such factors for process control limits the application of traditional SPC techniques. The Software Test Process (STP) characterized by the decay in the number of remaining errors, failure intensity, and an increase in code coverage, is one such process. A variant of the traditional SPC technique is proposed. This variant uses logarithmic transformation to allow the statistical control of processes whose dominant behavior is best described by an exponential. An evaluation of the proposed transformation carried out\u00a0\u2026", "num_citations": "36\n", "authors": ["309"]}
{"title": "On building non-intrusive performance instrumentation blocks for CORBA-based distributed systems\n", "abstract": " A non-intrusive, reusable framework for collecting performance statistics of CORBA-based distributed systems is proposed. The usefulness of the framework in the context of comparing performance improvement techniques is described. Also discussed is application of this framework to a large telecommunications application and experimental results on its relative overhead.", "num_citations": "34\n", "authors": ["309"]}
{"title": "Interface mutation test adequacy criterion: An empirical evaluation\n", "abstract": " An experiment was conducted to evaluate an inter-procedural test adequacy criterion named Interface Mutation. Program SPACE, developed for the European Space Agency (ESA), was used in this experiment. The development record available for this program was used to find the faults uncovered during its development. Using this information the test process was reproduced starting with a version of SPACE containing several faults and then applying Interface Mutation. Thus we could evaluate the fault revealing effectiveness of Interface Mutation. Results from the experiment suggest that (a) the application of Interface Mutation favors the selection of fault revealing test cases when they exist and (b) Interface Mutation tends to select fault revealing test cases more efficiently than in the case where random selection is used.", "num_citations": "32\n", "authors": ["309"]}
{"title": "Software engineering for secure software-state of the art: A survey\n", "abstract": " This report contains a survey of the state of the art in software engineering for secure software. Secure software is defined and techniques used in each phase of the software lifecycle to engineer the development of secure software are described. Also identified are open questions and areas where further research is needed. The survey reported here was undertaken to understand how the practice of software engineering blends with the requirement of secure software. This has resulted in a novel two-dimensional description of the relationship between the software lifecycle phases and techniques for satisfying security requirements. The report is organized around this relationship.", "num_citations": "31\n", "authors": ["309"]}
{"title": "Comparing the Fault Detection E ectiveness of Mutation and Data Flow Testing: An Empirical Study\n", "abstract": " We report results from an experiment to compare the fault detection e ectiveness of mutation, its variants, and data ow testing. As mutation is known to be a costly criterion to satisfy when compared with several other coverage criteria, we compared the fault detection e ectiveness of two variants of the mutation criterion with that of the all-uses criterion. Adequate test sets were generated randomly, as opposed to by human testers as in some previous studies. We view our results in the light of results from earlier studies comparing mutation with path-based testing strategies. We identify and discuss factors that one might consider while evaluating an adequacy criterion for use in practice. Results from our experiments strengthen the hypothesis that an adequacy criterion based on one of the two variants of mutation has superior fault detection e ectiveness than that of the all-uses criterion.", "num_citations": "29\n", "authors": ["309"]}
{"title": "Limitations of state estimation based cyber attack detection schemes in industrial control systems\n", "abstract": " An experiment was conducted on a water treatment plant to investigate the effectiveness of using Kalman filter based attack detection schemes in a Cyber Physical System (CPS). Kalman filter was implemented with Chi-Square detector. Random, stealthy bias, and replay attacks were launched and results analysed. Analysis indicates that stealthy false data injection and replay attacks cannot be detected by legacy failure detection methods.", "num_citations": "28\n", "authors": ["309"]}
{"title": "Discriminative probabilistic models for expert search in heterogeneous information sources\n", "abstract": " In many realistic settings of expert finding, the evidence for expertise often comes from heterogeneous knowledge sources. As some sources tend to be more reliable and indicative than the others, different information sources need to receive different weights to reflect their degrees of importance. However, most previous studies in expert finding did not differentiate data sources, which may lead to unsatisfactory performance in the settings where the heterogeneity of data sources is present. In this paper, we investigate how to merge and weight heterogeneous knowledge sources in the context of expert finding. A relevance-based supervised learning framework is presented to learn the combination weights from training data. Beyond just learning a fixed combination strategy for all the queries and experts, we propose a series of discriminative probabilistic models which have increasing capability to associate\u00a0\u2026", "num_citations": "28\n", "authors": ["309"]}
{"title": "Non-intrusive testing, monitoring and control of distributed CORBA objects\n", "abstract": " The architecture of Wabash, a tool for testing, monitoring and control of CORBA based distributed applications, is described. The need and utility of Wabash in testing and managing such applications is discussed. A comparison with similar tools is provided.", "num_citations": "28\n", "authors": ["309"]}
{"title": "Feedback control of the software test process through measurements of software reliability\n", "abstract": " A closed-loop feedback control model of the software test process (STP) is described. The model is grounded in the well established theory of automatic control. It offers a formal and novel procedure for using product reliability or failure intensity as a basis for closed loop control of the STP. The reliability or the failure intensity of the product is compared against the desired reliability at each checkpoint and the difference fed back to a controller. The controller uses this difference to compute changes necessary in the process parameters to meet the reliability, or failure intensity objective at the terminal checkpoint (the deadline). The STP continues beyond a checkpoint with a revised set of parameters. This procedure is repeated at each checkpoint until the termination of the STP. The procedure accounts for the possibility of changes (during testing), in reliability or failure intensity objective, the checkpoints, and the\u00a0\u2026", "num_citations": "26\n", "authors": ["309"]}
{"title": "Assessing the effectiveness of attack detection at a hackfest on industrial control systems\n", "abstract": " A hackfest named SWaT Security Showdown (S3) has been organized consecutively for two years. S3 has enabled researchers and practitioners to assess the effectiveness of methods and products aimed at detecting cyber attacks launched in real-time on an operational water treatment plant, namely, Secure Water Treatment (SWaT). In S3 independent attack teams design and launch attacks on SWaT while defence teams protect the plant passively and raise alarms upon attack detection. Attack teams are scored according to how successful they are in performing attacks based on specific intents while the defense teams are scored based on the effectiveness of their methods to detect the attacks. This paper focuses on the first two instances of S3 and summarizes the benefits of hackfest and the performance of an attack detection mechanism, namd Water Defense, that was exposed to attackers during S3.", "num_citations": "25\n", "authors": ["309"]}
{"title": "Facfinder: Search for expertise in academic institutions\n", "abstract": " Interdisciplinary collaboration and external funding opportunities have pushed academic institutions to take steps to advertise their most important organizational resource: faculty expertise. This paper presents FacFinder\u2013a publicly accessible faculty expertise search and ranking system across multiple universities. The key components of FacFinder are exposed and discussed along with the underlying rationale and design decisions. Also presented are the results of experiments aimed at evaluating the effectiveness and efficiency of FacFinder in its operational environment.", "num_citations": "25\n", "authors": ["309"]}
{"title": "Integrating design and data centric approaches to generate invariants for distributed attack detection\n", "abstract": " Process anomaly is used for detecting cyber-physical attacks on critical infrastructure such as plants for water treatment and electric power generation. Identification of process anomaly is possible using rules that govern the physical and chemical behavior of the process within a plant. These rules, often referred to as invariants, can be derived either directly from plant design or from the data generated in an operational. However, for operational legacy plants, one might consider a data-centric approach for the derivation of invariants. The study reported here is a comparison of design-centric and data-centric approaches to derive process invariants. The study was conducted using the design of, and the data generated from, an operational water treatment plant. The outcome of the study supports the conjecture that neither approach is adequate in itself, and hence, the two ought to be integrated.", "num_citations": "22\n", "authors": ["309"]}
{"title": "Mutation testing\n", "abstract": " We show how mutation testing can be used to detect simple and complex errors that are often found in production software. We present a classification of the errors of T) K reported by Knuth. Using this classification we show that indeed the simple errors that mutation models do form a significant percentage of the errors found in production software. We introduce the notion of an error revealing mutant and show how such mutants, created by simple alterations of the program under test, can expose complex errors. We use the data provided by Knuth to obtain the types of complex errors used in our examples.", "num_citations": "21\n", "authors": ["309"]}
{"title": "ICS-BlockOpS: Blockchain for operational data security in industrial control system\n", "abstract": " Industrial Control Systems (ICS) are the backbone of critical infrastructure found in power, water, manufacturing and other industries. An ICS controls a physical plant through the use of sensors and actuators. A Historian sits on a plant network and receives, parses, and saves data and commands transmitted over the network, across the Programmable Logic Controllers (PLCs), sensors and actuators. This data has at least two uses. One use is to check for any process anomalies that may occur due to component failures and cyber attacks. The other use of this data, and the focus of this work, is to serve as critical input to off-line activities such as forensic analysis. A cyber attack on the Historian could jeopardize any forensic analysis be it for maintenance, or discovering an attack trail. In this work, a novel architecture, named ICS-BlockOpS, is proposed to secure plant operational data recorded in the Historian. ICS\u00a0\u2026", "num_citations": "20\n", "authors": ["309"]}
{"title": "Argus: An orthogonal defense framework to protect public infrastructure against cyber-physical attacks\n", "abstract": " Argus, a framework for defending a public utility against cyber-physical attacks, contains intelligent checkers that use invariants derived from the physical and chemical interactions among various components and products of a utility. An Argus implementation is independent of the traditional layered defense that employs firewalls and other network-based logic to prevent intrusions into control systems, and hence is referred to as orthogonal defense. Portions of Argus have been implemented and tested in an operational water treatment testbed and found effective in detecting a number of single and complex multicomponent deception attacks.", "num_citations": "20\n", "authors": ["309"]}
{"title": "A control-theoretic approach to the management of the software system test phase\n", "abstract": " A quantitative, adaptive process control technique is described using an industrially validated model of the software system test phase (STP) as the concrete target to be controlled. The technique combines the use of parameter correction and Model Predictive Control to overcome the problems induced by modeling errors, parameter estimation errors, and limits on the resources available for productivity improvement.We present an example of the technique applied to data from the execution of the STP of a commercial software development effort at a large software manufacturer. The example shows that the control technique successfully achieves the schedule and quality objectives despite uncertainty in the estimation of the model parameters.", "num_citations": "20\n", "authors": ["309"]}
{"title": "A theoretical comparison between mutation and data flow based test adequacy criteria\n", "abstract": " Publication: CSC'94: Proceedings of the 22nd annual ACM computer science conference on Scaling up: meeting the challenge of complexity in real-world computing applications: meeting the challenge of complexity in real-world computing applications March 1994 Pages 38\u201345 https://doi. org/10.1145/197530.197554", "num_citations": "20\n", "authors": ["309"]}
{"title": "LSL: A specification language for program auralization\n", "abstract": " The need for specification of sound patterns to be played during program execution arises in contexts where program auralization is useful. We present a language named LSL (Listen Speciikation Language) designed for specifying program auralization. Specifications written in LSL and included in the program to be auralized are preprocessed by an LSL preprocessor. The preprocessed program when compiled and executed generates MIDI or voice data sent through a MIDI interface to a synthesizer module, or via audio channels, to an audio processor, which is transformed into audible sound. LSL has the generality to specify auralization of a variety of occurrences during program execution. It derives its broad applicability born its few generic elements, that when adapted to any procedural programming language, enable the use of LSL specifications for auralizing sequential, parallel, or object oriented programs. We view LSL as a useful tool for building general purpose multimedia applications, auditory displays, and for research in program auralization.", "num_citations": "20\n", "authors": ["309"]}
{"title": "NoiSense Print Detecting Data Integrity Attacks on Sensor Measurements Using Hardware-based Fingerprints\n", "abstract": " Fingerprinting of various physical and logical devices has been proposed for uniquely identifying users or devices of mainstream IT systems such as PCs, laptops, and smart phones. However, the application of such techniques in Industrial Control Systems (ICS) is less explored for reasons such as a lack of direct access to such systems and the cost of faithfully reproducing realistic threat scenarios. This work addresses the feasibility of using fingerprinting techniques in the context of realistic ICS related to water treatment and distribution systems. A model-free sensor fingerprinting scheme (NoiSense) and a model-based sensor fingerprinting scheme (NoisePrint) are proposed. Using extensive experimentation with sensors, it is shown that noise patterns due to microscopic imperfections in hardware manufacturing can uniquely identify sensors with accuracy as high as 97%. The proposed technique can be used to\u00a0\u2026", "num_citations": "19\n", "authors": ["309"]}
{"title": "Hardware identification via sensor fingerprinting in a cyber physical system\n", "abstract": " A lot of research in security of cyber physical systems focus on threat models where an attacker can spoof sensor readings by compromising the communication channel. A little focus is given to attacks on physical components. In this paper a method to detect potential attacks on physical components in a Cyber Physical System (CPS) is proposed. Physical attacks are detected through a comparison of noise pattern from sensor measurements to a reference noise pattern. If an adversary has physically modified or replaced a sensor, the proposed method issues an alert indicating that a sensor is probably compromised or is defective. A reference noise pattern is established from the sensor data using a deterministic model. This pattern is referred to as a fingerprint of the corresponding sensor. The fingerprint so derived is used as a reference to identify measured data during the operation of a CPS. Extensive\u00a0\u2026", "num_citations": "19\n", "authors": ["309"]}
{"title": "Intelligent checkers to improve attack detection in cyber physical systems\n", "abstract": " An approach to improve the security of cyber physical systems (CPS) through Intelligent Checkers (IC) is presented. ICs are designed to be behaviorally and structurally independent of the cyber-portion of a CPS and hence not subject to cyber attacks. An IC monitors the status of a physical process and raises an alarm when process measurements violate predefined constraints. A strict one-way communication from an IC to the CPS command center validates the authenticity of the data received from CPS sensors. An IC is able to raise an alarm despite communication failure or undetected cyber attacks. Strategic placement of one or more ICs throughout a CPS is expected to improve the potential for the early detection of attacks.", "num_citations": "19\n", "authors": ["309"]}
{"title": "Software Cybernetics.\n", "abstract": " Software Cybernetics explores the interplay between software and control. Though the concepts of software and cybernetics are well known when considered in isolation, the definition and scope of Software Cybernetics is sometimes blurred due to its novelty. A definition of Software Cybernetics and the delineation of its scope are the major goals of this article. Also presented here is a brief description of some applications of Software Cybernetics.", "num_citations": "19\n", "authors": ["309"]}
{"title": "Interceptor based constraint violation detection\n", "abstract": " Monitoring critical events such as constraints violations is one of the key issues of autonomic systems. This paper presents an interceptor based approach of constraint violation detection. In our approach, the monitor code is independent of functional code, and the monitor code can be generated automatically from XML-based constraint specifications. The experiment shows that our approach is feasible and is especially suitable for interface level constraints.", "num_citations": "18\n", "authors": ["309"]}
{"title": "A state model for the software test process with automated parameter identification\n", "abstract": " A model is proposed to assist software test managers in controlling the behavior and progress of the Software Test Process (STP) by allowing them to compare predicted behavior against observed progress made at various checkpoints. The model, whose parameters are based on measured data and process characteristics, generates the predicted behavior. An algorithm for the parameter estimation is set forth. The error between the predicted and desired behavior is used to drive a parametric control algorithm that tells the manager how to correct for schedule deviations.", "num_citations": "18\n", "authors": ["309"]}
{"title": "Analysis of an expert search query log\n", "abstract": " Expert search has made rapid progress in modeling, algorithms and evaluations in the recent years. However, there is very few work on analyzing how users interact with expert search systems. In this paper, we conduct analysis of an expert search query log. The aim is to understand the special characteristics of expert search usage. To the best of our knowledge, this is one of the earliest work on expert search query log analysis. We find that expert search users generally issue shorter queries, more common queries, and use more advanced search features, with fewer queries in a session, than general Web search users do. This study explores a new research direction in expert search by analyzing and exploiting query logs.", "num_citations": "17\n", "authors": ["309"]}
{"title": "Interface Mutation to assess the adequacy of tests for components and systems\n", "abstract": " Interface Mutation (IM) is proposed as an adequacy criterion to assess the adequacy of test sets developed to test components and systems. IM uses the description of a component's interface to generate interface mutants. Tests are assessed on their ability to distinguish interface mutants. Experiments conducted to evaluate the goodness of IM are reported. Analysis of experimental data suggests that test sets adequate with respect to IM are effective in revealing errors in components. The effort required to develop the tests is lower compared to the effort required to develop test sets adequate with respect to the traditional code coverage criteria based on control flow.", "num_citations": "17\n", "authors": ["309"]}
{"title": "Attacks on smart grid: power supply interruption and malicious power generation\n", "abstract": " Electric power supply is an essential component for several sectors including manufacturing, healthcare, building management, water distribution, and transportation systems. Hence, any interruption in electric power is likely to have an undesirable impact on the overall operation of any residential or commercial ecosystem. The serious impacts of power supply interruption attacks have been realized in the recent cyber incidents such as the Ukraine power blackout. It is also evident from recent incidents that both network and process vulnerabilities are crucial for an adversary to cause an adverse impact on the operation. This paper reports an investigation into power supply interruption and malicious power generation attacks focusing on process and network vulnerabilities. The investigation was conducted in two steps: First, a vulnerability assessment was conducted on a fully operational electric power testbed\u00a0\u2026", "num_citations": "16\n", "authors": ["309"]}
{"title": "A comprehensive approach, and a case study, for conducting attack detection experiments in Cyber\u2013Physical Systems\n", "abstract": " Several methods have been proposed by researchers to detect cyber attacks in Cyber\u2013Physical Systems (CPSs). This paper proposes a comprehensive approach for conducting experiments to assess the effectiveness of such methods in the context of a robot (Amigobot) that includes both cyber and physical components. The proposed approach includes a method for performing vulnerability analysis, several methods for attack detection, and guidelines for conducting experimental studies in the context of cyber security. The method for vulnerability analysis makes use of the Failure-Attack-CounTermeasure (FACT) graph. The experimental study to evaluate methods for attack detection comprises of three experiments. These methods have been implemented and evaluated, within and across all three experiments, with respect to their effectiveness, detection speed, and durability for injection, scaling, and stealthy\u00a0\u2026", "num_citations": "16\n", "authors": ["309"]}
{"title": "Perils of software reliability modeling\n", "abstract": " Using arguments based on an analysis of the state of the art in the eld of software reliability estimation and related practice, we warn practitioners that the road to reliability estimation is fraught with peril. Our analysis is based on several factors, with key factors including (a) di culties in estimating accurate operational pro les, and (b) inaccuracies in reliability estimates caused by erroneous operational pro les and/or invalid model assumptions. To circumvent these perils, we advocate new approaches to software reliability estimation, particularly robust approaches with minimal model assumptions, and approaches which combine failure data with code coverage data.", "num_citations": "16\n", "authors": ["309"]}
{"title": "A state variable model for the software test process\n", "abstract": " A novel approach to modeling the software development process is presented. This approach is based on the use of concepts and techniques from the theory of state variables and feedback control. The reasons to use this approach and its advantages are presented. A model for the Software Test Process is developed to show the approach applicability to the software development process. The assumptions and choice of parameters used in the model are discussed.", "num_citations": "15\n", "authors": ["309"]}
{"title": "Exploiting parallelism across program execution: a unification technique and its analysis\n", "abstract": " This paper describes a new technique for source-source transformation of sequential programs. We show that the transformed programs so generated provide significant speedups over the original program on vector-processors and vectormultiprocessors.. We exploit the parallelism that arises when multiple instances of a program are executed on simultaneously available data sets. This is in contrast to the existing approaches that aim. at detecting parallelism within a program. Analytic and simulation models of our technique clearly indicate the speedups that could be achieved when several data sets are available simultaneously, as is the case in many fields of interest", "num_citations": "15\n", "authors": ["309"]}
{"title": "Design and assessment of an Orthogonal Defense Mechanism for a water treatment facility\n", "abstract": " An Orthogonal Defense Mechanism (ODM) was designed and implemented to improve the detection of cyber attacks on an operational water treatment plant (WTreat). Successive design iterations led to an architecture that was prototyped and experimentally evaluated. ODM unobtrusively monitors WTreat using an independent network and gathers data from multiple data sources to corroborate the state of the plant using a state model. ODM is independent of, i.e. orthogonal to, any detection and defense mechanism, such as rule-based intrusion detection, that may otherwise exist in WTreat. ODM uses invariants created from plant design to detect and report anomalies in processes. While the architecture of OD, and its prototype, are specific to a water treatment plant, the underlying design ideas are generic and could be applied to other public infrastructure systems.", "num_citations": "14\n", "authors": ["309"]}
{"title": "Access control in water distribution networks: A case study\n", "abstract": " Industrial control systems (ICS) include devices, systems, networks and controls to operate industrial processes. ICS are found in many critical infrastructure systems such as transportation, energy and water processing. Given the wide proliferation of ICS, there is a heightened risk of cyber attacks on such infrastructure. It is well known that many such ICS lack appropriate access control, and there are guidelines on what such controls should be. However, there is a lack of case studies that point to specific access control shortfalls in real systems, relate such shortfalls to how an ICS could be compromised, and propose enhancements. This paper reports one such case study conducted on a water distribution plant built by professionals, and to professional standards. The study points to specific instances of inadequacy of access control in such systems that allow malicious entities to compromise system security. At the\u00a0\u2026", "num_citations": "14\n", "authors": ["309"]}
{"title": "Experimental evaluation of stealthy attack detection in a robot\n", "abstract": " An experiment was conducted to investigate the effectiveness of the Cumulative Sum (CUSUM) approach for detecting cyber attacks on Cyber Physical Systems (CPS). The Amigobot robot was used as the CPS in this study. Three types of stealthy attacks were considered, namely, surge, bias, and geometric. While a similar study has been reported earlier using a simulated chemical plant, the objective of the study reported here was to replicate the previous study in a realistic CPS environment and investigate whether the detection method performs differently. Cyber attacks were implemented on the Amigobot through its wireless control mechanism by changing the readings obtained from one of its sonar sensors. In addition, the investigation focused on understanding the impact of attack timing and duration on (a) detection effectiveness of the CUSUM method and (b) system safety. Analysis of experimental data\u00a0\u2026", "num_citations": "14\n", "authors": ["309"]}
{"title": "Scalable and effective test generation for access control systems that employ RBAC policies\n", "abstract": " Representation of Role Based Access Control (RBAC) policies as finite state models and three conformance testing procedures for generating tests from these models are proposed. A test suite generated using one of the three procedures has excellent fault detection ability but is astronomically large. Two approaches to reduce the size of the generated test suite were investigated. One is based on a set of six heuristics and the other directly generates a test suite from the finite state model using random selection of paths in the policy model. A fault model specific to the implementations of RBAC systems was used to evaluate the fault detection effectiveness of the generated test suites; the model incorporates both mutation-based and malicious faults. Empirical studies revealed that adequacy assessment of test suites using faults that correspond to first-order mutations may lead to a false sense of confidence in the correctness of policy implementation. The second approach to test suite generation is most effective in the detection of both first-order mutation and malicious faults and generates a significantly smaller test suite than the one generated directly from the finite state models.", "num_citations": "14\n", "authors": ["309"]}
{"title": "Software release control using defect based quality estimation\n", "abstract": " We describe two case studies to investigate the application of a state variable model to control the system test phase of software products. The model consists of two components: a feedback control portion and a model parameter estimation portion. The focus in this study is on the assessment of the goodness of the estimates and predictions of the model parameters and their utility in the management of the system test phase. Two large network management applications developed and tested at Sun Microsystems served as the subjects in these studies. Unlike the release of products based on marketing or deadline pressure, estimates of the number of residual defects are used to control the quality of the product being released. The estimates of the number of defects in the application when the test phase began and at the current checkpoint are obtained. In addition a prediction is made regarding the reduction in\u00a0\u2026", "num_citations": "14\n", "authors": ["309"]}
{"title": "Discriminative graphical models for faculty homepage discovery\n", "abstract": " Faculty homepage discovery is an important step toward building an academic portal. Although the general homepage finding tasks have been well studied (e.g., TREC-2001 Web Track), faculty homepage discovery has its own special characteristics and not much focused research has been conducted for this task. In this paper, we view faculty homepage discovery as text categorization problems by utilizing Yahoo BOSS API to generate a small list of high-quality candidate homepages. Because the labels of these pages are not independent, standard text categorization methods such as logistic regression, which classify each page separately, are not well suited for this task. By defining homepage dependence graph, we propose a conditional undirected graphical model to make joint predictions by capturing the dependence of the decisions on all the candidate pages. Three cases of dependencies among\u00a0\u2026", "num_citations": "13\n", "authors": ["309"]}
{"title": "A fault injection model of component security testing\n", "abstract": " The reliability and security of components block the development of component technology. Currently, component security testing is rarely researched as a special subject, and there are not some feasible approaches or technologies in detecting component security vulnerabilities. Problems with the component reliability and security have not yet been solved. The authors propose a FIM (fault injection model) of component security testing, and then specify some related definitions of FIM model and its matrix specification. A TGSM (test-cases generating based on solution matrix) algorithm of fault injection for component security is proposed based on FIM. The algorithm TGSM generates solution matrix that meets K factors coverage according to the matrix form of the FIM model. All rows data of the solution matrix compose the fault injection test-cases. The FIM model is implemented well based on research projects CSTS (component security testing system). Finally, the experimental results show that the approach which generates the fault injection test-cases of 3 factors coverage is effective. It can trigger the vast majority of the security exceptions by using the appropriate test-cases. FIM is effective and operable.", "num_citations": "13\n", "authors": ["309"]}
{"title": "On predicting reliability of modules using code coverage\n", "abstract": " The estimation of reliability of modules using code coverage is motivated by a two-phase method for estimating the quality of software using static structure and predicted quality estimates of component modules. A pilot experiment was conducted to investigate the accuracy of reliability estimates obtained from code coverage. The code coverage was measured using random testing. Three parameters that were varied are the size of programs, variations in operational profile, and the fault density. Results from this experiment were analyzed to understand how the accuracy of reliability estimates is affected by:(1) variation in fault density and (2) the size of programs. The results indicate that the degree of correlation between coverage and reliability has an inverse relationship with the fault density, and a direct relationship with program size.", "num_citations": "13\n", "authors": ["309"]}
{"title": "Investigation of cyber attacks on a water distribution system\n", "abstract": " A Cyber Physical System (CPS) consists of cyber components for computation and communication, and physical components such as sensors and actuators for process control. These components are networked and interact in a feedback loop. CPS are found in critical infrastructure such as water distribution, power grid, and mass transportation. Often these systems are vulnerable to attacks as the cyber components are potential targets for attackers. In this work, we report a study to investigate the impact of cyber attacks on a water distribution (WADI) system. Attacks were designed to meet attacker objectives and launched on WADI using a specially designed tool. This tool enables the launch of single and multi-point attacks where the latter are designed to specifically hide one or more attacks. The outcome of the experiments led to a better understanding of attack propagation and behavior of WADI in response to\u00a0\u2026", "num_citations": "12\n", "authors": ["309"]}
{"title": "Testing the effectiveness of attack detection mechanisms in industrial control systems\n", "abstract": " Industrial Control Systems (ICS) are found in critical infrastructure such as for power generation and water treatment. When security requirements are incorporated into an ICS, one needs to test the additional code and devices added do improve the prevention and detection of cyber attacks. Conducting such tests in legacy systems is a challenge due to the high availability requirement. An approach using Timed Automata (TA) is proposed to overcome this challenge. This approach enables assessment of the effectiveness of an attack detection method based on process invariants. The approach has been demonstrated in a case study on one stage of a 6- stage operational water treatment plant. The model constructed captured the interactions among components in the selected stage. In addition, a set of attacks, attack detection mechanisms, and security specifications were also modeled using TA. These TA models\u00a0\u2026", "num_citations": "12\n", "authors": ["309"]}
{"title": "Security relevancy analysis on the registry of windows nt 4.0\n", "abstract": " Many security breaches are caused by inappropriate inputs, crafted by people with malicious intents. To enhance the system security, we need either to ensure that inappropriate inputs are filtered out by the program, or to ensure that only trusted people can access those inputs. In the second approach, we certainly do not want to put such a constraint on every input; instead, we only want to restrict the access to the security-relevant inputs. This paper investigates how to identify which inputs are relevant to system security. We formulate the problem as a security relevancy problem and deploy static analysis technique to identify security-relevant inputs. Our approach is based on the dependency analysis technique; it identifies whether the behavior of any security-critical action depends on a certain input. If such a dependency relationship exists, we say that the input is security-relevant, otherwise we say the input is\u00a0\u2026", "num_citations": "12\n", "authors": ["309"]}
{"title": "A modeling framework for critical infrastructure and its application in detecting cyber-attacks on a water distribution system\n", "abstract": " Critical infrastructure (CI), such as systems for water treatment, water distribution, power generation and distribution, is vital for the well being of a society. Such systems are typically large, complex, and interconnected. A cyber-attack on one such system could affect the other. In this work, a generic agent-based framework is proposed to aid in modeling a CI. Combined with a proposed methodology, the framework can be used to model a CI, and those connected to it, as an aid to understanding their collective behavior when subjected to cyber attacks. In a case study, the proposed framework was used to create a model of an operational water distribution system and validated experimentally using a set of cyber attacks launched against an implementation of the model.", "num_citations": "11\n", "authors": ["309"]}
{"title": "Waterjam: An experimental case study of jamming attacks on a water treatment system\n", "abstract": " An experiment was conducted to investigate network jamming attacks on an Industrial Control Systems. The Secure Water Treatment (SWaT) system was chosen to perform the experiments. Jamming attacks were launched on SWaT using software defined radio. Attacks were designed to meet attacker objectives selected from a Cyber Physical Systems specific attacker model. Attacks exposed vulnerabilities associated with design of SWaT and suggested modifications in the architecture owing to broadcast nature of wireless system update and control. This investigation was focused on (a) how jamming attacks can be performed on Industrial Control Systems, (b) what communication links to jam and why? (c) what is the impact of jamming attacks on SWaT? The observed response to various attacks was then used to propose attack detection mechanisms. While the jamming attacks were applied to a specific water\u00a0\u2026", "num_citations": "11\n", "authors": ["309"]}
{"title": "From design to invariants: Detecting attacks on cyber physical systems\n", "abstract": " An approach is proposed to derive state-based invariants that, when programmed into a controller, proved to be effective in detecting cyber attacks on an Industrial Control System (ICS). The proposed approach begins with the ICS design and models its process dynamics using an extended hybrid automata from which the invariants are derived. Each invariant is programmed and inserted into an appropriate Programmable Logic Controller (PLC) as a companion to the control code. The invariant is active during ICS operation and serves to check on the validity of the system state in accordance with the system design. This approach, referred to as D2I for Design to Invariants, was evaluated on a fully operational 6-stage water treatment plant that uses a distributed process control system.", "num_citations": "11\n", "authors": ["309"]}
{"title": "A six-step model for safety and security analysis of cyber-physical systems\n", "abstract": " A Six-Step Model (SSM) is proposed for modeling and analysis of Cyber-Physical System (CPS) safety and security. SSM incorporates six dimensions (hierarchies) of a CPS, namely, functions, structure, failures, safety countermeasures, cyber-attacks, and security countermeasures. The inter-dependencies between these dimensions are defined using a set of relationship matrices. SSM enables comprehensive analysis of CPS safety and security, as it uses system functions and structure as a knowledge-base for understanding what effect the failures, cyber-attacks, and selected safety and security countermeasures might have on the system. A water treatment system is used as an example to illustrate how the proposed model could serve as a useful tool in the safety and security modeling and analysis of critical infrastructures.", "num_citations": "11\n", "authors": ["309"]}
{"title": "Ranking experts with discriminative probabilistic models\n", "abstract": " In the realistic settings of expert finding, the evidence for expertise often comes from heterogeneous knowledge sources. As some sources tend to be more reliable and indicative than the others, different data sources need to receive different weights to reflect their degrees of importance. However, most previous studies in expert finding did not differentiate data sources, which may lead to unsatisfactory performance in the settings where the heterogeneity of data sources is present.In this paper, we investigate how to merge and weight heterogeneous knowledge sources in the context of expert finding. A relevance-based supervised learning framework is presented to learn the combination weights from training data. Beyond just learning a fixed combination strategy for all the queries and experts, we propose a series of probabilistic models which have increasing capability to associate the combination weights with specific experts and queries. In the last (and also the most sophisticated) proposed model, the combination weights depend on both expert classes and query topics, and these classes and topics are derived from expert and query features. Compared with expert and query independent combination methods, the proposed combination strategy can better adjust to different types of experts and queries. In consequence, the model yields much flexibility of combining data sources when dealing with a broad range of expertise areas and a large variation in experts. Empirical studies on a real world faculty expertise testbed demonstrate the effectiveness and robustness of the proposed learning based models.", "num_citations": "11\n", "authors": ["309"]}
{"title": "A XML based policy-driven information service\n", "abstract": " The use of XML as a vendor, implementation and operating system neutral means of describing system events and system management policies is described. The need for handling heterogeneity in emerging distributed systems by management solutions and the utility of XML in this regard is discussed. The design and architecture of a prototype implementation of a XML based policy-driven information server is described.", "num_citations": "11\n", "authors": ["309"]}
{"title": "Architecture of TAMER: A Tool for dependability analysis of distributed fault-tolerant systems\n", "abstract": " The need for fault tolerant software has grown signi cantly with the need for providing computer-based continuous service in a variety of areas that include telecommunications, air and ground transportation, and defense. TAMER (a Testing, Analysis, and Measurement Environment for Robustness) is a tool designed to assess the dependability of such systems. Three key ideas make TAMER di erent from several existing tools aimed at dependability assessment of distributed fault tolerant systems. These three ideas are incorporated in:(a) a two-dimensional criterion for dependability assessment,(b) interface fault injection, and (c) a scheme for partitioning the system under assessment into subsystems that could be analyzed\\o-line\". The interactive nature of TAMER allows an assessor to identify portions of software that may need attention for additional testing, redesign, or recoding. Such identi cation becomes possible with the help of code and fault coverage information derived during the testing process under TAMER's control. Here we describe the architecture of TAMER.", "num_citations": "11\n", "authors": ["309"]}
{"title": "An agent-based framework for simulating and analysing attacks on cyber physical systems\n", "abstract": " An agent-based framework is presented to model and analyze physical and cyber attacks on Cyber Physical Systems (CPS). In the first phase of a two phase procedure in the framework, a CPS is modelled using State Condition Graphs (SCGs) representing the structural relations among the cyber and physical components. In the second phase, SCGs are partitioned into separate groups, referred to as agents, based on their functional dependency. Each agent is associated with a sensor and an actuator set. The actuator set is further divided into two subsets: active actuator and passive subsets that, respectively, directly or indirectly participate in controlling the environment. Interactions among different agents can be created by assigning an aggregate function to each agent with sensor and actuator sets as the domain of the function and status of the environment as its codomain. A case study was conducted\u00a0\u2026", "num_citations": "9\n", "authors": ["309"]}
{"title": "Design of intelligent checkers to enhance the security and safety of cyber physical systems\n", "abstract": " An approach for the design and placement of Intelligent Checkers (ICs) is presented along with an application to enhance the security and safety of a water purification system. Each IC, a smart sensor with a one-way outgoing communication over the network, monitors in real-time the status of one or more physical processes and raises an alarm when the monitored process measurements violate predefined constraints. ICs are structurally and behaviorally independent of the cyber-portion of a Cyber Physical System (CPS) and hence are much less likely to be affected by cyber attacks than the control system that monitors and controls a CPS. Thus, ICs serve as devices orthogonal to an existing intrusion detection system and assist in the early detection of successful attacks through independent monitoring of physical processes.", "num_citations": "9\n", "authors": ["309"]}
{"title": "Integrated 2D design in the curriculum: effectiveness of early cross-subject engineering challenges\n", "abstract": " Integrated 2D Design in the Curriculum: Effectiveness of Early Cross-Subject Engineering ChallengesAbstractMultidisciplinary engineering design is difficult to teach in the undergraduate curriculum, particularly in the early Freshman and Sophomore years, since the students have not enrolled in abreadth of subjects. Multidisciplinary projects are often left to latter years, thereby leaving thestudents with an incomplete picture of how course subject matters relate and fit in a larger viewof engineering and design. To address this, a novel approach to multi-disciplinary engineeringeducation was instituted in the Freshman and Sophomore years, where during a particular termall courses simultaneously attacked a common design problem. For one dedicated week, thecoursework was stopped and instead all instructors and students simultaneously worked on thedesign challenge problem engaging those courses\u2019 subject matter alone. We call this the 2Ddesign challenge, where the design problem is multidisciplinary, but exclusively restricted to thedomains of the courses being taught. The 2D design challenge approach was highly effective at providing early learning of themultidisciplinary nature of design problems, including statistically significant impact on studentperceptions of their ability to solve multidisciplinary design problems. As an example, coursesin biology, thermodynamics, differential equations, and software with controls were brought intoa 2D design challenge problem of developing a perishable food delivery system composed ofunrefrigerated unmanned ground vehicles. Students designed insulated cartons for transport byUGVs provided\u00a0\u2026", "num_citations": "9\n", "authors": ["309"]}
{"title": "Model-based testing of access control systems that employ RBAC policies\n", "abstract": " Access control is the key security service used for information and system security. The access control mechanisms can be used to enforce various security policies, but the desired access control objectives can only be achieved if the underlying software implementation is correct. It therefore becomes essential to not only verify that the implementation conforms to the given policy but also to confirm the absence of any violations in it. We propose a model-based strategy for testing implementations of access control systems that employ the RBAC policy specification. Our approach is based on the construction of a structural and behavioral model of the corresponding RBAC specification. The model is then used to generate static and dynamic test suites for the corresponding implementation. The code coverage and mutation score were used as metrics to determine the efficacy of the proposed approach in a case study. The results of the case study show that the tests generated using the proposed approach not only provide good control flow coverage of the implementation but are also effective in detecting faults induced via mutation operators.", "num_citations": "9\n", "authors": ["309"]}
{"title": "Concurrency enhancement through program unification: a performance analysis\n", "abstract": " A novel unification scheme was proposed that constructs an N-dimensional transformation of a given program such that a single execution of the transformed program effectively substitutes for a serial execution of the original program on N different data sets. Our studies show that the speedup thus obtained makes the scheme a very attractive alternative in situations where a given program P is required to be executed on several data sets simultaneously. This is true, in particular, when the original program P does not lend itself well to vectorization or concurrentization. In order to obtain an understanding of the operation of the transformed program, we construct stochastic urn models of program execution for small configurations. By comparing the execution times for the urn model counterparts of the original program P and its transformation, we obtain estimates of speedup given by the transformation. While the\u00a0\u2026", "num_citations": "9\n", "authors": ["309"]}
{"title": "Anomaly detection in critical infrastructure using probabilistic neural network\n", "abstract": " Supervisory Control and Data Acquisition (SCADA) systems forms a vital part of any critical infrastructure. Such systems are network integrated for remote monitoring and control making them vulnerable to intrusions by malicious actors. Such intrusions may lead to anomalous behavior of the underlying physical process. This work presents a Probabilistic Neural Network (PNN) based anomaly detector to detect anomalies arising consequent to a cyber attack. Experimental validation was conducted using the dataset obtained from an operational water treatment testbed, namely Secure Water Treatment (SWaT). The impact of the smoothening parameter on the performance of the PNN-based anomaly detector was analyzed. Experimental evaluations indicate the significance of the PNN-based anomaly detector, compared with several competing detectors, in terms of precision, F-score, false alarm rate, and\u00a0\u2026", "num_citations": "8\n", "authors": ["309"]}
{"title": "Introducing cyber security at the design stage of public infrastructures: A procedure and case study\n", "abstract": " Existing methodologies for the design of complex public infrastructure are effective in creating efficient systems such as for water treatment, electric power grid, and transportation. While such methodologies and the associated design tools account for potential component and subsystem failures, they generally ignore the cyber threats; such threats are now real. This paper presents a step towards a methodology that incorporates cyber security at an early stage in the design of complex systems. A novel graph theoretic mechanism, named Dynamic State Condition Graph, is proposed to capture the relationships among sensors and actuators in a cyber physical system and the functions that are affected when the state of an actuator changes. Through a case study on a modern and realistic testbed, it is shown that introducing security at an early stage will likely impact the design of the control software; it may\u00a0\u2026", "num_citations": "8\n", "authors": ["309"]}
{"title": "Computer supported cooperative work in software engineering\n", "abstract": " Field programmable gate arrays, FPGAs, have become an attractive implementation technology for a broad range of computing systems. We recently proposed a processor architecture, Tinuso, which achieves high performance by moving complexity from hardware to the compiler tool chain. This means that the compiler tool chain must handle the increased complexity. However, it is not clear if current production compilers can successfully meet the strict constraints on instruction order and generate efficient object code. In this paper, we present our experiences developing a compiler backend using the GNU Compiler Collection, GCC. For a set of C benchmarks, we show that a Tinuso implementation with our GCC backend reaches a relative speedup of up to 1.73 over a similar Xilinx Micro Blaze configuration while using 30% fewer hardware resources. While our experiences are generally positive, we expose\u00a0\u2026", "num_citations": "8\n", "authors": ["309"]}
{"title": "Digital Device Manuals for the management of ConnectedSpaces\n", "abstract": " An increasing number of devices that can be monitored and controlled via the Internet have begun to appear in the commercial domain. Health care devices, such as heart rate monitors and dialysis machines, home and other security devices, and climate control devices, are a few of the device types that are increasingly becoming connectable to the Internet. When many such devices come together we say they form a ConnectedSpace. Many nonfunctional behavioral constraints, such as those related to safety, security, privacy, and fault tolerance, may be imposed on a ConnectedSpace. These constraints may be monitored and enforced by a centralized or distributed management algorithm. Various approaches may be used for enforcing the constraints. Digital device manuals are envisioned to capture the generic information required by the different approaches to monitor and enforce the constraints.", "num_citations": "8\n", "authors": ["309"]}
{"title": "TDS: a tool for testing distributed component-based applications\n", "abstract": " Applications that utilize the broker-based architecture are often composed of several components that need to be tested both separately and together. An important activity during testing is the assessment of the adequacy of test sets. The interface mutation based test adequacy criterion for components can be used for test adequacy assessment. TDS is a tool that assists a tester in performing Interface Mutation testing. The architecture of the tool and the features are described here.", "num_citations": "8\n", "authors": ["309"]}
{"title": "On software reliability and code coverage\n", "abstract": " Failure and module coverage data were collected during the system testing phase of a commercial hardware-software system called Product S. The Goel-Okumoto and Musa-Okumoto models were used to predict the Mean Test Cases to Failure (MTCTF) for these data. It was observed that the errors in predictions of MTCTF correlate with a sharp rise in code coverage. The sharp rise occurs when testers test code which was not previously tested. This observation suggests that software reliability prediction models may be able to improve the accuracy of prediction by accounting for code coverage.", "num_citations": "8\n", "authors": ["309"]}
{"title": "Effect of test set minimization on the fault detection effectiveness of the all-uses criterion\n", "abstract": " Size and code coverage are important attributes of a set of tests. When a program P is executed on elements of the test set T, we can observe the fault detecting capacity of T for P. We can also observe the degree to which T induces code coverage on P according to some coverage criterion. We would like to know whether it is the size of T or the coverage of T on P which determines the fault detection effectiveness of T for P. To address this issue we ask the following question: While keeping coverage constant, what is the effect on fault detection of reducing the size of a test set? We report results from an empirical study using the all-uses criterion as the coverage measure. Keywords: fault detection effectiveness, test set minimization, block coverage, all-uses coverage W. Eric Wong is with Hughes Network Systems, Germantown, MD 20876. Joseph R. Horgan and Saul London are with Bell Communications Research, Morristown, NJ 07962. Aditya P. Mathur is with the Software Engineeri...", "num_citations": "8\n", "authors": ["309"]}
{"title": "A formal evaluation of mutation and data flow based test adequacy criteria\n", "abstract": " Evaluation of the adequacy of a test set is a frequently encountered problem in software testing. Data ow and mutation-based testing methods provide several criteria to evaluate test sets. This work considers the c-use, p-use, and all-uses based data ow criteria and compares these with a mutation-based criterion. We show that for the most general class of programs, mutation and the above data ow criteria are incomparable. We also identify classes of programs for which a mutation-based criterion subsumes the c-use, p-use, and alluses based criteria. Our results are signi cant in the light of the fact that the determination of the subsumption relationship between data ow and mutation continues to be an open problem in the eld of software testing.", "num_citations": "8\n", "authors": ["309"]}
{"title": "Preliminary report on design rationale, syntax, and semantics of LSL: A specification language for program auralization\n", "abstract": " The need for speci cation of sound patterns to be played during program execution arises in contexts where program auralization is useful. We present a language named LSL (Listen Speci cation Language) designed for specifying program auralization. Speci cations written in LSL and included in the program to be auralized are preprocessed by an LSL preprocessor. The preprocessed program when compiled and executed generates MIDI or voice data sent through a MIDI interface to a synthesizer module, or via audio channels, to an audio processor, which transforms the notes or voice into audible sound. LSL has the generality to specify auralization of a variety of occurrences during program execution. It derives its broad applicability from its few generic elements that when adapted to any procedural programming language, such as C, C++, or Ada, enable the writing and use of LSL speci cations for auralizing sequential, parallel, or object oriented programs in that language. We view LSL as a useful tool for building general purpose multimedia applications and for research in program auralization.This work was supported in part by an educational supplement from the National Science Foundation No. CCR 9102311 and 9123502-CDA. The authors are with Software Engineering Research Center and Department of Computer Sciences, Purdue University, W. Lafayette, IN 47907. Aditya P. Mathur can be contacted at (317) 494-7822 or via email at apm@ cs. purdue. edu. David Boardman can be contacted via email at boardman@ cs. purdue. edu.", "num_citations": "8\n", "authors": ["309"]}
{"title": "Design of mutant operators for the C programming language\n", "abstract": " iv 1 INTRODUCTION 1 2 AN OVERVIEW OF MUTATION BASED TESTING 2 3 THE RAISON D'ETRE OF A MUTANT OPERATOR 4 4 MUTANT OPERATOR CLASSIFICATION 6 5 NAMING CONVENTIONS 7 5.1 General::::::::::::::::::::::::::::::::::::::::::: 7 5.2 Naming of Binary Operator Mutations:::::::::::::::::::::::::: 8 6 WHAT IS NOT MUTATED? 10 7 OPTIMIZATIONS 12 8 CONCEPTS AND DEFINITIONS 13 8.1 Functions Introduced by Mutant Operators::::::::::::::::::::::: 13 8.2 Range of Applicability::::::::::::::::::::::::::::::::::: 14 8.3 Linearization:::::::::::::::::::::::::::::::::::::::: 14 8.4 Execution Sequence:::::::::::::::::::::::::::::::::::: 16 8.5 Effect of An Execution Sequence::::::::::::::::::::::::::::: 22 8.6 Global and Local Identifier Sets::::::::::::::::::::...", "num_citations": "8\n", "authors": ["309"]}
{"title": "SecWater: A multi-layer security framework for water treatment plants\n", "abstract": " A framework (SecWater) to assist in the design of secure water treatment plants is presented. SecWater enables plant designers to secure the entire Supervisory Control and Data Acquisition (SCADA) infrastructure using multi-layer security. The framework consists of seven layers labeled SL0 through SL6. Layer SL0 provides the first line of defense against cyber attacks while SL5 and SL6 provide real-time control when an attack is detected. Intermediate layers enable the detection of cyber and physical attacks. Several implementations could be derived using SecWater depending on cost constraints and risks to be tolerated. Four layers in SecWater are in use to secure an existing water treatment plant.", "num_citations": "7\n", "authors": ["309"]}
{"title": "A software cybernetic approach to control of the software system test phase\n", "abstract": " A quantitative, adaptive process control technique is described using an industrially validated model of the software system test phase as concrete target to be controlled. The technique combines the use of parameter correction and model predictive control to overcome the problems induced by modeling errors, parameter estimation errors, and limits on the resources available for productivity improvement.", "num_citations": "7\n", "authors": ["309"]}
{"title": "Synthesizing distributed controllers for the safe operation of ConnectedSpaces\n", "abstract": " A collection of one or more devices, each described by its digital device manual and reachable over a network, is a ConnectedSpace. A set of safety policies may be enforced on a ConnectedSpace to ensure the safety of the environment in which the ConnectedSpace is deployed. The enforcement of these safety policies by one or more safely controllers governs the behavior of the devices within the ConnectedSpace. We propose a policy-based partitioning scheme for synthesizing k distributed safety controllers such that: (a) each device is guaranteed to be controlled by no more than two controllers, and (b) each policy is guaranteed to be enforced by exactly one controller. We present an experimental evaluation of our scheme. The experimental results show that the scheme is scalable with respect to the number of devices and the number of policies. We also show how safety controllers that are correct with\u00a0\u2026", "num_citations": "7\n", "authors": ["309"]}
{"title": "Effect of disturbances on the convergence of failure intensity\n", "abstract": " We report a study to determine the impact of four types of disturbances on the failure intensity of a software product undergoing system test. Hardware failures, discovery of a critical fault, attrition in the test team, are examples of disturbances that will likely affect the convergence of the failure intensity to its desired value. Such disturbances are modeled as impulse, pulse, step, and white noise. Our study examined, in quantitative terms, the impact of such disturbances on the convergence behavior of the failure intensity. Results from this study reveal that the behavior of the state model, proposed elsewhere, is consistent with what one might predict. The model is useful in that it provides a quantitative measure of the delay one can expect when a disturbance occurs.", "num_citations": "7\n", "authors": ["309"]}
{"title": "Infrastructure for the Management of SmartHomes\n", "abstract": " The reduction in the cost and size of integrated circuits has given birth to smart devices. These devices can be monitored and controlled by an embedded application and may be capable of connecting to the Internet. Such devices may give rise to new types of services such as a Digital Music Library service or a Digital Home Security service. Such services may be provided by a service provider and realized using distributed applications. A network comprising of such devices and services is called a SmartHome. Acess from a remote location and the monitoring and control of rhese services and devices require architectures for their management. In this paper, we present the various types of users and their management needs in the context of SmartHomes. We describe an infrastructure consisting of architectures for the management of embedded and distributed applications in a SmartHome. We also present the criteria for evaluating various management architectures and discuss the evaluation of the described architectures.", "num_citations": "7\n", "authors": ["309"]}
{"title": "Modeling and controlling the software test process\n", "abstract": " A novel approach for modeling and control of the software test process is presented. The approach is based on the concept of state variables and uses techniques from the well-established field of automatic control theory. An initial model of the software test phase is described and the results of a case study analysis are presented.", "num_citations": "7\n", "authors": ["309"]}
{"title": "Concurrent stochastic simulations: Experiments with unification\n", "abstract": " We investigate the efficacy of a recently proposed concurrentization scheme-known as Program Unification. Unification is a technique which automates program transfor-mation from scalar to vector mode, in order to improve machine utilization and yield speedup. We present experimental results on program unification in the stochastic sim-ulation domain. The ideas demonstrate how a variety of simulation applications such as discrete-event simulations (eg, queueing systems), Monte Carlo simulations (eg, estimation of multidimensional integrals), and statistical simulations (eg, in estimating the average complexity of algorithms) lend themselves to unifying schemes with pleas-ing speedup characteristics. The experiments reported were conducted on an Alliant FX/80 and an Alliant FX/8.", "num_citations": "7\n", "authors": ["309"]}
{"title": "Generating invariants using design and data-centric approaches for distributed attack detection\n", "abstract": " A cyber attack launched on a critical infrastructure (CI), such as a power grid or a water treatment plant, could lead to anomalous behavior. There exist several methods to detect such behavior. This paper reports on a study conducted to compare two methods for detecting anomalies in CI. One of these methods, referred to as design-centric, generates invariants from the design of a CI. Another method, referred to as data-centric, generates the invariants from data collected from an operational CI. The key question that motivated the study is \u201cHow do design and data-centric methods compare in the effectiveness of the generated invariants in detecting process anomalies.\u201d The data-centric approach used Association Rule Mining for generating invariants from operational data. These invariants, and their performance in detecting anomalies, was compared against those generated by a design-centric approach reported\u00a0\u2026", "num_citations": "6\n", "authors": ["309"]}
{"title": "Detecting Multi-Point Attacks in a Water Treatment System Using Intermittent Control Actions.\n", "abstract": " A novel technique for detecting multi-point attacks on an Industrial Control System (ICS) is described. The technique, referred to as Intermittent Control Actions (ICA), sends control signals intermittently to selected components to monitor the system using a process invariant. ICA was assessed experimentally for its effectiveness in an operational water treatment testbed. The experiments revealed (a) multi-point attack scenarios where ICA succeeds or fails to detect an attack,(b) issues in the design of key ICS components to ensure that the control actions in ICA do not lead to undesirable process behavior, and (c) constraints on the design of the physical system for safe use of ICA.", "num_citations": "6\n", "authors": ["309"]}
{"title": "Countermeasures to enhance cyber-physical system security and safety\n", "abstract": " An application of two Cyber-Physical System (CPS) security countermeasures - Intelligent Checker (IC) and Cross-correlator - for enhancing CPS safety and achieving required CPS safety integrity level is presented. ICs are smart sensors aimed at detecting attacks in CPS and alerting the human operators. Cross-correlator is an anomaly detection technique for detecting deception attacks. We show how ICs could be implemented at three different CPS safety protection layers to maintain CPS in a safe state. In addition, we combine ICs with the cross-correlator technique to assure high probability of failure detection. Performance simulations show that a combination of these two security countermeasures is effective in detecting and mitigating CPS failures, including catastrophic failures.", "num_citations": "6\n", "authors": ["309"]}
{"title": "Detecting injection attacks in linear time invariant systems\n", "abstract": " A novel technique to detect injection attacks in linear time invariant systems is proposed. Detection involves the use of a cross-correlator that dynamically increases its analysis window to increase the probability of detection. For correlation analysis and potential remediation, the cross-correlator makes reference to a linear (internal) model of the system sensor. Unlike some existing techniques, it does not require the construction of a complex internal model of the entire control system, pre-knowledge of the attacking signal characteristics or perturbation of the control system. Performance simulations show probabilities of detection that can reach 100% with bounded analysis window size. Sensitivity analysis also show that the cross-correlator exhibits good robustness against unlikely to occasional transient sensor failures.", "num_citations": "6\n", "authors": ["309"]}
{"title": "A security assurance framework combining formal verification and security functional testing\n", "abstract": " Formal specification is usually employed to avoid ambiguity of security requirements. However, it is hard to assure correctness of this formal model and its conformance with security implementation. In this paper, a framework combining formal verification and security functional testing is proposed to support the correctness and conformance check procedure. Formal requirements are verified following integrated steps and formulae. Verified specification is used as the basis for security functional test and a test criterion called strict schema coverage is developed to derive tests. The framework is supported by Z specification Based Security Assurance Toolkit (ZBSAT). Empirical results on Chinese Wall Model (CWM) policy and its implementation demonstrate its feasibility. In addition, comparison results of mutation test explore the efficiency of this test approach.", "num_citations": "6\n", "authors": ["309"]}
{"title": "Synthesizing a Safety Controller for ConnectedSpaces Using Supervisory Control\n", "abstract": " Software Engineering Research CenterI Dept. of Computer Science, Purdue University, West Lafayette, IN, USA {baskars, apm}@ cs. purdue. edu", "num_citations": "6\n", "authors": ["309"]}
{"title": "Sensitivity analysis of a state variable model of the software test process\n", "abstract": " The paper reports the results of a sensitivity analysis of a state variable model of the Software Test Process (STP). Given a state model of the STP, a sensitivity matrix is calculated using tensor algebra. The sensitivity matrix allows computation of output variations to small perturbations in the model parameters. The results confirm that the model behaves in a manner very similar to what one might expect from a software test process. Results of this analysis also suggest changes and enhancements in the model to improve its accuracy in predicting the behavior of the Software Test Process.", "num_citations": "6\n", "authors": ["309"]}
{"title": "A multilayer perceptron model for anomaly detection in water treatment plants\n", "abstract": " Early and accurate anomaly detection in critical infrastructure (CI), such as water treatment plants and electric power grid, is necessary to avoid plant damage and service disruption. Several machine learning techniques have been employed for the design of an effective anomaly detector in such systems. However, threats such as from insiders and state actors, introduce challenges in the design of an effective anomaly detector. This work presents a multi-layer perceptron (MLP) based anomaly detector that uses an unsupervised approach to safeguard CI from the adverse impacts of cyber-attacks. The proposed detector was trained using the data collected under the normal operation of the plant. The model captures the temporal dependencies between the samples and predicts the plant behavior. Further, the well-known CUmulative SUM (CUSUM) approach was used to detect the abnormal deviations between the\u00a0\u2026", "num_citations": "5\n", "authors": ["309"]}
{"title": "Challenges in machine learning based approaches for real-time anomaly detection in industrial control systems\n", "abstract": " Data-centric approaches are becoming increasingly common in the creation of defense mechanisms for critical infrastructure such as the electric power grid and water treatment plants. Such approaches often use well-known methods from machine learning and system identification, ie, the Multi-Layer Perceptron, Convolutional Neural Network, and Deep Auto Encoders to create process anomaly detectors. Such detectors are then evaluated using data generated from an operational plant or a simulator; rarely is the assessment conducted in real time on a live plant. Regardless of the method to create an anomaly detector, and the data used for performance evaluation, there remain significant challenges that ought to be overcome before such detectors can be deployed with confidence in city-scale plants or large electric power grids. This position paper enumerates such challenges that the authors have faced when\u00a0\u2026", "num_citations": "5\n", "authors": ["309"]}
{"title": "An approach for formal analysis of the security of a water treatment testbed\n", "abstract": " An increase in the number of attacks on cyberphysical systems (CPS) has raised concerns over the vulnerability of critical infrastructure such as water treatment, oil, gas plants, against cyber attacks. Such systems are controlled by an Industrial Control System (ICS) that includes controllers communicating with each other, and with physical sensors and actuators, using a communications network. This paper focuses on a Multiple Security Domain Nondeducibility (MSDND) model to identify the vulnerable points of attack on the system that hide critical information rather than steal it, such as in the STUXNET virus. It is shown how MSDND analysis, conducted on a realistic multi-stage water treatment testbed, is useful in enhancing the security of a water treatment plant. Based on the MSDND analysis, this work offers a thorough documentation on the vulnerable points of attack, invariants used for removing the\u00a0\u2026", "num_citations": "5\n", "authors": ["309"]}
{"title": "Modeling and control of the incremental software test process\n", "abstract": " We present an overview of our current work in modeling and control of the software test process under an incremental life cycle. We first discuss the modeling primitives and their interaction to form a predictive model, and then discuss the direction by which we address the problem of controlling the process.", "num_citations": "5\n", "authors": ["309"]}
{"title": "A Two Dimensional Scheme to Evaluate the Adequacy of Fault Tolerance Testing\n", "abstract": " Virtualized execution environments in 5G network call for a linkage between network management and orchestration. Execution of 3GPP functionalities and management applications in the cloud also presents an opportunity to innovate outside of traditional paradigms. We describe a framework focusing on a virtual execution environment and utilizing agent composition to serve as a platform for realizations with particular goals\u2014for example\u2014in terms of coordination. The framework allows for focusing on the opportunities provided with cloud environment and microservices-based agent composition, and describing relevant aspects of orchestration, while avoiding aspects of orchestration which would bring unnecessary complexity to the analysis. The framework is planned to be used as a basis for research demonstrator later on for implementing 5G use cases.", "num_citations": "5\n", "authors": ["309"]}
{"title": "An application of program unification to priority queue vectorization\n", "abstract": " In this experimental study, we apply the technique of program unification to priority queues. We examine the performance of a variety of unified priority queue implementations on a Cray Y-MP. The scope of the study is restricted to determining if different implementations of priority queues exhibit markedly different performance characteristics under program unification. We found this to be true. In a larger view, this result has interesting consequences in the application of program unification to discrete event simulations on vector or SIMD machines. We find the heap to be a promising data structure in the program unification paradigm.", "num_citations": "5\n", "authors": ["309"]}
{"title": "Deep autoencoders as anomaly detectors: Method and case study in a distributed water treatment plant\n", "abstract": " Industrial Control Systems (ICS) are found in critical infrastructure, such as, water treatment plants and oil refineries. ICS are often the target of cyber-attacks leading to undesirable consequences. It is essential to detect process anomalies resulting from such attacks before appropriate defensive actions are considered. In this work, a deep autoencoder-based anomaly detector (DAE) is proposed. DAE is trained using data collected during normal operation of a plant. The detection effectiveness of three variants of DAE was experimentally evaluated on an operational Secure Water Treatment (SWaT) plant. Further, the amount of plant design knowledge needed to design DAE was compared with that needed to create design-centric approaches for anomaly detection. Experimental results indicate that the proposed DAE, constructed with minimal design knowledge, is effective in detecting process anomalies resulting\u00a0\u2026", "num_citations": "4\n", "authors": ["309"]}
{"title": "A method for testing distributed anomaly detectors\n", "abstract": " Distributed anomaly detectors are deployed in critical infrastructure to raise alerts when the underlying plant deviates from its expected behaviour. A novel method, referred to as SCM, that uses well defined state and command mutation operators, is proposed to test such detectors prior to their deployment. Cyber-attacks, each modelled as a timed-automaton, serve as reference attacks. A potentially large set of attacks is then created by systematically applying the mutation operators to each reference attack. In a case study, SCM was applied to a timed-automata model of a water treatment plant to assess its effectiveness in testing a distributed anomaly detector. Results attest to the value of SCM in identifying weaknesses in an anomaly detector, prior to its deployment, and improving its effectiveness in detecting process anomalies.", "num_citations": "4\n", "authors": ["309"]}
{"title": "On the limits of detecting process anomalies in critical infrastructure\n", "abstract": " Critical infrastructure are Cyber-Physical Systems that provide essential services to the society. Such infrastructure includes plants for power generation and distribution and for water treatment and distribution. Several such plants operate under a high availability constraint. In the presence of ever increasing cyber attacks, as demonstrated by several events in the past, it becomes imperative and challenging for a plant to meet the availability requirement. Such attacks raise the importance of adding to a plant mechanisms for attack prevention, detection, and secure control. Preventive measures aim to control the incoming and outgoing network traffic and prevent unauthorised access to the plant. Detection mechanisms aim at detecting whether the plant is behaving as expected and raise alarms otherwise. Mechanisms for secure control aim at ensuring that the plant remains in a stable state despite an attack. When a\u00a0\u2026", "num_citations": "4\n", "authors": ["309"]}
{"title": "Empirical assessment of methods to detect cyber attacks on a robot\n", "abstract": " An experiment was conducted using a robot to investigate the effectiveness of four methods for detecting cyber attacks and analyzing robot failures. Cyber attacks were implemented on three robots of the same make and model through their wireless control mechanisms. Analysis of experimental data indicates the differences in attack detection effectiveness across the detection methods. A method that compares sensors values at each time step to the average historical values, was the most effective. Further, the attack detection effectiveness was the same or lower in actual robots as compared to simulation. Factors such as attack size and timing, influenced attack detection effectiveness.", "num_citations": "4\n", "authors": ["309"]}
{"title": "Using Markov-Chains to Model Reliability and QoS for Deployed Service-Based Systems\n", "abstract": " An approach for modeling Service-based Systems (SBS) is proposed. The approach is intended as an aid for estimating the reliability and Quality of Service (QoS) attributes of an SBS against changes in its atomic services over a deployment period. A Markov-chain model is used to formally capture the status and evolution of an SBS. Service selection policy is considered in the model of the user operational profile to account for its impact on the reliability and QoS of an SBS. Both static and dynamic analysis techniques are used to obtain the estimates of reliability, availability, and the average response time of an SBS. Simulation studies aimed at investigating the effectiveness of the proposed approach are reported.", "num_citations": "4\n", "authors": ["309"]}
{"title": "The inverted pyramid approach in user interface design for interactive information retrieval\n", "abstract": " This paper extends the Inverted Pyramid approach to the design of user interfaces for systems used to deliver information on request. The focus of this paper is on speech based interfaces to answer a user\u2019s query.The \u2018inverted pyramid approach\u2019is a preferred way of writing by journalists. In brief it states:\u201cstart the article by telling the reader the conclusion\u201d. This style is known as the inverted pyramid for the simple reason that it turns the traditional pyramid style around. Newspapers and news based websites follow this as readers can stop at any time and can still get the most important parts of the article.", "num_citations": "4\n", "authors": ["309"]}
{"title": "Certification of distributed component computing middleware and applications\n", "abstract": " We focus on the issues related to the certification of components and applications conforming to the CORBA 3 standard. CORBA 3 is a standard for Distributed Component Computing (DCC) middleware. Similar standards include Enterprise Java Beans. The specifications for these technologies offer a set of services, such as security, transaction and persistence. Certification of DCC middleware and applications poses several challenging issues discussed in this paper.", "num_citations": "4\n", "authors": ["309"]}
{"title": "A two-semester undergraduate sequence in Software Engineering: Architecture and experience\n", "abstract": " A two-semester sequence in Software Engineering has been offered to Computer Science undergraduates at Purdue University since the fall of 1991. An attempt was made to balance the teaching of theory and practice of software engineering and provide the students with an opportunity to apply some of the techniques learned in the classroom to a controlled development project. The project was selected from an industrial setting and the product developed was returned to industry. We describe the architecture of the offering and our experience during the first offering in Fall 1991 and Spring 1992.", "num_citations": "4\n", "authors": ["309"]}
{"title": "Software and hardware quality assurance: towards a common platform for high reliability\n", "abstract": " The authors point out the conceptual similarity between two quality assurance technologies emerging independently in the worlds of software and hardware. They outline the advantages of these technologies over competing ones and then argue how, by exploiting this similarity and the emerging parallel processing technology, a common platform of integratable tools can be built to perform cost-effective hardware and software testing for complex products of the future. Both techniques use fault modeling for showing the presence of faults in the objects of interest, namely, the program and the circuit. However, the test generation method used in mutation-based testing appears more general than the one used for hardware testing when their functional details are examined. As most useful hardware consists of sequential circuits with memory, the test data generation method used in software testing seems worthy of\u00a0\u2026", "num_citations": "4\n", "authors": ["309"]}
{"title": "Can replay attacks designed to steal water from water distribution systems remain undetected?\n", "abstract": " Industrial Control Systems (ICS) monitor and control physical processes. ICS are found in, among others, critical infrastructures such as water treatment plants, water distribution systems, and the electric power grid. While the existence of cyber-components in an ICS leads to ease of operations and maintenance, it renders the system under control vulnerable to cyber and physical attacks. An experimental study was conducted with replay attacks launched on an operational water distribution (WADI) plant to understand under what conditions an attacker/attack can remain undetected while stealing water. A detection method, based on an input-output Linear Time-invariant system model of the physical process, was developed and implemented in WADI to detect such attacks. The experiments reveal the strengths and limitations of the detection method and challenges faced by an attacker while attempting to steal water\u00a0\u2026", "num_citations": "3\n", "authors": ["309"]}
{"title": "Fault coverage of constrained random test selection for access control: A formal analysis\n", "abstract": " A probabilistic model of fault coverage is presented. This model is used to analyze the variation in the fault detection effectiveness associated with the use of two test selection strategies: heuristics-based and Constrained Random Test Selection (CRTS). These strategies arise in the context of conformance test suite generation for Role Based Access Control (RBAC) systems. The proposed model utilizes coverage matrix based approach for fault coverage analysis. First, two boundary instances of fault distribution are considered and then generalized. The fault coverages of the test suites generated using the heuristics-based and the CRTS strategies, applied to a sample RBAC policy, are then compared through simulation. Finally the simulation results are correlated with a case study.", "num_citations": "3\n", "authors": ["309"]}
{"title": "Test generation for access control systems that employ RBAC policies\n", "abstract": " A method is proposed for generating tests for implementations of Role Based Access Control (RBAC) policies. First step in the method is construction of a finite state model that expresses the desired behavior of an RBAC implementation. Six heuristics are proposed to scale down the model for large systems consisting of thousands of users, roles, and permissions. Next, the model is input to a test generator that employs the existing automata theoretic W-or Wp-methods for test generation. Depending on the heuristic used, a combination of stress and random testing is recommended to enable detection of faults that might be missed by tests generated from the scaled down model. The fault detection effectiveness of the proposed method is evaluated against a fault model that corresponds well with the one used for analyzing the effectiveness of the W-and Wp-methods.", "num_citations": "3\n", "authors": ["309"]}
{"title": "Using supervisory control to synthesize safety controllers for connected spaces\n", "abstract": " A procedure for synthesizing safety controllers for pervasive computing environments is described. Control-theoretic techniques are used in the synthesis. We describe the notions of connected spaces and digital device manuals for modeling pervasive computing environments and devices, respectively. The safety requirements for the environment are specified as a set of safety policies. The safety policies are enforced by the safety controller. The notions of policy relaxation cost of the safety policies and safety criticality ranking of the devices are novel to this work. An experimental evaluation of the synthesis procedure is presented. Results show that the synthesis procedure scales linearly with respect to the density of the set of policies.", "num_citations": "3\n", "authors": ["309"]}
{"title": "A XML based policy-driven management information service\n", "abstract": " This paper presents the design and architecture of a prototype implementation of a XML based policy-driven management information server. It also describes the usage of such a server in building a flexible, extensible architecture for managing heterogeneous distributed systems.", "num_citations": "3\n", "authors": ["309"]}
{"title": "Software fault injection testing on a distributed system-a case study\n", "abstract": " We present a case study on fault injection testing at the interface level between components of a distributed system. We give an introduction to the fault-injection technique and explain the motivation for using this method. We enumerate the research issues involved in the study and describe the methodology used to investigate them. Finally we discuss the conclusions reached and experiences gained as a result of this project. This exercise is likely to help in testing fault tolerant software systems and building tools for fault injection testing.", "num_citations": "3\n", "authors": ["309"]}
{"title": "Experiments with Program unification on the Cray Y\u2010MP\n", "abstract": " Program unification is a technique for source\u2010to\u2010source transformation of code for enhanced execution performance on vector and SIMD architectures. This work focuses on simple examples of program unification to explain the methodology and demonstrate its promise as a practical technique for improved performance. Using simple examples to explain how unification is done, we outline two experiments in the simulation domain that benefit from unification, namely Monte Carlo and discrete\u2010event simulation. Empirical tests of unified code on a Cray Y\u2010MP multiprocessor show that unification improves execution performance by a factor of roughly 8 for given application. The technique is general in that it can be applied to computation\u2010intensive programs in various data\u2010parallel application domains.", "num_citations": "3\n", "authors": ["309"]}
{"title": "An Overview of Compiler-integrated Testing\n", "abstract": " Audiovisual material available from this site has been copied and communicated to you under a Screenrights licence pursuant to Section 113P of the Copyright Act 1968 solely for the educational purposes of your institution. No other use is authorised. For more information please contact Screenrights at [email protected] or at www. screenrights. org.", "num_citations": "3\n", "authors": ["309"]}
{"title": "Scanning the Cycle: Timing-based Authentication on PLCs\n", "abstract": " Programmable Logic Controllers (PLCs) are a core component of an Industrial Control System (ICS). However, if a PLC is compromised or the commands sent across a network from the PLCs are spoofed, consequences could be catastrophic. In this work, a novel technique to authenticate PLCs is proposed that aims at raising the bar against powerful attackers while being compatible with real-time systems. The proposed technique captures timing information for each controller in a non-invasive manner. It is argued that Scan Cycle is a unique feature of a PLC that can be approximated passively by observing network traffic. An attacker that spoofs commands issued by the PLCs would deviate from such fingerprints. To detect replay attacks a PLC Watermarking technique is proposed. PLC Watermarking models the relation between the scan cycle and the control logic by modeling the input/output as a function of\u00a0\u2026", "num_citations": "2\n", "authors": ["309"]}
{"title": "Blockchain Enabled Electric Vehicle Charging Infrastructure\n", "abstract": " Increasing pollution and lack of an alternate fuel have made electric vehicles the best alternative, the proper adoption of electric vehicles requires a sophisticated electric vehicle charging infrastructure, our proposed application based on blockchain technology will make the whole process secure and convenient for users. In order to implementation of blockchain technology-enabled coordination between charging stations and electric vehicle users, this paper proposes an approach to implementation and operation. This paper focuses to develop an approach of a secure transaction for identification of charging station such as fast charging or normal charging and booking of slots from remote locations automatically. The proposed approach will plan and schedule the charging time at a defined cost.", "num_citations": "2\n", "authors": ["309"]}
{"title": "Using datasets from industrial control systems for cyber security research and education\n", "abstract": " The availability of high-quality benchmark datasets is an important prerequisite for research and education in the cyber security domain. Datasets from realistic systems offer a platform for researchers to develop and test novel models and algorithms. Such datasets also offer students opportunities for active and project-centric learning. In this paper, we describe six publicly available datasets from the domain of Industrial Control Systems (ICS). Five of these datasets are obtained through experiments conducted in the context of operational ICS while the sixth is obtained from a widely used simulation tool, namely EPANET, for large scale water distribution networks. This paper presents two studies on the use of the datasets. The first study uses the dataset from a live water treatment plant. This study leads to a novel and explainable anomaly detection method based upon Timed Automata and Bayesian Networks. The\u00a0\u2026", "num_citations": "2\n", "authors": ["309"]}
{"title": "NoiSense: detecting data integrity attacks on sensor measurements using hardware based fingerprints\n", "abstract": " In recent years fingerprinting of various physical and logical devices has been proposed with the goal of uniquely identifying users or devices of mainstream IT systems such as PCs, Laptops and smart phones. On the other hand, the application of such techniques in Cyber-Physical Systems (CPS) is less explored due to various reasons, such as difficulty of direct access to critical systems and the cost involved in faithfully reproducing realistic scenarios. In this work we evaluate the feasibility of using fingerprinting techniques in the context of realistic Industrial Control Systems related to water treatment and distribution. Based on experiments conducted with 44 sensors of six different types, it is shown that noise patterns due to microscopic imperfections in hardware manufacturing can be used to uniquely identify sensors in a CPS with up to 97% accuracy. The proposed technique can be used in to detect physical\u00a0\u2026", "num_citations": "2\n", "authors": ["309"]}
{"title": "Comparison of corrupted sensor data detection methods in detecting stealthy attacks on cyber-physical systems\n", "abstract": " Effectiveness of seven methods for detecting stealthy attacks on Cyber Physical Systems (CPS) was investigated using an experimental study. The Amigobot robot was used as the CPS. The experiments were conducted in simulation as well as on the physical robot. Three types of stealthy attacks were implemented: surge, bias, and geometric. Two variations of Cumulative Sum (CUSUM) method for detecting attacks were evaluated: partial and full physics. Four attack scenarios were implemented. Results from the experiments indicate that stealthy attacks could remain undetected by the CUSUM methods for some attack scenarios. In addition to the CUSUM-based methods, a set of five methods to complement CUSUM were implemented and their effectiveness assessed. While the additional methods do improve the effectiveness of CUSUM-based methods, some attacks remained undetected regardless of which\u00a0\u2026", "num_citations": "2\n", "authors": ["309"]}
{"title": "Effective Crowdsourcing for Software Feature Ideation in Online Co-Creation Forums.\n", "abstract": " Many software companies are creating firm-centric online forums for customer engagement. These forums can be an effective crowdsourcing platform for software product feature ideation and co-creation with the end users. We studied the community interaction data from the ideation forums of two software providers. Link analysis revealed that a small core community was responsible for generating a large proportion of the implemented ideas. This indicated the need to identify key users in the online forum. Our analysis showed the applicability of centrality measures such as betweenness in ranking key users. We also found that commenting was likely to produce better community formation amongst the participants than voting.", "num_citations": "2\n", "authors": ["309"]}
{"title": "Cybernetics\n", "abstract": " The topic, originally defined as the study of communication and control in the animal and the machine, is reviewed under headings including history, feedback and servomechanisms, neurophysiology and digital computing, Shannon's Information Theory, artificial intelligence, second\u2010order cybernetics, management, sociocybernetics and finally the Gaia hypothesis, which refers to the biologic regulation of global variables. Second\u2010order cybernetics, or cybernetics that includes the observer, provides insights into management and sociocybernetics.", "num_citations": "2\n", "authors": ["309"]}
{"title": "DIG: A tool for software process data extraction and grooming\n", "abstract": " A data collection and grooming tool named DIG is reported. The tool allows the management of a software process to be able to extract data from a variety of project repositories, groom the data, and generate reports. The reports are aimed at assisting the management in the control of the software development process. DIG allows management to build a complete representation of the productivity data in order to better tease out the parameters and understand the causes of variation in the data. In addition, the data obtained and groomed by DIG may be used to calibrate and apply process control models.", "num_citations": "2\n", "authors": ["309"]}
{"title": "Synthesis of a safety controller for ConnectedSpaces using supervisory control\n", "abstract": " Baskar Sridharan\u2217, Aditya P. Mathur\u2020 Kai-Yuan Cai\u2021 Software Engineering Research Center, Dept. of Automatic Control, Dept. of Computer Science, Beijing University of Aeronautics Purdue University, and Astronautics, West Lafayette, IN, USA Beijing 100083, China {baskars, apm}@ cs. purdue. edu kyc@ ns. dept3. buaa. edu. cn", "num_citations": "2\n", "authors": ["309"]}
{"title": "Development of an Infrastructure for the Management of Smart Homes\n", "abstract": " 2 SmaxtHomes 2.1 What is a SmaxtHome? 2.2 Why SmartHomBs'!.. 2.3 Why manage SmartHomes? 2.'1 Requirements for the management of SmartHomes. 2.5 Event notiiication and correlation. 2.6 Embedded (EA) versus distributed applications (DA)", "num_citations": "2\n", "authors": ["309"]}
{"title": "Testing distributed systems\n", "abstract": " CiNii \u8ad6\u6587 - Testing distributed systems CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7 \u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092 \u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 Testing distributed systems MATHUR AP \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MATHUR AP \u53ce\u9332\u520a\u884c\u7269 Status Report of NSF and SERC Status Report of NSF and SERC, 1999 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Scenario-Based Web Services Testing with Distributed Agents TSAI Wei-Tek , PAUL Ray , YU Lian , SAIMI Akihiro , CAO Zhibin IEICE transactions on information and systems 86(10), 2130-2144, 2003-10-01 \u53c2\u8003\u6587\u732e59\u4ef6 \u88ab\u5f15\u7528\u6587\u732e2\u4ef6 CiNii\u5229\u7528\u8005\u30a2\u30f3\u30b1\u30fc\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10011766353 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u306b/\u2026", "num_citations": "2\n", "authors": ["309"]}
{"title": "E ect of Test Set Minimization on the Fault Detection E ectiveness of the All-Uses Criterion\n", "abstract": " Size and code coverage are important attributes of a set of tests. When a program P is executed on elements of the test set T, we can observe the fault detecting capacity of T for P. We can also observe the degree to which T induces code coverage on P according to some coverage criterion. We would like to know whether it is the size of T or the coverage of T on P which determines the fault detection e ectiveness of T for P. To address this issue we ask the following question: While keeping coverage constant, what is the e ect on fault detection of reducing the size of a test set? We report results from an empirical study using the all-uses criterion as the coverage measure.", "num_citations": "2\n", "authors": ["309"]}
{"title": "Experience in using three testing tools for research and education in software engineering\n", "abstract": " It is a common belief that good software tools are necessary to support both research and education in software engineering. The authors document their experience, in support of this belief, with two data flow testing tools named ASSET and ATAC and one mutation testing tool named MOTHRA. These tools have been in use at Purdue University in research projects related to software testing and reliability. The tools have also been used in both undergraduate and graduate courses in software engineering.<>", "num_citations": "2\n", "authors": ["309"]}
{"title": "Unified stochastic simulations for vector machines: Empirical results\n", "abstract": " We investigate the efficacy of< l recently proposed vectorization and concurrentization senemc known as Program Unification. The technique entails a source-La-source transformation of code designed for vector machines. As a pre-compilation technique, unification is applicable to programs which satisfy two criteria, namely, poor vectorizability and a need for repeated execution. In this study we present a set of experimental results on program unification in the stochastic simulation domain. The intent is to demonstrate how a variety of simulation applications including discrete--event simulations (eg, queueing systems), Monte Carlo simulations (eg, estimation of multidimensional integrals), and statistical simulations (eg, computing distributions of statistics) lend themselves to unifying schemes with encouraging utilization and speedup characteristics. All experiments reported here were conducted on an Alliant FXj8.", "num_citations": "2\n", "authors": ["309"]}
{"title": "Cascading effects of cyber-attacks on interconnected critical infrastructure\n", "abstract": " Modern critical infrastructure, such as a water treatment plant, water distribution system, and power grid, are representative of Cyber Physical Systems (CPSs) in which the physical processes are monitored and controlled in real time. One source of complexity in such systems is due to the intra-system interactions and inter-dependencies. Consequently, these systems are a potential target for attackers. When one or more of these infrastructure are attacked, the connected systems may also be affected due to potential cascading effects. In this paper, we report a study to investigate the cascading effects of cyber-attacks on two interdependent critical infrastructure namely, a Secure water treatment plant (SWaT) and a Water Distribution System (WADI).", "num_citations": "1\n", "authors": ["309"]}
{"title": "Challenges in Secure Engineering of Critical Infrastructure Systems\n", "abstract": " Modern critical infrastructure (CI), such as water supply, smart power grids, and transportation networks, face major security challenges that arise due to complex interactions between software and physical components as well as human operators. Such systems are an attractive target for attackers who intend to disrupt the safe, normal operation of CI by exploiting vulnerabilities in software components such as the supervisory control and data acquisition (SCADA) workstations and programmable logic controllers (PLCs). In this reference paper, we elaborate on problems and challenges learned from our own experience in automating security analysis, assessment, and defense mechanisms for CI. These challenges are presented in the context of two real-world CI systems-namely, a water treatment plant and a water distribution system.", "num_citations": "1\n", "authors": ["309"]}
{"title": "Assessment of a method for detecting process anomalies using digital-twinning\n", "abstract": " Several methods exist for detecting process anomalies resulting from cyber-attacks on critical infrastructure. The assessment of such methods could be conducted using simulation or directly on a realistic operational testbed. While the results of an assessment on a testbed may be more authentic than those carried out using simulation, conducting such experiments is fraught with challenges such as the time required to set up and launch attacks thus limiting the variety and number of attacks launched. To overcome such limitations, while maintaining the reliability of the outcome of the assessment, an approach based on timed automata models of a critical infrastructure was investigated. The investigation involved development of a digital twin for a 6-stage water treatment plant. A design-centric anomaly detection method, as well as an attack launcher, were integrated with the model and experiments were performed\u00a0\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "State estimation-based attack detection in cyber-physical systems: Limitations and solutions\n", "abstract": " In this chapter, we present a detailed case study regarding model-based attack detection procedures for Cyber-Physical Systems (CPSs). In particular data from a real-world water treatment plant is collected and analyzed. Using this dataset and the sub-space system identification technique, an input-output Linear Time Invariant (LTI) model for the water treatment plant is obtained. This model is used to derive a Kalman filter to estimate the evolution of the system dynamics. Then, residual variables are constructed by subtracting data coming from real-world water treatment system and the estimates obtained by using the Kalman filter. We use these residuals to evaluate the performance of statistical detectors namely the Bad-Data and the CUmulative Sum (CUSUM) change detection procedures. First, the limitations of these model-based statistical techniques are shown. Then, an attack detection technique is\u00a0\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "Assessment of school readiness of children and factors associated with risk of inadequate school readiness in Ujjain, India: an observational study\n", "abstract": " ObjectiveSchool readiness is a condition or state indicating that the child is ready to learn in a formal educational set-up. The objective of this study was to estimate the prevalence of and factors associated with school readiness in urban schoolchildren in Ujjain, India.MethodsThis cross-sectional study was conducted from February 2016 to March 2017. Two English-medium schools were conveniently selected. All children aged 5\u20137 years were eligible to participate. A subscale of Differential Ability Scales-Second Edition, namely \u2018school readiness scale\u2019, was used to assess school readiness in three major domains\u2014early number concept, matching letter-like forms and phonological processing. Data on factors associated with school readiness were collected through parent interview. Quantile regression analysis was used to explore school readiness scores.ResultsThis study included 203 school-going children\u00a0\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "Empirical assessment of corrupt sensor data detection methods in a robot\n", "abstract": " An experiment was conducted to investigate the response of a robot to cyber attacks and the effectiveness of methods to detect such attacks. The experiment was run in simulation as well as on an actual robot. To ensure validity of results, cyber attacks were implemented on three robots of the same make and model through their wireless control mechanisms. Attacks were launched to investigate their feasibility, impact, and the effectiveness of the detection methods. Analysis of experimental data indicates that, among the several methods examined, the one which compares sensor values to the average historical values, is the most effective. In some experiments, the effectiveness of various methods was found to be lower in actual robots as compared to that in simulation. Thus, when practically feasible, it is important to test security countermeasures in realistic environments. Furthermore, factors such as attack size\u00a0\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "Quantitative Modeling for Incremental Software Process Control\n", "abstract": " A software development process modeling framework is motivated and constructed under the formalism of State Modeling. The approach interconnects instances of general development-activity modeling components into a composite system that represents the software development process of the target organization. The composite system is then constrained according to the interdependencies in the work to be completed. Simulation results are presented, and implications for control-theoretic decision support (i.e., state variable control) are briefly discussed.", "num_citations": "1\n", "authors": ["309"]}
{"title": "The Case for a School of Computer Science and Engineering at Purdue University1\n", "abstract": " The Purdue Computer Science Department has evolved significantly over the years, and there are now compelling reasons why it should be moved from the College of Science to the College of Engineering. Like other well-established engineering disciplines, computer science deals with real processes and artifacts, its primary intention being the production of software and hardware of direct value to society. It uses the basic principles and tools of engineering design and analysis to produce these artifacts. Most graduates of Computer Science work alongside engineers. Relocating the Department to the College of Engineering will create a multitude of new opportunities for students, allow the outstanding CS faculty to fully exploit their research potential, and better align Purdue with its peer institutions without in any way sacrificing linkages with other academic units at Purdue. Based on the experience of several other universities, looking back in a few years one will likely conclude that the move was just the right decision.", "num_citations": "1\n", "authors": ["309"]}
{"title": "On the Adequacy of Statecharts as a Source of Tests for Cryptographic Protocols\n", "abstract": " The effectiveness of statecharts as a tool to express the desired behavior of security protocols and a source of tests for their implementations was investigated. Specifically, TLS protocol was modeled as a statechart and tests generated from its flattened version. The GnuTLS implementation of the protocol was then tested against the generated tests. The MC/DC coverage of different components of the implementation varied from 51% to 81%. A \"what if\" analysis revealed that while some defects in the uncovered code will not lead to any security vulnerability due to in-built fault tolerance, others might lead to improper authentication, integrity failure, session hijacking, denial of service, and loss of confidentiality. The analysis suggests that statecharts alone might not be an adequate tool as a source of tests for implementations of security protocols and that tests so generated must be augmented through other formal\u00a0\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "On the Equivalence of Two Model Based Test Generation Methods for Graphical User Interfaces\n", "abstract": " The E-method for the automatic generation of tests for graphical user interfaces (GUIs) is based on Event Sequence Graph (ESG) model of the expected GUI behavior. The W-method and its variants like the Wp method, are based on Finite State Machines (FSM) and can also be used for the same purpose. We show that tests generated using the E-and the W-methods have the same fault detection effectiveness when the FSM distinguishability index is known and used for test generation; in the absence of this knowledge, the ESG method could lead to fewer test cases thereby leaving undetected faults in the GUI under test. Based on the fault detection and modeling characteristics of the two methods, we make recommendations to help a tester decide which method to use in a given scenario. A prototype tool named BEASTT incorporates both the E-and the W-methods.", "num_citations": "1\n", "authors": ["309"]}
{"title": "Adequacy of Statecharts as a Source of Tests for Implementations of Cryptographic Protocols\n", "abstract": " Statecharts, now an integral part of the Unified Modeling Language (UML), serve as a requirement and/or a design specification. The effectiveness of statecharts as a tool to express the desired behavior of security protocols and a source of tests was investigated. Specifically, the TLS protocol was modeled as a statechart and tests generated from the flattened version of the model. The GnuTLS implementation of the TLS protocol (about 40KLOC in size) was then tested against the generated tests and their adequacy assessed using MC/DC coverage. The MC/DC coverage of different portions of the implementation varied from 51% to 81%. A \u201cwhat if\u201d analysis revealed that while some defects in the uncovered portion of the code will not lead to any security vulnerability due to in-built error detection, a few others might lead to improper authentication, integrity failure, session hijacking, denial of service, and loss of confidentiality. The analysis suggests that statecharts alone might not be an adequate tool as a source of tests for implementations of security protocols and that tests so generated must be augmented through other formal means such as random testing, stress testing, and code coverage analysis.", "num_citations": "1\n", "authors": ["309"]}
{"title": "Dependable and Secure Computing\n", "abstract": " [Front cover] Page 1 IEEE TRANSA CTIONS ON DEPEND ABLE AND SECURE COMPUTING V o l. 1, No. 4, October-December 2004 IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING A publication of the IEEE Computer Society VOLUME 1 NUMBER 4 ITDSCM (ISSN 1545-5971) http://computer.org tdsc@computer.org OCTOBER-DECEMBER 2004 PAPERS Change-Point Monitoring for the Detection of DoS Attacks H. Wang, D. Zhang, and KG Shin .................................................................................................................................... Defending a P2P Digital Preservation System B. Parno and M. Roussopoulos ........................................................................................................................................ Effective Fault Treatment for Improving the Dependability of COTS and Legacy-Based Applications A. Bondavalli, S. Chiaradonna, D. Cotroneo, and L. Romano............................................................................................ An Model V.\u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "On Errors and Failures in Distributed Systems Built to CORBA and COM Standards\n", "abstract": " CORBA and COM are standards for building distributed object oriented applications usually composed of several components. We report on the sources and type of errors and failures in such applications. These were identified by an examination of the specifications of CORBA and COM. While errors creep into an application during design and coding, failures arise during the execution of the application. Information presented here is considered useful for the design and use of a testing tool that can help assess (a) the completeness of a test of an application and (b) the behavior of the application in the presence of component failures. Applications with built-in fault-tolerance can benefit from such a tool in that the tool can help evaluate the \u201cgoodness\u201d of the fault tolerance mechanism.", "num_citations": "1\n", "authors": ["309"]}
{"title": "Testing for fault tolerance\n", "abstract": " Software is being used for building applications requiring extreme dependability. In many cases, systems must have high availability and fault tolerance. With the increasing complexity of software, testing becomes di cult and expensive. This report summarizes the goals and research issues involved in the area of testing distributed systems. It describes and analyzes prior work done in the area of fault injection testing. It describes sources of errors and failures in distributed systems that are compliant to distributed object architectures like CORBA and COM. A methodology is proposed for testing distributed software.", "num_citations": "1\n", "authors": ["309"]}
{"title": "Parallel parsing on a transputer network.\n", "abstract": " Parallel parsing on a transputer network | Computer Systems Science and Engineering ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Computer Systems Science and Engineering Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsComputer Systems Science and EngineeringVol. , No. Parallel parsing on a transputer network article Parallel parsing on a transputer network Share on Authors: Walter Batchelor , Iii Ligon profile image Walter B. Ligon View Profile , Aditya P Mathur profile image Aditya P. Mathur View Profile Authors Info & Affiliations Publication: Computer Systems Science and EngineeringJuly 1992 0citation 0 Downloads Metrics ! \u2026", "num_citations": "1\n", "authors": ["309"]}
{"title": "Feedback and Adaptive ControlforSoftware Testing\n", "abstract": " 1 Supported by the National Natural Science Foundation of China (60233020), the \u201c863\u201d Programme of China (2001AA113192) and the Aviation Science Foundation of China (01F51025). 2 Supported in part by SERC and NSF", "num_citations": "1\n", "authors": ["309"]}