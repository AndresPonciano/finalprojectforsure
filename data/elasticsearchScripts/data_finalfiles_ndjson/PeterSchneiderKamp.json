{"title": "Twenty-five comparators is optimal when sorting nine inputs (and twenty-nine for ten)\n", "abstract": " This paper describes a computer-assisted non-existence proof of 9-input sorting networks consisting of 24 comparators, hence showing that the 25-comparator sorting network found by Floyd in 1964 is optimal. As a corollary, we obtain that the 29-comparator network found by Waksman in 1969 is optimal when sorting 10 inputs. This closes the two smallest open instances of the optimal-size sorting network problem, which have been open since the results of Floyd and Knuth from 1966 proving optimality for sorting networks of up to 8 inputs. The proof involves a combination of two methodologies: one based on exploiting the abundance of symmetries in sorting networks, and the other based on an encoding of the problem to that of satisfiability of propositional logic. We illustrate that, while each of these can single-handedly solve smaller instances of the problem, it is their combination that leads to the more efficient\u00a0\u2026", "num_citations": "51\n", "authors": ["679"]}
{"title": "Synthesizing shortest linear straight-line programs over GF (2) using SAT\n", "abstract": " Non-trivial linear straight-line programs over the Galois field of two elements occur frequently in applications such as encryption or high-performance computing. Finding the shortest linear straight-line program for a given set of linear forms is known to be MaxSNP-complete, i.e., there is no \u03b5-approximation for the problem unless P\u2009=\u2009NP.               This paper presents a non-approximative approach for finding the shortest linear straight-line program. In other words, we show how to search for a circuit of XOR gates with the minimal number of such gates. The approach is based on a reduction of the associated decision problem (\u201cIs there a program of length k?\u201d) to satisfiability of propositional logic. Using modern SAT solvers, optimal solutions to interesting problem instances can be obtained.", "num_citations": "41\n", "authors": ["679"]}
{"title": "Efficient certified resolution proof checking\n", "abstract": " We present a novel propositional proof tracing format that eliminates complex processing, thus enabling efficient (formal) proof checking. The benefits of this format are demonstrated by implementing a proof checker in C, which outperforms a state-of-the-art checker by two orders of magnitude. We then formalize the theory underlying propositional proof checking in Coq, and extract a correct-by-construction proof checker for our format from the formalization. An empirical evaluation using 280 unsatisfiable instances from the 2015 and 2016 SAT competitions shows that this certified checker usually performs comparably to a state-of-the-art non-certified proof checker. Using this format, we formally verify the recent 200 TB proof of the Boolean Pythagorean Triples conjecture.", "num_citations": "32\n", "authors": ["679"]}
{"title": "Sorting networks: to the end and back again\n", "abstract": " New properties of the front and back ends of sorting networks are studied, illustrating their utility when searching for bounds on optimal networks. Search focuses first on the \u201cout-sides\u201d of the network and then on the inner part. Previous works focused on properties of the front end to break symmetries in the search. The new, out-side-in, properties shed understanding on how sorting networks sort, and facilitate the computation of new bounds on optimality. We present new, faster, parallel sorting networks for 17\u201320 inputs. For 17 inputs, we show that no sorting network using less layers exists.", "num_citations": "29\n", "authors": ["679"]}
{"title": "Optimal base encodings for pseudo-Boolean constraints\n", "abstract": " This paper formalizes the optimal base problem, presents an algorithm to solve it, and describes its application to the encoding of Pseudo-Boolean constraints to SAT. We demonstrate the impact of integrating our algorithm within the Pseudo-Boolean constraint solver MiniSat                 \u2009+\u2009. Experimentation indicates that our algorithm scales to bases involving numbers up to 1,000,000, improving on the restriction in MiniSat                 \u2009+\u2009 to prime numbers up to 17. We show that, while for many examples primes up to 17 do suffice, encoding with respect to optimal bases reduces the CNF sizes and improves the subsequent SAT solving time for many examples.", "num_citations": "23\n", "authors": ["679"]}
{"title": "Polytool: Polynomial interpretations as a basis for termination analysis of logic programs\n", "abstract": " Our goal is to study the feasibility of porting termination analysis techniques developed for one programming paradigm to another paradigm. In this paper, we show how to adapt termination analysis techniques based on polynomial interpretations\u2014very well known in the context of term rewrite systems\u2014to obtain new (nontransformational) termination analysis techniques for definite logic programs (LPs). This leads to an approach that can be seen as a direct generalization of the traditional techniques in termination analysis of LPs, where linear norms and level mappings are used. Our extension generalizes these to arbitrary polynomials. We extend a number of standard concepts and results on termination analysis to the context of polynomial interpretations. We also propose a constraint-based approach for automatically generating polynomial interpretations that satisfy the termination conditions. Based on this\u00a0\u2026", "num_citations": "21\n", "authors": ["679"]}
{"title": "Sorting nine inputs requires twenty-five comparisons\n", "abstract": " This paper describes a computer-assisted non-existence proof of 9-input sorting networks consisting of 24 comparators, hence showing that the 25-comparator sorting network found by Floyd in 1964 is optimal. As a corollary, the 29-comparator network found by Waksman in 1969 is optimal when sorting 10 inputs.This closes the two smallest open instances of the optimal-size sorting network problem, which have been open since the results of Floyd and Knuth from 1966 proving optimality for sorting networks of up to 8 inputs.", "num_citations": "19\n", "authors": ["679"]}
{"title": "The quest for optimal sorting networks: Efficient generation of two-layer prefixes\n", "abstract": " Previous work identifying depth-optimal n-channel sorting networks for 9 \u2264 n \u2264 16 is based on exploiting symmetries of the first two layers. However, the naive generate-and-test approach typically applied does not scale. This paper revisits the problem of generating two-layer prefixes modulo symmetries. An improved notion of symmetry is provided and a novel technique based on regular languages and graph isomorphism is shown to generate the set of non-symmetric representations. An empirical evaluation demonstrates that the new method outperforms the generate-and-test approach by orders of magnitude and easily scales until n = 40.", "num_citations": "19\n", "authors": ["679"]}
{"title": "Optimal-depth sorting networks\n", "abstract": " We solve a 40-year-old open problem on depth optimality of sorting networks. In 1973, Donald E. Knuth detailed sorting networks of the smallest depth known for n\u2264 16 inputs, quoting optimality for n\u2264 8 (Volume 3 of \u201cThe Art of Computer Programming\u201d). In 1989, Parberry proved optimality of networks with 9\u2264 n\u2264 10 inputs. We present a general technique for obtaining such results, proving optimality of the remaining open cases of 11\u2264 n\u2264 16 inputs. Exploiting symmetry, we construct a small set R n of two-layer networks such that: if there is a depth-k sorting network on n inputs, then there is one whose first layers are in R n. For each network in R n, we construct a propositional formula whose satisfiability is necessary for the existence of a depth-k sorting network. Using an off-the-shelf SAT solver we prove optimality of the sorting networks listed by Knuth. For n\u2264 10 inputs, our algorithm is orders of magnitude\u00a0\u2026", "num_citations": "16\n", "authors": ["679"]}
{"title": "Formally verifying the solution to the Boolean Pythagorean triples problem\n", "abstract": " The Boolean Pythagorean Triples problem asks: does there exist a binary coloring of the natural numbers such that every Pythagorean triple contains an element of each color? This problem was first solved in 2016, when Heule, Kullmann and Marek encoded a finite restriction of this problem as a propositional formula and showed its unsatisfiability. In this work we formalize their development in the theorem prover Coq. We state the Boolean Pythagorean Triples problem in Coq, define its encoding as a propositional formula and establish the relation between solutions to the problem and satisfying assignments to the formula. We verify Heule et al.\u2019s proof by showing that the symmetry breaks they introduced to simplify the propositional formula are sound, and by implementing a correct-by-construction checker for proofs of unsatisfiability based on reverse unit propagation.", "num_citations": "12\n", "authors": ["679"]}
{"title": "Sorting networks: the end game\n", "abstract": " This paper studies properties of the back end of a sorting network and illustrates the utility of these in the search for networks of optimal size or depth. All previous works focus on properties of the front end of networks and on how to apply these to break symmetries in the search. The new properties help shed understanding on how sorting networks sort and speed-up solvers for both optimal size and depth by an order of magnitude.", "num_citations": "12\n", "authors": ["679"]}
{"title": "Optimizing the AES S-Box using SAT.\n", "abstract": " In this paper we describe the implementation of a technique for minimizing XOR circuits used in cryptographic algorithms. More precisely, we present our work from [4] for encoding this synthesis problem to SAT with a focus on the case study of optimizing an important component of the Advanced Encryption Standard (AES)[8]. In addition to these previously published contributions, we report on novel encouraging experimental results that allow us to actually prove optimality of the results obtained.The AES algorithm consists of the (repeated) application of four steps. The main step for introducing non-linearity is the SubBytes step that is based on a so-called S-box. This S-box is a transformation based on multiplicative inverses in GF (28) combined with an invertible affine transformation. This step can be decomposed into two linear parts and a minimal non-linear part. We focus on the optimization of the linear parts, in particular the first one (called the \u201ctop matrix\u201d in [2]).", "num_citations": "12\n", "authors": ["679"]}
{"title": "repAIrC: A tool for ensuring data consistency by means of active integrity constraints\n", "abstract": " Consistency of knowledge repositories is of prime importance in organization management. Integrity constraints are a well-known vehicle for specifying data consistency requirements in knowledge bases; in particular, active integrity constraints go one step further, allowing the specification of preferred ways to overcome inconsistent situations in the context of database management. This paper describes a tool to validate an SQL database with respect to a given set of active integrity constraints, proposing possible repairs in case the database is inconsistent. The tool is able to work with the different kinds of repairs proposed in the literature, namely simple, founded, well-founded and justified repairs. It also implements strategies for parallelizing the search for them, allowing the user both to compute partitions of independent or stratified active integrity constraints, and to apply these partitions to find repairs of inconsistent databases efficiently in parallel.", "num_citations": "10\n", "authors": ["679"]}
{"title": "Applying sorting networks to synthesize optimized sorting libraries\n", "abstract": " This paper presents an application of the theory of sorting networks to facilitate the synthesis of optimized general-purpose sorting libraries. Standard sorting libraries are often based on combinations of the classic Quicksort algorithm with insertion sort applied as base case for small, fixed, numbers of inputs. Unrolling the code for the base case by ignoring loop conditions eliminates branching, resulting in code equivalent to a sorting network. This enables further program transformations based on sorting network optimizations, and eventually the synthesis of code from sorting networks. We show that, if considering the number of comparisons and swaps, the theory predicts no real advantage of this approach. However, significant speed-ups are obtained when taking advantage of instruction level parallelism and non-branching conditional assignment instructions, both of which are common in modern CPU\u00a0\u2026", "num_citations": "9\n", "authors": ["679"]}
{"title": "System design of an open-source cloud-based framework for internet of drones application\n", "abstract": " Unmanned Aerial Vehicles (UAV) are increasingly gaining interest in Internet of Drones (IoD) applications for automatizing the labor-intensive tasks. They are used in various areas such as infrastructure inspection. However, UAVs have limited energy resources and computational processing capabilities, which prevents them from running applications onboard and accessing the internet for gaining knowledge about their mission. In order to address these challenges imposed by limited resource on the drone, we propose a new cloud system infrastructure for building open-source IoD applications. We have designed a client-server architecture which hosts the drone as a client and the cloud as a scalable server. For validating the developed IoD application, an open-source drone simulator and flight controller are adopted to perform the tests. The overall architecture of a drone-cloud framework is presented along with\u00a0\u2026", "num_citations": "7\n", "authors": ["679"]}
{"title": "Optimizing sorting algorithms by using sorting networks\n", "abstract": " In this paper, we show how the theory of sorting networks can be applied to synthesize optimized general-purpose sorting libraries. Standard sorting libraries are often based on combinations of the classic Quicksort algorithm, with insertion sort applied as base case for small, fixed, numbers of inputs. Unrolling the code for the base case by ignoring loop conditions eliminates branching, resulting in code equivalent to a sorting network. By replacing it with faster sorting networks, we can improve the performance of these algorithms. We show that by considering the number of comparisons and swaps alone we are not able to predict any real advantage of this approach. However, significant speed-ups are obtained when taking advantage of instruction level parallelism and non-branching conditional assignment instructions, both of which are common in modern CPU architectures. Furthermore, a close control\u00a0\u2026", "num_citations": "7\n", "authors": ["679"]}
{"title": "Formalizing size-optimal sorting networks: Extracting a certified proof checker\n", "abstract": " Since the proof of the four color theorem in 1976, computer-generated proofs have become a reality in mathematics and computer science. During the last decade, we have seen formal proofs using verified proof assistants being used to verify the validity of such proofs.               In this paper, we describe a formalized theory of size-optimal sorting networks. From this formalization we extract a certified checker that successfully verifies computer-generated proofs of optimality on up\u00a0to 8\u00a0inputs. The checker relies on an untrusted oracle to shortcut the search for witnesses on more than 1.6 million NP-complete subproblems.", "num_citations": "7\n", "authors": ["679"]}
{"title": "Formally proving size optimality of sorting networks\n", "abstract": " Recent successes in formally verifying increasingly larger computer-generated proofs have relied extensively on (a) using oracles, to find answers for recurring subproblems efficiently, and (b) extracting formally verified checkers, to perform exhaustive case analysis in feasible time. In this work we present a formal verification of optimality of sorting networks on up to 9 inputs, making it one of the largest computer-generated proofs that has been formally verified. We show that an adequate pre-processing of the information provided by the oracle is essential for feasibility, as it improves the time required by our extracted checker by several orders of magnitude.", "num_citations": "6\n", "authors": ["679"]}
{"title": "Integrity constraints for general-purpose knowledge bases\n", "abstract": " Integrity constraints in databases have been studied extensively since the 1980s, and they are considered essential to guarantee database integrity. In recent years, several authors have studied how the same notion can be adapted to reasoning frameworks, in such a way that they achieve the purpose of guaranteeing a system\u2019s consistency, but are kept separate from the reasoning mechanisms.                 In this paper we focus on multi-context systems, a general-purpose framework for combining heterogeneous reasoning systems, enhancing them with a notion of integrity constraints that generalizes the corresponding concept in the database world.", "num_citations": "6\n", "authors": ["679"]}
{"title": "Optimizing a certified proof checker for a large-scale computer-generated proof\n", "abstract": " In recent work, we formalized the theory of optimal-size sorting networks with the goal of extracting a verified checker for the large-scale computer-generated proof that 25\u00a0comparisons are optimal when sorting 9\u00a0inputs, which required more than a decade of CPU time and produced 27\u00a0GB of proof witnesses. The checker uses an untrusted oracle based on these witnesses and is able to verify the smaller case of 8\u00a0inputs within a couple of days, but it did not scale to the full proof for 9 inputs. In this paper, we describe several non-trivial optimizations of the algorithm in the checker, obtained by appropriately changing the formalization and capitalizing on the symbiosis with an adequate implementation of the oracle. We provide experimental evidence of orders of magnitude improvements to both runtime and memory footprint for 8\u00a0inputs, and actually manage to check the full proof for 9\u00a0inputs.", "num_citations": "6\n", "authors": ["679"]}
{"title": "Static termination analysis for Prolog using term rewriting and SAT solving\n", "abstract": " The dissertation \u201cStatic Termination Analysis for Prolog using Term Rewriting and SAT Solving\u201d (Schneider-Kamp in Dissertation, RWTH Aachen University, 2008) presents a fresh approach to automated termination analysis of Prolog programs. This approach is based on the following three main concepts: the use of program transformations to obtain simpler termination problems, a framework for modular termination analysis, and the encoding of search problems into satisfiability of propositional logic (SAT) for efficient generation of ranking functions.", "num_citations": "6\n", "authors": ["679"]}
{"title": "Formally Proving the Boolean Pythagorean Triples Conjecture.\n", "abstract": " Abstract In 2016, Heule, Kullmann and Marek solved the Boolean Pythagorean Triples problem: is there a binary coloring of the natural numbers such that every Pythagorean triple contains an element of each color? By encoding a finite portion of this problem as a propositional formula and showing its unsatisfiability, they established that such a coloring does not exist. Subsequently, this answer was verified by a correct-by-construction checker extracted from a Coq formalization, which was able to reproduce the original proof. However, none of these works address the question of formally addressing the relationship between the propositional formula that was constructed and the mathematical problem being considered. In this work, we formalize the Boolean Pythagorean Triples problem in Coq. We recursively define a family of propositional formulas, parameterized on a natural number n, and show that unsatisfiability of this formula for any particular n implies that there does not exist a solution to the problem. We then formalize the mathematical argument behind the simplification step in the original proof of unsatisfiability and the logical argument underlying cube-and-conquer, obtaining a verified proof of Heule et al.\u2019s solution.", "num_citations": "4\n", "authors": ["679"]}
{"title": "Active integrity constraints for multi-context systems\n", "abstract": " We introduce a formalism to couple integrity constraints over general-purpose knowledge bases with actions that can be executed to restore consistency. This formalism generalizes active integrity constraints over databases. In the more general setting of multi-context systems, adding repair suggestions to integrity constraints allows defining simple iterative algorithms to find all possible grounded repairs \u2013 repairs for the global system that follow the suggestions given by the actions in the individual rules. We apply our methodology to ontologies, and show that it can express most relevant types of integrity constraints in this domain.", "num_citations": "4\n", "authors": ["679"]}
{"title": "When six gates are not enough\n", "abstract": " We apply the pigeonhole principle to show that there must exist Boolean functions on 7 inputs with a multiplicative complexity of at least 7, i.e., that cannot be computed with only 6 multiplications in the Galois field with two elements.", "num_citations": "4\n", "authors": ["679"]}
{"title": "Group communication patterns for high performance computing in scala\n", "abstract": " We developed a Functional Object-Oriented Parallel framework (FooPar) for high-level high-performance computing in Scala. Central to this framework are Distributed Memory Parallel Data structures (DPDs), ie, collections of data distributed in a shared nothing system together with parallel operations on these data.", "num_citations": "4\n", "authors": ["679"]}
{"title": "An analysis of Critical Chain concepts in the context of their possible application to traditional project management\n", "abstract": " Critical Chain Project Management is a relatively new, popular project management paradigm. Due to its radically different approach of addressing common project management problems, there is a strong interest in researching its utility. Much of the debate is focused on determining or refuting the superiority of the specific paradigm presented by Goldratt in his 1997 novel \u2018Critical Chain\u2019.", "num_citations": "4\n", "authors": ["679"]}
{"title": "Real-Time On-Board Deep Learning Fault Detection for Autonomous UAV Inspections\n", "abstract": " Inspection of high-voltage power lines using unmanned aerial vehicles is an emerging technological alternative to traditional methods. In the Drones4Energy project, we work toward building an autonomous vision-based beyond-visual-line-of-sight (BVLOS) power line inspection system. In this paper, we present a deep learning-based autonomous vision system to detect faults in power line components. We trained a YOLOv4-tiny architecture-based deep neural network, as it showed prominent results for detecting components with high accuracy. For running such deep learning models in a real-time environment, different single-board devices such as the Raspberry Pi 4, Nvidia Jetson Nano, Nvidia Jetson TX2, and Nvidia Jetson AGX Xavier were used for the experimental evaluation. Our experimental results demonstrated that the proposed approach can be effective and efficient for fully automatic real-time on-board visual power line inspection.", "num_citations": "3\n", "authors": ["679"]}
{"title": "Explainable Detection of Zero Day Web Attacks\n", "abstract": " The detection of malicious HTTP(S) requests is a pressing concern in cyber security, in particular given the proliferation of HTTP-based (micro-)service architectures. In addition to rule-based systems for known attacks, anomaly detection has been shown to be a promising approach for unknown (zero-day) attacks. This article extends existing work by integrating outlier explanations for individual requests into an end-to-end pipeline. These end-to-end explanations reflect the internal working of the pipeline. Empirically, we show that found explanations coincide with manually labelled explanations for identified outliers, allowing security professionals to quickly identify and understand malicious requests.", "num_citations": "3\n", "authors": ["679"]}
{"title": "Automating Termination Proofs Using Dependency Pairs and Recursive Path Orders\n", "abstract": " The dependency pair approach is a popular and powerful technique for proving (innermost) termination of term rewrite systems automatically. For any term rewite system, it generates a set of inequality constraints between terms. If there exists an order satisfying the constraints, then (innermost) termination is proved.The constraints generated by the dependency pair approach can often be solved by standard automatable simplification orders, even if a direct termination proof with such simplification orders would fail. One reason is that the dependency pair constraints are pre-processed by an argument filtering which eliminates arguments of function symbols.", "num_citations": "3\n", "authors": ["679"]}
{"title": "Outlier Detection with Explanations on Music Streaming Data: A Case Study with Danmark Music Group Ltd.\n", "abstract": " In the digital marketplaces, businesses can micro-monitor sales worldwide and in real-time. Due to the vast amounts of data, there is a pressing need for tools that automatically highlight changing trends and anomalous (outlier) behavior that is potentially interesting to users. In collaboration with Danmark Music Group Ltd. we developed an unsupervised system for this problem based on a predictive neural network. To make the method transparent to developers and users (musicians, music managers, etc.), the system delivers two levels of outlier explanations: the deviation from the model prediction, and the explanation of the model prediction. We demonstrate both types of outlier explanations to provide value to data scientists and developers during development, tuning, and evaluation. The quantitative and qualitative evaluation shows that the users find the identified trends and anomalies interesting and worth further investigation. Consequently, the system was integrated into the production system. We discuss the challenges in unsupervised parameter tuning and show that the system could be further improved with personalization and integration of additional information, unrelated to the raw outlier score.", "num_citations": "2\n", "authors": ["679"]}
{"title": "Optimal Path Planning for Drone Inspections of Linear Infrastructures.\n", "abstract": " Autonomous Beyond Visual Line of Sight (BVLOS) flights represent a huge opportunity in the drone industry due to their ability to monitor larger areas. Autonomous navigation and path planning are essential capabilities for BVLOS flights. In this paper, we introduce the routing component of a path planning system for inspecting linear infrastructures. We explore both a direct algorithm and a transformation algorithm. The direct algorithm is an extension of A* to allow limited routing through air as well as the use of non-logic intersections. The transformation algorithm pre-computes a graph that include edges for routing through air and nodes for non-logic intersections. We implemented both algorithms for routing along a particular type of linear infrastructure, power lines, and validated them through an empirical evaluation at three different scales: the Danish power grid, the French power grid, and the entire European power grid. The test results show that the transformation algorithm allows for sub-second routing performance for a small-to-medium sized power grid. Larger power grids can be routed in less than five seconds, and even an optimal route of more than six thousand kilometers along linear infrastructures from Portugal to Sweden via Russia is found in less than half a minute. All algorithms have been implemented and are available as an open-source Python package for Linear-infrastructure Mission Control (LiMiC).", "num_citations": "2\n", "authors": ["679"]}
{"title": "Real-time On-board Detection of Components and Faults in an Autonomous UAV System for Power Line Inspection.\n", "abstract": " The inspection of power line components is periodically conducted by specialized companies to identify possible faults and assess the state of the critical infrastructure. UAV-systems represent an emerging technological alternative in this field, with the promise of safer, more efficient, and less costly inspections. In the Drones4Energy project, we work toward a vision-based beyond-visual-line-of-sight (BVLOS) power line inspection architecture for automatically and autonomously detecting components and faults in real-time on board of the UAV. In this paper, we present the first step towards the vision system of this architecture. We train Deep Neural Networks (DNNs) and tune them for reliability under different conditions such as variations in camera used, lighting, angles, and background. For the purpose of real-time on-board implementation of the architecture, experimental evaluations and comparisons are performed on different hardware such as Raspberry Pi 4, Nvidia Jetson Nano, Nvidia Jetson TX2, and Nvidia Jetson AGX Xavier. The use of such Single Board Devices (SBDs) is an integral part of the design of the proposed power line inspection architecture. Our experimental results demonstrate that the proposed approach can be effective and efficient for fully-automatic real-time on-board visual power line inspection.", "num_citations": "2\n", "authors": ["679"]}
{"title": "Multi-Sense Language Modelling\n", "abstract": " The effectiveness of a language model is influenced by its token representations, which must encode contextual information and handle the same word form having a plurality of meanings (polysemy). Currently, none of the common language modelling architectures explicitly model polysemy. We propose a language model which not only predicts the next word, but also its sense in context. We argue that this higher prediction granularity may be useful for end tasks such as assistive writing, and allow for more a precise linking of language models with knowledge bases. We find that multi-sense language modelling requires architectures that go beyond standard language models, and here propose a structured prediction framework that decomposes the task into a word followed by a sense prediction task. For sense prediction, we utilise a Graph Attention Network, which encodes definitions and example uses of word senses. Overall, we find that multi-sense language modelling is a highly challenging task, and suggest that future work focus on the creation of more annotated training datasets.", "num_citations": "1\n", "authors": ["679"]}
{"title": "A Mixed Neural Network and Support Vector Machine Model for Tender Creation in the European Union TED Database.\n", "abstract": " This research article proposes a new method of automatized text generation and subsequent classification of the European Union (EU) Tender Electronic Daily (TED) text documents into predefined technological categories of the dataset. The TED dataset provides information about the respective tenders includes features like Name of project, Title, Description, Types of contract, Common procurement vocabulary (CPV) code, and Additional CPV codes. The dataset is obtained from the SIMAP-Information system for the European public procurement website, which is comprised of tenders described in XML files. The dataset was preprocessed using tokenization, removal of stop words, removal of punctuation marks etc. We implemented a neural machine learning model based on Long Short-Term Memory (LSTM) nodes for text generation and subsequent code classification. Text generation means that given a single line or just two or three words of the title, the model generates the sequence of a whole sentence. After generating the title, the model predicts the main applicable CPV code for that title. The LSTM model reaches an accuracy of 97% for the text generation and 95% for code classification using Support Vector Machine (SVM). This experiment is a first step towards developing a system that based on TED data is able to auto-generate and code classify tender documents, easing the process of creating and disseminating tender information to TED and ultimately relevant vendors. The development and automation of this system will future vision and understand current undergoing projects and the deliveries by a SIMAP-Information system\u00a0\u2026", "num_citations": "1\n", "authors": ["679"]}
{"title": "Active integrity constraints for general-purpose knowledge bases\n", "abstract": " In the database world, integrity constraints are essential to guarantee database integrity. The related problem of database repair deals with finding the best way to change a database so that it satisfies its integrity constraints. These two topics have been studied intensively since the 1980s. The formalism of active integrity constraints, proposed in 2004, aims at addressing them jointly, by providing a syntax whereby a particular subclass of integrity constraints can be specified together with preferred ways to repair inconsistency. In the last decade, several authors have proposed adaptations of the notion of integrity constraints to other reasoning frameworks than relational databases. In this article, we extend this line of work in two ways. First, we target multi-context systems, a general-purpose framework for combining heterogeneous reasoning systems, able to model most other reasoning frameworks, as we\u00a0\u2026", "num_citations": "1\n", "authors": ["679"]}
{"title": "How to Get More Out of Your Oracles\n", "abstract": " Formal verification of large computer-generated proofs often relies on certified checkers based on oracles. We propose a methodology for such proofs, advocating a separation of concerns between formalizing the underlying theory and optimizing the algorithm implemented in the checker, based on the observation that such optimizations can benefit significantly from adequately adapting the oracle.", "num_citations": "1\n", "authors": ["679"]}
{"title": "Active integrity constraints: From theory to implementation\n", "abstract": " The problem of database consistency relative to a set of integrity constraints has been extensively studied since the 1980s, and is still recognized as one of the most important and complex in the field. In recent years, with the proliferation of knowledge repositories (not only databases) in practical applications, there has also been an effort to develop implementations of consistency maintenance algorithms that have a solid theoretical basis.               The framework of active integrity constraints (AICs) is one example of such an effort, providing theoretical grounds for rule-based algorithms for ensuring database consistency. An AIC consists of an integrity constraint together with a specification of actions that may be taken to repair a database that does not satisfy it. Both denotational and operational semantics have been proposed for AICs. In this paper, we describe repAIrC, a prototype implementation of the\u00a0\u2026", "num_citations": "1\n", "authors": ["679"]}