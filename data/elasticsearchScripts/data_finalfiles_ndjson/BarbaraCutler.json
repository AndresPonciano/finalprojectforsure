{"title": "Stable real-time deformations\n", "abstract": " The linear strain measures that are commonly used in real-time animations of deformable objects yield fast and stable simulations. However, they are not suitable for large deformations. Recently, more realistic results have been achieved in computer graphics by using Green's non-linear strain tensor, but the non-linearity makes the simulation more costly and introduces numerical problems. In this paper, we present a new simulation technique that is stable and fast like linear models, but without the disturbing artifacts that occur with large deformations. As a precomputation step, a linear stiffness matrix is computed for the system. At every time step of the simulation, we compute a tensor field that describes the local rotations of all the vertices in the mesh. This field allows us to compute the elastic forces in a non-rotated reference frame while using the precomputed stiffness matrix. The method can be applied to both\u00a0\u2026", "num_citations": "592\n", "authors": ["919"]}
{"title": "A procedural approach to authoring solid models\n", "abstract": " We present a procedural approach to authoring layered, solid models. Using a simple scripting language, we define the internal structure of a volume from one or more input meshes. Sculpting and simulation operators are applied within the context of the language to shape and modify the model. Our framework treats simulation as a modeling operator rather than simply as a tool for animation, thereby suggesting a new paradigm for modeling as well as a new level of abstraction for interacting with simulation environments.Capturing real-world effects with standard modeling techniques is extremely challenging. Our key contribution is a concise procedural approach for seamlessly building and modifying complex solid geometry. We present an implementation of our language using a flexible tetrahedral representation. We show a variety of complex objects modeled in our system using tools that interface with finite\u00a0\u2026", "num_citations": "147\n", "authors": ["919"]}
{"title": "An intuitive daylighting performance analysis and optimization approach\n", "abstract": " The effective integration of daylighting considerations into the design process requires many issues to be considered simultaneously, such as daily and seasonal variations, illumination, and thermal comfort. To address the need for early integration into the design process, a new approach called Lightsolve has been developed. Its key objectives are to support the design process using a goal-oriented approach based on iterative design improvement suggestions; to provide climate-based annual metrics in a visual and synthesized format; and to relate quantitative and qualitative performance criteria using daylighting analysis data in various forms. This methodology includes the development of a time-segmentation process to represent weather and time in a condensed form, the adaptation of daylight metrics that encompass temporal and spatial considerations, and the creation of an interactive analysis interface to\u00a0\u2026", "num_citations": "115\n", "authors": ["919"]}
{"title": "Simplification and improvement of tetrahedral models for simulation\n", "abstract": " Most 3D mesh generation techniques require simplification and mesh improvement stages to prepare a tetrahedral model for efficient simulation. We have developed an algorithm that both reduces the number of tetrahedra in the model to permit interactive manipulation and removes the most poorly shaped tetrahedra to allow for stable physical simulations such as the finite element method. The initial tetrahedral model may be composed of several different materials representing internal structures. Our approach targets the elimination of poorly-shaped elements while simplifying the model using edge collapses and other mesh operations, such as vertex smoothing, tetrahedral swaps, and vertex addition. We present the results of our algorithm on a variety of inputs, including models with more than a million tetrahedra. In practice, our algorithm reliably reduces meshes to contain only tetrahedra that meet specified\u00a0\u2026", "num_citations": "70\n", "authors": ["919"]}
{"title": "Constrained planar remeshing for architecture\n", "abstract": " Material limitations and fabrication costs generally run at odds with the creativity of architectural design, producing a wealth of challenging computational geometry problems. We have developed an algorithm for solving an important class of fabrication constraints: those associated with planar construction materials such as glass or plywood.", "num_citations": "65\n", "authors": ["919"]}
{"title": "Interactive selection of optimal fenestration materials for schematic architectural daylighting design\n", "abstract": " Complex fenestration systems, such as prismatic and laser cut panels, are emerging as attractive options in architectural design thanks to their high potential to assist in energy and comfort issues. These systems can be used to redirect intense illumination from the sun but have complex transmissive properties that in turn depend on continuously changing illumination conditions due to the dynamic nature of natural light. The resulting non-intuitive interactions with the built environment make it necessary to develop tools that adequately represent these systems' behavior to the architect. The method presented in this paper enables simulation of the direct and indirect illumination from the sun and sky throughout each day for different months Preprint submitted to Automation in Construction 17 December 2007 of the year. The user can interactively explore the high-dimensional configuration space to select optimal\u00a0\u2026", "num_citations": "53\n", "authors": ["919"]}
{"title": "Robust adaptive 3-D segmentation of vessel laminae from fluorescence confocal microscope images and parallel GPU implementation\n", "abstract": " This paper presents robust 3-D algorithms to segment vasculature that is imaged by labeling laminae, rather than the lumenal volume. The signal is weak, sparse, noisy, nonuniform, low-contrast, and exhibits gaps and spectral artifacts, so adaptive thresholding and Hessian filtering based methods are not effective. The structure deviates from a tubular geometry, so tracing algorithms are not effective. We propose a four step approach. The first step detects candidate voxels using a robust hypothesis test based on a model that assumes Poisson noise and locally planar geometry. The second step performs an adaptive region growth to extract weakly labeled and fine vessels while rejecting spectral artifacts. To enable interactive visualization and estimation of features such as statistical confidence, local curvature, local thickness, and local normal, we perform the third step. In the third step, we construct an accurate\u00a0\u2026", "num_citations": "46\n", "authors": ["919"]}
{"title": "A spatially augmented reality sketching interface for architectural daylighting design\n", "abstract": " We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the interreflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural\u00a0\u2026", "num_citations": "44\n", "authors": ["919"]}
{"title": "Smugglers and border guards: the geostar project at RPI\n", "abstract": " We present the GeoStar project at RPI, which researches various terrain (ie, elevation) representations and operations thereon. This work is motivated by the large amounts of hi-res data now available. The purpose of each representation is to lossily compress terrain while maintaining important properties. Our ODETLAP representation generalizes a Laplacian partial differential equation by using two inconsistent equations for each known point in the grid, as well as one equation for each unknown point. The surface is reconstructed from a carefully-chosen small set of known points. Our second representation segments the terrain into a set of regions, each of which is simply described. Our third representation has the most long term potential: scooping, which forms the terrain by emulating surface water erosion.", "num_citations": "39\n", "authors": ["919"]}
{"title": "Efficient viewshed computation on terrain in external memory\n", "abstract": " The recent availability of detailed geographic data permits terrain applications to process large areas at high resolution. However the required massive data processing presents significant challenges, demanding algorithms optimized for both data movement and computation. One such application is viewshed computation, that is, to determine all the points visible from a given point p. In this paper, we present an efficient algorithm to compute viewsheds on terrain stored in external memory. In the usual case where the observer\u2019s radius of interest is smaller than the terrain size, the algorithm complexity is \u03b8(scan(n                         2)) where n                         2 is the number of points in an n \u00d7 n DEM and scan(n                         2) is the minimum number of I/O operations required to read n                         2 contiguous items from external memory. This is much faster than existing published algorithms.", "num_citations": "37\n", "authors": ["919"]}
{"title": "Global illumination compensation for spatially augmented reality\n", "abstract": " When projectors are used to display images on complex, non\u2010planar surface geometry, indirect illumination between the surfaces will disrupt the final appearance of this imagery, generally increasing brightness, decreasing contrast, and washing out colors. In this paper we predict through global illumination simulation this unintentional indirect component and solve for the optimal compensated projection imagery that will minimize the difference between the desired imagery and the actual total illumination in the resulting physical scene. Our method makes use of quadratic programming to minimize this error within the constraints of the physical system, namely, that negative light is physically impossible. We demonstrate our compensation optimization in both computer simulation and physical validation within a table\u2010top spatially augmented reality system. We present an application of these results for visualization\u00a0\u2026", "num_citations": "37\n", "authors": ["919"]}
{"title": "Parallel ODETLAP for terrain compression and reconstruction\n", "abstract": " We introduce a parallel approximation of an Over-determined Laplacian Partial Differential Equation solver (ODETLAP) applied to the compression and restoration of terrain data used for Geographical Information Systems (GIS). ODETLAP can be used to reconstruct a compressed elevation map, or to generate a dense regular grid from airborne Light Detection and Ranging (LIDAR) point cloud data. With previous methods, the time to execute ODETLAP does not scale well with the size of the input elevation map, resulting in running times that are prohibitively long for large data sets. Our algorithm divides the data set into patches, runs ODETLAP on each patch, and then merges the patches together. This method gives two distinct speed improvements. First, we provide scalability by reducing the complexity such that the execution time grows almost linearly with the size of the input, even when run on a single\u00a0\u2026", "num_citations": "30\n", "authors": ["919"]}
{"title": "Surface compression using over-determined Laplacian approximation\n", "abstract": " We describe a surface compression technique to lossily compress elevation datasets. Our approach first approximates the uncompressed terrain using an over-determined system of linear equations based on the Laplacian partial differential equation. Then the approximation is refined with respect to the uncompressed terrain using an error metric. These two steps work alternately until we find an approximation that is good enough. We then further compress the result to achieve a better overall compression ratio. We present experiments and measurements using different metrics and our method gives convincing results.", "num_citations": "30\n", "authors": ["919"]}
{"title": "Virtual heliodon: Spatially augmented reality for architectural daylighting design\n", "abstract": " We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the inter-reflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural\u00a0\u2026", "num_citations": "19\n", "authors": ["919"]}
{"title": "Dynamic projection environments for immersive visualization\n", "abstract": " We present a system for dynamic projection on large, human-scale, moving projection screens and demonstrate this system for immersive visualization applications in several fields. We have designed and implemented efficient, low-cost methods for robust tracking of projection surfaces, and a method to provide high frame rate output for computationally-intensive, low frame rate applications. We present a distributed rendering environment which allows many projectors to work together to illuminate the projection surfaces. This physically immersive visualization environment promotes innovation and creativity in design and analysis applications and facilitates exploration of alternative visualization styles and modes. The system provides for multiple participants to interact in a shared environment in a natural manner. Our new human-scale user interface is intuitive and novice users require essentially no instruction to\u00a0\u2026", "num_citations": "17\n", "authors": ["919"]}
{"title": "Evaluation of user interaction with daylighting simulation in a tangible user interface\n", "abstract": " We present a study of a tangible user interface for design and simulation applied to architectural daylighting analysis. This tool provides an intuitive way for architects and future occupants of a building to quickly construct physical models and interactively view in them a projected simulation of resulting daylighting.A user study was conducted of both architecture students and non-architects in a set of analysis and design exercises. The study investigates the effectiveness of this interface as an educational tool, the precision and accuracy of the constructed physical models, and its effectiveness for creative design exploration. The four part study investigates users' intuitions about daylighting and their interaction with the tangible user interface for analysis of an existing space. These exercises revealed and corrected misconceptions in many of the participants' intuitions about daylighting, and overall the participants\u00a0\u2026", "num_citations": "15\n", "authors": ["919"]}
{"title": "Analyses, Simulations, and Physical Modeling Validation of Levee and Embankment Erosion\n", "abstract": " We present a computer simulation of hydraulic erosion on levees, dams, and earth embankments, with emphasis on rill and gully initiation and propagation. We focus on erosion features that occur after an earthen structure is overtopped. We have developed a 3D fluid and hydraulic erosion simulation engine using Smoothed Particle Hydrodynamics (SPH). We present the results of digital simulations for different soil types. Furthermore, small-scale physical models of levees composed of different soils were constructed and tested experimentally. The digital simulations are compared to physical experimental results to validate the computer models.", "num_citations": "14\n", "authors": ["919"]}
{"title": "Computer simulation of levee erosion and overtopping\n", "abstract": " Improved computer models of erosion have been developed, considering soil hydraulic conductivity. The models deal with erosion of levees, dams and embankments due to overtopping. The simulations trace the formation of rills and gullies, beginning with initial overtopping and continuing to final breaching. Physical models performed at \"1-g\" and high \"g\" using a geotechnical centrifuge have been used to calibrate the models. Previous models did not consider soil hydraulic conductivity, and although results were quite good for the formation of rills and gullies and sediment quantities, breach times were underestimated. Essentially the water flow was treated as if passing over a solid surface, not entering the soil, and the total water flow was available for erosion. Thus, breach times were underestimated. Soil erodibility parameters had to be adjusted in order to achieve good agreement with breach times. The new\u00a0\u2026", "num_citations": "13\n", "authors": ["919"]}
{"title": "ARmy: A study of multi-user interaction in spatially augmented games\n", "abstract": " We present ARmy, a two-player military strategy game that uses spatially augmented reality to combine physical tabletop games with the virtual elements and computation characteristic of modern video games. As players move plastic miniatures within a small scale physical environment, the application moderates and augments play by maintaining a 3D representation of the scene, which it uses to validate movement paths and perform automatic line-of-sight calculations for combat. We describe the design and implementation of the ARmy gaming system. Furthermore, we conducted a user study to gauge the effectiveness, in-tuitiveness, and robustness of the application. We describe the process of this user study, present quantitative data of the study results, and discuss general design principles for the design and implementation of other engaging spatially-augmented games.", "num_citations": "13\n", "authors": ["919"]}
{"title": "Comparative grain topology\n", "abstract": " Space-filling polyhedral networks are commonly studied in biological, physical, and mathematical disciplines. The constraints governing the construction of each network varies considerably under each context, affecting the topological properties of the constituents. A method for mapping the topological symmetry of a space-filling population of polyhedra is presented, relative to all possible polyhedra. This method is applied to the topological comparison of populations generated by seven different processes: (i) natural grain growth in polycrystalline metal, ideal grain growth simulated by (ii) interface-tracking and (iii) phase-field methods, (iv) Poisson\u2013Voronoi and (v) ellipsoid tessellations, and (vi) graph-theoretic and (vii) Monte Carlo enumerations of individual polyhedra. Evidence for topological bias in these populations is discussed.", "num_citations": "11\n", "authors": ["919"]}
{"title": "Simulating levee erosion with physical modeling validation\n", "abstract": " This paper studies rill and gully initiation and propagation on levees, dams, and general earth embankments. It specifically studies where these erosion features occur, and how long a particular embankment can sustain overtopping before breaching and catastrophic failure. This contrasts to previous levee erosion analysis, which has primarily concerned the final effects of erosion, such as soil loss, depth of scour and breach width. This paper describes the construction of scaled-down physical models of levees composed of different homogeneous sands, as well as sand-clay mixtures, and their laboratory testing. A 3-D laser range scanner captured the surface features of the physical model, before and after erosion. The resulting data is utilized in developing digital simulations of the rill erosion process. Those simulations combine 3-D Navier-Stokes fluid simulations and a segmented height field data structure to\u00a0\u2026", "num_citations": "11\n", "authors": ["919"]}
{"title": "Drainage network and watershed reconstruction on simplified terrain\n", "abstract": " We present a new form of terrain compression to preserve the hydrological information that is lost using standard terrain simplification techniques. First, we compute the drainage by using a system of linear equations to determine the amount of water flowing into each cell. The flow is then computed on the inverted terrain which provides an approximation of the ridge network. The drainage and ridge networks are simplified using the Douglas-Peucker algorithm, selecting the most significant points. These points represent our compressed version of the hydrology. To uncompress, we use Over-determined Laplacian Partial Differential Equations (ODETLAP) to \u201cfill in\u201d the missing data points. Our results show that the flow and watersheds on the reconstructed terrain are typically better than original ODETLAP point selection and lossy JPEG2000 compression.", "num_citations": "11\n", "authors": ["919"]}
{"title": "Quantitative analysis of simulated erosion for different soils\n", "abstract": " Levee overtopping can lead to failure and cause catastrophic damage, as was the case during Hurricane Katrina. We present a computer simulation of erosion to study the development of the rills and gullies that form along an earthen embankment during overtopping. We have coupled 3D Smoothed Particle Hydrodynamics with an erodibility model to produce our simulation. Through comparison between simulations and between simulation and analogous laboratory experiments, we provide quantitative and qualitative results, evaluating the accuracy of our simulation.", "num_citations": "10\n", "authors": ["919"]}
{"title": "Goal-based daylighting design using an interactive simulation method\n", "abstract": " This paper proposes an interactive goal-based method for designing daylit buildings. The lighting simulation tool which supports this process is a hybrid global illumination rendering method which efficiently computes annual daylighting metrics. The goal-based method uses a knowledge base populated using a set of previously completed simulations that quantify the effects of various fa\u00e7ade design modifications. The knowledge base guides a simple algorithm over an iterative design process. The current knowledge base includes information about window size, shape, location on the fa\u00e7ade, and simple shading devices. Three case studies are given in which this iterative optimization method was applied; all resulted in improved daylighting performance.", "num_citations": "10\n", "authors": ["919"]}
{"title": "2. Interpreting Physical Sketches As Architectural Models\n", "abstract": " We present an algorithm for the automatic interpretation of a rough architectural sketch as a consistent 3D digital model. We compare our results to the designerVs intended geometry. We further validate the algorithm by studying the variations in possible interpretations made by other humans for a set of relatively ambiguous sketches. In our system, the user sketches an architectural design by arranging small-scale physical wall modules and simple markers for windows on a table. These color-coded elements are captured by a camera mounted above the scene and recognized using computer vision techniques. The architectural design is automatically inferred from this rough physical sketch transforming it into a consistent and manifold 3D triangle mesh representation. The resulting digital model is amenable to numerous building simulations including lighting, acoustics, heating/cooling, and structural analysis.", "num_citations": "9\n", "authors": ["919"]}
{"title": "Validation of erosion modeling: Physical and numerical\n", "abstract": " The overall intent of this research is to develop numerical models of erosion of levees, dams and embankments, validated by physical models. The physical model tests are performed at 1-g and at high g's using a geotechnical centrifuge facility. The erosion is modeled in detail, from beginning to end, that is from the time the levee is overtopped until the levee is breached. Typical quantities measured as a function of time including the depth, width and volume of rills, number of junction points, rills shape (straight or meandering), sediment transport quantities, and time to breach. This data can be obtained from the numerical modeling, but is difficult to obtain from the physical tests in real life. Video images indicate that the physical modeling results which have been tested in this research agree with the numerical modeling results. A comparison has also been made between observed breaching width and the FEMA\u00a0\u2026", "num_citations": "8\n", "authors": ["919"]}
{"title": "Comparing jailed sandboxes vs containers within an autograding system\n", "abstract": " With the continued growth of enrollment within computer science courses, it has become an increasing necessity to utilize autograding systems. These systems have historically graded assignments through either a jailed sandbox environment or within a virtual machine (VM). For a VM, each submission is given its own instantiation of a guest operating system and virtual hardware that runs atop the host system, preventing anything that runs within the VM communicating with any other VM or the host. However, using these VMs are costly in terms of system resources, making it less than ideal for running student submissions given reasonable, limited resources. Jailed sandboxes, on the other hand, run on the host itself, thus taking up minimal resources, and utilize a security model that restricts the process to specified directories on the system. However, due to running on the host machine, the approach suffers as\u00a0\u2026", "num_citations": "7\n", "authors": ["919"]}
{"title": "Submitty: An open source, highly-configurable platform for grading of programming assignments\n", "abstract": " Submitty (http://submitty. org) is an open source programming assignment submission system from the Rensselaer Center for Open Source Software (RCOS) at Rensselaer Polytechnic Institute (RPI). Students can submit their code via a web interface in a variety of ways, where it is then tested with a highly configurable and customizable automated grader. Students receive immediate feedback from the grader, and can resubmit to correct errors as needed. Through an online interface, TAs can access detailed grading results and supplement the automated scores with manual grading (numeric and written feedback) of overall program structure, good use of comments, reasonable error checking, etc. and any non-programming components of the assignment. The instructor can also configure the system to allow for a configurable late day policy on a per assignment and per student basis. We currently use Submitty in\u00a0\u2026", "num_citations": "7\n", "authors": ["919"]}
{"title": "New visualization method to evaluate erosion quantity and pattern\n", "abstract": " The objective of this research is to develop tools that would improve the understanding of the process of levee failure because of erosion and reduce the risk of failure. Hydraulic erosion is a complicated phenomenon and depends on many different parameters. To improve design criteria for levees, embankments, and earthen structures, the development of realistic computer models that can simulate the erosion process is necessary. Verification of these computer simulations, as with any simulation, is a necessity. In this research, a large number of physical levee erosion tests were performed at 1g and at high g's using a geotechnical centrifuge. Centrifuge tests were performed to simulate real (prototype) size levees, and thus to obtain a more realistic model. The erosion was modeled physically in detail. Conventional three-dimensional scanning was used to precisely verify the calculated dimensions of initial and\u00a0\u2026", "num_citations": "7\n", "authors": ["919"]}
{"title": "Perceptual global illumination cancellation in complex projection environments\n", "abstract": " The unintentional scattering of light between neighboring surfaces in complex projection environments increases the brightness and decreases the contrast, disrupting the appearance of the desired imagery. To achieve satisfactory projection results, the inverse problem of global illumination must be solved to cancel this secondary scattering. In this paper, we propose a global illumination cancellation method that minimizes the perceptual difference between the desired imagery and the actual total illumination in the resulting physical environment. Using Gauss\u2010Newton and active set methods, we design a fast solver for the bound constrained nonlinear least squares problem raised by the perceptual error metrics. Our solver is further accelerated with a CUDA implementation and multi\u2010resolution method to achieve 1\u20132 fps for problems with approximately 3000 variables. We demonstrate the global illumination\u00a0\u2026", "num_citations": "7\n", "authors": ["919"]}
{"title": "Slope accuracy and path planning on compressed terrain\n", "abstract": " We report on variants of the ODETLAP lossy terrain compression method where the reconstructed terrain has accurate slope as well as elevation. Slope is important for applications such as mobility, visibility and hydrology. One variant involves selecting a regular grid of points instead of selecting the most important points, requiring more points but which take less space. Another variant adds a new type of equation to the overdetermined system to force the slope of the reconstructed surface to be close to the original surface\u2019s slope. Tests on six datasets with elevation ranges from 505m to 1040m, compressed at ratios from 146:1 to 1046:1 relative to the original binary file size, showed RMS elevation errors of 10m and slope errors of 3 to 10 degrees. The reconstructed terrain also supports planning optimal paths that avoid observers\u2019 viewsheds. Paths planned on the reconstructed terrain were only 5% to 20\u00a0\u2026", "num_citations": "7\n", "authors": ["919"]}
{"title": "Multiple observer siting and path planning on a compressed terrain\n", "abstract": " We examine a smugglers and border guards scenario. We place observers on a terrain so as to optimize their visible coverage area. Then we compute a path that a smuggler would take so as to avoid detection, while also minimizing the path length. We also examine how our results are affected by using a lossy representation of the terrain instead. We propose three new application-specific error metrics for evaluating terrain compression. Our target terrain applications are the optimal placement of observers on a landscape and the navigation through the terrain by smugglers. Instead of using standard metrics such as average or maximum elevation error, we seek to optimize our compression on the specific real-world application of smugglers and border guards.", "num_citations": "7\n", "authors": ["919"]}
{"title": "Autograding Distributed Algorithms in Networked Containers\n", "abstract": " We present a container-based system to automatically run and evaluate networked applications that implement distributed algorithms. Our implementation of this design leverages lightweight, networked Docker containers to provide students with fast, accurate, and helpful feedback about the correctness of their submitted code. We provide a simple, easy-to-use interface for instructors to specify networks, deploy and run instances of student and instructor code, and to log and collect statistics concerning node connection types and message content. Instructors further have the ability to control network features such as message delay, drop, and reorder. Running student programs can be interfaced with via stream-controlled standard input or through additional containers running custom instructor software. Student program behavior can be automatically evaluated by analyzing console or file output and instructor\u00a0\u2026", "num_citations": "6\n", "authors": ["919"]}
{"title": "Collaborative training tools for emergency restoration of critical infrastructure systems\n", "abstract": " Large-scale disasters can produce profound disruptions in the fabric of critical infrastructure systems such as water, telecommunications and electric power. The work of post-disaster infrastructure restoration typically requires close collaboration across these sectors. Yet the technological means to support collaborative training for these activities lag far behind training needs. This paper motivates and describes the design and implementation of a multi-layered system for use in cross-organizational, scenario-based training for emergency infrastructure restoration. Ongoing evaluation studies are described in order suggest directions for further work.", "num_citations": "6\n", "authors": ["919"]}
{"title": "Computer simulations and physical modelling of erosion\n", "abstract": " Research is being done to study the details and progress of soil erosion on levees and dams, including the formation and progression of rills and gullies on the slopes, and eventually to final breaching. These detailed observations of erosion differ from the typical predictions of only the maximum erosion or scour depths, for example around submerged bridge piers. Computer simulations and geotechnical centrifuge modelling will, in the future, be validated using these observations. For testing, single layer sand models were utilized, and will be followed by clayey and mixed soils, and increased number of layers. The computer simulations will incorporate 3-D Navier-Stokes fluid simulations, and a novel segmented height field extended to allow soil undercuts was developed. The primary intent of the research is to study small-scale erosion on earthen embankments and, ultimately, develop novel and robust erosion software, validated by physical modelling. Figure 1: A levee that was overtopped for several hours during the Katrina hurricane but did not fail. The dramatic gouging/scooping on the lower portions of the levee is due to increased water flow at the base of the levee and the non-homogeneous nature of the embankment.", "num_citations": "6\n", "authors": ["919"]}
{"title": "Computer simulation of overtopping of levees\n", "abstract": " There have been many cases of earth embankment failures, for example, Hurricane Katrina in 2005, where breaching occurred and devastated the surrounding population. Levee failures are preventable by a better understanding of the ways in which these embankments are designed and fail. The objective of this research is to protect levees against future failures. This paper studies various overtopping quantities and durations to represent the same level of levee erosion hazard. This study is based on experimental results of steady flows on the land side of a levee. The effect of water flow has been investigated and a comparison has been done between rills formations and erosion time for various water flows. Results showed that the pictures of digital simulations and real photographs which have been taken during tests in the laboratory are in a good concordance. Ha habido muchos casos de fallos de terrapl\u00e9n, por ejemplo, el hurac\u00e1n Katrina en 2005, en el cual se produjo una ruptura, devastando la poblaci\u00f3n de los alrededores. Las fallas de diques se pueden prevenir, y es un objetivo de esta investigaci\u00f3n alcanzar una mejor comprensi\u00f3n de las maneras en que estos diques se dise\u00f1an y fallan, a fin de poder protegerlos contra futuros fallos. Este documento desarrolla y recomienda equivalencias preliminares de combinaciones acumulativas de varias cantidades de desbordamiento y las duraciones asociadas que representan el mismo nivel de riesgo de erosi\u00f3n del dique. Las metodolog\u00edas se basan en los resultados experimentales de flujos constantes en el lado seco de un dique. El efecto del flujo de agua se ha estudiado\u00a0\u2026", "num_citations": "6\n", "authors": ["919"]}
{"title": "Rendering lunar eclipses.\n", "abstract": " Johannes Kepler first attributed the visibility of lunar eclipses to refraction in the Earth\u2019s atmosphere in his Astronomiae Pars Optica in 1604. We describe a method for rendering images of lunar eclipses including color contributions due to refraction, dispersion, and scattering in the Earth\u2019s atmosphere. We present an efficient model of refraction and scattering in the atmosphere, including contributions of suspended volcanic dusts which contribute to the observed variation in eclipse brightness and color. We propose a method for simulating camera exposure to allow direct comparison between rendered images and digital photographs. Images rendered with our technique are compared to photographs of the total lunar eclipse of February 21, 2008.", "num_citations": "6\n", "authors": ["919"]}
{"title": "Path planning on a compressed terrain\n", "abstract": " We present a better algorithm for path planning on complex terrain in the presence of observers and define several metrics related to path planning to evaluate the quality of various terrain compression strategies.", "num_citations": "6\n", "authors": ["919"]}
{"title": "Approximating terrain with over-determined Laplacian PDEs\n", "abstract": " We extend Laplacian PDE by adding a new equation to form an over-determined system so that we can control the relative importance of smoothness and accuracy in the reconstructed surface. Benefits of the method include the ability to process isolated, scattered elevation points and the fact that reconstructed surface could generate local maxima, which is not possible in the original Laplacian PDE by the maximum principle. We use certain geometric algorithm including Triangulate Irregular Network, Visibility test, Level Set Component that discovers important points which reflect the terrain structure and use our extended Laplacian PDE to approximate the terrain from these points. We present experiments and measurements using different metrics and our method gives convincing results.", "num_citations": "6\n", "authors": ["919"]}
{"title": "Evaluating hydrology preservation of simplified terrain representations\n", "abstract": " We present an error metric based on the potential energy of water flow to evaluate the quality of lossy terrain simplification algorithms. Typically, terrain compression algorithms seek to minimize RMS (root mean square) and maximum error. These metrics fail to capture whether a reconstructed terrain preserves the drainage network. A quantitative measurement of how accurately a drainage network captures the hydrology is important for determining the effectiveness of a terrain simplification technique. Having a measurement for testing and comparing different models has the potential to be widely used in numerous applications (flood prevention, erosion measurement, pollutant propagation, etc). In this paper, we transfer the drainage network computed on reconstructed geometry onto the original uncompressed terrain and use our error metric to measure the level of error created by the simplification. We also\u00a0\u2026", "num_citations": "5\n", "authors": ["919"]}
{"title": "Analysis of container based vs. Jailed sandbox autograding systems\n", "abstract": " Traditionally, automated testing and grading of student programming assignments has been done in either a jailed sandbox environment or within a virtual machine (VM). For a VM, each submission is given its own instantiation of a guest operating system (OS) running atop the host OS, with no ability for a given submission to affect anything outside the VM. However, using a VM is expensive in terms of system resource usages, especially for RAM and memory, making it less than ideal for solutions without unlimited resources. Jailed sandboxes on the other hand allow student submissions to run directly on the server. Sufficient security measures must be implemented to ensure that students cannot access each other's submissions or the server at large, and must prevent runaway programs, over-utilization of system resources. Jailed sandboxes have a larger attack vector than VMs. Within the past several years\u00a0\u2026", "num_citations": "4\n", "authors": ["919"]}
{"title": "462: Informing daylighting design with the Lightsolve approach: why and how\n", "abstract": " To efficiently and appropriately integrate daylighting strategies in their projects, building designers need reliable methods to address issues such as daily and seasonal variations or the balance between sufficient illumination with visual and thermal comfort aspects. This integration must also happen early in the design process to have a significant impact on energy savings and ultimate building performance. This paper proposes to address this need by fulfilling three major objectives: support the design process using a goal-oriented approach based on iterative design improvement suggestions; provide climate-based annual metrics in a visual and synthesized form; and relate quantitative and qualitative performance criteria thanks to a novel interface for browsing daylighting analysis data in various forms. A methodology to achieve these objectives is described here as the Lightsolve approach.", "num_citations": "4\n", "authors": ["919"]}
{"title": "Progressive transmission of lossily compressed terrain\n", "abstract": " The distribution and management of spatial data requires strategies for handling large amount of terrain data that are now large available. Especially, data like LIDAR and Digital Elevation Model (DEM) data which are being used in a large group of diversified users. In this paper, we propose a progressive terrain data transmission scheme which take advantage of Over-determined Laplacian PDE (ODETLAP), which can achieve a compromise of high compression ratio and accuracy. The ODETLAP can be thought of as a compressor of original terrain data and we use Run Length encoding as well as linear prediction to reach a higher compression ratio. The latter two alone can do a better job than bzip2. Concretely, our technique is capable of reducing a hilly DEM dataset to 1% of its original binary size and a mountainous DEM dataset 3% of its original size. The accuracy loss in elevation and slope are discussed.", "num_citations": "4\n", "authors": ["919"]}
{"title": "Synthetic environments for investigating collaborative information seeking: An application in emergency restoration of critical infrastructures\n", "abstract": " Large-scale disasters can produce profound disruptions in the fabric of interdependent critical infrastructure systems such as water, telecommunications and electric power. The work of post-disaster infrastructure restoration typically requires information sharing and close collaboration across these sectors; yet \u2013 due to a number of factors \u2013 the means to investigate decision making phenomena associated with these activities are limited. This paper motivates and describes the design and implementation of a computer-based synthetic environment for investigating collaborative information seeking in the performance of a (simulated) infrastructure restoration task. The main contributions of this work are twofold. First, it develops a set of theoretically grounded measures of collaborative information seeking processes and embeds them within a computer-based system. Second, it suggests how these data may be\u00a0\u2026", "num_citations": "3\n", "authors": ["919"]}
{"title": "Evaluation of a tangible interface for architectural daylighting analysis\n", "abstract": " We present a study of a tangible user interface (TUI) for architectural design and daylighting analysis. This tool provides an intuitive way for architects and future building occupants to quickly construct physical models and then view a simulation of daylighting in the model at interactive rates. We conducted a user study of both formally-trained architects and non-architects in a set of analysis and design exercises. This study investigates the effectiveness of this interface as an educational tool, the precision and accuracy of the constructed physical models, and the overall effectiveness of the tangible interface. The four part study investigates users' intuitions about daylighting and their interaction with the tool for analysis of an existing space, for proposing renovations to the space, and for designing a totally new space with the same architectural program that better addresses the occupants' needs. These exercises\u00a0\u2026", "num_citations": "3\n", "authors": ["919"]}
{"title": "Segmented height field and smoothed particle hydrodynamics in erosion simulation\n", "abstract": " The New Orleans area levee failures during Hurricane Katrina drew media attention to an important prob-lem in Civil Engineering. The emphasis of our work is on earthen levees, dams, and embankments. A ma-jor cause of failures of such structures is overtopping, which causes erosion to the point of breaching the crest. Our research focuses on simulating the initial small-scale features of erosion\u2013the formation of rills and gullies on the embankment. We wish to study and eventually be able to simulate the way earthen embankments erode, with respect to the formation of these rills and gullies. Validation of computer simu-lations is the primary focus of our research. We will utilize RPI's geotechnical centrifuge to perform high-g erosion experiments on small-scale models to pre-dict and validate the model for full scale simulations. 2 Review of Literature Erosion Models and Erodibility A variety of ex-isting erosion models calculate the overall soil loss during the overtopping of an earthen embankment. For example, Wang and Kahawita present a two-dimensional mathematical model of erosion of the profile of an earthen embankment during overtopping (Wang and Kahawita 2003). The\" erodibility\" of soil is generally defined as the ratio of the rate the soil erodes to the velocity of the water causing the erosion. In his work, Jean-Louis Briaud defines erodibility as a function of hydraulic shear stress, or pull of the water on the soil and presents measurements of soil samples collected from many of the affected earthen levees in the New Orleans area (Briaud, Chen, Govindasamy, and Storesund 2008). Erosion Simulations Kristof et al., present an ero-sion\u00a0\u2026", "num_citations": "3\n", "authors": ["919"]}
{"title": "Measuring terrain distances through extracted channel networks\n", "abstract": " This paper initiates a forensic analysis of the causes of levee failures by analyzing and extracting information from a sequence of elevation data. This is a crucial step in bettering the design and construction of levees and dams. (Fully diagnosing failures usually requires knowledge beyond the geometry of the levee, such as weather conditions and material properties). We use results from computer simulations of levee overtopping for training data. The simulations use smoothed particle hydrodynamics coupled with a well-known erodibility model. Using the sequential nature of our data, we extract important channel networks that form as the soil is scoured away. We present a series of metrics to measure the distance between channel networks to assist in determining the critical threshold value used to extract important channels from the flow network. Methods for determining this \"ideal\" threshold have gone mainly\u00a0\u2026", "num_citations": "3\n", "authors": ["919"]}
{"title": "Program Analysis Tools in Automated Grading of Homework Assignments\n", "abstract": " With surging enrollment in Computer Science courses at both the introductory and advanced level, it is critical to leverage automated testing and grading to ensure consistent assessment of student learning. Program analysis tools allow us to streamline the grading process so instructors and TAs can spend more time teaching, one-on-one tutoring, and mentoring students. We present complex use cases of automated assignment testing and grading within the open-source homework submission system, Submitty. Students receive immediate and detailed feedback from the automated grader, and can resubmit to correct errors. Submitty uses custom-built grading tools, including difference checking of plaintext program output, instructor authored assignment-specific custom graders, and static analysis tools that reason about program structure. In addition, it employs a variety of external tools, including version control\u00a0\u2026", "num_citations": "2\n", "authors": ["919"]}
{"title": "A flexible late day policy reduces stress and improves learning\n", "abstract": " We present a non-grade-penalty late day policy used in many of the large lecture, required courses in our computer science department. We study the effectiveness of this late day policy in reducing student stress, distributing demand for teaching assistant resources in peak hours before the homework deadline, and in maintaining or improving student understanding and homework grades. A complex late day policy can be efficiently implemented and managed within our open-source homework submission system that utilizes automated testing and grading, allowing students to submit and resubmit homeworks as they make progress on the assignment.", "num_citations": "2\n", "authors": ["919"]}
{"title": "Physical avatars in a projector-camera tangible user interface enhance quantitative simulation analysis and engagement\n", "abstract": " We present an augmented reality environment for the visualization of architectural daylighting simulations. The new visualizations focus the users' attention on the problematic aspects of a building design. Architectural design is a task particularly well suited for Tangible User Interfaces (TUIs). The user physically constructs a scale model of the building, a lighting simulation is then performed on this space, and then the simulation results are projected into the physical model by a set of calibrated projectors. A user study of an earlier version of the system revealed that users lacked accurate quantitative information about the propagation of natural light within architectural spaces and had difficulties identifying and reasoning about areas of overillumination, under-illumination, and glare. This was our motivation for two important additions to the system: physical avatar tokens within the physical scale model to specify areas of interest for glare and false color visualizations. We render viewpoints from the perspective of each avatar and indicate glare for each viewpoint. To provide users with an additional way to minimize glare and provide visual interest, we introduce new complex and interesting shading materials. These features illustrated in our tool create a more immersive and educational experience for novice and experienced designers.", "num_citations": "2\n", "authors": ["919"]}
{"title": "Informing daylighting design with the Lightsolve approach: why and how\n", "abstract": " To efficiently and appropriately integrate daylighting strategies in their projects, building designers need reliable methods to address issues such as daily and seasonal variations or the balance between sufficient illumination with visual and thermal comfort aspects. This integration must also happen early in the design process to have a significant impact on energy savings and ultimate building performance. This paper proposes to address this need by fulfilling three major objectives: support the design process using a goal-oriented approach based on iterative design improvement suggestions; provide climate-based annual metrics in a visual and synthesized form; and relate quantitative and qualitative performance criteria thanks to a novel interface for browsing daylighting analysis data in various forms. A methodology to achieve these objectives is described here as the Lightsolve approach.", "num_citations": "2\n", "authors": ["919"]}
{"title": "Procedural authoring of solid models\n", "abstract": " This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects.", "num_citations": "2\n", "authors": ["919"]}
{"title": "Aggregating building fragments generated from geo-referenced imagery into urban models\n", "abstract": " The City Scanning Project of the MIT Computer Graphics Group is developing a system to create automatically three-dimensional models of urban environments from imagery annotated with the position and orientation of the camera, date, and time of day. These models can then be used in visualizations and walk-throughs of the acquired environments. A camera platform equipped with navigation information acquires the large datasets of images necessary to adequately represent the complex nature of the urban environment. Drawing appropriate conclusions from multiple images is difficult without a priori knowledge of the environment or human assistance to identify and correlate important model features. Existing tools to create detailed models from images require large amounts of human effort in both the data acquisition and data analysis stages. The aggregation algorithm presented merges the output from the work of several other graduate students on the project. Each of their programs analyzes the photographic data and generates three-dimensional building fragments. Some of these algorithms may work better and produce more accurate and detailed fragments for particular building structures or portions of environments than others. The aggregation algorithm determines appropriate weights for each these data fragments as it creates surfaces and volumes to represent structure in the urban environment. A graphical interface allows the user to compare building fragment data and inspect the final model.", "num_citations": "2\n", "authors": ["919"]}
{"title": "Random Input and Automated Output Generation in Submitty\n", "abstract": " Fuzzing, testing a codebase against a set of randomly generated inputs, has become a promising model of testing across the industry due to its ability to reveal difficult to detect bugs. Separately, the use of randomized inputs when testing student code submissions removes the potential for student hard-coding behavior. Motivated by these factors, we present a solution for the automated generation of testcase inputs and expected outputs within Submitty, an open source automated grading system from Rensselaer Polytechnic Institute. We detail an enhanced workflow that allows instructors to provide our testing system with an assignment-specific input generation script and an assignment solution. The input generation script is run at student test-time, providing students with either entirely generated inputs or a combination of generated and hand-crafted testcases. The instructor solution is run against the same inputs\u00a0\u2026", "num_citations": "1\n", "authors": ["919"]}
{"title": "Automated and Manual Grading of Web-Based Assignments\n", "abstract": " Grading web-based assignments poses many unique challenges when compared with other types of programming assignments. For introductory courses, grading involves not just validation of source code, but also performing some level of functional testing through a browser environment, where one clicks on content, and validates the browser state. In upper level courses, assignments increasingly use several different services, such as a web server and database, running concurrently, each potentially exposing ports for user access. Finally, for some assignments where students are encouraged to be creative, an instructor must then be able to view and interact with the running code, which has historically meant downloading, setting it up on their local machine, and running it--which can prove burdensome. In this work, we present a system that can perform the task of automated grading, create a web-accessible\u00a0\u2026", "num_citations": "1\n", "authors": ["919"]}
{"title": "Autograding Interactive Computer Graphics Applications\n", "abstract": " We present a system for the automated testing and grading of computer graphics applications. Our system runs, provides input to, and captures image and video output from graphical programming assignments. Instructors use a simple set of commands to script automated keyboard and mouse interactions with student programs at fixed times during execution. The resultant output, including plaintext standard output and mid-execution screenshots and GIFs, are displayed to the student to aid in debugging and ensure compliance with assignment specifications. Student output is automatically evaluated by basic text and image difference operations, or via an instructor-written validation method.", "num_citations": "1\n", "authors": ["919"]}
{"title": "Lichen: customizable, open source plagiarism detection in submitty\n", "abstract": " Prior education research, including Computer Science, has established that students will attempt to cheat and violate academic integrity, with one of the more common forms being code plagiarism. The majority of existing tools for software plagiarism are closed source, requiring instructors to use them in a prescribed configuration and sending student code to a third-party server for analysis. At the core of this analysis is the need to perform a language-specific tokenization of the input program and then to use\" digital fingerprinting\" on the code to identify significant markers. This has required developers to write their own parser for each supported language, which is time-consuming to create and keep up-to-date, and thus a barrier to creation of these tools. Instead we bootstrap new languages into our plagiarism system by leveraging the\" Language Server Protocol\", an initiative to create open-source parsers and\u00a0\u2026", "num_citations": "1\n", "authors": ["919"]}
{"title": "Correlation of a Flexible Late Day Policy with Student Stress and Programming Assignment Plagiarism\n", "abstract": " We investigate the potential for a flexible late submission policy to help address breaches of academic integrity and student stress as they relate to programming assignments. To this end, we examine the use of late days (penalty-free deadline extensions) during our large format, two-semester introductory programming sequence. In particular, we examine correlations between patterns of student assignment submission and student plagiarism. Lexical-analysis-based techniques are used to identify probable cases of plagiarism across student submissions. Furthermore, we present metrics for estimating student stress at submission time. Our metric utilizes the automated grading score of the student\u2019s current and previous submission attempts, the imminence of the assignments deadline, and the availability of late days. We corroborate these patterns with self-reported student surveys conducted at the end of the academic year.", "num_citations": "1\n", "authors": ["919"]}
{"title": "User Experience and Feedback on the RPI Homework Submission Server\n", "abstract": " \u200b The Rensselaer Polytechnic Institute (RPI) Homework Submission Server is an ongoing open source project that is used to collect, compile, and automatically grade the programming homeworks for students in our introductory and sophomore computer science classes. The server handles the viewing of homework, lab, test, and overall grades and late submissions and excused absences on homework. Our first hypothesis is that an electronic submission server is the preferred way for students to submit their coding homeworks because it provides immediate feedback about the correctness of their code and ensures fair and consistent grading since their code is compiled and run with the same test cases, on the same computer. Our second hypothesis is that students appreciate courses with a flexible policy for late submission of homeworks, allowing them to use a specific number of \u201clate days\u201d throughout the semester without penalty. The late day policy for each course can be configured by the course instructor and students can use these late days to better manage their time for difficult assignments and unexpected bugs in their code. We recently conducted a survey on user experience with the Homework Submission Server to test these hypotheses. We received 400 replies from approximately 850 students currently enrolled in courses using the server. This poster presents the results of our survey, including what students do or do not like about the server and specific feedback that we will incorporate as we continue development and expand the server to more courses at RPI and other universities.Significance and Relevance of Topic:\u200b There\u00a0\u2026", "num_citations": "1\n", "authors": ["919"]}
{"title": "Inferring Architectural Designs from Physical Sketches: Application to Daylighting Analysis\n", "abstract": " We present the algorithms and implementation of an interactive system to capture and interpret an architectural design from a collection of small-scale physical elements. The user sketches a proposed design by arranging 3D wall modules and simple markers for windows, materials, and other design features on a table. The color-coded elements are captured by a camera mounted above the scene and recognized using computer vision techniques. The architectural design is automatically inferred from this rough physical sketch and a closed, 3D triangle mesh representation is constructed. We apply the system to architectural daylighting analysis using an interactive global illumination simulation that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. The participants may interactively redesign the geometry and materials of the space by manipulating the physical design elements and visualize the revised lighting simulation.", "num_citations": "1\n", "authors": ["919"]}
{"title": "Robust adaptive 3-d segmentation of vessel laminae from fluorescence confocal microscope images & parallel gpu implementation\n", "abstract": " Extended abstract of a paper presented at Microscopy and Microanalysis 2008 in Albuquerque, New Mexico, USA, August 3 \u2013 August 7, 2008", "num_citations": "1\n", "authors": ["919"]}