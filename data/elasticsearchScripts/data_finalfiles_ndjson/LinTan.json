{"title": "A high throughput string matching architecture for intrusion detection and prevention\n", "abstract": " Network intrusion detection and prevention systems have emerged as one of the most effective ways of providing security to those connected to the network, and at the heart of almost every modern intrusion detection system is a string matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of ten thousand strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds a specialized device is required, one that can maintain tight bounds on worst case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included on\u00a0\u2026", "num_citations": "416\n", "authors": ["456"]}
{"title": "Online Defect Prediction for Imbalanced Data\n", "abstract": " Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice. We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced -- there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance. We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these\u00a0\u2026", "num_citations": "207\n", "authors": ["456"]}
{"title": "AutoComment: Mining Question and Answer Sites for Automatic Comment Generation\n", "abstract": " Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q&A) site. Q&A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings.We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoComment to analyze Java and Android tagged Q&A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate\u00a0\u2026", "num_citations": "197\n", "authors": ["456"]}
{"title": "Do time of day and developer experience affect commit bugginess?\n", "abstract": " Modern software is often developed over many years with hundreds of thousands of commits. Commit metadata is a rich source of social characteristics, including the commit's time of day and the experience and commit frequency of its author. The\" bugginess\" of a commit is also a critical property of that commit. In this paper, we investigate the correlation between a commit's social characteristics and its\" bugginess\"; such results can be very useful for software developers and software engineering researchers. For instance, developers or code reviewers might be well-advised to thoroughly verify commits that are more likely to be buggy. In this paper, we study the correlation between a commit's bugginess and the time of day of the commit, the day of week of the commit, and the experience and commit frequency of the commit authors. We survey two widely-used open source projects: the Linux kernel and PostgreSQL.", "num_citations": "162\n", "authors": ["456"]}
{"title": "Discovering, Reporting, and Fixing Performance Bugs.\n", "abstract": " Software performance is critical for how users perceive the quality of software products. Performance bugs - programming errors that cause significant performance degradation - lead to poor user experience and low system throughput. Designing effective techniques to address performance bugs requires a deep understanding of how performance bugs are discovered, reported, and fixed. In this paper, we study how performance bugs are discovered, reported to developers, and fixed by developers, and compare the results with those for non-performance bugs. We study performance and non-performance bugs from three popular code bases: Eclipse JDT, Eclipse SWT, and Mozilla. First, we find little evidence that fixing performance bugs has a higher chance to introduce new functional bugs than fixing non-performance bugs, which implies that developers may not need to be over-concerned about fixing\u00a0\u2026", "num_citations": "144\n", "authors": ["456"]}
{"title": "Bit-split string-matching engines for intrusion detection and prevention\n", "abstract": " Network Intrusion Detection and Prevention Systems have emerged as one of the most effective ways of providing security to those connected to the network and at the heart of almost every modern intrusion detection system is a string-matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of thousands of strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds, a specialized device is required, one that can maintain tight bounds on worst-case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included\u00a0\u2026", "num_citations": "120\n", "authors": ["456"]}
{"title": "R2Fix: Automatically Generating Bug Fixes from Bug Reports\n", "abstract": " Many bugs, even those that are known and documented in bug reports, remain in mature software for a long time due to the lack of the development resources to fix them. We propose a general approach, R2Fix, to automatically generate bug-fixing patches from free-form bug reports. R2Fix combines past fix patterns, machine learning techniques, and semantic patch generation techniques to fix bugs automatically. We evaluate R2Fix on three projects, i.e., the Linux kernel, Mozilla, and Apache, for three important types of bugs: buffer overflows, null pointer bugs, and memory leaks. R2Fix generates 57 patches correctly, 5 of which are new patches for bugs that have not been fixed by developers yet. We reported all 5 new patches to the developers; 4 have already been accepted and committed to the code repositories. The 57 correct patches generated by R2Fix could have shortened and saved up to an average of\u00a0\u2026", "num_citations": "98\n", "authors": ["456"]}
{"title": "Inferring semantically related words from software context\n", "abstract": " Code search is an integral part of software development and program comprehension. The difficulty of code search lies in the inability to guess the exact words used in the code. Therefore, it is crucial for keyword-based code search to expand queries with semantically related words, e.g., synonyms and abbreviations, to increase the search effectiveness. However, it is limited to rely on resources such as English dictionaries and WordNet to obtain semantically related words in software, because many words that are semantically related in software are not semantically related in English. This paper proposes a simple and general technique to automatically infer semantically related words in software by leveraging the context of words in comments and code. We achieve a reasonable accuracy in seven large and popular code bases written in C and Java. Our further evaluation against the state of art shows that our\u00a0\u2026", "num_citations": "96\n", "authors": ["456"]}
{"title": "Clocom: Mining existing source code for automatic comment generation\n", "abstract": " Code comments are an integral part of software development. They improve program comprehension and software maintainability. The lack of code comments is a common problem in the software industry. Therefore, it is beneficial to generate code comments automatically. In this paper, we propose a general approach to generate code comments automatically by analyzing existing software repositories. We apply code clone detection techniques to discover similar code segments and use the comments from some code segments to describe the other similar code segments. We leverage natural language processing techniques to select relevant comment sentences. In our evaluation, we analyze 42 million lines of code from 1,005 open source projects from GitHub, and use them to generate 359 code comments for 21 Java projects. We manually evaluate the generated code comments and find that only 23.7% of\u00a0\u2026", "num_citations": "93\n", "authors": ["456"]}
{"title": "Better test cases for better automated program repair\n", "abstract": " Automated generate-and-validate program repair techniques (G&V techniques) suffer from generating many overfitted patches due to in-capabilities of test cases. Such overfitted patches are incor-rect patches, which only make all given test cases pass, but fail to fix the bugs. In this work, we propose an overfitted patch detec-tion framework named Opad (Overfitted PAtch Detection). Opad helps improve G&V techniques by enhancing existing test cases to filter out overfitted patches. To enhance test cases, Opad uses fuzz testing to generate new test cases, and employs two test or-acles (crash and memory-safety) to enhance validity checking of automatically-generated patches. Opad also uses a novel metric (named O-measure) for deciding whether automatically-generated patches overfit. Evaluated on 45 bugs from 7 large systems (the same benchmark used by GenProg and SPR), Opad filters out 75.2%(321/427\u00a0\u2026", "num_citations": "90\n", "authors": ["456"]}
{"title": "Architectures for bit-split string scanning in intrusion detection\n", "abstract": " String matching is a critical element of modern intrusion detection systems because it lets a system make decisions based not just on headers, but actual content flowing through the network. Through careful codesign and optimization of an architecture with a new string matching algorithm, the authors show it is possible to build a system that is almost 12 times more efficient than the currently best known approaches", "num_citations": "77\n", "authors": ["456"]}
{"title": "iKernel: Isolating buggy and malicious device drivers using hardware virtualization support\n", "abstract": " The users of today's operating systems demand high reliability and security. However, faults introduced outside of the core operating system by buggy and malicious device drivers can significantly impact these dependability attributes. To help improve driver isolation, we propose an approach that utilizes the latest hardware virtualization support to efficiently sandbox each device driver in its own minimal virtual machine (VM) so that the kernel is protected from faults in these drivers. We present our implementation of a low-overhead virtual-machine based framework which allows reuse of existing drivers. We have constructed a prototype to demonstrate that it is feasible to utilize existing hardware virtualization techniques to allow device drivers in a VM to communicate with devices directly without frequent hardware traps into the virtual machine monitor (VMM). We have implemented a prototype parallel port driver\u00a0\u2026", "num_citations": "69\n", "authors": ["456"]}
{"title": "SWordNet: Inferring Semantically Related Words from Software Context\n", "abstract": " Code search is an integral part of software development and program comprehension. The difficulty of code search lies in the inability to guess the exact words used in the code. Therefore, it is crucial for keyword-based code search to expand queries with semantically related words, e.g., synonyms and abbreviations, to increase the search effectiveness. However, it is limited to rely on resources such as English dictionaries and WordNet to obtain semantically related words in software because many words that are semantically related in software are not semantically related in English. On the other hand, many words that are semantically related in English are not semantically related in software. This paper proposes a simple and general technique to automatically infer semantically related words (referred to as rPairs) in software by leveraging the context of words in comments and code. In addition, we\u00a0\u2026", "num_citations": "59\n", "authors": ["456"]}
{"title": "CRADLE: cross-backend validation to detect and localize bugs in deep learning libraries\n", "abstract": " Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the\u00a0\u2026", "num_citations": "51\n", "authors": ["456"]}
{"title": "Coconut: combining context-aware neural translation models using ensemble for program repair\n", "abstract": " Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages.", "num_citations": "49\n", "authors": ["456"]}
{"title": "Correlations between bugginess and time-based commit characteristics\n", "abstract": " Modern software is often developed over many years with hundreds of thousands of commits. Commit metadata is a rich source of time-based characteristics, including the commit\u2019s time of day and the commit frequency and seniority of its author. The \u201cbugginess\u201d of a commit is also a critical property of that commit. In this paper, we investigate the correlation between a commit\u2019s time-based characteristics and its \u201cbugginess\u201d; such results can be useful for software developers and software engineering researchers. For instance, developers or code reviewers might be well-advised to thoroughly verify commits that are more likely to be buggy. In this paper, we study the correlation between a commit\u2019s bugginess and the time of day of the commit, the day of week of the commit, the commit frequency and seniority of the commit authors, and whether or not the developers have marked a commit as a \u201cstable\u201d commit\u00a0\u2026", "num_citations": "39\n", "authors": ["456"]}
{"title": "Generating Precise Dependencies For Large Software\n", "abstract": " Intra- and inter-module dependencies can be a significant source of technical debt in the long-term software development, especially for large software with millions of lines of code. This paper designs and implements a precise and scalable tool that extracts code dependencies and their utilization for large C/C++ software projects. The tool extracts both symbol-level and module-level dependencies of a software system and identifies potential underutilized and inconsistent dependencies. Such information points to potential refactoring opportunities and help developers perform large-scale refactoring tasks.", "num_citations": "26\n", "authors": ["456"]}
{"title": "Pattern matching technique for high throughput network processing\n", "abstract": " A pattern matching technique for high throughput network processing includes a simple yet powerful special purpose architecture and a set of novel string matching algorithms that can work in unison. The novel set of algorithms allow for bit-level partitioning of rules such that may be more easily implemented in hardware or software. The result is a device that maintains tight worst case bounds on performance, can be updated with new rules without interrupting operation, compiles in seconds instead of hours, and is ten times more efficient than the existing best known solutions in this area.", "num_citations": "22\n", "authors": ["456"]}
{"title": "On the correctness of electronic documents: studying, finding, and localizing inconsistency bugs in PDF readers and files\n", "abstract": " Electronic documents are widely used to store and share information such as bank statements, contracts, articles, maps and tax information. Many different applications exist for displaying a given electronic document, and users rightfully assume that documents will be rendered similarly independently of the application used. However, this is not always the case, and these inconsistencies, regardless of their causes\u2014bugs in the application or the file itself\u2014can become critical sources of miscommunication. In this paper, we present a study on the correctness of PDF documents and readers. We start by manually investigating a large number of real-world PDF documents to understand the frequency and characteristics of cross-reader inconsistencies, and find that such inconsistencies are common\u201413.5% PDF files are inconsistently rendered by at least one popular reader. We then propose an approach to\u00a0\u2026", "num_citations": "16\n", "authors": ["456"]}
{"title": "Code comment analysis for improving software quality\n", "abstract": " Code comments contain a rich amount of information that can be leveraged to improve software maintainability and reliability. This chapter on studying and analyzing free-form and semistructured code comments will help practitioners, researchers, and students learn about (1) the characteristics and content of code comments, (2) the techniques, tools, and measures for studying and analyzing code comments automatically or semiautomatically, and (3) future research directions and challenges. Readers will acquire a unique blend of interdisciplinary techniques, including natural language processing, machine learning, and program analysis, which are also useful for analyzing other software engineering text. This chapter can serve as an introduction to comment study and analysis for practitioners, researchers, and students who are interested in conducting research in this area, applying comment analysis\u00a0\u2026", "num_citations": "12\n", "authors": ["456"]}
{"title": "Towards better utilizing static application security testing\n", "abstract": " Static application security testing (SAST) detects vulnerability warnings through static program analysis. Fixing the vulnerability warnings tremendously improves software quality. However, SAST has not been fully utilized by developers due to various reasons: difficulties in handling a large number of reported warnings, a high rate of false warnings, and lack of guidance in fixing the reported warnings. In this paper, we collaborated with security experts from a commercial SAST product and propose a set of approaches (Priv) to help developers better utilize SAST techniques. First, Priv identifies preferred fix locations for the detected vulnerability warnings, and group them based on the common fix locations. Priv also leverages visualization techniques so that developers can quickly investigate the warnings in groups and prioritize their quality-assurance effort. Second, Priv identifies actionable vulnerability warnings by\u00a0\u2026", "num_citations": "11\n", "authors": ["456"]}
{"title": "CURE: Code-Aware Neural Machine Translation for Automatic Program Repair\n", "abstract": " Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a\u00a0\u2026", "num_citations": "7\n", "authors": ["456"]}
{"title": "em-SPADE: a compiler extension for checking rules extracted from processor specifications\n", "abstract": " Traditional compilers ignore processor specifications, thousands of pages of which are available for modern processors. To bridge this gap, em-SPADE analyzes processor specifications and creates processor-specific rules to reduce low-level programming errors. This work shows the potential of automatically analyzing processor- and other hardware specifications to detect low-level programming errors at compile time. em-SPADE is a compiler extension to automatically detect software bugs in low-level programs. From processor specifications, a preprocessor extracts target-specific rules such as register use and read-only or reserved registers. A special LLVM pass then uses these rules to detect incorrect register assignments. Our experiments with em-SPADE have correctly extracted 652 rules from 15 specifications and consequently found 20 bugs in ten software projects. The work is generalizable to other\u00a0\u2026", "num_citations": "5\n", "authors": ["456"]}
{"title": "Leveraging code comments to improve software reliability\n", "abstract": " Commenting source code has long been a common practice in software development. This thesis, consisting of three pieces of work, made novel use of the code comments written in natural language to improve software reliability. Our solution combines Natural Language Processing (NLP), Machine Learning, Statistics, and Program Analysis techniques to achieve this goal.", "num_citations": "3\n", "authors": ["456"]}