{"title": "Feature selection for classification\n", "abstract": " Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines\u00a0\u2026", "num_citations": "4300\n", "authors": ["1157"]}
{"title": "Toward integrating feature selection algorithms for classification and clustering\n", "abstract": " This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to\u00a0\u2026", "num_citations": "3209\n", "authors": ["1157"]}
{"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution\n", "abstract": " Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality.", "num_citations": "2867\n", "authors": ["1157"]}
{"title": "Feature selection for knowledge discovery and data mining\n", "abstract": " As computer power grows and data collection technologies advance, a plethora of data is generated in almost every field where computers are used. The com puter generated data should be analyzed by computers; without the aid of computing technologies, it is certain that huge amounts of data collected will not ever be examined, let alone be used to our advantages. Even with today's advanced computer technologies (eg, machine learning and data mining sys tems), discovering knowledge from data can still be fiendishly hard due to the characteristics of the computer generated data. Taking its simplest form, raw data are represented in feature-values. The size of a dataset can be measUJ\u00b7 ed in two dimensions, number of features (N) and number of instances (P). Both Nand P can be enormously large. This enormity may cause serious problems to many data mining systems. Feature selection is one of the long existing methods that deal with these problems. Its objective is to select a minimal subset of features according to some reasonable criteria so that the original task can be achieved equally well, if not better. By choosing a minimal subset offeatures, irrelevant and redundant features are removed according to the criterion. When N is reduced, the data space shrinks and in a sense, the data set is now a better representative of the whole data population. If necessary, the reduction of N can also give rise to the reduction of P by eliminating duplicates.", "num_citations": "2730\n", "authors": ["1157"]}
{"title": "Efficient feature selection via analysis of relevance and redundancy\n", "abstract": " Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods.", "num_citations": "2470\n", "authors": ["1157"]}
{"title": "Cross-validation\n", "abstract": " Cross-validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model. In typical cross-validation, the training and validation sets must cross over in successive rounds such that each data point has a chance of being validated against. The basic form of cross-validation is k-fold cross-validation. Other forms of cross-validation are special cases of k-fold cross-validation or involve repeated rounds of k-fold cross-validation. In k-fold cross-validation, the data is first partitioned into k equally (or nearly equally) sized segments or folds. Subsequently k iterations of training and validation are performed such that within each iteration a different fold of the data is held out for validation, while the remaining k 1 folds are used for learning. Figure 1 demonstrates an example with k D 3. The darker section\u00a0\u2026", "num_citations": "1961\n", "authors": ["1157"]}
{"title": "Fake news detection on social media: A data mining perspective\n", "abstract": " Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \\fake news\", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we\u00a0\u2026", "num_citations": "1660\n", "authors": ["1157"]}
{"title": "Subspace clustering for high dimensional data: a review\n", "abstract": " Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset. Often in high dimensional data, many dimensions are irrelevant and can mask existing clusters in noisy data. Feature selection removes irrelevant and redundant dimensions by analyzing the entire dataset. Subspace clustering algorithms localize the search for relevant dimensions allowing them to find clusters that exist in multiple, possibly overlapping subspaces. There are two major branches of subspace clustering based on their search strategy. Top-down algorithms find an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, iteratively improving the results. Bottom-up approaches find dense regions in low dimensional spaces and combine them to form clusters. This paper presents a survey of the various subspace clustering algorithms along with a\u00a0\u2026", "num_citations": "1636\n", "authors": ["1157"]}
{"title": "Computational Methods of Feature Selection\n", "abstract": " Due to increasing demands for dimensionality reduction, research on feature selection has deeply and widely expanded into many fields, including computational statistics, pattern recognition, machine learning, data mining, and knowledge discovery. Highlighting current research issues, Computational Methods of Feature Selection introduces the", "num_citations": "1519\n", "authors": ["1157"]}
{"title": "Feature selection: A data perspective\n", "abstract": " Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the\u00a0\u2026", "num_citations": "1364\n", "authors": ["1157"]}
{"title": "Discretization: An enabling technique\n", "abstract": " Discrete values have important roles in data mining and knowledge discovery. They are about intervals of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from discretization: rules with discrete values are normally shorter and more understandable and discretization can lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require discrete features. All these prompt researchers and practitioners to discretize continuous features before or during a machine learning or data mining task. There are numerous discretization methods available in the literature. It is time for us to examine these seemingly different methods for discretization and find out how different they really are, what are the key\u00a0\u2026", "num_citations": "1197\n", "authors": ["1157"]}
{"title": "Chi2: Feature selection and discretization of numeric attributes\n", "abstract": " Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2 a simple and general algorithm that uses the /spl chi//sup 2/ statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi/sup 2/ is effective in feature selection and discretization of numeric and ordinal attributes.", "num_citations": "1167\n", "authors": ["1157"]}
{"title": "Feature selection for classification: A review\n", "abstract": " Nowadays, the growth of the high-throughput technologies has resulted in exponential growth in the harvested data with respect to both dimensionality and sample size. The trend of this growth of the UCI machine learning repository is shown in Figure 1. Efficient and effective management of these data becomes increasing challenging. Traditionally manual management of these datasets to be impractical. Therefore, data mining and machine learning techniques were developed to automatically discover knowledge and recognize patterns from these data.However, these collected data is usually associated with a high level of noise. There are many reasons causing noise in these data, among which imperfection in the technologies that collected the data and the source of the data itself are two major reasons. For example, in the medical images domain, any deficiency in the imaging device will be reflected as noise for the later process. This kind of noise is caused by the device itself. The development of social media changes the role of online users from traditional content consumers to both content creators and consumers. The quality of social media data varies from excellent data to spam or abuse content by nature. Meanwhile, social media data is usually informally written and suffer from grammatical mistakes, misspelling, and improper punctuation. Undoubtedly, extracting useful knowledge and patterns from such huge and noisy data is a challenging task.", "num_citations": "1161\n", "authors": ["1157"]}
{"title": "Is the sample good enough? comparing data from twitter's streaming api with twitter's firehose\n", "abstract": " Twitter is a social media giant famous for the exchange of short, 140-character messages called\" tweets\". In the scientific community, the microblogging site is known for openness in sharing its data. It provides a glance into its millions of users and billions of tweets through a\" Streaming API\" which provides a sample of all tweets matching some parameters preset by the API user. The API service has been used by many researchers, companies, and governmental institutions that want to extract knowledge in accordance with a diverse array of questions pertaining to social media. The essential drawback of the Twitter API is the lack of documentation concerning what and how much data users get. This leads researchers to question whether the sampled data is a valid representation of the overall activity on Twitter. In this work we embark on answering this question by comparing data collected using Twitter's sampled API service with data collected using the full, albeit costly, Firehose stream that includes every single published tweet. We compare both datasets using common statistical metrics as well as metrics that allow us to compare topics, networks, and locations of tweets. The results of our work will help researchers and practitioners understand the implications of using the Streaming API.", "num_citations": "1135\n", "authors": ["1157"]}
{"title": "Feature extraction, construction and selection: A data mining perspective\n", "abstract": " There is broad interest in feature extraction, construction, and selection among practitioners from statistics, pattern recognition, and data mining to machine learning. Data preprocessing is an essential step in the knowledge discovery process for real-world applications. This book compiles contributions from many leading and active researchers in this growing field and paints a picture of the state-of-art techniques that can boost the capabilities of many existing data mining tools. The objective of this collection is to increase the awareness of the data mining community about the research of feature extraction, construction and selection, which are currently conducted mainly in isolation. This book is part of our endeavor to produce a contemporary overview of modern solutions, to create synergy among these seemingly different branches, and to pave the way for developing meta-systems and novel approaches. Even with today's advanced computer technologies, discovering knowledge from data can still be fiendishly hard due to the characteristics of the computer generated data. Feature extraction, construction and selection are a set of techniques that transform and simplify data so as to make data mining tasks easier. Feature construction and selection can be viewed as two sides of the representation problem.", "num_citations": "1028\n", "authors": ["1157"]}
{"title": "Consistency-based search in feature selection\n", "abstract": " Feature selection is an effective technique in dealing with dimensionality reduction. For classification, it is used to find an \u201coptimal\u201d subset of relevant features such that the overall accuracy of classification is increased while the data size is reduced and the comprehensibility is improved. Feature selection methods contain two important aspects: evaluation of a candidate feature subset and search through the feature space. Existing algorithms adopt various measures to evaluate the goodness of feature subsets. This work focuses on inconsistency measure according to which a feature subset is inconsistent if there exist at least two instances with same feature values but with different class labels. We compare inconsistency measure with other measures and study different search strategies such as exhaustive, complete, heuristic and random search, that can be applied to this measure. We conduct an empirical study\u00a0\u2026", "num_citations": "1013\n", "authors": ["1157"]}
{"title": "A probabilistic approach to feature selection-a filter solution\n", "abstract": " Feature selection can be de ned as a problem of nding a minimum set of M relevant attributes that describes the dataset as well as the original N attributes do, where M N. After examining the problems with both the exhaustive and the heuristic approach to feature selection, this paper proposes a probabilistic approach. The theoretic analysis and the experimental study show that the proposed approach is simple to implement and guaranteed to nd the optimal if resources permit. It is also fast in obtaining results and e ective in selecting features that improve the performance of a learning algorithm. An on-site application involving huge datasets has been conducted independently. It proves the e ectiveness and scalability of the proposed algorithm. Discussed also are various aspects and applications of this feature selection algorithm.", "num_citations": "994\n", "authors": ["1157"]}
{"title": "Spectral feature selection for supervised and unsupervised learning\n", "abstract": " Feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance. Feature selection algorithms are largely studied separately according to the type of learning: supervised or unsupervised. This work exploits intrinsic properties underlying supervised and unsupervised feature selection algorithms, and proposes a unified framework for feature selection based on spectral graph theory. The proposed framework is able to generate families of algorithms for both supervised and unsupervised feature selection. And we show that existing powerful algorithms such as ReliefF (supervised) and Laplacian Score (unsupervised) are special cases of the proposed framework. To the best of our knowledge, this work is the first attempt to unify supervised and unsupervised feature selection, and enable their joint study under a general framework. Experiments\u00a0\u2026", "num_citations": "864\n", "authors": ["1157"]}
{"title": "Social media mining: an introduction\n", "abstract": " The growth of social media over the last decade has revolutionized the way individuals interact and industries conduct business. Individuals produce data at an unprecedented rate by interacting, sharing, and consuming content through social media. Understanding and processing this new type of data to glean actionable patterns presents challenges and opportunities for interdisciplinary research, novel algorithms, and tool development. Social Media Mining integrates social media, social network analysis, and data mining to provide a convenient and coherent platform for students, practitioners, researchers, and project managers to understand the basics and potentials of social media mining. It introduces the unique problems arising from social media data and presents fundamental concepts, emerging issues, and effective algorithms for network analysis and data mining. Suitable for use in advanced undergraduate and beginning graduate courses as well as professional short courses, the text contains exercises of different degrees of difficulty that improve understanding and help apply concepts, principles, and methods in various scenarios of social media mining.", "num_citations": "767\n", "authors": ["1157"]}
{"title": "Identifying the influential bloggers in a community\n", "abstract": " Blogging becomes a popular way for a Web user to publish information on the Web. Bloggers write blog posts, share their likes and dislikes, voice their opinions, provide suggestions, report news, and form groups in Blogosphere. Bloggers form their virtual communities of similar interests. Activities happened in Blogosphere affect the external world. One way to understand the development on Blogosphere is to find influential blog sites. There are many non-influential blog sites which form the\" the long tail\". Regardless of a blog site being influential or not, there are influential bloggers. Inspired by the high impact of the influentials in a physical community, we study a novel problem of identifying influential bloggers at a blog site. Active bloggers are not necessarily influential. Influential bloggers can impact fellow bloggers in various ways. In this paper, we discuss the challenges of identifying influential bloggers\u00a0\u2026", "num_citations": "721\n", "authors": ["1157"]}
{"title": "Relational learning via latent social dimensions\n", "abstract": " Social media such as blogs, Facebook, Flickr, etc., presents data in a network format rather than classical IID distribution. To address the interdependency among data instances, relational learning has been proposed, and collective inference based on network connectivity is adopted for prediction. However, connections in social media are often multi-dimensional. An actor can connect to another actor for different reasons, eg, alumni, colleagues, living in the same city, sharing similar interests, etc. Collective inference normally does not differentiate these connections. In this work, we propose to extract latent social dimensions based on network information, and then utilize them as features for discriminative learning. These social dimensions describe diverse affiliations of actors hidden in the network, and the discriminative learning can automatically determine which affiliations are better aligned with the class labels\u00a0\u2026", "num_citations": "644\n", "authors": ["1157"]}
{"title": "Effective data mining using neural networks\n", "abstract": " Classification is one of the data mining problems receiving great attention recently in the database community. The paper presents an approach to discover symbolic classification rules using neural networks. Neural networks have not been thought suited for data mining because how the classifications were made is not explicitly stated as symbolic rules that are suitable for verification or interpretation by humans. With the proposed approach, concise symbolic rules with high accuracy can be extracted from a neural network. The network is first trained to achieve the required accuracy rate. Redundant connections of the network are then removed by a network pruning algorithm. The activation values of the hidden units in the network are analyzed, and classification rules are generated using the result of this analysis. The effectiveness of the proposed approach is clearly demonstrated by the experimental results on a\u00a0\u2026", "num_citations": "587\n", "authors": ["1157"]}
{"title": "Feature Selection for Clustering - A Filter Solution\n", "abstract": " Processing applications with a large number of dimensions has been a challenge for the KDD community. Feature selection, an effective dimensionality reduction technique, is an essential pre-processing method to remove noisy features. In the literature only a few methods have been proposed for feature selection for clustering, and almost all these methods are 'wrapper' techniques that require a clustering algorithm to evaluate candidate feature subsets. The wrapper approach is largely unsuitable in real-world applications due to its heavy reliance on clustering algorithms that require parameters such as the number of clusters, and the lack of suitable clustering criteria to evaluate clustering in different subspaces. In this paper we propose a 'filter' method that is independent of any clustering algorithm. The proposed method is based on the observation that data with clusters has a very different point-to-point\u00a0\u2026", "num_citations": "556\n", "authors": ["1157"]}
{"title": "Exploring temporal effects for location recommendation on location-based social networks\n", "abstract": " Location-based social networks (LBSNs) have attracted an inordinate number of users and greatly enriched the urban experience in recent years. The availability of spatial, temporal and social information in online LBSNs offers an unprecedented opportunity to study various aspects of human behavior, and enable a variety of location-based services such as location recommendation. Previous work studied spatial and social influences on location recommendation in LBSNs. Due to the strong correlations between a user's check-in time and the corresponding check-in location, recommender systems designed for location recommendation inevitably need to consider temporal effects. In this paper, we introduce a novel location recommendation framework, based on the temporal properties of user movement observed from a real-world LBSN dataset. The experimental results exhibit the significance of temporal\u00a0\u2026", "num_citations": "521\n", "authors": ["1157"]}
{"title": "Incremental learning with support vector machines\n", "abstract": " As real-world databases increase in size, there is a need to scale up inductive learning algorithms to handle more training data. Incremental learning techniques are one possible solution to the scalability problem, where data is processed in parts, and the result combined so as to use less memory. Support Vector Machines (SVMs) have worked well for the batch mode learning and have shown impressive performance in many practical applications. They also have nice properties for summarizing data in the form of support vectors. This suggests one might be able to incorporate them into certain standard frameworks for incremental learning. This paper proposes a framework for incremental learning with SVMs and evaluates its eectiveness using a set of proposed goodness criteria on some standard machine learning benchmark datasets. 1 Introduction Example-based learning is an attractive framework for extracting knowledge from empirical data, with the goal of generalizing w...", "num_citations": "514\n", "authors": ["1157"]}
{"title": "Community detection and mining in social media\n", "abstract": " The past decade has witnessed the emergence of participatory Web and social media, bringing people together in many creative ways. Millions of users are playing, tagging, working, and socializing online, demonstrating new forms of collaboration, communication, and intelligence that were hardly imaginable just a short time ago. Social media also helps reshape business models, sway opinions and emotions, and opens up numerous possibilities to study human interaction and collective behavior in an unparalleled scale. This lecture, from a data mining perspective, introduces characteristics of social media, reviews representative tasks of computing with social media, and illustrates associated challenges. It introduces basic concepts, presents state-of-the-art algorithms with easy-to-understand examples, and recommends effective evaluation methods. In particular, we discuss graph-based community detection\u00a0\u2026", "num_citations": "513\n", "authors": ["1157"]}
{"title": "Neural-network feature selector\n", "abstract": " Feature selection is an integral part of most learning algorithms. Due to the existence of irrelevant and redundant attributes, by selecting only the relevant attributes of the data, higher predictive accuracy can be expected from a machine learning method. In this paper, we propose the use of a three-layer feedforward neural network to select those input attributes that are most useful for discriminating classes in a given set of input patterns. A network pruning algorithm is the foundation of the proposed algorithm. By adding a penalty term to the error function of the network, redundant network connections can be distinguished from those relevant ones by their small weights when the network training process has been completed. A simple criterion to remove an attribute based on the accuracy rate of the network is developed. The network is retrained after removal of an attribute, and the selection process is repeated until\u00a0\u2026", "num_citations": "509\n", "authors": ["1157"]}
{"title": "Cubesvd: a novel approach to personalized web search\n", "abstract": " As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web users' information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web users' interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the\u00a0\u2026", "num_citations": "467\n", "authors": ["1157"]}
{"title": "Twitter data analytics\n", "abstract": " Twitter\u00ae 1 is a massive social networking site tuned towards fast communication. More than 140 million active users publish over 400 million 140-character \u201cTweets\u201d every day. 2 Twitter\u2019s speed and ease of publication have made it an important communication medium for people from all walks of life. Twitter has played a prominent role in socio-political events, such as the Arab Spring 3 and the Occupy Wall Street movement. 4 Twitter has also been used to post damage reports and disaster preparedness information during large natural disasters, such as the Hurricane Sandy.This book is for the reader who is interested in understanding the basics of collecting, storing, and analyzing Twitter data. The first half of this book discusses collection and storage of data. It starts by discussing how to collect Twitter data, looking at the free APIs provided by Twitter. We then goes on to discuss how to store this data for use in\u00a0\u2026", "num_citations": "458\n", "authors": ["1157"]}
{"title": "Social recommendation: a review\n", "abstract": " Recommender systems play an important role in helping online users find relevant information by suggesting information of potential interest to them. Due to the potential value of social relations in recommender systems, social recommendation has attracted increasing attention in recent years. In this paper, we present a review of existing recommender systems and discuss some research directions. We begin by giving formal definitions of social recommendation and discuss the unique property of social recommendation and its implications compared with those of traditional recommender systems. Then, we classify existing social recommender systems into memory-based social recommender systems and model-based social recommender systems, according to the basic models adopted to build the systems, and review representative systems for each category. We also present some key findings from\u00a0\u2026", "num_citations": "458\n", "authors": ["1157"]}
{"title": "Feature selection via discretization\n", "abstract": " Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant and/or redundant attributes. Chi2 is a simple and general algorithm that uses the /spl chi//sup 2/ statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and redundant attributes.", "num_citations": "454\n", "authors": ["1157"]}
{"title": "Feature selection: An ever evolving frontier in data mining\n", "abstract": " The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, communications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining applications. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We first briefly introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vibrant research field of some contemporary interests, new applications, and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.", "num_citations": "427\n", "authors": ["1157"]}
{"title": "Unsupervised sentiment analysis with emotional signals\n", "abstract": " The explosion of social media services presents a great opportunity to understand the sentiment of the public via analyzing its large-scale and opinion-rich data. In social media, it is easy to amass vast quantities of unlabeled data, but very costly to obtain sentiment labels, which makes unsupervised sentiment analysis essential for various applications. It is challenging for traditional lexicon-based unsupervised methods due to the fact that expressions in social media are unstructured, informal, and fast-evolving. Emoticons and product ratings are examples of emotional signals that are associated with sentiments expressed in posts or words. Inspired by the wide availability of emotional signals in social media, we propose to study the problem of unsupervised sentiment analysis with emotional signals. In particular, we investigate whether the signals can potentially help sentiment analysis by providing a unified way to\u00a0\u2026", "num_citations": "424\n", "authors": ["1157"]}
{"title": "Exploiting social relations for sentiment analysis in microblogging\n", "abstract": " Microblogging, like Twitter and Sina Weibo, has become a popular platform of human expressions, through which users can easily produce content on breaking news, public events, or products. The massive amount of microblogging data is a useful and timely source that carries mass sentiment and opinions on various topics. Existing sentiment analysis approaches often assume that texts are independent and identically distributed (iid), usually focusing on building a sophisticated feature space to handle noisy and short texts, without taking advantage of the fact that the microblogs are networked data. Inspired by the social sciences findings that sentiment consistency and emotional contagion are observed in social networks, we investigate whether social relations can help sentiment analysis by proposing a Sociological Approach to handling Noisy and short Texts (SANT) for sentiment classification. In particular, we\u00a0\u2026", "num_citations": "411\n", "authors": ["1157"]}
{"title": "Connecting users across social media sites: a behavioral-modeling approach\n", "abstract": " People use various social media for different purposes. The information on an individual site is often incomplete. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We\u00a0\u2026", "num_citations": "385\n", "authors": ["1157"]}
{"title": "Searching for interacting features in subset selection\n", "abstract": " The evolving and adapting capabilities of robust intelligence are best manifested in its ability to learn. Machine learning enables computer systems to learn, and improve performance. Feature selection facilitates machine learning (eg, classification) by aiming to remove irrelevant features. Feature (attribute) interaction presents a challenge to feature subset selection for classification. This is because a feature by itself might have little correlation with the target concept, but when it is combined with some other features, they can be strongly correlated with the target concept. Thus, the unintentional removal of these features may result in poor classification performance. It is computationally intractable to handle feature interactions in general. However, the presence of feature interaction in a wide range of real-world applications demands practical solutions that can reduce high-dimensional data while preserving feature\u00a0\u2026", "num_citations": "372\n", "authors": ["1157"]}
{"title": "Feature selection for clustering: A review\n", "abstract": " Dimensionality reduction techniques can be categorized mainly into feature extraction and feature selection. In the feature extraction approach, features are projected into a new space with lower dimensionality. Feature selection is broadly categorized into four models: filter model, wrapper model, embedded model, and hybrid model. With the existence of a large number of features, learning models tend to overfit and their learning performance degenerates. Feature selection is one of the most used techniques to reduce dimensionality among practitioners. The existence of irrelevant features in the data set may degrade learning quality and consume more memory and computational time that could be saved if these features were removed. However, finding clusters in high-dimensional space is computationally expensive and may degrade the learning performance. Clustering is useful in several machine learning\u00a0\u2026", "num_citations": "362\n", "authors": ["1157"]}
{"title": "Feature Selection for Clustering\n", "abstract": " Clustering is an important data mining task. Data mining often concerns large and high-dimensional data but unfortunately most of the clustering algorithms in the literature are sensitive to largeness or high-dimensionality or both. Different features affect clusters differently, some are important for clusters while others may hinder the clustering task. An efficient way of handling it is by selecting a subset of important features. It helps in finding clusters efficiently, understanding the data better and reducing data size for efficient storage, collection and processing. The task of finding original important features for unsupervised data is largely untouched. Traditional feature selection algorithms work only for supervised data where class information is available. For unsupervised data, without class information, often principal components (PCs) are used, but PCs still require all features and they may be difficult to\u00a0\u2026", "num_citations": "361\n", "authors": ["1157"]}
{"title": "Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media\n", "abstract": " Social media has become a popular means for people to consume and share the news. At the same time, however, it has also enabled the wide dissemination of fake news, that is, news with intentionally false information, causing significant negative effects on society. To mitigate this problem, the research of fake news detection has recently received a lot of attention. Despite several existing computational solutions on the detection of fake news, the lack of comprehensive and community-driven fake news data sets has become one of major roadblocks. Not only existing data sets are scarce, they do not contain a myriad of features often required in the study such as news content, social context, and spatiotemporal information. Therefore, in this article, to facilitate fake news-related research, we present a fake news data repository FakeNewsNet, which contains two comprehensive data sets with diverse features in\u00a0\u2026", "num_citations": "359\n", "authors": ["1157"]}
{"title": "Leveraging social media networks for classification\n", "abstract": " Social media has reshaped the way in which people interact with each other. The rapid development of participatory web and social networking sites like YouTube, Twitter, and Facebook, also brings about many data mining opportunities and novel challenges. In particular, we focus on classification tasks with user interaction information in a social network. Networks in social media are heterogeneous, consisting of various relations. Since the relation-type information may not be available in social media, most existing approaches treat these inhomogeneous connections homogeneously, leading to an unsatisfactory classification performance. In order to handle the network heterogeneity, we propose the concept of social dimension to represent actors\u2019 latent affiliations, and develop a classification framework based on that. The proposed framework, SocioDim, first extracts social dimensions based on the\u00a0\u2026", "num_citations": "345\n", "authors": ["1157"]}
{"title": "Advancing feature selection research\n", "abstract": " The rapid advance of computer based high-throughput technique have provided unparalleled opportunities for humans to expand capabilities in production, services, communications, and research. Meanwhile, immense quantities of high-dimensional data are accumulated challenging state-of-the-art data mining techniques. Feature selection is an essential step in successful data mining applications, which can effectively reduce data dimensionality by removing the irrelevant (and the redundant) features. In the past few decades, researchers have developed large amount of feature selection algorithms. These algorithms are designed to serve different purposes, are of different models, and all have their own advantages and disadvantages. Although there have been intensive efforts on surveying existing feature selection algorithms, to the best of our knowledge, there is still not a dedicated repository that collects the representative feature selection algorithms to facilitate their comparison and joint study. To fill this gap, in this work we present a feature selection repository, which is designed to collect the most popular algorithms that have been developed in the feature selection research to serve as a platform for facilitating their application, comparison and joint study. The repository also effectively assists researchers to achieve more reliable evaluation in the process of developing new feature selection algorithms.", "num_citations": "343\n", "authors": ["1157"]}
{"title": "Beyond news contents: The role of social context for fake news detection\n", "abstract": " Social media is becoming popular for news consumption due to its fast dissemination, easy access, and low cost. However, it also enables the wide propagation of fake news, ie, news with intentionally false information. Detecting fake news is an important task, which not only ensures users receive authentic information but also helps maintain a trustworthy news ecosystem. The majority of existing detection algorithms focus on finding clues from news contents, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. Therefore, we need to explore auxiliary information to improve detection. The social context during news dissemination process on social media forms the inherent tri-relationship, the relationship among publishers, news pieces, and users, which has the potential to improve fake news detection. For example, partisan-biased publishers\u00a0\u2026", "num_citations": "338\n", "authors": ["1157"]}
{"title": "Content-aware point of interest recommendation on location-based social networks\n", "abstract": " The rapid urban expansion has greatly extended the physical boundary of users' living area and developed a large number of POIs (points of interest). POI recommendation is a task that facilitates users' urban exploration and helps them filter uninteresting POIs for decision making. While existing work of POI recommendation on location-based social networks (LBSNs) discovers the spatial, temporal, and social patterns of user check-in behavior, the use of content information has not been systematically studied. The various types of content information available on LBSNs could be related to different aspects of a user's check-in action, providing a unique opportunity for POI recommendation. In this work, we study the content information on LBSNs wrt POI properties, user interests, and sentiment indications. We model the three types of information under a unified POI recommendation framework with the consideration of their relationship to check-in actions. The experimental results exhibit the significance of content information in explaining user behavior, and demonstrate its power to improve POI recommendation performance on LBSNs.", "num_citations": "338\n", "authors": ["1157"]}
{"title": "Community evolution in dynamic multi-mode networks\n", "abstract": " A multi-mode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur. Identifying communities in a multi-mode network can help understand the structural properties of the network, address the data shortage and unbalanced problems, and assist tasks like targeted marketing and finding influential actors within or between groups. In general, a network and the membership of groups often evolve gradually. In a dynamic multi-mode network, both actor membership and interactions can evolve, which poses a challenging problem of identifying community evolution. In this work, we try to address this issue by employing the temporal information to analyze a multi-mode network. A spectral framework and its scalability issue are carefully studied. Experiments on both synthetic data and real-world large scale networks demonstrate the efficacy of our algorithm\u00a0\u2026", "num_citations": "329\n", "authors": ["1157"]}
{"title": "Sarcasm detection on twitter: A behavioral modeling approach\n", "abstract": " Sarcasm is a nuanced form of language in which individuals state the opposite of what is implied. With this intentional ambiguity, sarcasm detection has always been a challenging task, even for humans. Current approaches to automatic sarcasm detection rely primarily on lexical and linguistic cues. This paper aims to address the difficult task of sarcasm detection on Twitter by leveraging behavioral traits intrinsic to users expressing sarcasm. We identify such traits using the user's past tweets. We employ theories from behavioral and psychological studies to construct a behavioral modeling framework tuned for detecting sarcasm. We evaluate our framework and demonstrate its efficiency in identifying sarcastic tweets.", "num_citations": "324\n", "authors": ["1157"]}
{"title": "Semi-supervised feature selection via spectral analysis\n", "abstract": " Feature selection is an important task in effective data mining. A new challenge to feature selection is the so-called \u201csmall labeled-sample problem\u201d in which labeled data is small and unlabeled data is large. The paucity of labeled instances provides insufficient information about the structure of the target concept, and can cause supervised feature selection algorithms to fail. Unsupervised feature selection algorithms can work without labeled data. However, these algorithms ignore label information, which may lead to performance deterioration. In this work, we propose to use both (small) labeled and (large) unlabeled data in feature selection, which is a topic has not yet been addressed in feature selection research. We present a semi-supervised feature selection algorithm based on spectral analysis. The algorithm exploits both labeled and unlabeled data through a regularization framework, which provides an\u00a0\u2026", "num_citations": "324\n", "authors": ["1157"]}
{"title": "Exploring social-historical ties on location-based social networks\n", "abstract": " Location-based social networks (LBSNs) have become a popular form of social media in recent years. They provide location related services that allow users to\" check-in''at geographical locations and share such experiences with their friends. Millions of\" check-in''records in LBSNs contain rich information of social and geographical context and provide a unique opportunity for researchers to study user's social behavior from a spatial-temporal aspect, which in turn enables a variety of services including place advertisement, traffic forecasting, and disaster relief. In this paper, we propose a social-historical model to explore user's check-in behavior on LBSNs. Our model integrates the social and historical effects and assesses the role of social correlation in user's check-in behavior. In particular, our model captures the property of user's check-in history in forms of power-law distribution and short-term effect, and helps in explaining user's check-in behavior. The experimental results on a real world LBSN demonstrate that our approach properly models user's check-ins and shows how social and historical ties can help location prediction.", "num_citations": "313\n", "authors": ["1157"]}
{"title": "On similarity preserving feature selection\n", "abstract": " In the literature of feature selection, different criteria have been proposed to evaluate the goodness of features. In our investigation, we notice that a number of existing selection criteria implicitly select features that preserve sample similarity, and can be unified under a common framework. We further point out that any feature selection criteria covered by this framework cannot handle redundant features, a common drawback of these criteria. Motivated by these observations, we propose a new \"Similarity Preserving Feature Selection\u201d framework in an explicit and rigorous way. We show, through theoretical analysis, that the proposed framework not only encompasses many widely used feature selection criteria, but also naturally overcomes their common weakness in handling feature redundancy. In developing this new framework, we begin with a conventional combinatorial optimization formulation for similarity\u00a0\u2026", "num_citations": "308\n", "authors": ["1157"]}
{"title": "Redundancy based feature selection for microarray data\n", "abstract": " In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sample size. Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes. Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our\u00a0\u2026", "num_citations": "299\n", "authors": ["1157"]}
{"title": "Exploiting homophily effect for trust prediction\n", "abstract": " Trust plays a crucial role for online users who seek reliable information. However, in reality, user-specified trust relations are very sparse, ie, a tiny number of pairs of users with trust relations are buried in a disproportionately large number of pairs without trust relations, making trust prediction a daunting task. As an important social concept, however, trust has received growing attention and interest. Social theories are developed for understanding trust. Homophily is one of the most important theories that explain why trust relations are established. Exploiting the homophily effect for trust prediction provides challenges and opportunities. In this paper, we embark on the challenges to investigate the trust prediction problem with the homophily effect. First, we delineate how it differs from existing approaches to trust prediction in an unsupervised setting. Next, we formulate the new trust prediction problem into an\u00a0\u2026", "num_citations": "286\n", "authors": ["1157"]}
{"title": "Consistency based feature selection\n", "abstract": " Feature selection is an effective technique in dealing with dimensionality reduction for classification task, a main component of data mining. It searches for an \u201coptimal\u201d subset of features. The search strategies under consideration are one of the three: complete, heuristic, and probabilistic. Existing algorithms adopt various measures to evaluate the goodness of feature subsets. This work focuses on one measure called consistency. We study its properties in comparison with other major measures and different ways of using this measure in search of feature subsets. We conduct an empirical study to examine the pros and cons of these different search methods using consistency. Through this extensive exercise, we aim to provide a comprehensive view of this measure and its relations with other measures and a guideline of the use of this measure with different search strategies facing a new application.", "num_citations": "283\n", "authors": ["1157"]}
{"title": "Scalable learning of collective behavior based on sparse social dimensions\n", "abstract": " The study of collective behavior is to understand how individuals behave in a social network environment. Oceans of data generated by social media like Facebook, Twitter, Flickr and YouTube present opportunities and challenges to studying collective behavior in a large scale. In this work, we aim to learn to predict collective behavior in social media. In particular, given information about some individuals, how can we infer the behavior of unobserved individuals in the same network? A social-dimension based approach is adopted to address the heterogeneity of connections presented in social media. However, the networks in social media are normally of colossal size, involving hundreds of thousands or even millions of actors. The scale of networks entails scalable learning of models for collective behavior prediction. To address the scalability issue, we propose an edge-centric clustering scheme to extract sparse\u00a0\u2026", "num_citations": "282\n", "authors": ["1157"]}
{"title": "Instance selection and construction for data mining\n", "abstract": " The ability to analyze and understand massive data sets lags far behind the ability to gather and store the data. To meet this challenge, knowledge discovery and data mining (KDD) is growing rapidly as an emerging field. However, no matter how powerful computers are now or will be in the future, KDD researchers and practitioners must consider how to manage ever-growing data which is, ironically, due to the extensive use of computers and ease of data collection with computers. Many different approaches have been used to address the data explosion issue, such as algorithm scale-up and data reduction. Instance, example, or tuple selection pertains to methods or algorithms that select or search for a representative portion of data that can fulfill a KDD task as if the whole data is used. Instance selection is directly related to data reduction and becomes increasingly important in many KDD applications due to the need for processing efficiency and/or storage efficiency. One of the major means of instance selection is sampling whereby a sample is selected for testing and analysis, and randomness is a key element in the process. Instance selection also covers methods that require search. Examples can be found in density estimation (finding the representative instances-data points-for a cluster); boundary hunting (finding the critical instances to form boundaries to differentiate data points of different classes); and data squashing (producing weighted new data with equivalent sufficient statistics). Other important issues related to instance selection extend to unwanted precision, focusing, concept drifts, noise/outlier removal, data smoothing, etc\u00a0\u2026", "num_citations": "273\n", "authors": ["1157"]}
{"title": "Tweettracker: An analysis tool for humanitarian and disaster relief\n", "abstract": " Social media is becoming popular as a key source of information on disasters and crisis situations. Humanitarian Aid and Disaster Relief (HADR) respondents can gain valuable insights and situational awareness by monitoring social mediabased feeds. Specifically, the use of microblogs (ie, Twitter) has been shown to provide new information not otherwise attainable. In this paper, we present a new application designed to help HADR relief organizations to track, analyze, and monitor tweets. The purpose of this tool is to help these first responders gain situational awareness immediately after a disaster or crisis. The tool is capable of monitoring and analyzing location and keyword specific Tweets with nearreal-time trending, data reduction, historical review, and integrated data mining tools. In this paper, we discuss the utility of this tool through a case study on tweets related to the Cholera crisis in Haiti.", "num_citations": "270\n", "authors": ["1157"]}
{"title": "Attributed network embedding for learning in a dynamic environment\n", "abstract": " Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network. The learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. Most, if not all, of the existing works, are overwhelmingly performed in the context of plain and static networks. Nonetheless, in reality, network structure often evolves over time with addition/deletion of links and nodes. Also, a vast majority of real-world networks are associated with a rich set of node attributes, and their attribute values are also naturally changing, with the emerging of new content patterns and the fading of old content patterns. These changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which is of fundamental importance for learning in a dynamic\u00a0\u2026", "num_citations": "268\n", "authors": ["1157"]}
{"title": "Symbolic representation of neural networks\n", "abstract": " Neural networks often surpass decision trees in predicting pattern classifications, but their predictions cannot be explained. This algorithm's symbolic representations make each prediction explicit and understandable. Our approach to understanding a neural network uses symbolic rules to represent the network decision process. The algorithm, NeuroRule, extracts these rules from a neural network. The network can be interpreted by the rules which, in general, preserve network accuracy and explain the prediction process. We based NeuroRule on a standard three layer feed forward network. NeuroRule consists of four phases. First, it builds a weight decay backpropagation network so that weights reflect the importance of the network's connections. Second, it prunes the network to remove irrelevant connections and units while maintaining the network's predictive accuracy. Third, it discretizes the hidden unit\u00a0\u2026", "num_citations": "267\n", "authors": ["1157"]}
{"title": "Efficient spectral feature selection with minimum redundancy\n", "abstract": " Spectral feature selection identifies relevant features by measuring their capability of preserving sample similarity. It provides a powerful framework for both supervised and unsupervised feature selection, and has been proven to be effective in many real-world applications. One common drawback associated with most existing spectral feature selection algorithms is that they evaluate features individually and cannot identify redundant features. Since redundant features can have significant adverse effect on learning performance, it is necessary to address this limitation for spectral feature selection. To this end, we propose a novel spectral feature selection algorithm to handle feature redundancy, adopting an embedded model. The algorithm is derived from a formulation based on a sparse multi-output regression with a L 2, 1-norm constraint. We conduct theoretical analysis on the properties of its optimal solutions, paving the way for designing an efficient path-following solver. Extensive experiments show that the proposed algorithm can do well in both selecting relevant features and removing redundancy.", "num_citations": "263\n", "authors": ["1157"]}
{"title": "On issues of instance selection\n", "abstract": " The digital technologies and computer advances with the booming internet uses have led to massive data collection (corporate data, data warehouses, webs, just to name a few) and information (or misinformation) explosion. Szalay and Gray described this phenomenon as \u201cdrowning in data\u201d(Szalay and Gray, 1999). They reported that each year the detectors at the CERN particle collider in Switzerland record 1 petabyte of data; and researchers in areas of science from astronomy to the human genome are facing the same problems and choking on information. A very natural question is \u201cnow that we have gathered so much data, what do we do with it?\u201d Raw data is rarely of direct use and manual analysis simply cannot keep pace with the fast growth of data. Data mining and knowledge discovery (KDD), as a new emerging field comprising disciplines such as databases, statistics, machine learning, comes to the rescue. KDD attempts to turn raw data into nuggets and create special edges in this ever competitive world for science discovery and business intelligence. The KDD process is defined in Fayyad et al.(1996) as the nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data. Data Mining processes include data selection, preprocessing, data mining, interpretation and evaluation. The first two processes (data selection and preprocessing) play a pivotal role in successful data mining. Facing the mounting challenges of enormous amounts of data, much of the current research concerns itself with scaling up data mining algorithms (Provost and Kolluri, 1999). Researchers have also worked on\u00a0\u2026", "num_citations": "258\n", "authors": ["1157"]}
{"title": "Mining social media: a brief introduction\n", "abstract": " The pervasive use of social media has generated unprecedented amounts of social data. Social media provides easily an accessible platform for users to share information. Mining social media has its potential to extract actionable patterns that can be beneficial for business, users, and consumers. Social media data are vast, noisy, unstructured, and dynamic in nature, and thus novel challenges arise. This tutorial reviews the basics of data mining and social media, introduces representative research problems of mining social media, illustrates the application of data mining to social media using examples, and describes some projects of mining social media for humanitarian assistance and disaster relief for real-world applications.", "num_citations": "256\n", "authors": ["1157"]}
{"title": "mTrust: Discerning multi-faceted trust in a connected world\n", "abstract": " Traditionally, research about trust assumes a single type of trust between users. However, trust, as a social concept, inherently has many facets indicating multiple and heterogeneous trust relationships between users. Due to the presence of a large trust network for an online user, it is necessary to discern multi-faceted trust as there are naturally experts of different types. Our study in product review sites reveals that people place trust differently to different people. Since the widely used adjacency matrix cannot capture multi-faceted trust relationships between users, we propose a novel approach by incorporating these relationships into traditional rating prediction algorithms to reliably estimate their strengths. Our work results in interesting findings such as heterogeneous pairs of reciprocal links. Experimental results on real-world data from Epinions and Ciao show that our work of discerning multi-faceted trust can be\u00a0\u2026", "num_citations": "254\n", "authors": ["1157"]}
{"title": "Evolving feature selection\n", "abstract": " Data preprocessing is an indispensable step in effective data analysis. It prepares data for data mining and machine learning, which aim to turn data into business intelligence or knowledge. Feature selection is a preprocessing technique commonly used on high-dimensional data. Feature selection studies how to select a subset or list of attributes or variables that are used to construct models describing data. Its purposes include reducing dimensionality, removing irrelevant and redundant features, reducing the amount of data needed for learning, improving algorithms' predictive accuracy, and increasing the constructed models' comprehensibility. This article considers feature-selection overfitting with small-sample classifier design; feature selection for unlabeled data; variable selection using ensemble methods; minimum redundancy-maximum relevance feature selection; and biological relevance in feature\u00a0\u2026", "num_citations": "244\n", "authors": ["1157"]}
{"title": "Entropy-based fuzzy clustering and fuzzy modeling\n", "abstract": " Fuzzy clustering is capable of finding vague boundaries that crisp clustering fails to obtain. But time complexity of fuzzy clustering is usually high, and the need to specify complicated parameters hinders its use. In this paper, an entropy-based fuzzy clustering method is proposed. It automatically identifies the number and initial locations of cluster centers. It calculates the entropy at each data point and selects the data point with minimum entropy as the first cluster center. Next it removes all data points having similarity larger than a threshold with the chosen cluster center. This process is repeated till all data points are removed. Unlike previous methods of its kind, it does not need to revise entropy value for each data point after a cluster center is determined. This saves a lot of time. Also it requires just two parameters that are easy to specify. It is able to find the natural clusters in the data. The clustering method is also\u00a0\u2026", "num_citations": "241\n", "authors": ["1157"]}
{"title": "Connecting corresponding identities across communities\n", "abstract": " One of the most interesting challenges in the area of social computing and social media analysis is the so-called community analysis. A well known barrier in cross-community (multiple website) analysis is the disconnectedness of these websites. In this paper, our aim is to provide evidence on the existence of a mapping among identities across multiple communities, providing a method for connecting these websites. Our studies have shown that simple, yet effective approaches, which leverage social media's collective patterns can be utilized to find such a mapping. The employed methods successfully reveal this mapping with 66% accuracy.", "num_citations": "233\n", "authors": ["1157"]}
{"title": "Dimensionality reduction of unsupervised data\n", "abstract": " Dimensionality reduction is an important problem for efficient handling of large databases. Many feature selection methods exist for supervised data having class information. Little work has been done for dimensionality reduction of unsupervised data in which class information is not available. Principal component analysis (PCA) is often used. However, PCA creates new features. It is difficult to obtain intuitive understanding of the data using the new features only. We are concerned with the problem of determining and choosing the important original features for unsupervised data. Our method is based on the observation that removing an irrelevant feature from the feature set may not change the underlying concept of the data, but not so otherwise. We propose an entropy measure for ranking features, and conduct extensive experiments to show that our method is able to find the important features. Also it compares\u00a0\u2026", "num_citations": "230\n", "authors": ["1157"]}
{"title": "Big scholarly data: A survey\n", "abstract": " With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research\u00a0\u2026", "num_citations": "225\n", "authors": ["1157"]}
{"title": "A survey of signed network mining in social media\n", "abstract": " Many real-world relations can be represented by signed networks with positive and negative links, as a result of which signed network analysis has attracted increasing attention from multiple disciplines. With the increasing prevalence of social media networks, signed network analysis has evolved from developing and measuring theories to mining tasks. In this article, we present a review of mining signed networks in the context of social media and discuss some promising research directions and new frontiers. We begin by giving basic concepts and unique properties and principles of signed networks. Then we classify and review tasks of signed network mining with representative algorithms. We also delineate some tasks that have not been extensively studied with formal definitions and also propose research directions to expand the field of signed network mining.", "num_citations": "225\n", "authors": ["1157"]}
{"title": "Neurorule: A connectionist approach to data mining\n", "abstract": " Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. Approaches proposed so far for mining classification rules for large databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge generated by neural networks is not explicitly represented in the form of rules suitable for verification or interpretation by humans. This paper examines this issue. With our newly developed algorithms, rules which are similar to, or more concise than those generated by the symbolic methods can be extracted from the neural networks. The data mining process using neural networks with the emphasis on rule extraction is described. Experimental results and comparison with previously published works are presented.", "num_citations": "221\n", "authors": ["1157"]}
{"title": "Exploiting local and global social context for recommendation\n", "abstract": " With the fast development of social media, the information overload problem becomes increasingly severe and recommender systems play an important role in helping online users find relevant information by suggesting information of potential interests. Social activities for online users produce abundant social relations. Social relations provide an independent source for recommendation, presenting both opportunities and challenges for traditional recommender systems. Users are likely to seek suggestions from both their local friends and users with high global reputations, motivating us to exploit social relations from local and global perspectives for online recommender systems in this paper. We develop approaches to capture local and global social relations, and propose a novel framework LOCABAL taking advantage of both local and global social context for recommendation. Empirical results on real-world datasets demonstrate the effectiveness of our proposed framework and further experiments are conducted to understand how local and global social context work for the proposed framework.", "num_citations": "221\n", "authors": ["1157"]}
{"title": "Embedded unsupervised feature selection\n", "abstract": " Sparse learning has been proven to be a powerful techniquein supervised feature selection, which allows toembed feature selection into the classification (or regression) problem. In recent years, increasing attentionhas been on applying spare learning in unsupervisedfeature selection. Due to the lack of label information, the vast majority of these algorithms usually generatecluster labels via clustering algorithms and then formulateunsupervised feature selection as sparse learningbased supervised feature selection with these generatedcluster labels. In this paper, we propose a novel unsupervisedfeature selection algorithm EUFS, which directlyembeds feature selection into a clustering algorithm viasparse learning without the transformation. The AlternatingDirection Method of Multipliers is used to addressthe optimization problem of EUFS. Experimentalresults on various benchmark datasets demonstrate theeffectiveness of the proposed framework EUFS.", "num_citations": "220\n", "authors": ["1157"]}
{"title": "Data mining in social media\n", "abstract": " The rise of online social media is providing a wealth of social network data. Data mining techniques provide researchers and practitioners the tools needed to analyze large, complex, and frequently changing social media data. This chapter introduces the basics of data mining, reviews social media, discusses how to mine social media data, and highlights some illustrative examples with an emphasis on social networking sites and blogs.", "num_citations": "218\n", "authors": ["1157"]}
{"title": "Tracing fake-news footprints: Characterizing social media messages by how they propagate\n", "abstract": " When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classification of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specifically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to\u00a0\u2026", "num_citations": "213\n", "authors": ["1157"]}
{"title": "Social spammer detection in microblogging\n", "abstract": " The availability of microblogging, like Twitter and Sina Weibo, makes it a popular platform for spammers to unfairly overpower normal users with unwanted content via social networks, known as social spamming. The rise of social spamming can significantly hinder the use of microblogging systems for effective information dissemination and sharing. Distinct features of microblogging systems present new challenges for social spammer detection. First, unlike traditional social networks, microblogging allows to establish some connections between two parties without mutual consent, which makes it easier for spammers to imitate normal users by quickly accumulating a large number of``human\" friends. Second, microblogging messages are short, noisy, and unstructured. Traditional social spammer detection methods are not directly applicable to microblogging. In this paper, we investigate how to collectively use network and content information to perform effective social spammer detection in microblogging. In particular, we present an optimization formulation that models the social network and content information in a unified framework. Experiments on a real-world Twitter dataset demonstrate that our proposed method can effectively utilize both kinds of information for social spammer detection.", "num_citations": "211\n", "authors": ["1157"]}
{"title": "Global and local structure preservation for feature selection\n", "abstract": " The recent literature indicates that preserving global pairwise sample similarity is of great importance for feature selection and that many existing selection criteria essentially work in this way. In this paper, we argue that besides global pairwise sample similarity, the local geometric structure of data is also critical and that these two factors play different roles in different learning scenarios. In order to show this, we propose a global and local structure preservation framework for feature selection (GLSPFS) which integrates both global pairwise sample similarity and local geometric data structure to conduct feature selection. To demonstrate the generality of our framework, we employ methods that are well known in the literature to model the local geometric data structure and develop three specific GLSPFS-based feature selection algorithms. Also, we develop an efficient optimization algorithm with proven global\u00a0\u2026", "num_citations": "208\n", "authors": ["1157"]}
{"title": "Handling concept drifts in incremental learning with support vector machines\n", "abstract": " With the increase in the size of real-world databases, there is an ever-increasing need to scale up inductive learning algorithms. Incremental learning techniques are one possible solution to the scalability problem. In this paper, we propose three ctiteria to evaluate the robustness and reliability of incremental learning methods, and use them to study the robustness of an incremental training method for Support Vector Machines. We provide empirical results using benchmark machine learning datasets to show that support vectors form a svccdnct and suficient set for block-by-block incremental learning.", "num_citations": "208\n", "authors": ["1157"]}
{"title": "Text analytics in social media\n", "abstract": " The rapid growth of online social media in the form of collaborativelycreated content presents new opportunities and challenges to both producers and consumers of information. With the large amount of data produced by various social media services, text analytics provides an effective way to meet usres\u2019 diverse information needs. In this chapter, we first introduce the background of traditional text analytics and the distinct aspects of textual data in social media. We next discuss the research progress of applying text analytics in social media from different perspectives, and show how to improve existing approaches to text representation in social media, using real-world examples.", "num_citations": "207\n", "authors": ["1157"]}
{"title": "eTrust: Understanding trust evolution in an online world\n", "abstract": " Most existing research about online trust assumes static trust relations between users. As we are informed by social sciences, trust evolves as humans interact. Little work exists studying trust evolution in an online world. Researching online trust evolution faces unique challenges because more often than not, available data is from passive observation. In this paper, we leverage social science theories to develop a methodology that enables the study of online trust evolution. In particular, we propose a framework of evolution trust, eTrust, which exploits the dynamics of user preferences in the context of online product review. We present technical details about modeling trust evolution, and perform experiments to show how the exploitation of trust evolution can help improve the performance of online applications such as rating and trust prediction.", "num_citations": "201\n", "authors": ["1157"]}
{"title": "Signed network embedding in social media\n", "abstract": " Network embedding is to learn low-dimensional vector representations for nodes of a given social network, facilitating many tasks in social network analysis such as link prediction. The vast majority of existing embedding algorithms are designed for unsigned social networks or social networks with only positive links. However, networks in social media could have both positive and negative links, and little work exists for signed social networks. From recent findings of signed network analysis, it is evident that negative links have distinct properties and added value besides positive links, which brings about both challenges and opportunities for signed network embedding. In this paper, we propose a deep learning framework SiNE for signed network embedding. The framework optimizes an objective function guided by social theories that provide a fundamental understanding of signed social networks. Experimental\u00a0\u2026", "num_citations": "200\n", "authors": ["1157"]}
{"title": "Feature selection and classification-a probabilistic wrapper approach\n", "abstract": " ABSTRACT self as a criterion in selecting features since in the context of learning classification rules, the purpose Feature selection is defined as a problem to find of feature selection is to improve the performance of a minimum set of M features for an inductive al an induction algorithm. However, incorporating an gorithm to achieve the highest predictive accuracy induction algorithm in the process of feature selecfrom the data described by the original N features tion is not without a cost.(More discussion below). where MS N. A probabilistic wrapper model is In each category, methods can be further divided proposed as another method besides the exhaus into two types: exhaustive or heuristic search. The tive search and the heuristic approach. The aim of difficulty about feature selection can be stated as folthis model is to avoid local minima and exhaustive lows: except in a few very special cases, the optimal search. The highest predictive accuracy is the crite selection can only be done by testing all possible sets rion in search of the smallest M. Analysis and ex of M features chosen from the N features, ie, by apperiments show that this model can effectively find plying the criterion (M)= M (NEM)! times. If there relevant features and remove irrelevant ones in the are M relevant features, the total number of times context of improving the predictive accuracy of an is:(X)= O (NM). This is prohibitive when induction algorithm. It is simple, straightforward, N and/or M is large. In practice, heuristic methand providing fast solutions while searching for the ods are the way out of this exponential computation. optimal. The applications of such a model, its future\u00a0\u2026", "num_citations": "200\n", "authors": ["1157"]}
{"title": "A selective sampling approach to active feature selection\n", "abstract": " Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. Traditional feature selection methods resort to random sampling in dealing with data sets with a huge number of instances. In this paper, we introduce the concept of active feature selection, and investigate a selective sampling approach to active feature selection in a filter model setting. We present a formalism of selective sampling based on data variance, and apply it to a widely used feature selection algorithm Relief. Further, we show how it realizes active feature selection and reduces the required number of training instances to achieve time savings without performance deterioration. We design objective evaluation measures of performance, conduct extensive experiments using both synthetic and\u00a0\u2026", "num_citations": "196\n", "authors": ["1157"]}
{"title": "What your images reveal: Exploiting visual contents for point-of-interest recommendation\n", "abstract": " The rapid growth of Location-based Social Networks (LBSNs) provides a vast amount of check-in data, which facilitates the study of point-of-interest (POI) recommendation. The majority of the existing POI recommendation methods focus on four aspects, ie, temporal patterns, geographical influence, social correlations and textual content indications. For example, user's visits to locations have temporal patterns and users are likely to visit POIs near them. In real-world LBSNs such as Instagram, users can upload photos associating with locations. Photos not only reflect users' interests but also provide informative descriptions about locations. For example, a user who posts many architecture photos is more likely to visit famous landmarks; while a user posts lots of images about food has more incentive to visit restaurants. Thus, images have potentials to improve the performance of POI recommendation. However, little\u00a0\u2026", "num_citations": "195\n", "authors": ["1157"]}
{"title": "gSCorr: Modeling geo-social correlations for new check-ins on location-based social networks\n", "abstract": " Location-based social networks (LBSNs) have attracted an increasing number of users in recent years. The availability of geographical and social information of online LBSNs provides an unprecedented opportunity to study the human movement from their socio-spatial behavior, enabling a variety of location-based services. Previous work on LBSNs reported limited improvements from using the social network information for location prediction; as users can check-in at new places, traditional work on location prediction that relies on mining a user's historical trajectories is not designed for this\" cold start\" problem of predicting new check-ins. In this paper, we propose to utilize the social network information for solving the\" cold start\" location prediction problem, with a geo-social correlation model to capture social correlations on LBSNs considering social networks and geographical distance. The experimental results\u00a0\u2026", "num_citations": "193\n", "authors": ["1157"]}
{"title": "Unsupervised feature selection for linked social media data\n", "abstract": " The prevalent use of social media produces mountains of unlabeled, high-dimensional data. Feature selection has been shown effective in dealing with high-dimensional data for efficient data mining. Feature selection for unlabeled data remains a challenging task due to the absence of label information by which the feature relevance can be assessed. The unique characteristics of social media data further complicate the already challenging problem of unsupervised feature selection,(eg, part of social media data is linked, which makes invalid the independent and identically distributed assumption), bringing about new challenges to traditional unsupervised feature selection algorithms. In this paper, we study the differences between social media data and traditional attribute-value data, investigate if the relations revealed in linked data can be used to help select relevant features, and propose a novel unsupervised\u00a0\u2026", "num_citations": "193\n", "authors": ["1157"]}
{"title": "defend: Explainable fake news detection\n", "abstract": " In recent years, to mitigate the problem of fake news, computational detection of fake news has been studied, producing some promising early results. While important, however, we argue that a critical missing piece of the study be the explainability of such detection, ie, why a particular piece of news is detected as fake. In this paper, therefore, we study the explainable detection of fake news. We develop a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. We conduct extensive experiments on real-world datasets and demonstrate that the proposed method not only significantly outperforms 7 state-of-the-art fake news detection methods by at least 5.33% in F1-score, but also (concurrently) identifies top-k user comments that explain why a news piece is fake, better than\u00a0\u2026", "num_citations": "189\n", "authors": ["1157"]}
{"title": "Exception rule mining with a relative interestingness measure\n", "abstract": " This paper presents a method for mining exception rules based on a novel measure which estimates interestingness relative to its corresponding common sense rule and reference rule. Mining interesting rules is one of the important data mining tasks. Interesting rules bring novel knowledge that helps decision makers for advantageous actions. It is true that interestingness is a relative issue that depends on the other prior knowledge. However, this estimation can be biased due to the incomplete or inaccurate knowledge about the domain. Even if possible to estimate interestingness, it is not so trivial to judge the interestingness from a huge set of mined rules. Therefore, an automated system is required that can exploit the knowledge extractacted from the data in measuring interestingness. Since the extracted knowledge comes from the data, so it is possible to find a measure that is unbiased from the user\u2019s\u00a0\u2026", "num_citations": "188\n", "authors": ["1157"]}
{"title": "Understanding neural networks via rule extraction\n", "abstract": " Although backpropagation neural networks generally predict better than decision trees do for pattern classi cation problems, they are often regarded as black boxes, ie, their predictions are not as interpretable as those of decision trees. This paper argues that this is because there has been no proper technique that enables us to do so. With an algorithm that can extract rules1, by drawing parallels with those of decision trees, we show that the predictions of a network can be explained via rules extracted from it, thereby, the network can be understood. Experiments demonstrate that rules extracted from neural networks are comparable with those of decision trees in terms of predictive accuracy, number of rules and average number of conditions for a rule; they preserve high predictive accuracy of original networks.", "num_citations": "188\n", "authors": ["1157"]}
{"title": "Blogosphere: research issues, tools, and applications\n", "abstract": " Weblogs, or Blogs, have facilitated people to express their thoughts, voice their opinions, and share their experiences and ideas. Individuals experience a sense of community, a feeling of belonging, a bonding that members matter to one another and their niche needs will be met through online interactions. Its open standards and low barrier to publication have transformed information consumers to producers. This has created a plethora of open-source intelligence, or \"collective wisdom\" that acts as the storehouse of over-whelming amounts of knowledge about the members, their environment and the symbiosis between them. Nonetheless, vast amounts of this knowledge still remain to be discovered and exploited in its suitable way. In this paper, we introduce various state-of-the-art research issues, review some key elements of research such as tools and methodologies in Blogosphere, and present a case study\u00a0\u2026", "num_citations": "184\n", "authors": ["1157"]}
{"title": "User identity linkage across online social networks: A review\n", "abstract": " The increasing popularity and diversity of social media sites has encouraged more and more people to participate on multiple online social networks to enjoy their services. Each user may create a user identity, which can includes profile, content, or network information, to represent his or her unique public figure in every social network. Thus, a fundamental question arises -- can we link user identities across online social networks? User identity linkage across online social networks is an emerging task in social media and has attracted increasing attention in recent years. Advancements in user identity linkage could potentially impact various domains such as recommendation and link prediction. Due to the unique characteristics of social network data, this problem faces tremendous challenges. To tackle these challenges, recent approaches generally consist of (1) extracting features and (2) constructing predictive\u00a0\u2026", "num_citations": "180\n", "authors": ["1157"]}
{"title": "Customer retention via data mining\n", "abstract": " ``Customer Retention'' is an increasingly pressing issue intoday's ever-competitive commercial arena. This is especially relevantand important for sales and services related industries. Motivated by areal-world problem faced by a large company, we proposed a solution thatintegrates various techniques of data mining, such as featureselection via induction, deviation analysis, and mining multipleconcept-level association rules to form an intuitive and novel approachto gauging customer loyalty and predicting their likelihood ofdefection. Immediate action triggered by these ``early-warnings''resulting from data mining is often the key to eventual customerretention.", "num_citations": "176\n", "authors": ["1157"]}
{"title": "Incremental feature selection\n", "abstract": " Feature selection is a problem of finding relevant features. When the number of features of a dataset is large and its number of patterns is huge, an effective method of feature selection can help in dimensionality reduction. An incremental probabilistic algorithm is designed and implemented as an alternative to the exhaustive and heuristic approaches. Theoretical analysis is given to support the idea of the probabilistic algorithm in finding an optimal or near-optimal subset of features. Experimental results suggest that (1) the probabilistic algorithm is effective in obtaining optimal/suboptimal feature subsets; (2) its incremental version expedites feature selection further when the number of patterns is large and can scale up without sacrificing the quality of selected features.", "num_citations": "175\n", "authors": ["1157"]}
{"title": "Understanding user profiles on social media for fake news detection\n", "abstract": " Consuming news from social media is becoming increasingly popular nowadays. Social media brings benefits to users due to the inherent nature of fast dissemination, cheap cost, and easy access. However, the quality of news is considered lower than traditional news outlets, resulting in large amounts of fake news. Detecting fake news becomes very important and is attracting increasing attention due to the detrimental effects on individuals and the society. The performance of detecting fake news only from content is generally not satisfactory, and it is suggested to incorporate user social engagements as auxiliary information to improve fake news detection. Thus it necessitates an in-depth understanding of the correlation between user profiles on social media and fake news. In this paper, we construct real-world datasets measuring users trust level on fake news and select representative groups of both\u00a0\u2026", "num_citations": "174\n", "authors": ["1157"]}
{"title": "Query selection techniques for efficient crawling of structured web sources\n", "abstract": " The high quality, structured data from Web structured sources is invaluable for many applications. Hidden Web databases are not directly crawlable by Web search engines and are only accessible through Web query forms or via Web service interfaces. Recent research efforts have been focusing on understanding these Web query forms. A critical but still largely unresolved question is: how to efficiently acquire the structured information inside Web databases through iteratively issuing meaningful queries? In this paper we focus on the central issue of enabling efficient Web database crawling through query selection, i.e. how to select good queries to rapidly harvest data records from Web databases. We model each structured Web database as a distinct attribute-value graph. Under this theoretical framework, the database crawling problem is transformed into a graph traversal one that follows \"relational\" links. We\u00a0\u2026", "num_citations": "174\n", "authors": ["1157"]}
{"title": "Resource description framework: metadata and its applications\n", "abstract": " Universality, the property of the Web that makes it the largest data and information source in the world, is also the property behind the lack of a uniform organization scheme that would allow easy access to data and information. A semantic web, wherein different applications and Web sites can exchange information and hence exploit Web data and information to their full potential, requires the information about Web resources to be represented in a detailed and structured manner. Resource Description Framework (RDF), an effort in this direction supported by the World Wide Web Consortium, provides a means for the description of metadata which is a necessity for the next generation of interoperable Web applications. The success of RDF and the semantic web will depend on (1) the development of applications that prove the applicability of the concept, (2) the availability of application interfaces which enable the\u00a0\u2026", "num_citations": "168\n", "authors": ["1157"]}
{"title": "Knowledge-based control of grasping in robot hands using heuristics from human motor skills\n", "abstract": " The development of a grasp planner for multifingered robot hands is described. The planner is knowledge-based, selecting grasp postures by reasoning from symbolic information on target object geometry and the nature of the task. The ability of the planner to utilize task information is based on an attempt to mimic human grasping behavior. Several task attributes and a set of heuristics derived from observation of human motor skills are included in the system. The paper gives several examples of the reasoning of the system in selecting the appropriate grasp mode for spherical and cylindrical objects for different tasks.< >", "num_citations": "165\n", "authors": ["1157"]}
{"title": "An overview of sentiment analysis in social media and its applications in disaster relief\n", "abstract": " Sentiment analysis refers to the class of computational and natural language processing          based          techniques used to identify, extract or characterize subjective information, such as opinions, expressed in a given piece of text. The main purpose of sentiment analysis is to classify a writer\u2019s attitude towards various topics into positive, negative or neutral categories. Sentiment analysis has many applications in different domains including, but not limited to, business intelligence, politics, sociology, etc. Recent years, on the other hand, have witnessed the advent of social networking websites, microblogs, wikis and Web applications and consequently, an unprecedented growth in user-generated data is poised for sentiment mining. Data such as web-postings, Tweets, videos, etc., all express opinions on various topics and events, offer immense opportunities to study and analyze human opinions and\u00a0\u2026", "num_citations": "164\n", "authors": ["1157"]}
{"title": "Challenges of feature selection for big data analytics\n", "abstract": " We're surrounded by huge amounts of large-scale high-dimensional data, but learning tasks require reduced data dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive models, improving learning performance, and preparing clean, understandable data. Some unique characteristics of big data such as data velocity and data variety have presented challenges to the feature selection problem. In this article, the authors envision these challenges for big data analytics. To facilitate and promote feature selection research, they present an open source feature selection repository (scikit-feature) of popular algorithms.", "num_citations": "163\n", "authors": ["1157"]}
{"title": "Recent advances in feature selection and its applications\n", "abstract": " Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as\u00a0\u2026", "num_citations": "160\n", "authors": ["1157"]}
{"title": "When is it biased? Assessing the representativeness of twitter's streaming API\n", "abstract": " Twitter shares a free 1% sample of its tweets through the\" Streaming API\". Recently, research has pointed to evidence of bias in this source. The methodologies proposed in previous work rely on the restrictive and expensive Firehose to find the bias in the Streaming API data. We tackle the problem of finding sample bias without costly and restrictive Firehose data. We propose a solution that focuses on using an open data source to find bias in the Streaming API.", "num_citations": "160\n", "authors": ["1157"]}
{"title": "Mining social media with social theories: a survey\n", "abstract": " The increasing popularity of social media encourages more and more users to participate in various online activities and produces data in an unprecedented rate. Social media data is big, linked, noisy, highly unstructured and in- complete, and differs from data in traditional data mining, which cultivates a new research field - social media mining. Social theories from social sciences are helpful to explain social phenomena. The scale and properties of social media data are very different from these of data social sciences use to develop social theories. As a new type of social data, social media data has a fundamental question - can we apply social theories to social media data? Recent advances in computer science provide necessary computational tools and techniques for us to verify social theories on large-scale social media data. Social theories have been applied to mining social media. In this article, we review\u00a0\u2026", "num_citations": "157\n", "authors": ["1157"]}
{"title": "Mobile location prediction in spatio-temporal context\n", "abstract": " The increasing use of mobile devices and popular mobile services has led to massive availability of mobile data. Location prediction is a specific topic in mobile data mining, with its potential application in traffic planning, location-base advertisement, and user oriented coupon dispersion. Traditional location prediction methods often separately consider spatial or temporal approach. Although there have been some attempts to integrate both spatial and temporal information for location prediction, most of them suffer from the overfitting problem due to the large number of spatio-temporal trajectory patterns. Therefore, smoothing techniques are indispensable for proper training of spatio-temporal models. In this paper, we propose a novel location prediction model to capture the spatio-temporal context of user visits. It considers not only the spatial historical trajectories, but also the temporal periodic patterns. By applying smoothing techniques on both patterns, our model obtains significant improvement compared to the state-of-the-art approaches.", "num_citations": "154\n", "authors": ["1157"]}
{"title": "NeuroLinear: From neural networks to oblique decision rules\n", "abstract": " We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axisparallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.", "num_citations": "151\n", "authors": ["1157"]}
{"title": "A new approach to bot detection: striking the balance between precision and recall\n", "abstract": " The presence of bots has been felt in many aspects of social media. Twitter, one example of social media, has especially felt the impact, with bots accounting for a large portion of its users. These bots have been used for malicious tasks such as spreading false information about political candidates and inflating the perceived popularity of celebrities. Furthermore, these bots can change the results of common analyses performed on social media. It is important that researchers and practitioners have tools in their arsenal to remove them. Approaches exist to remove bots, however they focus on precision to evaluate their model at the cost of recall. This means that while these approaches are almost always correct in the bots they delete, they ultimately delete very few, thus many bots remain. We propose a model which increases the recall in detecting bots, allowing a researcher to delete more bots. We evaluate our\u00a0\u2026", "num_citations": "148\n", "authors": ["1157"]}
{"title": "Feature selection, extraction and construction\n", "abstract": " Feature selection is a process that chooses a subset of features from the original features so that the feature space is optimally reduced according to a certain criterion. Feature extraction/construction is a process through which a set of new features is created. They are used either in isolation or in combination. All attempt to improve performance such as estimated accuracy, visualization and comprehensibility of learned knowledge. Basic approaches to these three are reviewed giving pointers to references for further studies.", "num_citations": "145\n", "authors": ["1157"]}
{"title": "Measuring user credibility in social media\n", "abstract": " People increasingly use social media to get first-hand news and information. During disasters such as Hurricane Sandy and the tsunami in Japan people used social media to report injuries as well as send out their requests. During social movements such as Occupy Wall Street (OWS) and the Arab Spring, people extensively used social media to organize their events and spread the news. As more people rely on social media for political, social, and business events, it is more susceptible to become a place for evildoers to use it to spread misinformation and rumors. Therefore, users have the challenge to discern which piece of information is credible or not. They also need to find ways to assess the credibility of information. This problem becomes more important when the source of the information is not known to the consumer.               In this paper we propose a method to measure user credibility in social\u00a0\u2026", "num_citations": "144\n", "authors": ["1157"]}
{"title": "Feature selection with selective sampling\n", "abstract": " Feature selection, as a preprocessing step to machine learning, has been shown very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving comprehensibility. In this paper, we consider the problem of active feature selection in a lter model setting. We describe a formalism of active feature selection called selective sampling, demonstrate it by applying it to a widely used feature selection algorithm Relief, and show how it realizes active feature selection and reduces the required number of training data for Relief to achieve time savings without performance deterioration. We design objective evaluation measures, conduct extensive experiments using bench-mark data sets, and observe consistent and signi cant improvement.", "num_citations": "143\n", "authors": ["1157"]}
{"title": "Unsupervised fake news detection on social media: A generative approach\n", "abstract": " Social media has become one of the main channels for people to access and consume news, due to the rapidness and low cost of news dissemination on it. However, such properties of social media also make it a hotbed of fake news dissemination, bringing negative impacts on both individuals and society. Therefore, detecting fake news has become a crucial problem attracting tremendous research effort. Most existing methods of fake news detection are supervised, which require an extensive amount of time and labor to build a reliably annotated dataset. In search of an alternative, in this paper, we investigate if we could detect fake news in an unsupervised manner. We treat truths of news and users\u2019 credibility as latent random variables, and exploit users\u2019 engagements on social media to identify their opinions towards the authenticity of news. We leverage a Bayesian network model to capture the conditional dependencies among the truths of news, the users\u2019 opinions, and the users\u2019 credibility. To solve the inference problem, we propose an efficient collapsed Gibbs sampling approach to infer the truths of news and the users\u2019 credibility without any labelled data. Experiment results on two datasets show that the proposed method significantly outperforms the compared unsupervised methods.", "num_citations": "139\n", "authors": ["1157"]}
{"title": "Negative link prediction in social media\n", "abstract": " Signed network analysis has attracted increasing attention in recent years. This is in part because research on signed network analysis suggests that negative links have added value in the analytical process. A major impediment in their effective use is that most social media sites do not enable users to specify them explicitly. In other words, a gap exists between the importance of negative links and their availability in real data sets. Therefore, it is natural to explore whether one can predict negative links automatically from the commonly available social network data. In this paper, we investigate the novel problem of negative link prediction with only positive links and content-centric interactions in social media. We make a number of important observations about negative links, and propose a principled framework NeLP, which can exploit positive links and content-centric interactions to predict negative links. Our\u00a0\u2026", "num_citations": "137\n", "authors": ["1157"]}
{"title": "Online social spammer detection\n", "abstract": " The explosive use of social media also makes it a popular platform for malicious users, known as social spammers, to overwhelm normal users with unwanted content. One effective way for social spammer detection is to build a classifier based on content and social network information. However, social spammers are sophisticated and adaptable to game the system with fast evolving content and network patterns. First, social spammers continually change their spamming content patterns to avoid being detected. Second, reflexive reciprocity makes it easier for social spammers to establish social influence and pretend to be normal users by quickly accumulating a large number of\" human\" friends. It is challenging for existing anti-spamming systems based on batch-mode learning to quickly respond to newly emerging patterns for effective social spammer detection. In this paper, we present a general optimization framework to collectively use content and network information for social spammer detection, and provide the solution for efficient online processing. Experimental results on Twitter datasets confirm the effectiveness and efficiency of the proposed framework.", "num_citations": "137\n", "authors": ["1157"]}
{"title": "Graph mining applications to social network analysis\n", "abstract": " The prosperity of Web 2.0 and social media brings about many diverse social networks of unprecedented scales, which present new challenges for more effec- tive graph-mining techniques. In this chapter, we present some graph patterns that are commonly observed in large-scale social networks. As most networks demonstrate strong community structures, one basic task in social network anal- ysis is community detection which uncovers the group membership of actors in a network. We categorize and survey representative graph mining approaches and evaluation strategies for community detection. We then present and discuss some research issues for future exploration.", "num_citations": "131\n", "authors": ["1157"]}
{"title": "Heterogeneous data fusion for Alzheimer's disease study\n", "abstract": " Effective diagnosis of Alzheimer's disease (AD) is of primary importance in biomedical research. Recent studies have demonstrated that neuroimaging parameters are sensitive and consistent measures of AD. In addition, genetic and demographic information have also been successfully used for detecting the onset and progression of AD. The research so far has mainly focused on studying one type of data source only. It is expected that the integration of heterogeneous data (neuroimages, demographic, and genetic measures) will improve the prediction accuracy and enhance knowledge discovery from the data, such as the detection of biomarkers. In this paper, we propose to integrate heterogeneous data for AD prediction based on a kernel method. We further extend the kernel framework for selecting features (biomarkers) from heterogeneous data sources. The proposed method is applied to a collection of MRI\u00a0\u2026", "num_citations": "128\n", "authors": ["1157"]}
{"title": "A survey of learning causality with data: Problems and methods\n", "abstract": " This work considers the question of how convenient access to copious data impacts our ability to learn causal effects and relations. In what ways is learning causality in the era of big data different from\u2014or the same as\u2014the traditional one? To answer this question, this survey provides a comprehensive and structured review of both traditional and frontier methods in learning causality and relations along with the connections between causality and machine learning. This work points out on a case-by-case basis how big data facilitates, complicates, or motivates each approach.", "num_citations": "124\n", "authors": ["1157"]}
{"title": "Fake news: Fundamental theories, detection strategies and challenges\n", "abstract": " The explosive growth of fake news and its erosion to democracy, justice, and public trust increased the demand for fake news detection. As an interdisciplinary topic, the study of fake news encourages a concerted effort of experts in computer and information science, political science, journalism, social science, psychology, and economics. A comprehensive framework to systematically understand and detect fake news is necessary to attract and unite researchers in related areas to conduct research on fake news. This tutorial aims to clearly present (1) fake news research, its challenges, and research directions;(2) a comparison between fake news and other related concepts (eg, rumors);(3) the fundamental theories developed across various disciplines that facilitate interdisciplinary research;(4) various detection strategies unified under a comprehensive framework for fake news detection; and (5) the state-of-the-art\u00a0\u2026", "num_citations": "123\n", "authors": ["1157"]}
{"title": "Adaptive distance metric learning for clustering\n", "abstract": " A good distance metric is crucial for unsupervised learning from high-dimensional data. To learn a metric without any constraint or class label information, most unsupervised metric learning algorithms appeal to projecting observed data onto a low-dimensional manifold, where geometric relationships such as local or global pairwise distances are preserved. However, the projection may not necessarily improve the separability of the data, which is the desirable outcome of clustering. In this paper, we propose a novel unsupervised adaptive metric learning algorithm, called AML, which performs clustering and distance metric learning simultaneously. AML projects the data onto a low-dimensional manifold, where the separability of the data is maximized. We show that the joint clustering and distance metric learning can be formulated as a trace maximization problem, which can be solved via an iterative procedure in\u00a0\u2026", "num_citations": "123\n", "authors": ["1157"]}
{"title": "Maximizing benefits from crowdsourced data\n", "abstract": " Crowds of people can solve some problems faster than individuals or small groups. A crowd can also rapidly generate data about circumstances affecting the crowd itself. This crowdsourced data can be leveraged to benefit the crowd by providing information or solutions faster than traditional means. However, the crowdsourced data can hardly be used directly to yield usable information. Intelligently analyzing and processing crowdsourced information can help prepare data to maximize the usable information, thus returning the benefit to the crowd. This article highlights challenges and investigates opportunities associated with mining crowdsourced data to yield useful information, as well as details how crowdsource information and technologies can be used for response-coordination when needed, and finally suggests related areas for future research.", "num_citations": "122\n", "authors": ["1157"]}
{"title": "Spectral feature selection for data mining\n", "abstract": " This timely introduction to spectral feature selection illustrates the potential of this powerful dimensionality reduction technique in high-dimensional data processing. It presents the theoretical foundations of spectral feature selection, its connections to other algorithms, and its use in handling both large-scale data sets and small sample problems. Readers learn how to use spectral feature selection to solve challenging problems in real-life applications and discover how general feature selection and extraction are connected to spectral feature selection. Source code for the algorithms is available online.", "num_citations": "120\n", "authors": ["1157"]}
{"title": "Identifying evolving groups in dynamic multimode networks\n", "abstract": " A multimode network consists of heterogeneous types of actors with various interactions occurring between them. Identifying communities in a multimode network can help understand the structural properties of the network, address the data shortage and unbalanced problems, and assist tasks like targeted marketing and finding influential actors within or between groups. In general, a network and its group structure often evolve unevenly. In a dynamic multimode network, both group membership and interactions can evolve, posing a challenging problem of identifying these evolving communities. In this work, we try to address this problem by employing the temporal information to analyze a multimode network. A temporally regularized framework and its convergence property are carefully studied. We show that the algorithm can be interpreted as an iterative latent semantic analysis process, which allows for\u00a0\u2026", "num_citations": "120\n", "authors": ["1157"]}
{"title": "Feature selection with linked data in social media\n", "abstract": " Feature selection is widely used in preparing high-dimensional data for effective data mining. Increasingly popular social media data presents new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attribute-value data such as posts, tweets, comments, and images, and (2) linked data that describes the relationships between social media users as well as who post the posts, etc. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this paper, we illustrate the differences between attribute-value data and social media data, investigate if linked data can be exploited in a new feature selection framework by taking advantage of social science theories, extensively evaluate the effects of user-user and user-post relationships manifested in linked data on feature\u00a0\u2026", "num_citations": "118\n", "authors": ["1157"]}
{"title": "Multi-label informed feature selection.\n", "abstract": " Multi-label learning has been extensively studied in the area of bioinformatics, information retrieval, multimedia annotation, etc. In multi-label learning, each instance is associated with multiple interdependent class labels, the label information can be noisy and incomplete. In addition, multi-labeled data often has high-dimensional noisy, irrelevant and redundant features. As an effective data preprocessing step, feature selection has shown its effectiveness to prepare high-dimensional data for numerous data mining and machine learning tasks. Most of existing multi-label feature selection algorithms either boil down to solve multiple singlelabeled feature selection problems or directly make use of the flawed labels. Therefore, they may not be able to find discriminative features that are shared by multiple labels. In this paper, we propose a novel multi-label informed feature selection framework MIFS, which exploits label correlations to select discriminative features across multiple labels. Specifically, to reduce the negative effects of imperfect label information in finding label correlations, we decompose the multi-label information into a low-dimensional space and then employ the reduced space to steer the feature selection process. Empirical studies on real-world datasets demonstrate the effectiveness and efficiency of the proposed framework.", "num_citations": "117\n", "authors": ["1157"]}
{"title": "Feature subset selection bias for classification learning\n", "abstract": " Feature selection is often applied to high-dimensional data prior to classification learning. Using the same training dataset in both selection and learning can result in so-called feature subset selection bias. This bias putatively can exacerbate data over-fitting and negatively affect classification performance. However, in current practice separate datasets are seldom employed for selection and learning, because dividing the training data into two datasets for feature selection and classifier learning respectively reduces the amount of data that can be used in either task. This work attempts to address this dilemma. We formalize selection bias for classification learning, analyze its statistical properties, and study factors that affect selection bias, as well as how the bias impacts classification learning via various experiments. This research endeavors to provide illustration and explanation why the bias may not cause\u00a0\u2026", "num_citations": "116\n", "authors": ["1157"]}
{"title": "Data analysis on location-based social networks\n", "abstract": " The rapid growth of location-based social networks (LBSNs) has greatly enriched people\u2019s urban experience through social media, and attracted increasing number of users in recent years. Typical location-based social networking sites allow users to \u201ccheck in\u201d at a physical place and share the location with their online friends, and therefore bridge the gap between the real world and online social networks. The availability of large amounts of geographical and social data on LBSNs provides an unprecedented opportunity to study human mobile behavior through data analysis in a spatial\u2013temporal\u2013social context, enabling a variety of location-based services, from mobile marketing to disaster relief. In this chapter, we first introduce the background and framework of location-based mobile social networking. We next discuss the distinct properties, data analysis and research issues of location-based social\u00a0\u2026", "num_citations": "113\n", "authors": ["1157"]}
{"title": "A monotonic measure for optimal feature selection\n", "abstract": " Feature selection is a problem of choosing a subset of relevant features. In general, only exhaustive search can bring about the optimal subset. With a monotonic measure, exhaustive search can be avoided without sacrificing optimality. Unfortunately, most error- or distancebased measures are not monotonic. A new measure is employed in this work that is monotonic and fast to compute. The search for relevant features according to this measure is guaranteed to be complete but not exhaustive. Experiments are conducted for verification.", "num_citations": "111\n", "authors": ["1157"]}
{"title": "User intention modeling in web applications using data mining\n", "abstract": " The problem of inferring a user's intentions in Machine\u2013Human Interaction has been the key research issue for providing personalized experiences and services. In this paper, we propose novel approaches on modeling and inferring user's actions in a computer. Two linguistic features \u2013 keyword and concept features \u2013 are extracted from the semantic context for intention modeling. Concept features are the conceptual generalization of keywords. Association rule mining is used to find the proper concept of corresponding keyword. A modified Na\u00efve Bayes classifier is used in our intention modeling. Experimental results have shown that our proposed approach achieved 84% average accuracy in predicting user's intention, which is close to the precision (92%) of human prediction.", "num_citations": "109\n", "authors": ["1157"]}
{"title": "Studying fake news via network analysis: detection and mitigation\n", "abstract": " Social media is becoming increasingly popular for news consumption due to its easy access, fast dissemination, and low cost. However, social media also enables the wide propagation of \u201cfake news,\u201d i.e., news with intentionally false information. Fake news on social media can have significant negative societal effects. Identifying and mitigating fake news also presents unique challenges. To tackle these challenges, many existing research efforts exploit various features of the data, including network features. In essence, a news dissemination ecosystem involves three dimensions on social media, i.e., a content dimension, a social dimension, and a temporal dimension. In this chapter, we will review network properties for studying fake news, introduce popular network types, and propose how these networks can be used to detect and mitigate fake news on social media.", "num_citations": "108\n", "authors": ["1157"]}
{"title": "Unsupervised feature selection for multi-view data in social media\n", "abstract": " The explosive popularity of social media produces mountains of high-dimensional data and the nature of social media also determines that its data is often unla-belled, noisy and partial, presenting new challenges to feature selection. Social media data can be represented by heterogeneous feature spaces in the form of multiple views. In general, multiple views can be complementary and, when used together, can help handle noisy and partial data for any single-view feature selection. These unique challenges and properties motivate us to develop a novel feature selection framework to handle multi-view social media data. In this paper, we investigate how to exploit relations among views to help each other select relevant features, and propose a novel unsupervised feature selection framework, MVFS, for multiview social media data. We systematically evaluate the proposed framework in multi-view datasets from\u00a0\u2026", "num_citations": "108\n", "authors": ["1157"]}
{"title": "Efficient search of reliable exceptions\n", "abstract": " Finding patterns from data sets is a fundamental task of data mining. If we categorize all patterns into strong, weak, and random, conventional data mining techniques are designed only to find strong patterns, which hold for numerous objects and are usually consistent with the expectations of experts. While such strong patterns are helpful in prediction, the unexpectedness and contradiction exhibited by weak patterns are also very useful although they represent a relatively small number of objects. In this paper, we address the problem of finding weak patterns (i.e., reliable exceptions) from databases. A simple and efficient approach is proposed which uses deviation analysis to identify interesting exceptions and explore reliable ones. Besides, it is flexible in handling both subjective and objective exceptions. We demonstrate the effectiveness of the proposed approach through a set of real-life data sets, and\u00a0\u2026", "num_citations": "108\n", "authors": ["1157"]}
{"title": "Feature Engineering for Machine Learning and Data Analytics\n", "abstract": " Feature engineering plays a vital role in big data analytics. Machine learning and data mining algorithms cannot work without data. Little can be achieved if there are few features to represent the underlying data objects, and the quality of results of those algorithms largely depends on the quality of the available features. Feature Engineering for Machine Learning and Data Analytics provides a comprehensive introduction to feature engineering, including feature generation, feature extraction, feature transformation, feature selection, and feature analysis and evaluation. The book presents key concepts, methods, examples, and applications, as well as chapters on feature engineering for major data types such as texts, images, sequences, time series, graphs, streaming data, software engineering data, Twitter data, and social media data. It also contains generic feature generation approaches, as well as methods for generating tried-and-tested, hand-crafted, domain-specific features. The first chapter defines the concepts of features and feature engineering, offers an overview of the book, and provides pointers to topics not covered in this book. The next six chapters are devoted to feature engineering, including feature generation for specific data types. The subsequent four chapters cover generic approaches for feature engineering, namely feature selection, feature transformation based feature engineering, deep learning based feature engineering, and pattern based feature generation and engineering. The last three chapters discuss feature engineering for social bot detection, software management, and Twitter-based applications respectively. This\u00a0\u2026", "num_citations": "106\n", "authors": ["1157"]}
{"title": "Toward predicting collective behavior via social dimension extraction\n", "abstract": " The SocioDim framework demonstrates promising results toward predicting collective behavior. However, many challenges require further research. For example, networks in social media are continually evolving, with new members joining a network and new connections established between existing members each day. This dynamic nature of networks entails efficient update of the model for collective behavior prediction. It is also intriguing to consider temporal fluctuation into the problem of collective behavior prediction.", "num_citations": "106\n", "authors": ["1157"]}
{"title": "Modeling and data mining in blogosphere\n", "abstract": " This book offers a comprehensive overview of the various concepts and research  issues about blogs or weblogs. It introduces techniques and approaches, tools and  applications, and evaluation methodologies with examples and case studies. Blogs  allow people to express their thoughts, voice their opinions, and share their experiences  and ideas. Blogs also facilitate interactions among individuals creating a network  with unique characteristics. Through the interactions individuals experience a sense  of community. We elaborate on approaches that extract communities and cluster blogs  based on information of the bloggers. Open standards and low barrier to publication  in Blogosphere have transformed information consumers to producers, generating an  overwhelming amount of ever-increasing knowledge about the members, their environment  and symbiosis. We elaborate on approaches that sift through\u00a0\u2026", "num_citations": "106\n", "authors": ["1157"]}
{"title": "Feature transformation and subset selection\n", "abstract": " 1 BackgroundAs computer and database technologies constantly advance, human beings rely more and more on computers to accumulate data, process data, and make use of data. Machine learning, knowledge discovery, and data mining are some of Artificial Intelligence (AI) tools that help mankind accomplish those tasks. Researchers and practitioners realize that in order to use these tools effectively, an important part is pre-processing in which 1", "num_citations": "106\n", "authors": ["1157"]}
{"title": "Modeling temporal effects of human mobile behavior on location-based social networks\n", "abstract": " The rapid growth of location-based social networks (LBSNs) invigorates an increasing number of LBSN users, providing an unprecedented opportunity to study human mobile behavior from spatial, temporal, and social aspects. Among these aspects, temporal effects offer an essential contextual cue for inferring a user's movement. Strong temporal cyclic patterns have been observed in user movement in LBSNs with their correlated spatial and social effects (ie, temporal correlations). It is a propitious time to model these temporal effects (patterns and correlations) on a user's mobile behavior. In this paper, we present the first comprehensive study of temporal effects on LBSNs. We propose a general framework to exploit and model temporal cyclic patterns and their relationships with spatial and social data. The experimental results on two real-world LBSN datasets validate the power of temporal effects in capturing user\u00a0\u2026", "num_citations": "103\n", "authors": ["1157"]}
{"title": "Identifying users with opposing opinions in Twitter debates\n", "abstract": " In recent times, social media sites such as Twitter have been extensively used for debating politics and public policies. These debates span millions of tweets and numerous topics of public importance. Thus, it is imperative that this vast trove of data is tapped in order to gain insights into public opinion especially on hotly contested issues such as abortion, gun reforms etc. Thus, in our work, we aim to gauge users\u2019 stance on such topics in Twitter. We propose ReLP, a semi-supervised framework using a retweet-based label propagation algorithm coupled with a supervised classifier to identify users with differing opinions. In particular, our framework is designed such that it can be easily adopted to different domains with little human supervision while still producing excellent accuracy.", "num_citations": "101\n", "authors": ["1157"]}
{"title": "Research paper recommender systems: A subspace clustering approach\n", "abstract": " Researchers from the same lab often spend a considerable amount of time searching for published articles relevant to their current project. Despite having similar interests, they conduct independent, time consuming searches. While they may share the results afterwards, they are unable to leverage previous search results during the search process. We propose a research paper recommender system that avoids such time consuming searches by augmenting existing search engines with recommendations based on previous searches performed by others in the lab. Most existing recommender systems were developed for commercial domains with millions of users. The research paper domain has relatively few users compared to the large number of online research papers. The two major challenges with this type of data are the large number of dimensions and the sparseness of the data. The novel\u00a0\u2026", "num_citations": "101\n", "authors": ["1157"]}
{"title": "Linked document embedding for classification\n", "abstract": " Word and document embedding algorithms such as Skip-gram and Paragraph Vector have been proven to help various text analysis tasks such as document classification, document clustering and information retrieval. The vast majority of these algorithms are designed to work with independent and identically distributed documents. However, in many real-world applications, documents are inherently linked. For example, web documents such as blogs and online news often have hyperlinks to other web documents, and scientific articles usually cite other articles. Linked documents present new challenges to traditional document embedding algorithms. In addition, most existing document embedding algorithms are unsupervised and their learned representations may not be optimal for classification when labeling information is available. In this paper, we study the problem of linked document embedding for\u00a0\u2026", "num_citations": "97\n", "authors": ["1157"]}
{"title": "Recommendations in signed social networks\n", "abstract": " Recommender systems play a crucial role in mitigating the information overload problem in social media by suggesting relevant information to users. The popularity of pervasively available social activities for social media users has encouraged a large body of literature on exploiting social networks for recommendation. The vast majority of these systems focus on unsigned social networks (or social networks with only positive links), while little work exists for signed social networks (or social networks with positive and negative links). The availability of negative links in signed social networks presents both challenges and opportunities in the recommendation process. We provide a principled and mathematical approach to exploit signed social networks for recommendation, and propose a model, RecSSN, to leverage positive and negative links in signed social networks. Empirical results on real-world datasets\u00a0\u2026", "num_citations": "95\n", "authors": ["1157"]}
{"title": "Time-dependent event hierarchy construction\n", "abstract": " In this paper, an algorithm called Time Driven Documents-partition (TDD) is proposed to construct an event hierarchy in a text corpus based on a given query. Specifically, assume that a query contains only one feature-Election. Election is directly related to the events such as 2006 US Midterm Elections Campaign, 2004 US Presidential Election Campaign and 2004 Taiwan Presidential Election Campaign, where these events may further be divided into several smaller events (eg the 2006 US Midterm Elections Campaign can be broken down into events such as campaign for vote, election results and the resignation of Donald H. Rumsfeld). As such, an event hierarchy is resulted. Our proposed algorithm, TDD, tackles the problem by three major steps:(1) Identify the features that are related to the query according to both the timestamps and the contents of the documents. The features identified are regarded as\u00a0\u2026", "num_citations": "95\n", "authors": ["1157"]}
{"title": "Exploiting vulnerability to secure user privacy on a social networking site\n", "abstract": " As (one's) social network expands, a user's privacy protection goes beyond his privacy settings and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest privacy settings guarantee a secure protection? Given the open nature of social networking sites, is it possible to manage one's privacy protection? With the diversity of one's social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user's perspective is dependent on whether or not the user's friends' privacy settings protect the friend and the individual's network of friends (which includes the user). As a single vulnerable friend in a user's social network might place all friends at risk, we resort to experiments and observe how much security an individual\u00a0\u2026", "num_citations": "94\n", "authors": ["1157"]}
{"title": "'1+ 1> 2': Merging distance and density based clustering\n", "abstract": " Clustering is an important data exploration task. Its use in data mining is growing very fast. Traditional clustering algorithms which no longer cater for the data mining requirements are modified increasingly. Clustering algorithms are numerous which can be divided in several categories. Two prominent categories are distance-based and density-based (e.g. K-means and DBSCAN, respectively). While K-means is fast, easy to implement and converges to local optima almost surely, it is also easily affected by noise. On the other hand, while density-based clustering can find arbitrary shape clusters and handle noise well, it is also slow in comparison due to neighborhood search for each data point, and faces a difficulty in setting the density threshold properly. We propose BRIDGE that efficiently merges the two by exploiting the advantages of one to counter the limitations of the other and vice versa. BRIDGE enables\u00a0\u2026", "num_citations": "94\n", "authors": ["1157"]}
{"title": "Twitter for sparking a movement, reddit for sharing the moment:# metoo through the lens of social media\n", "abstract": " Social media platforms are revolutionizing the way users communicate by increasing the exposure to highly stigmatized issues in the society. Sexual abuse is one such issue that recently took over social media via attaching the hashtag #metoo to the shared posts. Individuals with different backgrounds and ethnicities began sharing their unfortunate personal experiences of being assaulted. Through comparative analysis of the tweets via #meToo on Twitter versus the posts shared on the #meToo subreddit, this paper makes an initial attempt to assess public reactions and emotions. Though nearly equal ratios of negative and positive posts are shared on both platforms, Reddit posts are focused on the sexual assaults within families and workplaces while Twitter posts are on showing empathy and encouraging others to continue the #metoo movement. The data collected in this research and preliminary analysis demonstrate that users use various ways to share their experience, exchange ideas and encourage each other, and social media is suitable for groundswells such as #metoo movement.", "num_citations": "93\n", "authors": ["1157"]}
{"title": "FakeNewsTracker: a tool for fake news collection, detection, and visualization\n", "abstract": " Nowadays social media is widely used as the source of information because of its low cost, easy to access nature. However, consuming news from social media is a double-edged sword because of the wide propagation of fake news, i.e., news with intentionally false information. Fake news is a serious problem because it has negative impacts on individuals as well as society large. In the social media the information is spread fast and hence detection mechanism should be able to predict news fast enough to stop the dissemination of fake news. Therefore, detecting fake news on social media is an extremely important and also a technically challenging problem. In this paper, we present FakeNewsTracker, a system for fake news understanding and detection. As we will show, FakeNewsTracker can automatically collect data for news pieces and social context, which benefits further research of understanding\u00a0\u2026", "num_citations": "91\n", "authors": ["1157"]}
{"title": "Mining misinformation in social media\n", "abstract": " A rapid increase of social media services in recent years has enabled people to share and seek information effectively. The openness, how-", "num_citations": "91\n", "authors": ["1157"]}
{"title": "Understanding twitter data with tweetxplorer\n", "abstract": " In the era of big data it is increasingly difficult for an analyst to extract meaningful knowledge from a sea of information. We present TweetXplorer, a system for analysts with little information about an event to gain knowledge through the use of effective visualization techniques. Using tweets collected during Hurricane Sandy as an example, we will lead the reader through a workflow that exhibits the functionality of the system.", "num_citations": "91\n", "authors": ["1157"]}
{"title": "Social spammer detection with sentiment information\n", "abstract": " Social media is a popular platform for spammers to unfairly overwhelm normal users with unwanted or fake content via social networking. The spammers significantly hinder the use of social media systems for effective information dissemination and sharing. Different from the spammers in traditional platforms such as email and the Web, spammers in social media can easily connect with each other, sometimes without mutual consent. They collude with each other to imitate normal users by quickly accumulating a large number of \"human\" friends. In addition, content information in social media is noisy and unstructured. It is infeasible to directly apply traditional spammer detection methods in social media. Understanding and detecting deception has been extensively studied in traditional sociology and social sciences. Motivated by psychological findings in physical world, we investigate whether sentiment analysis can\u00a0\u2026", "num_citations": "90\n", "authors": ["1157"]}
{"title": "Understanding user migration patterns in social media\n", "abstract": " The incredible growth of the social web over the last decade has ushered in a flurry of new social media sites. On one hand, users have an inordinate number of choices; on the other hand, users are constrained by limited time and resources and have to choose sites in order to remain social and active. Hence, dynamic social media entails user migration, a well studied phenomenon in fields such as sociology and psychology. Users are valuable assets for social media sites as they help contribute to the growth of a site and generate revenue by increased traffic. We are intrigued to know if social media user migration can be studied, and what migration patterns are. In particular, we investigate whether people migrate, and if they do, how they migrate. We formalize site and attention migration to help identify the migration between popular social media sites and determine clear patterns of migration between sites. This work suggests a feasible way to study migration patterns in social media. The discovered patterns can help understand social media sites and gauge their popularity to improve business intelligence and revenue generation through the retention of users.", "num_citations": "90\n", "authors": ["1157"]}
{"title": "Modeling blogger influence in a community\n", "abstract": " Blogging has become a popular and convenient way to communicate, publish information, share preferences, voice opinions, provide suggestions, report news, and form virtual communities in the Blogosphere. The blogosphere obeys a power law distribution with very few blogs being extremely influential and a huge number of blogs being largely unknown. Regardless of a (multi-author) blog being influential or not, there are influential bloggers. However, the sheer number of such blogs makes it extremely challenging to study each one of them. One way to analyze these blogs is to find influential bloggers and consider them as the community representatives. Influential bloggers can impact fellow bloggers in various ways. In this paper, we study the problem of identifying influential bloggers. We define influential bloggers, investigate their characteristics, discuss the challenges with identification, develop a\u00a0\u2026", "num_citations": "89\n", "authors": ["1157"]}
{"title": "On comparison of feature selection algorithms\n", "abstract": " Feature selection (FS) is extensively studied in machine learning. We often need to compare two FS algorithms (A1, A2). Without knowing true relevant features, a conventional way of evaluating A1 and A2 is to evaluate the effect of selected features on classification accuracy in two steps: selecting features from dataset D using Ai to form Di, and obtaining accuracy using each Di, respectively. The superiority of A1 or A2 can be statistically measured by their accuracy difference. To obtain reliable accuracy estimation, k\u2212 fold cross-validation (CV) is commonly used: one fold of data is reserved in turn for test. FS may be performed only once at the beginning and subsequently the results of the two algorithms can be compared using CV; or FS can be performed k-times inside the CV loop. At first glance, the latter is the obvious choice for accuracy estimation. We investigate in this work if the two really differ when comparing two FS algorithms and provide findings of bias analysis.", "num_citations": "89\n", "authors": ["1157"]}
{"title": "Deep anomaly detection on attributed networks\n", "abstract": " Attributed networks are ubiquitous and form a critical component of modern information infrastructure, where additional node attributes complement the raw network structure in knowledge discovery. Recently, detecting anomalous nodes on attributed networks has attracted an increasing amount of research attention, with broad applications in various high-impact domains, such as cybersecurity, finance, and healthcare. Most of the existing attempts, however, tackle the problem with shallow learning mechanisms by ego-network or community analysis, or through subspace selection. Undoubtedly, these models cannot fully address the computational challenges on attributed networks. For example, they often suffer from the network sparsity and data nonlinearity issues, and fail to capture the complex interactions between different information modalities, thus negatively impact the performance of anomaly detection\u00a0\u2026", "num_citations": "87\n", "authors": ["1157"]}
{"title": "Context-aware review helpfulness rating prediction\n", "abstract": " Online reviews play a vital role in the decision-making process for online users. Helpful reviews are usually buried in a large number of unhelpful reviews, and with the consistently increasing number of reviews, it becomes more and more difficult for online users to find helpful reviews. Therefore most online review websites allow online users to rate the helpfulness of a review and a global helpfulness score is computed for the review based on its available ratings. However, in reality, user-specified helpfulness ratings for reviews are very sparse-a few reviews attract large numbers of helpfulness ratings while most reviews obtain few or even no helpfulness ratings. The available helpfulness ratings are too sparse for online users to assess the helpfulness of reviews. Also the helpfulness of a review is not necessarily equally useful for all users and users with different background may treat the helpfulness of a review\u00a0\u2026", "num_citations": "86\n", "authors": ["1157"]}
{"title": "Efficiently handling feature redundancy in high-dimensional data\n", "abstract": " High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.", "num_citations": "86\n", "authors": ["1157"]}
{"title": "Identifying relevant databases for multidatabase mining\n", "abstract": " Various tools and systems for knowledge discovery and data mining are developed and available for applications. However, when we are immersed in heaps of databases, an immediate question facing practitioners is where we should start mining. In this paper, breaking away from the conventional data mining assumption that many databases be joined into one, we argue that the first step for multidatabase mining is to identify databases that are most likely relevant to an application; without doing so, the mining process can be lengthy, aimless and ineffective. A relevance measure is thus proposed to identify relevant databases for mining tasks with an objective to find patterns or regularities about certain attributes. An efficient implementation for identifying relevant databases is described. Experiments are conducted to validate the measure's performance and to show its promising applications.", "num_citations": "86\n", "authors": ["1157"]}
{"title": "Radar: Residual Analysis for Anomaly Detection in Attributed Networks.\n", "abstract": " Attributed networks are pervasive in different domains, ranging from social networks, gene regulatory networks to financial transaction networks. This kind of rich network representation presents challenges for anomaly detection due to the heterogeneity of two data representations. A vast majority of existing algorithms assume certain properties of anomalies are given a prior. Since various types of anomalies in real-world attributed networks coexist, the assumption that priori knowledge regarding anomalies is available does not hold. In this paper, we investigate the problem of anomaly detection in attributed networks generally from a residual analysis perspective, which has been shown to be effective in traditional anomaly detection problems. However, it is a non-trivial task in attributed networks as interactions among instances complicate the residual modeling process. Methodologically, we propose a learning framework to characterize the residuals of attribute information and its coherence with network information for anomaly detection. By learning and analyzing the residuals, we detect anomalies whose behaviors are singularly different from the majority. Experiments on real datasets show the effectiveness and generality of the proposed framework.", "num_citations": "84\n", "authors": ["1157"]}
{"title": "Visualizing social media sentiment in disaster scenarios\n", "abstract": " Recently, social media, such as Twitter, has been successfully used as a proxy to gauge the impacts of disasters in real time. However, most previous analyses of social media during disaster response focus on the magnitude and location of social media discussion. In this work, we explore the impact that disasters have on the underlying sentiment of social media streams. During disasters, people may assume negative sentiments discussing lives lost and property damage, other people may assume encouraging responses to inspire and spread hope. Our goal is to explore the underlying trends in positive and negative sentiment with respect to disasters and geographically related sentiment. In this paper, we propose a novel visual analytics framework for sentiment visualization of geo-located Twitter data. The proposed framework consists of two components, sentiment modeling and geographic visualization. In\u00a0\u2026", "num_citations": "83\n", "authors": ["1157"]}
{"title": "Attributed Signed Network Embedding\n", "abstract": " The major task of network embedding is to learn low-dimensional vector representations of social-network nodes. It facilitates many analytical tasks such as link prediction and node clustering and thus has attracted increasing attention. The majority of existing embedding algorithms are designed for unsigned social networks. However, many social media networks have both positive and negative links, for which unsigned algorithms have little utility. Recent findings in signed network analysis suggest that negative links have distinct properties and added value over positive links. This brings about both challenges and opportunities for signed network embedding. In addition, user attributes, which encode properties and interests of users, provide complementary information to network structures and have the potential to improve signed network embedding. Therefore, in this paper, we study the novel problem of signed\u00a0\u2026", "num_citations": "82\n", "authors": ["1157"]}
{"title": "Node classification in signed social networks\n", "abstract": " Node classification in social networks has been proven to be useful in many real-world applications. The vast majority of existing algorithms focus on unsigned social networks (or social networks with only positive links), while little work exists for signed social networks. It is evident from recent developments in signed social network analysis that negative links have added value over positive links. Therefore, the incorporation of negative links has the potential to benefit various analytical tasks. In this paper, we study the novel problem of node classification in signed social networks. We provide a principled way to mathematically model positive and negative links simultaneously and propose a novel framework NCSSN for node classification in signed social networks. Experimental results on real-world signed social network datasets demonstrate the effectiveness of the proposed framework NCSSN. Further\u00a0\u2026", "num_citations": "82\n", "authors": ["1157"]}
{"title": "Whom should i follow?: identifying relevant users during crises\n", "abstract": " Social media is gaining popularity as a medium of communication before, during, and after crises. In several recent disasters, it has become evident that social media sites like Twitter and Facebook are an important source of information, and in cases they have even assisted in relief efforts. We propose a novel approach to identify a subset of active users during a crisis who can be tracked for fast access to information. Using a Twitter dataset that consists of 12.9 million tweets from 5 countries that are part of the\" Arab Spring\" movement, we show how instant information access can be achieved by user identification along two dimensions: user's location and the user's affinity towards topics of discussion. Through evaluations, we demonstrate that users selected by our approach generate more information and the quality of the information is better than that of users identified using state-of-the-art techniques.", "num_citations": "82\n", "authors": ["1157"]}
{"title": "Misinformation in social media: definition, manipulation, and detection\n", "abstract": " The widespread dissemination of misinformation in social media has recently received a lot of attention in academia. While the problem of misinformation in social media has been intensively studied, there are seemingly different definitions for the same problem, and inconsistent results in different studies. In this survey, we aim to consolidate the observations, and investigate how an optimal method can be selected given specific conditions and contexts. To this end, we first introduce a definition for misinformation in social media and we examine the difference between misinformation detection and classic supervised learning. Second, we describe the diffusion of misinformation and introduce how spreaders propagate misinformation in social networks. Third, we explain characteristics of individual methods of misinformation detection, and provide commentary on their advantages and pitfalls. By reflecting\u00a0\u2026", "num_citations": "81\n", "authors": ["1157"]}
{"title": "Neural network architecture for robot hand control\n", "abstract": " A robot hand control system called GeSAM, which is under development at the University of Southern California, is described. The goal of the GeSAM architecture is to provide a generic robot hand controller that is based on a model of human prehensile function. It focuses on the relationship between geometric object primitives and the ways a hand can perform prehensile behaviors. It is shown how the relationship between object primitives and a useful set of grasp modes can be learned by an adaptive neural network. By adding training points as necessary, system performance can be improved, avoiding the tedious job of computing every relationship individually.< >", "num_citations": "81\n", "authors": ["1157"]}
{"title": "X2R: A fast rule generator\n", "abstract": " Although they can learn from raw data, many concept learning algorithms require that the training data contain only discrete data. However, real world problems contain, more often than not, both numeric and discrete data. So before these algorithms can be applied, data discretization (quantization) is needed. This paper introduces X2R, a simple and fast algorithm that can be applied to both numeric and discrete data, and generate rules from datasets, like season-classification and golf-playing that contain continuous and/or discrete data. The empirical results demonstrate that X2R can effectively generate rules from the raw data and perform better than some of its peers in terms of the quality of rules and time complexities.", "num_citations": "79\n", "authors": ["1157"]}
{"title": "User identification across social media\n", "abstract": " People use various social media sites for different purposes. The information on each site is often partial. When sources of complementary information are integrated, a better profile of a user can be built. This profile can help improve online services such as advertising across sites. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We provide evidence on the existence of a mapping among identities of individuals across social media sites, study the feasibility of finding this mapping, and illustrate and develop means for finding this mapping. Our studies show that effective approaches that exploit information redundancies due to users\u2019 unique behavioral patterns can be utilized to find such a mapping. This study paves the way for analysis and mining across social networking sites, and\u00a0\u2026", "num_citations": "77\n", "authors": ["1157"]}
{"title": "The multi-dimensional quality of task requirements for dextrous robot hand control\n", "abstract": " The authors identify four important task requirements for dextrous robot hand control. These requirements are stability, manipulability, torquability, and radial rotatability. High-level task descriptions, supplied by the user, are refined into detailed task descriptions that can be used to drive a robot hand. A knowledge-based approach for refining a reasonable set of tasks is used to infer values for a set of task attributes, which trigger several heuristics. Those heuristics are applied using a set of metaheuristics to determine good grasp postures and poses for the task. How to use this multidimensional grasping quality in grasp mode selection and performance evaluation is shown in an industrial assembly domain.<>", "num_citations": "77\n", "authors": ["1157"]}
{"title": "Learning representations of ultrahigh-dimensional data for random distance-based outlier detection\n", "abstract": " Learning expressive low-dimensional representations of ultrahigh-dimensional data, eg, data with thousands/millions of features, has been a major way to enable learning methods to address the curse of dimensionality. However, existing unsupervised representation learning methods mainly focus on preserving the data regularity information and learning the representations independently of subsequent outlier detection methods, which can result in suboptimal and unstable performance of detecting irregularities (ie, outliers).", "num_citations": "76\n", "authors": ["1157"]}
{"title": "Is distrust the negation of trust? The value of distrust in social media\n", "abstract": " Trust plays an important role in helping online users collect reliable information, and has attracted increasing attention in recent years. We learn from social sciences that, as the conceptual counterpart of trust, distrust could be as important as trust. However, little work exists in studying distrust in social media. What is the relationship between trust and distrust? Can we directly apply methodologies from social sciences to study distrust in social media? In this paper, we design two computational tasks by leveraging data mining and machine learning techniques to enable the computational understanding of distrust with social media data. The first task is to predict distrust from only trust, and the second task is to predict trust with distrust. We conduct experiments in real-world social media data. The empirical results of the first task provide concrete evidence to answer the question,\" is distrust the negation of trust?\" while\u00a0\u2026", "num_citations": "75\n", "authors": ["1157"]}
{"title": "Quantifying the trustworthiness of social media content\n", "abstract": " The growing popularity of social media in recent years has resulted in the creation of an enormous amount of user-generated content. A significant portion of this information is useful and has proven to be a great source of knowledge. However, since much of this information has been contributed by strangers with little or no apparent reputation to speak of, there is no easy way to detect whether the content is trustworthy. Search engines are the gateways to knowledge but search relevance cannot guarantee that the content in the search results is trustworthy. A casual observer might not be able to differentiate between trustworthy and untrustworthy content. This work is focused on the problem of quantifying the value of such shared content with respect to its trustworthiness. In particular, the focus is on shared health content as the negative impact of acting on untrustworthy content is high in this domain. Health\u00a0\u2026", "num_citations": "75\n", "authors": ["1157"]}
{"title": "Unsupervised streaming feature selection in social media\n", "abstract": " The explosive growth of social media sites brings about massive amounts of high-dimensional data. Feature selection is effective in preparing high-dimensional data for data analytics. The characteristics of social media present novel challenges for feature selection. First, social media data is not fully structured and its features are usually not predefined, but are generated dynamically. For example, in Twitter, slang words (features) are created everyday and quickly become popular within a short period of time. It is hard to directly apply traditional batch-mode feature selection methods to find such features. Second, given the nature of social media, label information is costly to collect. It exacerbates the problem of feature selection without knowing feature relevance. On the other hand, opportunities are also unequivocally present with additional data sources; for example, link information is ubiquitous in social media\u00a0\u2026", "num_citations": "73\n", "authors": ["1157"]}
{"title": "Learning a neural tree\n", "abstract": " A method to learn neural trees is proposed in this paper. Not only the weights of the network connections but also the structure of the whole network including the number of neurons and the interconnections among the neurons are all learned from the training set by our method. Issues about the optimization and pruning of the generated networks are investigated. Initial test results, comparisons with other learning methods and several possible applications are also discussed. 1 Introduction It is well known that neural networks can learn connection weights from examples. However, is it possible to learn a neural network structure from examples? Structures of neural networks are usually designed by human experts. It is quite tricky to choose a good structure to fit for the learning task at hand. This makes the advantages of connectionist learning much less attractive. Furthermore, it turns out that the structure of a neural network is closely related to its final performance in som...", "num_citations": "73\n", "authors": ["1157"]}
{"title": "Nonlinear adaptive distance metric learning for clustering\n", "abstract": " A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a low-dimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space. In this paper, we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm, called NAML, which performs clustering and distance metric learning simultaneously. NAML firstmaps the data to a high-dimensional space through a kernel function; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space\u00a0\u2026", "num_citations": "72\n", "authors": ["1157"]}
{"title": "An unsupervised feature selection framework for social media data\n", "abstract": " The explosive usage of social media produces massive amount of unlabeled and high-dimensional data. Feature selection has been proven to be effective in dealing with high-dimensional data for efficient learning and data mining. Unsupervised feature selection remains a challenging task due to the absence of label information based on which feature relevance is often assessed. The unique characteristics of social media data further complicate the already challenging problem of unsupervised feature selection, e.g., social media data is inherently linked, which makes invalid the independent and identically distributed assumption, bringing about new challenges to unsupervised feature selection algorithms. In this paper, we investigate a novel problem of feature selection for social media data in an unsupervised scenario. In particular, we analyze the differences between social media data and traditional attribute\u00a0\u2026", "num_citations": "71\n", "authors": ["1157"]}
{"title": "Manipulating data and dimension reduction methods: Feature selection\n", "abstract": " Article Outline: Glossary Definition of the Subject Introduction Basics of Feature Selection Supervised Feature Selection Unsupervised Feature Selection Some Recent Research Development Future Directions Bibliography", "num_citations": "71\n", "authors": ["1157"]}
{"title": "Intelligent disaster response via social media analysis a survey\n", "abstract": " The success of a disaster relief and response process is largely dependent on timely and accurate information regarding the status of the disaster, the surrounding environment, and the a ected people. This information is primarily provided by rst responders on-site and can be enhanced by the firsthand reports posted in real-time on social media. Many tools and methods have been developed to automate disaster relief by extracting, analyzing, and visualizing actionable information from social media. However, these methods are not well integrated in the relief and response processes and the relation between the two requires exposition for further advancement. In this survey, we review the new frontier of intelligent disaster relief and response using social media, show stages of disasters which are reflected on social media, establish a connection between proposed methods based on social media and relief efforts\u00a0\u2026", "num_citations": "70\n", "authors": ["1157"]}
{"title": "Gleaning wisdom from the past: Early detection of emerging rumors in social media\n", "abstract": " The explosive use of social media, in information dissemination and communication, has also made it a popular platform for the spread of rumors. Rumors could be easily propagated and received by a large number of users in social media, resulting in catastrophic effects in the physical world in a very short period. It is a challenging task, if not impossible, to apply classical supervised learning methods to the early detection of rumors, since the labeling process is time-consuming and labor-intensive. Motivated by the fact that abundant label information of historical rumors is publicly available, in this paper, we propose to investigate whether knowledge learned from historical data could potentially help identify newly emerging rumors. In particular, since a disputed factual claim arouses certain reactions such as curiosity, skepticism, and astonishment, we identify and utilize patterns from prior labeled data to help\u00a0\u2026", "num_citations": "70\n", "authors": ["1157"]}
{"title": "Robust unsupervised feature selection on networked data\n", "abstract": " Feature selection has shown its effectiveness to prepare high-dimensional data for many data mining and machine learning tasks. Traditional feature selection algorithms are mainly based on the assumption that data instances are independent and identically distributed. However, this assumption is invalid in networked data since instances are not only associated with high dimensional features but also inherently interconnected with each other. In addition, obtaining label information for networked data is time consuming and labor intensive. Without label information to direct feature selection, it is difficult to assess the feature relevance. In contrast to the scarce label information, link information in networks are abundant and could help select relevant features. However, most networked data has a lot of noisy links, resulting in the feature selection algorithms to be less effective. To address the above mentioned\u00a0\u2026", "num_citations": "70\n", "authors": ["1157"]}
{"title": "Leveraging the implicit structure within social media for emergent rumor detection\n", "abstract": " The automatic and early detection of rumors is of paramount importance as the spread of information with questionable veracity can have devastating consequences. This became starkly apparent when, in early 2013, a compromised Associated Press account issued a tweet claiming that there had been an explosion at the White House. This tweet resulted in a significant drop for the Dow Jones Industrial Average. Most existing work in rumor detection leverages conversation statistics and propagation patterns, however, such patterns tend to emerge slowly requiring a conversation to have a significant number of interactions in order to become eligible for classification. In this work, we propose a method for classifying conversations within their formative stages as well as improving accuracy within mature conversations through the discovery of implicit linkages between conversation fragments. In our experiments, we\u00a0\u2026", "num_citations": "69\n", "authors": ["1157"]}
{"title": "Recommendation with social dimensions\n", "abstract": " The pervasive presence of social media greatly enriches online users' social activities, resulting in abundant social relations. Social relations provide an independent source for recommendation, bringing about new opportunities for recommender systems. Exploiting social relations to improve recommendation performance attracts a great amount of attention in recent years. Most existing social recommender systems treat social relations homogeneously and make use of direct connections (or strong dependency connections). However, connections in online social networks are intrinsically heterogeneous and are a composite of various relations. While connected users in online social networks form groups, and users in a group share similar interests, weak dependency connections are established among these users when they are not directly connected. In this paper, we investigate how to exploit the heterogeneity of social relations and weak dependency connections for recommendation. In particular, we employ social dimensions to simultaneously capture heterogeneity of social relations and weak dependency connections, and provide principled ways to model social dimensions, and propose a recommendation framework SoDimRec which incorporates heterogeneity of social relations and weak dependency connections based on social dimensions. Experimental results on real-world data sets demonstrate the effectiveness of the proposed framework. We conduct further experiments to understand the important role of social dimensions in the proposed framework.", "num_citations": "69\n", "authors": ["1157"]}
{"title": "Lessons learned in using social media for disaster relief-ASU crisis response game\n", "abstract": " In disasters such as the earthquake in Haiti and the tsunami in Japan, people used social media to ask for help or report injuries. The popularity, efficiency, and ease of use of social media has led to its pervasive use during the disaster. This creates a pool of timely reports about the disaster, injuries, and help requests. This offers an alternative opportunity for first responders and disaster relief organizations to collect information about the disaster, victims, and their needs. It also presents a challenge for these organizations to aggregate and process the requests from different social media. Given the sheer volume of requests, it is necessary to filter reports and select those of high priority for decision making. Little is known about how the two phases should be smoothly integrated. In this paper we report the use of social media during a simulated crisis and crisis response process, the ASU Crisis Response Game\u00a0\u2026", "num_citations": "69\n", "authors": ["1157"]}
{"title": "Some issues on scalable feature selection\n", "abstract": " Feature selection determines relevant features in the data. It is often applied in pattern classification, data mining, as well as machine learning. A special concern for feature selection nowadays is that the size of a database is normally very large, both vertically and horizontally. In addition, feature sets may grow as the data collection process continues. Effective solutions are needed to accommodate the practical demands. This paper concentrates on three issues: large number of features, large data size, and expanding feature set. For the first issue, we suggest a probabilistic algorithm to select features. For the second issue, we present a scalable probabilistic algorithm that expedites feature selection further and can scale up without sacrificing the quality of selected features. For the third issue, we propose an incremental algorithm that adapts to the newly extended feature set and captures `concept drifts' by\u00a0\u2026", "num_citations": "69\n", "authors": ["1157"]}
{"title": "Bias analysis in text classification for highly skewed data\n", "abstract": " Feature selection is often applied to high-dimensional data as a preprocessing step in text classification. When dealing with highly skewed data, we observe that typical feature selection metrics like information gain or chi-squared are biased toward selecting features for the minor class, and the metric of bi-normal separation can select features for both minor and major classes. In this work, we investigate how these feature selection metrics impact on the performance of frequently used classifiers such as decision trees, naive bayes, and support vector machines via bias analysis for highly skewed data. Three types of biases are metric bias, class bias, and classifier bias. Extensive experiments are designed to understand how these biases can be employed in concert and efficiently to achieve good classification performance. We report our findings and present recommended approaches to text classification based on\u00a0\u2026", "num_citations": "68\n", "authors": ["1157"]}
{"title": "Evaluating subspace clustering algorithms\n", "abstract": " Clustering techniques often define the similarity between instances using distance measures over the various dimensions of the data [12, 14]. Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset. Traditional clustering algorithms consider all of the dimensions of an input dataset in an attempt to learn as much as possible about each instance described. In high dimensional data, however, many of the dimensions are often irrelevant. These irrelevant dimensions confuse clustering algorithms by hiding clusters in noisy data. In very high dimensions it is common for all of the instances in a dataset to be nearly equidistant from each other, completely masking the clusters. Subspace clustering algorithms localize the search for relevant dimensions allowing them to find clusters that exist in multiple, possibly overlapping subspaces. This paper presents a survey of the various subspace clustering algorithms. We then compare the two main approaches to subspace clustering using empirical scalability and accuracy tests.1 Introduction and Background Cluster analysis seeks to discover groups, or clusters, of similar objects. The objects are usually represented as a vector of measurements, or a point in multidimensional space. The similarity between objects is often determined using distance measures over the various dimensions of the data [12, 14]. Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset. Traditional clustering algorithms consider all of the dimensions of an input dataset in an attempt to learn as much\u00a0\u2026", "num_citations": "68\n", "authors": ["1157"]}
{"title": "Fast hierarchical clustering and its validation\n", "abstract": " Clustering is the task of grouping similar objects into clusters. A prominent and useful class of algorithm is hierarchical agglomerative clustering (HAC) which iteratively agglomerates the closest pair until all data points belong to one cluster. It outputs a dendrogram showing all N levels of agglomerations where N is the number of objects in the dataset. However, HAC methods have several drawbacks: (1) high time and memory complexities for clustering, and (2) inefficient and inaccurate cluster validation. In this paper we show that these drawbacks can be alleviated by closely studying the dendrogram. Empirical study shows that most HAC algorithms follow a trend where, except for a number of top levels of the dendrogram, all lower levels agglomerate clusters which are very small in size and close in proximity to other clusters. Methods are proposed that exploit this characteristic to reduce the time and memory\u00a0\u2026", "num_citations": "68\n", "authors": ["1157"]}
{"title": "Exploring implicit hierarchical structures for recommender systems\n", "abstract": " Items in real-world recommender systems exhibit certain hierarchical structures. Similarly, user preferences also present hierarchical structures. Recent studies show that incorporating the explicit hierarchical structures of items or user preferences can improve the performance of recommender systems. However, explicit hierarchical structures are usually unavailable, especially those of user preferences. Thus, there's a gap between the importance of hierarchical structures and their availability. In this paper, we investigate the problem of exploring the implicit hierarchical structures for recommender systems when they are not explicitly available. We propose a novel recommendation framework HSR to bridge the gap, which enables us to capture the implicit hierarchical structures of users and items simultaneously. Experimental results on two real world datasets demonstrate the effectiveness of the proposed framework.", "num_citations": "67\n", "authors": ["1157"]}
{"title": "Feature Extraction for Image Mining.\n", "abstract": " Due to the digitization of data and advances in technology, it has become extremely easy to obtain and store large quantities of data, particularly Multimedia data. Fields ranging from Commercial to Military need to analyze these data in an efficient and fast manner. Presently, tools for mining images are few and require human intervention. Feature selection and extraction is the pre-processing step of Image Mining. Obviously this is a critical step in the entire scenario of Image Mining. Our approach to mine from Images\u2013to extract patterns and derive knowledge from large collections of images, deals mainly with identification and extraction of unique features for a particular domain. Though there are various features available, the aim is to identify the best features and thereby extract relevant information from the images. We have tried various methods for extraction; the features extracted and the techniques used are evaluated for their contribution to solving the problem. Experimental results show that the features used are sufficient to identify the patterns from the Images. The extracted features were evaluated for goodness and tested on test images. An interactive system was developed which allows the user to define new features and to resolve uncertain regions.", "num_citations": "66\n", "authors": ["1157"]}
{"title": "The role of user profiles for fake news detection\n", "abstract": " Consuming news from social media is becoming increasingly popular. Social media appeals to users due to its fast dissemination of information, low cost, and easy access. However, social media also enables the widespread of fake news. Due to the detrimental societal effects of fake news, detecting fake news has attracted increasing attention. However, the detection performance only using news contents is generally not satisfactory as fake news is written to mimic true news. Thus, there is a need for an in-depth understanding on the relationship between user profiles on social media and fake news. In this paper, we study the problem of understanding and exploiting user profiles on social media for fake news detection. In an attempt to understand connections between user profiles and fake news, first, we measure users' sharing behaviors and group representative users who are more likely to share fake and real\u00a0\u2026", "num_citations": "65\n", "authors": ["1157"]}
{"title": "Next generation of data mining\n", "abstract": " Drawn from the US National Science Foundation's Symposium on Next Generation of Data Mining and Cyber-Enabled Discovery for Innovation (NGDM 07), Next Generation of Data Mining explores emerging technologies and applications in data mining as well as potential challenges faced by the field. Gathering perspectives from top experts across different di", "num_citations": "65\n", "authors": ["1157"]}
{"title": "Addressing the cold-start problem in location recommendation using geo-social correlations\n", "abstract": " Location-based social networks (LBSNs) have attracted an increasing number of users in recent years, resulting in large amounts of geographical and social data. Such LBSN data provide an unprecedented opportunity to study the human movement from their socio-spatial behavior, in order to improve location-based applications like location recommendation. As users can check-in at new places, traditional work on location prediction that relies on mining a user\u2019s historical moving trajectories fails as it is not designed for the cold-start problem of recommending new check-ins. While previous work on LBSNs attempting to utilize a user\u2019s social connections for location recommendation observed limited help from social network information. In this work, we propose to address the cold-start location recommendation problem by capturing the correlations between social networks and geographical distance on\u00a0\u2026", "num_citations": "64\n", "authors": ["1157"]}
{"title": "Less is more\n", "abstract": " As our world expands at an unprecedented speed from the physical into the virtual, we can conveniently collect more and more data in any ways one can imagine for various reasons. Is it \u201cThe more, the merrier (better)\u201d? The answer is \u201cYes\u201d and \u201cNo.\u201d It is \u201cYes\u201d because we can at least get what we might need. It is also \u201cNo\u201d because, when it comes to a point of too much, the existence of inordinate data is tantamount to non-existence if there is no means of effective data access. More can mean less. Without the processing of data, its mere existence would not become a useful asset that can impact our business, and many other matters. Since continued data accumulation is inevitable, one way out is to devise data selection techniques to keep pace with the rate of data collection. Furthermore, given the sheer volume of data, data generated by computers or equivalent mechanisms must be processed automatically, in\u00a0\u2026", "num_citations": "64\n", "authors": ["1157"]}
{"title": "Real-world behavior analysis through a social media lens\n", "abstract": " The advent of participatory web has enabled information consumers to become information producers via social media. This phenomenon has attracted researchers of different disciplines including social scientists, political parties, and market researchers to study social media as a source of data to explain human behavior in the physical world. Could the traditional approaches of studying social behaviors such as surveys be complemented by computational studies that use massive user-generated data in social media? In this paper, using a large amount of data collected from Twitter, the blogosphere, social networks, and news sources, we perform preliminary research to investigate if human behavior in the real world can be understood by analyzing social media data. The goals of this research is twofold: (1) determining the relative effectiveness of a social media lens in analyzing and predicting real-world\u00a0\u2026", "num_citations": "60\n", "authors": ["1157"]}
{"title": "Situation-aware personalized information retrieval for mobile internet\n", "abstract": " Recent rapid advances in Internet-based information systems and handheld devices make it possible for users to retrieve information anytime and anywhere. Existing information retrieval (IR) techniques usually require the users to spend much effort to continuously refine their queries to obtain the results they want. Since each refinement is an interaction between a user and information systems, the larger the number of interactions, the more communication overhead and energy consumption are introduced. Due to the severe resource constraints of handheld devices, it is necessary to have an IR technique that can efficiently retrieve information for the user with only one or two interactions. In this paper, an efficient IR technique for mobile Internet by combining situation-based adaptation and profile-based personalization is presented.", "num_citations": "60\n", "authors": ["1157"]}
{"title": "Topic taxonomy adaptation for group profiling\n", "abstract": " A topic taxonomy is an effective representation that describes salient features of virtual groups or online communities. A topic taxonomy consists of topic nodes. Each internal node is defined by its vertical path (i.e., ancestor and child nodes) and its horizonal list of attributes (or terms). In a text-dominant environment, a topic taxonomy can be used to flexibly describe a group's interests with varying granularity. However, the stagnant nature of a taxonomy may fail to timely capture the dynamic change of a group's interest. This article addresses the problem of how to adapt a topic taxonomy to the accumulated data that reflects the change of a group's interest to achieve dynamic group profiling. We first discuss the issues related to topic taxonomy. We next formulate taxonomy adaptation as an optimization problem to find the taxonomy that best fits the data. We then present a viable algorithm that can efficiently accomplish\u00a0\u2026", "num_citations": "59\n", "authors": ["1157"]}
{"title": "Provenance data in social media\n", "abstract": " Download Free Sample   Social media shatters the barrier to communicate anytime anywhere for people of all walks of life. The publicly available, virtually free information in social media poses a new challenge to consumers who have to discern whether a piece of information published in social media is reliable. For example, it can be difficult to understand the motivations behind a statement passed from one user to another, without knowing the person who originated the message. Additionally, false information can be propagated through social media, resulting in embarrassment or irreversible damages. Provenance data associated with a social media statement can help dispel rumors, clarify opinions, and confirm facts. However, provenance data about social media statements is not readily available to users today. Currently, providing this data to users requires changing the social media infrastructure or\u00a0\u2026", "num_citations": "58\n", "authors": ["1157"]}
{"title": "SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification\n", "abstract": " Sentiment information about social media posts is increasingly considered an important resource for customer segmentation, market understanding, and tackling other socio-economic issues.  However, sentiment in social media is difficult to measure since user-generated content is usually short and informal. Although many traditional sentiment analysis methods have been proposed, identifying slang sentiment words remains a challenging task for practitioners. Though some slang words are available in existing sentiment lexicons, with new slang being generated with emerging memes, a dedicated lexicon will be useful for researchers and practitioners. To this end, we propose to build a slang sentiment dictionary to aid sentiment analysis.  It is laborious and time-consuming to collect a comprehensive list of slang words and label the sentiment polarity. We present an approach to leverage web resources\u00a0\u2026", "num_citations": "56\n", "authors": ["1157"]}
{"title": "Dimensionality reduction via discretization\n", "abstract": " The existence of numeric data and large numbers of records in a database present a challenging task in terms of explicit concepts extraction from the raw data. The paper introduces a method that reduces data vertically and horizontally, keeps the discriminating power of the original data, and paves the way for extracting concepts. The method is based on discretization (vertical reduction) and feature selection (horizontal reduction). The experimental results show that (a) the data can be effectively reduced by the proposed method; (b) the predictive accuracy of a classifier (C4.5) can be improved after data and dimensionality reduction; and (c) the classification rules learned are simpler.", "num_citations": "56\n", "authors": ["1157"]}
{"title": "Turning clicks into purchases: Revenue optimization for product search in e-commerce\n", "abstract": " In recent years, product search engines have emerged as a key factor for online businesses. According to a recent survey, over 55% of online customers begin their online shopping journey by searching on an E-Commerce (EC) website like Amazon as opposed to a generic web search engine like Google. Information retrieval research to date has been focused on optimizing search ranking algorithms for web documents while little attention has been paid to product search. There are several intrinsic differences between web search and product search that make the direct application of traditional search ranking algorithms to EC search platforms difficult. First, the success of web and product search is measured differently; one seeks to optimize for relevance while the other must optimize for both relevance and revenue. Second, when using real-world EC transaction data, there is no access to manually annotated\u00a0\u2026", "num_citations": "54\n", "authors": ["1157"]}
{"title": "WisColl: Collective wisdom based blog clustering\n", "abstract": " The Blogosphere is expanding in an unprecedented speed. A better understanding of the blogosphere can greatly facilitate the development of the Social Web to serve the needs of users, service providers, and advertisers. One important task in this process is clustering blog sites. Although a good number of traditional clustering methods exists, they are not designed to take into account the blogosphere unique characteristics. Clustering blog sites presents new challenges. A prominent feature of the Social Web is that many enthusiastic bloggers voluntarily write, tag, and catalog their posts in order to reach the widest possible audience who will share their thoughts and appreciate their ideas. In the process a new kind of collective wisdom is generated. We propose WisColl by tapping into this collective wisdom when clustering blog sites. In this paper, we study how clustering with collective wisdom can be achieved\u00a0\u2026", "num_citations": "54\n", "authors": ["1157"]}
{"title": "Toward multidatabase mining: Identifying relevant databases\n", "abstract": " Various tools and systems for knowledge discovery and data mining have been developed and are available for applications. However, when there are many databases, an immediate question is where one should start mining. It is not true that data mining is better the more databases there are. It is only true when the databases involved are relevant to the task at hand. By breaking away from the conventional data mining assumption that many databases should be joined into one, we argue that the first step for multidatabase mining is to identify databases that are most relevant to an application; without doing so, the mining process can be lengthy, aimless, and ineffective. A measure of relevance is thus proposed for mining tasks with the objective of finding patterns or regularities of certain attributes. An efficient algorithm for identifying relevant databases is described. Experiments are conducted to verify the\u00a0\u2026", "num_citations": "54\n", "authors": ["1157"]}
{"title": "Discretization: An enabling technique\n", "abstract": " Discrete values have important roles in data mining and knowledge discovery. They are about intervals of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from discretization: rules with discrete values are normally shorter and more understandable and discretization can lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require discrete features. All these prompt researchers and practitioners to discretize continuous features before or during a machine learning or data mining task. There are numerous discretization methods available in the literature. It is time for us to examine these seemingly different methods for discretization and find out how different they really are, what are the key components of a discretization process, how we can improve the current level of research for new development as well as the use of existing methods. This paper aims at a systematic study of discretization methods with their history of development, effect on classification, and trade-off between speed and accuracy. Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical framework to categorize the existing methods and pave the way for further development, concise discussions of representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to choose a discretization method under various circumstances. We also identify some issues yet to\u00a0\u2026", "num_citations": "53\n", "authors": ["1157"]}
{"title": "Handling Large Unsupervised Data via Dimensionality Reduction.\n", "abstract": " Dimensionality reduction is often employed for e cient handling of large data sets, and feature selection is a much used technique. Many supervised feature selection methods exist. Little work has been done for unsupervised feature selection where class information is not available. We present an e ective method of selecting relevant features for unsupervised data. Our method is a 2-step procedure where we rst rank the features using an entropy measure and then choose a subset of features by an invariant criterion function for clustering. We empirically evaluate on synthetic and benchmark data sets. Issue of scaling for large data sets is discussed.", "num_citations": "53\n", "authors": ["1157"]}
{"title": "A survey on privacy in social media: identification, mitigation, and applications\n", "abstract": " The increasing popularity of social media has attracted a huge number of people to participate in numerous activities on a daily basis. This results in tremendous amounts of rich user-generated data. These data provide opportunities for researchers and service providers to study and better understand users\u2019 behaviors and further improve the quality of the personalized services. Publishing user-generated data risks exposing individuals\u2019 privacy. Users privacy in social media is an emerging research area and has attracted increasing attention recently. These works study privacy issues in social media from the two different points of views: identification of vulnerabilities and mitigation of privacy risks. Recent research has shown the vulnerability of user-generated data against the two general types of attacks, identity disclosure and attribute disclosure. These privacy issues mandate social media data publishers to\u00a0\u2026", "num_citations": "52\n", "authors": ["1157"]}
{"title": "A dilemma in assessing stability of feature selection algorithms\n", "abstract": " In realm, feature selection is an effective means for handling high-dimensional data that becomes increasingly abundant. The stability of a feature selection algorithm is becoming crucial for determining the fitness of the algorithm. Below, we review existing methods of stability assessment and analyse how they assess the stability of a feature selection algorithm. A common approach is to evaluate the similarity between the selected subsets of features produced by that algorithm over different training samples or over distributed datasets. We point out challenges facing the existing evaluation methods and suggest how to improve stability assessment of feature selection algorithms.", "num_citations": "52\n", "authors": ["1157"]}
{"title": "Interactive anomaly detection on attributed networks\n", "abstract": " Performing anomaly detection on attributed networks concerns with finding nodes whose patterns or behaviors deviate significantly from the majority of reference nodes. Its success can be easily found in many real-world applications such as network intrusion detection, opinion spam detection and system fault diagnosis, to name a few. Despite their empirical success, a vast majority of existing efforts are overwhelmingly performed in an unsupervised scenario due to the expensive labeling costs of ground truth anomalies. In fact, in many scenarios, a small amount of prior human knowledge of the data is often effortless to obtain, and getting it involved in the learning process has shown to be effective in advancing many important learning tasks. Additionally, since new types of anomalies may constantly arise over time especially in an adversarial environment, the interests of human expert could also change\u00a0\u2026", "num_citations": "51\n", "authors": ["1157"]}
{"title": "Multi-layered network embedding\n", "abstract": " Network embedding has gained more attentions in recent years. It has been shown that the learned low-dimensional node vector representations could advance a myriad of graph mining tasks such as node classification, community detection, and link prediction. A vast majority of the existing efforts are overwhelmingly devoted to single-layered networks or homogeneous networks with a single type of nodes and node interactions. However, in many real-world applications, a variety of networks could be abstracted and presented in a multilayered fashion. Typical multi-layered networks include critical infrastructure systems, collaboration platforms, social recommender systems, to name a few. Despite the widespread use of multi-layered networks, it remains a daunting task to learn vector representations of different types of nodes due to the bewildering combination of both within-layer connections and cross-layer\u00a0\u2026", "num_citations": "51\n", "authors": ["1157"]}
{"title": "Sentiment informed cyberbullying detection in social media\n", "abstract": " Cyberbullying is a phenomenon which negatively affects the individuals, the victims suffer from various mental issues, ranging from depression, loneliness, anxiety to low self-esteem. In parallel with the pervasive use of social media, cyberbullying is becoming more and more prevalent. Traditional mechanisms to fight against cyberbullying include the use of standards and guidelines, human moderators, and blacklists based on the profane words. However, these mechanisms fall short in social media and cannot scale well. Therefore, it is necessary to develop a principled learning framework to automatically detect cyberbullying behaviors. However, it is a challenging task due to short, noisy and unstructured content information and intentional obfuscation of the abusive words or phrases by social media users. Motivated by sociological and psychological findings on bullying behaviors and the correlation\u00a0\u2026", "num_citations": "51\n", "authors": ["1157"]}
{"title": "Hierarchical attention networks for cyberbullying detection on the instagram social network\n", "abstract": " Cyberbullying has become one of the most pressing online risks for young people and has raised serious concerns in society. The emerging literature identifies cyberbullying as repetitive acts that occur over time rather than one-off incidents. Yet, there has been relatively little work to model the hierarchical structure of social media sessions and the temporal dynamics of cyberbullying in online social network sessions. We propose a hierarchical attention network for cyberbullying detection that takes these aspects of cyberbullying into account. The primary distinctive characteristics of our approach include: (i) a hierarchical structure that mirrors the structure of a social media session; (ii) levels of attention mechanisms applied at the word and comment level, thereby enabling the model to pay different amounts of attention to words and comments, depending on the context; and (iii) a cyberbullying detection task that\u00a0\u2026", "num_citations": "50\n", "authors": ["1157"]}
{"title": "Adaptive spammer detection with sparse group modeling\n", "abstract": " Social spammers disseminate unsolicited information on social media sites that negatively impacts social networking systems. To detect social spammers, traditional methods leverage social network structures to identify the behavioral patterns hidden in their social interactions. They focus on accounts that are affiliated with groups comprising known spammers. However, since different parties are emerging to generate various spammers, they may form different kinds of groups, and some spammers may even detach from the flock. Therefore, it is challenging for existing methods to find the optimal group structure that captures different spammers simultaneously. Employing different approaches for specific spammers is time-consuming, and it also lacks the adaptivity of dealing with emerging spammers. In this work, we aim to propose a group modeling framework that adaptively characterizes social interactions of spammers. In particular, we introduce to integrate content information into the group modeling process. The proposed framework exploits additional content information in selecting groups and individuals that are likely to be involved in spamming activities. In order to alleviate the intensive computational cost, we transform the problem as a sparse learning task that can be solved efficiently. Experimental results on real-world datasets show that the proposed method outperforms the state-of-the-art approaches.", "num_citations": "50\n", "authors": ["1157"]}
{"title": "Searching multiple databases for interesting complexes\n", "abstract": " When many databases are available for knowledge discovery, one pressing problem is which databases have interesting knowledge. To solve this problem, users can pose a query when investigating databases and search for interesting information. In order to nd interesting information and avoid unnecessary search in irrelevant databases, interestingness measures and e cient search methods are needed. In this paper, we propose a method based on a statistical measure by which some relevant databases can be detected. Knowledge, in the form of complexes, can be extracted which contains potentially interesting information. Users can identify importance and relevance of each database according to extracted knowledge. A heuristic function is applied to make the search more e cient. The extension of this method is also discussed.", "num_citations": "49\n", "authors": ["1157"]}
{"title": "Xbully: Cyberbullying detection within a multi-modal context\n", "abstract": " Over the last decade, research has revealed the high prevalence of cyberbullying among youth and raised serious concerns in society. Information on the social media platforms where cyberbullying is most prevalent (eg, Instagram, Facebook, Twitter) is inherently multi-modal, yet most existing work on cyberbullying identification has focused solely on building generic classification models that rely exclusively on text analysis of online social media sessions (eg, posts). Despite their empirical success, these efforts ignore the multi-modal information manifested in social media data (eg, image, video, user profile, time, and location), and thus fail to offer a comprehensive understanding of cyberbullying. Conventionally, when information from different modalities is presented together, it often reveals complementary insights about the application domain and facilitates better learning performance. In this paper, we study\u00a0\u2026", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Adaptive implicit friends identification over heterogeneous network for social recommendation\n", "abstract": " The explicitly observed social relations from online social platforms have been widely incorporated into recommender systems to mitigate the data sparsity issue. However, the direct usage of explicit social relations may lead to an inferior performance due to the unreliability (eg, noises) of observed links. To this end, the discovery of reliable relations among users plays a central role in advancing social recommendation. In this paper, we propose a novel approach to adaptively identify implicit friends toward discovering more credible user relations. Particularly, implicit friends are those who share similar tastes but could be distant from each other on the network topology of social relations. Methodologically, to find the implicit friends for each user, we first model the whole system as a heterogeneous information network, and then capture the similarity of users through the meta-path based embedding representation\u00a0\u2026", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Social cyber-security\n", "abstract": " Social Cyber-Security is an emerging scientific discipline. Its methodological and scientific foundation, key challenges, and scientific direction are described. The multi-disciplinary nature of this field and its emphasis on dynamic information strategies is considered.", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Exploiting emotional information for trust/distrust prediction\n", "abstract": " Trust and distrust networks are usually extremely sparse and the vast majority of the existing algorithms for trust/distrust prediction suffer from the data sparsity problem. In this paper, following the research from psychology and sociology, we envision that users' emotions such as happiness and anger are strong indicators of trust/distrust relations. Meanwhile the popularity of social media encourages the increasing number of users to freely express their emotions; hence emotional information is pervasively available and usually denser than the trust and distrust relations. Therefore incorporating emotional information could have the potentials to alleviate the data sparsity in the problem of trust/distrust prediction. In this study, we investigate how to exploit emotional information for trust/distrust prediction. In particular, we provide a principled way to capture emotional information mathematically and propose a novel trust\u00a0\u2026", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Signed link analysis in social media networks\n", "abstract": " Numerous real-world relations can be represented by signed networks with positive links (eg, trust) and negative links (eg, distrust). Link analysis plays a crucial role in understanding the link formation and can advance various tasks in social network analysis such as link prediction. The majority of existing works on link analysis have focused on unsigned social networks. The existence of negative links determines that properties and principles of signed networks are substantially distinct from those of unsigned networks, thus we need dedicated efforts on link analysis in signed social networks. In this paper, following social theories in link analysis in unsigned networks, we adopt three social science theories, namely Emotional Information, Diffusion of Innovations and Individual Personality, to guide the task of link analysis in signed networks.", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Finding eyewitness tweets during crises\n", "abstract": " Disaster response agencies have started to incorporate social media as a source of fast-breaking information to understand the needs of people affected by the many crises that occur around the world. These agencies look for tweets from within the region affected by the crisis to get the latest updates of the status of the affected region. However only 1% of all tweets are geotagged with explicit location information. First responders lose valuable information because they cannot assess the origin of many of the tweets they collect. In this work we seek to identify non-geotagged tweets that originate from within the crisis region. Towards this, we address three questions: (1) is there a difference between the language of tweets originating within a crisis region and tweets originating outside the region, (2) what are the linguistic patterns that can be used to differentiate within-region and outside-region tweets, and (3) for non-geotagged tweets, can we automatically identify those originating within the crisis region in real-time?", "num_citations": "47\n", "authors": ["1157"]}
{"title": "Evaluation without ground truth in social media research\n", "abstract": " Even without it, some ingenious methods can be developed to help verify users' social media behavioral patterns.", "num_citations": "46\n", "authors": ["1157"]}
{"title": "Sentiment propagation in social networks: a case study in livejournal\n", "abstract": " Social networking websites have facilitated a new style of communication through blogs, instant messaging, and various other techniques. Through collaboration, millions of users participate in millions of discussions every day. However, it is still difficult to determine the extent to which such discussions affect the emotions of the participants. We surmise that emotionally-oriented discussions may affect a given user\u2019s general emotional bent and be reflected in other discussions he or she may initiate or participate in. It is in this way that emotion (or sentiment) may propagate through a network. In this paper, we analyze sentiment propagation in social networks, review the importance and challenges of such a study, and provide methodologies for measuring this kind of propagation. A case study has been conducted on a large dataset gathered from the LiveJournal social network. Experimental results are\u00a0\u2026", "num_citations": "46\n", "authors": ["1157"]}
{"title": "Multi-source feature selection via geometry-dependent covariance analysis\n", "abstract": " Feature selection is an effective approach to reducing dimensionality by selecting relevant original features. In this work, we studied a novel problem of multi-source feature selection for unlabeled data: given multiple heterogeneous data sources (or data sets), select features from one source of interest by integrating information from various data sources. In essence, we investigate how we can employ the information contained in multiple data sources to effectively derive intrinsic relationships that can help select more meaningful (or domain relevant) features. We studied how to adjust the covariance matrix of a data set using the geometric structure obtained from multiple data sources, and how to select features of the target source using geometry-dependent covariance. We designed and conducted experiments to systematically compare the proposed approach with representative methods in our attempt to solve the novel problem of multi-source feature selection. The empirical study demonstrated the efficacy and potential of multi-source feature selection.", "num_citations": "46\n", "authors": ["1157"]}
{"title": "Trust evaluation in health information on the World Wide Web\n", "abstract": " The impact of health information on the web is mounting and with the Health 2.0 revolution around the corner, online health promotion and management is becoming a reality. User-generated content is at the core of this revolution and brings to the fore the essential question of trust evaluation, a pertinent problem for health applications in particular. Evolving Web 2.0 health applications provide abundant opportunities for research. We identify these applications, discuss the challenges for trust assessment, characterize conceivable variables, list potential techniques for analysis, and provide a vision for future research.", "num_citations": "46\n", "authors": ["1157"]}
{"title": "Improving backpropagation learning with feature selection\n", "abstract": " There exist redundant, irrelevant and noisy data. Using proper data to train a network can speed up training, simplify the learned structure, and improve its performance. A two-phase training algorithm is proposed. In the first phase, the number of input units of the network is determined by using an information base method. Only those attributes that meet certain criteria for inclusion will be considered as the input to the network. In the second phase, the number of hidden units of the network is selected automatically based on the performance of the network on the training data. One hidden unit is added at a time only if it is necessary. The experimental results show that this new algorithm can achieve a faster learning time, a simpler network and an improved performance.", "num_citations": "46\n", "authors": ["1157"]}
{"title": "Identifying framing bias in online news\n", "abstract": " It has been observed that different media outlets exert bias in the way they report the news, which seamlessly influences the way that readers\u2019 knowledge is built through filtering what we read. Therefore, understanding bias in news media is fundamental for obtaining a holistic view of a news story. Traditional work has focused on biases in terms of \u201cagenda setting,\u201d where more attention is allocated to stories that fit their biased narrative. The corresponding method is straightforward, since the bias can be detected through counting the occurrences of different stories/themes within the documents. However, these methods are not applicable to biases which are implicit in wording, namely, \u201cframing\u201d bias. According to framing theory, biased communicators will select and emphasize certain facts and interpretations over others when telling their story. By focusing on facts and interpretations that conform to their bias, they\u00a0\u2026", "num_citations": "45\n", "authors": ["1157"]}
{"title": "Your age is no secret: Inferring microbloggers' ages via content and interaction analysis\n", "abstract": " Microblogging systems such as Twitter have seen explosive use in public and private sectors. The age information of microbloggers can be very useful for many applications such as viral marketing and social studies/surveys. Current microblogging systems, however, have very sparse age information. In this paper, we present MAIF, a novel framework that explores public content and interaction information in microblogging systems to explore the hidden ages of microbloggers. We thoroughly evaluate the accuracy of MAIF with a real-world dataset with 54,879 Twitter users. Our results show that MAIF can achieve up to 81.38% inference accuracy and outperforms the state of the art by 9.15%. We also discuss some countermeasures to alleviate the possible privacy concerns caused by MAIF.", "num_citations": "45\n", "authors": ["1157"]}
{"title": "Personalized privacy-preserving social recommendation\n", "abstract": " Privacy leakage is an important issue for social recommendation. Existing privacy preserving social recommendation approaches usually allow the recommender to fully control users' information. This may be problematic since the recommender itself may be untrusted, leading to serious privacy leakage. Besides, building social relationships requires sharing interests as well as other private information, which may lead to more privacy leakage. Although sometimes users are allowed to hide their sensitive private data using privacy settings, the data being shared can still be abused by the adversaries to infer sensitive private information. Supporting social recommendation with least privacy leakage to untrusted recommender and other users (ie, friends) is an important yet challenging problem. In this paper, we aim to address the problem of achieving privacy-preserving social recommendation under personalized privacy settings. We propose PrivSR, a novel framework for privacy-preserving social recommendation, in which users can model ratings and social relationships privately. Meanwhile, by allocating different noise magnitudes to personalized sensitive and non-sensitive ratings, we can protect users' privacy against the untrusted recommender and friends. Theoretical analysis and experimental evaluation on real-world datasets demonstrate that our framework can protect users' privacy while being able to retain effectiveness of the underlying recommender system.", "num_citations": "44\n", "authors": ["1157"]}
{"title": "Robot hand-eye coordination: Shape description and grasping\n", "abstract": " The successful execution of grasps by a robot hand requires a translation of visual information into control signals to the hand which produce the desired spatial orientation and preshape for an arbitrary object. An approach to this problem, based on separation of the task into two modules, is presented. A vision module is used to transform an image into a volumetric shape description using generalized cones. The data structure containing this geometric information becomes an input to the grasping module, which obtains a list of feasible grasp modes and a set of control signals for the robot hand. Various features of both modules are discussed.< >", "num_citations": "44\n", "authors": ["1157"]}
{"title": "Causal Interpretability for Machine Learning - Problems, Methods and Evaluation\n", "abstract": " Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as \"Why does this model makes such decisions?\" or \"Was it a specific feature that caused the decision made by the model?\". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable\u00a0\u2026", "num_citations": "43\n", "authors": ["1157"]}
{"title": "ANOMALOUS: A Joint Modeling Approach for Anomaly Detection on Attributed Networks.\n", "abstract": " The key point of anomaly detection on attributed networks lies in the seamless integration of network structure information and attribute information. A vast majority of existing works are mainly based on the Homophily assumption that implies the nodal attribute similarity of connected nodes. Nonetheless, this assumption is untenable in practice as the existence of noisy and structurally irrelevant attributes may adversely affect the anomaly detection performance. Despite the fact that recent attempts perform subspace selection to address this issue, these algorithms treat subspace selection and anomaly detection as two separate steps which often leads to suboptimal solutions. In this paper, we investigate how to fuse attribute and network structure information more synergistically to avoid the adverse effects brought by noisy and structurally irrelevant attributes. Methodologically, we propose a novel joint framework to conduct attribute selection and anomaly detection as a whole based on CUR decomposition and residual analysis. By filtering out noisy and irrelevant node attributes, we perform anomaly detection with the remaining representative attributes. Experimental results on both synthetic and real-world datasets corroborate the effectiveness of the proposed framework.", "num_citations": "43\n", "authors": ["1157"]}
{"title": "Trust in social media\n", "abstract": " Social media greatly enables people to participate in online activities and shatters the barrier for online users to create and share information at any place at any time. However, the explosion of user-generated content poses novel challenges for online users to find relevant information, or, in other words, exacerbates the information overload problem. On the other hand, the quality of user-generated content can vary dramatically from excellence to abuse or spam, resulting in a problem of information credibility. The study and understanding of trust can lead to an effective approach to addressing both information overload and credibility problems.  Trust refers to a relationship between a trustor (the subject that trusts a target entity) and a trustee (the entity that is trusted). In the context of social media, trust provides evidence about with whom we can trust to share information and from whom we can accept information\u00a0\u2026", "num_citations": "43\n", "authors": ["1157"]}
{"title": "Hierarchical propagation networks for fake news detection: Investigation and exploitation\n", "abstract": " Consuming news from social media is becoming increasingly popular. However, social media also enables the wide dissemination of fake news. Because of the detrimental effects of fake news, fake news detection has attracted increasing attention. However, the performance of detecting fake news only from news content is generally limited as fake news pieces are written to mimic true news. In the real world, news pieces spread through propagation networks on social media. The news propagation networks usually involve multi-levels. In this paper, we study the challenging problem of investigating and exploiting news hierarchical propagation network on social media for fake news detection.", "num_citations": "42\n", "authors": ["1157"]}
{"title": "Acclimatizing taxonomic semantics for hierarchical content classification\n", "abstract": " Hierarchical models have been shown to be effective in content classification. However, we observe through empirical study that the performance of a hierarchical model varies with given taxonomies; even a semantically sound taxonomy has potential to change its structure for better classification. By scrutinizing typical cases, we elucidate why a given semantics-based hierarchy does not work well in content classification, and how it could be improved for accurate hierarchical classification. With these understandings, we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification. We conduct extensive experiments on both toy and real-world data sets, report improved performance and interesting findings, and provide further analysis of algorithmic issues such as time complexity, robustness, and sensitivity to the number of features.", "num_citations": "42\n", "authors": ["1157"]}
{"title": "Sensor Sequence Modeling for Driving.\n", "abstract": " Intelligent systems in automobiles need to be aware of the driving and driver context. Available sensor data stream has to be modeled and monitored in order to do so. Currently there exist no building blocks for hierarchical modeling of driving. By semi-supervised segmentation such building blocks can be discovered. We call them drivemes in analogy to phonemes. More parsimonious modeling of driving becomes now possible.", "num_citations": "42\n", "authors": ["1157"]}
{"title": "Streaming link prediction on dynamic attributed networks\n", "abstract": " Link prediction targets to predict the future node interactions mainly based on the current network snapshot. It is a key step in understanding the formation and evolution of the underlying networks; and has practical implications in many real-world applications, ranging from friendship recommendation, click through prediction to targeted advertising. Most existing efforts are devoted to plain networks and assume the availability of network structure in memory before link prediction takes place. However, this assumption is untenable as many real-world networks are affiliated with rich node attributes, and often, the network structure and node attributes are both dynamically evolving at an unprecedented rate. Even though recent studies show that node attributes have an added value to network structure for accurate link prediction, it still remains a daunting task to support link prediction in an online fashion on such\u00a0\u2026", "num_citations": "41\n", "authors": ["1157"]}
{"title": "Modelling classification performance for large data sets\n", "abstract": " For many learning algorithms, their learning accuracy will increase as the size of training data increases, forming the well-known learning curve. Usually a learning curve can be fitted by interpolating or extrapolating some points on it with a specified model. The obtained learning curve can then be used to predict the maximum achievable learning accuracy or to estimate the amount of data needed to achieve an expected learning accuracy, both of which will be especially meaningful to data mining on large data sets. Although some models have been proposed to model learning curves, most of them do not test their applicability to large data sets. In this paper, we focus on this issue. We empirically compare six potentially useful models by fitting learning curves of two typical classification algorithms\u2014C4.5 (decision tree) and LOG (logistic discrimination) on eight large UCI benchmark data sets. By using all\u00a0\u2026", "num_citations": "41\n", "authors": ["1157"]}
{"title": "A behavior analytics approach to identifying tweets from crisis regions\n", "abstract": " The growing popularity of Twitter as an information medium has allowed unprecedented access to first-hand information during crises and mass emergency situations. Due to the sheer volume of information generated during a disaster, a key challenge is to filter tweets from the crisis region so their analysis can be prioritized. In this paper, we introduce the task of identifying whether a tweet is generated from crisis regions and formulate it as a decision problem. This problem is challenging due to the fact that only~ 1% of all tweets have location information. Existing approaches tackle this problem by predicting the location of the user using historical tweets from users or their social network. As collecting historical information is not practical during emergency situations, we investigate whether it is possible to determine that a tweet originates from the crisis region through the information in the tweet and the publishing\u00a0\u2026", "num_citations": "40\n", "authors": ["1157"]}
{"title": "Leveraging knowledge across media for spammer detection in microblogging\n", "abstract": " While microblogging has emerged as an important information sharing and communication platform, it has also become a convenient venue for spammers to overwhelm other users with unwanted content. Currently, spammer detection in microblogging focuses on using social networking information, but little on content analysis due to the distinct nature of microblogging messages. First, label information is hard to obtain. Second, the texts in microblogging are short and noisy. As we know, spammer detection has been extensively studied for years in various media, eg, emails, SMS and the web. Motivated by abundant resources available in the other media, we investigate whether we can take advantage of the existing resources for spammer detection in microblogging. While people accept that texts in microblogging are different from those in other media, there is no quantitative analysis to show how different they\u00a0\u2026", "num_citations": "40\n", "authors": ["1157"]}
{"title": "A connectionist approach to generating oblique decision trees\n", "abstract": " Neural networks and decision tree methods are two common approaches to pattern classification. While neural networks can achieve high predictive accuracy rates, the decision boundaries they form are highly nonlinear and generally difficult to comprehend. Decision trees, on the other hand, can be readily translated into a set of rules. In this paper, we present a novel algorithm for generating oblique decision trees that capitalizes on the strength of both approaches. Oblique decision trees classify the patterns by testing on linear combinations of the input attributes. As a result, an oblique decision tree is usually much smaller than the univariate tree generated for the same domain. Our algorithm consists of two components: connectionist and symbolic. A three-layer feedforward neural network is constructed and pruned, a decision tree is then built from the hidden unit activation values of the pruned network. An\u00a0\u2026", "num_citations": "40\n", "authors": ["1157"]}
{"title": "Sparse modeling-based sequential ensemble learning for effective outlier detection in high-dimensional numeric data\n", "abstract": " The large proportion of irrelevant or noisy features in real-life high-dimensional data presents a significant challenge to subspace/feature selection-based high-dimensional outlier detection (aka outlier scoring) methods. These methods often perform the two dependent tasks: relevant feature subset search and outlier scoring independently, consequently retaining features/subspaces irrelevant to the scoring method and downgrading the detection performance. This paper introduces a novel sequential ensemble-based framework SEMSE and its instance CINFO to address this issue. SEMSE learns the sequential ensembles to mutually refine feature selection and outlier scoring by iterative sparse modeling with outlier scores as the pseudo target feature. CINFO instantiates SEMSE by using three successive recurrent components to build such sequential ensembles. Given outlier scores output by an existing outlier scoring method on a feature subset, CINFO first defines a Cantelli's inequality-based outlier thresholding function to select outlier candidates with a false positive upper bound. It then performs lasso-based sparse regression by treating the outlier scores as the target feature and the original features as predictors on the outlier candidate set to obtain a feature subset that is tailored for the outlier scoring method. Our experiments show that two different outlier scoring methods enabled by CINFO (i) perform significantly better on 11 real-life high-dimensional data sets, and (ii) have much better resilience to noisy features, compared to their bare versions and three state-of-the-art competitors. The source code of CINFO is available at https://sites\u00a0\u2026", "num_citations": "39\n", "authors": ["1157"]}
{"title": "Reconstruction-based Unsupervised Feature Selection: An Embedded Approach.\n", "abstract": " Feature selection has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. Since real-world data is usually unlabeled, unsupervised feature selection has received increasing attention in recent years. Without label information, unsupervised feature selection needs alternative criteria to define feature relevance. Recently, data reconstruction error emerged as a new criterion for unsupervised feature selection, which defines feature relevance as the capability of features to approximate original data via a reconstruction function. Most existing algorithms in this family assume predefined, linear reconstruction functions. However, the reconstruction function should be data dependent and may not always be linear especially when the original data is high-dimensional. In this paper, we investigate how to learn the reconstruction function from the data automatically for unsupervised feature selection, and propose a novel reconstruction-based unsupervised feature selection framework REFS, which embeds the reconstruction function learning process into feature selection. Experiments on various types of realworld datasets demonstrate the effectiveness of the proposed framework REFS.", "num_citations": "39\n", "authors": ["1157"]}
{"title": "Toward online node classification on streaming networks\n", "abstract": " The proliferation of networked data in various disciplines motivates a surge of research interests on network or graph mining. Among them, node classification is a typical learning task that focuses on exploiting the node interactions to infer the missing labels of unlabeled nodes in the network. A vast majority of existing node classification algorithms overwhelmingly focus on static networks and they assume the whole network structure is readily available before performing learning algorithms. However, it is not the case in many real-world scenarios where new nodes and new links are continuously being added in the network. Considering the streaming nature of networks, we study how to perform online node classification on this kind of streaming networks (a.k.a. online learning on streaming networks). As the existence of noisy links may negatively affect the node classification performance, we first present\u00a0\u2026", "num_citations": "38\n", "authors": ["1157"]}
{"title": "Unsupervised sentiment analysis with signed social networks\n", "abstract": " Huge volumes of opinion-rich data is user-generated in social media at an unprecedented rate, easing the analysis of individual and public sentiments. Sentiment analysis has shown to be useful in probing and understanding emotions, expressions and attitudes in the text. However, the distinct characteristics of social media data present challenges to traditional sentiment analysis. First, social media data is often noisy, incomplete and fast-evolved which necessitates the design of a sophisticated learning model. Second, sentiment labels are hard to collect which further exacerbates the problem by not being able to discriminate sentiment polarities. Meanwhile, opportunities are also unequivocally presented. Social media contains rich sources of sentiment signals in textual terms and user interactions, which could be helpful in sentiment analysis. While there are some attempts to leverage implicit sentiment signals in positive user interactions, little attention is paid on signed social networks with both positive and negative links. The availability of signed social networks motivates us to investigate if negative links also contain useful sentiment signals. In this paper, we study a novel problem of unsupervised sentiment analysis with signed social networks. In particular, we incorporate explicit sentiment signals in textual terms and implicit sentiment signals from signed social networks into a coherent model SignedSenti for unsupervised sentiment analysis. Empirical experiments on two real-world datasets corroborate its effectiveness.", "num_citations": "38\n", "authors": ["1157"]}
{"title": "A study of support vectors on model independent example selection\n", "abstract": " As databases for real-world problems increase in size, there is a need in many situations to select and keep relevant training data for efficient storage and processing reasons. Support vector machines (SVMs) reportedly exhibit certain desirable properties in selecting and preserving useful training data as support vectors. This paper attempts to quantify the extent to which SVM training behaves like a model independent example selection procedure. Using several common machine-learning training databases, we compare the prediction results obtained by different classifiers, trained with data selected by SVMs and by two other example selection methods (IB2 and random sampling). Some interesting observations are made with explanations.", "num_citations": "38\n", "authors": ["1157"]}
{"title": "Hybrid search of feature subsets\n", "abstract": " Feature selection is a search problem for an \u201coptimal\u201d subset of features. The class separability is normally used as one of the basic feature selection criteria. Instead of maximizing the class separability as in the literature, this work adopts a criterion aiming to maintain the discriminating power of the data. After examining the pros and cons of two existing algorithms for feature selection, we propose a hybrid algorithm of probabilistic and complete search that can take advantage of both algorithms. It begins by running LVF (probabilistic search) to reduce the number of features; then it runs \u201cAutomatic Branch & Bound (ABB)\u201d (complete search). By imposing a limit on the amount of time this algorithm can run, we obtain an approximation algorithm. The empirical study suggests that dividing the time equally between the two phases yields nearly the best performance, and that the hybrid search algorithm\u00a0\u2026", "num_citations": "38\n", "authors": ["1157"]}
{"title": "Less is more\n", "abstract": " As computer and database technologies rapidly advance, human beings rely more and more on computers to accumulate data, process data, and make use of data. Machine learning, knowledge discovery, and data mining are some intelligent tools that help mankind accomplish those tasks. Researchers and practitioners realize that in order to use these tools effectively, an important part is pre-processing in which data is processed before it is presented to any learning, discovering, or visualizing algorithm. In many discovery applications (for example, marketing data analysis), a key operation is to find subsets of the population that behave enough alike to be worthy of focused analysis (Brackman and Anand, 1996). Although many learning methods attempt to select, extract, or construct features, both theoretical analyses and experimental studies indicate that many algorithms scale poorly in domains with\u00a0\u2026", "num_citations": "38\n", "authors": ["1157"]}
{"title": "Next-item recommendation with sequential hypergraphs\n", "abstract": " There is an increasing attention on next-item recommendation systems to infer the dynamic user preferences with sequential user interactions. While the semantics of an item can change over time and across users, the item correlations defined by user interactions in the short term can be distilled to capture such change, and help in uncovering the dynamic user preferences. Thus, we are motivated to develop a novel next-item recommendation framework empowered by sequential hypergraphs. Specifically, the framework:(i) adopts hypergraph to represent the short-term item correlations and applies multiple convolutional layers to capture multi-order connections in the hypergraph;(ii) models the connections between different time periods with a residual gating layer; and (iii) is equipped with a fusion layer to incorporate both the dynamic item embedding and short-term user intent to the representation of each\u00a0\u2026", "num_citations": "37\n", "authors": ["1157"]}
{"title": "Unsupervised feature selection for outlier detection by modelling hierarchical value-feature couplings\n", "abstract": " Proper feature selection for unsupervised outlier detection can improve detection performance but is very challenging due to complex feature interactions, the mixture of relevant features with noisy/redundant features in imbalanced data, and the unavailability of class labels. Little work has been done on this challenge. This paper proposes a novel Coupled Unsupervised Feature Selection framework (CUFS for short) to filter out noisy or redundant features for subsequent outlier detection in categorical data. CUFS quantifies the outlierness (or relevance) of features by learning and integrating both the feature value couplings and feature couplings. Such value-to-feature couplings capture intrinsic data characteristics and distinguish relevant features from those noisy/redundant features. CUFS is further instantiated into a parameter-free Dense Subgraph-based Feature Selection method, called DSFS. We prove that\u00a0\u2026", "num_citations": "37\n", "authors": ["1157"]}
{"title": "10 bits of surprise: Detecting malicious users with minimum information\n", "abstract": " Malicious users are a threat to many sites and defending against them demands innovative countermeasures. When malicious users join sites, they provide limited information about themselves. With this limited information, sites can find it difficult to distinguish between a malicious user and a normal user. In this study, we develop a methodology that identifies malicious users with limited information. As information provided by malicious users can vary, the proposed methodology utilizes minimum information to identify malicious users. It is shown that as little as 10 bits of information can help greatly in this challenging task. The experiments results verify that this methodology is effective in identifying malicious users in the realistic scenario of limited information availability.", "num_citations": "37\n", "authors": ["1157"]}
{"title": "Predictive risk modelling for forecasting high-cost patients: a real-world application using Medicaid data\n", "abstract": " Approximately two?thirds of healthcare costs are accounted for by 10% of the patients. Identifying such high-cost patients early can help improve their health and reduce costs. Data from the Arizona Health Care Cost Containment System provides a unique opportunity to exploit state-of-the-art data analysis algorithms to mine data and provide actionable findings that can aid cost containment. A novel data mining approach is proposed for this challenging healthcare problem of predicting patients who are likely to be high-risk in the future. This study indicates that the proposed approach is highly effective and can benefit further research on cost containment.", "num_citations": "37\n", "authors": ["1157"]}
{"title": "Mining \u201chidden phrase\u201d definitions from the Web\n", "abstract": " Keyword searching is the most common form of document search on the Web. Many Web publishers manually annotate the META tags and titles of their pages with frequently queried phrases in order to improve their placement and ranking. A \u201c hidden phrase\u201d is defined as a phrase that occurs in the META tag of a Web page but not in its body. In this paper we present an algorithm that mines the definitions of hidden phrases from the Web documents. Phrase definitions allow (i) publishers to find relevant phrases with high query frequency, and, (ii) search engines to test if the content of the body of a document matches the phrases. We use co-occurrence clustering and association rule mining algorithms to learn phrase definitions from high-dimensional data sets. We also provide experimental results.", "num_citations": "37\n", "authors": ["1157"]}
{"title": "Exploring hierarchical structures for recommender systems\n", "abstract": " Items in real-world recommender systems exhibit certain hierarchical structures. Similarly, user preferences also present hierarchical structures. Recent studies show that incorporating the hierarchy of items or user preferences can improve the performance of recommender systems. However, hierarchical structures are often not explicitly available, especially those of user preferences. Thus, there's a gap between the importance of hierarchies and their availability. In this paper, we investigate the problem of exploring the implicit hierarchical structures for recommender systems when they are not explicitly available. We propose a novel recommendation framework to bridge the gap, which enables us to explore the implicit hierarchies of users and items simultaneously. We then extend the framework to integrate explicit hierarchies when they are available, which gives a unified framework for both explicit and implicit\u00a0\u2026", "num_citations": "36\n", "authors": ["1157"]}
{"title": "Unsupervised feature selection in signed social networks\n", "abstract": " The rapid growth of social media services brings a large amount of high-dimensional social media data at an unprecedented rate. Feature selection is powerful to prepare high-dimensional data by finding a subset of relevant features. A vast majority of existing feature selection algorithms for social media data exclusively focus on positive interactions among linked instances such as friendships and user following relations. However, in many real-world social networks, instances may also be negatively interconnected. Recent work shows that negative links have an added value over positive links in advancing many learning tasks. In this paper, we study a novel problem of unsupervised feature selection in signed social networks and propose a novel framework SignedFS. In particular, we provide a principled way to model positive and negative links for user latent representation learning. Then we embed the user\u00a0\u2026", "num_citations": "36\n", "authors": ["1157"]}
{"title": "Discovering, assessing, and mitigating data bias in social media\n", "abstract": " Social media has generated a wealth of data. Billions of people tweet, sharing, post, and discuss everyday. Due to this increased activity, social media platforms provide new opportunities for research about human behavior, information diffusion, and influence propagation at a scale that is otherwise impossible. Social media data is a new treasure trove for data mining and predictive analytics. Since social media data differs from conventional data, it is imperative to study its unique characteristics. This work investigates data collection bias associated with social media. In particular, we propose computational methods to assess if there is bias due to the way a social media site makes its data available, to detect bias from data samples without access to the full data, and to mitigate bias by designing data collection strategies that maximize coverage to minimize bias. We also present a new kind of data bias stemming\u00a0\u2026", "num_citations": "36\n", "authors": ["1157"]}
{"title": "Coselect: Feature selection with instance selection for social media data\n", "abstract": " Feature selection is widely used in preparing high-dimensional data for effective data mining. Attribute-value data in traditional feature selection differs from social media data, although both can be large-scale. Social media data is inherently not independent and identically distributed (i.i.d.), but linked. Furthermore, there is a lot of noise. The quality of social media data can vary drastically. These unique properties present challenges as well as opportunities for feature selection. Motivated by these differences, we propose a novel feature selection framework, CoSelect, for social media data. In particular, CoSelect can exploit link information by applying social correlation theories, incorporate instance selection with feature selection, and select relevant instances and features simultaneously. Experimental results on real-world social media datasets demonstrate the effectiveness of our proposed framework and its\u00a0\u2026", "num_citations": "36\n", "authors": ["1157"]}
{"title": "The good, the bad, and the ugly: uncovering novel research opportunities in social media mining\n", "abstract": " Big data is ubiquitous and can only become bigger, which challenges traditional data mining and machine learning methods. Social media is a new source of data that is significantly different from conventional ones. Social media data are mostly user-generated, and are big, linked, and heterogeneous. We present the good, the bad and the ugly associated with the multi-faceted social media data and exemplify the importance of some original problems with real-world examples. We discuss bias in social media data, evaluation dilemma, data reduction, inferring invisible information, and big-data paradox. We illuminate new opportunities of developing novel algorithms and tools for data science. In our endeavor of employing the good to tame the bad with the help of the ugly, we deepen the understanding of ever growing and continuously evolving data and create innovative solutions with interdisciplinary and\u00a0\u2026", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Users joining multiple sites: Friendship and popularity variations across sites\n", "abstract": " Our social media experience is no longer limited to a single site. We use different social media sites for different purposes and our information on each site is often partial. By collecting complementary information for the same individual across sites, one can better profile users. These profiles can help improve online services such as advertising or recommendation across sites. To combine complementary information across sites, it is critical to understand how information for the same individual varies across sites. In this study, we aim to understand how two fundamental properties of users vary across social media sites. First, we study how user friendship behavior varies across sites. Our findings show how friend distributions for individuals change as they join new sites. Next, we analyze how user popularity changes across sites as individuals join different sites. We evaluate our findings and demonstrate how our\u00a0\u2026", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Mining human mobility in location-based social networks\n", "abstract": " In recent years, there has been a rapid growth of location-based social networking services, such as Foursquare and Facebook Places, which have attracted an increasing number of users and greatly enriched their urban experience. Typical location-based social networking sites allow a user to \"check in\" at a real-world POI (point of interest, e.g., a hotel, restaurant, theater, etc.), leave tips toward the POI, and share the check-in with their online friends. The check-in action bridges the gap between real world and online social networks, resulting in a new type of social networks, namely location-based social networks (LBSNs). Compared to traditional GPS data, location-based social networks data contains unique properties with abundant heterogeneous information to reveal human mobility, i.e., \"when and where a user (who) has been to for what,\" corresponding to an unprecedented opportunity to better\u00a0\u2026", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Connecting users with similar interests via tag network inference\n", "abstract": " The popularity of social networking greatly increases interaction among people. However, one major challenge remains---how to connect people who share similar interests. In a social network, the majority of people who share similar interests with given a user are in the long tail that accounts for 80% of total population. Searching for similar users by following links in social network has two limitations: it is inefficient and incomplete. Thus, it is desirable to design new methods to find like-minded people. In this paper, we propose to use collective wisdom from the crowd or tag networks to solve the problem. In a tag network, each node represents a tag as described by some words, and the weight of an undirected edge represents the co-occurrence of two tags. As such, the tag network describes the semantic relationships among tags. In order to connect to other users of similar interests via a tag network, we use\u00a0\u2026", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Efficiently determining the starting sample size for progressive sampling\n", "abstract": " Given a large data set and a classification learning algorithm, Progressive Sampling (PS) uses increasingly larger random samples to learn until model accuracy no longer improves. It is shown that the technique is remarkably efficient compared to using the entire data. However, how to set the starting sample size for PS is still an open problem. We show that an improper starting sample size can still make PS expensive in computation due to running the learning algorithm on a large number of instances (of a sequence of random samples before achieving convergence) and excessive database scans to fetch the sample data. Using a suitable starting sample size can further improve the efficiency of PS. In this paper, we present a statistical approach which is able to efficiently find such a size. We call it the Statistical Optimal Sample Size(SOSS), in the sense that a sample of this size sufficiently resembles the\u00a0\u2026", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Towards an evolutionary algorithm: a comparison of two feature selection algorithms\n", "abstract": " In order to deal with a large number of attributes, probabilistic feature selection algorithms have been proposed. Pure random walk entails mediocre performance in terms of search time. Introducing adaptiveness into a probabilistic algorithm can lead to a more focused search that results in a better search time. We compare two algorithms in search of an efficient but not myopic algorithm for feature selection. Based on the comparative study, we suggest some ways of improvement towards an evolutionary feature selection algorithm for data mining.", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Shape description and grasping for robot hand-eye coordination\n", "abstract": " The successful execution of grasping by a robot hand requires translation of visual information into control signals to the hand, which produce the desired spatial orientation and preshape for grasping an arbitrary object. An approach to this problem that is based on separation of the task into two modules is presented. A vision module is used to transform an image into a volumetric shape description using generalized cones. The data structure containing this geometric information becomes an input to the grasping module, which obtains a list of feasible grasping modes and a set of control signals for the robot hand. Features of both modules are discussed.< >", "num_citations": "35\n", "authors": ["1157"]}
{"title": "Learning Homophily Couplings from Non-IID Data for Joint Feature Selection and Noise-Resilient Outlier Detection.\n", "abstract": " This paper introduces a novel wrapper-based outlier detection framework (WrapperOD) and its instance (HOUR) for identifying outliers in noisy data (ie, data with noisy features) with strong couplings between outlying behaviors. Existing subspace or feature selection-based methods are significantly challenged by such data, as their search of feature subset (s) is independent of outlier scoring and thus can be misled by noisy features. In contrast, HOUR takes a wrapper approach to iteratively optimize the feature subset selection and outlier scoring using a top-k outlier ranking evaluation measure as its objective function. HOUR learns homophily couplings between outlying behaviors (ie, abnormal behaviors are not independent-they bond together) in constructing a noise-resilient outlier scoring function to produce a reliable outlier ranking in each iteration. We show that HOUR (i) retains a 2-approximation outlier ranking to the optimal one; and (ii) significantly outperforms five stateof-the-art competitors on 15 real-world data sets with different noise levels in terms of AUC and/or P@ n. The source code of HOUR is available at https://sites. google. com/site/gspangsite/sourcecode.", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Exploring characteristics of suspended users and network stability on Twitter\n", "abstract": " Social media is rapidly becoming a medium of choice for understanding the cultural pulse of a region; e.g. for identifying what the population is concerned with and what kind of help is needed in a crisis. To assess this cultural pulse, it is critical to have an accurate assessment of who is saying what. Unfortunately, social media is also the home of users who engage in disruptive, disingenuous, and potentially illegal activity. A range of users, both human and non-human, carry out such social cyber-attacks. We ask, to what extent does the presence or absence of such users influence our ability to assess the cultural pulse of a region? Our prior research on this topic showed that Twitter-based network structures and content are unstable and can be highly impacted by the removal of suspended users. Because of this, statistical techniques can be established to differentiate potential types of suspended and non\u00a0\u2026", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Finding requests in social media for disaster relief\n", "abstract": " Natural disasters create an uncertain environment in which first responders face the challenge of locating affected people and dispatching aids and resources in a timely manner. In recent years, crowdsourcing systems have been developed to exploit the power of volunteers to facilitate humanitarian logistic efforts. Most of the current systems require volunteers to directly provide input to them and do not have the capability to benefit the large number of disaster-related posts that are published on social media. Hence, many social media posts in the aftermath of disasters remain hidden. Among these hidden posts are those that need immediate attention, such as requests for help. Hence, we have implemented a system that detects requests on Twitter using content and context of tweets.", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Group profiling for understanding social structures\n", "abstract": " The prolific use of participatory Web and social networking sites is reshaping the ways in which people interact with one another. It has become a vital part of human social life in both the developed and developing world. People sharing certain similarities or affiliates tend to form communities within social media. At the same time, they participate in various online activities: content sharing, tagging, posting status updates, etc. These diverse activities leave behind traces of their social life, providing clues to understand changing social structures. A large body of existing work focuses on extracting cohesive groups based on network topology. But little attention is paid to understanding the changing social structures. In order to help explain the formation of a group, we explore different group-profiling strategies to construct descriptions of a group. This research can assist network navigation, visualization, and analysis, as\u00a0\u2026", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Integrating social media data for community detection\n", "abstract": " Community detection is an unsupervised learning task that discovers groups such that group members share more similarities or interact more frequently among themselves than with people outside groups. In social media, link information can reveal heterogeneous relationships of various strengths, but often can be noisy. Since different sources of data in social media can provide complementary information, e.g., bookmarking and tagging data indicates user interests, frequency of commenting suggests the strength of ties, etc., we propose to integrate social media data of multiple types for improving the performance of community detection. We present a joint optimization framework to integrate multiple data sources for community detection. Empirical evaluation on both synthetic data and real-world social media data shows significant performance improvement of the proposed approach. This work\u00a0\u2026", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Predicting future high-cost patients: A real-world risk modeling application\n", "abstract": " Health care data from patients in the Arizona Health Care Cost Containment System, Arizona's Medicaid program, provides a unique opportunity to exploit state-of-the-art data processing and analysis algorithms to mine the data and provide actionable results that can aid cost containment. This work addresses specific challenges in this real-life health care application to build predictive risk models for forecasting future high-cost users. Such predictive risk modeling has received attention in recent years with statistical techniques being the backbone of proposed methods. We survey the literature and propose a novel data mining approach customized for this potent application. Our empirical study indicates that this approach is useful and can benefit further research on cost containment in the health care industry.", "num_citations": "34\n", "authors": ["1157"]}
{"title": "Self-generating neural networks\n", "abstract": " A method for generating neural networks automatically is proposed. Not only the weights of the connections but also the structure of the network, including the number of neurons, the number of layers, and the interconnections among the neurons, is learned from the training examples. Issues of optimization and pruning of the generated networks are investigated. An experimental system has been implemented based on the proposed method and some experimental results and comparisons between this method and other methods are also given.< >", "num_citations": "34\n", "authors": ["1157"]}
{"title": "User identification across social media\n", "abstract": " Users may be identified across websites, such as social media websites. Prior user information data and candidate user information data may be received. An algorithm may identify a first plurality of behavioral patterns in the prior user information data and a second plurality of behavioral patterns in the candidate user information datum. The algorithm may determine whether the candidate user information datum and the prior user information data correspond to the same user based, at least in part, on the first and second pluralities of behavioral patterns.", "num_citations": "33\n", "authors": ["1157"]}
{"title": "Toward time-evolving feature selection on dynamic networks\n", "abstract": " Recent years have witnessed the prevalence of networked data in various domains. Among them, a large number of networks are not only topologically structured but also have a rich set of features on nodes. These node features are usually of high dimensionality with noisy, irrelevant and redundant information, which may impede the performance of other learning tasks. Feature selection is useful to alleviate these critical issues. Nonetheless, a vast majority of existing feature selection algorithms are predominantly designed in a static setting. In reality, real-world networks are naturally dynamic, characterized by both topology and content changes. It is desirable to capture these changes to find relevant features tightly hinged with network structure continuously, which is of fundamental importance for many applications such as disaster relief and viral marketing. In this paper, we study a novel problem of time\u00a0\u2026", "num_citations": "33\n", "authors": ["1157"]}
{"title": "The fragility of twitter social networks against suspended users\n", "abstract": " Social media is rapidly becoming one of the mediums of choice for understanding the cultural pulse of a region; eg, for identifying what the population is concerned with and what kind of help is needed in a crisis. To assess this cultural pulse it is critical to have an accurate assessment of who is saying what in social media. However, social media is also the home of malicious users engaged in disruptive, disingenuous, and potentially illegal activity. A range of users, both human and non-human, carry out such social cyber-attacks. We ask, to what extent does the presence or absence of such users influence our ability to assess the cultural pulse of a region? We conduct a series of experiments to analyze the fragility of social network assessments based on Twitter data by comparing changes in both the structural and content results when suspended users are left in and taken out. Because a Twitter account can be\u00a0\u2026", "num_citations": "33\n", "authors": ["1157"]}
{"title": "Social computing, behavioral modeling, and prediction\n", "abstract": " Social computing concerns the study of social behavior and context based on computational systems. Behavioral modeling reproduces the social behavior, and allows for experimenting with and deep understanding of behavior, patterns, and potential outcomes. The pervasive use of computer and Internet technologies provides an unprecedented environment where people can share opinions and experiences, offer suggestions and advice, debate, and even conduct experiments. Social computing facilitates behavioral modeling in model building, analysis, pattern mining, anticipation, and prediction. The proceedings from this interdisciplinary workshop provide a platform for researchers, practitioners, and graduate students from sociology, behavioral and computer science, psychology, cultural study, information systems, and operations research to share results and develop new concepts and methodologies aimed at advancing and deepening our understanding of social and behavioral computing to aid critical decision making.", "num_citations": "33\n", "authors": ["1157"]}
{"title": "Active feature selection using classes\n", "abstract": " Feature selection is frequently used in data pre-processing for data mining. When the training data set is too large, sampling is commonly used to overcome the difficulty. This work investigates the applicability of active sampling in feature selection in a filter model setting. Our objective is to partition data by taking advantage of class information so as to achieve the same or better performance for feature selection with fewer but more relevant instances than random sampling. Two versions of active feature selection that employ class information are proposed and empirically evaluated. In comparison with random sampling, we conduct extensive experiments with benchmark data sets, and analyze reasons why class-based active feature selection works in the way it does. The results will help us deal with large data sets and provide ideas to scale up other feature selection algorithms.", "num_citations": "33\n", "authors": ["1157"]}
{"title": "Detecting fake news on social media\n", "abstract": " In the past decade, social media has become increasingly popular for news consumption due to its easy access, fast dissemination, and low cost. However, social media also enables the wide propagation of \"fake news,\" i.e., news with intentionally false information. Fake news on social media can have significant negative societal effects. Therefore, fake news detection on social media has recently become an emerging research area that is attracting tremendous attention. This book, from a data mining perspective, introduces the basic concepts and characteristics of fake news across disciplines, reviews representative fake news detection methods in a principled way, and illustrates challenging issues of fake news detection on social media. In particular, we discussed the value of news content and social context, and important extensions to handle early detection, weakly-supervised detection, and explainable\u00a0\u2026", "num_citations": "32\n", "authors": ["1157"]}
{"title": "A single-actuator prosthetic hand using a continuum differential mechanism\n", "abstract": " Substantial progresses have been made in building versatile anthropomorphic prosthetic hands in the past two decades using emerging technologies. However the trade-offs between functionality, reliability, affordability, appearance, etc. have not been fully settled. Many existing designs, particularly the commercial prosthetic hands, are underactuated and they can realize various grasps through compliant structures or differential mechanisms. This paper presents the design of an underactuated prosthetic hand with one actuator using a continuum differential mechanism. Structure of the continuum differential mechanism is simple enough to allow all the components, including a battery pack, to be packed into the palm. The design concept, component descriptions, and hand constructions are elaborated. Experimental verifications are presented to demonstrate the efficacy of the proposed design.", "num_citations": "32\n", "authors": ["1157"]}
{"title": "Scaling and correlation of human movements in cyberspace and physical space\n", "abstract": " Understanding the dynamics of human movements is key to issues of significant current interest such as behavioral prediction, recommendation, and control of epidemic spreading. We collect and analyze big data sets of human movements in both cyberspace (through browsing of websites) and physical space (through mobile towers) and find a superlinear scaling relation between the mean frequency of visit\u2329 f\u232a and its fluctuation \u03c3: \u03c3\u223c\u2329 f\u232a \u03b2 with \u03b2\u2248 1.2. The probability distribution of the visiting frequency is found to be a stretched exponential function. We develop a model incorporating two essential ingredients, preferential return and exploration, and show that these are necessary for generating the scaling relation extracted from real data. A striking finding is that human movements in cyberspace and physical space are strongly correlated, indicating a distinctive behavioral identifying characteristic and implying\u00a0\u2026", "num_citations": "32\n", "authors": ["1157"]}
{"title": "Uncovering cross-dimension group structures in multi-dimensional networks\n", "abstract": " With the proliferation of Web 2.0 and social networking sites, people can interact with each other easily through various social media. For instance, popular sites like Del. icio. us, Flickr, and YouTube allow users to comment sharing content (bookmark, photos, videos), and users can tag her own favorite content. Users can also connect to friends, and subscribe to or become a fan of other users. These diverse individual activities result in a multi-dimensional network among actors, forming cross-dimension group structures with group members focusing on similar topics. It is challenging to effectively integrate the network information of multiple dimensions to find out the cross-dimension group structure. In this work, we propose a two-phase strategy to identify the hidden structures shared across dimensions in multi-dimensional networks. We extract structural features from each dimension of the network via modularity analysis, and then integrate them to find out a robust community structure among actors. Experiments on synthetic and real-world data validate the superiority of our strategy, enabling the analysis of collective behavior underneath diverse individual activities in a large scale.", "num_citations": "32\n", "authors": ["1157"]}
{"title": "Crossfire: Cross media joint friend and item recommendations\n", "abstract": " Friend and item recommendation on a social media site is an important task, which not only brings conveniences to users but also benefits platform providers. However, recommendation for newly launched social media sites is challenging because they often lack user historical data and encounter data sparsity and cold-start problem. Thus, it is important to exploit auxiliary information to help improve recommendation performances on these sites. Existing approaches try to utilize the knowledge transferred from other mature sites, which often require overlapped users or similar items to ensure an effective knowledge transfer. However, these assumptions may not hold in practice because 1) Overlapped user set is often unavailable and costly to identify due to the heterogeneous user profile, content and network data, and 2) Different schemes to show item attributes across sites cause the attribute values inconsistent\u00a0\u2026", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Ppp: Joint pointwise and pairwise image label prediction\n", "abstract": " Pointwise label and Pairwise label are both widely used in computer vision tasks. For example, supervised image classification and annotation approaches use pointwise label, while attribute-based image relative learning often adopts pairwise labels. These two types of labels are often considered independently and most existing efforts utilize them separately. However, pointwise labels in image classification and tag annotation are inherently related to the pairwise labels. For example, an image labeled with\" coast\" and annotated with\" beach, sea, sand, sky\" is more likely to have a higher ranking score in terms of the attribute\" open\"; while\" men shoes\" ranked highly on the attribute\" formal\" are likely to be annotated with\" leather, lace up\" than\" buckle, fabric\". The existence of potential relations between pointwise labels and pairwise labels motivates us to fuse them together for jointly addressing related vision tasks. In particular, we provide a principled way to capture the relations between class labels, tags and attributes; and propose a novel framework PPP (Pointwise and Pairwise image label Prediction), which is based on overlapped group structure extracted from the pointwise-pairwise-label bipartite graph. With experiments on benchmark datasets, we demonstrate that the proposed framework achieves superior performance on three vision tasks compared to the state-of-the-art methods.", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Actnet: Active learning for networked texts in microblogging\n", "abstract": " Supervised learning, e.g., classification, plays an important role in processing and organizing microblogging data. In microblogging, it is easy to mass vast quantities of unlabeled data, but would be costly to obtain labels, which are essential for supervised learning algorithms. In order to reduce the labeling cost, active learning is an effective way to select representative and informative instances to query for labels for improving the learned model. Different from traditional data in which the instances are assumed to be independent and identically distributed (i.i.d.), instances in microblogging are networked with each other. This presents both opportunities and challenges for applying active learning to microblogging data. Inspired by social correlation theories, we investigate whether social relations can help perform effective active learning on networked data. In this paper, we propose a novel Active learning\u00a0\u2026", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Community detection in multi-dimensional networks\n", "abstract": " The pervasiveness of Web 2.0 and social networking sites has enabled people to interact with each other easily through various social media. For instance, popular sites like Del. icio. us, Flickr, and YouTube allow users to comment on shared content bookmarks, photos, videos, and users can tag their favorite content. Users can also connect with one another, and subscribe to or become a fan or a follower of others. These diverse activities result in a multi-dimensional network among actors, forming group structures with group members sharing similar interests or a liations. This work systematically addresses two challenges. First, it is challenging to e ectively integrate interactions over multiple dimensions to discover hidden community structures shared by heterogeneous interactions. We show that representative community detection methods for single-dimensional networks can be presented in a uni-ed view. Based on this uni ed view, we present and analyze four possible integration strategies to extend community detection from single-dimensional to multi-dimensional networks. In particular, we propose a novel integration scheme based on structural features. Another challenge is the evaluation of different methods without ground truth information about community membership. We employ a novel cross-dimension network validation procedure to compare the performance of di erent methods. We use synthetic data to deepen our understanding, and real-world data to compare integration strategies as well as baseline methods in a large scale. We study further the computational time of di erent methods, normalization e ect during integration\u00a0\u2026", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Clustering blogs with collective wisdom\n", "abstract": " Blogosphere is expanding in an unprecedented speed. A better understanding of the blogosphere can greatly facilitate the development of the Social Web to serve the needs of users, service providers and advertisers. One important task in this process is clustering blog sites. Clustering blog sites presents new challenges. We propose to tap into collective wisdom in clustering blog sites, present statistical and visual results, report findings, and suggest future work extending to many real-world applications.", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Scalable feature selection for large sized databases\n", "abstract": " Feature selection determines relevant features in the data. It is often applied in pattern classi cation. A special constraint for feature selection nowadays is that the size of a database is normally very large. An e ective method is needed to accommodate the practical demands. A scalable probabilistic algorithm is presented here as an alternative to the exhaustive and heuristics approaches. The scalable probabilistic algorithm is designed and implemented to meet the needs arising from real-world data mining applications. Through experiments, we show that (1) the probabilistic algorithm is e ective in obtaining optimal/suboptimal subsets of features;(2) its scalable version expedites feature selection further and can scale up without sacri cing the quality of selected features.", "num_citations": "31\n", "authors": ["1157"]}
{"title": "Mining disinformation and fake news: Concepts, methods, and recent advancements\n", "abstract": " In recent years, disinformation including fake news, has became a global phenomenon due to its explosive growth, particularly on social media. The wide spread of disinformation and fake news can cause detrimental societal effects. Despite the recent progress in detecting disinformation and fake news, it is still non-trivial due to its complexity, diversity, multi-modality, and costs of fact-checking or annotation. The goal of this chapter is to pave the way for appreciating the challenges and advancements via: (1) introducing the types of information disorder on social media and examine their differences and connections; (2) describing important and emerging tasks to combat disinformation for characterization, detection and attribution; and (3) discussing a weak supervision approach to detect disinformation with limited labeled data. We then provide an overview of the chapters in this book that represent the recent\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Deep headline generation for clickbait detection\n", "abstract": " Clickbaits are catchy social posts or sensational headlines that attempt to lure readers to click. Clickbaits are pervasive on social media and can have significant negative impacts on both users and media ecosystems. For example, users may be misled to receive inaccurate information or fall into click-jacking attacks. Similarly, media platforms could lose readers' trust and revenues due to the prevalence of clickbaits. To computationally detect such clickbaits on social media using a supervised learning framework, one of the major obstacles is the lack of large-scale labeled training data, due to the high cost of labeling. With the recent advancements of deep generative models, to address this challenge, we propose to generate synthetic headlines with specific styles and explore their utilities to help improve clickbait detection. In particular, we propose to generate stylized headlines from original documents with style\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Feature Selection.\n", "abstract": " Data dimensionality is growing rapidly, which poses challenges to the vast majority of existing mining and learning algorithms, such as the curse of dimensionality, large storage requirement, and high computational cost. Feature selection has been proven to be an effective and efficient way to prepare high-dimensional data for data mining and machine learning. The recent emergence of novel techniques and new types of data and features not only advances existing feature selection research but also evolves feature selection continually, becoming applicable to a broader range of applications. In this entry, we aim to provide a basic introduction to feature selection including basic concepts, classifications of existing systems, recent development, and applications.", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Surpassing the limit: Keyword clustering to improve Twitter sample coverage\n", "abstract": " Social media services have become a prominent source of research data for both academia and corporate applications. Data from social media services is easy to obtain, highly structured, and comprises opinions from a large number of extremely diverse groups. The microblogging site, Twitter, has garnered a particularly large following from researchers by offering a high volume of data streamed in real time. Unfortunately, the methods in which Twitter selects data to disseminate through the stream are either vague or unpublished. Since Twitter maintains sole control of the sampling process, it leaves us with no knowledge of how the data that we collect for research is selected. Additionally, past research has shown that there are sources of bias present in Twitters dissemination process. Such bias introduces noise into the data that can reduce the accuracy of learning models and lead to bad inferences. In this work\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Feature selection for social media data\n", "abstract": " Feature selection is widely used in preparing high-dimensional data for effective data mining. The explosive popularity of social media produces massive and high-dimensional data at an unprecedented rate, presenting new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attribute-value data such as posts, tweets, comments, and images, and (2) linked data that provides social context for posts and describes the relationships between social media users as well as who generates the posts, and so on. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this article, we study a novel feature selection problem of selecting features for social media data with its social context. In detail, we illustrate the differences between attribute-value data and social media data\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "A tool for collecting provenance data in social media\n", "abstract": " In recent years, social media sites have provided a large amount of information. Recipients of such information need mechanisms to know more about the received information, including the provenance. Previous research has shown that some attributes related to the received information provide additional context, so that a recipient can assess the amount of value, trust, and validity to be placed in the received information. Personal attributes of a user, including name, location, education, ethnicity, gender, and political and religious affiliations, can be found in social media sites. In this paper, we present a novel web-based tool for collecting the attributes of interest associated with a particular social media user related to the received information. This tool provides a way to combine different attributes available at different social media sites into a single user profile. Using different types of Twitter users, we also evaluate\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Proceedings of the 2011 SIAM International Conference on Data Mining\n", "abstract": " Welcome to the Eleventh SIAM International Conference on Data Mining (SDM 2011). This year's conference is the continuation of the great success in the SDM conference series as one of the leading forums for data mining researchers, practitioners, developers and users to exchange cutting-edge ideas, techniques, and experience. SDM 2011 received 350 submissions from 35 countries on six continents (only Antarctica is missing); almost half from a contact author outside the hosting United States. This truly reflects the international character of this conference. Moreover, 206 submissions had a student as the first author. We are proud that SDM is attracting the coming generation of data miners. We continued the two-round review protocol initiated last year. All papers were first assigned to two program committee members. We evaluated these reviews and selected half of the papers for a second round. The\u00a0\u2026", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Data reduction via instance selection\n", "abstract": " Selection pressures are pervasive. As data grows, the demand for data reduction increases for effective data mining. Instance selection is one of effective means to data reduction. This chapter expounds basic concepts of instance selection, its context, necessity and functionality. It briefly introduces the state-of-the-art methods for instance selection, and presents an overview of the field as well as a summary of contributing chapters in this collection. Its coverage also includes evaluation issues, related work, and future directions.", "num_citations": "30\n", "authors": ["1157"]}
{"title": "Associations between selenium content in hair and Kashin-Beck disease/Keshan disease in children in northwestern China: a prospective cohort study\n", "abstract": " The objective of this study was to investigate the relationship between selenium content in hair and the incidence of Kashin-Beck disease (KBD) and Keshan disease (KD) in China. A prospective cohort study was conducted among children aged 5\u201312\u00a0years with different levels of low-selenium (group 1, Se\u00a0\u2264\u00a0110\u00a0ng/g; group 2, 110\u00a0<\u00a0Se\u00a0\u2264\u00a0150\u00a0ng/g; and group 3, 150\u00a0<\u00a0Se\u00a0\u2264\u00a0200\u00a0ng/g) or selenium-supplemented (group 4, Se >\u00a0200\u00a0ng/g) exposure. A person-years approach was used to calculate the incidence and rate of positive clinical signs. Relative risk (RR), attributable risk, and etiologic fraction were used to determine the strength of association between selenium and disease incidence. Seven new KBD cases were diagnosed during 3-year follow-up. Positive clinical signs of KBD were found in 17.78 (95% confidence interval [CI] 14.27\u201321.29) cases per 100 person-years in group 1, 13.28 (9.82\u201316.74\u00a0\u2026", "num_citations": "29\n", "authors": ["1157"]}
{"title": "CLARE: A joint approach to label classification and tag recommendation\n", "abstract": " Data classification and tag recommendation are both important and challenging tasks in social media. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, labels in data classification and tags in tag recommendation are inherently related. For example, a Youtube video annotated with NCAA, stadium, pac12 is likely to be labeled as football, while a video/image with the class label of coast is likely to be tagged with beach, sea, water and sand. The existence of relations between labels and tags motivates us to jointly perform classification and tag recommendation for social media data in this paper. In particular, we provide a principled way to capture the relations between labels and tags, and propose a novel framework CLARE, which fuses data CLAssification and tag REcommendation into a coherent model. With experiments on three social media datasets, we demonstrate that the proposed framework CLARE achieves superior performance on both tasks compared to the state-of-the-art methods.", "num_citations": "29\n", "authors": ["1157"]}
{"title": "Discriminant analysis for unsupervised feature selection\n", "abstract": " Feature selection has been proven to be efficient in preparing high dimensional data for data mining and machine learning. As most data is unlabeled, unsupervised feature selection has attracted more and more attention in recent years. Discriminant analysis has been proven to be a powerful technique to select discriminative features for supervised feature selection. To apply discriminant analysis, we usually need label information which is absent for unlabeled data. This gap makes it challenging to apply discriminant analysis for unsupervised feature selection. In this paper, we investigate how to exploit discriminant analysis in unsupervised scenarios to select discriminative features. We introduce the concept of pseudo labels, which enable discriminant analysis on unlabeled data, propose a novel unsupervised feature selection framework DisUFS which incorporates learning discriminative features with\u00a0\u2026", "num_citations": "29\n", "authors": ["1157"]}
{"title": "Electrochemical Gating of Tricarboxylic Acid Cycle in Electricity-Producing Bacterial Cells of Shewanella\n", "abstract": " Energy-conversion systems mediated by bacterial metabolism have recently attracted much attention, and therefore, demands for tuning of bacterial metabolism are increasing. It is widely recognized that intracellular redox atmosphere which is generally tuned by dissolved oxygen concentration or by appropriate selection of an electron acceptor for respiration is one of the important factors determining the bacterial metabolism. In general, electrochemical approaches are valuable for regulation of redox-active objects. However, the intracellular redox conditions are extremely difficult to control electrochemically because of the presence of insulative phospholipid bilayer membranes. In the present work, the limitation can be overcome by use of the bacterial genus Shewanella, which consists of species that are able to respire via cytochromes abundantly expressed in their outer-membrane with solid-state electron acceptors, including anodes. The electrochemical characterization and the gene expression analysis revealed that the activity of tricarboxylic acid (TCA) cycle in Shewanella cells can be reversibly gated simply by changing the anode potential. Importantly, our present results for Shewanella cells cultured in an electrochemical system under poised potential conditions showed the opposite relationship between the current and electron acceptor energy level, and indicate that this unique behavior originates from deactivation of the TCA cycle in the (over-)oxidative region. Our result obtained in this study is the first demonstration of the electrochemical gating of TCA cycle of living cells. And we believe that our findings will contribute to a deeper\u00a0\u2026", "num_citations": "29\n", "authors": ["1157"]}
{"title": "The effect of the characteristics of the dataset on the selection stability\n", "abstract": " Feature selection is an effective technique to reduce the dimensionality of a data set and to select relevant features for the domain problem. Recently, stability of feature selection methods has gained increasing attention. In fact, it has become a crucial factor in determining the goodness of a feature selection algorithm besides the learning performance. In this work, we conduct an extensive experimental study using verity of data sets and different well-known feature selection algorithms in order to study the behavior of these algorithms in terms of the stability.", "num_citations": "29\n", "authors": ["1157"]}
{"title": "Behavior informatics: A new perspective\n", "abstract": " This installment of Trends & Controversies provides an array of perspectives on the latest research in behavior informatics. Longbing Cao introduces the work in \"Behavior Informatics: A New Perspective.\" Then, in \"Behavior Computing,\" Longbing Cao and Thorsten Joachims provide a basic overview of the topic. Next is \"Coupled Behavior Representation, Modeling, Analysis, and Reasoning\" by Can Wang, Longbing Cao, Eric Gaussier, Jinjiu Li, Yuming Ou, and Dan Luo. The fourth article is \"Behavior Analysis in Social Media,\" by Reza Zafarani and Huan Liu. The fifth article is \"Group Recommendation and Behavior,\" by Guandong Xu and Zhiang Wu. Gabriella Pasi wrote the sixth article, \"Web Search and Behavior.\" The seventh article, \"Behaviors of IPTV Users,\" is by Ya Zhang, Xiaokang Yang, and Hongyuan Zha. Finally, \"Should Behavioral Models of Terror Groups Be Disclosed?\" is by Edoardo Serra and V.S\u00a0\u2026", "num_citations": "28\n", "authors": ["1157"]}
{"title": "Identifying biologically relevant genes via multiple heterogeneous data sources\n", "abstract": " Selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post-array analysis. Recent development in bioinformatics has made various data sources available such as mRNA and miRNA expression profiles, biological pathway and gene annotation, etc. Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance. In this work, we studied a novel problem of multi-source gene selection: given multiple heterogeneous data sources (or data sets), select genes from expression profiles by integrating information from various data sources. We investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection. We\u00a0\u2026", "num_citations": "28\n", "authors": ["1157"]}
{"title": "Dimensionality Reduction.\n", "abstract": " Dimensionality reduction studies methods that effectively reduce data dimensionality for efficient data processing tasks such as pattern recognition, machine learning, text retrieval, and data mining. We introduce the field of dimensionality reduction by dividing it into two parts: feature extraction and feature selection. Feature extraction creates new features resulting from the combination of the original features; and feature selection produces a", "num_citations": "28\n", "authors": ["1157"]}
{"title": "A balanced ensemble approach to weighting classifiers for text classification\n", "abstract": " This paper studies the problem of constructing an effective heterogeneous ensemble classifier for text classification. One major challenge of this problem is to formulate a good combination function, which combines the decisions of the individual classifiers in the ensemble. We show that the classification performance is affected by three weight components and they should be included in deriving an effective combination function. They are: (1) Global effectiveness, which measures the effectiveness of a member classifier in classifying a set of unseen documents; (2) Local effectiveness, which measures the effectiveness of a member classifier in classifying the particular domain of an unseen document; and (3) Decision confidence, which describes how confident a classifier is when making a decision when classifying a specific unseen document. We propose a new balanced combination function, called dynamic\u00a0\u2026", "num_citations": "28\n", "authors": ["1157"]}
{"title": "Similar but different: Exploiting users\u2019 congruity for recommendation systems\n", "abstract": " The pervasive use of social media provides massive data about individuals\u2019 online social activities and their social relations. The building block of most existing recommendation systems is the similarity between users with social relations, i.e., friends. While friendship ensures some homophily, the similarity of a user with her friends can vary as the number of friends increases. Research from sociology suggests that friends are more similar than strangers, but friends can have different interests. Exogenous information such as comments and ratings may help discern different degrees of agreement (i.e., congruity) among similar users. In this paper, we investigate if users\u2019 congruity can be incorporated into recommendation systems to improve it\u2019s performance. Experimental results demonstrate the effectiveness of embedding congruity related information into recommendation systems.", "num_citations": "27\n", "authors": ["1157"]}
{"title": "Remarkable impact of steam temperature on ginsenosides transformation from fresh ginseng to red ginseng\n", "abstract": " BackgroundTemperature is an essential condition in red ginseng processing. The pharmacological activities of red ginseng under different steam temperatures are significantly different.MethodsIn this study, an ultrahigh-performance liquid chromatography quadrupole time-of-flight tandem mass spectrometry was developed to distinguish the red ginseng products that were steamed at high and low temperatures. Multivariate statistical analyses such as principal component analysis and supervised orthogonal partial least squared discrimination analysis were used to determine the influential components of the different samples.ResultsThe results showed that different steamed red ginseng samples can be identified, and the characteristic components were 20-gluco-ginsenoside Rf, ginsenoside Re, ginsenoside Rg1, and malonyl-ginsenoside Rb1 in red ginseng steamed at low temperature. Meanwhile, the\u00a0\u2026", "num_citations": "27\n", "authors": ["1157"]}
{"title": "Network quantification despite biased labels\n", "abstract": " The increasing availability of participatory web and social media presents enormous opportunities to study human relations and collective behaviors. Many applications involving decision making want to obtain certain generalized properties about the population in a network, such as the proportion of actors given a category, instead of the category of individuals. While data mining and machine learning researchers have developed many methods for link-based classification or relational learning, most are optimized to classify individual nodes in a network. In order to accurately estimate the prevalence of one class in a network, some quantification method has to be used. In this work, two kinds of approaches are presented: quantification based on classification or quantification based on link analysis. Extensive experiments are conducted on several representative network data, with interesting findings reported\u00a0\u2026", "num_citations": "27\n", "authors": ["1157"]}
{"title": "Purification, characterization and biological effect of reversing the kidney-yang deficiency of polysaccharides from semen cuscutae\n", "abstract": " Semen cuscutae is a well-known Chinese medicine which has been used to nourish kidney. It is the first study to demonstrate that the polysaccharides from semen cuscutae showed significant activity of nourishing kidney-yang by increasing the levels of testosterone and estradiol, decreasing the level of blood urea nitrogen, improving immune function, possessing antioxidant effect. Three homogeneous polysaccharides were obtained by DEAE-cellulose and Sephacryl S-400 which were named as C-7WR1, C-7WR2 and C-7WR3 with average molecular weight of 7.59\u00a0\u00d7\u00a0104, 3.23\u00a0\u00d7\u00a0104 and 2.25\u00a0\u00d7\u00a0104 respectively. C-7WR1 was composed of fructose: mannose\u00a0=\u00a00.02:1. C-7WR2 was composed of fructose: mannose: xylose: arabinose\u00a0=\u00a00.01:1:0.14:0.33. C-7WR3 was composed of fructose: mannose: xylose: arabinose\u00a0=\u00a00.01:1:0.10:0.47. They mainly contained mannose. Their fourier transform infrared features were\u00a0\u2026", "num_citations": "26\n", "authors": ["1157"]}
{"title": "Blocking objectionable web content by leveraging multiple information sources\n", "abstract": " The World Wide Web has now become a humongous archive of various contents. The inordinate amount of information found on the web presents a challenge to deliver right information to the right users. On one hand, the abundant information is freely accessible to all web denizens; on the other hand, much of such information may be irrelevant or even deleterious to some users. For example, some control and filtering mechanisms are desired to prevent inappropriate or offensive materials such as pornographic websites from reaching children. Ways of accessing websites are termed as Access Scenarios. An Access Scenario can include using search engines (e.g., image search that has very little textual content), URL redirection to some websites, or directly typing (porn) website URLs. In this paper we propose a framework to analyze a website from several different aspects or information sources, and generate a\u00a0\u2026", "num_citations": "26\n", "authors": ["1157"]}
{"title": "Sampling: knowing whole from its part\n", "abstract": " Sampling is a well-established statistical technique that selects a part from a whole to make inferences about the whole. It can be employed to overcome problems caused by high dimensionality of attributes as well as large volumes of data in data mining. This chapter summarizes the basic ideas, assumptions, considerations and advantages as well as limitations of sampling, categorizes representative sampling methods by their features, provides a preliminary guideline on how to choose suitable sampling methods. We hope this can help users build a big picture of sampling methods and apply them in data mining.", "num_citations": "26\n", "authors": ["1157"]}
{"title": "Fake news research: Theories, detection strategies, and open problems\n", "abstract": " Fake news has become a global phenomenon due its explosive growth, particularly on social media. The goal of this tutorial is to (1) clearly introduce the concept and characteristics of fake news and how it can be formally differentiated from other similar concepts such as mis-/dis-information, satire news, rumors, among others, which helps deepen the understanding of fake news;(2) provide a comprehensive review of fundamental theories across disciplines and illustrate how they can be used to conduct interdisciplinary fake news research, facilitating a concerted effort of experts in computer and information science, political science, journalism, social science, psychology and economics. Such concerted efforts can result in highly efficient and explainable fake news detection;(3) systematically present fake news detection strategies from four perspectives (ie, knowledge, style, propagation, and credibility) and the\u00a0\u2026", "num_citations": "25\n", "authors": ["1157"]}
{"title": "Mitogenic stimulation accelerates influenza-induced mortality by increasing susceptibility of alveolar type II cells to infection\n", "abstract": " Development of pneumonia is the most lethal consequence of influenza, increasing mortality more than 50-fold compared with uncomplicated infection. The spread of viral infection from conducting airways to the alveolar epithelium is therefore a pivotal event in influenza pathogenesis. We found that mitogenic stimulation with keratinocyte growth factor (KGF) markedly accelerated mortality after infectious challenge with influenza A virus (IAV). Coadministration of KGF with IAV markedly accelerated the spread of viral infection from the airways to alveoli compared with challenge with IAV alone, based on spatial and temporal analyses of viral nucleoprotein staining of lung tissue sections and dissociated lung cells. To better define the temporal relationship between KGF administration and susceptibility to IAV infection in vivo, we administered KGF 120, 48, 24, and 0 h before intrapulmonary IAV challenge and assessed\u00a0\u2026", "num_citations": "25\n", "authors": ["1157"]}
{"title": "Protecting user privacy: An approach for untraceable web browsing history and unambiguous user profiles\n", "abstract": " The overturning of the Internet Privacy Rules by the Federal Communications Commissions (FCC) in late March 2017 allows Internet Service Providers (ISPs) to collect, share and sell their customers' Web browsing data without their consent. With third-party trackers embedded on Web pages, this new rule has put user privacy under more risk. The need arises for users on their own to protect their Web browsing history from any potential adversaries. Although some available solutions such as Tor, VPN, and HTTPS can help users conceal their online activities, their use can also significantly hamper personalized online services, ie, degraded utility. In this paper, we design an effective Web browsing history anonymization scheme, PBooster, aiming to protect users' privacy while retaining the utility of their Web browsing history. The proposed model pollutes users' Web browsing history by automatically inferring how\u00a0\u2026", "num_citations": "24\n", "authors": ["1157"]}
{"title": "Securing social media user data: An adversarial approach\n", "abstract": " Social media users generate tremendous amounts of data. To better serve users, it is required to share the user-related data among researchers, advertisers and application developers. Publishing such data would raise more concerns on user privacy. To encourage data sharing and mitigate user privacy concerns, a number of anonymization and de-anonymization algorithms have been developed to help protect privacy of social media users. In this work, we propose a new adversarial attack specialized for social media data. We further provide a principled way to assess effectiveness of anonymizing different aspects of social media data. Our work sheds light on new privacy risks in social media data due to innate heterogeneity of user-generated data which require striking balance between sharing user data and protecting user privacy.", "num_citations": "24\n", "authors": ["1157"]}
{"title": "Using a random forest to inspire a neural network and improving on it\n", "abstract": " Neural networks have become very popular in recent years because of the astonishing success of deep learning in various domains such as image and speech recognition. In many of these domains, specific architectures of neural networks, such as convolutional networks, seem to fit the particular structure of the problem domain very well, and can therefore perform in an astonishingly effective way. However, the success of neural networks is not universal across all domains. Indeed, for learning problems without any special structure, or in cases where the data is somewhat limited, neural networks are known not to perform well with respect to traditional machine learning methods such as random forests. In this paper, we show that a carefully designed neural network with random forest structure can have better generalization ability. In fact, this architecture is more powerful than random forests, because the back\u00a0\u2026", "num_citations": "24\n", "authors": ["1157"]}
{"title": "User vulnerability and its reduction on a social networking site\n", "abstract": " Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user\u2019s social network, privacy settings alone are often inadequate to protect a user\u2019s profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual users\u2019 vulnerability? (2) With the diversity of one\u2019s social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual user\u2019s perspective. User vulnerability is dependent on whether or not the user\u2019s friends\u2019 privacy settings protect the friend and the individual\u2019s network of\u00a0\u2026", "num_citations": "24\n", "authors": ["1157"]}
{"title": "Users joining multiple sites: Distributions and patterns\n", "abstract": " The rise of social media has led to an explosion in the number of possible sites users can join. However, this same profusion of social media sites has made it nearly impossible for users to actively engage in all of them simultaneously. Accordingly, users must make choices about which sites to use or to neglect. In this paper, we study users that have joined multiple sites. We study how individuals are distributed across sites, the way they select sites to join, and behavioral patterns they exhibit while selecting sites. Our study demonstrates that while users have a tendency to join the most popular or trendiest sites, this does not fully explain users' selections. We demonstrate that peer pressure also influences the decisions users make about joining emerging sites.", "num_citations": "24\n", "authors": ["1157"]}
{"title": "From tweets to events: exploring a scalable solution for twitter streams\n", "abstract": " The unprecedented use of social media through smartphones and other web-enabled mobile devices has enabled the rapid adoption of platforms like Twitter. Event detection has found many applications on the web, including breaking news identification and summarization. The recent increase in the usage of Twitter during crises has attracted researchers to focus on detecting events in tweets. However, current solutions have focused on static Twitter data. The necessity to detect events in a streaming environment during fast paced events such as a crisis presents new opportunities and challenges. In this paper, we investigate event detection in the context of real-time Twitter streams as observed in real-world crises. We highlight the key challenges in this problem: the informal nature of text, and the high volume and high velocity characteristics of Twitter streams. We present a novel approach to address these challenges using single-pass clustering and the compression distance to efficiently detect events in Twitter streams. Through experiments on large Twitter datasets, we demonstrate that the proposed framework is able to detect events in near real-time and can scale to large and noisy Twitter streams.", "num_citations": "24\n", "authors": ["1157"]}
{"title": "Social computing and behavioral modeling\n", "abstract": " Social computing is concerned with the study of social behavior and social c-text based on computational systems. Behavioral modeling reproduces the social behavior, and allows for experimenting, scenario planning, and deep understa-ing of behavior, patterns, and potential outcomes. The pervasive use of computer and Internet technologies provides an unprecedented environment of various-cial activities. Social computing facilitates behavioral modeling in model building, analysis, pattern mining, and prediction. Numerous interdisciplinary and inter-pendent systems are created and used to represent the various social and physical systems for investigating the interactions between groups, communities, or nati-states. This requires joint efforts to take advantage of the state-of-the-art research from multiple disciplines, social computing, and behavioral modeling in order to document lessons learned and develop novel theories, experiments, and methodo-gies in terms of social, physical, psychological, and governmental mechanisms. The goal is to enable us to experiment, create, and recreate an operational environment with a better understanding of the contributions from each individual discipline, forging joint interdisciplinary efforts. This is the second international workshop on Social Computing, Behavioral ModelingandPrediction. The submissions were from Asia, Australia, Europe, and America. Since SBP09 is a single-track workshop, we could not accept all the good submissions. The accepted papers cover a wide range of interesting topics.", "num_citations": "24\n", "authors": ["1157"]}
{"title": "Leveraging multi-source weak social supervision for early detection of fake news\n", "abstract": " Social media has greatly enabled people to participate in online activities at an unprecedented rate. However, this unrestricted access also exacerbates the spread of misinformation and fake news online which might cause confusion and chaos unless being detected early for its mitigation. Given the rapidly evolving nature of news events and the limited amount of annotated data, state-of-the-art systems on fake news detection face challenges due to the lack of large numbers of annotated training instances that are hard to come by for early detection. In this work, we exploit multiple weak signals from different sources given by user and content engagements (referred to as weak social supervision), and their complementary utilities to detect fake news. We jointly leverage the limited amount of clean data along with weak signals from social engagements to train deep neural networks in a meta-learning framework to estimate the quality of different weak instances. Experiments on realworld datasets demonstrate that the proposed framework outperforms state-of-the-art baselines for early detection of fake news without using any user engagements at prediction time.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Compounds identification in semen cuscutae by ultra-high-performance liquid chromatography (UPLCs) coupled to electrospray ionization mass spectrometry\n", "abstract": " Semen Cuscutae is commonly used in traditional Chinese medicine and contains a series of compounds such as flavonoids, chlorogenic acids and lignans. In this study, we identified different kinds of compositions by ultra-high-performance liquid chromatography (UPLC) coupled to electrospray ionization mass spectrometry (MS). A total of 45 compounds were observed, including 20 chlorogenic acids, 23 flavonoids and 2 lignans. 23 of them are reported for the first time including 6-O-caffeoyl-\u03b2-glucose, 3-O-(4\u2032-O-Caffeoylglucosyl) quinic acid, etc. Their structures were established by retention behavior, extensive analyses of their MS spectra and further determined by comparison of their MS data with those reported in the literature. As chlorogenic acids and flavonoids are phenolic compounds that are predominant in Semen Cuscutae, in conclusion, phenolic compounds are the major constituents of Semen Cuscutae. View Full-Text", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Unsupervised personalized feature selection\n", "abstract": " Feature selection is effective in preparing high-dimensional data for a variety of learning tasks such as classification, clustering and anomaly detection. A vast majority of existing feature selection methods assume that all instances share some common patterns manifested in a subset of shared features. However, this assumption is not necessarily true in many domains where data instances could show high individuality. For example, in the medical domain, we need to capture the heterogeneous nature of patients for personalized predictive modeling, which could be characterized by a subset of instance-specific features. Motivated by this, we propose to study a novel problem of personalized feature selection. In particular, we investigate the problem in an unsupervised scenario as label information is usually hard to obtain in practice. To be specific, we present a novel unsupervised personalized feature selection framework UPFS to find some shared features by all instances and instance-specific features tailored to each instance. We formulate the problem into a principled optimization framework and provide an effective algorithm to solve it. Experimental results on real-world datasets verify the effectiveness of the proposed UPFS framework.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Can one tamper with the sample api? Toward neutralizing bias from spam and bot content\n", "abstract": " While social media mining continues to be an active area of research, obtaining data for research is a perennial problem. Even more, obtaining unbiased data is a challenge for researchers who wish to study human behavior, and not technical artifacts induced by the sampling algorithm of a social media site. In this work, we evaluate one social media data outlet that gives data to its users in the form of a stream: Twitter's Sample API. We show that in its current form, this API can be poisoned by bots or spammers who wish to promote their content, jeopardizing the credibility of the data collected through this API. We design a proof-of-concept algorithm that shows how malicious users could increase the probability of their content appearing in the Sample API, thus biasing the content towards spam and bot content and harming the representativity of this data outlet.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Seeking provenance of information using social media\n", "abstract": " Social media propagates breaking news and disinformation alike fast and on an unsurpassed scale. Because of its democratizing nature, social media users can easily produce, receive, and propagate a piece of information without necessarily providing traceable information. Thus, there are no means for a user to verify the provenance (aka sources or originators) of information. The disinformation can cause tragic consequences to society and individuals. This work aims to take advantage of characteristics of social media to provide a solution to the problem of lacking traceable information. Such knowledge can provide additional context to received information such that a user can assess how much value, trust, and validity should be placed in it. In this paper, we are studying a novel research problem that facilitates the seeking of the provenance of information for a few known recipients (less than 1% of the total\u00a0\u2026", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Social status and role analysis of palin's email network\n", "abstract": " Email usage is pervasive among people from different backgrounds, and email corpus can be an important data source to study intricate social structures. Social status and role analysis on a personal email network can help reveal hidden information. The availability of Sarah Palin's email corpus presents a great opportunity to study social statuses and social roles in an email network. However, the email corpus does not readily lend itself to social network analysis due to problems such as noisy email data, scale in size, and temporal constraints. In this paper, we report an initial investigation of social status and role analysis on Sarah Palin's email corpus. In particular, we conduct a preliminary study on Palin's social statuses and roles. To the best of our knowledge, this work is the first exploration of Sarah Palin's email corpus recently released by the state of Alaska.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Blogtrackers: A tool for sociologists to track and analyze blogosphere\n", "abstract": " We present a tool, BlogTrackers, which helps sociologists to track and analyze blogs of particular interests by designing and integrating unique features. We present an overview of BlogTrackers, illustrate its functions of various components of BlogTrackers, and outline future work for expansion in meeting the growing needs of sociologists.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "A subspace clustering framework for research group collaboration\n", "abstract": " Researchers spend considerable time searching for relevant papers on the topic in which they are currently interested. Often, despite having similar interests, researchers in the same laboratory do not find it convenient to share results of bibliographic searches and thus conduct independent time-consuming searches. Research paper recommender systems can help the researcher avoid such time-consuming searches by allowing each researcher to automatically take advantage of previous searches performed by others in the lab. Existing recommender systems were developed for commercial domains to assist users by focusing toward products of their interests. Unlike those domains, the research paper domain has relatively few users when compared with the significantly larger number of research papers. In this paper, we present a novel system to recommend relevant research papers to a user based on the\u00a0\u2026", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Sensor selection for maneuver classification\n", "abstract": " To determine when to present information from various devices or services to the driver of an automobile, it is necessary to determine whether a driver is engaged in a difficult driving situation that requires extensive attention. We present simulator experiments in determining which sensors make the classification of driving states into such maneuvers possible, using various machine learning techniques. Our findings indicate that a small number of derived sensor signals can accomplish the task.", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Efficient hierarchical clustering algorithms using partially overlapping partitions\n", "abstract": " Clustering is an important data exploration task. A prominent clustering algorithm is agglomerative hierarchical clustering. Roughly, in each iteration, it merges the closest pair of clusters. It was first proposed way back in 1951, and since then there have been numerous modifications. Some of its good features are: a natural, simple, and non-parametric grouping of similar objects which is capable of finding clusters of different shape such as spherical and arbitrary. But large CPU time and high memory requirement limit its use for large data. In this paper we show that geometric metric (centroid, median, and minimum variance) algorithms obey a 90-10 relationship where roughly the first 90iterations are spent on merging clusters with distance less than 10the maximum merging distance. This characteristic is exploited by partially overlapping partitioning. It is shown with experiments and analyses that different\u00a0\u2026", "num_citations": "23\n", "authors": ["1157"]}
{"title": "Graph prototypical networks for few-shot learning on attributed networks\n", "abstract": " Attributed networks nowadays are ubiquitous in a myriad of high-impact applications, such as social network analysis, financial fraud detection, and drug discovery. As a central analytical task on attributed networks, node classification has received much attention in the research community. In real-world attributed networks, a large portion of node classes only contains limited labeled instances, rendering a long-tail node class distribution. Existing node classification algorithms are unequipped to handle the few-shot node classes. As a remedy, few-shot learning has attracted a surge of attention in the research community. Yet, few-shot node classification remains a challenging problem as we need to address the following questions:(i) How to extract meta-knowledge from an attributed network for few-shot node classification?(ii) How to identify the informativeness of each labeled instance for building a robust and\u00a0\u2026", "num_citations": "22\n", "authors": ["1157"]}
{"title": "Privacy-aware recommendation with private-attribute protection using adversarial learning\n", "abstract": " Recommendation is one of the critical applications that helps users find information relevant to their interests. However, a malicious attacker can infer users' private information via recommendations. Prior work obfuscates user-item data before sharing it with recommendation system. This approach does not explicitly address the quality of recommendation while performing data obfuscation. Moreover, it cannot protect users against private-attribute inference attacks based on recommendations. This work is the first attempt to build a Recommendation with Attribute Protection (RAP) model which simultaneously recommends relevant items and counters private-attribute inference attacks. The key idea of our approach is to formulate this problem as an adversarial learning problem with two main components: the private attribute inference attacker, and the Bayesian personalized recommender. The attacker seeks to infer\u00a0\u2026", "num_citations": "22\n", "authors": ["1157"]}
{"title": "Paired restricted boltzmann machine for linked data\n", "abstract": " Restricted Boltzmann Machines (RBMs) are widely adopted unsupervised representation learning methods and have powered many data mining tasks such as collaborative filtering and document representation. Recently, linked data that contains both attribute and link information has become ubiquitous in various domains. For example, social media data is inherently linked via social relations and web data is networked via hyperlinks. It is evident from recent work that link information can enhance a number of real-world applications such as clustering and recommendations. Therefore, link information has the potential to advance RBMs for better representation learning. However, the majority of existing RBMs have been designed for independent and identically distributed data and are unequipped for linked data. In this paper, we aim to design a new type of Restricted Boltzmann Machines that takes advantage of\u00a0\u2026", "num_citations": "22\n", "authors": ["1157"]}
{"title": "Trust evolution: Modeling and its applications\n", "abstract": " Trust plays a crucial role in helping online users collect reliable information and it has gained increasing attention from the computer science community in recent years. Traditionally, research about online trust assumes static trust relations between users. However, trust, as a social concept, evolves as people interact. Most existing studies about trust evolution are from sociologists in the physical world while little work exists in an online world. Studying online trust evolution faces unique challenges because more often than not, available data is from passive observation. In this work, we leverage social science theories to develop a methodology that enables the study of online trust evolution. In particular, we identify the differences of trust evolution study in physical and online worlds and propose a framework, eTrust, to study trust evolution using online data from passive observation in the context of product review\u00a0\u2026", "num_citations": "22\n", "authors": ["1157"]}
{"title": "Evaluating the trustworthiness of Wikipedia articles through quality and credibility\n", "abstract": " Wikipedia has become a very popular destination for Web surfers seeking knowledge about a wide variety of subjects. While it contains many helpful articles with accurate information, it also consists of unreliable articles with inaccurate or incomplete information. A casual observer might not be able to differentiate between the good and the bad. In this work, we identify the necessity and challenges for trust assessment in Wikipedia, and propose a framework that can help address these challenges by identifying relevant features and providing empirical means to meet the requirements for such an evaluation. We select relevant variables and perform experiments to evaluate our approach. The results demonstrate promising performance that is better than comparable approaches and could possibly be replicated with other social media applications.", "num_citations": "22\n", "authors": ["1157"]}
{"title": "Deep reinforcement learning-based text anonymization against private-attribute inference\n", "abstract": " User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User\u2019s privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text wrt a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations wrt the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Adaptive unsupervised feature selection on attributed networks\n", "abstract": " Attributed networks are pervasive in numerous of high-impact domains. As opposed to conventional plain networks where only pairwise node dependencies are observed, both the network topology and node attribute information are readily available on attributed networks. More often than not, the nodal attributes are depicted in a high-dimensional feature space and are therefore notoriously difficult to tackle due to the curse of dimensionality. Additionally, features that are irrelevant to the network structure could hinder the discovery of actionable patterns from attributed networks. Hence, it is important to leverage feature selection to find a high-quality feature subset that is tightly correlated to the network structure. Few of the existing efforts either model the network structure at a macro-level by community analysis or directly make use of the binary relations. Consequently, they fail to exploit the finer-grained tie strength\u00a0\u2026", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Toward personalized relational learning\n", "abstract": " Relational learning exploits relationships among instances manifested in a network to improve the predictive performance of many network mining tasks. Due to its empirical success, it has been widely applied in myriad domains. In many cases, individuals in a network are highly idiosyncratic. They not only connect to each other with a composite of factors but also are often described by some content information of high dimensionality specific to each individual. For example in social media, as user interests are quite diverse and personal; posts by different users could differ significantly. Moreover, social content of users is often of high dimensionality which may negatively degrade the learning performance. Therefore, it would be more appealing to tailor the prediction for each individual while alleviating the issue related to the curse of dimensionality. In this paper, we study a novel problem of Personalized R\u00a0\u2026", "num_citations": "21\n", "authors": ["1157"]}
{"title": "NK cell activating receptor ligand expression in lymphangioleiomyomatosis is associated with lung function decline\n", "abstract": " Lymphangioleiomyomatosis (LAM) is a rare lung disease of women that leads to progressive cyst formation and accelerated loss of pulmonary function. Neoplastic smooth muscle cells from an unknown source metastasize to the lung and drive destructive remodeling. Given the role of NK cells in immune surveillance, we postulated that NK cell activating receptors and their cognate ligands are involved in LAM pathogenesis. We found that ligands for the NKG2D activating receptor UL-16 binding protein 2 (ULBP2) and ULBP3 are localized in cystic LAM lesions and pulmonary nodules. We found elevated soluble serum ULBP2 (mean= 575 pg/ml\u00b1142) in 50 of 100 subjects and ULBP3 in 30 of 100 (mean= 8,300 pg/ml\u00b11,515) subjects. LAM patients had fewer circulating NKG2D+ NK cells and decreased NKG2D surface expression. Lung function decline was associated with soluble NKG2D ligand (sNKG2DL\u00a0\u2026", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Directed dynamical influence is more detectable with noise\n", "abstract": " Successful identification of directed dynamical influence in complex systems is relevant to significant problems of current interest. Traditional methods based on Granger causality and transfer entropy have issues such as difficulty with nonlinearity and large data requirement. Recently a framework based on nonlinear dynamical analysis was proposed to overcome these difficulties. We find, surprisingly, that noise can counterintuitively enhance the detectability of directed dynamical influence. In fact, intentionally injecting a proper amount of asymmetric noise into the available time series has the unexpected benefit of dramatically increasing confidence in ascertaining the directed dynamical influence in the underlying system. This result is established based on both real data and model time series from nonlinear ecosystems. We develop a physical understanding of the beneficial role of noise in enhancing detection\u00a0\u2026", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Toward dual roles of users in recommender systems\n", "abstract": " Users usually play dual roles in real-world recommender systems. One is as a reviewer who writes reviews for items with rating scores, and the other is as a rater who rates the helpfulness scores of reviews. Traditional recommender systems mainly consider the reviewer role while not taking into account the rater role. However, the rater role allows users to express their opinions toward reviews about items; hence it may indirectly indicate their opinions about items, which could be complementary to the reviewer role. Since most real-world recommender systems provide convenient mechanisms for the rater role, recent studies show that typically there are much more helpfulness ratings from the rater role than item ratings from the reviewer role. Therefore, incorporating the rater role of users may have the potentials to mitigate the data sparsity and cold-start problems in traditional recommender systems. In this paper\u00a0\u2026", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Personalized location recommendation on location-based social networks\n", "abstract": " Personalized location recommendation is a special topic of recommendation. It is related to human mobile behavior in the real world regarding various contexts including spatial, temporal, social, and content. The development of this topic is subject to the availability of human mobile data. The recent rapid growth of location-based social networks has alleviated such limitation, which promotes the development of various location recommendation techniques. This tutorial offers an overview, in a data mining perspective, of personalized location recommendation on location-based social networks. It introduces basic concepts, summarizes unique LBSN characteristics and research opportunities, elaborates associated challenges, reviews state-of-the-art algorithms with illustrative examples and real-world LBSN datasets, and discusses effective evaluation methods.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Trust-aware recommender systems\n", "abstract": " Recommender systems are an effective solution to the information overload problem, specially in the online world where we are constantly faced with inordinately many choices. These systems try to find the items such as books or movies that match best with users\u2019 preferences. Based on the different approaches to finding the items of interests to users, we can classify the recommender systems into three major groups. First, content based recommender systems use content information to make a recommendation. For example, such systems might recommend a romantic movie to a user that showed interest in romantic movies in her profile. Second, collaborative filtering recommender systems rely only on the past behavior of the users such as their previous transactions or ratings. By comparing this information, a collaborative filtering recommender system finds new items or users to users. In order to address the cold-start problem and fend off various types of attacks, the third class of recommender systems, namely trust-aware recommender systems, is proposed. These systems use social media and trust information to make a recommendation, which is shown to be promising in improving the accuracy of the recommendations. In this chapter, we give an overview of state-of-theart recommender systems with a focus on trust-aware recommender systems. In particular, we describe the ways that trust information can help to improve the quality of the recommendations. In the rest of the chapter, we introduce recommender systems, then trust in social media, and next trust-aware recommender systems.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "An empirical study of building compact ensembles\n", "abstract": " Ensemble methods can achieve excellent performance relying on member classifiers\u2019 accuracy and diversity. We conduct an empirical study of the relationship of ensemble sizes with ensemble accuracy and diversity, respectively. Experiments with benchmark data sets show that it is feasible to keep a small ensemble while maintaining accuracy and diversity similar to those of a full ensemble. We propose a heuristic method that can effectively select member classifiers to form a compact ensemble. The idea of compact ensembles is motivated to use them for effective active learning in tasks of classification of unlabeled data.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Data reduction: feature selection\n", "abstract": " Feature selection is introduced as a search problem that consists of feature subset generation, evaluation, and selection. The purpose of feature selection is three-fold: reducing the number of features, improving classification accuracy, and simplifying the learned representation. We review major evaluation measures and various feature selection approaches, list some existing methods, and show by example the role of feature selection in data mining.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Discretization of ordinal attributes and feature selection\n", "abstract": " The performance of classification algorithms may deteriorate due to irrelevant attributes. Numeric attributes make the situation worse, since many classification algorithms require that the training data contain only discrete attributes. Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2, a simple and general algorithm that uses the  statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. In addition, it can handle mixed attributes and multi-class data naturally. %data, and achieves feature selection and discretization in one go. Experiments are conducted on the real and synthetic data sets. The empirical results demonstrate that Chi2 is effective in feature selection and discretization of numeric and ordinal attributes.", "num_citations": "21\n", "authors": ["1157"]}
{"title": "Combating disinformation in a social media age\n", "abstract": " The creation, dissemination, and consumption of disinformation and fabricated content on social media is a growing concern, especially with the ease of access to such sources, and the lack of awareness of the existence of such false information. In this article, we present an overview of the techniques explored to date for the combating of disinformation with various forms. We introduce different forms of disinformation, discuss factors related to the spread of disinformation, elaborate on the inherent challenges in detecting disinformation, and show some approaches to mitigating disinformation via education, research, and collaboration. Looking ahead, we present some promising future research directions on disinformation. This article is categorized under: Algorithmic Development > Multimedia Commercial, Legal, and Ethical Issues > Social Considerations Application Areas > Education and Learning", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Early detection of rumours on twitter via stance transfer learning\n", "abstract": " Rumour detection on Twitter is an important problem. Existing studies mainly focus on high detection accuracy, which often requires large volumes of data on contents, source credibility or propagation. In this paper we focus on early detection of rumours when data for information sources or propagation is scarce. We observe that tweets attract immediate comments from the public who often express uncertain and questioning attitudes towards rumour tweets. We therefore propose to learn user attitude distribution for Twitter posts from their comments, and then combine it with content analysis for early detection of rumours. Specifically we propose convolutional neural network (CNN) CNN and BERT neural network language models to learn attitude representation for user comments without human annotation via transfer learning based on external data sources for stance classification. We further propose CNN\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Learning individual causal effects from networked observational data\n", "abstract": " The convenient access to observational data enables us to learn causal effects without randomized experiments. This research direction draws increasing attention in research areas such as economics, healthcare, and education. For example, we can study how a medicine (the treatment) causally affects the health condition (the outcome) of a patient using existing electronic health records. To validate causal effects learned from observational data, we have to control confounding bias--the influence of variables which causally influence both the treatment and the outcome. Existing work along this line overwhelmingly relies on the unconfoundedness assumption that there do not exist unobserved confounders. However, this assumption is untestable and can even be untenable. In fact, an important fact ignored by the majority of previous work is that observational data can come with network information that can be\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "#suicidal-A multipronged approach to identify and explore suicidal ideation in twitter\n", "abstract": " Technological advancements have led to the creation of social media platforms like Twitter, where people have started voicing their views over rarely discussed and socially stigmatizing issues. Twitter, is increasingly being used for studying psycho-linguistic phenomenon spanning from expressions of adverse drug reactions, depressions, to suicidality. In this work we focus on identifying suicidal posts from Twitter. Towards this objective we take a multipronged approach and implement different neural network models such assequential models andgraph convolutional networks, that are trained on textual content shared in Twitter, the historical tweeting activity of the users and social network formed between different users posting about suicidality. We train a stacked ensemble of classifiers representing different aspects of suicidal tweeting activity, and achieve state-of-the-art results on a new manually annotated\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "\" Bridge\" Enhanced Signed Directed Network Embedding\n", "abstract": " Signed directed networks with positive or negative links convey rich information such as like or dislike, trust or distrust. Existing work of sign prediction mainly focuses on triangles (triadic nodes) motivated by balance theory to predict positive and negative links. However, real-world signed directed networks can contain a good number of\" bridge''edges which, by definition, are not included in any triangles. Such edges are ignored in previous work, but may play an important role in signed directed network analysis.% Such edges serve as fundamental building blocks and may play an important role in signed network analysis.", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Exploiting emotion on reviews for recommender systems\n", "abstract": " Review history is widely used by recommender systems to infer users' preferences and help find the potential interests from the huge volumes of data, whereas it also brings in great concerns on the sparsity and cold-start problems due to its inadequacy. Psychology and sociology research has shown that emotion information is a strong indicator for users' preferences. Meanwhile, with the fast development of online services, users are willing to express their emotion on others' reviews, which makes the emotion information pervasively available. Besides, recent research shows that the number of emotion on reviews is always much larger than the number of reviews. Therefore incorporating emotion on reviews may help to alleviate the data sparsity and cold-start problems for recommender systems. In this paper, we provide a principled and mathematical way to exploit both positive and negative emotion on reviews, and propose a novel framework MIRROR, exploiting eMotIon on Reviews for RecOmmendeR systems from both global and local perspectives. Empirical results on real-world datasets demonstrate the effectiveness of our proposed framework and further experiments are conducted to understand how emotion on reviews works for the proposed framework.", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Propagation-based sentiment analysis for microblogging data\n", "abstract": " The explosive popularity of microblogging services encourages more and more online users to share their opinions, and sentiment analysis on such opinion-rich resources has been proven to be an effective way to understand public opinions. On the one hand, the brevity and informality of microblogging data plus its wide variety and rapid evolution of language in microblogging pose new challenges to the vast majority of existing methods. On the other hand, microblogging texts contain various types of emotional signals strongly associated with their sentiment polarity, which brings about new opportunities for sentiment analysis. In this paper, we investigate propagation-based sentiment analysis for microblogging data. In particular, we provide a propagating process to incorporate various types of emotional signals in microblogging data into a coherent model, and propose a novel sentiment analysis framework PSA\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Document clustering via matrix representation\n", "abstract": " Vector Space Model (VSM) is widely used to represent documents and web pages. It is simple and easy to deal computationally, but it also oversimplifies a document into a vector, susceptible to noise, and cannot explicitly represent underlying topics of a document. A matrix representation of document is proposed in this paper: rows represent distinct terms and columns represent cohesive segments. The matrix model views a document as a set of segments, and each segment is a probability distribution over a limited number of latent topics which can be mapped to clustering structures. The latent topic extraction based on the matrix representation of documents is formulated as a constraint optimization problem in which each matrix (i.e., a document) A i  is factorized into a common base determined by non-negative matrices L and R T , and a non-negative weight matrix M i  such that the sum of reconstruction error on\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Socialtagger-collaborative tagging for blogs in the long tail\n", "abstract": " Social bookmarking is the process through which users share tags for online resources like blogs with others. Such collaborative tags provide valuable metadata for retrieval systems. While the successes of collaborative tagging systems have been demonstrated by popular websites like Del. icio. us, these sites cover only a small fraction of the available blogs on the web. The vast majority of the blogs are not available on any collaborative tagging system and are often tagged only by the authors. This lack of coverage of collaborative tags is a considerable roadblock in using the tag metadata in a web scale information retrieval system. To solve this problem we propose and implement a system to automatically recommend collaborative tags for a blog. The automatically generated tags will help to surface the blogs by making them available on social book marking sites and allow them to be easily discovered and\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Searching for familiar strangers on blogosphere: problems and challenges\n", "abstract": " In this work, we examine familiar strangers on Blogosphere and issues of finding them. In our daily life, familiar strangers, as coined by Stanley Milgram, do not know each other, but frequently exhibit some common patterns. Blogosphere is a part of the Web where bloggers post in individual or community blog sites. The nature of the Web is a scale-free network, which determines that a power law distribution applies to bloggers. That is, the majority bloggers are only connected with a small number of fellow bloggers, and these blogging groups are largely disconnected from each other. Familiar strangers on Blogosphere are not directly connected, but share some patterns in their blogging activities. We present a new problem: Aggregating familiar strangers on Blogosphere that allows for better personalized services, targeted marketing, exploration of new business opportunities, and predictive modeling and marketing. Finding familiar strangers on Blogosphere presents a challenge resulting from their disconnectedness. We look at typical blogs and understand the status quo, while seeking innovative ways to improve business intelligence. We define the problem of searching for familiar strangers on Blogosphere, elucidate the significance of doing so, study the challenges of finding them, and present and discuss some potential approaches.", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Feature transformation and multivariate decision tree induction\n", "abstract": " Univariate decision trees (UDT\u2019s) have inherent problems of replication, repetition, and fragmentation. Multivariate decision trees (MDT\u2019s) have been proposed to overcome some of the problems. Close examination of the conventional ways of building MDT\u2019s, however, reveals that the fragmentation problem still persists. A novel approach is suggested to minimize the fragmentation problem by separating hyperplane search from decision tree building. This is achieved by feature transformation. Let the initial feature vector be x, the new feature vector after feature transformation T is y, i.e., y = T(x). We can obtain an MDTb y (1) building a UDT on y; and (2) replacing new features y at each node with the combinations of initial features x. We elaborate on the advantages of this approach, the details of T, and why it is expected to perform well. Experiments are conducted in order to confirm the analysis, and results\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "Feature extraction via neural networks\n", "abstract": " A method for feature extraction which makes use of feedforward neural networks with a single hidden layer is presented. The topology of the networks is determined by a network construction algorithm and a network pruning algorithm. Network construction is achieved by having just 1 hidden unit initially; additional units are added only when they are needed to improve the network predictive accuracy. Once a fully connected network has been constructed, irrelevant/redundant network connections are removed by pruning. The hidden unit activations of the pruned network are the features extracted from the original dataset. Using artificial datasets, we illustrate how the method works and interpret the extracted features in terms of the original attributes of the datasets. We also discuss how the feature extraction method can be used in conjunction with other learning algorithms such as decision tree methods to\u00a0\u2026", "num_citations": "20\n", "authors": ["1157"]}
{"title": "defend: A system for explainable fake news detection\n", "abstract": " Despite recent advancements in computationally detecting fake news, we argue that a critical missing piece be the explainability of such detection--ie, why a particular piece of news is detected as fake--and propose to exploit rich information in users' comments on social media to infer the authenticity of news. In this demo paper, we present our system for an explainable fake news detection called dEFEND, which can detect the authenticity of a piece of news while identifying user comments that can explain why the news is fake or real. Our solution develops a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. The system is publicly accessible.", "num_citations": "19\n", "authors": ["1157"]}
{"title": "Uncovering deception in social media\n", "abstract": " Social media is quickly arising as a new, popular form of media. Facebook, Twitter, and LinkedIn are some examples of an inordinate number of social media services that are loved and used by people of all walks of life for various purposes such as sharing news, expressing opinions, documenting thoughts, launching political campaigns, maintaining and developing friendships or professional connection. Some key characteristics of social media include low entry barrier, instant updates (thus, instant gratification), large numbers of friends, open platform, and anonymity. The last two properties make people comfortable to become users but also make social media vulnerable to activities of ill intentions. Deception in social media is an epitome of such activities. Deception is a distortion with an intention to mislead users, analysts, organizations, etc. A distortion can be about content, source, identity, age, sex, or location\u00a0\u2026", "num_citations": "19\n", "authors": ["1157"]}
{"title": "Information provenance in social media\n", "abstract": " Information appearing in social media provides a challenge for determining the provenance of the information. However, the same characteristics that make the social media environment challenging provide unique and untapped opportunities for solving the information provenance problem for social media. Current approaches for tracking provenance information do not scale for social media and consequently there is a gap in provenance methodologies and technologies providing exciting research opportunities for computer scientists and sociologists. This paper introduces a theoretical approach aimed guiding future efforts to realize a provenance capability for social media that is not available today. The guiding vision is the use of social media information itself to realize a useful amount provenance data for information in social media.", "num_citations": "19\n", "authors": ["1157"]}
{"title": "A multi-resolution approach to learning with overlapping communities\n", "abstract": " The recent few years have witnessed a rapid surge of participatory web and social media, enabling a new laboratory for studying human relations and collective behavior on an unprecedented scale. In this work, we attempt to harness the predictive power of social connections to determine the preferences or behaviors of individuals such as whether a user supports a certain political view, whether one likes one product, whether he/she would like to vote for a presidential candidate, etc. Since an actor is likely to participate in multiple different communities with each regulating the actor's behavior in varying degrees, and a natural hierarchy might exist between these communities, we propose to zoom into a network at multiple different resolutions and determine which communities are informative of a targeted behavior. We develop an efficient algorithm to extract a hierarchy of overlapping communities. Empirical\u00a0\u2026", "num_citations": "19\n", "authors": ["1157"]}
{"title": "Guest editors' introduction: Social computing in the blogosphere\n", "abstract": " The widespread phenomenon of blogging demonstrates the power of citizen journalism and anytime information sharing. People can exchange personal experiences, voice opinions, offer suggestions, and form groups with genuine social activities. Blogs also act as conduits, propagating data at an unprecedented pace that has led to a gigantic and dynamic open source data archive as well as a unique opportunity for various research activities studying influence, trust, reputation, privacy, search, spam, and group interaction. An important challenge lies in modeling and mining this vast pool of data. Social computing is an emerging interdisciplinary field and offers unique opportunities for developing novel algorithms and tools, such as text and content mining, and graph and link mining. An associated challenge is data collection and objective evaluation: How can we effectively collect data and share it for research\u00a0\u2026", "num_citations": "19\n", "authors": ["1157"]}
{"title": "Graph neural networks for user identity linkage\n", "abstract": " The increasing popularity and diversity of social media sites has encouraged more and more people to participate in multiple online social networks to enjoy their services. Each user may create a user identity to represent his or her unique public figure in every social network. User identity linkage across online social networks is an emerging task and has attracted increasing attention, which could potentially impact various domains such as recommendations and link predictions. The majority of existing work focuses on mining network proximity or user profile data for discovering user identity linkages. With the recent advancements in graph neural networks (GNNs), it provides great potential to advance user identity linkage since users are connected in social graphs, and learning latent factors of users and items is the key. However, predicting user identity linkages based on GNNs faces challenges. For example, the user social graphs encode both \\textit{local} structure such as users' neighborhood signals, and \\textit{global} structure with community properties. To address these challenges simultaneously, in this paper, we present a novel graph neural network framework ({\\m}) for user identity linkage. In particular, we provide a principled approach to jointly capture local and global information in the user-user social graph and propose the framework {\\m}, which jointly learning user representations for user identity linkage. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework.", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Linked causal variational autoencoder for inferring paired spillover effects\n", "abstract": " Modeling spillover effects from observational data is an important problem in economics, business, and other fields of research. It helps us infer the causality between two seemingly unrelated set of events. For example, if consumer spending in the United States declines, it has spillover effects on economies that depend on the US as their largest export market. In this paper, we aim to infer the causation that results in spillover effects between pairs of entities (or units); we call this effect as paired spillover. To achieve this, we leverage the recent developments in variational inference and deep learning techniques to propose a generative model called Linked Causal Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA incorporates an encoder neural network to learn the latent attributes and a decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the latent attributes\u00a0\u2026", "num_citations": "18\n", "authors": ["1157"]}
{"title": "# metoo through the lens of social media\n", "abstract": " Sexual abuse \u2013 a highly stigmatized topic in the society has spurred a revolution in the recent days especially through the shared posts on social media platforms via attaching the hashtag #metoo. Individuals from different backgrounds and ethnicities began sharing on the online venues about their personal experiences of getting sexually assaulted. This paper makes an initial attempt to asses the public reactions and emotions by utilizing the publicly shared #metoo posts by performing a comparative analysis of the tweets shared on Twitter as well as on Reddit. Though nearly equal ratios of negative and positive posts are shared on both platforms, Reddit posts are focused on the sexual assaults within families and workplaces while Twitter posts are on showing empathy and encouraging others to continue the #metoo movement. The data collected in this research helps in the preliminary analysis of the\u00a0\u2026", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Chemical comparison of two drying methods of mountain cultivated ginseng by UPLC-QTOF-MS/MS and multivariate statistical analysis\n", "abstract": " In traditional Chinese medicine practice, drying method is an essential factor to influence the components of Chinese medicinal herbs. In this study, an ultra-performance liquid chromatography quadrupole time-of-flight tandem mass spectrometry (UPLC-QTOF-MS/MS)-based approach was used to compare the content of chemical compounds of mountain cultivated ginseng that had been natural air dried (LX-P) and vacuum freeze-dried (LX-L). Multivariate statistical analysis such as principal component analysis (PCA) and supervised orthogonal partial least squared discrimination analysis (OPLS-DA) were used to select the influential components of different samples. There were 41 ginsenosides unambiguously identified and tentatively assigned in both LX-L and LX-P. The results showed that the characteristic components in LX-P were ginsenoside Rb1, ginsenoside Rc, ginsenoside Rg6, dendrolasin, and ginsenoside Rb2. The characteristic components in LX-L were malonyl-ginsenoside Re, malonyl-ginsenoside Rb1, malonyl-ginsenoside Rc, malonyl-ginsenoside Rb1 isomer, malonyl-ginsenoside Rb2, malonyl-ginsenoside Rb3, malonyl-ginsenoside Rd isomer, gypenoside XVII, and notoginsenoside Fe. This is the first time that the differences between LX-L and LX-P have been observed systematically at the chemistry level. It was indicated that vacuum freeze-drying method can improve the content of malonyl-ginsensides in mountain cultivated ginseng. View Full-Text", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Trust in social computing\n", "abstract": " The rapid development of social media exacerbates the information overload and credibility problems. Trust, providing information about with whom we can trust to share information and from whom we can accept information, plays an important role in helping users collect relevant and reliable information in social media. Trust has become a research topic of increasing importance and of practical significance. In this tutorial, we illustrate properties and representation models of trust, elucidate trust measurements with representative algorithms, and demonstrate real-world applications where trust is explicitly used. As a new dimension of the trust study, we discuss the concept of distrust and its roles in trust measurements and applications.", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Understanding online groups through social media\n", "abstract": " Multiple fields including sociology, anthropology, and business are interested in understanding group behavior. Applying data mining techniques to social media can help provide insights into group behavior and divulge a group's characteristics by identifying a group, developing a profile for a group, revealing the sentiment of a group, and detailing a group's composition. The ability to accomplish these tasks has practical business and scientific applications such as understanding customers better and providing new insights into influence propagation, as well as the ability to accurately categorize groups over time. This paper highlights some ongoing research efforts aiming at understanding groups through social media. \u00a9 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 330\u2013338 DOI: 10.1002/widm.37 This article is categorized under:  Algorithmic Development > Web Mining Application\u00a0\u2026", "num_citations": "18\n", "authors": ["1157"]}
{"title": "An integrative approach to identifying biologically relevant genes\n", "abstract": " Gene selection aims at detecting biologically relevant genes to assist biologists' research. The cDNA Microarray data used in gene selection is usually \u201cwide\u201d. With more than several thousand genes, but only less than a hundred of samples, many biologically irrelevant genes can gain their statistical relevance by sheer randomness. Addressing this problem goes beyond what the cDNA Microarray can offer and necessitates the use of additional information. Recent developments in bioinformatics have made various knowledge sources available, such as the KEGG pathway repository and Gene Ontology database. Integrating different types of knowledge could provide more information about genes and samples. In this work, we propose a novel approach to integrate different types of knowledge for identifying biologically relevant genes. The approach converts different types of external knowledge to its internal\u00a0\u2026", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Building a generic architecture for robot hand control\n", "abstract": " As various dextrous robot hands are designed and built, a major question is how to develop device-independent robot hand controllers. This would allow the low-level control problems to be separated from high level functionality. GeSAM is a generic robot hand controller that is based on a model of human prehensile function. It focuses on the relationship between geometric object primitives and the ways a hand can perform prehensile behaviors. The authors show how the relationship between object primitives and a useful set of grasp modes can be learned by an adaptive neural network. By adding training points as necessary, system performance can be improved, avoiding the tedious job of computing every relationship by hand.< >", "num_citations": "18\n", "authors": ["1157"]}
{"title": "Towards privacy preserving social recommendation under personalized privacy settings\n", "abstract": " Privacy leakage is an important issue for social relationships-based recommender systems (i.e., social recommendation). Existing privacy preserving social recommendation approaches usually allow the recommender to fully control users\u2019 information. This may be problematic since the recommender itself may be untrusted, leading to serious privacy leakage. Besides, building social relationships requires sharing interests as well as other private information, which may lead to more privacy leakage. Although sometimes users are allowed to hide their sensitive private data using personalized privacy settings, the data being shared can still be abused by the adversaries to infer sensitive private information. Supporting social recommendation with least privacy leakage to untrusted recommender and other users (i.e., friends) is an important yet challenging problem. In this paper, we aim to achieve privacy-preserving\u00a0\u2026", "num_citations": "17\n", "authors": ["1157"]}
{"title": "INITIATOR: Noise-contrastive Estimation for Marked Temporal Point Process.\n", "abstract": " Copious sequential event data has consistently increased in various high-impact domains such as social media and sharing economy. When events start to take place in a sequential fashion, an important question arises:\u201cwhat type of event will happen at what time in the near future?\u201d To answer the question, a class of mathematical models called the marked temporal point process is often exploited as it can model the timing and properties of events seamlessly in a joint framework. Recently, various recurrent neural network (RNN) models are proposed to enhance the predictive power of mark temporal point process. However, existing marked temporal point models are fundamentally based on the Maximum Likelihood Estimation (MLE) framework for the training, and inevitably suffer from the problem resulted from the intractable likelihood function. Surprisingly, little attention has been paid to address this issue. In this work, we propose INITIATOR-a novel training framework based on noise-contrastive estimation to resolve this problem. Theoretically, we show the exists a strong connection between the proposed INITIATOR and the exact MLE. Experimentally, the efficacy of INITIATOR is demonstrated over the state-of-the-art approaches on several real-world datasets from various areas.", "num_citations": "17\n", "authors": ["1157"]}
{"title": "Detecting camouflaged content polluters\n", "abstract": " The connectivity and openness of the Internet have cultivated a blistering expansion of online media websites. However, the culture of openness also makes the emerging platforms an effective channel for content pollution, such as fraud, phishing, and other online abuses. To complicate the problem, content polluters actively manipulate the characteristics of the Internet through establishing links with normal users and blending the malicious information with legitimate content. The manipulated links and content, being used as camouflage, make it very intricate to detect content polluters. Recent work has investigated camouflaged fraud in networks. However, due to the lack of availability of label information for camouflaged content, it is challenging to detect content polluters with traditional approaches. In this paper, we make the first attempt on detecting camouflaged content polluters. In order to evaluate the proposed approach, we conduct experiments on real-world data. The results show that our method achieves better results than existing approaches.", "num_citations": "17\n", "authors": ["1157"]}
{"title": "Exploring a scalable solution to identifying events in noisy twitter streams\n", "abstract": " The unprecedented use of social media through smartphones and other web-enabled mobile devices has enabled the rapid adoption of platforms like Twitter. Event detection has found many applications on the web, including breaking news identification and summarization. The recent increase in the usage of Twitter during crises has attracted researchers to focus on detecting events in tweets. However, current solutions have focused on static Twitter data. The necessity to detect events in a streaming environment during fast paced events such as a crisis presents new opportunities and challenges. In this paper, we investigate event detection in the context of real-time Twitter streams as observed in real-world crises. We highlight the key challenges in this problem: the informal nature of text, and the high-volume and high-velocity characteristics of Twitter streams. We present a novel approach to address these\u00a0\u2026", "num_citations": "17\n", "authors": ["1157"]}
{"title": "Predictability of distrust with interaction data\n", "abstract": " Trust plays a crucial role in helping users collect reliable information in an online world, and has attracted more and more attention in research communities lately. As a conceptual counterpart of trust, distrust can be as important as trust. However, distrust is rarely studied in social media because distrust information is usually unavailable. The value of distrust has been widely recognized in social sciences and recent work shows that distrust can benefit various online applications in social media. In this work, we investigate whether we can obtain distrust information via learning when it is not directly available, and propose to study a novel problem-predicting distrust using pervasively available interaction data in an online world. In particular, we analyze interaction data, provide a principled way to mathematically incorporate interaction data in a novel framework dTrust to predict distrust information. Experimental\u00a0\u2026", "num_citations": "17\n", "authors": ["1157"]}
{"title": "A hierarchical word-merging algorithm with class separability measure\n", "abstract": " In image recognition with the bag-of-features model, a small-sized visual codebook is usually preferred to obtain a low-dimensional histogram representation and high computational efficiency. Such a visual codebook has to be discriminative enough to achieve excellent recognition performance. To create a compact and discriminative codebook, in this paper we propose to merge the visual words in a large-sized initial codebook by maximally preserving class separability. We first show that this results in a difficult optimization problem. To deal with this situation, we devise a suboptimal but very efficient hierarchical word-merging algorithm, which optimally merges two words at each level of the hierarchy. By exploiting the characteristics of the class separability measure and designing a novel indexing structure, the proposed algorithm can hierarchically merge 10,000 visual words down to two words in merely 90\u00a0\u2026", "num_citations": "17\n", "authors": ["1157"]}
{"title": "A distributed hierarchical clustering system for web mining\n", "abstract": " This paper proposes a novel method of distributed hierarchical clustering for Web mining. The method is closely related to our early work of Self-Generated Neural Networks (SGNN), which is in turn based on both self-organizing neural network and concept formation. The complexity of the algorithm is at most O(MNlogN). With the distributed implementation the method can be easily scaled up. The method is independent of the order the web documents presented. The method produces a natural conceptual hierarchy but not a binary tree. The method can include multimedia information into the same cluster hierarchy. A visualization mechanism has been developed for the clustering method and it shows the cluster hierarchy generated by the method has very high quality. The clustering process is fully automatic, and no human intervention is required. A clustering system has been built based on the\u00a0\u2026", "num_citations": "17\n", "authors": ["1157"]}
{"title": "Multi-level network embedding with boosted low-rank matrix approximation\n", "abstract": " As opposed to manual feature engineering which is tedious and difficult to scale, network embedding has attracted a surge of research interests as it automates the feature learning on graphs. The learned low-dimensional node vectors ease the knowledge discovery on graphs by enabling various off-the-shelf machine learning tools to be directly applied. Recent research has shown that the past decade of network embedding approaches either explicitly factorize a carefully designed matrix or are closely related to implicit matrix factorization, with the fundamental assumption that the factorized node connectivity matrix is low-rank. Nonetheless, the global low-rank assumption does not necessarily hold especially when the factorized matrix encodes complex node interactions, and the resultant single low-rank embedding matrix is insufficient to capture all the observed connectivity patterns. In this regard, we propose a\u00a0\u2026", "num_citations": "16\n", "authors": ["1157"]}
{"title": "PI-bully: Personalized cyberbullying detection with peer influence\n", "abstract": " Cyberbullying has become one of the most pressing online risks for adolescents and has raised serious concerns in society. Recent years have witnessed a surge in research aimed at developing principled learning models to detect cyberbullying behaviors. These efforts have primarily focused on building a single generic classification model to differentiate bullying content from normal (nonbullying) content among all users. These models treat users equally and overlook idiosyncratic information about users that might facilitate the accurate detection of cyberbullying. In this paper, we propose a personalized cyberbullying detection framework, PI-Bully, that draws on empirical findings from psychology highlighting unique characteristics of victims and bullies and peer influence from like-minded users as predictors of cyberbullying behaviors. Our framework is novel in its ability to model peer influence in a collaborative environment and tailor cyberbullying prediction for each individual user. Extensive experimental evaluations on real-world datasets corroborate the effectiveness of the proposed framework.", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Exploiting emojis for sarcasm detection\n", "abstract": " Modern social media platforms largely rely on text. However, the written text lacks the emotional cues of spoken and face-to-face dialogue, ambiguities are common, which is exacerbated in the short, informal nature of many social media posts. Sarcasm represents the nuanced form of language that individuals state the opposite of what is implied. Sarcasm detection on social media is important for users to understand the underlying messages. The majority of existing sarcasm detection algorithms focus on text information; while emotion information expressed such as emojis are ignored. In real scenarios, emojis are widely used as emotion signals, which have great potentials to advance sarcasm detection. Therefore, in this paper, we study the novel problem of exploiting emojis for sarcasm detection on social media. We propose a new framework ESD, which simultaneously captures various signals from text\u00a0\u2026", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Diagnostic value of circulating microRNAs for osteosarcoma in Asian populations: a meta-analysis\n", "abstract": " A large number of studies have provided new insights into the diagnostic value of circulating microRNAs (miRNA) for osteosarcoma (OS), one of the most common primary malignancies in adolescents. However, inconsistent conclusions on the diagnostic performance of various kinds of miRNAs have been made. To assess the true diagnostic value of circulating miRNA for the early detection of OS in this meta-analysis, multiple databases, including PubMed, EMBASE, Web of Science, Chinese National Knowledge Infrastructure\u00a0(CNKI), and Technology of Chongqing\u00a0(VIP), were carefully searched for available studies up to October 30, 2015. The quality of each study was scored using the quality assessment of diagnostic accuracy studies-2 (QUADAS-2). Sensitivity and specificity was pooled using a random-effects model. Positive likelihood ratio (PLR), negative likelihood ratio (NLR), diagnostic odds ratio\u00a0\u2026", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Predicting online protest participation of social media users\n", "abstract": " Social media has emerged to be a popular platform for people to express their viewpoints on political protests like the Arab Spring. Millions of people use social media to communicate and mobilize their viewpoints on protests. Hence, it is a valuable tool for organizing social movements. However, the mechanisms by which protest affects the population is not known, making it difficult to estimate the number of protestors. In this paper, we are inspired by sociological theories of protest participation and propose a framework to predict from the user's past status messages and interactions whether the next post of the user will be a declaration of protest. Drawing concepts from these theories, we model the interplay between the user's status messages and messages interacting with him over time and predict whether the next post of the user will be a declaration of protest. We evaluate the framework using data from the social media platform Twitter on protests during the recent Nigerian elections and demonstrate that it can effectively predict whether the next post of a user is a declaration of protest.", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Understanding and identifying advocates for political campaigns on social media\n", "abstract": " Social media is increasingly being used to access and disseminate information on sociopolitical issues like gun rights and general elections. The popularity and openness of social media makes it conducive for some individuals, known as advocates, who use social media to push their agendas on these issues strategically. Identifying these advocates will caution social media users before reading their information and also enable campaign managers to identify advocates for their digital political campaigns. A significant challenge in identifying advocates is that they employ nuanced strategies to shape user opinion and increase the spread of their messages, making it difficult to distinguish them from random users posting on the campaign. In this paper, we draw from social movement theories and design a quantitative framework to study the nuanced message strategies, propagation strategies, and community\u00a0\u2026", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Identifying information spreaders in twitter follower networks\n", "abstract": " A number of research efforts on Twitter have been contributed towards understanding various factors that are related to retweetability, analyzing retweeting and diffusion patterns, predicting retweets, etc. One fundamental research question remains untackled: given a user and her followers, which of the followers are likely to spread her tweets to the world (the information spreader identification problem)? Answering this new and open problem helps to bridge the gap between analyzing retweetbility and understanding information diffusion. Using a large scale Twitter data set, we first find that retweet history is not an ideal method for identifying information spreaders, especially for the long tail users. Backed by statistical analysis, we set forward to extract meaningful features and present a set of feasible approaches for identifying information spreaders in the Twitter follower networks. Our study reports interesting findings, sheds light on many practical applications, helps understand the mechanisms of relaying information from one user to her followers, and offers future lines of research.", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Fragmentation problem and automated feature construction\n", "abstract": " Selective induction algorithms are efficient in learning target concepts but inherit a major limitation each time only one feature is used to partition the data until the data is divided into uniform segments. This limitation results in problems like replication, repetition, and fragmentation. Constructive induction has been an effective means to overcome some of the problems. The underlying idea is to construct compound features that increase the representation power so as to enhance the learning algorithm's capability in partitioning data. Unfortunately, many constructive operators are often manually designed and choosing which one to apply poses a serious problem itself. We propose an automatic way of constructing compound features. The method can be applied to both continuous and discrete data and thus all the three problems can be eliminated or alleviated. Our empirical results indicate the effectiveness of the\u00a0\u2026", "num_citations": "16\n", "authors": ["1157"]}
{"title": "Debiasing grid-based product search in e-commerce\n", "abstract": " The widespread usage of e-commerce websites in daily life and the resulting wealth of implicit feedback data form the foundation for systems that train and test e-commerce search ranking algorithms. While convenient to collect, implicit feedback data inherently suffers from various types of bias since user feedback is limited to products they are exposed to by existing search ranking algorithms and impacted by how the products are displayed. In the literature, a vast majority of existing methods have been proposed towards unbiased learning to rank for list-based web search scenarios. However, such methods cannot be directly adopted by e-commerce websites mainly for two reasons. First, in e-commerce websites, search engine results pages (SERPs) are displayed in 2-dimensional grids. The existing methods have not considered the difference in user behavior between list-based web search and grid-based\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Robust cyberbullying detection with causal interpretation\n", "abstract": " Cyberbullying poses serious threats to preteens and teenagers, therefore, understanding the incentives behind cyberbullying is critical to prevent its happening and mitigate the impact. Most existing work towards cyberbullying detection has focused on the accuracy, and overlooked causes of the outcome. Discovering the causes of cyberbullying from observational data is challenging due to the existence of confounders, variables that can lead to spurious causal relationships between covariates and the outcome. This work studies the problem of robust cyberbullying detection with causal interpretation and proposes a principled framework to identify and block the influence of the plausible confounders, ie, p-confounders. The de-confounded model is causally interpretable and is more robust to the changes in data distribution. We test our approach using the state-of-the-art evaluation method, causal transportability\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Understanding cyber attack behaviors with sentiment information on social media\n", "abstract": " In today\u2019s increasingly connected world, cyber attacks have become a serious threat with detrimental effects on individuals, businesses, and broader society. Truly mitigating the negative impacts of these attacks requires a deeper understanding of malicious cyber activities and the capability of predicting these attacks before they occur. However, detecting the occurrence of cyber attacks is non-trivial due to the anonymity of cyber attacks and the ambiguity or unavailability of network data collected within organizations. Thus, we need to explore more nuanced auxiliary information that can provide improved predictive power and insight into the behavioral factors involved in planning and executing a cyber attack. Evidence suggests that public discourse in online sources, such as social media, is strongly correlated with the occurrence of real-world behavior; we believe this same premise can provide predictive\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Long noncoding RNA expression profile reveals lncRNAs signature associated with extracellular matrix degradation in kashin-beck disease\n", "abstract": " Kashin-Beck disease (KBD) is a deformative, endemic osteochondropathy involving degeneration and necrosis of growth plates and articular cartilage. The pathogenesis of KBD is related to gene expression and regulation mechanisms, but long noncoding RNAs (lncRNAs) in KBD have not been investigated. In this study, we identified 316 up-regulated and 631 down-regulated lncRNAs (\u2265 2-fold change) in KBD chondrocytes using microarray analysis, of which more than three-quarters were intergenic lncRNAs and antisense lncRNAs. We also identified 232 up-regulated and 427 down-regulated mRNAs (\u2265 2-fold change). A lncRNA-mRNA correlation analysis combined 343 lncRNAs and 292 mRNAs to form 509 coding-noncoding gene co-expression networks (CNC networks). Eleven lncRNAs were predicted to have cis-regulated target genes, including NAV2 (neuron navigator 2), TOX (thymocyte selection\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Identifying rhetorical questions in social media\n", "abstract": " Social media provides a platform for seeking information from a large user base. Information seeking in social media, however, occurs simultaneously with users expressing their viewpoints by making statements. Rhetorical questions have the form of a question but serve the function of a statement and might mislead platforms assisting information seeking in social media. It becomes difficult to identify rhetorical questions as they are not syntactically different from other questions. In this paper, we develop a framework to identify rhetorical questions by modeling the motivations of the users to post them. We focus on one motivation of the users drawing from linguistic theories, to implicitly convey a message. We develop a framework from this motivation to identify rhetorical questions in social media and evaluate the framework using questions posted on Twitter. This is the first framework to model the motivations for posting rhetorical questions to identify them on social media platforms.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Discovering Location Information in Social Media.\n", "abstract": " Social media is immensely popular, with billions of users across various platform. The study of social media has allowed for deeper inquiries into questions posed by computer scientists, social scientists, and others. Social media posts tagged with location have provided means for researchers to perform even deeper analysis into their data. While location information allows for rich insight into social media data, very few posts are explicitly tagged with geographic information. In this work, we begin by introducing some state-of-the-art analysis techniques that can be performed using the location of a social media post. Next, we introduce some systems that help first responders provide relief with the help of the location of social media posts. Finally, we discuss how machine learning techniques can be applied to infer the location of a social media post, bringing this analysis to any message posted on social media.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Recommendation in social media: recent advances and new frontiers\n", "abstract": " The pervasive use of social media generates massive data in an unprecedented rate and the information overload problem becomes increasingly severe for social media users. Recommendation has been proven to be effective in mitigating the information overload problem, demonstrated its strength in improving the quality of user experience, and positively impacted the success of social media. New types of data introduced by social media not only provide more information to advance traditional recommender systems but also manifest new research possibilities for recommendation. In this tutorial, we aim to provide a comprehensive overview of various recommendation tasks in social media, especially their recent advances and new frontiers. We introduce basic concepts, review state-of-the-art algorithms, and deliberate the emerging challenges and opportunities. Finally we summarize the tutorial with\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Navigating information facets on Twitter (NIF-T)\n", "abstract": " Recent years have seen an exponential increase in the number of users of social media sites. As the number of users of these sites continues to grow at an extraordinary rate, the amount of data produced follows in magnitude. With this deluge of social media data, the need for comprehensive tools to analyze user interactions is ever increasing. In this paper, we present a novel tool, Navigating Information Facets on Twitter (NIF-T), which helps users to explore data generated on social media sites. Using the three dimensions or facets: time, location, and topic as an example of the many possible facets, we enable the users to explore large social media datasets. With the help of a large corpus of tweets collected from the Occupy Wall Street movement on the Twitter platform we show how our system can be used to identify important aspects of the event along these facets.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "A social network analysis approach to detecting suspicious online financial activities\n", "abstract": " Social network analysis techniques can be applied to help detect financial crimes. We discuss the relationship between detecting financial crimes and the social web, and use select case studies to illustrate the potential for applying social network analysis techniques. With the increasing use of online financing services and online financial activities, it becomes more challenging to find suspicious activities among massive numbers of normal and legal activities.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Learning with large-scale social media networks\n", "abstract": " Social media such as blogs, Facebook, Twitter, YouTube and Flickr enables people of all walks of life to express their thoughts, voice their opinions, and connect to each other more conveniently than ever. The boom of social media opens up a vast range of possibilities to study human interactions and collective behavior on an unprecedented scale. This dissertation presents a framework for learning with large-scale social media networks in order to understand human interactions and to predict collective behavior. Network interactions are typically heterogeneous, representing disparate relations, but most social media sites present only connections with no or limited relation information. Hence, social dimension is introduced to differentiate heterogeneous relations. A learning approach based on social dimensions is proposed, achieving substantial improvement over the state of the art. It is then extended to unify\u00a0\u2026", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Relative measure for mining interesting rules\n", "abstract": " This paper presents a measure which estimates interestingness of a rule relative to its corresponding common sense rules. Mining interesting rules is one of the important data mining tasks. Interesting rules bring novel knowledge that helps decision makers for advantageous actions. Interestingness is a relative issue. It is relative with what is known about the domain. A measure which can estimate the interestingness of a rule relative to the known knowledge is thus required. However, this estimation may not be accurate due to the incomplete or inaccurate knowledge about the domain. Even if it is possible to estimate interestingness, it is not so trivial to judge the interestingness from a huge set of mined rules manually. Therefore, an automated system is required that can exploit the common sense rules extracted from the data to estimate interestingness. Since the common sense rules extracted from the data can represent the true nature about the domain, it is possible to find an interestingness measure that is free from user's biased belief. A measure that can estimate the interestingness of a rule with respect to the extracted common sense rules can be more acceptable to the user. In this work we try to show through the experiments, how our proposed relative measure can estimate relative interestingness in a rule considering already mined rules.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "A data mining application: customer retention at the Port of Singapore Authority (PSA)\n", "abstract": " \u201cCustomer retention\u201d is an important real-world problem in many sales and services related industries today. This work illustrates how we can integrate the various techniques of data-mining, such as decision-tree induction, deviation analysis and multiple concept-level association rules to form an intuitive and novel approach to gauging customer's loyalty and predicting their likelihood of defection. Immediate action taken against these \u201cearly-warnings\u201d is often the key to the eventual retention or loss of the customers involved.", "num_citations": "15\n", "authors": ["1157"]}
{"title": "Inductive anomaly detection on attributed networks\n", "abstract": " Anomaly detection on attributed networks has attracted a surge of research attention due to its broad applications in various high-impact domains, such as security, finance, and healthcare. Nonetheless, most of the existing efforts do not naturally generalize to unseen nodes, leading to the fact that people have to retrain the detection model from scratch when dealing with newly observed data. In this study, we propose to tackle the problem of inductive anomaly detection on attributed networks with a novel unsupervised framework: AEGIS (adversarial graph differentiation networks). Specifically, we design a new graph neural layer to learn anomaly-aware node representations and further employ generative adversarial learning to detect anomalies among new data. Extensive experiments on various attributed networks demonstrate the efficacy of the proposed approach.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "MM-COVID: A Multilingual and Multidimensional Data Repository for CombatingCOVID-19 Fake New\n", "abstract": " The COVID-19 epidemic is considered as the global health crisis of the whole society and the greatest challenge mankind faced since World War Two. Unfortunately, the fake news about COVID-19 is spreading as fast as the virus itself. The incorrect health measurements, anxiety, and hate speeches will have bad consequences on people's physical health, as well as their mental health in the whole world. To help better combat the COVID-19 fake news, we propose a new fake news detection dataset MM-COVID (Multilingual and Multidimensional COVID-19 Fake News Data Repository). This dataset provides the multilingual fake news and the relevant social context. We collect 3981 pieces of fake news content and 7192 trustworthy information from English, Spanish, Portuguese, Hindi, French and Italian, 6 different languages. We present a detailed and exploratory analysis of MM-COVID from different perspectives\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Be more with less: Hypergraph attention networks for inductive text classification\n", "abstract": " Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Disinformation, Misinformation, and Fake News in Social Media\n", "abstract": " Lecture Notes in Social Networks (LNSN) comprises volumes covering the theory, foundations and applications of the new emerging multidisciplinary field of social networks analysis and mining. LNSN publishes peer-reviewed works (including monographs, edited works) in the analytical, technical as well as the organizational side of social computing, social networks, network sciences, graph theory, sociology, Semantics Web, Web applications and analytics, information networks, theoretical physics, modeling, security, crisis and risk management, and other related disciplines. The volumes are guest-edited by experts in a specific domain. This series is indexed by DBLP. Springer and the Series Editors welcome book ideas from authors. Potential authors who wish to submit a book proposal should contact Christoph Baumann, Publishing Editor, Springer e-mail: Christoph. Baumann@ springer. com", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Privacy preserving text representation learning\n", "abstract": " Online users generate tremendous amounts of textual information by participating in different online activities. This data provides opportunities for researchers and business partners to understand individuals. However, this user-generated textual data not only can reveal the identity of the user but also may contain individual's private attribute information. Publishing the textual data thus compromises the privacy of users. It is challenging to design effective anonymization techniques for textual information which minimize the chances of re-identification and does not contain private information while retaining the textual semantic meaning. In this paper, we study this problem and propose a novel double privacy preserving text representation learning framework, DPText. We show the effectiveness of DPText in preserving privacy and utility.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "I am not what i write: Privacy preserving text representation learning\n", "abstract": " Online users generate tremendous amounts of textual information by participating in different activities, such as writing reviews and sharing tweets. This textual data provides opportunities for researchers and business partners to study and understand individuals. However, this user-generated textual data not only can reveal the identity of the user but also may contain individual's private information (e.g., age, location, gender). Hence, \"you are what you write\" as the saying goes. Publishing the textual data thus compromises the privacy of individuals who provided it. The need arises for data publishers to protect people's privacy by anonymizing the data before publishing it. It is challenging to design effective anonymization techniques for textual information which minimizes the chances of re-identification and does not contain users' sensitive information (high privacy) while retaining the semantic meaning of the data for given tasks (high utility). In this paper, we study this problem and propose a novel double privacy preserving text representation learning framework, DPText, which learns a textual representation that (1) is differentially private, (2) does not contain private information and (3) retains high utility for the given task. Evaluating on two natural language processing tasks, i.e., sentiment analysis and part of speech tagging, we show the effectiveness of this approach in terms of preserving both privacy and utility.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Sub-chronic toxicity study of arecae semen aqueous extract in Wistar rats\n", "abstract": " Ethnopharmacological relevanceArecae semen, the ripe seed of Areca catechu L., has been used as vermifuge and digestant in traditional Chinese medicine (TCM). However, the potential toxicity effect of arecae semen has not been completely investigated.The aim of the studyThe present study was aimed at evaluating the sub-chronic toxicity of arecae semen by oral administration in Wistar rats.Materials and methodsA total of 120 Wistar rats were randomly divided into 4 groups (15 males and 15 females per group). The treated groups were given arecae semen aqueous extract (ASAE) at the dose of 750, 1500 and 4500\u00a0mg/kg/day by oral administration respectively, and the control group was received distilled water only. The rats and their consumed feed were weighted every 3 days. The clinical changes and mortality were observed and recorded daily. Hematological parameters, biochemical parameters, organ\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "In search of coherence and consensus: measuring the interpretability of statistical topics\n", "abstract": " Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (eg, their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence---model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "The potential of induced pluripotent stem cells as a tool to study skeletal dysplasias and cartilage-related pathologic conditions\n", "abstract": " The development of induced pluripotent stem cells (iPSCs) technology has opened up new horizons for development of new research tools especially for skeletal dysplasias, which often lack human disease models. Regenerative medicine and tissue engineering could be the next areas to benefit from refinement of iPSC methods to repair focal cartilage defects, while applications for osteoarthritis (OA) and drug screening have evolved rather slowly. Although the advances in iPSC research of skeletal dysplasias and repair of focal cartilage lesions are not directly relevant to OA, they can be considered to pave the way to future prospects and solutions to OA research, too. The same problems which face the present cell-based treatments of cartilage injuries concern also the iPSC-based ones. However, established iPSC lines, which have no genomic aberrations and which efficiently differentiate into extracellular\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Machine learning to predict rapid progression of carotid atherosclerosis in patients with impaired glucose tolerance\n", "abstract": " Prediabetes is a major epidemic and is associated with adverse cardio-cerebrovascular outcomes. Early identification of patients who will develop rapid progression of atherosclerosis could be beneficial for improved risk stratification. In this paper, we investigate important factors impacting the prediction, using several machine learning methods, of rapid progression of carotid intima-media thickness in impaired glucose tolerance (IGT) participants. In the Actos Now for Prevention of Diabetes (ACT NOW) study, 382 participants with IGT underwent carotid intima-media thickness (CIMT) ultrasound evaluation at baseline and at 15\u201318\u00a0months, and were divided into rapid progressors (RP, n\u2009=\u200939, 58\u2009\u00b1\u200917.5\u00a0\u03bcM change) and non-rapid progressors (NRP, n\u2009=\u2009343, 5.8\u2009\u00b1\u200920\u00a0\u03bcM change, p\u2009<\u20090.001 versus RP). To deal with complex multi-modal data consisting of demographic, clinical, and laboratory variables, we\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Uncovering and predicting human behaviors\n", "abstract": " This installment of Trends & Controversies provides an array of perspectives on the latest research in modeling user behavior. Peng Cui, Huan Liu, Charu Aggarwal, and Fei Wang introduce the field in \"Uncovering and Predicting Human Behaviors.\" The essays included are \"Computational Modeling of Complex User Behaviors: Challenges and Opportunities,\" by Peng Cui, Huan Liu, Charu Aggarwal, and Fei Wang; \"Non-IID Recommendation Theories and Systems,\" by Longbing Cao and Philip S. Yu; \"User Behavior Modeling and Fraud Detection,\" by Alex Beutel and Christos Faloutsos; and \"Transfer Learning for Behavior Prediction,\" by Weike Pan and Qiang Yang.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Social role identification via dual uncertainty minimization regularization\n", "abstract": " In this paper, we study a challenging problem of inferring individuals' role and statuses in a professional social network, which is of central importance in workforce optimization and human capital management. Realizing the natural setting of social nodes associated with dual view information, i.e., The local node characteristics and the global network influence, we present a novel model that explores graph regularization techniques and integrates such information to achieve improved prediction performance. In particular, our prediction model is built upon the graph transductive learning framework that encodes an uncertainty regularization term in the conventional empirical risk minimization principle. Through taking advantage of the information from both the local profile and the global network characteristics, the final inference of the role or statues achieves minimum an empirical loss on the labeled set, as well as a\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Discovering social events through online attention\n", "abstract": " Twitter is a major social media platform in which users send and read messages (\u201ctweets\u201d) of up to 140 characters. In recent years this communication medium has been used by those affected by crises to organize demonstrations or find relief. Because traffic on this media platform is extremely heavy, with hundreds of millions of tweets sent every day, it is difficult to differentiate between times of turmoil and times of typical discussion. In this work we present a new approach to addressing this problem. We first assess several possible \u201cthermostats\u201d of activity on social media for their effectiveness in finding important time periods. We compare methods commonly found in the literature with a method from economics. By combining methods from computational social science with methods from economics, we introduce an approach that can effectively locate crisis events in the mountains of data generated on Twitter. We demonstrate the strength of this method by using it to locate the social events relating to the Occupy Wall Street movement protests at the end of 2011.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Recovering information recipients in social media via provenance\n", "abstract": " In recent years, social media has changed the way we interact and communicate. Although the existing structure of social media allows users to easily create, receive, and propagate pieces of information, many a time, users do not have background knowledge about the received information, including the provenance (sources or originators) of information, and other recipients who may have retransmitted or modified the information. Providing such additional context to the received information can help users know how much value, trust, and validity should be placed in received information. To judge the credibility of the received piece of information, it is vital to know who are its sources, and how information propagates from sources to other social media users.", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Analyzing behavior of the influentials across social media\n", "abstract": " The popularity of social media as an information source, in the recent years has spawned several interesting applications, and consequently challenges to using it effectively. Identifying and targeting influential individuals on sites is a crucial way to maximize the returns of advertising and marketing efforts. Recently, this problem has been well studied in the context of blogs, microblogs, and other forms of social media sites. Understanding how these users behave on a social media site and even across social media sites will lead to more effective strategies. In this book chapter, we present existing techniques to identify influential individuals in a social media site. We present a user identification strategy, which can help us to identify influential individuals across sites. Using a combination of these approaches we present a study of the characteristics and behavior of influential individuals across sites. We\u00a0\u2026", "num_citations": "14\n", "authors": ["1157"]}
{"title": "Graph Learning: A Survey\n", "abstract": " Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Random-forest-inspired neural networks\n", "abstract": " Neural networks have become very popular in recent years, because of the astonishing success of deep learning in various domains such as image and speech recognition. In many of these domains, specific architectures of neural networks, such as convolutional networks, seem to fit the particular structure of the problem domain very well and can therefore perform in an astonishingly effective way. However, the success of neural networks is not universal across all domains. Indeed, for learning problems without any special structure, or in cases where the data are somewhat limited, neural networks are known not to perform well with respect to traditional machine-learning methods such as random forests. In this article, we show that a carefully designed neural network with random forest structure can have better generalization ability. In fact, this architecture is more powerful than random forests, because the back\u00a0\u2026", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Identification and analysis of amygdalin, neoamygdalin and amygdalin amide in different processed bitter almonds by HPLC-ESI-MS/MS and HPLC-DAD\n", "abstract": " Processing is a traditional pharmacy technology based on traditional Chinese medicine theory. The traditional Chinese medicine (TCM) ingredients should be processed before being used as a medicine. Processed bitter almonds are widely used in the clinic in TCM for the treatment of cough and asthma. In this work the amygdalin profile of three producing areas in China was determined, with respect to three differently processed bitter almond products: raw, stir-fried and scalded. Identification of the compounds was done by using high performance liquid chromatography coupled to electrospray ionization mass spectrometry (HPLC-ESI-MS/MS). Results indicated that amygdalin, neoamygdalin and amygdalin amide were identified in the different processed bitter almonds. Meanwhile, amygdalin was used as a standard to calculate the quantification of amygdalin and the concentration ratio of neoamygdalin and total amygdalin by HPLC-DAD. The data suggested that composition of amygdalin isomers in bitter almonds was influenced by the processing method. It also gives a new understanding of the processing principle of bitter almonds. Moreover, the classification of different processed bitter almonds can be achieved on the basis of amygdalin isomers levels. View Full-Text", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Context-aware experience extraction from online health forums\n", "abstract": " Online health forums provide a large repository for patients, caregivers, and researchers to seek valuable information. The extraction of patient-reported personal health experience from the forums has many important applications. For example, medical researchers can discover trustable knowledge from the extracted experience. Patients can search for peers with similar experience and connect with them. In this paper, we model the extraction of patient experience as a classification problem: classifying each sentence in a forum post as containing patient experience or not containing patient experience. We propose to exploit the sentence context information for such experience extraction task, and classify the context information into global context and local context. A unified Context-Aware Experience Extraction (CARE) framework is proposed to incorporate these two types of context information. Our experimental\u00a0\u2026", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Embracing information explosion without choking: Clustering and labeling in microblogging\n", "abstract": " The explosive popularity of microblogging services produce a large volume of microblogging messages. It presents great difficulties for a user to quickly gauge his/her followees' opinions when the user interface is overwhelmed by a large number of messages. Useful information is buried in disorganized, incomplete, and unstructured text messages. We propose to organize the large amount of messages into clusters with meaningful cluster labels, thus provide an overview of the content to fulfill users' information needs. Clustering and labeling of microblogging messages are challenging because that the length of the messages are much shorter than conventional text documents. They usually cannot provide sufficient term co-occurrence information for capturing their semantic associations. As a result, traditional text representation models tend to yield unsatisfactory performance. In this paper, we present a text\u00a0\u2026", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Connecting sparsely distributed similar bloggers\n", "abstract": " The nature of the Blogosphere determines that the majority of bloggers are only connected with a small number of fellow bloggers, and similar bloggers can be largely disconnected from each other. Aggregating them allows for cost-effective personalized services, targeted marketing, and exploration of new business opportunities. As most bloggers have only a small number of adjacent bloggers, the problem of aggregating similar bloggers presents challenges that demand novel algorithms of connecting the non-adjacent due to the fragmented distributions of bloggers. In this work, we define the problem, delineate its challenges, and present an approach that uses innovative ways to employ contextual information and collective wisdom to aggregate similar bloggers. A real-world blog directory is used for experiments. We demonstrate the efficacy of our approach, report findings, and discuss related issues and future\u00a0\u2026", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Intelligent instance selection of data streams for smart sensor applications\n", "abstract": " The purpose of our work is to mine streaming data from a variety of hundreds of automotive sensors in order to develop methods to minimize driver distraction from in-vehicle communications and entertainment systems such as audio/video devices, cellphones, PDAs, Fax, eMail, and other messaging devices. Our endeavor is to create a safer driving environment, by providing assistance in the form of warning, delaying, or re-routing, incoming signals if the assistance system detects that the driver is performing, or is about to perform, a critical maneuver, such as passing, changing lanes, making a turn, or during a sudden evasive maneuver. To accomplish this, our assistance system relies on maneuver detection by continuously evaluating various embedded vehicle sensors, such as speed, steering, acceleration, lane distance, and many others, combined into representing an instance of the \u201cstate\u201d of the vehicle. One\u00a0\u2026", "num_citations": "13\n", "authors": ["1157"]}
{"title": "Counterfactual evaluation of treatment assignment functions with networked observational data\n", "abstract": " Counterfactual evaluation of novel treatment assignment functions (e.g., advertising algorithms and recommender systems) is one of the most crucial causal inference problems for practitioners. Traditionally, randomized controlled trials (e.g., A/B tests) are performed to evaluate treatment assignment functions. However, they can be time-consuming, expensive, and even unethical in some cases. Therefore, counterfactual evaluation of treatment assignment functions becomes a pressing issue because a massive amount of observational data becomes available in the big data era. Counterfactual evaluation requires controlling the influence of hidden confounders \u2013 the unmeasured features that causally influence both treatment assignments and outcomes. However, most of the existing methods rely on the assumption of no hidden confounders. This assumption can be untenable in the context of massive\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Academic team formulation based on Liebig\u2019s barrel: Discovery of anticask effect\n", "abstract": " Academic team formulation is one of the most critical and fundamental issues in team research. Inspired by the Liebig's metaphor, we formulate an academic team by a barrel composed of planks, namely, Liebig's barrel. We study the capacity of Liebig's barrel from several aspects. Then, we abstract the features in academic teams as those in Liebig's barrel due to the correspondences between team size and plank number, individual member ability and plank height, individual contribution and plank width, allocation of work and plank combination, teamwork attitude and plank tilt angle, team output property, and barrel's content granularity. The investigated model can also be extended to formulate various multifactor objective optimization issues for team science research. In order to verify the rationality of the proposed model, we implement case studies of this model in both the American Physics Society and the\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Signed link prediction with sparse data: The role of personality information\n", "abstract": " Predicting signed links in social networks often faces the problem of signed link data sparsity, ie, only a small percentage of signed links are given. The problem is exacerbated when the number of negative links is much smaller than that of positive links. Boosting signed link prediction necessitates additional information to compensate for data sparsity. According to psychology theories, one rich source of such information is user\u2019s personality such as optimism and pessimism that can help determine her propensity in establishing positive and negative links. In this study, we investigate how personality information can be obtained, and if personality information can help alleviate the data sparsity problem for signed link prediction. We propose a novel signed link prediction model that enables empirical exploration of user personality via social media data. We evaluate our proposed model on two datasets of real-world\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Identifying novel privacy issues of online users on social media platforms\n", "abstract": " Online users generate tremendous amounts of data. To better serve users, it is required to share the user-related data with advertisers and application developers. Socia media user-related data might make users susceptible to unintended user privacy breach. To encourage data sharing and mitigate user privacy concerns, a number of anonymization and de-anonymization algorithms have been developed to help protect privacy of users. This article introduces our recent research on online users privacy in social media. In particular, we review an approach to identifying novel privacy issues via an adversarial attack specialized for social media data. Our work sheds light on the study of new privacy risks in social media data arising from the innate heterogeneity of user-generated data.", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Linked variational autoencoders for inferring substitutable and supplementary items\n", "abstract": " Recommendation in the modern world is not only about capturing the interaction between users and items, but also about understanding the relationship between items. Besides improving the quality of recommendation, it enables the generation of candidate items that can serve as substitutes and supplements of another item. For example, when recommending Xbox, PS4 could be a logical substitute and the supplements could be items such as game controllers, surround system, and travel case. Therefore, given a network of items, our objective is to learn their content features such that they explain the relationship between items in terms of substitutes and supplements. To achieve this, we propose a generative deep learning model that links two variational autoencoders using a connector neural network to create Linked Variational Autoencoder (LVA). LVA learns the latent features of items by conditioning on the\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Dendritic cell trafficking and function in rare lung diseases\n", "abstract": " Dendritic cells (DCs) are highly specialized immune cells that capture antigens and then migrate to lymphoid tissue and present antigen to T cells. This critical function of DCs is well defined, and recent studies further demonstrate that DCs are also key regulators of several innate immune responses. Studies focused on the roles of DCs in the pathogenesis of common lung diseases, such as asthma, infection, and cancer, have traditionally driven our mechanistic understanding of pulmonary DC biology. The emerging development of novel DC reagents, techniques, and genetically modified animal models has provided abundant data revealing distinct populations of DCs in the lung, and allow us to examine mechanisms of DC development, migration, and function in pulmonary disease with unprecedented detail. This enhanced understanding of DCs permits the examination of the potential role of DCs in diseases\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Exploiting hierarchical structures for unsupervised feature selection\n", "abstract": " Feature selection has been proven to be effective and efficient in preparing high-dimensional data for many mining and learning tasks. Features of real-world high-dimensional data such as words of documents, pixels of images and genes of microarray data, usually present inherent hierarchical structures. In a hierarchical structure, features could share certain properties. Such information has been exploited to help supervised feature selection but it is rarely investigated for unsupervised feature selection, which is challenging due to the lack of labels. Since real world data is often unlabeled, it is of practical importance to study the problem of feature selection with hierarchical structures in an unsupervised setting. In particular, we provide a principled method to exploit hierarchical structures of features and propose a novel framework HUFS, which utilizes the given hierarchical structures to help select features without\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Am i more similar to my followers or followees? Analyzing homophily effect in directed social networks\n", "abstract": " Homophily is the formation of social ties between two individuals due to similar characteristics or interests. Based on homophily, in a social network it is expected to observe a higher degree of homogeneity among connected than disconnected people. Many researchers use this simple yet effective principal to infer users' missing information and interests based on the information provided by their neighbors. In a directed social network, the neighbors can be further divided into followers and followees. In this work, we investigate the homophily effect in a directed network. To explore the homophily effect in a directed network, we study if a user's personal preferences can be inferred from those of users connected to her (followers or followees). We investigate which of followers or followees are more effective in helping to infer users' personal preferences. Our findings can help to raise the awareness of users over their\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Analyzing twitter data\n", "abstract": " So far we have discussed the collection and management of a large set of Tweets. It is time to put these Tweets to work to gain information about the data we have collected. This chapter focuses on two key aspects of Twitter data for data analysis: networks and text.", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Understanding group structures and properties in social media\n", "abstract": " The rapid growth of social networking sites enables people to connect to each other more conveniently than ever. With easy-to-use social media, people contribute and consume contents, leading to a new form of human interaction and the emergence of online collective behavior. In this chapter, we aim to understand group structures and properties by extracting and profiling communities in social media. We present some challenges of community detection in social media. A prominent one is that networks in social media are often heterogeneous. We introduce two types of heterogeneity presented in online social networks and elaborate corresponding community detection approaches for each type, respectively. Social media provides not only interaction information but also textual and tag data. This variety of data can be exploited to profile individual groups in understanding group formation and\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Discovering mappings in hierarchical data from multiple sources using the inherent structure\n", "abstract": " Unprecedented amounts of media data are publicly accessible. However, it is increasingly difficult to integrate relevant media from multiple and diverse sources for effective applications. The functioning of a multimodal integration system requires metadata, such as ontologies, that describe media resources and media components. Such metadata are generally application-dependent and this can cause difficulties when media needs to be shared across application domains. There is a need for a mechanism that can relate the common and uncommon terms and media components. In this paper, we develop an algorithm to mine and automatically discover mappings in hierarchical media data, metadata, and ontologies, using the structural information inherent in these types of data. We evaluate the performance of this algorithm for various parameters using both synthetic and real-world data collections and\u00a0\u2026", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Fostering biological relevance in feature selection for microarray data\n", "abstract": " The biological relevance in feature selection for microarray data was enriched. A tool, Reporter-Surrogate Variable Program, was developed that reduces the number of selected genes while increasing the overall discriminative power. RSVP aims to produce results that are both statistically significant and biologically relevant. Enriching statistically significant gene lists with biologically relevant genes can help expedite biological discovery and downstream analysis.", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Detecting hidden information in images: A comparative study\n", "abstract": " During the process of information hiding in a cover image, LSB-based steganographic techniques like JSteg change the statistical properties of the cover image. Accordingly, such information hiding techniques are vulnerable to statistical attack. The understanding of steganalysis methods and their effects can help in designing methods and algorithms preserving data privacy. In this paper, we compare some steganalysis methods for attacking LSB-based steganographic techniques (logistic regression, the tree-based method C4. 5, and a popular method Stegdetect). Experimental results show that the first two methods, especially the logistic regression method, are able to detect hidden information with high accuracy. We also study the relationship between the number of attributes (the frequencies of quantized DCT coefficients) and the performance of a classifier.", "num_citations": "12\n", "authors": ["1157"]}
{"title": "Interactive unknowns recommendation in e-learning systems\n", "abstract": " The arise of E-learning systems has led to an anytime-anywhere-learning environment for everyone by providing various online courses and tests. However, due to the lack of teacher-student interaction, such ubiquitous learning is generally not as effective as offline classes. In traditional offline courses, teachers facilitate real-time interaction to teach students in accordance with personal aptitude from students' feedback in classes. Without the interruption of instructors, it is difficult for users to be aware of personal unknowns. In this paper, we address an important issue on the exploration of 'user unknowns' from an interactive question-answering process in E-learning systems. A novel interactive learning system, called CagMab, is devised to interactively recommend questions with a round-by-round strategy, which contributes to applications such as a conversational bot for self-evaluation. The flow enables users to\u00a0\u2026", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Understanding and identifying rhetorical questions in social media\n", "abstract": " Social media provides a platform for seeking information from a large user base. Information seeking in social media, however, occurs simultaneously with users expressing their viewpoints by making statements. Rhetorical questions have the form of a question but serve the function of a statement and are an important tool employed by users to express their viewpoints. Therefore, rhetorical questions might mislead platforms assisting information seeking in social media. It becomes difficult to identify rhetorical questions as they are not syntactically different from other questions. In this article, we develop a framework to identify rhetorical questions by modeling some motivations of the users to post them. We focus on two motivations of the users drawing from linguistic theories to implicitly convey a message and to modify the strength of a statement previously made. We develop a quantitative framework from these\u00a0\u2026", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Cross-platform emoji interpretation: analysis, a solution, and applications\n", "abstract": " Most social media platforms are largely based on text, and users often write posts to describe where they are, what they are seeing, and how they are feeling. Because written text lacks the emotional cues of spoken and face-to-face dialogue, ambiguities are common in written language. This problem is exacerbated in the short, informal nature of many social media posts. To bypass this issue, a suite of special characters called \"emojis,\" which are small pictograms, are embedded within the text. Many emojis are small depictions of facial expressions designed to help disambiguate the emotional meaning of the text. However, a new ambiguity arises in the way that emojis are rendered. Every platform (Windows, Mac, and Android, to name a few) renders emojis according to their own style. In fact, it has been shown that some emojis can be rendered so differently that they look \"happy\" on some platforms, and \"sad\" on others. In this work, we use real-world data to verify the existence of this problem. We verify that the usage of the same emoji can be significantly different across platforms, with some emojis exhibiting different sentiment polarities on different platforms. We propose a solution to identify the intended emoji based on the platform-specific nature of the emoji used by the author of a social media post. We apply our solution to sentiment analysis, a task that can benefit from the emoji calibration technique we use in this work. We conduct experiments to evaluate the effectiveness of the mapping in this task.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "A novel measure for coherence in statistical topic models\n", "abstract": " Big data presents new challenges for understanding large text corpora. Topic modeling algorithms help understand the underlying patterns, or \u201ctopics\u201d, in data. Researchersauthor often read these topics in order to gain an understanding of the underlying corpus. It is important to evaluate the interpretability of these automatically generated topics. Methods have previously been designed to use crowdsourcing platforms to measure interpretability. In this paper, we demonstrate the necessity of a key concept, coherence, when assessing the topics and propose an effective method for its measurement. We show that the proposed measure of coherence captures a different aspect of the topics than existing measures. We further study the automation of these topic measures for scalability and reproducibility, showing that these measures can be automated.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Relational learning with social status analysis\n", "abstract": " Relational learning has been proposed to cope with the interdependency among linked instances in social network analysis, which often adopts network connectivity and social media content for prediction. A common assumption in existing relational learning methods is that data instances are equally important. The algorithms developed based on the assumption may be significantly affected by outlier data and thus less robust. In the meantime, it has been well established in social sciences that actors are naturally of different social status in a social network. Motivated by findings from social sciences, in this paper, we investigate whether social status analysis could facilitate relational learning. Particularly, we propose a novel framework RESA to model social status using the network structure. It extracts robust and intrinsic latent social dimensions for social actors, which are further exploited as features for\u00a0\u2026", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Biodegradable behaviors of ultrafine-grained ZE41A magnesium alloy in DMEM solution\n", "abstract": " The main limitation to the clinical application of magnesium alloys is their too-fast degradation rate in the physiological environment. Bio-corrosion behaviors of the ZE41A magnesium alloy processed by multi-pass equal channel angular pressing (ECAP) were investigated in Dulbecco's Modified Eagle Medium (DMEM) solution, in order to tailor the effect of grain ultrafining on the biodegradation rate of the alloy implant. Hydrogen evolution tests indicated that a large number of ECAP passes decreased the stable corrosion rate of the alloy after the initial incubation period. Potentiodynamic polarization curves showed that more ECAP passes made the corrosion potential nobler and the corrosion tendency lower. Corroded surfaces of the ECAPed alloy indicated a higher resistance toward localized corrosion due to the homogeneous redistribution of broken second phases on the ultrafine-grained Mg matrix. It suggests that grain ultrafining can decrease the biodegradable rate of the magnesium alloy-containing rare-earth elements and tailor the lifetime of the biodegradable material. View Full-Text", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Feature selection strategy in text classification\n", "abstract": " Traditionally, the best number of features is determined by the so-called \u201crule of thumb\u201d, or by using a separate validation dataset. We can neither find any explanation why these lead to the best number nor do we have any formal feature selection model to obtain this number. In this paper, we conduct an in-depth empirical analysis and argue that simply selecting the features with the highest scores may not be the best strategy. A highest scores approach will turn many documents into zero length, so that they cannot contribute to the training process. Accordingly, we formulate the feature selection process as a dual objective optimization problem, and identify the best number of features for each document automatically. Extensive experiments are conducted to verify our claims. The encouraging results indicate our proposed framework is effective.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Mapping socio-cultural dynamics in Indonesian blogosphere\n", "abstract": " Understanding socio-cultural dynamics of a society requires an exhaustive study of the people who form the society. This requires collection of large amount of demographic data, which is a difficult task. Blogosphere provides a convenient solution to this problem. With the pervasion of Internet, and blogging in particular, it has become easier to collect data from the virtual world, which can be used to study the social and cultural behavior of a community. The Indonesian blogosphere offers rich examples for interactions between multi-cultural society and the Internet. With the help of a social scientist, we motivate the need to perform a study of the Indonesian blogosphere. We identify relevant theories that explain our observations. We also point out interesting problems that can be studied in the Indonesian blogosphere and present a preliminary study of the blogosphere based on the Indonesian blog data that we have collected.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Understanding emerging social structures\u2014a group profiling approach\n", "abstract": " The prolific use of participating web and social networking sites is reshaping the way in which people interact with each other, and it has become an increasingly vital part of social life of human beings. People sharing certain similarities tend to form communities in social media. At the same time, they participate in various online activities: content sharing, tagging, twittering, etc. These diverse activities leave traces of their social life, providing clues to understand emerging social structures. Plenty of existing work focus on extracting cohesive groups based on network topology. In this work, we advance further to explore different group-profiling strategies to construct descriptions of a group, helping explain the group formation. This research and results can help network navigation, visualization and analysis, as well as monitoring and tracking the ebbs and tides of different groups in evolving networks. By exploiting the information collected from real-world social media sites, we conduct extensive experiments to evaluate group-profiling results. The pros and cons of different group-profiling strategies are analyzed with concrete examples. We then use LiveJournal as a testbed to show some potential applications based on group profiling. Interesting findings with discussions are reported.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Multiclass probabilistic kernel discriminant analysis\n", "abstract": " Kernel discriminant analysis (KDA) is an effective approach for supervised nonlinear dimensionality reduction. Probabilistic models can be used with KDA to improve its robustness. However, the state of the art of such models could only handle binary class problems, which confines their application in many real world problems. To overcome this limitation, we propose a novel nonparametric probabilistic model based on Gaussian Process for KDA to handle multiclass problems. The model provides a novel Bayesian interpretation for KDA, which allows its parameters to be automatically tuned through the optimization of the marginal loglikelihood of the data. Empirical study demonstrates the efficacy of the proposed model.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Understanding group interaction in blogosphere: a case study\n", "abstract": " Social interactions are an essential ingredient of our lives. People convene groups and share views, opinions, thoughts, and perspectives. Similar tendencies for social behavior are observed in the World Wide Web. This inspires us to study and understand social interactions evolving in online communities especially in the blogosphere. In this paper, we study and analyze various interaction patterns in community blogs. This would lead to better understanding of the sociocultural ties between these communities to foster collaboration, better personalization, predictive modeling, and enable tracking and monitoring. Tapping community interactions via link analysis has its limitations due to exponentially large search space. We propose a model, circumventing the challenges with link analysis based approach, to observe interaction within community blogs via an observed event and community reaction to that by studying the opinion and sentiments of the members towards that event. We present a case study on ethnic community blogs exploiting the proposed model and report our findings and observations. During our study we encountered several challenges with the proposed model. We discuss these issues and present future directions to make the model more robust.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "NeuroLinear: A system for extracting oblique decision rules from neural networks\n", "abstract": " We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail using a heart disease diagnosis problem. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.", "num_citations": "11\n", "authors": ["1157"]}
{"title": "Selenium promotes metabolic conversion of T-2 toxin to HT-2 toxin in cultured human chondrocytes\n", "abstract": " To explore the metabolism of T-2 toxin in human chondrocytes (HCs) and determine the impact of selenium supplementation. For determination of cytotoxicity using the MTT assay, optical density values were read with an automatic enzyme-linked immunosorbent assay reader at 510\u202fnm. Cell survival was calculated and the cytotoxicity estimated. To identify the metabolites of T-2 toxin, the medium supernatants and C28/I2 cells were analyzed by high-performance liquid chromatography tandem mass spectrometry (HPLC\u2013MS/MS) separately. For HPLC\u2013MS/MS, the mobile phase A was water and phase B was 98% methanol. The gradient for the elution was: 0\u20130.5\u202fmin, 50% of B; 0.5\u20132.0\u202fmin, 100% of B; 2.0\u20133.5\u202fmin, 100% of B; 3.6\u20136\u202fmin, 50% of B. T-2 toxin increased the toxicity to C28/I2 cells significantly in a dose- and time-dependent manner (viability range 91.5\u201322.0%). Supplementation with selenium (100\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "Facilitating time critical information seeking in social media\n", "abstract": " Social media plays a major role in helping people affected by natural calamities. These people use social media to request information and help in situations where time is a critical commodity. However, generic social media platforms like Twitter and Facebook are not conducive for obtaining answers promptly. Algorithms to ensure prompt responders for questions in social media have to understand and model the factors affecting their response time. In this paper, we draw from sociological studies on information seeking and organizational behavior to identify users who can provide timely and relevant responses to questions posted on social media. We first draw from these theories to model the future availability and past response behavior of the candidate responders and integrate these criteria with user relevance. We propose a learning algorithm from these criteria to derive optimal rankings of responders for a\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "FeatureMiner: a tool for interactive feature selection\n", "abstract": " The recent popularity of big data has brought immense quantities of high-dimensional data, which presents challenges to traditional data mining tasks due to curse of dimensionality. Feature selection has shown to be effective to prepare these high dimensional data for a variety of learning tasks. To provide easy access to feature selection algorithms, we provide an interactive feature selection tool FeatureMiner based on our recently released feature selection repository scikit-feature. FeatureMiner eases the process of performing feature selection for practitioners by providing an interactive user interface. Meanwhile, it also gives users some practical guidance in finding a suitable feature selection algorithm among many given a specific dataset. In this demonstration, we show (1) How to conduct data preprocessing after loading a dataset;(2) How to apply feature selection algorithms;(3) How to choose a suitable\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "Embassies burning: toward a near-real-time assessment of social media using geo-temporal dynamic network analytics\n", "abstract": " Effective crisis response requires rapid assessment of a situation in order to form actionable plans. Social media and traditional media are critical to this assessment. This paper describes a rapid ethnographic approach for extracting information from Twitter and news media and then assessing that information using dynamic network analysis techniques. Text mining high-dimensional network analytics and visualization are combined to provide an integrated approach to assessing large dynamic networks. This approach was used as the Benghazi consulate and the Egyptian embassy were attacked in 2012. This near-real-time assessment was set against a backdrop of ongoing data collection associated with the Arab Spring countries. This ongoing collection provided a baseline for Libya and Egypt against which the new data could be assessed. Herein, the outcome of that near-real-time assessment, the\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "Temporal dynamics in social trust prediction\n", "abstract": " Inferring unknown social trust relations attracts increasing attention in recent years. However, social trust, as a social concept, is intrinsically dynamic, and exploiting temporal dynamics provides challenges and opportunities for social trust prediction. In this paper, we investigate social trust prediction by exploiting temporal dynamics. In particular, we model the dynamics of user preferences in two principled ways. The first one focuses on temporal weight; the second one targets temporal smoothness. By incorporating these two types of temporal dynamics into traditional matrix factorization based social trust prediction model, two extended social trust prediction models are proposed and the corresponding algorithms to solve the models are designed too. We conduct experiments on a real-world dataset and the results demonstrate the effectiveness of our proposed new models. Further experiments are also\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "A tool for assisting provenance search in social media\n", "abstract": " In recent years, social media sites are witnessing an information explosion. Determining the reliability of such a large amount of information is a major area of research. Information provenance (aka, sources or origin) provides a way to measure the reliability of information in social networks. The main challenge in seeking provenance is the availability of suitable data consisting of sufficient unique propagation paths. Knowledge of the actual propagation paths for a piece of information will be a valuable asset in provenance search. This paper presents a tool for capturing the propagation network of a given tweet or URL (Uniform Resource Locator) in the Twitter network. Researchers can use this tool to collect information propagation data, design effective strategies for determining the provenance, and gain information about the tweet such as impact, growth rate and users influencing the spread. Two case studies are\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "Network denoising in social media\n", "abstract": " Social media expands the ways people communicate with each other. On a popular social media website, a user typically has hundreds of contacts (or friends) on average. As a person's social network grows, friend management is increasingly important for effective communications. Often, one can only afford to maintain close friendship in a small scale due to limited time and other resources. In other words, the majority of one's connections are so-so friends and do not hold strong influence on the user. One approach resorts to network denoising, by which unimportant connections are removed as noise. We study the challenges of network denoising in social media and how we can leverage a variety of social media information to denoise the links. We formulate the network denoising task as an optimization problem, and show the efficacy of our network denoising approach and its scalability experimentally in the\u00a0\u2026", "num_citations": "10\n", "authors": ["1157"]}
{"title": "Few-shot Network Anomaly Detection via Cross-network Meta-learning\n", "abstract": " Network anomaly detection, also known as graph anomaly detection, aims to find network elements (eg, nodes, edges, subgraphs) with significantly different behaviors from the vast majority. It has a profound impact in a variety of applications ranging from finance, healthcare to social network analysis. Due to the unbearable labeling cost, existing methods are predominately developed in an unsupervised manner. Nonetheless, the anomalies they identify may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies of interest. Hence, it is critical to investigate and develop few-shot learning for network anomaly detection. In real-world scenarios, few labeled anomalies are also easy to be accessed on similar networks from the same domain as of the target network, while most of the existing works omit to leverage them and merely focus on a single network. Taking\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Unsupervised cyberbullying detection via time-informed Gaussian mixture model\n", "abstract": " Social media is a vital means for information-sharing due to its easy access, low cost, and fast dissemination characteristics. However, increases in social media usage have corresponded with a rise in the prevalence of cyberbullying. Most existing cyberbullying detection methods are supervised and, thus, have two key drawbacks: (1) The data labeling process is often time-consuming and labor-intensive; (2) Current labeling guidelines may not be generalized to future instances because of different language usage and evolving social networks. To address these limitations, this work introduces a principled approach for unsupervised cyberbullying detection. The proposed model consists of two main components: (1) A representation learning network that encodes the social media session by exploiting multi-modal features, e.g., text, network, and time. (2) A multi-task learning network that simultaneously fits the comment inter-arrival times and estimates the bullying likelihood based on a Gaussian Mixture Model. The proposed model jointly optimizes the parameters of both components to overcome the shortcomings of decoupled training. Our core contribution is an unsupervised cyberbullying detection model that not only experimentally outperforms the state-of-the-art unsupervised models, but also achieves competitive performance compared to supervised models.", "num_citations": "9\n", "authors": ["1157"]}
{"title": "To your surprise: Identifying serendipitous collaborators\n", "abstract": " Scientific collaboration has become a universal phenomenon in recent years. Meanwhile, scholars tend to hunt for surprising collaborators for broadening their horizons. Serendipity initially denotes the fortunate discovery. Although a lot of literature is available on the topic of serendipity, little research has investigated serendipity in scientific collaborations. The objective of this paper is to identify serendipitous scientific collaborators of target scholars based on their collaboration data. First, we induce the definition of serendipitous scientific collaborators by three components, which are relevance, unexpectedness, and value, respectively. They are quantified as three intuitive indices corresponding to the network proximity, topic diversity, and collaborator influence, respectively. Second, we propose a classification model, called RUVMod, to classify all collaborators based on the analysis of three indices in definition\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "A comprehensive study on the weak magnetic sensor character of different geometries for proton precession magnetometer\n", "abstract": " The measurement precision or uncertainty of proton precession magnetometer (PPM) depends on the signal-to-noise ratio (SNR) of the free induction decay (FID) signal induced in the sensing coil. A diversity of weak magnetic measurement applications has benefited from various coils through a more comprehensive and reliable design. Likewise, numerous optimized sensors for PPM have been proposed and published in literature. However, due to lack of commonly accepted assessment measures and benchmark resources, it is hard to identify the performance of the proposed sensors and corresponding implementations. This paper investigates and categorizes sensing coils for PPM which are well used and accepted. Three state-of-the-art geometries of the coil including solenoid, toroid, and cylindric, are considered to evaluate the significance of these designs in terms of sensing signal strength analyses within\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Understanding and predicting delay in reciprocal relations\n", "abstract": " Reciprocity in directed networks points to user\u00bb s willingness to return favors in building mutual interactions. High reciprocity has been widely observed in many directed social media networks such as following relations in Twitter and Tumblr. Therefore, reciprocal relations between users are often regarded as a basic mechanism to create stable social ties and play a crucial role in the formation and evolution of networks. Each reciprocity relation is formed by two parasocial links in a back-and-forth manner with a time delay. Hence, understanding the delay can help us gain better insights into the underlying mechanisms of network dynamics. Meanwhile, the accurate prediction of delay has practical implications in advancing a variety of real-world applications such as friend recommendation and marketing campaign. For example, by knowing when will users follow back, service providers can focus on the users with a\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Cloud information retrieval: Model description and scheme design\n", "abstract": " The fast development of cloud technology has brought about a new trend in the field of information service: more and more information is being transferred to the cloud as requested. However, the data, such as texts, images, sounds, and videos, before being moved to the cloud, in most cases, has to be encrypted so that intelligible information will not be obtained from unauthorized accesses. While having done a nice work in protecting the data privacy of its owners, this encrypting process, has produced a great challenge for retrieval of the document stored via traditional IR model based on document, query and relevance. In order to retrieve encrypted information from cloud, an alternative retrieval system is needed. To satisfy such a need, we have: 1) build a cloud information retrieval framework characterized by its retrieval risk formula, which, enables, for the very first time to the best of our knowledge, an effective\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Randomized feature engineering as a fast and accurate alternative to kernel methods\n", "abstract": " Feature engineering has found increasing interest in recent years because of its ability to improve the effectiveness of various machine learning models. Although tailored feature engineering methods have been designed for various domains, there are few that simulate the consistent effectiveness of kernel methods. At the core, the success of kernel methods is achieved by using similarity functions that emphasize local variations in similarity. Unfortunately, this ability comes at the price of the high level of computational resources required and the inflexibility of the representation as it only provides the similarity of two data points instead of vector representations of each data point; while the vector representations can be readily used as input to facilitate various models for different tasks. Furthermore, kernel methods are also highly susceptible to overfitting and noise and it cannot capture the variety of data locality. In\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Leveraging social foci for information seeking in social media\n", "abstract": " The rise of social media provides a great opportunity for people to reach out to their social connections to satisfy their information needs. However, generic social media platforms are not explicitly designed to assist information seeking of users. In this paper, we propose a novel framework to identify the social connections of a user able to satisfy his information needs. The information need of a social media user is subjective and personal, and we investigate the utility of his social context to identify people able to satisfy it. We present questions users post on Twitter as instances of information seeking activities in social media. We infer soft community memberships of the asker and his social connections by integrating network and content information. Drawing concepts from the social foci theory, we identify answerers whose community memberships in the question domain overlap with that of the asker. Our experiments demonstrate that the framework is effective in identifying answerers to social media questions.", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Behavior analysis in social media\n", "abstract": " With the rise of social media, information sharing has been democratized. As a result, users are given opportunities to exhibit different behaviors such as sharing, posting, liking, commenting, and befriending conveniently and on a daily basis. By analyzing behaviors observed on social media, we can categorize these behaviors into individual and collective behavior. Individual behavior is exhibited by a single user, whereas collective behavior is observed when a group of users behave together. For instance, users using the same hashtag on Twitter or migrating to another social media site are examples of collective behavior. User activities on social media generate behavioral data, which is massive, expansive, and indicative of user preferences, interests, opinions, and relationships. This behavioral data provides a new lens through which we can observe and analyze individual and collective behaviors of users. The emergence of this new type of data presents behavior analysis on social media with new challenges. We detail first what individual and collective behavior analysis is, and then outline novel challenges with future work.", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Finding friends on a new site using minimum information\n", "abstract": " With the emergence of numerous social media sites, individuals, with their limited time, often face a dilemma of choosing a few sites over others. Users prefer more engaging sites, where they can find familiar faces such as friends, relatives, or colleagues. Link prediction methods help find friends using link or content information. Unfortunately, whenever users join any site, they have no friends or any content generated. In this case, sites have no chance other than recommending random influential users to individuals hoping that users by befriending them create sufficient information for link prediction techniques to recommend meaningful friends. In this study, by considering social forces that form friendships, namely, influence, homophily, and confounding, and by employing minimum information available for users, we demonstrate how one can significantly improve random predictions without link or content\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Information diffusion in social media\n", "abstract": " In February 2013, during the third quarter of Super Bowl XLVII, a power outage stopped the game for 34 minutes. Oreo, a sandwich cookie company, tweeted during the outage:\u201cPower out? No Problem, You can still dunk it in the dark.\u201d The tweet caught on almost immediately, reaching nearly 15,000 retweets and 20,000 likes on Facebook in less than two days. A simple tweet diffused into a large population of individuals. It helped the company gain fame with minimum cost in an environment where companies spent as much as $4 million to run a 30-second ad. This is an example of information diffusion.Information diffusion is a field encompassing techniques from a plethora of sciences. In this chapter, we discuss methods from fields such as sociology, epidemiology, and ethnography, which can help social media mining. Our focus is on techniques that can model information diffusion. Societies provide means for individuals to exchange information through various channels. For instance, people share knowledge with their immediate network (friends) or broadcast it via public media (TV, newspapers, etc.) throughout the society. Given this flow of information, different research fields have disparate views of what is an information diffusion process. We define information diffusion as the process by which a piece of information (knowledge) is spread and reaches individuals through interactions. The diffusion process involves the following three elements:", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Network measures\n", "abstract": " In February 2012, Kobe Bryant, the American basketball star, joined Chinese microblogging site Sina Weibo. Within a few hours, more than 100,000 followers joined his page, anxiously waiting for his first microblogging post on the site. The media considered the tremendous number of followers Kobe Bryant received as an indication of his popularity in China. In this case, the number of followers measured Bryant\u2019s popularity among Chinese social media users. In social media, we often face similar tasks in which measuring different structural properties of a social media network can help us better understand individuals embedded in it. Corresponding measures need to be designed for these tasks. This chapter discusses measures for social media networks. When mining social media, a graph representation is often used. This graph shows friendships or user interactions in a social media network. Given this graph, some of the questions we aim to answer are as follows:", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Ensemble feature selection in face recognition: ICMLA 2012 challenge\n", "abstract": " Ensemble feature selection is known for its robustness and generalization of highly accurate predictive models. In this paper, we use different filter-based feature selection methods in an ensemble manner to improve face recognition. The goal is to distinguish human faces from avatar faces. Our approach was able to achieve very high accuracy, 99%, using less than 1% of the pixels in each image. This was obtained after removing irrelevant features which is known to degrade learning performance and model stability.", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Extracting geographic knowledge from sensor intervention data using spatial association rules\n", "abstract": " Large networks of sensors are used to detect intrusions and provide security at the borders of the United States. Sensor signals are used to detect possible intrusions such as illegal immigration traffic in drugs, weapons, and smuggled goods at specific targeted geographic locations. GIS systems can be used to capture, store and analyze this location based intervention data. Using a GIS system, a spatial database can be generated from the sensor intervention data which can take into account relevant geographic information in the vicinity of the sensed interventions. Important geographic features that are close to the intervention locations such as: plateaus, hills, valleys or roadways can be extracted and added to the analysis using ArcGIS. GIS techniques alone cannot reveal meaningful hidden information within geographic data. We have developed an integrated approach involving data mining and GIS techniques\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Convergence of influential bloggers for topic discovery in the blogosphere\n", "abstract": " In this paper, we propose a novel approach to automatically detect \u201chot\u201d or important topics of discussion in the blogosphere. The proposed approach is based on analyzing the activity of influential bloggers to determine specific points in time when there is a convergence amongst the influential bloggers in terms of their topic of discussion. The tool BlogTrackers, is used to identify influential bloggers and the Normalized Google Distance is used to define the similarity amongst the topics of discussion of influential bloggers. The key advantage of the proposed approach is its ability to automatically detect events which are important in the blogger community.", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Quantifying utility and trustworthiness for advice shared on online social media\n", "abstract": " The growing popularity of social media in recent years has resulted in the creation of an enormous amount of user-developed content. While information is readily available, there is no easy way to find the most useful content or to detect whether it is trustworthy. A casual observer might not be able to differentiate between the useful and the useless or the trustworthy and the untrustworthy. In this work, we wish to study the problem of quantifying the value of such user-shared content. In particular, we are focussed on health content as the negative impacts are higher for this domain. We use advice shared on a health social network, Daily Strength, for this study. We describe and define the notions of trustworthiness and utility for social media content. We identify the necessity and challenges for their assessment, and propose a framework that helps address these challenges by identifying relevant features and providing\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Aggressive feature selection by feature ranking\n", "abstract": " Recently, text classification has become one of the fastest growing applications of machine learning and data mining [15]. There are many applications that use text classification techniques, such as natural language processing and information retrieval [9]. All of these applications use text classification techniques in dealing with natural language documents. Since text classification is a supervised learning process, a good many learning methods such as K-nearest neighbor (KNN), regression models, na\u0131ve Bayes classifier (NBC), decision trees, inductive rule learning, neural networks, and support vector machines (SVM) can be employed [1]. Most text classification algorithms use vector space model, and bag-ofwords representation, as proposed by Salton [22], to model textual documents. Some extensions of the vector space model have also been proposed that utilize the semantic and syntactic relationships\u00a0\u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Dimensionality Reduction for Data Mining-Techniques, Applications and Trends\n", "abstract": " Dimensionality Reduction for Data Mining Page 1 Dimensionality Reduction for Data Mining - Techniques, Applications and Trends Lei Yu Binghamton University Jieping Ye, Huan Liu Arizona State University Page 2 2 Outline \u220e Introduction to dimensionality reduction \u220e Feature selection (part I) \u2751 Basics \u2751 Representative algorithms \u2751 Recent advances \u2751 Applications \u220e Feature extraction (part II) \u220e Recent trends in dimensionality reduction Page 3 3 Why Dimensionality Reduction? \u220e It is so easy and convenient to collect data \u2751 An experiment \u220e Data is not collected only for data mining \u220e Data accumulates in an unprecedented speed \u220e Data preprocessing is an important part for effective machine learning and data mining \u220e Dimensionality reduction is an effective approach to downsizing data Page 4 4 \u220e Most machine learning and data mining techniques may not be effective for highdimensional data \u2751 Curse of \u2026", "num_citations": "9\n", "authors": ["1157"]}
{"title": "Adversarial Attacks and Defenses: An Interpretation Perspective\n", "abstract": " Despite the recent advances in a wide spectrum of applications, machine learning models, especially deep neural networks, have been shown to be vulnerable to adversarial attacks. Attackers add carefully-crafted perturbations to input, where the perturbations are almost imperceptible to humans, but can cause models to make wrong predictions. Techniques to protect models against adversarial input are called adversarial defense methods. Although many approaches have been proposed to study adversarial attacks and defenses in different scenarios, an intriguing and crucial challenge remains that how to really understand model vulnerability? Inspired by the saying that \"if you know yourself and your enemy, you need not fear the battles\", we may tackle the challenge above after interpreting machine learning models to open the black-boxes. The goal of model interpretation, or interpretable machine learning, is\u00a0\u2026", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Detecting Crowdturfing in Social Media.\n", "abstract": " Crowdturfing in Social Media is the crowdturfing activities that regard social networking platforms as the main information channel of the campaign. Crowdturfing workers use social media accounts to spread information and may result in unfair popular popularity, such as a hijacked trending topic, in social networks.", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Variants of CARD14 gene and psoriasis vulgaris in southern Chinese cohort\n", "abstract": " BACKGROUND: Recent mutation analysis identified several missense mutations in CARD14 in psoriasis. OBJECTIVES: We performed the genomic sequence analysis on CARD14 in southern Chinese Han Cantonese with Psoriasis Vulgaris (PsV) to reveal more causative missense mutations. METHODS: A total of 131 patients with PsV and 207 matched controls were included. We conducted sequence analysis of all the exon and exon-intron boundaries of CARD14 in the group of PsV patients and subsequent case control analysis of potential sequence variants of significance. RESULTS: We found five rare mutations and four of them are annotated or reported. Only the variant (c.1291C>G) has not been reported and annotated, but the variant was also found in controls. No significant difference was detected among all rare variant allele frequencies of patients and controls. CONCLUSION: None of the new definite variants were pathogenic. The other pathogenic mutations for PsV are still elusive in our cohort.", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Finding the right social media site for questions\n", "abstract": " Social media has become a part of our daily life and we use it for many reasons. One of its uses is to get our questions answered. Given a multitude of social media sites, however, one immediate challenge is to pick the most relevant site for a question. This is a challenging problem because (1) questions are usually short, and (2) social media sites evolve. In this work, we propose to utilize topic specialization to find the most relevant social media site for a given question. In particular, semantic knowledge is considered for topic specialization as it can not only make a question more specific, but also dynamically represent the content of social sites, which relates a given question to a social media site. Thus, we propose to rank social media sites based on combined search engine query results. Our algorithm yields compelling results for providing a meaningful and consistent site recommendation. This work helps\u00a0\u2026", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Automatically adjusting content taxonomies for hierarchical classification\n", "abstract": " Hierarchical models have been shown to be effective in content classification. However, the performance of the model heavily depends on the given hierarchical taxonomy. We empirically show that different taxonomies can result in significant differences in hierarchical classification performance. Motivated by some real application problems, we aim to modify a content taxonomy automatically for different applications. In this work, we formulate the problem, discuss why it is feasible to achieve better performance in terms of classification performance via adjusting a given hierarchy, and present one effective solution to find better hierarchies compared with that of the given original hierarchy. Preliminary experiments on some real world data sets are reported and discussed.", "num_citations": "8\n", "authors": ["1157"]}
{"title": "A Novel Approach to Model Generation for Heterogeneous Data Classification.\n", "abstract": " Ensemble methods such as bagging and boosting have been successfully applied to classification problems. Two important issues associated with an ensemble approach are: how to generate models to construct an ensemble, and how to combine them for classification. In this paper, we focus on the problem of model generation for heterogeneous data classification. If we could partition heterogeneous data into a number of homogeneous partitions, we will likely generate reliable and accurate classification models over the homogeneous partitions. We examine different ways of forming homogeneous subsets and propose a novel method that allows a data point to be assigned multiple times in order to generate homogeneous partitions for ensemble learning. We present the details of the new algorithm and empirical studies over the UCI benchmark datasets and datasets of image classification, and show that the proposed approach is effective for heterogeneous data classification.", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Robust feature induction for support vector machines\n", "abstract": " The goal of feature induction is to automatically create nonlinear combinations of existing features as additional input features to improve classification accuracy. Typically, nonlinear features are introduced into a support vector machine (SVM) through a nonlinear kernel function. One disadvantage of such an approach is that the feature space induced by a kernel function is usually of high dimension and therefore will substantially increase the chance of over-fitting the training data. Another disadvantage is that nonlinear features are induced implicitly and therefore are difficult for people to understand which induced features are critical to the classification performance. In this paper, we propose a boosting-style algorithm that can explicitly induces important nonlinear features for SVMs. We present empirical studies with discussion to show that this approach is effective in improving classification accuracy for SVMs\u00a0\u2026", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Efficient yet accurate clustering\n", "abstract": " The authors show that most hierarchical agglomerative clustering (HAC) algorithms follow a 90-10 rule where roughly 90% iterations from the beginning merge cluster pairs with dissimilarity less than 10% of the maximum dissimilarity. We propose two algorithms: 2-phase and nested, based on partially overlapping partitioning (POP). To handle high-dimensional data efficiently, we propose a tree structure particularly suitable for POP. Extensive experiments show that the proposed algorithms reduce the time and memory requirement of existing HAC algorithms significantly without compromising accuracy.", "num_citations": "8\n", "authors": ["1157"]}
{"title": "Rule mining with prior knowledge\u2013a belief networks approach\n", "abstract": " Some existing data mining methods, such as classification trees, neural networks and association rules, have the drawbacks that the user's prior knowledge cannot be easily specified and incorporated into the knowledge discovery process, and the rules mined from databases lack quantitative analyses. In this paper, we propose a belief networks method for rule mining, which takes the advantage of belief networks as the directed acyclic graph language and their function for numerical representation of probabilistic dependencies among the variables in the database, so that it can overcome the drawbacks. Since belief networks provide a natural representation for capturing causal relationship among a set of variables, our proposed method can mine more general correlation rules which can capture the relationship of more than two attribute variables. The potential application of the proposed method is\u00a0\u2026", "num_citations": "8\n", "authors": ["1157"]}
{"title": "An intelligent micro-fluidic system for drug delivery\n", "abstract": " This paper describes the development and characterization of a micro-fluidic system which comprises of a micro-pump, passive micro-valve and its control circuit. Some applications for such a system include micro-coolant systems, micro-chemical analysis systems and fluid handling systems. Micro-fluidic systems are generally application specific and the focus of the proposed system is for drug delivery. The micro-pump is of the reciprocating membrane type and is based on piezoelectric actuation. It can be manufactured using MEMS fabrication technology such as silicon micro-machining. The pump utilizes check valves made of photosensitive polyimide as the rectifying unit and a O10 mm piezoelectric diaphragm as the actuator unit. The theoretical analysis for the actuator and valve characteristics is presented in this paper. The resulting effects on the flow characteristics and performance are also presented\u00a0\u2026", "num_citations": "8\n", "authors": ["1157"]}
{"title": "IGNITE: A minimax game toward learning individual treatment effects from networked observational data\n", "abstract": " Networked observational data presents new opportunities for learning individual causal effects, which plays an indispensable role in decision making. Such data poses the challenge of confounding bias. Previous work presents two desiderata to handle confounding bias. On the treatment group level, we aim to balance the distributions of confounder representations. On the individual level, it is desirable to capture patterns of hidden confounders that predict treatment assignments. Existing methods show the potential of utilizing network information to handle confounding bias, but they only try to satisfy one of the two desiderata. This is because the two desiderata seem to contradict each other. When the two distributions of confounder representations are highly overlapped, then we confront the undiscriminating problem between the treated and the controlled. In this work, we formulate the two desiderata as a minimax game. We propose IGNITE that learns representations of confounders from networked observational data, which is trained by a minimax game to achieve the two desiderata. Experiments verify the efficacy of IGNITE on two datasets under various settings.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Socially responsible ai algorithms: Issues, purposes, and challenges\n", "abstract": " In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, health care, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI's indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Session-based cyberbullying detection: Problems and challenges\n", "abstract": " Cyberbullying has become one of the most pressing online risks for young people, due in part to the rapid increase in social media use, and has raised serious concerns in society. Existing studies have examined various approaches to cyberbullying detection focusing on a single piece of text, whereas relatively little is known about cyberbullying detection within a  social media session . A social media session typically consists of an initial post, images/videos, a sequence of comments that involves user interactions, user information, spatial location, and other social content. By investigating cyberbullying at the level of social media sessions, researchers can draw on data that are more complex, diverse, and crucial for understanding two defining characteristics of cyberbullying, in particular:  repetitive acts  and  power imbalance . This article thus highlights the importance of studying session-based cyberbullying\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Detecting fake news with weak social supervision\n", "abstract": " Limited labeled data are becoming one of the largest bottlenecks for supervised learning systems. This is especially the case for many real-world tasks, where large-scale labeled examples are either too expensive to acquire or unavailable due to privacy or data access constraints. Weak supervision has shown to be effective in mitigating the scarcity of labeled data by leveraging weak labels or injecting constraints from heuristic rules and/or extrinsic knowledge sources. Social media has little labeled data but possesses unique characteristics that make it suitable for generating weak supervision, resulting in a new type of weak supervision, i.e., weak social supervision. In this article, we illustrate how various aspects of social media can be used as weak social supervision. Specifically, we use the recent research on fake news detection as the use case, where social engagements are abundant but annotated\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Social science\u2013guided feature engineering: A novel approach to signed link analysis\n", "abstract": " Many real-world relations can be represented by signed networks with positive links (e.g., friendships and trust) and negative links (e.g., foes and distrust). Link prediction helps advance tasks in social network analysis such as recommendation systems. Most existing work on link analysis focuses on unsigned social networks. The existence of negative links piques research interests in investigating whether properties and principles of signed networks differ from those of unsigned networks and mandates dedicated efforts on link analysis for signed social networks. Recent findings suggest that properties of signed networks substantially differ from those of unsigned networks and negative links can be of significant help in signed link analysis in complementary ways. In this article, we center our discussion on a challenging problem of signed link analysis. Signed link analysis faces the problem of data sparsity, i.e., only\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Toward keyword extraction in constrained information retrieval in vehicle social network\n", "abstract": " With the emergence of various vehicular apps for automotive navigation, driving safety, and in-car entertainment system, vehicular social data is starting to explode. Such an explosion has posed a great challenge for a centralized pattern of computing, storing, and managing information, calling for a more decentralized (cloud-based) style of processing data for vehicular social network. Decentralized information processing requires encryption of sensitive information prior to out-sourcing for the purpose of protecting data from unsolicited access and thus a searchable keyword index needs to be extracted from the encrypted document for solicited access. However, most existing searchable encryption schemes, based on the words in the subject or content, suffers from poor retrieval performance. In this study, a novel keyword extraction metric based on spatial distribution of a particular text is proposed with the view to\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Real-time crisis mapping using language distribution\n", "abstract": " With the increase in GPS-enabled devices, social media sites, such as Twitter, are quickly becoming a prime outlet for timely geo-spatial data. Such data can be leveraged to aid in emergency response planning and recovery operations. Unfortunately, the information overload poses significant difficulty to the quick discovery and identification of emergency situation areas. The system tackles this challenge by providing real-time mapping of influence areas based on automatic analysis of the flow of discussion using language distributions. The workflow is then further enhanced through the addition of keyword surprise mapping which projects the general divergence map onto specific task-level keywords for precise and focused response.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "A study of tagging behavior across social media\n", "abstract": " Social bookmarking services enable users to share, manage, and search bookmarks. Bookmarks can be organized by annotating them with tags. A deeper understanding of tagging behavior across social media would help in tag usage modeling, tag recommendation, and cross-media user information integration. In this study, we perform a cross-site study of user tagging behavior in two popular social bookmarking sites, StumbleUpon and Delicious. Our study suggests that user behavior significantly varies across sites, thus, shedding light on the future development of tag usage modeling and recommendation across social media.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Understanding the effects of sampling on healthcare risk modeling for the prediction of future high-cost patients\n", "abstract": " Rapidly rising healthcare costs represent one of the major issues plaguing the healthcare system. Data from the Arizona Health Care Cost Containment System, Arizona\u2019s Medicaid program provide a unique opportunity to exploit state-of-the-art machine learning and data mining algorithms to analyze data and provide actionable findings that can aid cost containment. Our work addresses specific challenges in this real-life healthcare application with respect to data imbalance in the process of building predictive risk models for forecasting high-cost patients. We survey the literature and propose novel data mining approaches customized for this compelling application with specific focus on non-random sampling. Our empirical study indicates that the proposed approach is highly effective and can benefit further research on cost containment in the healthcare industry.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Local Feature Selection for Classi\ufb01cation\n", "abstract": " In a classi\ufb01cation problem, we are given C classes and M training observations. The training observations consist of N feature measurements x=(x1,\u00b7\u00b7\u00b7, xN) T\u2208 N and the known class labels y= 1,..., C. The goal is to predict the class label of a given query x0.ABSTRACT", "num_citations": "7\n", "authors": ["1157"]}
{"title": "SWITCH: A novel approach to ensemble learning for heterogeneous data\n", "abstract": " The standard framework of machine learning problems assumes that the available data is independent and identically distributed (i.i.d.). However, in some applications such as image classification, the training data are often collected from multiple sources and heterogeneous. Ensemble learning is a proven effective approach to heterogeneous data, which uses multiple classification models to capture the diverse aspects of heterogeneous data. If an ensemble can learn the relationship between different portions of data and their corresponding models, the ensemble can selectively apply models to unseen data according to the learned relationship. We propose a novel approach to enable the learning of the relationships between data and models by creating a set of \u2018switches\u2019 that can route a testing instance to appropriate classification models in an ensemble. Our empirical study on both real-world data\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Knowledge Discovery and Data Mining. Current Issues and New Applications 4th Pacific-Asia Conference, PAKDD 2000 Kyoto, Japan, April 18\u201320, 2000 Proceedings\n", "abstract": " The Fourth Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2000) was held at the Keihanna-Plaza, Kyoto, Japan, April 18-20, 2000. PAKDD 2000 provided an international forum for researchers and applica tion developers to share their original research results and practical development experiences. A wide range of current KDD topics were covered including ma chine learning, databases, statistics, knowledge acquisition, data visualization, knowledge-based systems, soft computing, and high performance computing. It followed the success of PAKDD 97 in Singapore, PAKDD 98 in Austraha, and PAKDD 99 in China by bringing together participants from universities, indus try, and government from all over the world to exchange problems and challenges and to disseminate the recently developed KDD techniques. This PAKDD 2000 proceedings volume addresses both current issues\u00a0\u2026", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Efficient rule induction from noisy data\n", "abstract": " The single-pattern-based rule induction method is sensitive to the order of data. Naturally, it is not suitable for data with noise. This paper reports a new rule induction method of this kind that handles noise effectively. Experiments are conducted to show that rules generated are compact and accurate, and the method is efficient and handles noise effectively. Its computational complexity is also given for comparison with other methods and as a guide for future application of this method.", "num_citations": "7\n", "authors": ["1157"]}
{"title": "Collaborative filtering with network representation learning for citation recommendation\n", "abstract": " Citation recommendation is important in the environment of scholarly big data, where finding relevant papers has become more difficult because of information overload. Applying traditional collaborative filtering (CF) to citation recommendation is rather challenging due to the cold start problem and the lack of paper ratings. To address these two challenges, in this paper, we propose a Collaborative filtering with Network representation learning framework for Citation Recommendation dubbed as CNCRec, which is a hybrid user-based CF considering both paper content and network topology. It aims at recommending citations in heterogeneous academic information networks. CNCRec creates the paper rating matrix based on attributed citation network representation learning, where the attributes are topics extracted from the paper text information. Meanwhile, the learned representations of attributed collaboration\u00a0\u2026", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Challenges in Combating COVID-19 Infodemic--Data, Tools, and Ethics\n", "abstract": " While the COVID-19 pandemic continues its global devastation, numerous accompanying challenges emerge. One important challenge we face is to efficiently and effectively use recently gathered data and find computational tools to combat the COVID-19 infodemic, a typical information overloading problem. Novel coronavirus presents many questions without ready answers; its uncertainty and our eagerness in search of solutions offer a fertile environment for infodemic. It is thus necessary to combat the infodemic and make a concerted effort to confront COVID-19 and mitigate its negative impact in all walks of life when saving lives and maintaining normal orders during trying times. In this position paper of combating the COVID-19 infodemic, we illustrate its need by providing real-world examples of rampant conspiracy theories, misinformation, and various types of scams that take advantage of human kindness, fear, and ignorance. We present three key challenges in this fight against the COVID-19 infodemic where researchers and practitioners instinctively want to contribute and help. We demonstrate that these three challenges can and will be effectively addressed by collective wisdom, crowdsourcing, and collaborative research.", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Bot Detection: Will Focusing on Recall Cause Overall Performance Deterioration?\n", "abstract": " Social bots are an effective tool in the arsenal of malicious actors who manipulate discussions on social media. Bots help spread misinformation, promote political propaganda, and inflate the popularity of users and content. Hence, it is necessary to differentiate bot accounts and human users. There are several bot detection methods that approach this problem. Conventional methods either focus on precision regardless of the overall performance or optimize overall performance, say , without monitoring its effect on precision or recall. Focusing on precision means that those users marked as bots are more likely than not bots but a large portion of the bots could remain undetected. From a user\u2019s perspective, however, it is more desirable to have less interaction with bots, even if it would incur a loss in precision. This can be achieved by a detection method with higher recall. A trivial, but useless, solution for\u00a0\u2026", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Individual and combined toxicity of T\u20102 toxin and deoxynivalenol on human C\u201028/I2 and rat primary chondrocytes\n", "abstract": " Deoxynivalenol (DON) and T\u20102 toxin are prevalent mycotoxin contaminants in the food and feed stuffs worldwide, with non\u2010negligible co\u2010contamination and co\u2010exposure conditions. Meanwhile, they are considerable risk factors for Kashin\u2010Beck disease, a chronic endemic osteochondropathy. The aim of this study was to investigate the individual and combined cytotoxicity of DON and T\u20102 toxin on proliferating human C\u201028/I2 and newborn rat primary costal chondrocytes by MTT assay. Four molar concentration combination ratios of DON and T\u20102 toxin were used, 1:1 for R1 mixture, 10:1 for R10, 100:1 for R100 and 1000:1 for R1000. The toxicological interactions were quantified by the MixLow method. DON, T\u20102 toxin, and their mixtures all showed a clear dose\u2010dependent toxicity for chondrocytes. The cytotoxicity of T\u20102 toxin was 285\u2010fold higher than DON was in human chondrocytes, and 22\u2010fold higher in the rat\u00a0\u2026", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Text, topics, and turkers: A consensus measure for statistical topics\n", "abstract": " Topic modeling is an important tool in social media analysis, allowing researchers to quickly understand large text corpora by investigating the topics underlying them. One of the fundamental problems of topic models lies in how to assess the quality of the topics from the perspective of human interpretability. How well can humans understand the meaning of topics generated by statistical topic modeling algorithms? In this work we advance the study of this question by introducing Topic Consensus: a new measure that calculates the quality of a topic through investigating its consensus with some known topics underlying the data. We view the quality of the topics from three perspectives: 1) topic interpretability, 2) how documents relate to the underlying topics, and 3) how interpretable the topics are when the corpus has an underlying categorization. We provide insights into how well the results of Mechanical Turk\u00a0\u2026", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Online diffusion source detection in social networks\n", "abstract": " In this paper we study a new problem of online diffusion source detection in social networks. Existing work on diffusion source detection focuses on offline learning, which assumes data collected from network detectors are static and a snapshot of network is available before learning. However, an offline learning model does not meet the needs of early warning, real-time awareness, and real-time response of malicious information spreading in social networks. In this paper, we combine online learning and regression-based detection methods for real-time diffusion source detection. Specifically, we propose a new \u2113 1  non-convex regression model as the learning function, and an Online Stochastic Sub-gradient algorithm (OSS for short). The proposed model is empirically evaluated on both synthetic and real-world networks. Experimental results demonstrate the effectiveness of the proposed model.", "num_citations": "6\n", "authors": ["1157"]}
{"title": "An efficient privacy preserving location based service system\n", "abstract": " Location based service is an indispensable part of today's mobile era. While it brings a lot of benefits to people, the breach to individual location privacy is always a concern and impedes the smooth development of location based service. A user can be easily tracked once she subscribes or uses the service from an untrusted location based service server. In this paper, we try to address this problem by proposing a secure and efficient location based service system. In our system, a user does not leak any of her location information while she can still obtain the desired information associated with the location. We propose a novel method to map a user's current location to the index of the information stored in the location based service server. We demonstrated the efficiency of our system through simulations.", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Mining social media: Challenges and opportunities\n", "abstract": " The opportunities presented by social networking have led to millions of users flocking to sites like Facebook, Twitter, and Foursquare. Even sites like Amazon have added the ability for users to interact with one another, though it seems tangential to the site's stated purpose. These social networking sites and social networking features generate massive amounts of data that can be used to draw conclusions about social behavior that could previously only be studied using relatively small sample sizes. This unlocks the ability to validate existing social theories, generate new models for how individuals and groups interact, and leverage the power of the crowd, among others.", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Protecting privacy in incremental maintenance for distributed association rule mining\n", "abstract": " Distributed association rule mining algorithms are used to discover important knowledge from databases. Privacy concerns can prevent parties from sharing the data. New algorithms are required to solve traditional mining problems without disclosing (original or derived) information of their own data to other parties. Research results have been developed on (i) incrementally maintaining the discovered association rules, and (ii) computing the distributed association rules while preserving privacy. However, no study has been conducted on the problem of the maintenance of the discovered rules with privacy protection when new sites join the old sites. We propose an algorithm SIMDAR for this problem. Some techniques we developed can even further reduce the cost in a normal association rule mining algorithm with privacy protection. Experimental results showed that SIMDAR can significantly reduce the\u00a0\u2026", "num_citations": "6\n", "authors": ["1157"]}
{"title": "From incremental learning to model independent instance selection-a support vector machine approach\n", "abstract": " With large amounts of data being available to machine learning community, the need to design techniques that scale well is more critical than ever before. As some data may be collected over long periods, there is also a continuous need to incorporate the new data into the previously learned concept. Incremental learning techniques can satisfy the need for both the scalability and incremental update. In this paper, we categorize the incremental techniques into two broad categories: block by block vs instance by instance. We suggest three criteria to evaluate the robustness and reliability of incremental learning methods. We then propose an incremental learning method for Support Vector Machines, and use the suggested criteria to evaluate the effectiveness of the suggested training method. Motivated by positive results on these experiments, we research the possibility of using SVMs for another approach to handling very large datasets. We have carried out a study to evaluate whether the Support Vector Machine (SVM) training can be used to select a small subset of examples from the training set in a model independent way. We compare the results of SVM selection, with IB2 selection method and random sampling. We analyze the experiment results, and discuss their implications. All the results have been illustrated using standard machine learning benchmark datasets.", "num_citations": "6\n", "authors": ["1157"]}
{"title": "A family of efficient rule generators\n", "abstract": " This paper describes a family of rule generators that can be used to extract classification rules in various applications. It includes versions that can handle noise in data, that can produce perfect rules, that can induce order independent or dependent rules based on a core algorithm. Evaluation measures are given to compare different rule generators in the family and with some known methods. Empirical results and applications of these rule generators in the family show that they are efficient, compact and accurate in various situations. The systems are available for trial use upon request. Key Words: Classification, Rule Compactness and Comprehensibility, Noise Handling 1 I. Introduction Classification rules are sought in many areas from automatic knowledge acquisition [15, 16] to data mining [1, 22], neural network rule extraction [19, 6, 17]. This is because classification rules possess some attractive features. They are explicit, understandable and verifiable by domain...", "num_citations": "6\n", "authors": ["1157"]}
{"title": "Modeling Temporal Patterns of Cyberbullying Detection with Hierarchical Attention Networks\n", "abstract": " Cyberbullying is rapidly becoming one of the most serious online risks for adolescents. This has motivated work on machine learning methods to automate the process of cyberbullying detection, which have so far mostly viewed cyberbullying as one-off incidents that occur at a single point in time. Comparatively less is known about how cyberbullying behavior occurs and evolves over time. This oversight highlights a crucial open challenge for cyberbullying-related research, given that cyberbullying is typically defined as intentional acts of aggression via electronic communication that occur repeatedly and persistently. In this article, we center our discussion on the challenge of modeling temporal patterns of cyberbullying behavior. Specifically, we investigate how temporal information within a social media session, which has an inherently hierarchical structure (e.g., words form a comment and comments form a\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Causal Inference for Time series Analysis: Problems, Methods and Evaluation\n", "abstract": " Time series data is a collection of chronological observations which is generated by several domains such as medical and financial fields. Over the years, different tasks such as classification, forecasting, and clustering have been proposed to analyze this type of data. Time series data has been also used to study the effect of interventions over time. Moreover, in many fields of science, learning the causal structure of dynamic systems and time series data is considered an interesting task which plays an important role in scientific discoveries. Estimating the effect of an intervention and identifying the causal relations from the data can be performed via causal inference. Existing surveys on time series discuss traditional tasks such as classification and forecasting or explain the details of the approaches proposed to solve a specific task. In this paper, we focus on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data, and provide a comprehensive review of the approaches in each task. Furthermore, we curate a list of commonly used evaluation metrics and datasets for each task and provide in-depth insight. These metrics and datasets can serve as benchmarks for research in the field.", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Representation learning for imbalanced cross-domain classification\n", "abstract": " Deep architectures are trained on massive amounts of labeled data to guarantee the performance of classification. In the absence of labeled data, domain adaptation often provides an attractive option given that labeled data of a similar nature but from a different domain is available. Previous work has chiefly focused on learning domain invariant representations but overlooked the issues of label imbalance in a single domain or across domains, which are common in many machine learning applications such as fake news detection. In this paper, we study a new cross-domain classification problem where data in each domain can be imbalanced (data imbalance), i.e., the classes are not evenly distributed, and the ratio of the number of positive over negative samples varies across domains (domain imbalance). This cross-domain problem is challenging as it entails covariate bias in the input feature space and\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "A practical data repository for causal learning with big data\n", "abstract": " The recent success in machine learning (ML) has led to a massive emergence of AI applications and the increases in expectations for AI systems to achieve human-level intelligence. Nevertheless, these expectations have met with multi-faceted obstacles. One major obstacle is ML aims to predict future observations given real-world data dependencies while human-level intelligence AI is often beyond prediction and seeks the underlying causal mechanism. Another major obstacle is that the availability of large-scale datasets has significantly influenced causal study in various disciplines. It is crucial to leverage effective ML techniques to advance causal learning with big data. Existing benchmark datasets for causal inference have limited use as they are too \u201cideal\u201d, i.e., small, clean, homogeneous, low-dimensional, to describe real-world scenarios where data is often large, noisy, heterogeneous and high\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Learning from networks: Algorithms, theory, and applications\n", "abstract": " Arguably, every entity in this universe is networked in one wayr another. With the prevalence of network data collected, such as social media and biological networks, learning from networks has become an essential task in many applications. It is well recognized that network data is intricate and large-scale, and analytic tasks on network data become more and more sophisticated. In this tutorial, we systematically review the area of learning from networks, including algorithms, theoretical analysis, and illustrative applications. Starting with a quick recollection of the exciting history of the area, we formulate the core technical problems. Then, we introduce the fundamental approaches, that is, the feature selection based approaches and the network embedding based approaches. Next, we extend our discussion to attributed networks, which are popular in practice. Last, we cover the latest hot topic, graph neural based\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Evaluation of the chemical consistency of Yin\u2010Chen\u2010Hao\u2010Tang prepared by combined and separated decoction methods using high\u2010performance liquid chromatography and quadrupole\u00a0\u2026\n", "abstract": " In this study, Yin\u2010Chen\u2010Hao\u2010Tang prepared by two decoction methods, namely, combined decoction (modern decoction method) and separated decoction (traditional decoction method), was analyzed by high\u2010performance liquid chromatography with quadrupole time\u2010of\u2010flight mass spectrometry. The acquired datasets containing sample codes, tR\u2010m/z pairs and ion intensities were processed with multivariate statistical analyses, such as principal component analysis and an orthogonal partial least squared discriminant analysis model, to globally compare the chemical differences between the different decoction samples. Then, the chemical differences between the combined and separated decoctions were screened out by S\u2010plots generated from the orthogonal partial least squared discriminant analysis model and compared with chemical information from an established in\u2010house library. The six components that\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "A study of reddit-user's response to rape\n", "abstract": " The growth of social media has created an open web where people freely share their opinion and even discuss sensitive subjects in online forums. Forums such as Reddit help support seekers by serving as a portal for open discussions for various stigmatized subjects such as rape. This paper investigates the potential roles of online forums and if such forums provide intended resources to the people who seek support. Specifically, the open nature of forums allows us to study how online users respond to seeker's queries or needs; through their response, we attempt to assess the range of topics covered by responders in regards to the issues, concerns and, obstacles faced by the victims of rape and sexual abuse, using rape-related posts from Reddit. We employ natural language processing techniques to extract topics of responses, examine how diverse these topics are to answer research questions such as\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Field synopsis and meta-analyses of genetic epidemiological evidence for Kashin\u2013Beck disease, an endemic osteoarthropathy in China\n", "abstract": " Kashin\u2013Beck disease (KBD) is a chronic degenerative osteoarthropathy with unclear etiology. To provide current evidence supporting a genetic predisposition for KBD, we conducted a systematic review and meta-analysis of published literature on the genetic epidemiology of KBD. The PubMed, China National Knowledge Infrastructure and Wan Fang Data were searched up to August 2015 for articles published in English and Chinese. Genome-wide and exome sequencing, linkage, and case\u2013control association studies for any genetic variants associated with KBD were included. Meta-analysis was performed for all single nucleotide polymorphisms (SNPs) that were evaluated in two or more studies. The effect size was summarized as odds ratios (ORs) with 95\u00a0% confidence intervals (CIs) by fixed and random effects models. A total of 24 articles were systematically reviewed. Eleven short tandem repeats\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Visualizing twitter data\n", "abstract": " Twitter\u00ae is a massive social networking site tuned towards fast communication. More than 140 million active users publish over 400 million 140-character \u201cTweets\u201d every day.", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Clustering of blog sites using collective wisdom\n", "abstract": " The blogosphere is expanding at an unprecedented speed. A better understanding of the blogosphere can greatly facilitate the development of the social Web to serve the needs of users, service providers, and advertisers. One important task in this process is the clustering of blog sites. Although a good number of traditional clustering methods exist, they are not designed to take into account the blogosphere\u2019s unique characteristics. Clustering blog sites presents new challenges. A prominent feature of the social Web is that many enthusiastic bloggers voluntarily write, tag, and catalog their posts in order to reach the widest possible audience who will share their thoughts and appreciate their ideas. In the process, a new kind of collective wisdom is generated. The objective of this work is to make use of this collective wisdom in the clustering of blog sites. As such, we study how clustering with collective wisdom\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Completing missing views for multiple sources of web media\n", "abstract": " Combining multiple data sources, each with its own features, to achieve optimal inference has received a lot of attention in recent years. In inference from multiple data sources, each source can be thought of as providing one view of the underlying object. In general, different views may provide complementary information for the inference task. However, often not all the views are available all the time for the available instances in an application. In this paper, we propose a view completion approach based on canonical correlation analysis that heuristically predicts the missing views and further ranks all within-view features, through learning the intrinsic correlation among the views from training set. We evaluate our approach and compare it with existing approaches in the literature, using web page classification and photo tag recommendation as case studies. Experiments demonstrate the improved performance of\u00a0\u2026", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Advances in Knowledge Discovery and Data Mining: 9th Pacific-Asia Conference, PAKDD 2005, Hanoi, Vietnam, May 18-20, 2005, Proceedings\n", "abstract": " The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) is a leading international conference in the area of data mining and knowledge discovery. It provides an international forum for researchers and industry practitioners to share their new ideas, original research results and practical development experiences from all KDD-related areas including data mining, data warehousing, machine learning, databases, statistics, knowledge acquisition and automatic scientific discovery, data visualization, causality induction, and knowledge-based systems. This year\u2019s conference (PAKDD 2005) was the ninth of the PAKDD series, and carried the tradition in providing high-quality technical programs to facilitate research in knowledge discovery and data mining. It was held in Hanoi, Vietnam at the Melia Hotel, 18\u201320 May 2005. We are pleased to provide some statistics about PAKDD 2005. This year we received 327 submissions (a 37% increase over PAKDD 2004), which is the highest number of submissions since the first PAKDD in 1997) from 28 countries/regions: Australia (33), Austria (1), Belgium (2), Canada (11), China (91), Switzerland (2), France (9), Finland (1), Germany (5), Hong Kong (11), Indonesia (1), India (2), Italy (2), Japan (21), Korea (51), Malaysia (1), Macau (1), New Zealand (3), Poland (4), Pakistan (1), Portugal (3), Singapore (12), Taiwan (19), Thailand (7), Tunisia (2), UK (5), USA (31), and Vietnam (9). The submitted papers went through a rigorous reviewing process. Each submission was reviewed by at least two reviewers, and most of them by three or four reviewers.", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Compact dual ensembles for active learning\n", "abstract": " Generic ensemble methods can achieve excellent learning performance, but are not good candidates for active learning because of their different design purposes. We investigate how to use diversity of the member classifiers of an ensemble for efficient active learning. We empirically show, using benchmark data sets, that (1) to achieve a good (stable) ensemble, the number of classifiers needed in the ensemble varies for different data sets; (2) feature selection can be applied for classifier selection from ensembles to construct compact ensembles with high performance. Benchmark data sets and a real-world application are used to demonstrate the effectiveness of the proposed approach.", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Some Performance Comparisons for Self-Generating Neural Tree\n", "abstract": " The performance of Self-Generating Neural Tree (SGNT) method is analysed using some public domain data sets and the Monk problems-a de facto standard benchmark set. Comparisons are performed between SGNT and other methods. The results show that SGNT method is superior to all the unsupervised learning methods and BOrne popular supervised learning method. in both accuracy and speed of learning.", "num_citations": "5\n", "authors": ["1157"]}
{"title": "Joint Spatial and Temporal Modeling for Hydrological Prediction\n", "abstract": " The accurate and timely estimation of river discharge plays an important role in hydrological modeling, especially for avoiding the consequences of flood events. The majority of existing work on hydrologic prediction focuses on modeling the inherent physical process for specific river basins, while the geographic-connections between rivers are largely ignored. Geographically connected rivers provide rich spatial information that can be used to predict discharge amounts. In this paper, we study a novel problem of exploiting both temporal patterns and spatial connections for hydrological prediction. We construct three relationship graphs for hydrological gauges in the study area: the hydraulic distance graph, the Euclidean distance graph and the correlation graph. We fuse these graphs into one hydrological network graph, and propose a novel framework ST-Hydro which exploits Graph Convolutional Networks (GCN\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Unsupervised cyberbullying detection via time-informed deep clustering\n", "abstract": " Social media is a vital means for information-sharing due to its easy access, low cost, and fast dissemination characteristics. However, increases in social media usage have corresponded with a rise in the prevalence of cyberbullying. Most existing cyberbullying detection methods are supervised and, thus, have two key drawbacks:(1) The data labeling process is often time-consuming and laborintensive;(2) Current labeling guidelines may not be generalized to future instances because of different language usage and evolving social networks. To address these limitations, this work introduces a principled approach for unsupervised cyberbullying detection. The proposed model consists of two main components:(1) A representation learning network that encodes the social media session by exploiting multi-modal features, eg, text, network, and time.(2) A multi-task learning network that simultaneously fits the comment inter-arrival times and estimates the bullying likelihood based on a Gaussian Mixture Model. The proposed model jointly optimizes the parameters of both components to overcome the shortcomings of decoupled training. Our core contribution is an unsupervised cyberbullying detection model that not only experimentally outperforms the state-of-the-art unsupervised models, but also achieves competitive performance compared to supervised models.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Causal learning in question quality improvement\n", "abstract": " To improve the quality of questions asked in Community-based questions answering forums, we create a new dataset from the website, Stack Overflow, which contains three components: (1) context: the text features of questions, (2) treatment: categories of revision suggestions and (3) outcome: the measure of question quality (e.g., the number of questions, upvotes or clicks). This dataset helps researchers develop causal inference models towards solving two problems: (i) estimating the causal effects of aforementioned treatments on the outcome and (ii) finding the optimal treatment for the questions. Empirically, we performed experiments with three state-of-the-art causal effect estimation methods on the contributed dataset. In particular, we evaluated the optimal treatments recommended by the these approaches by comparing them with the ground truth labels \u2013 treatments (suggestions) provided by experts.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "NKG2D regulation of lung pathology and dendritic cell function following respiratory syncytial virus infection\n", "abstract": " Background           Respiratory syncytial virus (RSV) is a common cause of respiratory tract infection in vulnerable populations. Natural killer (NK) cells and dendritic cells (DC) are important for the effector functions of both cell types following infection.                             Methods           Wild-type and NKG2D-deficient mice were infected with RSV. Lung pathology was assessed by histology. Dendritic cell function and phenotype were evaluated by enzyme-linked immunosorbent assay and flow cytometry. The expression of NKG2D ligands on lung and lymph node DCs was measured by immunostaining and flow cytometry. Adoptive transfer experiments were performed to assess the importance of NKG2D-dependent DC function in RSV infection.                             Results           NKG2D-deficient mice exhibited greater lung pathology, marked by the accumulation of DCs following RSV infection. Dendritic cells\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Towards explainable networked prediction\n", "abstract": " Networked prediction has attracted lots of research attention in recent years. Compared with the traditional learning setting, networked prediction is even harder to understand due to its coupled,\\em multi-level nature. The learning process propagates top-down through the underlying network from the macro level (the entire learning system), to meso level (learning tasks), and to micro level (individual learning examples). In the meanwhile, the networked prediction setting also offers rich context to explain the learning process through the lens of\\em multi-aspect, including training examples (eg, what are the most influential examples), the learning tasks (eg, which tasks are most important) and the task network (eg, which task connections are the keys). Thus, we propose a multi-aspect, multi-level approach to explain networked prediction. The key idea is to efficiently quantify the influence on different levels of the\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Exploiting multilabel information for noise-resilient feature selection\n", "abstract": " In a conventional supervised learning paradigm, each data instance is associated with one single class label. Multilabel learning differs in the way that data instances may belong to multiple concepts simultaneously, which naturally appear in a variety of high impact domains, ranging from bioinformatics and information retrieval to multimedia analysis. It targets leveraging the multiple label information of data instances to build a predictive learning model that can classify unlabeled instances into one or multiple predefined target classes. In multilabel learning, even though each instance is associated with a rich set of class labels, the label information could be noisy and incomplete as the labeling process is both time consuming and labor expensive, leading to potential missing annotations or even erroneous annotations. The existence of noisy and missing labels could negatively affect the performance of underlying\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Finding time-critical responses for information seeking in social media\n", "abstract": " Social media is being increasingly used to request information and help in situations like natural disasters, where time is a critical commodity. However, generic social media platforms are not explicitly designed for timely information seeking, making it difficult for users to obtain prompt responses. Algorithms to ensure prompt responders for questions in social media have to understand the factors affecting their response time. In this paper, we draw from sociological studies on information seeking and organizational behavior to model the future availability and past response behavior of the candidate responders. We integrate these criteria with their interests to identify users who can provide timely and relevant responses to questions posted in social media. We propose a learning algorithm to derive optimal rankings of responders for a given question. We present questions posted on Twitter as a form of information\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "LIKE and recommendation in social media\n", "abstract": " This tutorial covers the state-of-the-art developments in LIKE and recommendation in social media. It is designed for graduate students, practitioners, or IT managers with general understanding on WWW and social media. No prerequisite is expected.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Quantifying features using false nearest neighbors: An unsupervised approach\n", "abstract": " Real-world datasets commonly present high dimensional data, which means an increased amount of information. However, this does not always imply an improvement in learning technique performance. Furthermore, some features may be correlated or add unexpected noise, thereby reducing data clustering performance. This has motivated the development of feature selection methods to find the most relevant subset of features to describe data. In this work, we focus on the problem of unsupervised feature selection. The main goal is to define a method to identify the number of features to select after sorting them based on some criterion. This task is done by means of the False Nearest Neighbor technique, which is rooted in chaos theory. Results have shown that this technique gives a good approximate number of features to select. When compared to other techniques, in most of the analyzed cases, it maintains\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Towards building a social computing tool for social scientists\n", "abstract": " This paper presents a social computing tool that centers around social scientists. In the past years, we have worked with social scientists and cultural anthropologists. We learned their ways of studying subjects in social media, what their needs are, and their interests. In the process, we have built a generic platform for collecting data in the blogosphere, tracking blogs of particular interests, and facilitating comparative data analysis. We report our progress in building and expanding this social-scientist-centered social computing tool. We show how we are inspired by the research of social scientists in our effort to build the platform - BlogTrackers, which can help identify key topics of discussion, identify influential bloggers, track topics over time, and search blogs. These features can help social scientists to quickly analyze both blogs and bloggers at a scale, which is otherwise impossible through manual investigation.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "A study of friendship networks and Blogosphere\n", "abstract": " In Golbeck and Hendler (2006), authors consider those social friendship networking sites where users explicitly provide trust ratings to other members. However, for large social friendship networks it is infeasible to assign trust ratings to each and every member so they propose an inferring mechanism which would assign binary trust ratings (trustworthy/non-trustworthy) to those who have not been assigned one. They demonstrate the use of these trust values in e-mail? ltering application domain and report encouraging results. Authors also assume three crucial properties of trust for their approach to work: transitivity, asymmetry, and personalization. These trust scores are often transitive, meaning, if Alice trusts Bob and Bob trusts Charles then Alice can trust Charles. Asymmetry says that for two people involved in a relationship, trust is not necessarily identical in both directions. This is contrary to what was proposed\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Some current issues of streaming data mining\n", "abstract": " Editorial: Some current issues of streaming data mining: Information Sciences: an International Journal: Vol 176, No 14 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information Sciences: an International Journal Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation Sciences: an International JournalVol. , No. Editorial: Some current issues of streaming data mining article Editorial: Some current issues of streaming data mining Share on Authors: Jianping Zhang profile image Jianping Zhang AOL, Inc., 44900 Prentice Drive, Dulles, VA 20116, United States AOL, Inc., 44900 Prentice Drive, Dulles, VA 20116, United States View Profile , \u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Exploiting statistical redundancy in expression microarray data to foster biological relevancy\n", "abstract": " Discriminating meaningful biochemical differences between normal and tumor cells, as well as discerning nuances in the gene expression patterns among tumor cells of differential behaviors are likely to lead to foundational insights empowering improved diagnosis, prognosis, and therapeutics for cancer patients. Laboratory techniques now are routinely employed which portray the expression levels of tens of thousands of genes under experimental conditions, affording an unprecedented glimpse into the controlling influences underlying pathological behaviors. Basic statistical methods assess the differential expression of individual genes without considering the statistical redundancy between genes and often produce an overwhelming number of candidate genes for subsequent biological and clinical validation. This work points out the necessity to exploit statistical redundancy to foster biological relevance. It describes a method RSVP (Reporter Surrogate Variable Program) which reduces the number of selected candidate genes while increasing the overall discriminative power; use of the tool also fosters guided biological relevance of the candidate gene set for subsequent validation. From a set of differentially expressed genes, RSVP identifies a subset of reporter genes which are mutually non-redundant and jointly provide a signature profile for discriminating distinct specimen types or cellular phenotypes. In addition, the method exploits the correlation between each of the reporter genes and non-reporter genes and enables biologists to select candidate genes of biological relevance according to biological knowledge. The effectiveness of\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Class-specific ensembles for active learning in digital imagery\n", "abstract": " In many real-world tasks of image classification, limited amounts of labeled data are available to train automatic classifiers. Consequently, extensive human expert involvement is required for instance labeling. Detecting Egeria densa in digital imagery is one such real-world classification task. It presents an additional challenge due to subtle spectral changes in Egeria, which makes it difficult to find a single accurate classifier. A novel solution is proposed to employ an ensemble of classifiers for each class (class-specific ensembles), combined with an active learning scheme. The class-specific ensembles are implicitly diverse. Diversity is required to increase the overall accuracy when combining predictions. The combined predictions of the ensembles can be used to reduce the uncertainty in detecting Egeria. Iterative active learning is then suggested to adapt the ensembles to the new images, unseen to the active\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Active sampling: an effective approach to feature selection\n", "abstract": " Feature selection is frequently used in data preprocessing for data mining. It decreases number of features, removes irrelevant or noisy data, and increases mining performance such as predictive accuracy and comprehensibility. This work investigates active sampling in feature selection in a filter model setting. Three versions of active sampling are proposed and empirically evaluated: two employ class information and the other utilizes feature variance. They are applied to a widely used, efficient feature selection algorithm Relief. In comparison with random sampling, we conduct extensive experiments with benchmark data sets.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Feature selection for image data via learning\n", "abstract": " Image mining requires the extraction of features from image data. Hundreds of features can be extracted, but only a subset of features is useful. Domain experts are usually required to determine relevant features in an ad hoc manner. We introduce a learning approach to feature selection for image mining. This approach allows domain experts to focus on tasks they do well: providing a labeled training image data set, instead of selecting relevant features ad hoc. Extensive experiments are conducted to verify the effectiveness of this learning approach.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "An Image Mining Approach for Measuring Intensity, Size and Geographical localization of Stained Bodies in Cultured Cells: Application in Apoptosis Detection\n", "abstract": " Biological research relies on both visual and numerical data to explain and confirm a proposed hypothesis. Here we develop a high throughput method to measure the relative extent of apoptosis from digitally acquired images of glioma cells stained with the nuclear dye, DAPI. It is very tedious and time consuming to analyze these images manually. We present an Image Mining system that automates the majority of the tasks with significant speed up. Parameters such as total cell count, number of apoptotic cells and percentage of apoptotic cells are measured from the images. The system also has a human-inthe-loop module which allows the domain expert to fine-tune the parameters. Extensive training and experimentation has been done and sample results have been provided.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Active learning for classifying a spectrally variable subject\n", "abstract": " Classifying Egeria densa, Brazilian waterweed, in scan-digitized color infrared (CIR) airphotos by automated methods presents a challenging problem due to a number of variable and unfavorable conditions: changes in imaging conditions, problems associated with water-related subjects, and other environmental changes as well as expected lack of spectral separation between Egeria and other land cover classes in CIR imagery. To address these challenges, we are developing an interactive computer system based on data mining techniques with Active Learning capabilities. The key components of this system are: feature extraction, automatic classification, active learning, and experimental evaluation. We anticipate creating an interactive learning system that can learn from human analysts by relating results to extracted objects and that can learn analytic rules for classification. In this paper, we report the concept of the system, preliminary experimental results, and anticipated future work.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Sampling from databases using B+-trees\n", "abstract": " Sampling techniques are becoming increasingly important for very large databases. However, the problem of obtaining a random sample from index structures has not received much attention. In this paper, we examine sampling techniques for B+-tree. As the fanout of each node varies, a random walk through the index structure does not produce a good representative sample of the data set. We propose a new technique, called B+-Tree based Weighted Random Sampling (BTWRS), that alters the inclusion probabilities of records accordingly to allow more records from leaves, along the paths with higher fanouts, to be extracted. We extensively evaluated our method, and the results show that there is an improvement in BTWRS over the existing schemes in terms of the quality of the samples obtained and the efficiency of the sampling process. The proposed method can be readily adopted in existing commercial\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Joint concept formation\n", "abstract": " Many concept formation systems construct disjoint-concept trees. However, a priori imposed tree structures may restrict the application of these systems in some domains. A joint concept formation scheme is thus proposed, which learns from observation, and constructs acyclic directed concept graphs (trees are a special case). We show that the joint concept formation system can avoid or alleviate some problems the disjoint concept formation system would face, such as the unique winner and oscillation problems. We also demonstrate that a joint concept formation system is able to generate a concept tree if such a regularity is found among the data. The experimental results are consistent with the expectations that the joint system is a generalized version of the disjoint system and improves the learning performance. Joint concept formation extends the classic works, such as COBWEB and ARACHNE.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Knowledge-Based models of human and robot grasping\n", "abstract": " A knowledge-based approach to modeling complex systems is introduced in this paper and applied to the sythesis of a model of grasping in multifingered robotic hands. The method begins with a description of the knowledge required to describe human grasping, and then proceeds to a transfer of the basic ideas to robot hands. The application of the resulting model to grasp planning and control of the Belgrade/USC hand are described.", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Automating the design of telecommunication distribution networks\n", "abstract": " This paper describes an automated system for the design of telecommunication distribution networks. The paper discusses models for the design process and shows how appropriate modelling and representation of the problem domain can simplify design. In particular, the paper shows how the decomposition of the problem in terms of the design components and the constraints on the design solutions can simplify the problem of generating new design structures. The embedding of the automated design process into a logical framework based on this decomposition is described, including the difficulties associated with representing all of the information required by the system that is taken for granted in manual design. The process of resource allocation to dimension the structure and provide a design solution is then described as well as how this process is optimised. Finally, the paper describes a technique for the\u00a0\u2026", "num_citations": "4\n", "authors": ["1157"]}
{"title": "Improving Cyberbullying Detection with User Interaction\n", "abstract": " Cyberbullying, identified as intended and repeated online bullying behavior, has become increasingly prevalent in the past few decades. Despite the significant progress made thus far, the focus of most existing work on cyberbullying detection lies in the independent content analysis of different comments within a social media session. We argue that such leading notions of analysis suffer from three key limitations: they overlook the temporal correlations among different comments; they only consider the content within a single comment rather than the topic coherence across comments; they remain generic and exploit limited interactions between social media users. In this work, we observe that user comments in the same session may be inherently related, eg, discussing similar topics, and their interaction may evolve over time. We also show that modeling such topic coherence and temporal interaction are critical to\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Long-Term Effect Estimation with Surrogate Representation\n", "abstract": " There are many scenarios where short-and long-term causal effects of an intervention are different. For example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. This work, therefore, studies the the problem of long-term effect where the outcome of primary interest, orprimary outcome, takes months or even years to accumulate. The observational study of long-term effect presents unique challenges. First, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. Second, short-term outcomes are often directly used as the proxy of the primary outcome, ie, thesurrogate. Nevertheless, this method entails the strong surrogacy assumption that is often impractical. To tackle these challenges, we propose to build connections between long-term causal inference and sequential models in\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Fact-enhanced synthetic news generation\n", "abstract": " The advanced text generation methods have witnessed great success in text summarization, language translation, and synthetic news generation. However, these techniques can be abused to generate disinformation and fake news. To better understand the potential threats of synthetic news, we develop a novel generation method FACTGEN to generate high-quality news content. The majority of existing text generation methods either afford limited supplementary information or lose consistency between the input and output which makes the synthetic news less trustworthy. To address these issues, FACTGEN retrieves external facts to enrich the output and reconstructs the input claim from the generated content to improve the consistency among the input and the output. Experiment results on real-world datasets demonstrate that the generated news contents of FACTGEN are consistent and contain rich facts. We also discuss an effective defending technique to identify these synthetic news pieces if FACTGEN was used to generate fake news.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Towards causal understanding of fake news dissemination\n", "abstract": " Recent years have witnessed remarkable progress made towards the computational detection of fake news. To mitigate its negative impact, however, we argue that a critical element is to understand why people spread fake news. Central to the question of \"why\" is the need to study the fake news sharing behavior. Deeply related to user characteristics and online activities, fake news sharing behavior is important to uncover the causal relationships between user attributes and the probability of this user to spread fake news. One obstacle in learning such user behavior is that most data is subject to selection bias, rendering partially observed fake news dissemination among users. To discover causal user attributes, we confront another obstacle of finding the confounders in fake news dissemination. Drawing on theories in causal inference, in this work, we first propose a principled approach to unbiased modelings of fake news dissemination under selection bias. We then consider the learned fake news sharing behavior as the measured confounder and further identify the user attributes that potentially cause users to spread fake news. We theoretically and empirically characterize the effectiveness of the proposed approach and find that it could be useful in protecting society from the perils of fake news.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Feature selection for hierarchical classification via joint semantic and structural information of labels\n", "abstract": " Hierarchical Classification is widely used in many real-world applications, where the label space is exhibited as a tree or a Directed Acyclic Graph (DAG) and each label has rich semantic descriptions. Feature selection, as a type of dimension reduction technique, has proven to be effective in improving the performance of machine learning algorithms. However, many existing feature selection methods cannot be directly applied to hierarchical classification problems since they ignore the hierarchical relations and take no advantage of the semantic information in the label space. In this paper, we propose a novel feature selection framework based on semantic and structural information of labels. First, we transform the label description into a mathematical representation and calculate the similarity score between labels as the semantic regularization. Second, we investigate the hierarchical relations in a tree structure of\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Tracking disaster footprints with social streaming data\n", "abstract": " Social media has become an indispensable tool in the face of natural disasters due to its broad appeal and ability to quickly disseminate information. For instance, Twitter is an important source for disaster responders to search for (1) topics that have been identified as being of particular interest over time, ie, common topics such as \u201cdisaster rescue\u201d;(2) new emerging themes of disaster-related discussions that are fast gathering in social media streams (Saha and Sindhwani 2012), ie, distinct topics such as \u201cthe latest tsunami destruction\u201d. To understand the status quo and allocate limited resources to most urgent areas, emergency managers need to quickly sift through relevant topics generated over time and investigate their commonness and distinctiveness. A major obstacle to the effective usage of social media, however, is its massive amount of noisy and undesired data. Hence, a naive method, such as set intersection/difference to find common/distinct topics, is often not practical. To address this challenge, this paper studies a new topic tracking problem that seeks to effectively identify the common and distinct topics with social streaming data. The problem is important as it presents a promising new way to efficiently search for accurate information during emergency response. This is achieved by an online Nonnegative Matrix Factorization (NMF) scheme that conducts a faster update of latent factors, and a joint NMF technique that seeks the balance between the reconstruction error of topic identification and the losses induced by discovering common and distinct topics. Extensive experimental results on real-world datasets collected during\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Spatial Community-Informed Evolving Graphs for Demand Prediction.\n", "abstract": " The rapidly increasing number of sharing bikes has facilitated people\u2019s daily commuting significantly. However, the number of available bikes in different stations may be imbalanced due to the free check-in and check-out of users. Therefore, predicting the bike demand in each station is an important task in a city to satisfy requests in different stations. Recent works mainly focus on demand prediction in settled stations, which ignore the realistic scenarios that bike stations may be deployed or removed. To predict station-level demands with evolving new stations, we face two main challenges:(1) How to effectively capture new interactions in time-evolving station networks;(2) How to learn spatial patterns for new stations due to the limited historical data. To tackle these challenges, we propose a novel Spatial Community-informed Evolving Graphs (SCEG) framework to predict station-level demands, which considers two different grained interactions. Specifically, we learn time-evolving representation from fine-grained interactions in evolving station networks using EvolveGCN. And we design a Bi-grained Graph Convolutional Network (B-GCN) to learn community-informed representation from coarse-grained interactions between communities of stations. Experimental results on real-world datasets demonstrate the effectiveness of SCEG on demand prediction for both new and settled stations. Our code is available at https://github. com/RoeyW/Bikes-SCEG", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Smart ccr iot: Internet of things testbed\n", "abstract": " In this vision article, we presented a large-scale IoT testbed that is currently under development at the Phoenix metropolitan area called Smart CCR. In particular, the description focused on a pilot Smart IoT testbed project called \"Blue Light IoT testbed\" on ASU campuses. Smart CCR targets at establishing a scale and distributed IoT data collection and processing platform to support future IoT applications considering real-time, location, and security constraints. In addition to the presented IoT testbed infrastructure, we also presented the conceptual model of the IoT testbed by answering a set of design, development, and operation questions.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Beyond word2vec: Distance-graph tensor factorization for word and document embeddings\n", "abstract": " The\\em word2vec methodology such as Skip-gram and CBOW has seen significant interest in recent years because of its ability to model semantic notions of word similarity and distances in sentences. A related methodology, referred to as\\em doc2vec is also able to embed sentences and paragraphs. These methodologies, however, lead to different embeddings that cannot be related to one another. In this paper, we present a tensor factorization methodology, which simultaneously embeds words and sentences into latent representations in one shot. Furthermore, these latent representations are concretely related to one another via tensor factorization. Whereas\\em word2vec and\\em doc2vec are dependent on the use of contextual windows in order to create the projections, our approach treats each document as a structural graph on words. Therefore, all the documents in the corpus are jointly factorized in order to\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Feature Interaction-aware Graph Neural Networks\n", "abstract": " Inspired by the immense success of deep learning, graph neural networks (GNNs) are widely used to learn powerful node representations and have demonstrated promising performance on different graph learning tasks. However, most real-world graphs often come with high-dimensional and sparse node features, rendering the learned node representations from existing GNN architectures less expressive. In this paper, we propose \\textit{Feature Interaction-aware Graph Neural Networks (FI-GNNs)}, a plug-and-play GNN framework for learning node representations encoded with informative feature interactions. Specifically, the proposed framework is able to highlight informative feature interactions in a personalized manner and further learn highly expressive node representations on feature-sparse graphs. Extensive experiments on various datasets demonstrate the superior capability of FI-GNNs for graph learning tasks.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "The MERO Hand: A Mechanically Robust Anthropomorphic Prosthetic Hand using Novel Compliant Rolling Contact Joint *\n", "abstract": " Although substantial progresses have been made in building prosthetic hands, lack of mechanical robustness still restrains wide adoption of robotic hand prostheses. This paper presents the design and evaluation of the MERO hand, which is a MEchanically RObust anthropomorphic prosthetic hand using novel COmpliant Rolling-contact Element (CORE) joints. The proposed CORE joint, which has a simple structure, exhibits compliance in multiple directions. Its structural parameters were designed, to form underactuated finger designs that can perform adaptive finger motion during grasping. Experiments showed that the hand could withstand severe disarticulation and violent impact. The hand could perform various adaptive grasps and also in-hand manipulation, suggesting that the proposed design might be a viable solution for robust prosthetic hand.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "InterSpot: Interactive Spammer Detection in Social Media.\n", "abstract": " Spammer detection in social media has recently received increasing attention due to the rocketing growth of user-generated data. Despite the empirical success of existing systems, spammers may continuously evolve over time to impersonate normal users while new types of spammers may also emerge to combat with the current detection system, leading to the fact that a built system will gradually lose its efficacy in spotting spammers. To address this issue, grounded on the contextual bandit model, we present a novel system for conducting interactive spammer detection. We demonstrate our system by showcasing the interactive learning process, which allows the detection model to keep optimizing its detection strategy through incorporating the feedback information from human experts.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Multimodal fusion of brain networks with longitudinal couplings\n", "abstract": " In recent years, brain network analysis has attracted considerable interests in the field of neuroimaging analysis. It plays a vital role in understanding biologically fundamental mechanisms of human brains. As the upward trend of multi-source in neuroimaging data collection, effective learning from the different types of data sources, e.g. multimodal and longitudinal data, is much in demand. In this paper, we propose a general coupling framework, the multimodal neuroimaging network fusion with longitudinal couplings (MMLC), to learn the latent representations of brain networks. Specifically, we jointly factorize multimodal networks, assuming a linear relationship to couple network variance across time. Experimental results on two large datasets demonstrate the effectiveness of the proposed framework. The new approach integrates information from longitudinal, multimodal neuroimaging data and boosts\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Toward relational learning with misinformation\n", "abstract": " Relational learning has been proposed to cope with the interdependency among linked instances in a network, and it is a fundamental tool to categorize social network users for various tasks. However, the emerging widespread of misinformation in social networks, information that is inaccurate or false, poses novel challenges to utilizing social media data. Malicious users may actively manipulate their content and characteristics, which easily lead to a noisy dataset. Hence, it is intricate for traditional relational learning approaches to deliver an accurate predictive model in the presence of misinformation. In this work, we precisely focus on the problem by proposing a joint framework that simultaneously constructs a relational learning model and mitigates the effect of misinformation by restraining anomalous points. Empirical results on real-world social media data prove the superiority of the proposed approach\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Soil-pipe interaction modeling for pipe behavior prediction with super learning based methods\n", "abstract": " Underground pipelines are subject to severe distress from the surrounding expansive soil. To investigate the structural response of water mains to varying soil movements, field data, including pipe wall strains in situ soil water content, soil pressure and temperature, was collected. The research on monitoring data analysis has been reported, but the relationship between soil properties and pipe deformation has not been well-interpreted. To characterize the relationship between soil property and pipe deformation, this paper presents a super learning based approach combining feature selection algorithms to predict the water mains structural behavior in different soil environments. Furthermore, automatic variable selection method, e.i. recursive feature elimination algorithm, were used to identify the critical predictors contributing to the pipe deformations. To investigate the adaptability of super learning to different\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Deep learning for feature representation\n", "abstract": " Deep learning methods have become increasingly popular in recent years because of their tremendous success in image classification [19], speech recognition [20] and natural language processing tasks [60]. In fact, deep learning methods have regularly won many recent challenges in these domains [19]. The great success of deep learning mainly comes from specially designed structures", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Early identification of personalized trending topics in microblogging\n", "abstract": " Social media has become a primary platform for the spread of information. Trending topics, which are breaking news and immediately popular stories, have become an attractive data source facilitating the spread of emerging issues. Motivated by the diverse trending topics covering from sports to politics, it is essential to help users find personalized trending topics. Since a topic in social media may start trending and get obsoleted quickly, the personalization would be more valuable to a user if the trending topic can be recommended before it is outdated. In order to identify personalized trending topics at an early stage, we propose to identify and exploit the auxiliary information. In particular, through collectively modeling content of similar users with social network information, we identify additional past contents that can enrich the training data of trending topics and users. The key insight is that though most posts of a user may be irrelevant, a few key posts can be signals revealing interests towards a particular topic. Experiments on real-world data demonstrate that our proposed approach effectively personalizes trending topics when they just start trending.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "A Close Look at Tinder Bots\n", "abstract": " Tinder is a popular dating app that allows users to discover potential dating partners with close geographical proximity. Tinder is the first dating app in several countries and has more than 50 million users. However, many of these users are bots with malicious intent. The first step in dealing with this issue is understanding the characteristics of Tinder bots. Toward this aim, we have proposed a ground truth collection method to acquire bots to study. Our method combines honeypot methods and manual annotation. We find that probing messages is a reliable method to distinguish bots from humans as bots promote malicious URLs and direct users to phishing sites. Our observations on the collected bots show that they are more complex than bots that are studied in other social media sites. Tinder bots have profiles that are very hard to differentiate from normal users. We explore activity and profiles of these bots and report the characteristics that can be used in building a supervised learning approach for bot detection.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Influence and homophily\n", "abstract": " Social forces connect individuals in different ways. When individuals get connected, one can observe distinguishable patterns in their connectivity networks. One such pattern is assortativity, also known as social similarity. Assortativity In networks with assortativity, similar nodes are connected to one another more often than dissimilar nodes. For instance, in social networks, a high similarity between friends is observed. This similarity is exhibited by similar behavior, similar interests, similar activities, and shared attributes such as language, among others. In other words, friendship networks are assortative. Investigating assortativity patterns that individuals exhibit on social media helps one better understand user interactions. Assortativity is the most commonly observed pattern among linked individuals. This chapter discusses assortativity along with principal factors that result in assortative networks.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Mining and profiling in social media\n", "abstract": " The phenomenal rise of social media services in recent years presents new opportunities and challenges to both information consumers and service providers. With its growing popularity, social media has the potential to mine actionable patterns from a large amount of data to understand user behavior and to meet users' information needs. Profiling has the potential to better describe users and their relationships in social media. This entry introduces data mining and profiling in social media and discusses the characteristics of social media data in the context of research that is being undertaken in this area.Social media services like Facebook and Twitter have emerged as important platforms for large-scale information sharing and communication in fields such as marketing, journalism, and public relations. Social media is transforming internet users from information consumers to producers by providing a way for users to collaboratively create content. Social media have become a major platform that enables people to communicate with each other.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Crawling twitter data\n", "abstract": " Users on Twitter generate over 400 million Tweets everyday.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Mining social media: issues and challenges\n", "abstract": " The prevalence of social media offers a new kind of laboratory for behavioral study. In the era of the social Web, we are presented with unparalleled opportunities and novel challenges. In this talk, we will use some of our recent studies of human behavior to illustrate our endeavors to improve the understanding of human behaviors in social media. We explore how one can efficiently gauge what's happening in social media with an inordinate number of groups and growing; inquire whether one can disentangle the complicated connections among users to find their group memberships; look into user migration patterns in the presence of seemingly unlimited choices of social media services; and investigate ways of exploiting vulnerability to protect user privacy on a social networking site. We can benefit significantly from extant sociological theories and methodologies in carrying out interdisciplinary research that sheds\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Social dimension approach to classification in large-scale networks\n", "abstract": " Social media often provides social network information of users. However, the relationship hidden in the connections are inhomogeneous. Social dimensions are introduced to represent the heterogeneous interactions people interact with each other (latent affiliations one actor is involved in). Here, we list some of our published work as well as the data sets and code used for experiments.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Robust integration of multiple information sources by view completion\n", "abstract": " There are many applications where multiple data sources, each with its own features, are integrated in order to perform an inference task in an optimal way. Researchers have shown that for many tasks like webpage classification, image classification, and pattern recognition, combining data from multiple information sources yields significantly better results than using a single source. In these tasks each of the multiple data sources can be thought of as providing one view of the underlying object. However in many domains not all of the views are available for the available instances; some of the views would be missing. This problem of missing views affects the performance of the machine learning task. In this paper we provide a method of view completion to heuristically predict the missing views. We show that with view completion we are able to achieve significantly better results. We also show that by considering\u00a0\u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "A Bayesian Feature Selection Score Based on Na\u00a8 \u0131ve Bayes Models\n", "abstract": " The past decade has seen the emergence of truly massive data analysis challenges across a range of human endeavors. Standard statistical algorithms came into being long before such challenges were even imagined, and spurred on by a myriad of important applications, much statistical research now focuses on the development of algorithms that scale well. Feature selection represents a central issue on this research.ABSTRACT", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Error-sensitive grading for model combination\n", "abstract": " Ensemble learning is a powerful learning approach that combines multiple classifiers to improve prediction accuracy. An important decision while using an ensemble of classifiers is to decide upon a way of combining the prediction of its base classifiers. In this paper, we introduce a novel grading-based algorithm for model combination, which uses cost-sensitive learning in building a meta-learner. This method distinguishes between the grading error of classifying an incorrect prediction as correct, and the other-way-round, and tries to assign appropriate costs to the two types of error in order to improve performance. We study issues in error-sensitive grading, and then with extensive experiments show the empirically effectiveness of this new method in comparison with representative meta-classification techniques.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Active learning with ensembles for image classification\n", "abstract": " In many real-world tasks of image classification, limited amounts of labeled data are available to train automatic classifiers. Consequently, extensive human expert involvement is required for verification. A novel solution is presented that makes use of active learning combined with an ensemble of classifiers for each class. The result is a significant reduction in required expert involvement for uncertain image region classification.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Feature selection using consistency measure\n", "abstract": " Feature selection methods search for an \u201coptimal\u201d subset of features. Many methods exist. We evaluate consistency measure along with different search techniques applied in the literature and suggest a guideline of its use.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Similarity detection among data files-a machine learning approach\n", "abstract": " In any database, description files are essential to understand the data files in it. However, it is not uncommon that one is left with data files without any description file. An example is the aftermath of a system crash; other examples are related to security problems. Manual determination of the subject of a data file can be a difficult and tedious task, particularly if files look alike. An example is a big survey database where data files that look alike are actually related to different subjects. Two data files on the same subject will probably have similar semantic structures of attributes. We detect the similarity between two attributes. Then we create clusters of attributes to compare the similarity of the subjects of two data files. Finally, a machine learning technique is used to predict the subject of unseen data files.", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Knowledge-based grasp planning for robot hands\n", "abstract": " Knowledge-based grasp planning for robot hands | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleThesesKnowledge-based grasp planning for robot hands ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books cover image Knowledge-based grasp planning for robot hands January 1989 1 pages Author: Huan Liu, Chairman: George A. Bekey Copyright \u00a9 1989 Publisher University of Southern California United States \u2026", "num_citations": "3\n", "authors": ["1157"]}
{"title": "Mitigating bias in session-based cyberbullying detection: A non-compromising approach\n", "abstract": " The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographic-identity terms (eg,\u201cgay\u201d or \u201cblack\u201d) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (eg, Instagram). We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Mechanisms and Attributes of Echo Chambers in Social Media\n", "abstract": " Echo chambers may exclude social media users from being exposed to other opinions, therefore, can cause rampant negative effects. Among abundant evidence are the 2016 and 2020 US presidential elections conspiracy theories and polarization, as well as the COVID-19 disinfodemic. To help better detect echo chambers and mitigate its negative effects, this paper explores the mechanisms and attributes of echo chambers in social media. In particular, we first illustrate four primary mechanisms related to three main factors: human psychology, social networks, and automatic systems. We then depict common attributes of echo chambers with a focus on the diffusion of misinformation, spreading of conspiracy theory, creation of social trends, political polarization, and emotional contagion of users. We illustrate each mechanism and attribute in a multi-perspective of sociology, psychology, and social computing with recent case studies. Our analysis suggest an emerging need to detect echo chambers and mitigate their negative effects.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Efficient continuous big data integrity checking for decentralized storage\n", "abstract": " Decentralized storage powered by blockchain is becoming a new trend that allows data owners to outsource their data to remote storage resources offered by various storage providers. Unfortunately, unqualified storage providers easily encounter unpredictable downtime due to security threats, such as malicious attacks or system failures, which is unacceptable in many real-time or data-driven applications. As a result, continuous data integrity should be guaranteed in decentralized storage, which ensures that data is intact and available for the entire storage period. However, this requires frequent checking for long time periods and incurs heavy burdens of both communication and computation, especially in a big data scenario. In this paper, we propose an efficient continuous big data integrity checking approach for decentralized storage. We design a data-time sampling strategy that randomly checks the integrity of\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Topic-Preserving Synthetic News Generation: An Adversarial Deep Reinforcement Learning Approach\n", "abstract": " Nowadays, there exist powerful language models such as OpenAI's GPT-2 that can generate readable text and can be fine-tuned to generate text for a specific domain. Considering GPT-2, it cannot directly generate synthetic news with respect to a given topic and the output of the language model cannot be explicitly controlled. In this paper, we study the novel problem of topic-preserving synthetic news generation. We propose a novel deep reinforcement learning-based method to control the output of GPT-2 with respect to a given news topic. When generating text using GPT-2, by default, the most probable word is selected from the vocabulary. Instead of selecting the best word each time from GPT-2's output, an RL agent tries to select words that optimize the matching of a given topic. In addition, using a fake news detector as an adversary, we investigate generating realistic news using our proposed method. In this paper, we consider realistic news as news that cannot be easily detected by a fake news classifier. Experimental results demonstrate the effectiveness of the proposed framework on generating topic-preserving news content than state-of-the-art baselines.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Disinformation in the Online Information Ecosystem: Detection, Mitigation and Challenges\n", "abstract": " With the rapid increase in access to internet and the subsequent growth in the population of online social media users, the quality of information posted, disseminated and consumed via these platforms is an issue of growing concern. A large fraction of the common public turn to social media platforms and in general the internet for news and even information regarding highly concerning issues such as COVID-19 symptoms. Given that the online information ecosystem is extremely noisy, fraught with misinformation and disinformation, and often contaminated by malicious agents spreading propaganda, identifying genuine and good quality information from disinformation is a challenging task for humans. In this regard, there is a significant amount of ongoing research in the directions of disinformation detection and mitigation. In this survey, we discuss the online disinformation problem, focusing on the recent 'infodemic' in the wake of the coronavirus pandemic. We then proceed to discuss the inherent challenges in disinformation research, and then elaborate on the computational and interdisciplinary approaches towards mitigation of disinformation, after a short overview of the various directions explored in detection efforts.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Causal adversarial network for learning conditional and interventional distributions\n", "abstract": " We propose a generative Causal Adversarial Network (CAN) for learning and sampling from conditional and interventional distributions. In contrast to the existing CausalGAN which requires the causal graph to be given, our proposed framework learns the causal relations from the data and generates samples accordingly. The proposed CAN comprises a two-fold process namely Label Generation Network (LGN) and Conditional Image Generation Network (CIGN). The LGN is a GAN-based architecture which learns and samples from the causal model over labels. The sampled labels are then fed to CIGN, a conditional GAN architecture, which learns the relationships amongst labels and pixels and pixels themselves and generates samples based on them. This framework is equipped with an intervention mechanism which enables. the model to generate samples from interventional distributions. We quantitatively and qualitatively assess the performance of CAN and empirically show that our model is able to generate both interventional and conditional samples without having access to the causal graph for the application of face generation on CelebA data.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Deep causal representation learning for unsupervised domain adaptation\n", "abstract": " Studies show that the representations learned by deep neural networks can be transferred to similar prediction tasks in other domains for which we do not have enough labeled data. However, as we transition to higher layers in the model, the representations become more task-specific and less generalizable. Recent research on deep domain adaptation proposed to mitigate this problem by forcing the deep model to learn more transferable feature representations across domains. This is achieved by incorporating domain adaptation methods into deep learning pipeline. The majority of existing models learn the transferable feature representations which are highly correlated with the outcome. However, correlations are not always transferable. In this paper, we propose a novel deep causal representation learning framework for unsupervised domain adaptation, in which we propose to learn domain-invariant causal representations of the input from the source domain. We simulate a virtual target domain using reweighted samples from the source domain and estimate the causal effect of features on the outcomes. The extensive comparative study demonstrates the strengths of the proposed model for unsupervised domain adaptation via causal representations.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "5 sources of clickbaits you should know! using synthetic clickbaits to improve prediction and distinguish between bot-generated and human-written headlines\n", "abstract": " Clickbait is an attractive yet misleading headline that lures readers to commit click-conversion. Development of robust clickbait detection models has been, however, hampered due to the shortage of high-quality labeled training samples. To overcome this challenge, we investigate how to exploit human-written and machine-generated synthetic clickbaits. We first ask crowdworkers and journalism students to generate clickbaity news headlines. Second, we utilize deep generative models to generate clickbaity headlines. Through empirical evaluations, we demonstrate that synthetic clickbaits by human entities and deep generative models are consistently useful in improving the accuracy of various prediction models, by as much as 14.5% in AUC, across two real datasets and different types of algorithms. Especially, we observe an improvement in accuracy, up to 8.5% in AUC, even for top-ranked clickbait detectors\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Legislative prediction with dual uncertainty minimization from heterogeneous information\n", "abstract": " Voting on legislative bills to form new laws serves as a key function of most of the legislatures. Predicting the votes of such deliberative bodies leads better understanding of government policies and generate actionable strategies for social good. However, it is very difficult to predict legislative votes due to the myriad factors that affect the political decision\u2010making process. In this paper, we present a novel prediction model that maximizes the usage of publicly accessible heterogeneous data, i.e., bill text and lawmakers' profile data, to carry out effective legislative prediction. In particular, we propose to design a probabilistic prediction model which archives high consistency with past vote recorders while ensuring the minimum uncertainty of the vote prediction reflecting the firm legal ground often hold by the lawmakers. In addition, the proposed legislative prediction model enjoys the following properties: inductive and\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Exploring personal attributes from unprotected interactions\n", "abstract": " Research, so far, has shown that many personal attributes, including religious and political affiliations, sexual orientation, relationship status, age, and gender, are predictable providing users' interaction data. To address these privacy concerns, users on a social networking site like Faceboook are usually left with profile settings to mark some of their data invisible. However, users sometimes interact with others using unprotected posts (eg, posts from a``Faceboook page''). Although the aim of such interactions is to help users to become more social, visibilities of these interactions are beyond their profile settings and publicly accessible to everyone. The focus of this paper is to explore such unprotected interactions so that users' are well aware of these new vulnerabilities and adopt measures to mitigate them further. In particular, we ask-are users' personal attributes predictable using only the unprotected interactions? To answer this question, we design a novel problem of predictability of users' personal attributes with unprotected interactions. The extreme sparsity patterns in users' unprotected interactions pose a serious challenge for the proposed problem. Therefore, we first provide a way to mitigate the data sparsity challenge and propose a novel attribute prediction framework using only the unprotected interactions. Experimental results on Faceboook dataset demonstrates that the proposed framework can predict users' personal attributes.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Misinformation in Social Media: Diffusion, Detection and Intervention\n", "abstract": " A rapid increase in social media services in recent years has enabled people to share and seek information effectively. The openness, however, also makes them one of the most effective channels for misinformation. Given the speed of information diffusion on social networks coupled with the widespread propagation of fake news, phishing URLs, and inaccurate information, misinformation escalates quickly and can significantly impact users with undesirable consequences and wreak havoc instantaneously. In this tutorial, we define the concept of misinformation in social media, discuss the diffusion of misinformation in social media, and introduce challenges of its identification, intervention, and prevention methods in terms of misinformation and misinformation spreaders. We use examples to illustrate how to mine misinformation in social media, and also suggest available datasets as well as possible future work.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Online Behavioral Analysis and Modeling [Guest Editorial]\n", "abstract": " Online behavioral analysis and modeling has aroused considerable interest from closely related research fields such as data mining, machine learning, and information retrieval. This special issue provides a forum for researchers in behavior analysis to review pressing needs, discuss challenging research issues, and showcase state-of-the-art research and development in modern Web platforms.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Supervised low rank matrix approximation for stable feature selection\n", "abstract": " Increasing attention has been focused on the stability of selected features or selection stability, which is becoming a new measure in determining the effectiveness of a feature selection algorithm besides the learning performance. A recent study has shown that data characteristics play a significant role in selection stability. Hence, the solution to selection instability should begin with data. In this work, we propose a novel framework with a noise-reduction step before feature selection. Noise reduction is achieved via well-known low rank matrix approximation techniques (namely SVD and NMF) in a supervised manner to reduce data noise and variance between samples from the same class. The new framework is empirically shown to be highly effective with real high-dimensional datasets improving both selection stability and the precision of selecting relevant features while maintaining the classification accuracy for\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Healthcare risk modeling for medicaid patients the impact of sampling on the prediction of high-cost patients\n", "abstract": " Healthcare data from the Arizona Health Care Cost Containment System, Arizona's Medicaid program provides a unique opportunity to exploit state-of-the-art data processing and analysis algorithms to mine data and provide actionable findings that can aid cost containment. Our work addresses specific challenges in this real-life healthcare application to build predictive risk models for forecasting future high-cost patients. We survey the literature and propose novel data mining approaches customized for this compelling application with specific focus on non-random sampling. Our empirical study indicates that the proposed approach is highly effective and can benefit further research on cost containment in the healthcare industry.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Familiar Strangers: Connecting Dots on Blogosphere\n", "abstract": " We propose to study\" familiar strangers\" on the blogosphere who are not directly connected, but share some patterns in their blogging activities. The nature of the Web is a scale-free network such that a power law distribution applies to the bloggers. That is, the majority of bloggers are only connected with a small number of fellow bloggers, and these blogging groups are largely disconnected from each other. Motivated by Web 2.0 marketing 4Ps, we study a novel problem: connecting the dots in the long tail and aggregating disconnected bloggers to allow for cost-effective personalized services, targeted marketing, and exploration of new business opportunities. The challenges of searching the long tail demand novel algorithms due to the fragmented distributions of bloggers. In this work, we define the problem, illustrate its challenges, and present a context-based approach that innovatively employs contextual\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Identifying the influentials in Blogosphere\n", "abstract": " Blogging becomes a popular way for a Web user to publish information on the Web. Bloggers write blog posts, share their likes and dislikes, voice their opinions, provide suggestions, report news, and form groups in Blogosphere. As its size doubled for every 6 months, Blogosphere is expanding with about 175,500 daily blogs. Bloggers form their virtual communities of similar interests. Activities happened in Blogosphere affect the external world in many ways. The conventional journalism now looks more into this form of participatory journalism for events of special interests, and collective wisdoms are tapped for unique deals and great opportunities. In order to track and understand the development on Blogosphere, in the past, researchers have focused on finding influential blog sites. Regardless of a blog site being influential or not, inspired by the high impact of the influentials in a physical community, we endeavor to identify the influential bloggers in a virtual community (a blog site). There are many non-influential blog sites (\u201cthe long tail\u201d) and they can still have their influential bloggers. Active bloggers are not necessarily influential ones. There may be more than one influential blogger at one site given the nature of Blogosphere. Influential bloggers can impact other fellow bloggers in various ways. In this paper, recognizing the challenges of identifying influential bloggers, we investigate what constitutes influential bloggers, present a preliminary model to quantify an influential blogger to pave the way for a robust model that allows for finding various types of the influentials. To illustrate these issues, we conduct experiments with data from a real\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Clustering with Collective Wisdom-A Comparative Study\n", "abstract": " Blogosphere is expanding in an unprecedented speed. A better understanding of the blogosphere can greatly facilitate the development of the Social Web to serve the needs of users, service providers and advertisers. One important task in this process is clustering blog sites. Though there exist a good number of traditional clustering methods, they are not designed to take into account the unique characteristics of the blogosphere. Clustering blog sites presents new challenges. A prominent feature of the Social Web is that many enthusiastic bloggers voluntarily write, tag, and catalog their posts in order to reach the widest possible audience who will share their thoughts and appreciate their ideas. In the process a new kind of collective wisdom is generated. We propose to tap into this collective wisdom in clustering blog sites. In this paper, we study how clustering with collective wisdom can be achieved and compare\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Active Learning of Feature Relevance\n", "abstract": " This chapter deals with active feature value acquisition for feature relevance estimation in domains where feature values are expensive to measure. The following two examples motivate our work.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Active learning for detecting a spectrally variable subject in color infrared imagery\n", "abstract": " To classify Egeria densa, Brazilian waterweed, in scan-digitized color infrared aerial photographs, we are developing an interactive computer system based on data-mining techniques with active learning capabilities. Key components of the system are: feature extraction, automatic classification, active learning, and experimental evaluation.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Using feature transformation and selection with polynomial networks\n", "abstract": " Polynomial networks have proven successful in authentication applications such as speaker recognition. A drawback of these methods is that as the degree of the polynomial network is increased, the number of model terms increases rapidly. This rapid increase can result in over fitting and make the network difficult to use in real-world applications because of the large number of model terms. We propose and contrast two solutions to this problem. First, we show how random dimension reduction can be used to effectively control model complexity. We describe a novel method which allows quick reduction of the dimension using an FFT. Applying these methods to a speaker recognition problem shows an approximately linear relation between the log of the number of model parameters and the log of the error rate. Second, we apply several methods of feature selection to reduce both model complexity and\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Sampling and its application in data mining: A survey\n", "abstract": " Large data sets are becoming obstacles for efficient data mining. Sampling, as a well-established technique in statistics, is desired to play its role in overcoming the obstacles. Statistical community has provided plenty of sampling strategies which are generally believed also applicable in data mining. However, since data mining community has different starting-points and requirements from statistics community, some of these strategies may need to be reexamined when applied to data mining and it is also desirable to invent novel strategies for specific data mining tasks on specific data. This paper summarizes basic ideas and general considerations of sampling and categorizes sampling strategies existing in statistics so as to obtain potentially useful sampling strategies for data mining. Then the state-of-the-art ways of applying sampling in data mining are reviewed. By analyzing the strategies used in different data mining tasks and relating them to their precedents in statistics, we show that how traditional strategies are directly or indirectly applied. We discuss general considerations and research issues of sampling in data mining. We show that these issues are either usually not considered in statistics or not well-studied yet but essential to data mining. We believe extensive studies on sampling will contribute more to data mining, especially when dealing with large data sets.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Feature selection methods\n", "abstract": " With the unified model, we were able to study the three major aspect in the previous chapter. Now we look at possible combinations of these aspects, which can be used to construct a feature selection method. The objective of this chapter is three-fold: (a) to categorize the existing methods in a framework defined by the three major aspects; (b) to discover what have not been done and what can be done; and (c) to pave the way towards a meta system that links applications to specific methods.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Feature selection aspects\n", "abstract": " With a unified model of feature selection, we are ready to discuss in detail different aspects of feature selection. The major aspects of feature selection are (1) search directions (feature subset generation), (2) search strategies, and (3) evaluation measures. The objective of this chapter is two-fold: (a) to study the various options for each aspect in a systematic and principled way and (b) to identify the essential and different characteristics of various feature selection systems.", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Perspectives of Feature Selection\n", "abstract": " From here on, we study feature selection for classification. By choosing this type of feature selection, we can focus on many common perspectives of feature selection, obtain a deep understanding of basic issues of feature selection, appreciate many different methods of feature selection, and later in the book move on to topics related to feature selection. The problem of feature selection can be examined in many perspectives. The four major ones are (1) how to search for the \u201cbest\u201d features? (2) what should be used to determine best features, or what are the criteria for evaluation? (3) how should new features be generated for selection, adding or deleting one feature to the existing subset or changing a subset of features? (That is, feature generation is conducted sequentially or in parallel.) and (4) how applications determine feature selection? Applications have different requirements in terms of computational\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Critics for knowledge-based design systems\n", "abstract": " Expert critics have been built to critique human performance in various areas such as engineering design, decision making, etc. We suggest that critics can also be useful in the building and use of knowledge based design systems (KBDSs). Knowledge engineers elicit knowledge from domain experts and build a knowledge based design system. The system generates designs. The amount of knowledge the system possesses and the way it applies the knowledge directly influence the performance of its designs. Therefore, critics are proposed to assist in: acquiring sufficient knowledge for constructing a desirable system; and applying proper knowledge to generating designs. Methodologies of equipping a KBDS with critics are developed. Our practice in building and using a KBDS shows the applicability and capability of these critics.< >", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Design, evaluation and redesign\n", "abstract": " A knowledge-based design (KBD) system applies human expertise to create designs. The right choice of a particular set of heuristics for a given design is considered. The design problem is outlined, and a KB system that automates design in this domain is described. The methods used to deal with the ad hoc nature of such a system are discussed. It is demonstrated that the system generates better designs more often by choosing different sets of design rules in the light of varied situations. The problems, such as how to evaluate, how to redesign, and what is the role of experts in redesign, are studied. The result is a practical, operational system with backtracking capabilities. A practical case in telecommunications is exhibited to show how evaluation and redesign are performed.<>", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Structure based Mining of Hierarchical Media Data, Meta-Data, and Ontologies\n", "abstract": " Users now have access to unprecedented amounts of media data, but it is increasingly difficult to integrate relevant media from multiple and diverse sources. The functioning of a multimodal integration system requires metadata, such as ontologies, that describe media resources and media components. Such metadata are generally application dependent and this can cause difficulties when media needs to be shared across application domains. There is a need for a mechanism that can relate the common and uncommon terms and media components. In this paper, we develop an algorithm to mine and automatically discover mappings in hierarchical media data, meta-data, and ontologies, using the structural information inherent in these types of data. We evaluate the performance of this algorithm for various parameters using both synthetic and real data collections and show that the structure based mining of\u00a0\u2026", "num_citations": "2\n", "authors": ["1157"]}
{"title": "Cross-Domain Graph Anomaly Detection\n", "abstract": " Anomaly detection on attributed graphs has received increasing research attention lately due to the broad applications in various high-impact domains, such as cybersecurity, finance, and healthcare. Heretofore, most of the existing efforts are predominately performed in an unsupervised manner due to the expensive cost of acquiring anomaly labels, especially for newly formed domains. How to leverage the invaluable auxiliary information from a labeled attributed graph to facilitate the anomaly detection in the unlabeled attributed graph is seldom investigated. In this study, we aim to tackle the problem of cross-domain graph anomaly detection with domain adaptation. However, this task remains nontrivial mainly due to: 1) the data heterogeneity including both the topological structure and nodal attributes in an attributed graph and 2) the complexity of capturing both invariant and specific anomalies on the target\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Privacy Preserving Text Representation Learning Using BERT\n", "abstract": " The availability of user generated textual data in different activities online, such as tweets and reviews has been used in many machine learning models. However, the user generated text could be a privacy leakage source for the individuals\u2019 private-attributes. In this paper, we study the privacy issues in the user generated text and propose a privacy-preserving text representation learning framework, , which learns the textual representation. Our proposed framework uses BERT to extract the sentences embedding to learn the textual representation that (1) is differentially private to protect against identity leakage (e.g., if a target instance in the data or not), (2) protects against leakage of private-attributes information (e.g., age, gender, location), and (3) maintains the high utility of the given text.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Causal Mediation Analysis with Hidden Confounders\n", "abstract": " An important problem in causal inference is to break down the total effect of treatment into different causal pathways and quantify the causal effect in each pathway. Causal mediation analysis (CMA) is a formal statistical approach for identifying and estimating these causal effects. Central to CMA is the sequential ignorability assumption that implies all pre-treatment confounders are measured and they can capture different types of confounding, e.g., post-treatment confounders and hidden confounders. Typically unverifiable in observational studies, this assumption restrains both the coverage and practicality of conventional methods. This work, therefore, aims to circumvent the stringent assumption by following a causal graph with a unified confounder and its proxy variables. Our core contribution is an algorithm that combines deep latent-variable models and proxy strategy to jointly infer a unified surrogate confounder and estimate different causal effects in CMA from observed variables. Empirical evaluations using both synthetic and semi-synthetic datasets validate the effectiveness of the proposed method.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "An Overview on Protecting User Private-Attribute Information on Social Networks\n", "abstract": " Online social networks enable users to participate in different activities, such as connecting with each other and sharing different contents online. These activities lead to the generation of vast amounts of user data online. Publishing user-generated data causes the problem of user privacy as this data includes information about users' private and sensitive attributes. This privacy issue mandates social media data publishers to protect users' privacy by anonymizing user-generated social media data. Existing private-attribute inference attacks can be classified into two classes: friend-based private-attribute attacks and behavior-based private-attribute attacks. Consequently, various privacy protection models are proposed to protect users against private-attribute inference attacks such as k-anonymity and differential privacy. This chapter will overview and compare recent state-of-the-art researches in terms of private\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "\"Let's Eat Grandma\": When Punctuation Matters in Sentence Representation for Sentiment Analysis\n", "abstract": " Neural network-based embeddings have been the mainstream approach for creating a vector representation of the text to capture lexical and semantic similarities and dissimilarities. In general, existing encoding methods dismiss the punctuation as insignificant information; consequently, they are routinely eliminated in the pre-processing phase as they are shown to improve task performance. In this paper, we hypothesize that punctuation could play a significant role in sentiment analysis and propose a novel representation model to improve syntactic and contextual performance. We corroborate our findings by conducting experiments on publicly available datasets and verify that our model can identify the sentiments more accurately over other state-of-the-art baseline methods.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Toward A Multilingual and Multimodal Data Repository for COVID-19 Disinformation\n", "abstract": " The COVID-19 epidemic is considered as the global health crisis of the whole society and the greatest challenge mankind faced since World War Two. Unfortunately, the fake news about COVID-19 is spreading as fast as the virus itself. The incorrect health measurements, anxiety, and hate speeches will have bad consequences on people\u2019s physical health, as well as their mental health in the whole world. To help better combat the COVID-19 fake news, we propose a new fake news detection dataset MM-COVID 1  (Multilingual and Multidimensional COVID-19 Fake News Data Repository). This dataset provides the multilingual fake news and the relevant social context. We collect 3981 pieces of fake news content and 7192 trustworthy information from English, Spanish, Portuguese, Hindi, French and Italian, 6 different languages. We present a detailed and exploratory analysis of MM-COVID from different perspectives.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Early detection of fake news with multi-source weak social supervision\n", "abstract": " Social media has greatly enabled people to participate in online activities at an unprecedented rate. However, this unrestricted access also exacerbates the spread of misinformation and fake news which cause confusion and chaos if not detected in a timely manner. Given the rapidly evolving nature of news events and the limited amount of annotated data, state-of-the-art systems on fake news detection face challenges for early detection. In this work, we exploit multiple weak signals from different sources from user engagements with contents (referred to as weak social supervision), and their complementary utilities to detect fake news. We jointly leverage limited amount of clean data along with weak signals from social engagements to train a fake news detector in a meta-learning framework which estimates the quality of different weak instances. Experiments on real-world datasets demonstrate that the\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Systems and methods for predicting personal attributes based on public interaction data\n", "abstract": " Embodiments of a system for determining personal attributes based on public interaction data are illustrated. In one embodiment, the system employs a process for predicting personal attributes based on public interaction data by constructing matrices based on user interactions drawn from public posts on a social media website. The process may further learn a compact representation for a plurality of users based on public posts using the matrices, extract the compact representation of one or more users that have been labeled, and apply a classifier to learn about a particular personal attribute. Through this, a prediction of personal attributes of users that have not been labeled may be obtained.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Detecting pattern anomalies in hydrological time series with weighted probabilistic suffix trees\n", "abstract": " Anomalous patterns are common phenomena in time series datasets. The presence of anomalous patterns in hydrological data may represent some anomalous hydrometeorological events that are significantly different from others and induce a bias in the decision-making process related to design, operation and management of water resources. Hence, it is necessary to extract those \u201canomalous\u201d knowledge that can provide valuable and useful information for future hydrological analysis and forecasting from hydrological data. This paper focuses on the problem of detecting anomalous patterns from hydrological time series data, and proposes an effective and accurate anomalous pattern detection approach, TFSAX_wPST, which combines the advantages of the Trend Feature Symbolic Aggregate approximation (TFSAX) and weighted Probabilistic Suffix Tree (wPST). Experiments with different hydrological real-world time series are reported, and the results indicate that the proposed methods are fast and can correctly detect anomalous patterns for hydrological time series analysis, and thus promote the deep analysis and continuous utilization of hydrological time series data. View Full-Text", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Method and apparatus for detecting fake news in a social media network\n", "abstract": " Messages are transmitted in a social media network. Embeddings of social media network users in the social media network are inferred. Propagation pathways over which the plurality of messages are transmitted through the social media network are classified. Action is taken on one or more of the messages that are transmitted through the social media network, based on the classification of the propagation pathways over which the messages are transmitted through the social media network and the inferred embeddings of the social media network users.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Online newton step algorithm with estimated gradient\n", "abstract": " Online learning with limited information feedback (bandit) tries to solve the problem where an online learner receives partial feedback information from the environment in the course of learning. Under this setting, Flaxman et al.[8] extended Zinkevich's classical Online Gradient Descent (OGD) algorithm [29] by proposing the Online Gradient Descent with Expected Gradient (OGDEG) algorithm. Specifically, it uses a simple trick to approximate the gradient of the loss function  by evaluating it at a single point and bounds the expected regret as  [8], where the number of rounds is . Meanwhile, past research efforts have shown that compared with the first-order algorithms, second-order online learning algorithms such as Online Newton Step (ONS) [11] can significantly accelerate the convergence rate of traditional online learning algorithms. Motivated by this, this paper aims to exploit the second-order information to speed up the convergence of the OGDEG algorithm. In particular, we extend the ONS algorithm with the trick of expected gradient and develop a novel second-order online learning algorithm, i.e., Online Newton Step with Expected Gradient (ONSEG). Theoretically, we show that the proposed ONSEG algorithm significantly reduces the expected regret of OGDEG algorithm from  to  in the bandit feedback scenario. Empirically, we further demonstrate the advantages of the proposed algorithm on multiple real-world datasets.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Exploiting user actions for app recommendations\n", "abstract": " Mobile Applications (or Apps) are becoming more and more popular in recent years, which has attracted increasing attention on mobile App recommendations. The majority of existing App recommendation algorithms focus on mining App functionality or user usage data for discovering user preferences; while actions taken by a user when he/she decides to download an App or not are ignored. In realistic scenarios, a user will first view the description of the App and then decide if he/she wants to download it or not. The actions such as viewing or downloading provide rich information about users' preferences and tastes for Apps, which have great potentials to advance App recommendations. However, the work on exploring action data for App recommendations is rather limited. Therefore, in this paper we study the novel problem of exploiting user actions for App recommendations. We propose a new framework\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Using social media to understand cyber attack behavior\n", "abstract": " As networked and computer technologies continue to pervade all aspects of our lives, the threat from cyber attacks has also increased. However, detecting attacks, much less predicting them in advance, is a non-trivial task due to the anonymity of cyber attackers and the ambiguity of network data collected within an organization; often, by the time an attack pattern is recognized, the damage has already been done. Evidence suggests that the public discourse in external sources, such as news and social media, is often correlated with the occurrence of larger phenomena, such as election results or violent attacks. In this paper, we propose an approach that uses sentiment polarity as a sensor to analyze the social behavior of groups on social media as an indicator of cyber at-tack behavior. We developed an unsupervised sentiment prediction method that uses emotional signals to enhance the sentiment signal\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Beyond the Scale of Big Data\n", "abstract": " Voluminous data brings out a new heat-wave of machine learning with the concomitant escalation of computing power. People of all walks of life have witnessed unprecedented achievements by tapping on data. Complementing the adage of \u201cknowledge is power,\u201d we learn that \u201cdata is the new oil.\u201d The pervasive big data has demonstrated its great potential in artificial intelligent (AI) research and advancement. We are getting better at discovering knowledge from data and acquiring intelligence from information. Data, as an indispensable source for data mining and machine learning, can only become bigger and more. Big data also exhibits characteristics that go beyond scale. Big data is multifaceted including disparate dimensions such as social, spatial, relational, educational, structured, unstructured or semi-structured, and user-generated. Big data evolves in temporal or streaming forms. With such a big variety of data, big data is not just confined in centralized database management systems or warehouses as in the recent past. Big data can be produced anytime anywhere, shared explicitly or implicitly, linked automatically, and obtained by easy-to-use tools via crawling or scraping.All these new developments of data production result in some obvious questions for and/or from researchers, practitioners, and concerned citizens. Two prominent issues at the heart of this specialty section of \u201cData Mining and Management\u201d are privacy and integration of data mining and data management in the era of big data. The issue of privacy cannot be overstated given recent egregious large-scale privacy breaches and the promulgation of the General Data\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "NKG2D is Required for Regulation of Lung Pathology and Dendritic Cell Function Following RSV Infection\n", "abstract": " Background           Respiratory syncytial virus (RSV) is a common cause of respiratory tract infection in vulnerable populations. Natural killer (NK) cells and dendritic cells (DC) are important for the effector functions of both cell types following infection.                             Methods           Wild-type and NKG2D-deficient mice were infected with RSV. Lung pathology was assessed by histology. Dendritic cell function and phenotype were evaluated by enzyme-linked immunosorbent assay and flow cytometry. The expression of NKG2D ligands on lung and lymph node DCs was measured by immunostaining and flow cytometry. Adoptive transfer experiments were performed to assess the importance of NKG2D-dependent DC function in RSV infection.                             Results           NKG2D-deficient mice exhibited greater lung pathology, marked by the accumulation of DCs following RSV infection. Dendritic cells\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Systems and methods for embedded unsupervised feature selection\n", "abstract": " Systems and methods for executing an unsupervised feature selection algorithm on a processor which directly embeds feature selection into a clustering algorithm using sparse learning are disclosed. The direct embedding of the feature selection, via sparse learning, reduces storage requirement of collected data. In one method, unsupervised feature selection may be accomplished through a removal of redundant, irrelevant, and/or noisy features of incoming high-dimensional data.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Replacing mechanical turkers? challenges in the evaluation of models with semantic properties\n", "abstract": " Some machine-learning algorithms offer more than just predictive power. For example, algorithms provide additional insight into the underlying data. Examples of these algorithms are topic modeling algorithms such as Latent Dirichlet Allocation (LDA)[Blei et al. 2003], whose topics are often inspected as part of the analysis that many researchers perform on their data. Recently, deep learning algorithms such as word embedding algorithms like Word2Vec [Mikolov et al. 2013] have produced models with semantic properties. These algorithms are immensely useful; they tell us something about the environment from which they generate their predictions. One pressing challenge is how to evaluate the quality of the semantic information produced by these algorithms. When we employ algorithms for their semantic properties, it is important that these properties can be understood by a human. Currently, there are no\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Community cores: Removing size bias from community detection\n", "abstract": " Community discovery in social networks has received a significant amount of attention in the social me-dia research community. The techniques developed by the community have become quite adept at identifying the large communities in a network, but often neglect smaller communities. Evaluation techniques also show this bias, as the resolution limit problem in modular-ity indicates. Small communities, however, account for a higher proportion of a social network\u2019s community membership and reveal important information about the members of these communities. In this work, we intro-duce a re-weighting method to improve both the over-all performance of community detection algorithms and performance on small community detection.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Storing Twitter Data\n", "abstract": " In the previous chapter, we covered data collection methodologies. Using these methods, one can quickly amass a large volume of Tweets, Tweeters, and network information. Managing even a moderately-sized dataset is cumbersome when storing data in a text-based archive, and this solution will not give the performance needed for a real-time application. In this chapter we present some common storage methodologies for Twitter data using NoSQL.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Some computational challenges in mining social media\n", "abstract": " \u2013Identifying users with high value to the network, eg, high network activity, user activity, and external exposure", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Minimizing User Vulnerability and Retaining Social Utility in Social Media\n", "abstract": " Privacy and security are major concerns for many users of social media. When users share information (eg, data and photos) with friends, they can make their friends vulnerable to security and privacy breach with dire consequences. In our recent work, we show that it is feasible to measure user vulnerability and reduce one\u2019s vulnerability without changing the structure of a social networking site. The approach is to unfriend one\u2019s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him would significantly reduce one\u2019s own social status. In this work, we address the problem of vulnerability minimization with social utility constraints. We formally formulate the optimization problem, propose an approximation algorithm with a proven bound, and conduct empirical experiments with different forms of social utility on a large-scale Facebook dataset for performance evaluation and comparison.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Social media data integration for community detection\n", "abstract": " Community detection is an unsupervised learning task that discovers groups such that group members share more similarities or interact more frequently among themselves than with people outside groups. In social media, link information can reveal heterogeneous relationships of various strengths, but often can be noisy. Since different sources of data in social media can provide complementary information, eg, bookmarking and tagging data indicates user interests, frequency of commenting suggests the strength of ties, etc., we propose to integrate social media data of multiple types for improving the performance of community detection. We present a joint optimization framework to integrate multiple data sources for community detection. Empirical evaluation on both synthetic data and real-world social media data shows significant performance improvement of the proposed approach. This work elaborates the need for and challenges of multi-source integration of heterogeneous data types, and provides a principled way of multi-source community detection.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Introduction to the ACM TIST special issue AI in social computing and cultural modeling\n", "abstract": " Computer technology is leading to sweeping changes in the relationship between AI research and the social and behavioral sciences. Two closely related topics, social computing and cultural modeling, have become especially active and dynamic areas of research. Social computing is the study of social behavior and social context based on computational systems; cultural modeling is the modeling of perceptions and attitudes that are shared across social groups. The research in these two areas promises to provide a deeper understanding of behaviors, patterns, and potential outcomes. For this special issue, our aim was to provide a forum for interdisciplinary researchers to share their views and report original, cutting-edge research in social computing and culture modeling. Our call for papers elicited a tremendous response, and, after a careful review process by international experts, we have more high-quality\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Border Security: Supplementing Human Intelligence in a Sensor Network Using Sequential Pattern Mining\n", "abstract": " Sensors are being deployed to improve border security generating enormous collections of data and databases. Unfortunately these sensors can respond to a variety of stimuli, sometimes reacting to meaningful events and sometimes triggered by random events which are considered false alarms. The intent of this project is to supplement human intelligence in a sensor network framework that can assist in filtering and real-time decision making from the large volume of data generated. Our conceptual design of a human-computer system is to use off-line learning to identify the important patterns. The critical real-time system uses the identified patterns from off-line learning in a system that relates the risks of false alarms with the length of patterns and the time interval distributions between sensors in the patterns to allow the human to generate intervention decisions. The human would supplement the computer\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Introduction to special issue on social computing, behavioral modeling, and prediction\n", "abstract": " 6: 2\u2022 Introduction in this special issue. We are grateful to the many authors and external reviewers who helped to review papers. It is regrettable that not all good submissions could be included.\u201cExpanding Network Communities from Representative Examples\u201d by Andrew Mehler and Steven Skiena discusses a form of a classic problem of community discovery in social network analysis\u2014finding representative examples in a seed group to expand a network. Their approach incrementally expands from a seed group and adopts retrospective analysis to determine the ultimate boundaries of their community. The core problem becomes identifying a small conductance subgraph containing many members in a seed group in search of as much of the full community as possible. The authors also provide details on how they address the challenging task of experimental evaluation, for example, identifying gold standards in a\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Recent developments in reactive phosphorus-containing polymer systems\n", "abstract": " \u76ee\u524d\u56fd\u5185\u5916\u542b\u78f7\u805a\u5408\u7269\u4f53\u7cfb\u7684\u5f00\u53d1\u548c\u5e94\u7528\u5c1a\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5, \u5c06\u78f7\u5f15\u5165\u805a\u5408\u7269\u5206\u5b50\u9aa8\u67b6\u4e0a\u800c\u8d4b\u4e88\u5176\u963b\u71c3\u6027\u80fd\u662f\u76ee\u524d\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898. \u672c\u6587\u7b80\u8ff0\u4e86\u542b\u78f7\u963b\u71c3\u5242\u7684\u963b\u71c3\u673a\u7406, \u7efc\u8ff0\u4e86\u542b\u78f7\u53cd\u5e94\u578b\u963b\u71c3\u805a\u5408\u7269, \u5305\u62ec\u53cd\u5e94\u963b\u71c3\u73af\u6c27\u6811\u8102, \u805a\u82ef\u5e76\u6076\u55ea, \u805a\u4e59\u70ef\u57fa\u548c\u805a\u4e19\u70ef\u9178, \u805a\u9170\u80fa, \u805a\u9170\u4e9a\u80fa\u53ca\u805a\u6c28\u916f\u7b49\u5408\u6210\u65b9\u6cd5\u53ca\u6027\u80fd, \u5b83\u4eec\u5747\u4e3a\u5206\u5b50\u4e3b\u94fe\u6216\u4fa7\u94fe\u5e26\u542b\u78f7\u57fa\u56e2\u7684\u65e0\u5364\u578b\u9ad8\u805a\u7269.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "New challenges for feature selection in data mining and knowledge discovery\n", "abstract": " Yvan Saeys Yvan. Saeys@ psb. ugent. be Huan Liu Huan. Liu@ asu. edu Inaki Inza Inaki. Inza@ ehu. es Louis Wehenkel L. Wehenkel@ ulg. ac. be Yves Van de Peer Yves. Vandepeer@ psb. ugent. be", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Blogosphere: Research Issues, Tools and Applications\n", "abstract": " \u2022 Background: Web 2.0 and Social Networks\u2022 Blogosphere: Definition, Types, and Comparison\u2022 Blogosphere Research Issues\u2022 Tools and APIs\u2022 Data Collection\u2022 Measures, Models, and Methods\u2022 Performance, Evaluation, and Metrics\u2022 Case Studies", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Learning with Labeled Sessions.\n", "abstract": " Traditional supervised learning deals with labeled instances. In many applications such as physiological data modeling and speaker identification, however, training examples are often labeled objects and each of the labeled objects consists of multiple unlabeled instances. When classifying a new object, its class is determined by the majority of its instance classes. As a consequence of this decision rule, one challenge to learning with labeled objects (or sessions) is to determine during training which subset of the instances inside an object should belong to the class of the object. We call this type of learning \u2018session-based learning\u2019to distinguish it from the traditional supervised learning. In this paper, we introduce session-based learning problems, give a formal description of session-based learning in the context of related work, and propose an approach that is particularly designed for sessionbased learning. Empirical studies with UCI datasets and real-world data show that the proposed approach is effective for session-based learning.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Detecting a spectrally variable subject in color infrared imagery using data-mining and knowledge-engine methods\n", "abstract": " To classify Egeria densa, Brazilian waterweed, in scan-digitized color infrared aerial photographs, we are developing automated methods based on data-mining and knowledge-engine techniques. In this paper, we present progress to date, compare the results of the two approaches, and discuss current problems and anticipated solutions.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Unsupervised Feature Ranking and Selection\n", "abstract": " Dimensionality reduction is an important issue for efficient handling of large data sets. Feature selection is effective in dimensionality reduction. Many supervised feature selection methods exist. Little work has been done for unsupervised feature ranking and selection where class information is not available. In this chapter, we are concerned with the problem of determining and choosing the important original features for unsupervised data. Our method is based on the observation that removing an irrelevant feature may not change the underlying concept of the data, but not so otherwise. We propose an entropy measure for ranking features, and conduct experiments to verify that the proposed method is able to find important features. For verification purpose, we compare it with a feature ranking method (Relief) that requires class information, and test the reduced data for tasks of clustering and model\u00a0\u2026", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Data reduction: feature aggregation\n", "abstract": " Feature aggregation is a process through which a set of new features is created, its purpose is improving performance such as estimated accuracy, visualization, and comprehensibility of learned knowledge. Feature aggregation is briefly reviewed in the framework of constructive induction and functional mapping. In the former we introduce basic operators for constructing new features and a typical algorithm; in the latter, we introduce some statistical methods and a neural network method.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Mining weak rules\n", "abstract": " Finding patterns from data sets is a fundamental task of data mining. If we categorize all patterns into strong, weak, and random, conventional data mining techniques are designed only to find strong patterns, which hold for numerous objects and are usually consistent with the expectations of experts. We address the problem of finding weak patterns (i.e., reliable exceptions) from databases. They are valid for a small number of objects. A simple approach is proposed which uses deviation analysis to identify interesting exceptions and explore reliable ones. It is also flexible in handling both subjective and objective exceptions. We demonstrate the effectiveness of the proposed approach through a benchmark data set.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "E ective Data Mining Using Neural Networks\n", "abstract": " Classi cation is one of the data mining problems receiving great attention recently in the database community. This paper presents an approach to discover symbolic classi cation rules using neural networks. Neural networks have not been thought suited for data mining because how the classi cations were made is not explicitly stated as symbolic rules that are suitable for veri cation or interpretation by humans. With the proposed approach, concise symbolic rules with high accuracy can be extracted from a neural network. The network is rst trained to achieve the required accuracy rate. Redundant connections of the network are then removed by a network pruning algorithm. The activation values of the hidden units in the network are analyzed, and classi cation rules are generated using the result of this analysis. The e ectiveness of the proposed approach is clearly demonstrated by the experimental results on a set of standard data mining test problems.", "num_citations": "1\n", "authors": ["1157"]}
{"title": "Recommendation in Social Media\n", "abstract": " Individuals in social media make a variety of decisions on a daily basis. These decisions are about buying a product, purchasing a service, adding a friend, and renting a movie, among others. The individual often faces many options to choose from. These diverse options, the pursuit of optimality, and the limited knowledge that each individual has create a desire for external help. At times, we resort to search engines for recommendations; however, the results in search engines are rarely tailored to our particular tastes and are query-dependent, independent of the individuals who search for them.", "num_citations": "1\n", "authors": ["1157"]}