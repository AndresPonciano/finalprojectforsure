{"title": "Predicting the severity of a reported bug\n", "abstract": " The severity of a reported bug is a critical factor in deciding how soon it needs to be fixed. Unfortunately, while clear guidelines exist on how to assign the severity of a bug, it remains an inherent manual process left to the person reporting the bug. In this paper we investigate whether we can accurately predict the severity of a reported bug by analyzing its textual description using text mining algorithms. Based on three cases drawn from the open-source community (Mozilla, Eclipse and GNOME), we conclude that given a training set of sufficient size (approximately 500 reports per severity), it is possible to predict the severity with a reasonable accuracy (both precision and recall vary between 0.65-0.75 with Mozilla and Eclipse; 0.70-0.85 in the case of GNOME).", "num_citations": "373\n", "authors": ["107"]}
{"title": "Refactoring-improving coupling and cohesion of existing code\n", "abstract": " Refactorings are widely recognised as ways to improve the internal structure of object-oriented software while maintaining its external behaviour. Unfortunately, refactorings concentrate on the treatment of symptoms (the so called code-smells), thus improvements depend a lot on the skills of the maintained coupling and cohesion on the other hand are quality attributes which are generally recognized as being among the most likely quantifiable indicators for software maintainability. Therefore, this paper analyzes how refactorings manipulate coupling/cohesion characteristics, and how to identify refactoring opportunities that improve these characteristics. As such we provide practical guidelines for the optimal usage of refactoring in a software maintenance process.", "num_citations": "200\n", "authors": ["107"]}
{"title": "On the detection of test smells: A metrics-based approach for general fixture and eager test\n", "abstract": " As a fine-grained defect detection technique, unit testing introduces a strong dependency on the structure of the code. Accordingly, test coevolution forms an additional burden on the software developer which can be tempered by writing tests in a manner that makes them easier to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying unit testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we clarify the structural deficiencies encapsulated in test smells by formalizing core test concepts and their characteristics. To support the detection of two such test smells, General Fixture and Eager Test, we propose a set of metrics defined in terms of unit test concepts. We compare their detection effectiveness using manual inspection and through a comparison with human reviewing. Although the\u00a0\u2026", "num_citations": "148\n", "authors": ["107"]}
{"title": "Studying software evolution information by visualizing the change history\n", "abstract": " Before re-engineering a large and complex software system, it is wise to study its change history in order to identify the most valuable and problematic parts. Unfortunately, typical change histories contain thousands of entries, therefore the challenge is to discover those changes which are relevant for both the current and future situations of our product and process. We demonstrate how a simple visualization allows us to recognize relevant changes. Applying the technique on the change history of Tomcat, we have been able to identify (a) unstable components, (b) coherent entities, (c) design and architectural evolution, and (d) fluctuations in team productivity.", "num_citations": "132\n", "authors": ["107"]}
{"title": "Establishing traceability links between unit test cases and units under test\n", "abstract": " Coding and testing are two activities that are tightly intermingled in agile software development, requiring developers to frequently shift between production code and test artifacts. Unfortunately, links between these artifacts are typically implicitly present in the source code, forcing developers towards time consuming code inspections. In this work, we evaluate the potential of six traceability resolution strategies involving test naming and design conventions, static call graphs, call behavior before asserts, lexical analysis and version log mining to make the relation between developer test cases and units under test explicit. The results show that test conventions yield highly accurate results, yet in their absence capturing the destination type of calls right before assert statements appears as a valuable alternative. Combining these strategies allows the user to find a balance between improved applicability and accuracy.", "num_citations": "116\n", "authors": ["107"]}
{"title": "Why unified is not universal\n", "abstract": " UML is currently embraced as \u201cthe\u201d standard in object-oriented modeling languages, the recent work of OMG on the Meta Object Facility (MOF) being the most noteworthy example. We welcome these standardisation efforts, yet warn against the tendency to use UML as the panacea for all exchange standards. In particular, we argue that UML is not sufficient to serve as a tool-interoperability standard for integrating round-trip engineering tools, because one is forced to rely on UML\u2019s built-in extension mechanisms to adequately model the reality in source-code. Consequently, we propose an alternative meta-model (named FAMIX), which serves as the tool interoperability standard within the FAMOOS project and which includes a number of constructive suggestions that we hope will influence future releases of the UML and MOF standards.", "num_citations": "103\n", "authors": ["107"]}
{"title": "Evaluating clone detection techniques from a refactoring perspective\n", "abstract": " In the last decade, several researchers have investigated techniques to automatically detect duplicated code in programs exceeding hundreds of thousands lines of code. All of these techniques have known merits and deficiencies, but as of today, little is known on how these techniques fit into the refactoring process of object-oriented systems. This work compares three representative detection techniques (simple line matching, parameterized matching, and metric fingerprints) by means of five small to medium sized cases and analyses the differences between the reported matches. Based on this comparison, we conclude that (1) simple line matching is best suited for a partial, yet advanced restructuring with little effort; (2) metric fingerprints work best for refactoring a system with minimal effort; (3) parameterized matching demands more effort yet allows a more profound, less obvious restructuring of the code.", "num_citations": "101\n", "authors": ["107"]}
{"title": "Reconstruction of successful software evolution using clone detection\n", "abstract": " In modern software engineering, researchers regard a software system as an organic life form that must continue to evolve to remain successful. Unfortunately, little is known about how successful software systems have evolved, and consequently little has been learned from previous experience. We demonstrate a heuristic to reconstruct evolution processes of existing software systems by exploiting techniques to detect duplication in large amounts of data. A case study, evaluating various versions of Tomcat using this heuristic, revealed that the removal of duplicated code is a much smaller concern than grouping functionality in classes with one clear responsibility.", "num_citations": "97\n", "authors": ["107"]}
{"title": "Making design patterns explicit in face\n", "abstract": " Tools incorporating design patterns combine the advantage of having a high-abstraction level of describing a system and the possibility of coupling these abstractions to some underlying implementation. Still, all current tools are based on generating source code in which the design patterns become implicit. After that, further extension and adaptation of the software is needed but this can no longer be supported at the same level of abstraction. This paper presents FACE, an environment based on an explicit representation of design patterns, sustaining an incremental development style without abandoning the higher-level design pattern abstraction. A visual composition tool for FACE has been developed in the Self programming language.", "num_citations": "82\n", "authors": ["107"]}
{"title": "Evaluating clone detection techniques\n", "abstract": " In the last decade, several researchers have investigated techniques to detect duplicated code in programs exceeding hundreds of thousands lines of code. All of these techniques have known merits and deficiencies, but as of today, little is known on where to fit these techniques into the software maintenance process. This paper compares three representative detection techniques (simple line matching, parameterized matching, and metric fingerprints) by means of five small to medium cases and analyses the differences between the reported matches. Based on this experiment, we conclude that (1) simple line matching is best suited for a first crude overview of the duplicated code;(2) metric fingerprints work best in combination with a refactoring tool that is able to remove duplicated subroutines;(3) parameterized matching works best in combination with more fine-grained refactoring tools that work on the statement level.", "num_citations": "72\n", "authors": ["107"]}
{"title": "The FAMOOS Object-Oriented Reengineering Handbook\n", "abstract": " BORIS Deutsch English Fran\u00e7ais Login BORIS Bern Open Repository and Information System University of Bern Home Statistics The FAMOOS Object-Oriented Reengineering Handbook Ducasse, St\u00e9phane; Demeyer, Serge (eds.) (1999). The FAMOOS Object-Oriented Reengineering Handbook. University of Bern Full text not available from this repository. (Request a copy) Official URL: http://scg.unibe.ch/archive/papers/Duca99xFamoosHa... Item Type: Book (Edited Volume) Division/Institute: 08 Faculty of Science > Institute of Computer Science (INF) (SCG) UniBE Contributor: Ducasse, Stephane Subjects: 000 Computer science, knowledge & systems 500 Science > 510 Mathematics Publisher: University of Bern Language: English Submitter: Manuela Bamert Date Deposited: 13 Dec 2017 09:05 Last Modified: 13 Dec 2017 :: \u2026", "num_citations": "58\n", "authors": ["107"]}
{"title": "An experimental investigation of UML modeling conventions\n", "abstract": " Modelers tend to exploit the various degrees of freedom provided by the UML. The lack of uniformity and the large amount of defects contained in UML models result in miscommunication between different readers. To prevent these problems we propose modeling conventions, analogue to coding conventions for programming. This work reports on a controlled experiment to explore the effect of modeling conventions on defect density and modeling effort. 106 masters\u2019 students participated over a six-weeks period. Our results indicate that decreased defect density is attainable at the cost of increased effort when using modeling conventions, and moreover, that this trade-off is increased if tool-support is provided. Additionally we report observations on the subjects\u2019 adherence to and attitude towards modeling conventions. Our observations indicate that efficient integration of convention support in the modeling\u00a0\u2026", "num_citations": "53\n", "authors": ["107"]}
{"title": "Mining Version Control Systems for FACs (Frequently Applied Changes).\n", "abstract": " Today, programmers are forced to maintain a software system based on their gut feeling and experience. This paper makes an attempt to turn the software maintenance craft into a more disciplined activity, by mining for frequently applied changes in a version control system. Next to some initial results, we show how this technique allows to recover and study successful maintenance strategies, adopted for the redesign of long\u2013lived systems.", "num_citations": "52\n", "authors": ["107"]}
{"title": "Metrics, Do they really help?\n", "abstract": " Maturing a well designed framework requires a set of software metrics to steer the iterative development process. Based on a case study of the VisualWorks/Smalltalk framework for user-interface building, we conclude that today\u2019s size and inheritance metrics are not reliable to detect problems but are useful in measuring stability. We expect that this work will contribute to the application of metrics as a project management tool.", "num_citations": "49\n", "authors": ["107"]}
{"title": "Does the\" Refactor to Understand\" reverse engineering pattern improve program comprehension?\n", "abstract": " Program comprehension is a fundamental requirement for all but the most trivial maintenance activities. Previous research has demonstrated key principles for improving comprehension. Among others, these consist of the introduction of beacons as indexes into knowledge, and the chunking of low-level structures into higher-level abstractions. These principles are naturally reflected in the reverse engineering pattern Refactor to Understand, which uses incremental renaming and extracting of program elements as the means to decipher cryptic code. In this paper, we discuss a controlled experiment to explore differences in program comprehension between the application of Refactor to Understand and the traditional Read to Understand pattern. Our results support added value of Refactor to Understand regarding specific aspects of program comprehension and specific types of source code. These findings illustrate\u00a0\u2026", "num_citations": "46\n", "authors": ["107"]}
{"title": "Migrating towards microservices: migration and architecture smells\n", "abstract": " Migrating to microservices is an error-prone process with deep pitfalls resulting in high costs for mistakes. Microservices is a relatively new architectural style, resulting in the lack of general guidelines for migrating monoliths towards microservices. We present 9 common pitfalls in terms of bad smells with their potential solutions. Using these bad smells, pitfalls can be identified and corrected in the migration process.", "num_citations": "40\n", "authors": ["107"]}
{"title": "Fine-tuning spectrum based fault localisation with frequent method item sets\n", "abstract": " Continuous integration is a best practice adopted in modern software development teams to identify potential faults immediately upon project build. Once a fault is detected it must be repaired immediately, hence continuous integration provides an ideal testbed for experimenting with the state of the art in fault localisation. In this paper we propose a variant of what is known as spectrum based fault localisation, which leverages patterns of method calls by means of frequent itemset mining. We compare our variant (we refer to it as patterned spectrum analysis) against the state of the art and demonstrate on 351 real bugs drawn from five representative open source java projects that patterned spectrum analysis is more effective in localising the fault. Based on anecdotal evidence from this comparison, we suggest avenues for further improvements. Keywords: Automated developer tests; Continuous Integration; Spectrum\u00a0\u2026", "num_citations": "40\n", "authors": ["107"]}
{"title": "An exploratory qualitative and quantitative analysis of emotions in issue report comments of open source systems\n", "abstract": " Software development\u2014just like any other human collaboration\u2014inevitably evokes emotions like joy or sadness, which are known to affect the group dynamics within a team. Today, little is known about those individual emotions and whether they can be discerned at all in the development artifacts produced during a project. This paper analyzes (a) whether issue reports\u2014a common development artifact, rich in content\u2014convey emotional information and (b) whether humans agree on the presence of these emotions. From the analysis of the issue comments of 117 projects of the Apache Software Foundation, we find that developers express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue report, the more human raters start to doubt and nuance their interpretation. Based on these results, we demonstrate the feasibility of a machine learning classifier\u00a0\u2026", "num_citations": "39\n", "authors": ["107"]}
{"title": "Filtering bug reports for fix-time analysis\n", "abstract": " Several studies have experimented with data mining algorithms to predict the fix-time of reported bugs. Unfortunately, the fix-times as reported in typical open-source cases are heavily skewed with a significant amount of reports registering fix-times less than a few minutes. Consequently, we propose to include an additional filtering step to improve the quality of the underlying data in order to gain better results. Using a small-scale replication of a previously published bug fix-time prediction experiment, we show that the additional filtering of reported bugs indeed improves the outcome of the results.", "num_citations": "39\n", "authors": ["107"]}
{"title": "Characterizing the relative significance of a test smell\n", "abstract": " Test code, just like any other code we write, erodes when frequently changed. As such, refactoring, which has been shown to impact maintainability and comprehensibility, can be part of a solution to counter this erosion. We propose a metric-based heuristical approach, which allows to rank occurrences of so-called test smells (i.e. symptoms of poorly designed tests) according to their relative significance. This ranking can subsequently be used to start refactoring. Through an open-source case study, ArgoUML, we demonstrate that we are able to identify those test cases who violate unit test criteria", "num_citations": "39\n", "authors": ["107"]}
{"title": "A light but formal introduction to XQuery\n", "abstract": " We give a light-weight but formal introduction to XQuery by defining a sublanguage of XQuery. We ignore typing, and don\u2019t consider namespaces, comments, programming instructions, and entities. To avoid confusion we call our version LiXQuery (Light XQuery). LiXQuery is fully downwards compatible with XQuery. Its syntax and its semantics are far less complex than that of XQuery, but the typical expressions of XQuery are included in LiXQuery. We claim that LiXQuery is an elegant and simple sublanguage of XQuery that can be used for educational and research purposes. We give the complete syntax and the formal semantics of LiXQuery.", "num_citations": "39\n", "authors": ["107"]}
{"title": "Research methods in computer science.\n", "abstract": " Computer Science as a research discipline has always struggled with its identity. On the one hand, it is a field deeply rooted in mathematics which resulted in strong theories. 1 For example, there is computational complexity theory (turing machines, the halting problem), database theory (the relational model, expresive power of query languages), formal language theory (the chomsky hierarchy, well-formedness, formal semantics). On the other hand, it is a field deeply rooted in engineering which resulted in machines that have completely warped our society: the von Neumann architecture (the basis for digital computers), parallel processors (the new generation of multi-core machines), distributed computers (a prerequisite for the success of the internet and recent phenomena like grid computing). Consequently, computer science has inherited its research methods from the same disciplines: on the one hand, the mathematical approach with axioms, postulates and proofs; on the other hand the engineering approach with quantification, measurements and comparison.Software Engineering research in particular has suffered from this identity crisis, and several authors have argued the need for stronger research methods [1],[2],[3]. Moreover, software engineering research\u2014with its emphasis on processes and team work\u2014must also take into account group dynamics and cognitive factors, hence borrows research methods from sociology and psychology as well [4],[5]. Certainly, with innovations like distributed development and open source release, software engineering is at the forefront of introducing new communication paradigms, hence is itself a\u00a0\u2026", "num_citations": "31\n", "authors": ["107"]}
{"title": "Happy birthday! a trend analysis on past MSR papers\n", "abstract": " On the occasion of the 10th anniversary of the MSR conference, it is a worthwhile exercise to meditate on the past, present and future of our research discipline. Indeed, since the MSR community has experienced a big influx of researchers bringing in new ideas, state-of-the art technology and contemporary research methods it is unclear what the future might bring. In this paper, we report on a text mining exercise applied on the complete corpus of MSR papers to reflect on where we come from; where we are now; and where we should be going. We address issues like the trendy (and outdated) research topics; the frequently (and less frequently) cited cases; the popular (and emerging) mining infrastructure; and finally the proclaimed actionable information which we are deemed to uncover.", "num_citations": "30\n", "authors": ["107"]}
{"title": "Predicting reassignments of bug reports-an exploratory investigation\n", "abstract": " Bug tracking systems play an important role in the software development process since they allow users to report bugs they have encountered. Unfortunately, the information provided in the bug reports may contain errors since the reporter is not always acquainted with the technical nature of the software project. It remains a manual process left to the bug triager to correct reports with erroneous information. Consequently, this results in a delay of the resolution time of a bug. In this study, we propose to use data-mining techniques to make early predictions of which particular reported bugs are assigned to the incorrect component. This way, bug triagers are assisted in their task of identifying reported bugs where the \"component\" field contains an incorrect value. By using open-source cases like Mozilla and Eclipse, we demonstrate the possibility of predicting which particular bug is likely to be reassigned to a different\u00a0\u2026", "num_citations": "28\n", "authors": ["107"]}
{"title": "Maintainability versus performance: What\u2019s the effect of introducing polymorphism\n", "abstract": " The notion of refactoring\u2014transforming the sourcecode of an object-oriented program without changing its external behavior\u2014has been embraced by many objectoriented software developers as a way to accommodate changing requirements. If applied well, refactoring improves the maintainability of the software, however it is believed that it does so at the sake of performance. To investigate this trade-off, we compared the performance of a program which contains large conditionals against one where the conditionals were replaced by polymorphic method calls. We discovered that C++ programs refactored this way perform faster than their non-refactored counterparts, hence advise programmers not to obfuscate their programs with conditional logic in order to obtain good performance.", "num_citations": "27\n", "authors": ["107"]}
{"title": "Lightweight visualisations of COBOL code for supporting migration to SOA\n", "abstract": " In this age of complex business landscapes, many enterprises turn to Service Oriented Architecture (SOA) for aligning their IT portfolio with their business. Because of the enormous business risk involved with replacing an enterprise\u00e2s IT landscape, a stepwise migration to SOA is required. As a first step, they need to understand and assess the current structure of their legacy systems. Based on existing reverse engineering techniques, we provide visualisations to support this process for COBOL systems and present preliminary results of an ongoing industrial case study.", "num_citations": "23\n", "authors": ["107"]}
{"title": "Reverse engineering on the mainframe: Lessons learned from\" in vivo\" research\n", "abstract": " Despite growth in the popularity of desktop systems, Web applications, and mobile computing, mainframe systems remain the dominant force in large-scale enterprise computing. Although they're sometimes referred to as \"the dinosaurs of computing,\" even mainframe systems must adapt to changing circumstances to survive. Although reverse-engineering and reengineering techniques can help identify and achieve these adaptations, current techniques are mainly geared mainly toward more modern environments, languages, and platforms. It remains to be seen whether successful techniques can be easily transferred to a mainframe context. This article reports on the application of two proven reverse-engineering techniques (software visualization and feature location) in the context of mainframe systems. The authors conclude that these techniques remain viable but become very labor intensive when implemented\u00a0\u2026", "num_citations": "21\n", "authors": ["107"]}
{"title": "Exploring the composition of unit test suites\n", "abstract": " In agile software development, test code can considerably contribute to the overall source code size. Being a valuable asset both in terms of verification and documentation, the composition of a test suite needs to be well understood in order to identify opportunities as well as weaknesses for further evolution. In this paper, we argue that the visualization of structural characteristics is a viable means to support the exploration of test suites. Thanks to general agreement on a limited set of key test design principles, such visualizations are relatively easy to interpret. In particular, we present visualizations that support testers in (i) locating test cases; (ii) examining the relation between test code and production code; and (iii) studying the composition of and dependencies within test cases. By means of two case studies, we demonstrate how visual patterns help to identify key test suite characteristics.", "num_citations": "19\n", "authors": ["107"]}
{"title": "An exchange model for reengineering tools\n", "abstract": " Tools support is recognised as a key issue in the reengineering of large scale object-oriented systems. However, due to the heterogeneity in today\u2019s object-oriented programming languages, it is hard to reuse reengineering tools across legacy systems. This paper proposes a language independent exchange model, so that tools may perform their tasks independent of the underlying programming language. Beside supporting reusability between tools, we expect that this exchange model will enhance the interoperability between tools for metrics, visualization, reorganisation and other reengineering activities.", "num_citations": "19\n", "authors": ["107"]}
{"title": "Using storyboards to integrate models and informal design knowledge\n", "abstract": " Model-driven development of user interfaces has become increasingly powerful in recent years. Unfortunately, model-driven approaches have the inherent limitation that they cannot handle the informal nature of some of the artifacts used in truly multidisciplinary user interface development such as storyboards, sketches, scenarios and personas. In this chapter, we present an approach and tool support for multidisciplinary user interface development bridging informal and formal artifacts in the design and development process. Key features of the approach are the usage of annotated storyboards, which can be connected to other models through an underlying meta-model, and cross-toolkit design support based on an abstract user interface model.", "num_citations": "18\n", "authors": ["107"]}
{"title": "Estimation of test code changes using historical release data\n", "abstract": " In order to remain effective, test suites have to co-evolve alongside the production system. As such, quantifying the amount of changes in test code should be a part of effort estimation models for maintenance activities. In this paper, we verify to which extent (i) production code size, (ii) coverage measurements; and (iii) testability metrics predict the size of test code changes between two releases. For three Java and one C++ system, the size of production code changes appears to be the best predictor. We subsequently use this predictor to construct, calibrate and validate an estimation model using the historical release data. We demonstrate that is feasible to obtain a reliable prediction model, provided that at least 5 to 10 releases are available.", "num_citations": "18\n", "authors": ["107"]}
{"title": "Refactor conditionals into polymorphism: what's the performance cost of introducing virtual calls?\n", "abstract": " The notion of refactoring - transforming the source-code of an object-oriented program without changing its external behavior - has been embraced by many object-oriented software developers as a way to accommodate changing requirements. If applied well, refactoring improves the maintainability of the software. However, it is believed that it does so at the sake of performance. Especially in a C++ context, the introduction of virtual function calls is often blamed for performance reduction because it introduces an extra indirection via the so-called virtual function table. To investigate the performance trade-off involved when introducing virtual functions, we compared the execution time of four benchmark programs which contain large conditionals against refactored versions where the conditionals were replaced by virtual function calls. Depending on the compiler and compiler optimizations being used, we discovered\u00a0\u2026", "num_citations": "18\n", "authors": ["107"]}
{"title": "Indoor environmental quality index for conservation environments: The importance of including particulate matter\n", "abstract": " It is commonly known that the conservation state of works of arts exhibited inside museums is strongly influenced by the indoor environmental quality (IEQ). Heritage institutions traditionally record and evaluate their IEQ by monitoring temperature, relative humidity, and -more rarely-light. However, smart use of technology enables monitoring other parameters that give a more complete insight in environmental \u2018air aggressiveness\u2019. One of this parameters is particulate matter (PM) and especially its concentration, size distribution and chemical composition. In this work, we present a selection of data sets which were obtained in a measuring campaign performed in the War Heritage Institute in Brussels, Belgium. A continuous monitoring of PM concentration with a light scattering based particle counter was performed. In addition the daily mass concentration and size distribution of airborne PM was monitored by means of\u00a0\u2026", "num_citations": "17\n", "authors": ["107"]}
{"title": "Formalising refactorings with graph transformations\n", "abstract": " The widespread interest in refactoring\u2014transforming the source-code of an objectoriented program without changing its external behaviour\u2014has increased the need for a precise definition of refactoring transformations and their properties. This paper introduces a graph representation of those aspects of the source code that should be preserved by refactorings, and graph transformations as a formal specification for the refactorings themselves. To this aim, we use type graphs, forbidden subgraphs, embedding mechanisms, negative application conditions and controlled graph rewriting. We show that it is feasible to reason about the effect of refactorings on object-oriented programs independently of the programming language being used. This is crucial for the next generation of refactoring tools.", "num_citations": "17\n", "authors": ["107"]}
{"title": "Analysis of overridden methods to infer hot spots\n", "abstract": " An object-oriented framework represents a design for a family of applications, where variations in the application domain are specified by means of hot spots. Identifying the right combination of hot spots is known to be difficult and best achieved via an iterative development process. However, due to this iterative process, project managers are tempted to postpone the writing of documentation until the framework is stable. Moreover, since parts of the program will be redesigned frequently, it is hard to synchronize documentation with source-code. Thus, iterative development leads to undocumented -or worse- incorrectly documented hot spots, limiting the reusability of the framework architecture.", "num_citations": "17\n", "authors": ["107"]}
{"title": "Zypher-tailorability as a link from object-oriented software engineering to open hypermedia\n", "abstract": " The dissertation concerns a study of state of the art object-oriented software engineering applied within the domain of open hypermedia systems. The results of this study are discussed within the context of a software artefact named Zypher. The scientific contribution of this work is situated in the domain of object-oriented software engineering. The contribution is a proper combination of frameworks and meta-object protocols, which are two promising techniques in object-oriented software engineering. We show that, when combining both approaches, explicit representations of framework contracts are part of a meta-object protocol. This insight is valuable in the design of metaobject protocols. The design of meta-object protocols is crucial in the development of large scale software systems, because it leads to greater flexibility. Flexibility is a key requirement, since large scale software systems are bound to satisfy ever changing demands, hence must be adapted continuously. This flexi...", "num_citations": "17\n", "authors": ["107"]}
{"title": "Redocumentation of a legacy banking system: an experience report\n", "abstract": " Successful software systems need to be maintained. In order to do that, deep knowledge about their architecture and implementation details is required. This knowledge is often kept implicit (inside the heads of the experts) and sometimes made explicit in documentation. The problem is that systems often lack up-to-date documentation and that system experts are frequently unavailable (as they got another job or retired). Redocumentation addresses that problem by recovering knowledge about the system and making it explicit in documentation. Automating the redocumentation process can limit the tedious and error-prone manual effort, but it is no'silver bullet'. In this paper, we report on our experience with applying redocumentation techniques in industry. We provide insights on what (not) to document, what (not) to automate and how to automate it. A concrete lesson learned during this study is that the\" less is more\u00a0\u2026", "num_citations": "16\n", "authors": ["107"]}
{"title": "Why FAMIX and not UML\n", "abstract": " UML is currently embraced as\u2019 the\u2019standard in object-oriented modeling languages, the recent work of OMG on the Meta Object Facility (MOF) being the most noteworthy example. We welcome these standardisation efforts, yet warn against the tendency to use UML as the panacea for all exchange standards. In particular, we argue that UML is not sufficient to serve as a toolinteroperability standard for integrating round-trip engineering tools, because one is forced to rely on UML\u2019s built-in extension mechanisms to adequately model the reality in source-code. Consequently, we propose an alternative meta-model (named FAMIX), which serves as the tool interoperability standard within the FAMOOS project and which includes a number of constructive suggestions that we hope will influence future releases of the UML and MOF standards.", "num_citations": "16\n", "authors": ["107"]}
{"title": "SNiFF+ talks to Rational Rose-interoperability using a common exchange model\n", "abstract": " Nowadays development environments are required to be open: users want to be able to work with a combination of their preferred commercial and home-grown tools. TakeFive has opened up SNiFF+ with a so-called\" Symbol Table API\"; Rational has opened up the UML tool Rose via the so-called\" Rose Extensibility Interface (REI)\". On the other hand, efforts are underway to define standards for exchanging information between case-tools; CDIF being a notable example. This paper reports on our experience to generate UML diagrams in Rational Rose from the symbol table in SNiFF+ using a standard CDIF exchange format. More information on the model and the SNiFF+ extractor is available at: http://www. iam. unibe. ch/~ famoos/InfoExchFormat/. All comments are welcome: famoos@ iam. unibe. ch. 1) Introduction Tool interoperability is becoming of more and more importance. Developers want their favourite environment for their favourite programming language and might use in parallel some external tools, for inst...", "num_citations": "15\n", "authors": ["107"]}
{"title": "Feature location in COBOL mainframe systems: An experience report\n", "abstract": " Over the last decade, numerous techniques have been proposed in the literature to reverse engineer large legacy systems, several of them even claiming success on industrial scale. Consequently, when faced with a reverse engineering request from a large banking company, we decided to reuse an existing technique (namely, feature location using formal concept analysis on execution profiles) and see whether we could replicate their results. This paper reports our experience with such a replication experiment: we list those things that worked well (and fortunately, there were quite a few) and those things that did not work so well (and try to identify root causes and solutions for the problems we encountered).", "num_citations": "14\n", "authors": ["107"]}
{"title": "Studying versioning information to understand inheritance hierarchy changes\n", "abstract": " With the widespread adoption of object-oriented programming, changing the inheritance hierarchy became an inherent part of today's software maintenance activities. Unfortunately, little is known about the \"state-of-the-practice \" with respect to changing an application's inheritance hierarchy, and consequently we do not know how the change process can be improved. In this paper, we report on a study of the hierarchy changes stored in a versioning system to explore the answers to three research questions: (1) why are hierarchy changes made? (2) what kind of hierarchy changes are made? (3) what is the impact of these changes? Based on the results of this study, we formulate 7 hypotheses which should be investigated further to make conclusive interpretations on how hierarchy changes fit in the actual change process.", "num_citations": "14\n", "authors": ["107"]}
{"title": "DEVS for AUTOSAR-based system deployment modeling and simulation\n", "abstract": " AUTOSAR (AUTomotive Open System ARchitecture) is an open and standardized automotive software architecture, developed by automobile manufacturers, suppliers, and tool developers. Its design is a direct consequence of the increasingly important role played by software in vehicles. As design choices during the software deployment phase have a large impact on the behavior of the system, designers need to explore various trade-offs. Examples of such design choices are the mapping of software components to processors, the priorities of tasks and messages, and buffer allocation. In this paper, we evaluate the appropriateness of DEVS, the Discrete-Event System specification, for modeling and subsequent performance evaluation of AUTOSAR-based systems. Moreover, a DEVS simulation model is constructed for AUTOSAR-based electronic control units connected by a communication bus. To aid\u00a0\u2026", "num_citations": "13\n", "authors": ["107"]}
{"title": "Detecting move operations in versioning information\n", "abstract": " Recently, there is an increasing research interest in mining versioning information, i.e. the analysis of the transactions made on version systems to understand how and when a software system evolves. One particular area of interest is the identification of move operations as these are key indicators for refactorings. Unfortunately, there exists no evaluation which identifies the quality (expressed in precision and recall) of the most commonly used detection technique and its underlying principle of name identity. To overcome this problem, the paper compares the precision and recall values of the name-based technique with two alternative techniques, one based on line matching and one based on identifier matching, by means of two case studies. From the results of these studies we conclude that the name-based technique is very precise, yet misses a significant number of move operations (low recall value). To\u00a0\u2026", "num_citations": "13\n", "authors": ["107"]}
{"title": "Consistent framework documentation with computed links and framework contracts\n", "abstract": " Since an object-oriented framework is an evolving artifact, ensuring consistency between its documentation and its implementation is difficult. This paper reports on the use of open hypermedia to keep framework documentation up-to-date. In particular, we demonstrate how one can feed framework contracts into computational hypermedia links to ensure the consistency between the source code and the framework cookbook.", "num_citations": "13\n", "authors": ["107"]}
{"title": "Localising faults in test execution traces\n", "abstract": " With the advent of agile processes and their emphasis on continuous integration, automated tests became the prominent driver of the development process. When one of the thousands of tests fails, the corresponding fault should be localised as quickly as possible as development can only proceed when the fault is repaired. In this paper we propose a heuristic named SPEQTRA which mines the execution traces of a series of passing and failing tests, to localise the class which contains the fault. SPEQTRA produces ranking of classes that indicates the likelihood of classes to be at fault. We compare our spectrum based fault localisation heuristic with the state of the art (AMPLE) and demonstrate on a small yet representative case (NanoXML) that the ranking of classes proposed by SPEQTRA is significantly better than the one of AMPLE.", "num_citations": "11\n", "authors": ["107"]}
{"title": "Change impact analysis for UML model maintenance\n", "abstract": " Software maintenance is generally considered to be the most costly phase in the software life-cycle. The software system to be maintained consists of numerous inter-dependent artifacts that inevitably undergo various changes during maintenance. What makes planning and executing these changes difficult is that each change may have severe \u201cripple effects\u201d to other points of the system that are difficult to assess due to the inter-dependent nature of the artifacts. The goal of this chapter is to introduce a lightweight and accurate change impact analysis technique for UML models. Impact analysis rules are created that trace different relationships between UML model elements depending on the type of change applied. We will show that we can achieve good accuracy. To validate the technique, a change scenario that consists of changes that occur during the resolution of inconsistencies between different UML models\u00a0\u2026", "num_citations": "11\n", "authors": ["107"]}
{"title": "Optimizing data structures at the modeling level in embedded multimedia\n", "abstract": " Traditional design techniques for embedded systems apply transformations on the source code to optimize hardware-related cost factors. Unfortunately, such transformations cannot adequately deal with the highly dynamic nature of today\u2019s multimedia applications. Therefore, we go one step back in the design process. Starting from a conceptual UML model, we first transform the model before refining it into executable code. This paper presents: various model transformations, an estimation technique for the steering cost parameters, and three case studies that show how our model transformations result in factors improvement in memory footprint and performance with respect to the initial implementation.", "num_citations": "11\n", "authors": ["107"]}
{"title": "Why FAMIX and not UML? UML Shortcomings for Coping with Round-trip Engineering\n", "abstract": " [Note that this report will appear in the UML'99 Conference Proceedings, published by Springer-Verlag in the LNCS series.] Abstract: UML is currently embraced as\u2019 the\u2019standard in object-oriented modeling languages, the recent work of OMG on the Meta Object Facility (MOF) being the most noteworthy example. We welcome these standardisation efforts, yet warn against the tendency to use UML as the panacea for all exchange standards. In particular, we argue that UML is not sufficient to serve as a toolinteroperability standard for integrating round-trip engineering tools, because one is forced to rely on UML\u2019s built-in extension mechanisms to adequately model the reality in source-code. Consequently, we propose an alternative meta-model (named FAMIX), which serves as the tool interoperability standard within the FAMOOS project and which includes a number of constructive suggestions that we hope will influence future releases of the UML and MOF standards.", "num_citations": "10\n", "authors": ["107"]}
{"title": "On the differences between unit and integration testing in the travistorrent dataset\n", "abstract": " Already from the early days of testing, practitioners distinguish between unit tests and integration tests as a strategy to locate defects. Unfortunately, the mining software engineering community rarely distinguishes between these two strategies, mainly because it is not straightforward to separate them in the code repositories under study. In this paper we exploited the TravisTorrent dataset provided for the MSR 2017 mining challenge, separated unit tests from integration tests, and correlated these against the workflow as recorded in the corresponding issue reports. Further analysis confirmed that it is worthwhile to treat unit tests and integration tests differently: we discovered that unit tests cause more breaking builds, that fixing the defects exposed by unit tests takes longer and implies more coordination between team members.", "num_citations": "9\n", "authors": ["107"]}
{"title": "On stories, models and notations: Storyboard creation as an entry point for model-based interface development with UsiXML\n", "abstract": " Storyboards are excellent tools to create a high level specification of an interactive system. Because of the emphasis on graphical depiction they are both an accessible means for communicating the requirements and properties of an interactive system and allow the specification of complex context-aware systems while avoiding the need for technical details. We present a storyboard meta-model that captures the high level information from a storyboard and allows relating this information with other models that are common for engineering interactive systems. We show that a storyboard can be used as an entry point for using UsiXML models. Finally, this approach is accompanied by a tool set to make the connection between the storyboard model, UsiXML models and the program code required for maintaining these connections throughout the engineering process.", "num_citations": "9\n", "authors": ["107"]}
{"title": "Serious: Software evolution, refactoring, improvement of operational and usable systems\n", "abstract": " Software intensive systems evolve during their lifetime, which inevitably results in degrading software quality. In order to extend the lifetime of their products, organizations must adopt a more mature - evolutionary - software development approach that pays attention to quality aspects during all phases of the product life cycle. In this paper we list the achievements and lessons learned that were obtained during the SERIOUS project, an European research project that tested state-of-the-art techniques, tools and processes on numerous case studies in varying industrial contexts. The application of evolutionary software development resulted in changes in the state-of-practice, yielding reduced total development costs and increased product lifetimes for the participating business units. more complex systems.", "num_citations": "9\n", "authors": ["107"]}
{"title": "Transformation techniques can make students excited about formal methods\n", "abstract": " Formal methods have always been controversial. In spite of the fact that the disbelief about their usefulness has been corrected by a growing number of applications and even more publications, it remains a challenge to demonstrate the strengths and weaknesses of formal methods within the time constraints of a typical semester course. This article1 reports on a new course at the University of Antwerp in which the introduction of a new formalism yields a better understanding of previously taught ones. While the exercises are designed to reveal the limitations of the formalisms used, students remain convinced that their formal models have more value than conventional source code.", "num_citations": "9\n", "authors": ["107"]}
{"title": "A qualitative investigation of UML modeling conventions\n", "abstract": " Analogue to the more familiar notion of coding conventions, modeling conventions attempt to ensure uniformity and prevent common modeling defects. While it has been shown that modeling conventions can decrease defect density, it is currently unclear whether this decreased defect density results in higher model quality, i.e., whether models created with modeling conventions exhibit higher fitness for purpose.               In a controlled experiment with 27 master-level computer science students, we evaluated quality differences between UML analysis and design models created with and without modeling conventions. We were unable to discern significant differences w.r.t. the clarity, completeness and validity of the information the model is meant to represent.               We interpret our findings as an indication that modeling conventions should guide the analyst in identifying what information to model, as well\u00a0\u2026", "num_citations": "9\n", "authors": ["107"]}
{"title": "Object-oriented architectural evolution\n", "abstract": " Software Architecture has become an established area of study within the software engineering community for a considerable time now. Recently, Software Architecture has become a topic of interest within the object-oriented community as well. The quality of an object-oriented architecture can be described by a set of characteristics, such as modularity, extensibility, flexibility, adaptability, understandability, testability and reusability, which are recognized to facilitate the evolution and the maintenance of software systems. Architecture represents the highest level of design decisions about a system and evolution aspects have to be considered at this level. Moreover, the ever-changing world makes evolvability a strong quality requirement for the majority of software architectures. The main objective of this workshop was to establish a working dialogue about the effective use of techniques, formalisms and tools\u00a0\u2026", "num_citations": "9\n", "authors": ["107"]}
{"title": "Test amplification in the pharo smalltalk ecosystem\n", "abstract": " Test amplification is the act of strengthening existing unittests to exercise the boundary conditions of the unit under test. It is an emerging research idea which has been demonstrated to work for Java, relying on the type system to safely transform the code under test.In this paper we report on a feasibility study concerning test amplification in the context of the Smalltalk eco-system. We introduce a proof-of-concept test amplifier named Small-Amp, and discuss the advantages and challenges we encountered while incorporating the tool into the Pharo Smalltalk environment. We demonstrate that by building on top of the Refactoring Browser API, the MuTalk mutation tool, it is feasible to build a test amplifier in Pharo Smalltalk despite the absence of a type system.", "num_citations": "8\n", "authors": ["107"]}
{"title": "Design guidelines for coordination components\n", "abstract": " The distributed nature of a typical web application combined with the rapid evolution of underlying platforms demands for a plug-in component architecture. Nevertheless, code for controlling distributed activities is usually spread over multiple subsystems, which makes it hard to dynamically reconfigure coordination services. This paper investigates coordination components as a way to encapsulate the coordination of a distributed system into a separate, pluggable entity. In an object-oriented context we introduce two design guidelines (namely,\"'turn contracts into objects\" and\" turn configuration into a factory object\") that help developers to separate coordination from computation and to develop reusable and flexible solutions for coordination in distributed systems.", "num_citations": "8\n", "authors": ["107"]}
{"title": "Three reverse engineering patterns\n", "abstract": " Whereas a design pattern describes and discusses a solution to a design problem, a reverse engineering pattern describes how to understand aspects of an object-oriented design and how to identify problems in that design. In the context of a project developing a methodology for reengineering objectoriented legacy systems into frameworks, we are working on a pattern language for reengineering. This paper presents three samples of that pattern language, all dealing with reverse engineering. 1", "num_citations": "8\n", "authors": ["107"]}
{"title": "A novel approach for detecting type-iv clones in test code\n", "abstract": " The typical structure of unit test code (setup - stimulate - verify - teardown) gives rise to duplicated test logic. Researchers have demonstrated the widespread use of syntactic clones in test code, yet if duplicated test code is indeed a problem, then semantic clones may be an issue as well. However, while detecting syntactic similarities can be done relatively easy, semantic similarities are more difficult to find. In this paper we present a novel way of detecting semantic clones by exploiting the unique features present in test code. We demonstrate on the Apache Commons Math Library's test suite that our approach can detect 259 semantic clones, of which only 54 were also detected by NiCad. This confirms that it is both feasible and worthwhile to investigate semantic clones in test code.", "num_citations": "7\n", "authors": ["107"]}
{"title": "Accommodating changing requirements with EJB\n", "abstract": " Component Based Software Development promises to lighten the task of web application developers by providing a standard component architecture for building distributed object oriented business applications. Hard evidence consolidat-ing this promise has yet to be provided, especially knowing that the standard libraries of today\u2019s programming languages offer considerable support for distribution (e.g. remote method invocations, database interfaces). Therefore, this paper compares three Java implementations of the same functionality \u2013 one using straightforward library-calls, one using a custom-made framework and one using the Enterprise Java Beans framework (EJB) \u2013 to assess the maintainability of each of the approaches. We ob-serve that EJB results in better maintainability (code is less complex and exhibits more explicit weak coupling) but that the framework version without the framework cost\u00a0\u2026", "num_citations": "7\n", "authors": ["107"]}
{"title": "Report: Workshop on object-oriented re-engineering (WOOR'97)\n", "abstract": " The ability to reengineer object-oriented systems has become a vital matter in today's software industry. Early adopters of the object-oriented programming paradigm are now facing the problem of transforming their object-oriented'legacy'systems into full-fledged frameworks. Dealing with programs exceeding 10,000 lines of badly documented code definitely requires support from methodologies as well as tools.During the ESEC/FSE'97 conference 1, a workshop was organised with the explicit goal to gather people working on solutions for object-oriented legacy systems. In here we report on the experiences exchanged, the solutions discussed and the ideas explored. More information can be found in the workshop proceedings [Demeyer, Gall'97] or via the world-wide web (http://iamwww. unibe. ch/~ famoos/ESEC97/).", "num_citations": "7\n", "authors": ["107"]}
{"title": "Class composition in FACE, a framework adaptive composition environment\n", "abstract": " Creating applications using object-oriented frameworks is often difficult, since subclassing plays a too important role. Subclassing is a \u201cwhite-box\u201d form of reuse, and thus requires the developer to understand the underlying implementation. In the approach described in this paper, class composition is introduced as a form of black-box class reuse. It may be seen to extend the concept of parameterized (generic) classes, especially since it allows mutual relationships as parameters. We illustrate class composition on basis of a design pattern.", "num_citations": "7\n", "authors": ["107"]}
{"title": "On the use of sequence mining within spectrum based fault localisation\n", "abstract": " Spectrum based fault localisation is a widely studied class of heuristics for locating faults within a software program. Unfortunately, the current state of the art ignores the inherent dependencies between the methods leading up to the fault, hence having a limited diagnostic accuracy. In this paper we present a variant of spectrum based fault localisation, which leverages series of method calls by means of sequence mining. We validate our variant (we refer to it as sequenced spectrum analysis) on the Defects4J benchmark, demonstrating that sequenced spectrum analysis gains a 12% points improvement against the state of the art.", "num_citations": "6\n", "authors": ["107"]}
{"title": "Opportunities and challenges in deriving metric impacts from refactoring postconditions\n", "abstract": " Refactoring\u2013transforming the source-code of an object-oriented program without changing its external observable behaviour\u2013is a restructuring process aimed at resolving evolution obstacles. Currently however, the efficiency of the refactor process in terms of quality improvements remains unclear. Such quality improvement can be expressed in terms of an impact on OO metrics. The formalization of these metrics is based on the same constructs as refactoring postconditions. Therefore, in this position paper, we elaborate on a research approach to derive Object-Oriented metric impacts from refactoring postconditions, in order to provide qualitative guidelines on the application of specific refactorings to resolve quality deficiencies.", "num_citations": "6\n", "authors": ["107"]}
{"title": "Studying software evolution using clone detection\n", "abstract": " In modern software engineering, researchers regard a software system as an organic life form that must continue to evolve to remain successful. Unfortunately, little is known about how successful software systems have evolved, and consequently little has been learned from previous experience. In this paper, we demonstrate a heuristic to reconstruct evolution processes of existing software systems by exploiting techniques to detect duplication in large amounts of data. Based on two/three small experiments, we conclude that it helps to acquire a better understanding of the evolution processes of successful software, which may help to improve current software development methods.", "num_citations": "6\n", "authors": ["107"]}
{"title": "Structural computing: the case for reengineering tools\n", "abstract": " Within the decade of organized hypermedia conferences,\" ending the tyranny of the link\"[1] has been a recurring theme. Researchers agree that the basic node/link/anchor model is sufficiently powerful to implement all navigational metaphors, yet that this is not necessarily the best way to achieve the desired navigation functionality. In this quest to\" end the tyranny of the link\", many researchers have been exploring extensions on that basic node/link/anchor model. One specific extension is known under the name of virtual links (also called computational, or dynamic, or generic links). In contrast with a normal static link\u2014which is defined as a fixed association between a number of anchors attached to nodes\u2014a virtual link is only fixed to the source-anchors and uses computations for connecting to the other parts. Accordingly, a virtual link can wait to associate the source-anchor with the node until the node is actually displayed, and can defer the retrieval of the target-anchors until the source-anchor is really activated. Thus, virtual links establish a late binding between the pieces of information that are linked together.The late binding achieved via virtual links offers some advantages over static links. First, virtual links usually reduce the total number of links and anchors that have to be stored in the link-base, this way saving considerable storage space and thus improving scalability. Second, virtual links make it a lot easier to handle evolving information, because the actual linking of information is not done when the information is updated, but deferred until the navigation takes place. Thus, virtual links contribute a lot to the maintainability of the link-base.", "num_citations": "6\n", "authors": ["107"]}
{"title": "Clone detection in test code: an empirical evaluation\n", "abstract": " Duplicated test code (a.k.a. test code clones) has a negative impact on test comprehension and maintenance. Moreover, the typical structure of unit test code induces structural similarity, increasing the amount of duplication. Yet, most research on software clones and clone detection tools is focused on production code, often ignoring test code. In this paper we fill this gap by comparing four different clone detection tools (NiCad, CPD, iClones, TCORE) against the test code of three open-source projects. Our analysis confirms the prevalence of test code clones, as we observed between 23% and 29% test code duplication. We also show that most of the tools suffer from false negatives (NiCad = 83%, CPD = 84%, iClones = 21%, TCORE = 65%), which leaves ample room for improvement. These results indicate that further research on test clone detection is warranted.", "num_citations": "5\n", "authors": ["107"]}
{"title": "DEVS for AUTOSAR platform modelling.\n", "abstract": " AUTOSAR (AUTomotive Open System ARchitecture) is an open and standardized automotive software architecture, jointly developed by automobile manufacturers, suppliers and tool developers. Its design is a direct consequence of the increasingly important role software plays in vehicles. As design choices during the software deployment phase may have a large impact on the real-time properties of the system, designers need a method to explore various trade-offs. In this paper we evaluate the appropriateness of DEVS, the Discrete-Event system Specification, for modelling and subsequent performance evaluation of AUTOSAR-based systems. We demonstrate and validate our work by means of a power window and ABS case study.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Supporting inconsistency resolution through predictive change impact analysis\n", "abstract": " Today, model-driven software processes rely on consistency management approaches to deal with the multitude of inconsistencies that occur in large systems. To resolve a detected inconsistency, the software designer applies one resolution out of a set of resolution options to each inconsistency. To do so, the designer needs to understand the extent of changes posed by each resolution. In this paper we propose change impact analysis to support the designer in this task. We present a simple algorithm for predicting the impact of inconsistency resolutions by checking the instantiation of different meta-model relationships. Based on one small case study, we demonstrate that our algorithm provides a reasonable estimate for the number of changes that actually will be applied. We demonstrate the usage of impact analysis for inconsistency resolution and make a first step towards a decision support tool to help a\u00a0\u2026", "num_citations": "5\n", "authors": ["107"]}
{"title": "Wrapping a real-time operating system with an OSEK compliant interface\u2014a feasibility study\n", "abstract": " The drive towards standardization in the automotive sector puts a lot of pressure on software suppliers to comply with standards such as OSEK and AUTOSAR. However, many of these suppliers have a vested interest in proprietary software and are seeking ways to migrate their existing code-base to comply to these standards. This paper reports on a feasibility study to wrap a proprietary real-time operating system with an OSEK compliant interface. Besides investigating whether this is feasible, we also assess the performance impact in terms of computation time and memory consumption, as this is critically important for real-time systems. As such, we evaluate the typical trade-offs one has to make when adopting an incremental migration strategy towards a standard compliant interface.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Famix 2.0\n", "abstract": " This document defines the exchange model for usage by tool prototypes within the FAMOOS reengineering project. These tools exchange information concerning object-oriented source code. This information is then transferred via flat ASCII streams using the CDIF standard.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Experimentally investigating the effectiveness and effort of modeling conventions for the UML\n", "abstract": " Modelers tend to exploit the various degrees of freedom provided by the UML. The lack of uniformity and the large amount of defects contained in UML models result in miscommunication between different readers. To prevent for these problems we propose modeling conventions, analogue to coding conventions for programming. This work reports on a controlled experiment to explore the effect of modeling conventions on defect density and modeling effort. 106 masters\u2019 students participated over a six-weeks-period. Our results indicate that decreased defect density is attainable at the cost of increased effort when using modeling conventions, and moreover, that this trade-off is stressed if tool-support is provided. Additionally we report observations on the subjects\u2019 adherence to and attitude towards modeling conventions. Our observations indicate that efficient integration of convention support in the modeling process, eg through training and seamless tool integration, forms a promising direction towards preventing defects.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Class composition for specifying framework design\n", "abstract": " Object\u2010oriented frameworks are a particularly appealing approach towards software reuse. An object\u2010oriented framework represents a design for a family of applications, where variations in the application domain are tackled by filling in the so\u2010called hot spots. However, experience has shown that the current object\u2010oriented mechanisms (class inheritance and object composition) are not able to elegantly support the \u201cfill in the hot spot\u201d idea. This paper introduces class composition as a more productive approach towards hot spots, offering all of the advantages of both class inheritance and object composition but involving extra work for the framework designer. \u00a9 1999 John Wiley & Sons, Inc.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Techniques for building open hypermedia systems\n", "abstract": " This paper describes a methodology the authors found very useful in the development of open systems for objectoriented languages, user-interface builders and hypermedia. We promote the idea of\" open designs\" as being a key factor for success and discuss software engineering techniques useful in implementing such designs.", "num_citations": "5\n", "authors": ["107"]}
{"title": "Comparing mutation coverage against branch coverage in an industrial setting\n", "abstract": " The state-of-the-practice in software development is driven by constant change fueled by continues integration servers. Such constant change demands for frequent and fully automated tests capable to detect faults immediately upon project build. As the fault detection capability of the test suite becomes so important, modern software development teams continuously monitor the quality of the test suite as well. However, it appears that the state-of-the-practice is reluctant to adopt strong coverage metrics (namely mutation coverage), instead relying on weaker kinds of coverage (namely branch coverage). In this paper, we investigate three reasons that prohibit the adoption of mutation coverage in a continuous integration setting: (1) the difficulty of its integration into the build system, (2) the perception that branch coverage is \u201cgood enough\u201d, and (3) the performance overhead during the build. Our investigation is based\u00a0\u2026", "num_citations": "4\n", "authors": ["107"]}
{"title": "An Exploratory Study on Migrating Single-Products towards Product Lines in Startup Contexts\n", "abstract": " A majority of technology startups fail; inadequate software engineering practices are known to be a contributing factor. The smooth transitioning towards software product lines in particular is a major stumbling block for startups that must broaden their product portfolio to deal with divergent demands imposed by the market. We conducted a preliminary study within two software engineering startups, which revealed the motivating factors and benefits that would lead to the migration from a single product into a product line. Despite the benefits, tackling the challenges foreseen and the identification of features and their relations in the current product is the crucial first step towards implementing an appropriate highly-configurable product portfolio.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Test behaviour detection as a test refactoring safety\n", "abstract": " When refactoring production code, software developers rely on an automated test suite as a safeguard. However, when refactoring the test suite itself, there is no such safeguard. Therefore, there is a need for tool support that can verify if a refactored test suite preserves its behaviour pre-and post-refactoring. In this work we present T-CORE (Test COde REfactoring tool); a tool that captures the behaviour of Java tests in the form of a Test Behaviour Tree. T-CORE allows developers to verify that the refactoring of a test suite has preserved the behaviour of the test.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Poster: Unit tests and component tests do make a difference on fault localisation effectiveness\n", "abstract": " Agile testers distinguish between unit tests and component tests as a way to automate the bulk of the developer tests. Research on fault localisation largely ignores this distinction, evaluating the effectiveness of these techniques irrespective of whether the fault is exposed by unit tests-where the search space to locate the fault is constrained to the unit under test- or by component tests- where the search space expands to all objects involved in the test. Based on a comparison of sixteen spectrum based fault localisation techniques, we show that there is indeed a big difference in performance when facing unit tests and component tests. Consequently, researchers should distinguish between easy and difficult to locate faults when evaluating new fault localisation techniques.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Evolution of Software Product Development in Startup Companies.\n", "abstract": " This position paper addresses the software engineering practices in startups. The focus of most software engineering research has been on established companies. However, startup technology companies have become important producers of innovative and software intensive products despite the fact that they are under severe time-to-market pressure. Given that software engineering is the core activity in said startups, inadequacies in such practices might be a substantial contributing factor to this pressure to keep up with the software industry competitive needs. Startups build non-traditional business architectures by taking the easy path to find a product-market fit and thus, accumulate large amounts of technical debt. We shed light on the major efforts in the domain and indicate the research directions we plan to explore further.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Calibration of deployment simulation models: A multi-paradigm modelling approach\n", "abstract": " In embedded systems development, software engineers increasingly rely on modelling and simulation to produce optimal design solutions. A bottleneck in the Modelling and Simulation Based Design (MSBD) process is model calibration. Setting up experiments to estimate parameter values such that the model accurately reflects real-world system structure and behaviour is technically complex and labour intensive. Parameters to be estimated are for example effective processor speed, memory consumption and network throughput of the hardware platform on which software is deployed. In this paper we show how Multi-Paradigm Modelling (MPM) allows for the synthesis of a model calibration infrastructure. This includes the synthesis, from a model, of a simulator for the \u201cenvironment\u201d in which a system-to-be built will operate. To demonstrate the feasibility of our approach, we calibrate the model of an automotive power window controller running on the AUTOSAR-platform.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Preserving aspects via automation: a maintainability study\n", "abstract": " This paper presents an empirical study comparing two alternatives for generating code from aspect-oriented models. In an aspect \"disrupting\" process, an object oriented implementation in Java is automatically generated from domain specific models, comprising a mix of UML (for core functionality) and DSLs (for qualities like security and perfor mance). In an aspect \"preserving\" process, an aspect oriented implementation in AspectJ is automatically generated from the same models. In both alternatives, a number of subjects are asked to perform several maintenance tasks requiring the addition and improvement of functionality. The results show that, in most of the cases, the AO alternative provides for shorter maintenance cycles.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Students can get excited about Formal Methods: a model-driven course on Petri-Nets, Metamodels and Graph Grammars\n", "abstract": " Formal Methods have always been controversial. In spite of the fact that the disbelief about their usefulness has been corrected by a growing number of applications and even more publications, it remains a challenge to demonstrate the strengths and weaknesses of formal methods within the time constraints of a typical semester course. This paper reports on a new course at the University of Antwerp in which the introduction of a new formalism yields a better understanding of previously taught ones. While the exercises are designed to reveal the limitations of the formalisms used, students remain convinced that their formal models have more value than conventional source code.", "num_citations": "4\n", "authors": ["107"]}
{"title": "The LAN-simulation: A refactoring lab session\n", "abstract": " The notion of refactoring\u2014transforming the sourcecode of an object-oriented program without changing its external behaviour\u2014has been studied intensively within the last decade. Despite the acknowledgment in the software engineering body of knowledge. there is currently no accepted way of teaching good refactoring practices. This paper presents one possible teaching approach: a lab session (called the LAN-simulation) which has received positive feedback in both an academic as well as industrial context. By sharing our experience, we hope to convince refactoring enthusiasts to try the lab-session and to stimulate teachers to incorporate refactoring lab sessions into their courses.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Definition of a common exchange model\n", "abstract": " This document defines the exchange model for usage by tool prototypes within the FAMOOS reengineering project. The model is based upon the CDIF standard so that it can be transferred via flat ASCII streams.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Reflective application builders\n", "abstract": " Current visual application builders and application frameworks do not live up to their expectations of rapid application development or non-programming-expert application development. They fall short when compared to component-oriented development environments in which applications are built with components that have a strong a nity with the problem domain (ie being domain-speci c). Although the latter environments are very powerful, they are hard to build and, in general, do not allow much variation in the problem domain that is covered. In this paper we show how this apparent con ict between generality and domain speci city can be overcome by considering application building itself as the problem domain. This naturally leads to the notion of a re ective application builder, ie an application framework| application builder pair that incorporates all the tools for the visual construction of (domain-speci c) application builders.", "num_citations": "4\n", "authors": ["107"]}
{"title": "A Layered Approach to Dedicated Application Builders Based on Application Frameworks\n", "abstract": " In this paper we investigate what is needed to make user interface builders incrementally refinable. The need for dedicated user interface builders is motivated by drawing a parallel with programming language design and object-oriented application frameworks. We show that reflection techniques borrowed from the programming language community can be successfully applied to make user interface builders incrementally refinable.", "num_citations": "4\n", "authors": ["107"]}
{"title": "Value-based technical debt management: an exploratory case study in start-ups and scale-ups\n", "abstract": " Software start-ups face fierce competition in the market forcing them to release their products quickly and often under tough time constraints. To meet their deadlines, start-ups take shortcuts in software development leading to the accumulation of technical debt. They are able to put their product in users hands faster, get feedback, and improve at the expense of quality issues in the long run. As a start-up evolves through inception, stabilization and growth this debt will have to be managed. Technical debt management and software product line engineering techniques have some similar benefits of increased productivity and reduced time-to-market. Our aim is to check whether software product line engineering can be a candidate technique for start-ups to employ in managing technical debt as a response to their life-cycle phase goals and challenges. We conducted expert interviews with nine start-up professionals to\u00a0\u2026", "num_citations": "3\n", "authors": ["107"]}
{"title": "An empirical study of clone density evolution and developer cloning tendency\n", "abstract": " Code clones commonly occur during software evolution. They impact the effort of software development and maintenance, and therefore they need to be monitored. We present a large-scale empirical study (237 open-source Java projects maintained by 500 individuals) that investigates how the number of clones changes throughout software evolution, as well as the tendency of individual developers to introduce clones. Our results will set a point-of-reference against which development teams can compare and, if needed, adjust.", "num_citations": "3\n", "authors": ["107"]}
{"title": "Improving spectrum based fault localisation techniques\n", "abstract": " Spectrum based fault localisation techniques merely use the coverage of the program elements to localise the fault. These techniques ignore the dependency relationships between the elements, such as a combination of methods to be called together, and hence come at the cost of their diagnostic accuracy. In this paper we present a variation of spectrum based fault localisation techniques which leverages the sequences of method calls as coverage elements than merely individual method coverage. We compare our technique with traditional spectrum based fault localisation techniques, which use the coverage of individual methods, and demonstrate with a small set of faults that our technique outperforms these techniques.", "num_citations": "3\n", "authors": ["107"]}
{"title": "A meta-model approach to inconsistency management\n", "abstract": " Transformations between abstract design models and platform specific models are the basis of the modeldriven development process. In the transformation process inconsistencies are introduced, that neither can nor always should be eradicated. This paper presents a meta-model that addresses the need to manage inconsistencies arising during vertical model transformations. The meta-model captures both characteristics of inconsistencies and provides support for inconsistency management activities such as monitoring and analysis. We motivate the inconsistency management approach with an example transformation and show the scope of the proposed meta-model solution.", "num_citations": "3\n", "authors": ["107"]}
{"title": "An Empirical Study on Accidental Cross-Project Code Clones\n", "abstract": " Software clones are considered a code smell in software development. While most clones occur due to developers copy - paste behaviour, some of them arise accidentally as a symptom of coding idioms. If such accidental clones occur across projects, then they may indicate a lack of abstraction in the underlying programming language or libraries. In this research, we study accidental cross-project clones from the perspective of missing abstraction. We discuss the six cases of frequent cross-project clones, three of them symptoms of missing language features (which have been resolved with the release of Java 7 and Java 12), and two of them symptoms of missing library features (which have not yet been addressed).", "num_citations": "2\n", "authors": ["107"]}
{"title": "Evaluating intermittent and concurrent feedback during an HRTF measurement\n", "abstract": " Authentic 3D audio is a critical factor for a truly immersive virtual reality experience. As each individual perceives 3D audio in a different way, personalization of this experience is required. We have recently developed a low-cost and efficient HRTF measurement procedure that allows for such personalization. In this paper, we compare two proof-of-concept prototypes, which gamify this measurement procedure: a lightweight mobile solution which provides intermittent visual feedback and a virtual reality approach which provides concurrent visual feedback. Results of a user study indicate that user performance is better with having concurrent feedback, although the concurrent feedback approach was perceived as less intuitive by most test subjects.", "num_citations": "2\n", "authors": ["107"]}
{"title": "A data mining approach for indoor air assessment, an alternative tool for cultural heritage conservation\n", "abstract": " The exposure of cultural heritage to the environment has a significant impact on its degradation process and degradation rate. Consequently, managing the indoor air quality is vital to minimize further damage to historical artefacts and works of art. Despite its potential impact, the traditional assessment of the indoor air quality still represents a challenge for most collection guardians. This approach typically relays on the comparison of measured environmental parameters and corresponding acceptable values. However, determining the acceptable values and relative importance of the different environmental parameters turns out to be quite complex since it depends on the material types present in the collection and their preservation state. Furthermore, the significant amount of data generated during the measurements hampers the application of traditional methods of analysis. Considering all these, we propose the\u00a0\u2026", "num_citations": "2\n", "authors": ["107"]}
{"title": "Test refactoring: a research agenda\n", "abstract": " Research on software testing generally focusses on the effectiveness of test suites to detect bugs. The quality of the test code in terms of maintainability remains mostly ignored. However, just like production code, test code can suffer from code smells that imply refactoring opportunities. In this paper, we will summerize the state-of-the-art in the field of test refactoring. We will show that there is a gap in the tool support, and propose future work which will aim to fill this gap.", "num_citations": "2\n", "authors": ["107"]}
{"title": "New approach to indoors air quality assessment for cultural heritage conservation\n", "abstract": " Indoor air quality (IAQ) plays a major role in preventive cultural heritage conservation. Several studies have illustrated how it is possible to slow down the degradation process of objects by controlling different parameters such as temperature, relative humidity, light, UV radiation, particulate matter, and reactive gases concentration. Unfortunately, collection caretakers wishing to monitor these parameters are quickly overwhelmed by the amount of data presented. Therefore, we combine these parameters into a single IAQ-index, representing the global environmental risk for collections as a function of time. For the creation of the IAQ-index we consider different sets of thresholds. In the present paper, we focus on two of the most common environmental parameters already controlled at museums: temperature, and relative humidity. We demonstrate on one realistic case how we can use this IAQ-index to visualize the historical evolution of the air quality and identify periods where the collection was at risk.", "num_citations": "2\n", "authors": ["107"]}
{"title": "Studying the co-evolution of application code and test cases\n", "abstract": " Writing application code and the respective test cases are tightly coupled activities while software applications are maintained. Unfortunately, little is known about how these two activities co-evolve in the lifetime of a software project. In this study, we propose an approach to investigate how these two maintenance activities (writing application code and test cases) relate to each other. Our first step consists of the reconstruction of this traceability between the two activities. Subsequently, we use this relation to investigate whether it correlates with certain properties of the source code (length, nesting level,...).", "num_citations": "2\n", "authors": ["107"]}
{"title": "An Experimental Design for Evaluating the Maintainability of Aspect-Oriented Models Enhanced with Domain-Specific Constructs\n", "abstract": " Abstraction, modularity and composability are considered to be the fundamental properties behind aspect-oriented software development and aspect-oriented modeling (AOM) in particular. The same properties are expected to be supported through the use of domain-specific modeling languages (DSMLs). However, little research is done to investigate the symbiosis between the two paradigms. In this position paper we firstly present the key challenges for the successful integration of DSMLs with an AOM approach. Furthermore, we elaborate in detail on the question whether leveraging on aspect-oriented programming languages offers benefits over the use of traditional model composition. We propose an experimental approach to evaluate these alternatives.", "num_citations": "2\n", "authors": ["107"]}
{"title": "Moving up to the modeling level for the transformation of data structures in embedded multimedia applications\n", "abstract": " Traditional design- and optimization techniques for embedded devices apply local transformations of source-code to maximize the performance and minimize the power consumption. Unfortunately, such transformations cannot adequately deal with the highly dynamic nature of today\u2019s multimedia applications as they do not exploit application specific knowledge. We decided to go one step back in the design process. Starting from the original UML (Unified Modeling Language) model of the source code, we transform the UML model first before refining it into executable code. In this paper we present (i) the transformation of various UML models, (ii) a fast technique for the estimation of the high-level cost parameters that steer our transformations, and (iii) experiments based on three case-studies (a Snake game, a Tetris game and a 3D rendering engine) that show that our transformations can result in factors\u00a0\u2026", "num_citations": "2\n", "authors": ["107"]}
{"title": "Reverse Engineering based on Metrics and Program Visualization\n", "abstract": " BORIS Deutsch English Fran\u00e7ais Login BORIS Bern Open Repository and Information System University of Bern Home Statistics Reverse Engineering based on Metrics and Program Visualization Ducasse, St\u00e9phane; Lanza, Michele; Demeyer, Serge (1999). Reverse Engineering based on Metrics and Program Visualization. Lecture notes in computer science(1743), pp. 168-169. Springer [img] Text Seiten aus 10.1007_3-540-46589-8.pdf - Published Version Restricted to registered users only Available under License Publisher holds Copyright. Download (116kB) Item Type: Conference or Workshop Item (Speech) Division/Institute: 08 Faculty of Science > Institute of Computer Science (INF) 08 Faculty of Science > Institute of Computer Science (INF) > Software Composition Group (SCG) UniBE Contributor: Ducasse, Stephane and Lanza, Michele Subjects: 000 Computer science, knowledge & systems 500 Science > \u2026", "num_citations": "2\n", "authors": ["107"]}
{"title": "A framework browser scenario\n", "abstract": " The hypermedia system should not impose a single view of what constitutes a hypermedia data model, but should be configurable and extensible so that new hypermedia data models may be incorporated. It should thus be possible to interoperate with external hypermedia systems, and to exchange data with external systems.", "num_citations": "2\n", "authors": ["107"]}
{"title": "The Zypher Meta Object Protocol\n", "abstract": " This paper discusses the necessity of a meta object protocol in the design of an open hypermedia system. It shows that a meta object protocol enables to tailor the behaviour and configuration of the hypermedia system, independent of its constituting elements.", "num_citations": "2\n", "authors": ["107"]}
{"title": "A Survey of Object Oriented Databases\n", "abstract": " In [KIM'90a] we found the following definition of an object oriented database:\" An objectoriented database system is a database system which directly supports an object-oriented data model.\" We would reject this statement as a definition (it is not precise enough) but the sentence is a perfectly good explanation of what an object-oriented database is. If one understands what is meant by each the words one might understand better what is meant by \u201can object-oriented database\u201d. To acquire this knowledge, one needs to study the two fields object oriented databases emerged from: programming languages and databases. Sketching the evolution of these fields will reveal the crucial aspects of object oriented databases, and will explain why the field is so promising. It will also allow to understand some of the difficulties, because-as also stated in [BL/ZD'87]-both fields are culturally biased and this leads to different viewpoints and, sometimes even, misunderstandings. The following three chapters try to give an overview on the evolution of both communities. Only the aspects important to object oriented data bases are mentioned, so this is by no means a complete evolution of the two fields. In some cases interesting concepts are not covered, and in other cases it is hard to pin-point the exact time some idea emerged. As a consequence of this, priority was given to describe a clear time line instead of giving an accurate sequence of events.", "num_citations": "2\n", "authors": ["107"]}
{"title": "Summary of the 1st IEEE Workshop on the Next Level of Test Automation (NEXTA 2018)\n", "abstract": " NEXTA is a new workshop on test automation that provides a meeting point for academic researchers and industry practitioners. While test automation already is an established practice in industry, the concept needs to evolve to go beyond its current state to support the ever faster release cycles of tomorrow's software engineering. NEXTA implications for research and practice will include test case generation, automated test result analysis, test suite assessment and maintenance, and infrastructure for the future of test automation. The rst instance of NEXTA was co-located with the 11th IEEE Conference on Software Testing, Veri cation and Validation (ICST 2018) in V asteras, Sweden on April 9, 2018. NEXTA 2018 o ered an interactive setting with a keynote and paper presentations, stimulated by two novel awards to incentivize interaction and dissemination: a Best Questions Award and a Most Viral Tweet Award\u00a0\u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "Adopting Program Synthesis for Test Amplification.\n", "abstract": " Program synthesis is the task of enabling a computer system to automatically write program code based on user intent. Test amplification on the other hand is an emerging research area, where the goal is to generate new test cases from manually written ones. From this perspective, test amplification is awfully similar to program synthesis. Therefore, in this short paper, we explore the benefits of using program synthesis for test amplification.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Comparing spectrum based fault localisation against test-to-code traceability links\n", "abstract": " The recent shift towards automated software tests stimulated research interest in fault localisation. Fault localisation addresses the question which program elements need to be fixed to repair a failing test. The current state of the art in that field is named spectrum based fault localisation, which relies on dynamic coverage information from both failing and passing test cases to pinpoint the faulty program elements. This is in sharp contrast with the na\u00efve approach which extracts traceability links between the test code and the program elements under test and enumerates those until the faulty element is found. In this paper we ask ourselves the question whether the state-of-the-art approach (spectrum based fault localisation) is so much better than the na\u00efve approach (test-to-code traceability). We demonstrate on 178 defects from three representative projects in the recent Defects4J dataset that spectrum based fault\u00a0\u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "An initial investigation of a multi-layered approach for optimizing and parallelizing real-time media and audio applications\n", "abstract": " The advent of multi-core processors revamped the research on parallel computing, among others in the area of real-time media and audio processing. Indeed, the added calculating power offers application developers new opportunities for high-performance computing, but demands for new approaches in application design. In this paper we perform an initial investigation of a systematic parallelization and optimization method, based on a layered application model. This approach will aid in the identification of potential candidates for parallelism and finding an optimal solution in a potentially very large and complex design space.", "num_citations": "1\n", "authors": ["107"]}
{"title": "The CHA-Q meta-model: A comprehensive, change-centric software representation\n", "abstract": " The Cha-Q Meta-Model: A Comprehensive, Change-Centric Software Representation Page 1 The Cha-Q Meta-Model: A Comprehensive, Change-Centric Software Representation Coen De Roover, Christophe Scholliers, Angela Lozano, Viviane Jonckers Javier Perez, Alessandro Murgia, Serge Demeyer Page 2 \u2713 of the different entities of a software system Cha-Q Meta-Model \u2713 each change to an entity that results in a new entity state \u2713 system snapshots under control of a VCS first interconnected representation of: object-oriented (ie, each concept and relation is represented by a class) memory-efficient tracking of states driven by positive experience with FAMIX, RING/C/H, Cheops driven by poor scalability reported for Hismo and Syde change state & evolution snapshots \u279c to be shared by prototypes for analyzing, repeating, tracing changes Page 3 Cha-Q Meta Model: Overview & Inspiration state & evolution Ring/'\u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "Maintainability studies investigating aspect preservation via automation: Lessons learned\n", "abstract": " In the recent past the authors have conducted two experimental studies concerning the benefits of preserving modularity from design to code. In this paper we report on three key lessons learned in designing those investigations.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Migrating from a Proprietary RTOS to the OSEK Standard Using a Wrapper\n", "abstract": " The drive towards standardization in the automotive sector puts a lot of pressure on software suppliers to comply with standards such as OSEK and AUTOSAR. However, many of these suppliers have a vested interest in proprietary software and are seeking ways to migrate their existing code base to comply with these standards. This paper reports on a feasibility study to migrate an automotive off-highway application to the OSEK-OS using a wrapper around the proprietary real-time operating system (RTOS). Besides investigating whether this is feasible, we also assess the performance impact in terms of computation time and memory consumption, as this is critically important for real-time systems. Finally some pitfalls are given when porting a given application to OSEK-compliant RTOS. As such, we evaluate the typical trade-offs one has to make when adopting an incremental migration strategy towards a\u00a0\u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "An ADT profile\n", "abstract": " This report presents our ADT profile, a UML profile for modeling the data-structure related aspects of an application. The main goal is to support global data-structure optimization activities at the modeling level. We implemented this ADT profile using MagicDraw UML 12.1 [15], a visual UML 2 modeling and CASE tool that is widely adopted in the software industry. MagicDraw supports the XML Metadata Interchange (XMI) 2.1 standard for the distribution of UML profiles and UML user models. This ADT profile is available at the website of the LORE research group in XML format for import and ready for use in other UML 2 compliant modeling tools [16]. This report is organized as follows: Section 1 gives an overview of the structure of the ADT profile. In the following three sections, the new ADT-specific modeling concepts are discussed in detail and illustrated with clarifying examples.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Using Refactoring Techniques to Exploit Variability in Conceptual Modeling\n", "abstract": " The term variability refers to the possibility of building different correct conceptual models for a given set of requirements in a given modeling language (for example, UML or the Entity Relationship model). We call each model a variant of the others. In previous work, we presented a framework identifying 3 different types of variability. These 3 types of differences have been shown to be present even under restrictive conditions; or in other words, these 3 types can occur in almost any modeling effort. Moreover, controlled experiments have shown that some of these types of variability affect quality characteristics such as the evolvability of the conceptual model. This suggests that a developer has to choose the variant with the most appropriate quality characteristics. However, changes in the environment of the information system over time can make other variants more appropriate. This paper focuses on the question whether a developer can still switch to another variant later in the system life cycle. This would allow him to use the quality benefits of the different variants at different points in time. Our results show the applicability of refactoring for these purposes and demonstrate the promising practical relevance of the variability framework.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Towards energy-conscious class transformations for data-dominant applications: a case study\n", "abstract": " For data-dominant applications running on embedded systems, the energy consumed by the data memory organisation represents a very large cost. We present a method to explore the design space of an objectoriented application in order to optimise the energy consumption and the footprint of the memory. Because we are working at the class design level, we can exploit the functionality of the system and focus on optimising the data related properties of the classes. We demonstrate our technique with a case study in which we achieve a gain of at least 35% in the energy cost factor.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Extensibility via a meta-level architecture\n", "abstract": " Meta-level architectures are recognized as a means to achieve run-time extensibility, and have been applied as such in existing hypermedia systems. Yet, designing a good meta-level architecture is notoriously hard and remains an art rather than a science. This paper shows how to derive a meta-level architecture for hypermedia navigation, thereby providing a way to control how third-party components interact with the linking engine. This extra level of control allows for a better and safer integration between an extensible system and the third-party components extending it.", "num_citations": "1\n", "authors": ["107"]}
{"title": "Workshop ion on Object-Oriented Technology\n", "abstract": " Workshop ion on Object-Oriented Technology | Guide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsECOOP '98 ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide Proceedings cover image ECOOP '98: Workshop ion on Object-Oriented Technology July 1998 589 pages ISBN:3540654607 Editors: Serge Demeyer profile image Serge Demeyer, Jan Bosch profile image Jan Bosch Copyright \u00a9 1998 Publisher -, : \u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "Proceedings of the 2nd Workshop on Open Hypermedia Systems\n", "abstract": " Proceedings of the 2nd Workshop on Open Hypermedia Systems \u2014 University of Southern Denmark Skip to main navigation Skip to search Skip to main content University of Southern Denmark Logo Help & FAQ Dansk English Home Researchers Research Units Research output Activities Projects Press / Media Prizes Teaching Impacts Datasets Search by expertise, name or affiliation Proceedings of the 2nd Workshop on Open Hypermedia Systems Uffe Kock Wiil (Editor), Serge Demeyer (Editor) SDU Health Informatics The Maersk Mc-Kinney Moller Institute Research output: Book/report \u203a Report \u203a Research Overview Original language English Publisher University of California, Irvine Volume UCI-ICS Technical Report 96-10 Publication status Published - 1996 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Wiil, UK, & Demeyer, S. (Eds.) (1996). Proceedings of the 2nd Workshop on Open Systems. , (\u2026", "num_citations": "1\n", "authors": ["107"]}
{"title": "Virtual hypertext based on paths and warm links\n", "abstract": " Throughout the last years a huge amount of work has been devoted to the definition of hypertext models. Even more resources have been directed towards the domain of virtual (dynamic/computational) hypertext, among others motivated by the idea of building open systems. Surprisingly enough, almost nobody stressed the role of the underlying model in such virtual systems. That is precisely the aim of this text: to define a general hypertext model that is able to support the notion of virtuality. Our assertion is that the combination of the ancient concepts' Paths' and'Warm Links' provide just the extra support needed. Moreover this allows for a model where links are but one of the possible ways to relate nodes.While experimenting with the model, an interesting question arose: do bi-directional links fit into a virtual model? This paper attempts to answer the question. We chose a constructive approach, because our aim was to create a laboratory where ideas concerning virtual hypertext might be explored. We applied recent viewpoints from the field of software engineering (namely object oriented frameworks and mixins) to assist the iterative design process.", "num_citations": "1\n", "authors": ["107"]}