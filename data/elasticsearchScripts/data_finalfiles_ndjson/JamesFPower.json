{"title": "Using fuzzy logic: Towards intelligent systems\n", "abstract": " Fuzzy logic\u2013based control has been used effectively in applications such as robot control, object recognition, appliance control, camera autofocusing, subway operations, and automobile control. This book covers the theory and practice of developing fuzzy logic\u2013based control systems. It contains fuzzy logic control design and analysis examples, and code for the construction of actual hardware and software control systems. Chapter 1 gives a broad introduction to the concepts of fuzzy sets and fuzzy logic controllers. A fuzzy logic controller may be viewed as a fuzzy expert system, that is, an expert system whose inference mechanism is based on fuzzy logic. The main components of a fuzzy logic controller are: A fuzzification unit, which converts numerical inputs into fuzzy linguistic values. The knowledge base, which is a collection of fuzzy rules and facts. For example,\u201cIF the temperature is high THEN the pressure is\u00a0\u2026", "num_citations": "339\n", "authors": ["1762"]}
{"title": "A metrics suite for grammar\u2010based software\n", "abstract": " One approach to measuring and managing the complexity of software, as it evolves over time, is to exploit software metrics. Metrics have been used to estimate the complexity of the maintenance effort, to facilitate change impact analysis, and as an indicator for automatic detection of a transformation that can improve the quality of a system. However, there has been little effort directed at applying software metrics to the maintenance of grammar\u2010based software applications, such as compilers, editors, program comprehension tools and embedded systems. In this paper, we adapt the software metrics that are commonly used to measure program complexity and apply them to the measurement of the complexity of grammar\u2010based software applications. Since the behaviour of a grammar\u2010based application is typically choreographed by the grammar rules, the measure of complexity that our metrics provide can guide\u00a0\u2026", "num_citations": "94\n", "authors": ["1762"]}
{"title": "Using object-level run-time metrics to study coupling between objects\n", "abstract": " In this paper we present an investigation into the run-time behaviour of objects in Java programs, using specially adapted coupling metrics. We identify objects from the same class that exhibit non-uniform coupling behaviour when measured dynamically. We define a number of object level run-time metrics, based on the static Chidamber and Kemerer coupling between objects (CBO) measure. These new metrics seek to quantify coupling at different layers of granularity, that is at class-class and object-class level. We outline our method of collecting such metrics and present a study of the programs from the JOlden benchmark suite as an example of their use. A number of statistical techniques, principally agglomerative hierarchical clustering analysis, are used to facilitate the identification of such objects.", "num_citations": "65\n", "authors": ["1762"]}
{"title": "Platform independent dynamic java virtual machine analysis: the java grande forum benchmark suite\n", "abstract": " In this paper we present a platform independent analysis of the dynamic profiles of Java programs when executing on the Java Virtual Machine. The Java programs selected are taken from the Java Grande Forum benchmark suite, and five different Java-to-bytecode compilers are analysed. The results presented describe the dynamic instruction usage frequencies, as well as the sizes of the local variable, parameter and operand stacks during execution on the JVM.", "num_citations": "62\n", "authors": ["1762"]}
{"title": "An analysis of rule coverage as a criterion in generating minimal test suites for grammar-based software\n", "abstract": " The term grammar-based software describes software whose input can be specified by a context-free grammar. This grammar may occur explicitly in the software, in the form of an input specification to a parser generator, or implicitly, in the form of a hand-written parser, or other input-verification routines. Grammar-based software includes not only programming language compilers, but also tools for program analysis, reverse engineering, software metrics and documentation generation. Such tools often play a crucial role in automated software development, and ensuring their completeness and correctness is a vital prerequisite for their us. In this paper we propose a strategy for the construction of test suites for grammar based software, and illustrate this strategy using the ISO CPP grammar. We use the concept of rule coverage as a pivot for the reduction of implementation-based and specification-based test suites\u00a0\u2026", "num_citations": "59\n", "authors": ["1762"]}
{"title": "On the application of software metrics to UML models\n", "abstract": " In this position paper we discuss a number of issues relating to model metrics, with particular emphasis on metrics for UML models. Our discussion is presented as a series of nine observations where we examine some of the existing work on applying metrics to UML models, present some of our own work in this area, and specify some topics for future research that we regard as important. Furthermore, we identify three categories of challeges for model metrics and describe how our nine observations can be partitioned into these categories.", "num_citations": "57\n", "authors": ["1762"]}
{"title": "An interpretation of Purdom's algorithm for automatic generation of test cases\n", "abstract": " We present a structured reformulation of the seminal algorithm for automatic generation of test cases for a context-free grammar. Our reformulation simplifies the algorithm in several ways. First, we provide a structured reformulation so that it is obvious where to proceed at each step. Second, we partition the intricate third phase into five functions, so that the discussion and comprehension of this phase can be modularized. Our implementation of the algorithm provides information about the grammatic, syntactic and semantic correctness of the generated test cases for two important languages in use today: C and C++. The results of our study of C and C++ highlight a lacuna latent in the research to date. In particular, if one or more of the automatically generated test cases is syntactically or semantically incorrect, then the confidence of structural \\coverage\" may be compromised for the particular grammar-based tool under test. Our ongoing work focuses on a solution to this problem.", "num_citations": "48\n", "authors": ["1762"]}
{"title": "Program annotation in XML: a parse-tree based approach\n", "abstract": " In this paper we describe a technique that can be used to annotate source code with syntactic tags in XML format. This is achieved by modifying the parser generator bison to emit these tags for an arbitrary LALR grammar. We also discuss an immediate application of this technique, a portable modification of the gcc compiler, that allows for XML output for C, Objective C, C++ and Java programs. While our approach is based on a representation of the parse-tree and does not have the same semantic richness as other approaches, it does have the advantage of being language independent, and thus re-usable in a number of different domains.", "num_citations": "47\n", "authors": ["1762"]}
{"title": "An empirical investigation into the dimensions of run-time coupling in Java programs\n", "abstract": " Software quality is an important external software attribute that is di\u00b1cult to measure objectively. Several studies have identified a clear empirical relationship between static coupling metrics and software quality. However due to the nature of object-oriented programs, static metrics fail to quantify all the underlying dimensions of coupling, as program behaviour is a function of its operational environment as well as the complexity of the source code. In this paper a set of run-time object-oriented coupling metrics are described. A method of collecting such metrics which utilises the Java Platform Debug Architecture is described and a collection of Java programs from the SPECjvm98 benchmark suite are evaluated. A number of statistical techniques including descriptive statistics, a correlation study and principal component analysis are used to assess the fundamental properties of the measures and investigate whether they are redundant with respect to the Chidamber and Kemerer static CBO metric. Results to date indicate that run-time coupling metrics can provide an interesting and informative qualitative analysis of a program and complement existing static coupling metrics.", "num_citations": "42\n", "authors": ["1762"]}
{"title": "Reveal: A tool to reverse engineer class diagrams\n", "abstract": " Many systems are constructed without the use of modeling and visualization artifacts, due to constraints imposed by deadlines or a shortage of manpower. Nevertheless, such systems might profit from the visualization provided by diagrams to facilitate maintenance of the constructed system. In this paper, we present a tool, Reveal, to reverse engineer a class diagram from the C + + source code representation of the software. In Reveal, we remain faithful to the UML standard definition of a class diagram wherever possible. However, to accommodate the vagaries of the C + + language, we offer some extensions to the standard notation to include representations for namespaces, stand-alone functions and friend functions. We compare our representation to three other tools that reverse-engineer class diagrams, for both compliance to the UML standard and for their ability to faithfully represent the software system under study.", "num_citations": "40\n", "authors": ["1762"]}
{"title": "Exploiting UML dynamic object modeling for the visualization of C++ programs\n", "abstract": " In this paper we present an approach to modeling and visualizing the dynamic interactions among objects in a C++ application. We exploit UML diagrams to expressively visualize both the static and dynamic properties of the application. We make use of a class diagram and call graph of the application to select the parts of the application to be modeled, thereby reducing the number of objects and methods under consideration with a concomitant reduction in the cognitive burden on the user of our system. We use aspects to insert probes into the application to enable profiling of the interactions of objects and methods and we visualize these interactions by providing sequence and communication diagrams for the parts of the program under consideration. We complement our static selectors with dynamic selectors that enable the user to further filter objects and methods from the sequence and communication\u00a0\u2026", "num_citations": "39\n", "authors": ["1762"]}
{"title": "Toward a definition of run-time object-oriented metrics\n", "abstract": " This position paper outlines a programme of research based on the quantification of run-time elements of Java programs. In particular, we adapt two common object- oriented  metrics,  coupling  and  cohesion,  so  that  they  can be applied at run-time.  We demonstrate some preliminary results of our analysis on programs from the SPEC JVM98 benchmark suite", "num_citations": "38\n", "authors": ["1762"]}
{"title": "A survey of UML-based coverage criteria for software testing\n", "abstract": " The Unified Modeling Language (UML) is a standard notation that can be used to model object oriented software systems. With the growing adoption of UML by the software development industry and academia, researchers have begun to investigate how it can be used in the testing phase of the software development process. Several approaches to software testing have been proposed in which test requirements and coverage criteria are derived from UML models. This report introduces and analyses the various UML-based coverage criteria that exist in the software testing literature.", "num_citations": "37\n", "authors": ["1762"]}
{"title": "Run-time cohesion metrics: An empirical investigation\n", "abstract": " Cohesion is one of the fundamental measures of the \u2019goodness\u2019 of a software design. The most accepted and widely studied object-oriented cohesion metric is Chidamber and Kemerer\u2019s Lack of Cohesion in Methods measure. However due to the nature of object-oriented programs, static design metrics fail to quantify all the underlying dimensions of cohesion, as program behaviour is a function of it operational environment as well as the complexity of the source code. For these reasons two run-time object-oriented cohesion metrics are described in this paper, and applied to Java programs from the SPECjvm98 benchmark suite. A statistical analysis is conducted to assess the fundamental properties of the measures and investigate whether they are redundant with respect to the static cohesion metric. Results to date indicate that run-time cohesion metrics can provide an interesting and informative qualitative analysis of a program and complement existing static cohesion metrics.", "num_citations": "37\n", "authors": ["1762"]}
{"title": "White-box coverage criteria for model transformations\n", "abstract": " Model transformations are core to MDE, and one of the key aspects for all model transformations is that they are validated. In this paper we develop an approach to testing model transformations based on white-box coverage measures of the transformations. To demonstrate the use of this approach we apply it to some examples from the ATL metamodel zoo.", "num_citations": "36\n", "authors": ["1762"]}
{"title": "Symbol table construction and name lookup in ISO C++\n", "abstract": " The authors present an object oriented model of symbol table construction and name lookup for ISO C++ using the Unified Modeling Language (UML). Our use of UML class, activity and sequence diagrams serves to explicate our model and our use of patterns such as decorator and facade increase the understandability of the model. Clause three of the ISO C++ standard describes the procedures and rules for performing name lookup; our activity and sequence diagrams serve to simulate these procedures in graphical fashion. An advantage of our approach is that our model can increase C++ understandability for those practitioners with a working UML knowledge. An important contribution of our work is that our model forms the basis for construction of a parser front-end for ISO C++. Our explication of the name lookup problem represents a necessary first step in this construction and our component approach is\u00a0\u2026", "num_citations": "35\n", "authors": ["1762"]}
{"title": "Towards the re-usability of software metric definitions at the meta level\n", "abstract": " A large number of metrics for evaluating the quality of software have been proposed in the literature. However, there is no standard terminology or formalism for defining metrics and consequently many of the metrics proposed have some ambiguity in their definitions. This hampers the empirical validation of these metrics. To address this problem, we generalise an existing approach to defining metrics that is based on the Object Constraint Language and the Unified Modelling Language metamodel. We have developed a prototype tool called DMML (Defining Metrics at the Meta Level) that supports this approach and we present details of this tool. To illustrate the approach, we present formal definitions for the Chidamber and Kemerer metrics suite.", "num_citations": "34\n", "authors": ["1762"]}
{"title": "Exploiting attributed type graphs to generate metamodel instances using an SMT solver\n", "abstract": " In this paper we present an approach to generating instances of metamodels using a Satisfiability Modulo Theories (SMT) solver as a back-end engine. Our goal is to automatically translate a metamodel and its invariants into SMT formulas which can be investigated for satisfiability by an external SMT solver, with each satisfying assignment for SMT formulas interpreted as an instance of the original metamodel. Our automated translation works by interpreting a metamodel as a bounded Attributed Type Graph with Inheritance (ATGI) and then deriving a finite universe of all bounded attribute graphs typed over this bounded ATGI. The graph acts as an intermediate representation which we then translate into SMT formulas. The full translation process, from metamodels to SMT formulas, and then from SMT instances back to metamodel instances, has been successfully automated in our tool, with the results showing the\u00a0\u2026", "num_citations": "33\n", "authors": ["1762"]}
{"title": "A metamodel for the measurement of object-oriented systems: An analysis using Alloy\n", "abstract": " This paper presents a MOF-compliant metamodel for calculating software metrics and demonstrates how it is used to generate a metrics tool that calculates coupling and cohesion metrics. We also describe a systematic approach to the analysis of MOF-compliant metamodels and illustrate the approach using the presented metamodel. In this approach, we express the metamodel using UML and OCL and harness existing automated tools in a framework that generates a Java implementation and an Alloy specification of the metamodel, and use this both to examine the metamodel constraints, and to generate instantiations of the metamodel. Moreover, we describe how the approach can be used to generate test data for any software based on a MOF-compliant metamodel. We extend our framework to support this approach and use it to generate a test suite for the metrics calculation tool that is based on our metamodel.", "num_citations": "31\n", "authors": ["1762"]}
{"title": "A study of the influence of coverage on the relationship between static and dynamic coupling metrics\n", "abstract": " This paper examines the relationship between the static coupling between objects (CBO) metric and some of its dynamic counterparts. The dimensions of the relationship for Java programs are investigated, and the influence of instruction coverage on this relationship is measured. An empirical evaluation of 14 Java programs taken from the SPEC JVM98 and the JOlden benchmark suites is conducted using the static CBO metric, six dynamic metrics and instruction coverage data.The results presented here confirm preliminary studies indicating the independence of static and dynamic coupling metrics, but point to a strong influence of coverage on the relationship. Based on this, this paper suggests that dynamic coupling metrics might be better interpreted in the context of coverage measures, rather than as stand-alone software metrics.", "num_citations": "30\n", "authors": ["1762"]}
{"title": "Decorating tokens to facilitate recognition of ambiguous language constructs\n", "abstract": " Software tools are fundamental to the comprehension, analysis, testing and debugging of application systems. A necessary first step in the development of many tools is the construction of a parser front\u2010end that can recognize the implementation language of the system under development. In this paper, we describe our use of token decoration to facilitate recognition of ambiguous language constructs. We apply our approach to the C++ language since its grammar is replete with ambiguous derivations such as the declaration/expression and template\u2010declaration/expression ambiguity. We describe our implementation of a parser front\u2010end for C++, keystone, and we describe our results in decorating tokens for our test suite including the examples from Clause Three of the C++ standard. We are currently exploiting the keystone front\u2010end to develop a taxonomy for implementation\u2010based class testing and to reverse\u00a0\u2026", "num_citations": "29\n", "authors": ["1762"]}
{"title": "A sound execution semantics for ATL via translation validation\n", "abstract": " In this work we present a translation validation approach to encode a sound execution semantics for the ATL specification. Based on our sound encoding, the goal is to soundly verify an ATL specification against the specified OCL contracts. To demonstrate our approach, we have developed the VeriATL verification system using the Boogie2 intermediate verification language, which in turn provides access to the Z3 theorem prover. Our system automatically encodes the execution semantics of each ATL specification (as it appears in the ATL matched rules) into the intermediate verification language. Then, to ensure the soundness of the encoding, we verify that it soundly represents the runtime behaviour of its corresponding compiled implementation in terms of bytecode instructions for the ATL virtual machine. The experiments demonstrate the feasibility of our approach. They also illustrate how to\u00a0\u2026", "num_citations": "28\n", "authors": ["1762"]}
{"title": "Metric-based analysis of context-free grammars\n", "abstract": " Recent advances in software engineering have produced a variety of well-established approaches, formalisms and techniques to facilitate the construction of large-scale applications. Developers who are interested in the construction of robust, extensible software that is easy to maintain should expect to deploy a range of these techniques, as appropriate to the task. In this paper, we provide a foundation for the application of established software metrics to the measurement of context-free grammars. The usual application of software metrics is to program code; we provide a mapping that allows these metrics to be applied to grammars. This allows us to interpret six software engineering metrics in a grammatical context, including T.J. McCabe's (1976) complexity metric and N.E. Fenton et al.'s (1996) impurity metric. We have designed and implemented a tool to automatically compute the six metrics; as a case study\u00a0\u2026", "num_citations": "28\n", "authors": ["1762"]}
{"title": "An infrastructure to support interoperability in reverse engineering\n", "abstract": " The reverse engineering community has recognized the importance of interoperability, the cooperation of two or more systems to enable the exchange and utilization of data, and has noted that the current lack of interoperability is a contributing factor to the lack of adoption of available infrastructures. To address the problems of interoperability and reproducing previous results, we present an infrastructure that supports interoperability among reverse engineering tools and applications. We present the design of our infrastructure, including the hierarchy of schemas that captures the interactions among graph structures. We also develop and utilize our implementation, which is designed using a GXL-based pipe-filter architecture, to perform a case study that demonstrates the feasibility of our infrastructure.", "num_citations": "26\n", "authors": ["1762"]}
{"title": "Notes on formal language theory and parsing\n", "abstract": " Lexical analysis, also called scanning, is the phase of the compilation process which deals with the actual program being compiled, character by character. The higher level parts of the compiler will call the lexical analyzer with the command\u201d get the next word from the input\u201d, and it is the scanner\u2019s job to sort through the input characters and find this word.", "num_citations": "25\n", "authors": ["1762"]}
{"title": "Applying software engineering techniques to parser design: the development of a C# parser\n", "abstract": " In this paper we describe the development of a parser for the C# programming language. We outline the development process used, detail its application to the development of a C# parser and present a number of metrics that describe the parser\u2019s evolution. This paper presents and reinforces an argument for the application of software engineering techniques in the area of parser design. The development of a parser for the C# programming language is in itself important to software engineering, since parsers form the basis for tools such as metrics generators, refactoring tools, pretty-printers and reverse engineering tools.", "num_citations": "25\n", "authors": ["1762"]}
{"title": "Experiences of using the dagstuhl middle metamodel for defining software metrics\n", "abstract": " In this paper we report on our experiences of using the Dagstuhl Middle Metamodel as a basis for defining a set of software metrics. This approach involves expressing the metrics as Object Constraint Language queries over the metamodel. We provide details of a system for specifying Java-based software metrics through a tool that instantiates the metamodel from Java class files and a tool that automatically generates a program to calculate the expressed metrics. We present details of an exploratory data analysis of some cohesion metrics to illustrate the use of our approach.", "num_citations": "24\n", "authors": ["1762"]}
{"title": "Platform independent timing of java virtual machine bytecode instructions\n", "abstract": " The accurate measurement of the execution time of Java bytecode is one factor that is important in order to estimate the total execution time of a Java application running on a Java Virtual Machine. In this paper we document the difficulties and solutions for the accurate timing of Java bytecode. We also identify trends across the execution times recorded for all imperative Java bytecodes. These trends would suggest that knowing the execution times of a small subset of the Java bytecode instructions would be sufficient to model the execution times of the remainder. We first review a statistical approach for achieving high precision timing results for Java bytecode using low precision timers and then present a more suitable technique using homogeneous bytecode sequences for recording such information. We finally compare instruction execution times acquired using this platform independent technique against\u00a0\u2026", "num_citations": "22\n", "authors": ["1762"]}
{"title": "Working with linear logic in coq\n", "abstract": " In this paper we describe the encoding of linear logic in the Coq system, a proof assistant for higher-order logic. This process involved encoding a suitable consequence relation, the relevant operators, and some auxiliary theorems and tactics. The encoding allows us to state and prove theorems in linear logic, and we demonstrate its use through two examples: a simple blocks world scenario, and the Towers of Hanoi problem.", "num_citations": "22\n", "authors": ["1762"]}
{"title": "An approach to quantifying the run-time behaviour of Java GUI applications\n", "abstract": " This paper outlines a new technique for collecting dynamic trace information from Java GUI programs. The problems of collecting run-time information from such interactive applications in comparison with traditional batch style execution benchmark programs is outlined. The possible utility of such run-time information is discussed and from this a number of simple run-time metrics are suggested. The metrics results for a small CelsiusConverter Java GUI program are illustrated to demonstrate the viability of such an analysis.", "num_citations": "21\n", "authors": ["1762"]}
{"title": "A method\u2010level comparison of the Java Grande and SPEC JVM98 benchmark suites\n", "abstract": " In this paper we seek to provide a foundation for the study of the level of use of object\u2010oriented techniques in Java programs in general, and scientific applications in particular. Specifically, we investigate the profiles of Java programs from a number of perspectives, including the use of class library methods, the size of methods called, the mode of invoke instruction used and the polymorphicity of call sites. We also present a categorization of the nature of small methods used in Java programs. We compare the Java Grande and SPEC JVM98 benchmark suites, and note a significant difference in the nature and composition of these suites, with the programs from the Java Grande suite demonstrating a less object\u2010oriented approach. Copyright \u00a9 2005 John Wiley & Sons, Ltd.", "num_citations": "20\n", "authors": ["1762"]}
{"title": "Benchmarking the java virtual architecture\n", "abstract": " In this chapter we present a study of the SPEC JVM98 benchmark suite at a dynamic platform-independent level. The results presented describe the influence of class library code, the relative importance of various methods in the suite, as well as the sizes of the local variable, parameter and operand stacks. We also examine the dynamic bytecode instruction usage frequencies, and discuss their relevance. The influence of the choice of Java source to bytecode compiler is shown to be relatively insignificant at present.             These results have implications for the coverage aspects of the SPEC JVM98 benchmark suites, for the performance of the Java-to-bytecode compilers, and for the design of the Java Virtual Machine.", "num_citations": "20\n", "authors": ["1762"]}
{"title": "Testing C++ compilers for ISO language conformance\n", "abstract": " In this paper, we describe our construction of a test harness to measure conformance of some popular C++ compilers and to measure the progress of the gcc C++ compiler as it moves toward ISO conformance. In an attempt to apply the same standard to all of the vendors, we use the same test cases and the same testing framework for all executions, even though some of the compilers are platform dependent and there is no common platform for all compilers. We found that the Python language provided the functionality that we needed with its scripting facility, its platform independence and its object orientation to facilitate code reuse. Python includes a testing framework as a module of the language and we have extended the framework to measure C++ ISO conformance.", "num_citations": "19\n", "authors": ["1762"]}
{"title": "gccXfront: Exploiting gcc as a front end for program comprehension tools via XML/XSLT\n", "abstract": " Parsing programming languages is an essential component of the front end of most program comprehension tools. Languages such as C++ can be difficult to parse and so it can prove useful to re-use existing front ends such as those from the GNU compiler collection, gcc. We have modified gcc to provide syntactic tags in XML format around the source code which can greatly enhance our comprehension of the program structure. Further, by using XML transformation stylesheets, the XML outputted by our modified gcc can be translated into a more readable format. Our tool, gccXfront leverages the power and portability of the gcc suite, since any C, C++, Objective C or Java program can be processed using gcc. Our tool can thus act as a bridge between gcc and other program comprehension tools that accept XML formatted input.", "num_citations": "18\n", "authors": ["1762"]}
{"title": "Platform independent dynamic Java virtual machine analysis: the Java Grande Forum benchmark suite\n", "abstract": " In this paper we present a platform independent analysis of the dynamic profiles of Java programs when executing on the Java Virtual Machine.  The Java programs selected are taken from the Java Grande Forum benchmark suite and five different Java\u2010to\u2010bytecode compilers are analysed.  The results presented describe the dynamic instruction usage frequencies, as well as the sizes of the local variable, parameter and operand stacks during execution on the JVM. These results, presenting a picture of the actual (rather than presumed) behaviour of the JVM, have implications both for the coverage aspects of the Java Grande benchmark suites, for the performance of the Java\u2010to\u2010bytecode compilers and for the design of the JVM. Copyright \u00a9 2003 John Wiley & Sons, Ltd.", "num_citations": "18\n", "authors": ["1762"]}
{"title": "An approach for modeling the name lookup problem in the C++ programming lanaguage\n", "abstract": " Formal grammars are well established for specifying the syntax of programming languages. However, the formal specification of programming language semantics has proven more elusive. A recent standard, the Unified Modeling Language (UML), has quickly become established as a common framework for the specification of large scale software applications. In this paper, we describe an approach for using the UML to solve the name lookup problem for the recently standardized C++ programming language. We apply our approach to C++ because a solution to the name lookup problem is required for parser construction and the development of analysis and testing tools.", "num_citations": "17\n", "authors": ["1762"]}
{"title": "Quantifying the transition from Python 2 to 3: An empirical study of Python applications\n", "abstract": " Background: Python is one of the most popular modern programming languages. In 2008 its authors introduced a new version of the language, Python 3.0, that was not backward compatible with Python 2, initiating a transitional phase for Python software developers. Aims: The study described in this paper investigates the degree to which Python software developers are making the transition from Python 2 to Python 3. Method: We have developed a Python compliance analyser, PyComply, and have assembled a large corpus of Python applications. We use PyComply to measure and quantify the degree to which Python 3 features are being used, as well as the rate and context of their adoption. Results: In fact, Python software developers are not exploiting the new features and advantages of Python 3, but rather are choosing to retain backward compatibility with Python 2. Conclusions: Python developers are\u00a0\u2026", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Metamodel instance generation: A systematic literature review\n", "abstract": " Modelling and thus metamodelling have become increasingly important in Software Engineering through the use of Model Driven Engineering. In this paper we present a systematic literature review of instance generation techniques for metamodels, i.e. the process of automatically generating models from a given metamodel. We start by presenting a set of research questions that our review is intended to answer. We then identify the main topics that are related to metamodel instance generation techniques, and use these to initiate our literature search. This search resulted in the identification of 34 key papers in the area, and each of these is reviewed here and discussed in detail. The outcome is that we are able to identify a knowledge gap in this field, and we offer suggestions as to some potential directions for future research.", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Analysing the effectiveness of rule-coverage as a reduction criterion for test suites of grammar-based software\n", "abstract": " The term grammar-based software describes software whose input can be specified by a context-free grammar. This grammar may occur explicitly in the software, in the form of an input specification to a parser generator, or implicitly, in the form of a hand-written parser. Grammar-based software includes not only programming language compilers, but also tools for program analysis, reverse engineering, software metrics and documentation generation. Hence, ensuring their completeness and correctness is a vital prerequisite for their use. In this paper we propose a strategy for the construction of test suites for grammar based software, and illustrate this strategy using the ISO C\u2009+\u2009+\u2009 grammar. We use the concept of grammar-rule coverage as a pivot for the reduction of an implementation-based test suite, and demonstrate a significant decrease in the size of this suite. The effectiveness of this reduced test\u00a0\u2026", "num_citations": "16\n", "authors": ["1762"]}
{"title": "A tool chain for reverse engineering C++ applications\n", "abstract": " We describe a tool chain that enables experimentation and study of real C++\u00a0applications. Our tool chain enables reverse engineering and program analysis by exploiting gcc, and thus accepts any C++\u00a0application that can be analysed by the C++\u00a0parser and front end of gcc. Our current test suite consists of large, open-source applications with diverse problem domains, including language processing and gaming. Our tool chain is designed using a GXL-based pipe-filter architecture; therefore, the individual applications and libraries that constitute our tool chain each provide a point of access. The preferred point of access is the g4api Application Programming Interface (API), which is located at the end of the chain. g4api provides access to information about the C++\u00a0program under study, including information about declarations, such as classes (including template instantiations); namespaces; functions; and\u00a0\u2026", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Using a molecular metaphor to facilitate comprehension of 3d object diagrams\n", "abstract": " This paper presents a strategy for the visualization of dynamic object relationships in Java programs. The metaphor of a chemical molecule is used to aid comprehension, and to help in reducing the size of the object graph. Our strategy has been implemented by dynamically instrumenting Java bytecode to collect trace data, which is then analyzed and visualized in 3D using VRML. Quantitative and graphical results are presented, based on an analysis of programs in the SPEC JVM98 and JOlden benchmark suites.", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Progression toward conformance of C++ language compilers\n", "abstract": " Establishing the conformance of a compiler is a difficult matter, especially for C++, since a standard for the language was slow to develop and acceptance of the standard occurred years after the introduction of the language. In this paper, we re-visit our conformance study presented in June 2002. We describe the construction of our test suite composed of test cases extracted from the code examples in the ISO C++ standard together with the process that we use to remove test cases extracted from examples under investigation by the working committee. We then use our test suite to provide some measure of conformance to the ISO standard for eight popular C++ compilers.", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Bi-gram analysis of Java bytecode sequences\n", "abstract": " We report on a project that performed a bigram analysis of dynamic bytecode sequences. The objective was to identify the most commonly used bytecode pairs, and to examine the relative frequency of occurrence of these bytecodes. In all, 12 large Java programs were analysed, taken from the Java Grande and SPEC benchmark suites. Our findings are of relevance to research into instruction set design and implementation, as well as JVM optimisation", "num_citations": "16\n", "authors": ["1762"]}
{"title": "Teaching discrete structures: a systematic review of the literature\n", "abstract": " This survey paper reviews a large sample of publications on the teaching of discrete structures and discrete mathematics in computer science curricula. The approach is systematic, in that a structured search of electronic resources has been conducted, and the results are presented and quantitatively analyzed. A number of broad themes in discrete structures education are identified relating to course content, teaching strategies and the means of evaluating the success of a course.", "num_citations": "15\n", "authors": ["1762"]}
{"title": "A definition of the Chidamber and Kemerer metrics suite for the Unified Modeling Language\n", "abstract": " Since there is no standard formalism for defining software metrics, many of the measures that exist have some ambiguity in their definitions which hinders their comparison and implementation. We address this problem by presenting an approach for defining software metrics. This approach is based on expressing the measures as Object Constraint Language queries over a language metamodel. To illustrate the approach, we specify how the Chidamber and Kemerer metrics suite can be measured from Unified Modelling Language class diagrams by presenting formal definitions for these metrics using the Unified Modelling Language 2.0 metamodel. Keywords: OO metrics, class diagram metrics, metamodels, UML, OCL. 1", "num_citations": "15\n", "authors": ["1762"]}
{"title": "A Java distributed computation library\n", "abstract": " This paper describes the design and development of a Java Distributed Computation Library, which provides a simple development platform for developers who wish to quickly implement a distributed computation in the context of an SPMD architecture (Single Program, Multiple Data). The need for this research arose out of the realisation that the currently available distributed computation libraries and systems do not adequately meet certain criteria, such as ease of development, dynamic changes to system behaviour, and easy deployment of distributed software. The proposed solution to this problem was to produce a Java-based distributed computation library which enables developers to use the Java language to quickly and easily implement a distributed computation. The results of experiments conducted using DCL are also presented, as a means of showing that DCL met its design goals.", "num_citations": "15\n", "authors": ["1762"]}
{"title": "Toward an infrastructure to support interoperability in reverse engineering\n", "abstract": " In this paper we present an infrastructure that supports interoperability among various reverse engineering tools and applications. We include an application programmer's interface that permits extraction of information about declarations, including classes, functions and variables, as well as information about scopes, types and control statements in C++ applications. We also present a hierarchy of canonical schemas that capture minimal functionality for middle-level graph structures. This hierarchy facilitates an unbiased comparison of results for different tools that implement the same or a similar schema. We have a repository, hosted by SourceForge.net, where we have placed the artifacts of our infrastructure", "num_citations": "14\n", "authors": ["1762"]}
{"title": "An empirical analysis of the transition from Python 2 to Python 3\n", "abstract": " Python is one of the most popular and widely adopted programming languages in use today. In 2008 the Python developers introduced a new version of the language, Python 3.0, that was not backward compatible with Python 2, initiating a transitional phase for Python software developers. In this paper, we describe a study that investigates the degree to which Python software developers are making the transition from Python 2 to Python 3. We have developed a Python compliance analyser, PyComply, and have analysed a previously studied corpus of Python applications called Qualitas. We use PyComply to measure and quantify the degree to which Python 3 features are being used, as well as the rate and context of their adoption in the Qualitas corpus. Our results indicate that Python software developers are not exploiting the new features and advantages of Python 3, but rather are choosing to retain\u00a0\u2026", "num_citations": "12\n", "authors": ["1762"]}
{"title": "An empirical study of run-time coupling and cohesion software metrics\n", "abstract": " The extent of coupling and cohesion in an object-oriented system has implications for its external quality. Various static coupling and cohesion metrics have been proposed and used in past empirical investigations, however none of these have taken the run-time properties of a program into account. As program behaviour is a function of its operational environment as well as the complexity of the source code, static metrics may fail to quantify all the underlying dimensions of coupling and cohesion. By considering both of these influences, one will acquire a more comprehensive understanding of the quality of critical components of a software system. We believe that any measurement of these attributes should include changes that take place at run-time. For this reason, in this work we address the utility of run-time coupling and cohesion complexity through the empirical evaluation of a selection of run-time measures for these properties. This study is carried out using a comprehensive selection of Java benchmark and real world programs. Our first case study investigates the influence of instruction coverage on the relationship between static and run-time coupling metrics. Our second case study defines a new run-time coupling metric that can be used to study object behaviour and investigates the ability of measures of run-time cohesion to predict such behaviour. Finally, we investigate whether run-time coupling metrics are good predictors of software fault-proneness in comparison to standard coverage measures. To the best of our knowledge this is the largest empirical study that has been performed to date on the run-time analysis of Java programs.", "num_citations": "12\n", "authors": ["1762"]}
{"title": "g4re: Harnessing gcc to reverse engineer C++ applications\n", "abstract": " In this paper, we describe g4re, our tool chain that exploits GENERIC, an intermediate format incorporated into the gcc C++ compiler, to facilitate analysis of real C++ applications. The gcc GENERIC representation is available through a file generated for each translation unit (TU), and g4re reads each TU file and constructs a corresponding Abstract Semantic Graph (ASG). Since TU files can be prohibitively large, ranging from 11 megabytes for a\" hello world\" program, to 18 gigabytes for a version of Mozilla Thunderbird, we describe our approach for reducing the size of the generated ASG.", "num_citations": "12\n", "authors": ["1762"]}
{"title": "Predicting SMT solver performance for software verification\n", "abstract": " The approach Why3 takes to interfacing with a wide variety of interactive and automatic theorem provers works well: it is designed to overcome limitations on what can be proved by a system which relies on a single tightly-integrated solver. In common with other systems, however, the degree to which proof obligations (or \u201cgoals\u201d) are proved depends as much on the SMT solver as the properties of the goal itself. In this work, we present a method to use syntactic analysis to characterise goals and predict the most appropriate solver via machine-learning techniques. Combining solvers in this way-a portfolio-solving approach-maximises the number of goals which can be proved. The driver-based architecture of Why3 presents a unique opportunity to use a portfolio of SMT solvers for software verification. The intelligent scheduling of solvers minimises the time it takes to prove these goals by avoiding solvers which return\u00a0\u2026", "num_citations": "11\n", "authors": ["1762"]}
{"title": "A top-down presentation of Purdom\u2019s sentence-generation algorithm\n", "abstract": " In this paper we present a reformulation of a sentencegeneration algorithm for context free grammars, originally due to Purdom. We restructure the original algorithm so that it resembles the familiar pattern of a top-down parsing algorithm. We express the algorithm in a declarative style that significantly facilitates its comprehension and implementation.Purdom\u2019s sentence generation algorithm takes a contextfree grammar, and generates sentences from the language represented by that grammar [2]. Run to completion, the algorithm will generate a set of sentences such that each rule in the grammar has been used at least once. The original formulation of the algorithm, published in 1972, can be somewhat difficult to understand, since the steps involved are described in an imperative style. In previous work we have reformulated the algorithm in a more procedural style, and implemented it in C++[1]. This paper takes that process a step further by expressing the algorithm in a more declarative style, yielding a significantly simpler version. In what follows we describe our reformulation of Purdom\u2019s algorithm. The reader is encouraged to consult [1] for more background information. Section 1 presents an overview of our approach, drawing an analogy between the sentence-generation problem and the standard approach to top-down parsing. Section 2 informally describes the crux of the algorithm, which involves choosing the correct rule for a non-terminal at a given stage in the generation process. Section 3 describes short and prev the main algorithms in this process, in detail.", "num_citations": "10\n", "authors": ["1762"]}
{"title": "Identifying and evaluating a generic set of superinstructions for embedded Java programs\n", "abstract": " In this paper we present an approach to the optimisation of interpreted Java programs using superinstructions. Unlike existing techniques, we examine the feasibility of identifying a generic set of superinstructions across a suite of programs, and implementing them statically on a JVM. We formally present the sequence analysis algorithm and we describe the resulting sets of superinstructions for programs from the embedded CaffeineMark benchmark suite. We have implemented the approach on the Jam VM, a lightweight JVM, and we present results showing the level of speedup possible from this approach.", "num_citations": "9\n", "authors": ["1762"]}
{"title": "NUIM-CS-TR2002-07\n", "abstract": " In this paper we seek to provide a foundation for the study of the level of use of object-oriented techniques in Java programs in general, and scientific applications in particular. Specifically, we focus on the use of small methods, and the frequency with which they are called, since this forms the basis for the study of method inlining, an important optimisation technique. We compare the Grande and SPEC benchmark suites, and note a significant difference in the nature and composition of these suites, with the programs from the Grande suite demonstrating a less object-oriented approach. On average, they make less use of the class library than SPEC applications and use longer methods.", "num_citations": "9\n", "authors": ["1762"]}
{"title": "Exploiting metrics to facilitate grammar transformation into LALR format\n", "abstract": " Exploiting Metrics to Facilitate Grammar Transformation into LALR Format Page 1 Exploiting Metrics to Facilitate Grammar Transformation into LALR Format James F. Power National University of Ireland, Maynooth Computer Science Dept County Kildare, Ireland James.Power@may.ie Brian A. Malloy Clemson University Computer Science Dept Clemson, SC USA malloy@cs.clemson.edu ABSTRACT \u00a2\u00a1\u00a4\u00a3\u00a6 \u00a5\u00a7 \u00a9 \u00a3 \u00a9 \u00a3 \u00a4\u00a3 \u00a9!\u00a7#\" $#\u00a9& %')((1 02 3 $ \u00a4\u00a3\u00a6 $)4\u00a2 \" \u00a1\u00a4\u00a3\u00a6 $ 576\u00a4\u00a3 8\"& \u00a3@9A\u00a7#B\u00a6\u00a5A52\u00a3 C $#4 \u00a7D 6\u00a4$)BE\u00a7)02 \u00a4 \u00a5F\u00a3G0IHAGP 57\u00a7) \u00a4 Q\u00a7# \u00a3 R\u00a2 \u00a5\u00a4\u00a9 $ ST0U6V02 AW 07 ) A0IHG\u00a7) X\"E \u00a3 A\u00a1A\u00a7) \u00a4G\u00a3 B\u00a6\u00a3 T\" \u00a6 $ SY\u00a3@\u00a9` \u00a1A\u00a7) A6V G$T6\u00a4\u00a36a \u00a5\u00a7 \u00a9 \u00a3 \u00a9 ` 02 a \" \u00a1\u00a4\u00a3b \u00a7#\u00a9 \u00a3c\u00a7d $#43 \u00a5F\u00a3\u00a3c6eRf \u00a3 g1 G 07\u00a3 AG hW \u00a7# 6i BE\u00a7)02 X\"!\u00a7)02 \u00a7#pA02520I\"qhXr3 st\u00a3 \u00a5\u00a40I\" \u00a3u 0I\" w vf076\u00a4\u00a3 \u00a5V\u00a9 \u00a3c\u00a7 6b QA \u00a3)Rx $#4y \" \u00a3 \u00a6 07 \u00a6 \u00a1\u00a402 \u00a1A5Ihu G$ B\u00a6\u00a5\u00a457\u00a3@9u 8hV 8\" \u00a3B\u00a6 QAG!\u00a1E \u00a7) G$)B\u00a6\u00a5A0252\u00a3 \u00a9 $#\u00a9 \u00a5V\u00a9 $ )\u00a9!\u00a7#B \u00a7# \u00a7#52hV 02 \" $X$ \u2026", "num_citations": "9\n", "authors": ["1762"]}
{"title": "Using a class abstraction technique to predict faults in OO classes: a case study through six releases of the eclipse JDT\n", "abstract": " In this paper, we propose an innovative suite of metrics based on a class abstraction that uses a taxonomy for OO classes (CAT) to capture aspects of software complexity through combinations of class characteristics. We empirically validate their ability to predict fault prone classes using fault data for six versions of the Java-based open-source Eclipse Integrated Development Environment. We conclude that this proposed CAT metric suite, even though it treats classes in groups rather than individually, is as effective as the traditional Chidamber and Kemerer metrics in identifying fault-prone classes.", "num_citations": "8\n", "authors": ["1762"]}
{"title": "Automated validation of class invariants in C++ applications\n", "abstract": " In this paper, we describe a non-invasive approach for validation of class invariants in C++ applications. Our approach is fully automated so that the user need only supply the class invariants for each class hierarchy to be checked and our validator constructs an InvariantVisitor, a variation of the Visitor Pattern, and an InvariantFacilitator. Instantiations of the InvariantVisitor and InvariantFacilitator classes encapsulate the invariants in C++ statements and facilitate the validation of the invariants. We describe both our approach and our results of validating invariants in keystone, a well tested parser front-end for C++.", "num_citations": "8\n", "authors": ["1762"]}
{"title": "Relating static and dynamic measurements for the Java virtual machine instruction set\n", "abstract": " It has previously been noted that, for conventional machine code, there is a strong relationship between static and dynamic code measurements. One of the goals of this paper is to examine whether this same relationship is true of Java programs at the bytecode level. To this end, the hypothesis of a linear correlation between static and dynamic frequencies was investigated using Pearson\u2019s correlation coefficient. Programs from the Java Grande and SPEC benchmarks suites were used in the analysis.", "num_citations": "8\n", "authors": ["1762"]}
{"title": "Measurement and analysis of runtime profiling data for Java programs\n", "abstract": " The authors examine a procedure for the analysis of data produced by the dynamic profiling of Java programs. In particular, we describe the issues involved in dynamic analysis, propose a metric for discrimination between the resulting data sets, and examine its application over different test suites and compilers.", "num_citations": "8\n", "authors": ["1762"]}
{"title": "Implementing protocol verification for E-commerce\n", "abstract": " This paper presents a survey of the practical application of protocol verification techniques to applications in ecommerce. We concentrate in particular on logic based approaches, and review the current state of the art as well as the prospects for realistic deployment of protocol verification techniques in the near future.", "num_citations": "8\n", "authors": ["1762"]}
{"title": "An institution for Event-B\n", "abstract": " This paper presents a formalisation of the Event-B formal specification language in terms of the theory of institutions. The main objective of this paper is to provide: (1) a mathematically sound semantics and (2) modularisation constructs for Event-B using the specification-building operations of the theory of institutions. Many formalisms have been improved in this way and our aim is thus to define an appropriate institution for Event-B, which we call . We provide a definition of  and the proof of its satisfaction condition. A motivating example of a traffic-light simulation is presented to illustrate our approach.", "num_citations": "7\n", "authors": ["1762"]}
{"title": "A testing strategy for abstract classes\n", "abstract": " One of the characteristics of the increasingly widespread use of object\u2010oriented libraries and the resulting intensive use of inheritance is the proliferation of dependencies on abstract classes. Since abstract classes cannot be instantiated, they cannot be tested in isolation using standard execution\u2010based testing strategies. A standard approach to testing abstract classes is to instantiate a concrete descendant class and test the features that are inherited. This paper presents a structured approach that supports the testing of features in abstract classes, paying particular attention to ensuring that the features tested are those defined in the abstract class. Two empirical studies are performed on a suite of large Java programs and the results presented. The first study analyses the role of abstract classes from a testing perspective. The second study investigates the impact of the testing strategy on the programs in this suite\u00a0\u2026", "num_citations": "7\n", "authors": ["1762"]}
{"title": "Exploiting design patterns to automate validation of class invariants\n", "abstract": " In this paper, techniques are presented that exploit two design patterns, the Visitor pattern and the Decorator pattern, to validate invariants about the data attributes in a C++ class automatically. To investigate the pragmatics involved in using the two patterns, a study of an existing, well\u2010tested application, keystone, a parser and front\u2010end for the C++ language, is presented. Results from the study indicate that these two patterns provide flexibility in terms of the frequency and level of granularity of validation of the class invariants, which are expressed in the Object Constraint Language (OCL). The quantitative results measure the impact of these approaches and the additional faults uncovered through validation of the case study. Copyright \u00a9 2005 John Wiley & Sons, Ltd.", "num_citations": "7\n", "authors": ["1762"]}
{"title": "Some observations on the application of software metrics to UML models\n", "abstract": " In this position paper we discuss some of the existing work on applying metrics to UML models, present some of our own work in this area, and specify some topics for future research that we regard as important.", "num_citations": "7\n", "authors": ["1762"]}
{"title": "A coverage analysis of java benchmark suites\n", "abstract": " The Java programming language provides an almost ideal environment for both static and dynamic analysis, being easy to parse, and supporting a standardised, easily-profiled virtual environment. In this paper we study the relationship between results obtainable from static and dynamic analysis of Java programs, and in particular the difficulties of correlating static and dynamic results. As a foundation for this study, we focus on various criteria related to run-time code coverage, as commonly used in test suite analysis. We have implemented a dynamic coverage analysis tool for Java programs, and we use it to evaluate several standard Java benchmark suites using line, instruction and branch coverage criteria. We present data indicating a considerable variance in static and dynamic analysis results between these suites, and even between programs in these suites.", "num_citations": "7\n", "authors": ["1762"]}
{"title": "Evaluating the use of a general-purpose benchmark suite for domain-specific SMT-solving\n", "abstract": " Benchmark suites are an important resource in validating performance requirements for software. However, generalpurpose suites may be unsuitable for domain-specific purposes, and may provide an incorrect indication of the software performance.", "num_citations": "6\n", "authors": ["1762"]}
{"title": "PACT: An initiative to introduce computational thinking to second-level education in Ireland\n", "abstract": " PACT (Programming \u2227 Algorithms \u21d2 Computational Thinking) is a partnership between researchers in the Department of Computer Science at Maynooth University and teachers at selected post-primary schools around Ireland. Starting in September 2013, seven Irish secondary schools took part in a pilot study, delivering material prepared by the PACT team to Transition Year students. Three areas of Computer Science were identified as being key to delivering a successful course in computational thinking, namely, programming, algorithms and computability. An overview of the PACT module is provided, as well as analysis of the feedback obtained from students and teachers involved in delivering the initial pilot.", "num_citations": "6\n", "authors": ["1762"]}
{"title": "Specification Clones: An empirical study of the structure of Event-B specifications\n", "abstract": " In this paper we present an empirical study of formal specifications written in the Event-B language. Our study is exploratory, since it is the first study of its kind, and we formulate metrics for Event-B specifications which quantify the diversity of such specifications in practice. We pay particular attention to refinement as this is one of the most notable features of Event-B. However, Event-B is less well-equipped with other standardised modularisation constructs, and we investigate the impact of this by detecting and analysing specification clones at different levels. We describe our algorithm used to identify clones at the machine, context and event level, and present results from an analysis of a large corpus of Event-B specifications. Our study contributes to furthering research into the area of metrics and modularisation in Event-B.", "num_citations": "5\n", "authors": ["1762"]}
{"title": "Can a computationally creative system create itself? Creative artefacts and creative processes\n", "abstract": " This paper begins by briefly looking at two of the dominant perspectives on computational creativity; focusing on the creative artefacts and the creative processes respectively. We briefly describe two projects; one focused on (artistic) creative artefacts the other on a (scientific) creative process, to highlight some similarities and differences in approach. We then look at a 2- dimensional model of Learning Objectives that uses independent axes of knowledge and (cognitive) processes. This educational framework is then used to cast artefact and process perspectives into a common framework, opening up new possibilities for discussing and comparing creativity between them. Finally, arising from our model of creative processes, we propose a new and broad 4-level hierarchy of computational creativity, which asserts that the highest level of computational creativity involves processes whose creativity is comparable to that of the originating process itself.", "num_citations": "5\n", "authors": ["1762"]}
{"title": "Formalised EMFTVM bytecode language for sound verification of model transformations\n", "abstract": " Model-driven engineering is an effective approach for addressing the full life cycle of software development. Model transformation is widely acknowledged as one of its central ingredients. With the increasing complexity of model transformations, it is urgent to develop verification tools that prevent incorrect transformations from generating faulty models. However, the development of sound verification tools is a non-trivial task, due to unimplementable or erroneous execution semantics encoded for the target model transformation language. In this work, we develop a formalisation for the EMFTVM bytecode language by using the Boogie intermediate verification language. It ensures the model transformation language has an implementable execution semantics by reliably prototyping the implementation of the model transformation language. It also ensures the absence of erroneous execution semantics\u00a0\u2026", "num_citations": "4\n", "authors": ["1762"]}
{"title": "Measurement of exception-handling code: An exploratory study\n", "abstract": " This paper presents some preliminary results from an empirical study of 12 Java applications from the Qualitas corpus. We measure the quantity and distribution of exception-handling constructs, and study their change as the systems evolve through several versions.", "num_citations": "4\n", "authors": ["1762"]}
{"title": "Intra-class testing of abstract class features\n", "abstract": " One of the characteristics of the increasingly widespread use of object-oriented libraries and the resulting intensive use of inheritance is the proliferation of dependencies on abstract classes. Such classes defer the implementation of some features, and are typically used as a specification or design tool. However, since their features are not fully implemented, abstract classes cannot be instantiated, and thus pose challenges for execution-based testing strategies. This paper presents a structured approach that supports the testing of features in abstract classes. Core to the approach is a series of static analysis steps that build a comprehensive view of the inter-class dependencies in the system under test. We then leveraged this information to define a test order for the methods in an abstract class that minimizes the number of stubs required during testing, and clearly identifies the required functionality of these stubs\u00a0\u2026", "num_citations": "4\n", "authors": ["1762"]}
{"title": "Ensuring behavioural equivalence in test-driven porting\n", "abstract": " In this paper we present a test-driven approach to porting code from one object-oriented language to another. We derive an order for the porting of the code, along with a testing strategy to verify the behaviour of the ported system at intra and inter-class level. We utilise the recently defined methodology for porting C++ applications, eXtreme porting, as a framework for porting. This defines a systematic routine based upon porting and unit-testing classes in turn. We augment this approach by using Object Relation Diagrams to define an order for porting that minimises class stubbing. Since our strategy is class-oriented and test-driven, we can ensure the structural equivalence of the ported system, along with the limited behavioural equivalence of each class. In order to extend this to integration-level equivalence, we exploit aspect-oriented programming to generate UML sequence diagrams, and we present a technique to compare such automatically-generated diagrams for equivalence. We demonstrate and evaluate our approach using a case study that involves porting an application from C++ to Java.", "num_citations": "4\n", "authors": ["1762"]}
{"title": "Generation strategies for test-suites of grammar-based software\n", "abstract": " The use of statement coverage has proved to be a useful metric when testing code with a test-suite. Similarly, the coverage of a grammar\u2019s rules is an effective metric when testing a parser. However when testing a whole parser front-end, it is not immediately obvious whether there is a correlation between rule coverage and underlying code coverage. We use a number of generation strategies to generate a series of test-suites. We apply these test-suites to keystone, a parser front-end for ISO C++ and offer empirical evidence to suggest which generation strategy offers the best coverage whilst using the least amount of test-cases.", "num_citations": "4\n", "authors": ["1762"]}
{"title": "Thue's 1914 paper: a translation\n", "abstract": " This paper includes notes to accompany a reading of Thue's 1914 paper \"Probleme uber Veranderungen von Zeichenreihen nach gegebenen Reglen\", along with a translation of that paper. Thue's 1914 paper is mainly famous for proving an early example of an undecidable problem, cited prominently by Post. However, Post's paper principally makes use of the definition of Thue systems, described on the first two pages of Thue's paper, and does not depend on the more specific results in the remainder of Thue's paper. A closer study of the remaining parts of that paper highlight a number of important themes in the history of computing: the transition from algebra to formal language theory, the analysis of the \"computational power\" (in a pre-1936 sense) of rules, and the development of algorithms to generate rule-sets.", "num_citations": "3\n", "authors": ["1762"]}
{"title": "Test case generation for programming language metamodels\n", "abstract": " One of the central themes in software language engineering is the specification of programming languages, and domain-specific languages, using a metamodel. One problem associated with the use of programming language metamodels, and metamodels in general, is determining whether or not they are correct. In this context, the question addressed by our research is: given a programming language metamodel, how can we generate an appropriate test suite to show that it is valid?", "num_citations": "3\n", "authors": ["1762"]}
{"title": "An Analysis of Basic Blocks within SPECjvm98 Applications\n", "abstract": " In this report the authors present the results of performing a quantitative analysis of basic blocks that are contained within SPECjvm98 applications. They firstly investigate the distribution of basic block sizes both statically and dynamically and then focus on frequently occurring basic blocks. They then present the findings on the workload differences between top ranked basic blocks and overall application. Finally the authors present the results from the investigation into the existence of a linear correlation between static and dynamic basic block frequencies", "num_citations": "3\n", "authors": ["1762"]}
{"title": "Specifying and verifying TCP/IP using Mixed Intuitionistic Linear Logic\n", "abstract": " In this article we present a specification of two of the fundamental communications protocols, namely TCP and IP, which form the basis of many distributed systems. The logical formalism used is Mixed Intuitionistic Linear Logic in order to use both commutative and non-commutative operators to model the concurrent and sequential processes in these protocols. Key properties of both protocols are proved.", "num_citations": "3\n", "authors": ["1762"]}
{"title": "Specifying and Verifying IP with Linear Logic\n", "abstract": " This paper presents a specification of the IP layer in linear logic and shows how linear logic can be used to prove some properties of this layer. Both the specification and the correctness proofs have been validated using the Coq proof assistant, via the authors' embedding of linear logic into this constructive framework.", "num_citations": "3\n", "authors": ["1762"]}
{"title": "Verifying SimpleGT transformations using an intermediate verification language\n", "abstract": " Previously, we have developed the VerMTLr framework that allows rapid verifier construction for relational model transformation languages. VerMTLr draws on the Boogie intermediate verification language to systematically design a modular and reusable verifier. It also includes a modular formalisation of EMFTVM bytecode to ensure verifier soundness. In this work, we will illustrate how to adapt VerMTLr to design a verifier for the SimpleGT graph transformation language, which allows us to soundly prove the correctness of graph transformations. An experiment with the Pacman game demonstrates the feasibility of our approach.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "Specifying coupling and cohesion metrics using OCL and Alloy\n", "abstract": " This report presents a MOF-compliant metamodel for calculating software metrics and demonstrates how it is used to generate a metrics tool that calculates coupling and cohesion metrics. We also describe a systematic approach to the analysis of MOF-compliant metamodels and illustrate the approach using the presented metamodel. In this approach, we express the metamodel using UML and OCL and harness existing automated tools in a framework that generates a Java implementation and an Alloy specification of the metamodel, and use this both to examine the metamodel constraints, and to generate instantiations of the metamodel. Moreover, we describe how the approach can be used to generate test data for any software based on a MOF-compliant metamodel. We extend our framework to support this approach and use it to generate a test suite for the metrics calculation tool that is based on our metamodel.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "A Formal Model of Forth Control Words in the Pi-Calculus\n", "abstract": " In this paper we develop a formal specification of aspects of the Forth programming language. We describe the operation of the Forth compiler as it translates Forth control words, dealing in particular with the interpretation of immediate words during compilation. Our goal here is to provide a basis for the study of safety properties of embedded systems, many of which are constructed using Forth or Forth-like languages. To this end we construct a model of the Forth compiler in the \u03c0-calculus, and have simulated its execution by animating this model using the Pict programming language.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "A dynamic comparison of the SPEC98 and Java Grande benchmark suites\n", "abstract": " Two of the most commonly used benchmark suites for Java Programs are the SPEC98 and Grande Forum benchmark suites. This research uses a Platform Independent Dynamic Analysis Technique to study these suites and quantify the significant similarities and differences in behaviour between the suites. Dynamic frequencies adduced include method execution divided into program, API and native categories. The most informative basis for measurement is shown to be percentages of executed bytecodes charged to each method, and results are reported for the API packages.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "Comparison of Bytecode and Stack Frame Usage by Eiffel and Java Programs in the Java Virtual Machine\n", "abstract": " Dynamic quantatative measurements Bytecode and Stack Frame Usage by Eiffel and Java Programs in the Java Virtual Machine are made. Two Eiffel programs are dynamically analysed while executing on the JVM, and the results compared with those from the Java Programs. The aim is to examine whether properties like instruction usage and stack frame size are properties of the Java programming language itself or are exhibited by Eiffel programs as well. Remarkably local_load, push_const and local_store always account for very close to 40% of instructions executed, a property of the Java Virtual Machine irrespective of the programming language compiler or compiler optimizations used.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "Applying software engineering techniques to parser design\n", "abstract": " In this paper we describe the development of a parser for the C# programming language. We outline the development process used, detail its application to the development of a C# parser and present a number of metrics that describe the parser\u2019s evolution. This paper presents and reinforces an argument for the application of software engineering techniques in the area of parser design. The development of a parser for the C# programming language is in itself important to software engineering, since parsers form the basis for tools such as metrics generators, refactoring tools, pretty-printers and reverse engineering tools.", "num_citations": "2\n", "authors": ["1762"]}
{"title": "Combining Event-B and CSP: An Institution Theoretic approach to Interoperability\n", "abstract": " In this paper we present a formal framework designed to facilitate interoperability between the Event-B specification language and the process algebra CSP. Our previous work used the theory of institutions to provide a mathematically sound framework for Event-B, and this enables interoperability with CSP, which has already been incorporated into the institutional framework. This paper outlines a comorphism relationship between the institutions for Event-B and CSP, leveraging existing tool-chains to facilitate verification. We compare our work to the combined formalism Event-BCSP and use a supporting example to illustrate the benefits of our approach.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "Discrete Structures Teaching: A Systematic Literature Review. Technical Report: NUIM-CS-TR-2010-01\n", "abstract": " The ACM curriculum defines Discrete Structures as foundational material for computer science; material that the ability to work with is necessary for many other areas of computer science. However a significant amount of uncertainty remains on how exactly to teach the subject to computer science undergraduates in a tangible way. This technical report presents a systematic literature review of the literature relevant to the teaching of discrete structures and evaluates the findings of the process. A categorisation of the results is described followed by an analysis of each category.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "REM4j-A framework for measuring the reverse engineering capability of UML CASE tools\n", "abstract": " Reverse Engineering is becoming increasingly important in the software development world today as many organizations are battling to understand and maintain old legacy systems. Today\u2019s software engineers have inherited these legacy systems which they may know little about yet have to maintain, extend and improve. Currently there is no framework or strategy that an organisation can use to determine which UML CASE tool to use. This paper sets down such a framework, to allow organisations to base their tool choice on this reliable framework. We present the REM4j tool, an automated tool, for benchmarking UML CASE tools, we then use REM4j to carry out one such evaluation with eleven UML CASE tools. This framework allows us to reach a conclusion as to which is the most accurate and reliable UML CASE tool.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "Specifying and verifying communications protocols using mixed intuitionistic linear logic\n", "abstract": " In this paper we present a technique for specifying and verifying communications protocols and demonstrate this approach by specifying and verifying two of the fundamental communications protocols, namely TCP and IP, which form the basis of many distributed systems. The logical formalism used is Mixed Intuitionistic Linear Logic in order to use both commutative and non-commutative operators to model the concurrent and sequential processes in these protocols. Key properties of both protocols are proved.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "C++ Compilers & ISO Conformance\n", "abstract": " Conformance to Standards is becoming recognized as one of the most important assurances compiler vendors can provide to programmers. Conformance enables code portability and wider use of a language and its libraries. However, establishing the conformance of a compiler is difficult\u2014especially for C++, which was slow to develop (with acceptance of a Standard occurring years after the language was introduced).", "num_citations": "1\n", "authors": ["1762"]}
{"title": "Four logics and a protocol\n", "abstract": " The Internet Protocol (IP) is the protocol used to provide connectionless communication between hosts connected to the Internet. It provides a basic internetworking service to transport protocols such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). These in turn provide both connection-oriented and connectionless services to applications such as file transfer (FTP) and WWW browsing. In this paper we present four separate specifications of the interface to the internetworking layer implemented by IP using four types of logic: classical, constructive, temporal and linear logic.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "Institutional approaches to programming language specification\n", "abstract": " Formal specification has become increasingly important in software engineering, both as a design tool, and as a basis for verified software design. Formal methods have long been in use in the field of programming language design and implementation, and many formalisms, in both the syntactic and semantic domains, have evolved for this purpose.  In this thesis we examine the possibilities of integrating specifications written in different formalisms used in the description of programming languages within a single framework. We suggest that the theory of institutions provides a suitable background for such integration, and we develop descriptions of several formalisms within this framework. While we do not merge the formalisms themselves, we see that it is possible to relate modules from specifications in each of them, and this is demonstrated in a small example.", "num_citations": "1\n", "authors": ["1762"]}
{"title": "Platform Independent Dynamic Java Virtual Machine Analysis: the Java Grande Forum Benchmark Suite\n", "abstract": " \u0421\u0432 \u0438 \u0437 \u0434 \u0434 \u0436 \u043b \u0434\u0436 \u0437 \u0432\u0438 \u0434\u0430 \u0438 \u0433\u0436\u0431 \u0432 \u0434 \u0432 \u0432\u0438 \u0432 \u0430\u043d\u0437 \u0437 \u0433 \u0438 \u043d\u0432 \u0431 \u0434\u0436\u0433\u040c\u0430 \u0437 \u0433 \u0422 \u043a \u0434\u0436\u0433 \u0436 \u0431\u0437 \u043b \u0432 \u043c \u0439\u0438 \u0432 \u0433\u0432 \u0438 \u0422 \u043a \u042e \u0436\u0438\u0439 \u0430 \u0425 \u0432 \u041a \u042c \u0422 \u043a \u0434\u0436\u0433 \u0436 \u0431\u0437 \u0437 \u0430 \u0438 \u0436 \u0438 \u0432 \u0436\u0433\u0431 \u0438 \u0422 \u043a \u0436 \u0432 \u0433\u0436\u0439\u0431 \u0432 \u0431 \u0436 \u0437\u0439 \u0438 \u0418 \u0432 \u040c\u043a \u040b \u0436 \u0432\u0438 \u0422 \u043a \u0419\u0438\u0433\u0419 \u043d\u0438 \u0433 \u0433\u0431\u0434 \u0430 \u0436\u0437 \u0436 \u0432 \u0430\u043d\u0437 \u041a \u042c \u0436 \u0437\u0439\u0430\u0438\u0437 \u0434\u0436 \u0437 \u0432\u0438 \u0437 \u0436 \u0438 \u043d\u0432 \u0431 \u0432\u0437\u0438\u0436\u0439 \u0438 \u0433\u0432 \u0439\u0437 \u0436 \u0419 \u0435\u0439 \u0432 \u0437\u0418 \u0437 \u043b \u0430\u0430 \u0437 \u0438 \u0437 \u043e \u0437 \u0433 \u0438 \u0430\u0433 \u0430 \u043a \u0436 \u0430 \u0418 \u0434 \u0436 \u0431 \u0438 \u0436 \u0432 \u0433\u0434 \u0436 \u0432 \u0437\u0438 \u0437 \u0439\u0436 \u0432 \u043c \u0439\u0438 \u0433\u0432 \u0433\u0432 \u0438 \u0422\u042e\u0425\u041a\u042c \u0437 \u0436 \u0437\u0439\u0430\u0438\u0437\u0418 \u0434\u0436 \u0437 \u0432\u0438 \u0432 \u0434 \u0438\u0439\u0436 \u0433 \u0438 \u0438\u0439 \u0430 \u0414\u0436 \u0438 \u0436 \u0438 \u0432 \u0434\u0436 \u0437\u0439\u0431 \u0415 \u043a \u0433\u0439\u0436 \u0433 \u0438 \u0422\u042e\u0425\u0418 \u043a \u0431\u0434\u0430 \u0438 \u0433\u0432\u0437 \u0433\u0438 \u0433\u0436 \u0438 \u0433\u043a \u0436 \u0437\u0434 \u0438\u0437 \u0433 \u0438 \u0422 \u043a \u0436 \u0432 \u0432 \u0431 \u0436 \u0437\u0439 \u0438 \u0437\u0418 \u0433\u0436 \u0438 \u0434 \u0436 \u0433\u0436\u0431 \u0432 \u0433 \u0438 \u0422 \u043a \u0419\u0438\u0433\u0419 \u043d\u0438 \u0433 \u0433\u0431\u0434 \u0430 \u0436\u0437\u0418 \u0432 \u0433\u0436 \u0438 \u0437 \u0432 \u0433 \u0438 \u0422\u042e\u0425\u041a", "num_citations": "1\n", "authors": ["1762"]}