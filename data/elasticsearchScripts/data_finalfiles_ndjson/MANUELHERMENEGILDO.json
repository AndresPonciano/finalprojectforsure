{"title": "Compile-time derivation of variable dependency using abstract interpretation\n", "abstract": " Traditional schemes for abstract interpretation-based global analysis of logic programs generally focus on obtaining procedure-argument mode and type information. Variable-sharing information is often given only the attention needed to preserve the correctness of the analysis. However, such sharing information can be very useful. In particular, it can be used for predicting run-time goal independence, which can eliminate costly run-time checks in AND-parallel execution. In this paper, a new algorithm for doing abstract interpretation in logic programs is described which concentrates on inferring the dependencies of the terms bound to program variables with increased precision and at all points in the execution of the program, rather than just at a procedure level. Algorithms are presented for computing abstract entry and success substitutions which extensively keep track of variable-aliasing and term-dependence\u00a0\u2026", "num_citations": "295\n", "authors": ["265"]}
{"title": "Parallel execution of prolog programs: a survey\n", "abstract": " Since the early days of logic programming, researchers in the field realized the potential for exploitation of parallelism present in the execution of logic programs. Their high-level nature, the presence of nondeterminism, and their referential transparency, among other characteristics, make logic programs interesting candidates for obtaining speedups through parallel execution. At the same time, the fact that the typical applications of logic programming frequently involve irregular computations, make heavy use of dynamic data structures with logical variables, and involve search and speculation, makes the techniques used in the corresponding parallelizing compilers and run-time systems potentially interesting even outside the field. The objective of this article is to provide a comprehensive survey of the issues arising in parallel execution of logic programming languages along with the most relevant approaches\u00a0\u2026", "num_citations": "282\n", "authors": ["265"]}
{"title": "Integrated program debugging, verification, and optimization using abstract interpretation (and the Ciao system preprocessor)\n", "abstract": " The technique of Abstract Interpretation has allowed the development of very sophisticated global program analyses which are at the same time provably correct and practical. We present in a tutorial fashion a novel program development framework which uses abstract interpretation as a fundamental tool. The framework uses modular, incremental abstract interpretation to obtain information about the program. This information is used to validate programs, to detect bugs with respect to partial specifications written using assertions (in the program itself and/or in system libraries), to generate and simplify run-time tests, and to perform high-level program transformations such as multiple abstract specialization, parallelization, and resource usage control, all in a provably correct way. In the case of validation and debugging, the assertions can refer to a variety of program points such as procedure entry, procedure exit\u00a0\u2026", "num_citations": "218\n", "authors": ["265"]}
{"title": "Combined Determination of Sharing and Freeness of Program Variables through Abstract Interpretation.\n", "abstract": " In this paper, abstract interpretation algorithms are described for computing the sharmg as well as the freeness information about the run-time instantiations of program variables. An abstract domain is proposed which accurately and concisely represents combined freeness and sharing information for program variables. Abstract unification and all other domain-specific functions for an abstract interpreter working on this domain are presented. These functions are illustrated with an example. The importance of inferring freeness is stressed by showing (1) the central role it plays in non-strict goal independence, and (2) the improved accuracy it brings to the analysis of sharing information when both are computed together. Conversely, it is shown that keeping accurate track of sharing allows more precise inference of freeness, thus resulting in an overall much more powerful abstract interpreter.", "num_citations": "203\n", "authors": ["265"]}
{"title": "An abstract machine for restricted AND-parallel execution of logic programs\n", "abstract": " Although the sequential execution speed of logic programs has been greatly improved by the concepts introduced in the Warren Abstract Machine (WAM), parallel execution represents the only way to increase this speed beyond the natural limits of sequential systems. However, most proposed parallel logic programming execution models lack the performance optimizations and storage efficiency of sequential systems. This paper presents a parallel abstract machine which is an extension of the WAM and is thus capable of supporting AND-Parallelism without giving up the optimizations present in sequential implementations. A suitable instruction set, which can be used as a target by a variety of logic programming languages, is also included. Special instructions are provided to support a generalized version of \"Restricted AND-Parallelism\" (RAP), a technique which reduces the overhead traditionally\u00a0\u2026", "num_citations": "183\n", "authors": ["265"]}
{"title": "Prolog and its performance: Exploiting independent And-Parallelism\n", "abstract": " Prolog and its performance: exploiting independent and-parallelism | Logic programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksLogic programmingProlog and its performance: exploiting independent and-parallelism chapter Prolog and its performance: exploiting independent and-parallelism Share on Authors: Manuel Victor Hermenegildo profile image MV Hermenegildo View Profile , Kevin John Greene profile image KJ Greene View Profile Authors Info & Affiliations Publication: Logic programmingMay 1990 Pages 253\u2013268 11citation 0 Downloads Metrics Total Citations11 Total Downloads0 Last 12 Months0 Last 6 0 ! \u2026", "num_citations": "175\n", "authors": ["265"]}
{"title": "An abstract machine based execution model for computer architecture design and efficient implementation of logic programs in parallel\n", "abstract": " The term\" Logic Programming\" refers to a variety of computer languages and execution models which are based on the traditional concept of Symbolic Logic. The expressive power of these languages offers promise to be of great assistance in facing the programming challenges of present and future symbolic processing applications in Artificial Intelligence, Knowledge-based systems, and many other areas of computing. The sequential execution speed of logic programs has been greatly improved since the advent of the first interpreters. However, higher inference speeds are still required in order to meet the demands of applications such as those contemplated for next generation computer systems. The execution of logic programs in parallel is currently considered a promising strategy for attaining such inference speeds. Logic Programming in turn appears as a suitable programming paradigm for parallel\u00a0\u2026", "num_citations": "175\n", "authors": ["265"]}
{"title": "An overview of Ciao and its design philosophy\n", "abstract": " We provide an overall description of the Ciao multiparadigm programming system emphasizing some of the novel aspects and motivations behind its design and implementation. An important aspect of Ciao is that, in addition to supporting logic programming (and, in particular, Prolog), it provides the programmer with a large number of useful features from different programming paradigms and styles and that the use of each of these features (including those of Prolog) can be turned on and off at will for each program module. Thus, a given module may be using, e.g., higher order functions and constraints, while another module may be using assignment, predicates, Prolog meta-programming, and concurrency. Furthermore, the language is designed to be extensible in a simple and modular way. Another important aspect of Ciao is its programming environment, which provides a powerful preprocessor (with an\u00a0\u2026", "num_citations": "173\n", "authors": ["265"]}
{"title": "Determination of Variable Dependence Information through Abstract Interpretation.\n", "abstract": " Traditional schemes for abstract interpretation-based global analysis of logic programs generally focus on obtaining procedure argument mode and type information. Variable sharing information is often given only the attention needed to preserve the correctness of the analysis. However, such sharing information can be very useful. In particular, it can be used for predicting run-time goal independence, which can eliminate costly run-time checks in and-parallel execution. In this paper, a new algorithm for doing abstract interpretation in logic programs is described which infers the dependencies of the terms bound to program variables with increased precision and at all points in the execution of the program, rather than just at a procedure level. Algorithms are presented for computing abstract entry and success substitutions which extensively keep track of variable aliasing and term dependence information. The algorithms are illustrated with examples.", "num_citations": "150\n", "authors": ["265"]}
{"title": "The &-Prolog system: Exploiting independent and-parallelism\n", "abstract": " The &-Prolog system, a practical implementation of a parallel execution model for Prolog exploiting strict and non-strict independent and-parallelism, is described. Both automatic and manual parallelization of programs are supported. This description includes a summary of the system\u2019s language and architecture, some details of its execution model (based on the RAP-WAM model), and data on its performance on sequential workstations and shared memory multiprocessors, which is compared to that of current Prolog systems. The results to date show significant speed advantages over state-of-the-art sequential systems.", "num_citations": "147\n", "authors": ["265"]}
{"title": "Improving abstract interpretations by combining domains\n", "abstract": " This article considers static analysis based on abstract interpretation of logic programs over combined domains. It is known that analyses over combined domains provide more information potentially than obtained by the independent analyses. However, the construction of a combined analysis often requires redefining the basic operations for the combined domain. A practical approach to maintain precision in combined analyses of logic programs which reuses the individual analyses and does not redefine the basic operations is illustrated. The advantages of the approach are that (1) proofs of correctness for the new domains are not required and (2) implementations can be reused. The approach is demonstrated by showing that a combined sharing analysis\u2014constructed from \u201cold\u201d proposals\u2014compares well with other \u201cnew\u201d proposals suggested in recent literature both from the point of view of efficiency and\u00a0\u2026", "num_citations": "144\n", "authors": ["265"]}
{"title": "An assertion language for constraint logic programs\n", "abstract": " In an advanced program development environment, such as that discussed in the introduction of this book, several tools may coexist which handle both the program and information on the program in different ways. Also, these tools may interact among themselves and with the user. Thus, the different tools and the user need some way to communicate. It is our design principle that such communication be performed in terms of assertions. Assertions are syntactic objects which allow expressing properties of programs. Several assertion languages have been used in the past in different contexts, mainly related to program debugging. In this chapter we propose a general language of assertions which is used in different tools for validation and debugging of constraint logic programs in the context of the DiSCiPl project. The assertion language proposed is parametric w.r.t. the particular constraint domain and\u00a0\u2026", "num_citations": "135\n", "authors": ["265"]}
{"title": "On the Role of Semantic Approximations on Validation and Diagnosis of Contraint Logic Programs.\n", "abstract": " This paper presents some on-going work in the ESPRIT project DiSCiPl. The project aims at devising advanced tools for debugging of constraint logic programs. A central problem in program development is obtaining a program which satisfies the user's expectations. When considering a given program, a natural question is then whether or not it fulfils expectations of some kind (requirements). To be able to formulate this question, some formal or informal way of specifying such requirements is needed. That is, a (formal or informal) program semantics is needed, in which what the program computes and what it is required to compute can be expressed.It may then be possible either to verify that the program satisfies the requirement for every computation (in the considered class), or to show a specific computation where the requirement is violated. The process of identifying the part of the program responsible for the violation is referred to as diagnosis. The program then needs to be modified to correct the error. Since the requirement documentation is often not complete, the user's requirements are often given as approximations, ie, safe specifications of (parts of) the intended semantics of the program. The process of debugging consists of the study of the program semantics, observation of error symptoms, localization of program\\errors\" and their correction until no symptom can be observed anymore and the program is considered correct.", "num_citations": "126\n", "authors": ["265"]}
{"title": "User-definable resource bounds analysis for logic programs\n", "abstract": " We present a static analysis that infers both upper and lower bounds on the usage that a logic program makes of a set of user-definable resources. The inferred bounds will in general be functions of input data sizes. A resource in our approach is a quite general, user-defined notion which associates a basic cost function with elementary operations. The analysis then derives the related (upper- and lower-bound) resource usage functions for all predicates in the program. We also present an assertion language which is used to define both such resources and resource-related properties that the system can then check based on the results of the analysis. We have performed some preliminary experiments with some concrete resources such as execution steps, bytes sent or received by an application, number of files left open, number of accesses to a database, number of calls to a procedure, number of\u00a0\u2026", "num_citations": "114\n", "authors": ["265"]}
{"title": "Strict and nonstrict independent and-parallelism in logic programs: Correctness, efficiency, and compile-time conditions\n", "abstract": " This paper presents some fundamental properties of independent and- parallelism and extends its applicability by enlarging the class of goals eligible for parallel execution. A simple model of (independent) and-parallel execution is proposed and issues of correctness and efficiency are discussed in the light of this model. Two conditions, \u201cstrict\u201d and \u201cnonstrict\u201d independence, are defined and then proved sufficient to ensure correctness and efficiency of parallel execution: If goals which meet these conditions are executed in parallel, the solutions obtained are the same as those produced by standard sequential execution. Also, in the absence of failure, the parallel proof procedure does not generate any additional work (with respect to standard SLD resolution), while the actual execution time is reduced. Finally, in case of failure of any of the goals, no slowdown will occur. For strict independence, the results are shown\u00a0\u2026", "num_citations": "113\n", "authors": ["265"]}
{"title": "ACE: And/Or-parallel copying-based execution of logic programs\n", "abstract": " The stack copying approach, as exemplified by the MUSE system, has been shown to be a quite successful alternative for representing multiple environments during or-parallel execution of logic programs. In this paper we present a novel execution model for parallel implementation of logic programs which is capable of exploiting both independent and-parallelism and or-parallelism in an efficient way and which combines the stack copying approach in the implementation of or-parallelism with proven techniques in the implementation of independent and-parallelism, such as those used in &-Prolog. The and parallelism is expressed via &-Prolog's conditional graph expressions. We show how all solutions to non-deterministic and-parallel goals are found without repetitions. This is done through recomputation as in Prolog (and &-Prolog), i.e., solutions of and-parallel goals are not shared. We also propose a\u00a0\u2026", "num_citations": "112\n", "authors": ["265"]}
{"title": "The Ciao prolog system\n", "abstract": " This is the Reference Manual for the Ciao Prolog development system. It contains basic information on how to install Ciao Prolog and how to write, debug, and run Ciao Prolog programs from the command line, from inside GNU emacs, or from a windowing desktop. It also documents all the libraries available in the standard distribution. This manual has been generated using the LPdoc semi-automatic documentation generator for LP/CLP programs [HC97, Her00]. lpdoc processes Prolog files (and files in other CLP languages) adorned with assertions and machine-readable comments, which should be written in the Ciao assertion language [PBH97, PBH00]. From these, it generates manuals in many formats including postscript, pdf, texinfo, info, HTML, man, etc., as well as on-line help, ascii README files, entries for indices of manuals (info, WWW,...), and maintains WWW distribution sites.The big advantage of this approach is that it is easier to keep the on-line and printed documentation in sync with the source code [Knu84]. As a result, this manual changes continually as the source code is modified. Because of this, the manual has a version number. You should make sure the manual you are reading, whether it be printed or on-line, coincides with the version of the software that you are using.", "num_citations": "110\n", "authors": ["265"]}
{"title": "The CIAO Multi-Dialect Compiler and System: An Experimentation Workbench for Future (C) LP Systems.\n", "abstract": " CIAO is an advanced programming environment supporting Logic and Constraint programming. It offers a simple concurrent kernel on top of which declarative and non-declarative extensions are added via librarles. Librarles are available for supporting the ISO-Prolog standard, several constraint domains, functional and higher order programming, concurrent and distributed programming, internet programming, and others. The source language allows declaring properties of predicates via assertions, including types and modes. Such properties are checked at compile-time or at run-time. The compiler and system architecture are designed to natively support modular global analysis, with the two objectives of proving properties in assertions and performing program optimizations, including transparently exploiting parallelism in programs. The purpose of this paper is to report on recent progress made in the context of the CIAO system, with special emphasis on the capabilities of the compiler, the techniques used for supporting such capabilities, and the results in the \u00e1reas of program analysis and transformation already obtained with the system.", "num_citations": "104\n", "authors": ["265"]}
{"title": "Using global analysis, partial specifications, and an extensible assertion language for program validation and debugging\n", "abstract": " We present a framework for the application of abstract interpretation as an aid during program development, rather than in the more traditional application of program optimization. Program validation and detection of errors is first performed statically by comparing (partial) specifications written in terms of assertions against information obtained from static analysis of the program. The results of this process are expressed in the user assertion language. Assertions (or parts of assertions) which cannot be verified statically are translated into run-time tests. The framework allows the use of assertions to be optional. It also allows using very general properties in assertions, beyond the predefined set understandable by the static analyzer and including properties defined by means of user programs. We also report briefly on an implementation of the framework. The resulting tool generates and checks assertions for\u00a0\u2026", "num_citations": "98\n", "authors": ["265"]}
{"title": "Effectiveness of Global Analysis in Strict Independence-Based Automatic Parallelization.\n", "abstract": " This paper presents a study of the effectiveness of global analysis in the parallelization of logic programs using strict independence. A number of well-known approximation domains are selected and tlieir usefulness for the application in hand is explained. Also, methods for using the information provided by such domains to improve parallelization are proposed. Local and global analyses are built using these domains and such analyses are embedded in a complete parallelizing compiler. Then, the performance of the domains (and the system in general) is assessed for this application through a number of experiments. We arg\u00fce that the results offer significant insight into the characteristics of these domains, the demands of the application, and the tradeoffs involved.", "num_citations": "97\n", "authors": ["265"]}
{"title": "Incremental analysis of constraint logic programs\n", "abstract": " Global analyzers traditionally read and analyze the entire program at  once, in a nonincremental way. However, there are many situations which are not well suited to this simple model and which instead require reanalysis of certain parts of a program which has already been analyzed. In these cases, it appears inefficient to perform the analysis of the program again from scratch, as needs to be done with current systems. We describe how the fixed-point algorithms used in current generic analysis engines for (constraint) logic programming languages can be extended to support incremental analysis. The possible changes to a program are classified into three types: addition, deletion, and arbitrary change. For each one of these, we provide one or more algorithms for identifying the parts  of the analysis that must be recomputed and for performing the actual recomputation. The potential benefits and drawbacks of\u00a0\u2026", "num_citations": "96\n", "authors": ["265"]}
{"title": "A new module system for Prolog\n", "abstract": " It is now widely accepted that separating programs into modules is useful in program development and maintenance. While many Prolog implementations include useful module systems, we argue that these systems can be improved in a number of ways, such as, for example, being more amenable to effective global analysis and transformation and allowing separate compilation or sensible creation of standalone executables. We discuss a number of issues related to the design of such an improved module system for Prolog and propose some novel solutions. Based on this, we present the choices made in the Ciao module system, which has been designed to meet a number of objectives: allowing separate compilation, extensibility in features and in syntax, amenability to modular global analysis and transformation, enhanced error detection, support for meta-programming and higher-order, compatibility to\u00a0\u2026", "num_citations": "91\n", "authors": ["265"]}
{"title": "Efficient Management of Backtracking in AND-parallelism\n", "abstract": " A backtracking algorithm for AND-Parallelism and its implementation at the Abstract Machine level are presented: first, a class of AND-Parallelism models based on goal independence is defined, and a generalized version of Restricted AND-Parallelism (RAP) introduced as characteristic of this class. A simple and efficient backtracking algorithm for RAP is then discussed. An implementation scheme is presented for this algorithm which offers minimum overhead, while retaining the performance and storage economy of sequential implementations and taking advantage of goal independence to avoid unnecessary backtracking (\"restricted intelligent backtracking\"). Finally, the implementation of backtracking in sequential and AND-Parallel systems is explained through a number of examples.", "num_citations": "90\n", "authors": ["265"]}
{"title": "Combined static and dynamic assertion-based debugging of constraint logic programs\n", "abstract": " We propose a general framework for assertion-based debugging of constraint logic programs. Assertions are linguistic constructions for expressing properties of programs. We define several assertion schemas for writing (partial) specifications for constraint logic programs using quite general properties, including user-defined programs. The framework is aimed at detecting deviations of the program behavior (symptoms) with respect to the given assertions, either at compile-time (i.e., statically) or run-time (i.e., dynamically). We provide techniques for using information from global analysis both to detect at compile-time assertions which do not hold in at least one of the possible executions (i.e., static symptoms) and assertions which hold for all possible executions (i.e., statically proved assertions). We also provide program transformations which introduce tests in the program for checking at run-time those\u00a0\u2026", "num_citations": "89\n", "authors": ["265"]}
{"title": "A flexible,(C) LP-based approach to the analysis of object-oriented programs\n", "abstract": " Static analyses of object-oriented programs usually rely on intermediate representations that respect the original semantics while having a more uniform and basic syntax. Most of the work involving object-oriented languages and abstract interpretation usually omits the description of that language or just refers to the Control Flow Graph (CFG) it represents. However, this lack of formalization on one hand results in an absence of assurances regarding the correctness of the transformation and on the other it typically strongly couples the analysis to the source language. In this work we present a framework for analysis of object-oriented languages in which in a first phase we transform the input program into a representation based on Horn clauses. This facilitates on one hand proving the correctness of the transformation attending to a simple condition and on the other allows applying existing analyzers for\u00a0\u2026", "num_citations": "88\n", "authors": ["265"]}
{"title": "Energy consumption analysis of programs based on XMOS ISA-level models\n", "abstract": " Energy consumption analysis of embedded programs requires the analysis of low-level program representations. This is challenging because the gap between the high-level program structure and the low-level energy models needs to be bridged. Here, we describe techniques for recreating the structure of low-level programs and transforming these into Horn clauses in order to make use of a generic resource analysis framework (CiaoPP). Our analysis, which makes use of an energy model we produce for the underlying hardware, characterises the energy consumption of the program, and returns energy formulae parametrised by the size of the input data. We have performed an initial experimental assessment and obtained encouraging results when comparing the statically inferred formulae to direct energy measurements from the hardware running a set of benchmarks. Static energy estimation has\u00a0\u2026", "num_citations": "81\n", "authors": ["265"]}
{"title": "Abstraction-carrying code\n", "abstract": " Proof-Carrying Code (PCC) is a general approach to mobile code safety in which programs are augmented with a certificate (or proof). The practical uptake of PCC greatly depends on the existence of a variety of enabling technologies which allow both to prove programs correct and to replace a costly verification process by an efficient checking procedure on the consumer side. In this work we propose Abstraction-Carrying Code (ACC), a novel approach which uses abstract interpretation as enabling technology. We argue that the large body of applications of abstract interpretation to program verification is amenable to the overall PCC scheme. In particular, we rely on an expressive class of safety policies which can be defined over different abstract domains. We use an abstraction (or abstract model) of the program computed by standard static analyzers as a certificate. The validity of the abstraction\u00a0\u2026", "num_citations": "81\n", "authors": ["265"]}
{"title": "Global analysis of constraint logic programs\n", "abstract": " This article presents and illustrates a practical approach to the data ow analysis of constraint logic programming languages using abstract interpretation. It is rst argued that, from the framework point of view, it su ces to propose relatively simple extensions of traditional analysis methods which have already been proved useful and practical and for which e cient xpoint algorithms exist. This is shown by proposing a simple extension of Bruynooghe's traditional framework which allows it to analyze constraint logic programs. Then, and using this generalized framework, two abstract domains and their required abstract functions are presented: the rst abstract domain approximates de niteness information and the second one freeness. Finally, an approach for combining those domains is proposed. The two domains and their combination have been implemented and used in the analysis of CLP (<) and Prolog-III applications. Results from this implementation showing its performance and accuracy are also presented.", "num_citations": "81\n", "authors": ["265"]}
{"title": "The pillow/ciao library for internet/www programming using computational logic systems\n", "abstract": " We discuss from a practical point of view a number of issues involved in writing Internet and WWW applications using LP/CLP systems. We describe PiLLoW, an Internet and WWW programming library for LP/CLP systems which we argue signi cantly simpli es the process of writing such applications. PiLLoW provides facilities for generating HTML structured documents, producing HTML forms, writing form handlers, accessing and parsing WWW documents, and accessing code posted at HTTP addresses. We also describe the architecture of some application classes, using a high-level model of client-server interaction, active modules. Finally we describe an architecture for automatic LP/CLP code downloading for local execution, using generic browsers. The PiLLoW library has been developed in the context of the &-Prolog and CIAO systems, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality.", "num_citations": "79\n", "authors": ["265"]}
{"title": "Global analysis of standard Prolog programs\n", "abstract": " Abstract interpretation-based data-flow analysis of logic programs is, at this point, relatively well understood from the point of view of general frameworks and abstract domains. On the other hand, comparatively little attention has been given to the problems which arise when analysis of a full, practical dialect of the Prolog language is attempted, and only few solutions to these problems have been proposed to date. Existing proposals generally restrict in one way or another the classes of programs which can be analyzed. This paper attempts to fill this gap by considering a full dialect of Prolog, essentially the recent ISO standard, pointing out the problems that may arise in the analysis of such a dialect, and proposing a combination of known and novel solutions that together allow the correct analysis of arbitrary programs which use the full power of the language.", "num_citations": "74\n", "authors": ["265"]}
{"title": "Distributed WWW programming using (Ciao-) Prolog and the PiLLoW library\n", "abstract": " We discuss from a practical point of view a number of issues involved in writing distributed Internet and WWW applications using LP/CLP systems. We describe PiLLoW, a public-domain Internet and WWW programming library for LP/CLP systems that we have designed to simplify the process of writing such applications. PiLLoW provides facilities for accessing documents and code on the WWW; parsing, manipulating and generating HTML and XML structured documents and data; producing HTML forms; writing form handlers and CGI-scripts; and processing HTML/XML templates. An important contribution of PiLLoW is to model HTML/XML code (and, thus, the content of WWW pages) as terms. The PiLLoW library has been developed in the context of the Ciao Prolog system, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality. We also describe the use of concurrency\u00a0\u2026", "num_citations": "73\n", "authors": ["265"]}
{"title": "The DCG, UDG, and MEL Methods for Automatic Compile-time Parallelization of Logic Programs for Independent And-parallelism.\n", "abstract": " There has been significant interest in parallel execution models for logic programs which exploit Independent And-Parallelism (IAP). In these models, it is necessary to determine which goals are independent and therefore eligible for parallel execution and which goals have to wait for which others during execution. Although this can be done at run-time, it can imply a very heavy overhead. In this paper, we present three algorithms for automatic compiletime parallelization of logic programs using IAP. This is done by converting a clause into a graph-based computational form and then transforming this graph into linear expressions based on &-Prolog, a language for IAP. We also present an algorithm which, given a clause, determines if there is any loss of parallelism due to linearization, for the case in which only unconditional parallelism is desired. Finally, the performance of these annotation algorithms is discussed for some benchmark programs.", "num_citations": "73\n", "authors": ["265"]}
{"title": "Analysis and visualization tools for constraint programming: constraint debugging\n", "abstract": " Coordinating production across a supply chain, designing a new VLSI chip, allocating classrooms or scheduling maintenance crews at an airport are just a few examples of complex (combinatorial) problems that can be modeled as a set of decision variables whose values are subject to a set of constraints. The decision variables may be the time when production of a particular lot will start or the plane that a maintenance crew will be working on at a given time. Constraints may range from the number of students you can? t in a given classroom to the time it takes to transfer a lot from one plant to another. Despiteadvancesincomputingpower, manyformsoftheseandother combinatorial problems have continued to defy conventional programming approaches. Constraint Logic Programming (CLP)? rst emerged in the mid-eighties as a programming technique with the potential of signi? cantly reducing the time it takes to develop practical solutions to many of these problems, by combining the expressiveness of languages such as Prolog with the compu-tional power of constrained search. While the roots of CLP can be traced to Monash University in Australia, it is without any doubt in Europe that this new software technology has gained the most prominence, bene? ting, among other things, from sustained funding from both industry and public R&D programs over the past dozen years. These investments have already paid o?, resulting in a number of popular commercial solutions as well as the creation of several successful European startups.", "num_citations": "72\n", "authors": ["265"]}
{"title": "Some methodological issues in the design of CIAO, a generic, parallel concurrent constraint logic programming system\n", "abstract": " We informally discuss several issues related to the parallel execution of logic programming systems and concurrent logic programming systems, and their generalization to constraint programming. We propose a new view of these systems, based on a particular definition of parallelism. We argue that, under this view, a large number of the actual systems and models can be explained through the application, at different levels of granularity, of only a few basic principles: determinism, non-failure, independence (also referred to as stability), granularity, etc. Also, and based on the convergence of concepts that this view brings, we sketch a model for the implementation of several parallel constraint logic programming source languages and models based on a common, generic abstract machine and an intermediate kernel language.", "num_citations": "70\n", "authors": ["265"]}
{"title": "Concurrency in Prolog Using Threads and a Shared Database.\n", "abstract": " Concurrency in Logic Programming has received much attention in the past. One problem with many proposals, when applied to Prolog, is that they involve large modifications to the standard implementations, and/or the communication and synchronization facilities provided do not fit as naturally within the language model as we feel is possible. In this paper we propose a new mechanism for implementing synchronization and communication for concurrency, based on atomic accesses to designated facts in the (shared) dat\u00e1base. We arg\u00fce that this model is comparatively easy to implement and harmonizes better than previous proposals within the Prolog control model and standard set of built-ins. We show how in the proposed model it is easy to express classical concurrency algorithms and to subsume other mechanisms such as Linda, variable-based communication, or classical parallelism-oriented primitives. We also report on an implementation of the model and provide performance and resource consumption data.", "num_citations": "68\n", "authors": ["265"]}
{"title": "&ACE: A high-performance parallel Prolog system\n", "abstract": " In recent years a lot of research has been invested in parallel processing of numerical applications. However, parallel processing of Symbolic and AI applications has received less attention. This paper presents a system for parallel symbolic computing, named ACE, based on the logic programming paradigm. ACE is a computational model for the full Prolog language, capable of exploiting Or-parallelism and Independent And-parallelism. In this paper we focus on the implementation of the and-parallel part of the ACE system (called &ACE) on a shared memory multiprocessor, describing its organization, some optimizations, and presenting some performance figures, proving the ability of &ACE to efficiently exploit parallelism.< >", "num_citations": "68\n", "authors": ["265"]}
{"title": "Effectivness of abstract interpretation in automatic parallelization: a case study in logic programming\n", "abstract": " We report on a detailed study of the application and effectiveness of program analysis based on abstract interpretation of automatic program parallelization. We study the case of parallelizing logic programs using the notion of strict independence. We first propose and prove correct a methodology for the application in the parallelization task of the information inferred by abstract interpretation, using a parametric domain. The methodology is generic in the sense of allowing the use of different analysis domains. A number of well-known approximation  domains are then studied and the transformation into the parametric domain defined. The transformation directly illustrates the revelance and applicability of each abstract domain for the application. Both local and global analyzers are  then built using these domains and embedded in a complete parallelizing compiler. Then, the performance of the domains in this context\u00a0\u2026", "num_citations": "66\n", "authors": ["265"]}
{"title": "Program Analysis, Debugging, and Optimization Using the Ciao System Preprocessor.\n", "abstract": " We present a tutorial overview of Ciaopp, the Ciao system preprocessor. Ciao is a public-domain, next-generation logic programming system, which subsumes ISO-Prolog and is specifically designed to a) be highly extensible via libraries and b) support modular program analysis, debugging, and optimization. The latter tasks are performed in an integrated fashion by Ciaopp. Ciaopp uses modular, incremental abstract interpretation to infer properties of program predicates and literals, including types, variable instantiation properties (including modes), non-failure, determinacy, bounds on computational cost, bounds on sizes of terms in the program, etc. Using such analysis information, Ciaopp can find errors at compile-time in programs and/or perform partial verification. Ciaopp checks how programs call system libraries and also any assertions present in the program or in other modules used by the program. These assertions are also used to generate documentation automatically. Ciaopp also uses analysis information to perform program transformations and optimizations such as multiple abstract specialization, parallelization (including granularity control), and optimization of run-time tests for properties which cannot be checked completely at compile-time. We illustrate\" hands-on\" the use of Ciaopp in all these tasks. By design, Ciaopp is a generic tool, which can be easily tailored to perform these and other tasks for different LP and CLP dialects.", "num_citations": "65\n", "authors": ["265"]}
{"title": "On the Correctness and Efficiency of Independent And-Parallelism in Logic Programs.\n", "abstract": " This paper presents and proves some fundamental results for independent and-parallelism (IAP). First, the paper treats the issues of correctness and efficiency: after defining strict and non-strict goal independence, it is proved that if strictly independent goals are executed in parallel the solutions obtained are the same as those produced by standard sequential execution. It is also shown that, in the absence of failure, the parallel proof procedure doesn't gen\u00e9rate any additional work (with respect to standard SLD-resolution) while the actual execution time is reduced. The same results hold even if non-strictly independent goals are executed in parallel, provided a trivial rewriting of such goals is performed. In addition, and most importantly, treats the issue of compile-time generation of IAP by proposing conditions, to be written at compile-time, to efficiently check strict and non-strict goal independence at run-time and proving the sufficiency of such conditions. It is also shown how simpler conditions can be constructed if some information regarding the binding context of the goals to be executed in parallel is available to the compiler trough either local or program-level analysis. These results therefore provide a formal basis for the automatic compile-time generation of IAP. As a corollary of such results, the paper also proves that negative goals are always non-strictly independent, and that goals which share a first occurrence of an existential variable are never independent.", "num_citations": "65\n", "authors": ["265"]}
{"title": "Analyzing logic programs with dynamic scheduling\n", "abstract": " Traditional logic programming languages, such as Prolog, use a fixed left-to-right atom scheduling rule. Recent logic programming languages, however, usually provide more flexible scheduling in which computation generally proceed left-to-right but in which some calls are dynamically \u201cdelayed\u201d until their arguments are sufficiently instantiated to allow the call to run efficiently. Such dynamic scheduling has a significant cost. We give a framework for the global analysis of logic programming languages with dynamic scheduling and show that program analysis based on this framework supports optimizations which remove much of the overhead of dynamic scheduling.", "num_citations": "64\n", "authors": ["265"]}
{"title": "Non-strict independent and-parallelism\n", "abstract": " This paper presents and develops a generalized concept of Non-Strict Independent And Parallelism (NSIAP). NSIAP extends the applicability of Independent And-Parallelism (IAP) by enlarging the class of goals which are eligible for parallel execution. At the same time it maintains IAP's ability to run non-deterministic goals in parallel and to preserve the computational complexity expected in the execution of the program by the programmer. First, a parallel execution framework is defined and some fundamental correctness results, in the sense of equivalence of solutions with the sequential model, are discussed for this framework. The issue of efficiency is then considered. Two new definitions of NSI are given for the cases of pur\u00e9 and impure goals respectively and efficiency results are provided for programs parallelized under these definitions which include treatment of the case of goal failure: not only is reduction of execution time guaranteed (modulo run-time overheads) in the absence of failure but it is also shown that in the worst case of failure no speed-down will occur. In addition to applying to NSI, these results carry over and complete previous results shown in the context of IAP which did not deal with the case of goal failure. Finally, some practical examples of the application of the NSIAP concept to the parallelization of a set of programs are presented and performance results, showing the advantage of using NSI, are given.", "num_citations": "64\n", "authors": ["265"]}
{"title": "IDIOM: Integrating Dependent and-, Independent and-, and Or-parallelism\n", "abstract": " Independent and-parallelism, dependent and-parallelism and or-parallelism are the three main forms of implicit parallelism present in logic programs. In this paper we present a model, IDIOM, which exploits all three forms of parallelism in a single framework. IDIOM is based on a combination of the Basic Andorra Model and the Extended And-Or Tree Model. Our model supports both Prolog as well as the flat concurrent logic languages. We discuss the issues that arise in combining the three forms of parallelism, and our solutions to them. We also present an implementation scheme, based on binding arrays, for implementing IDIOM.", "num_citations": "62\n", "authors": ["265"]}
{"title": "Deriving a fixpoint computation algorithm for top-down abstract interpretation of logic programs\n", "abstract": " Bruynooghe described a framework for the top-down abstract interpretation of logic programs. In this framework, abstract interpretation is carried out by constructing an abstract and-or tree in a top-down fashion for a given query and program. Such an abstract interpreter requires fixpoint computation for programs which contain recursive predicates. This paper presents in detail a fixpoint algorithm that has been developed for this purpose and the motivation behind it. We start off by describing a simple-minded algorithm. After pointing out its shortcomings, we present a series of refinements to this algorithm, until we reach the final version. The aim is to give an intuitive grasp and provide justification for the relative complexity of the final algorithm. We also present an informal proof of correctness of the algorithm and some results obtained from an implementation.", "num_citations": "60\n", "authors": ["265"]}
{"title": "Constraint-based runtime prediction of SLA violations in service orchestrations\n", "abstract": " Service compositions put together loosely-coupled component services to perform more complex, higher level, or cross-organizational tasks in a platform-independent manner. Quality-of-Service (QoS) properties, such as execution time, availability, or cost, are critical for their usability, and permissible boundaries for their values are defined in Service Level Agreements (SLAs). We propose a method whereby constraints that model SLA conformance and violation are derived at any given point of the execution of a service composition. These constraints are generated using the structure of the composition and properties of the component services, which can be either known or empirically measured. Violation of these constraints means that the corresponding scenario is unfeasible, while satisfaction gives values for the constrained variables (start / end times for activities, or number of loop iterations) which\u00a0\u2026", "num_citations": "59\n", "authors": ["265"]}
{"title": "Lock-free parallel dynamic programming\n", "abstract": " We show a method for parallelizing top down dynamic programs in a straightforward way by a careful choice of a lock-free shared hash table implementation and randomization of the order in which the dynamic program computes its subproblems. This generic approach is applied to dynamic programs for knapsack, shortest paths, and RNA structure alignment, as well as to a state-of-the-art solution for minimizing the maximum number of open stacks. Experimental results are provided on three different modern multicore architectures which show that this parallelization is effective and reasonably scalable. In particular, we obtain over 10 times speedup for 32 threads on the open stacks problem.", "num_citations": "58\n", "authors": ["265"]}
{"title": "Integrating software testing and run-time checking in an assertion verification framework\n", "abstract": " We present a framework that unifies unit testing and run-time verification (as well as static verification and static debugging). A key contribution of our overall approach is that we preserve the use of a unified assertion language for all of these tasks. We first describe a method for compiling run-time checks for (parts of) assertions which cannot be verified at compile-time via program transformation. This transformation allows checking preconditions and postconditions, including conditional postconditions, properties at arbitrary program points, and certain computational properties. Most importantly, we propose a minimal addition to the assertion language which allows defining unit tests to be run in order to detect possible violations of the (partial) specifications expressed by the assertions. We have implemented the framework within the Ciao/CiaoPP\u00a0system and effectively applied it to the verification of ISO\u00a0\u2026", "num_citations": "58\n", "authors": ["265"]}
{"title": "Abstract multiple specialization and its application to program parallelization\n", "abstract": " Program specialization optimizes programs for known values of the input. It is often the case that the set of possible input values is unknown, or this set is infinite. However, a form of specialization can still be performed in such cases by means of abstract interpretation, specialization then being with respect to abstract values (substitutions), rather than concrete ones. We study the multiple specialization of logic programs based on abstract interpretation. This involves in principle, and based on information from global analysis, generating several versions of a program predicate for different uses of such predicate, optimizing these versions, and, finally, producing a new, \u201cmultiply specialized\u201d program. While multiple specialization has received theoretical attention, little previous evidence exists on its practicality. In this paper, we report on the incorporation of multiple specialization in a parallelizing compiler and quantify\u00a0\u2026", "num_citations": "58\n", "authors": ["265"]}
{"title": "Optimized algorithms for incremental analysis of logic programs\n", "abstract": " Global analysis of logic programs can be performed effectively by the use of one of several existing efficient algorithms. However, the traditional global analysis scheme in which all the program code is known in advance and no previous analysis information is available is unsatisfactory in many situations. Incremental analysis of logic programs has been shown to be feasible and much more efficient in certain contexts than traditional (non-incremental) global analysis. However, incremental analysis poses additional requirements on the fixpoint algorithm used. In this work we identify these requirements, present an important class of strategies meeting the requirements, present sufficient a priori conditions for such strategies, and propose, implement, and evaluate experimentally a novel algorithm for incremental analysis based on these ideas. The experimental results show that the proposed algorithm\u00a0\u2026", "num_citations": "57\n", "authors": ["265"]}
{"title": "Implementation of multiple specialization in logic programs\n", "abstract": " We study the multiple specialization of logic programs based on abstract interpretation. This involves in general generating several versions of a program predicate for different uses of such predicate, making use of information obtained from global analysis performed by an abstract interpreter, and finally producing a new,\u2018~ multiply specialized\u201d program. While the topic of multiple specialization of logic programs has received considerable theoretical attention, it has never been actually incorporated in a compiler and its effects quantified. We perform such a study in the context of a parallelizing compiler and show that it is indeed a relevant technique in practice. Also, we propose an implementation technique which has the same power as the strongest of the previously proposed techniques but requires little or no modification of an existing abstract interpreter.", "num_citations": "55\n", "authors": ["265"]}
{"title": "Incremental analysis of logic programs\n", "abstract": " Global analyzers traditionally read and analyze the entire program at once, in a non-incremental way. However, there are many situations which are not well suited to this simple model and which instead require reanalysis of certain parts of a program which has already been analyzed. In these cases, it appears inefficient to perform the analysis of the program again from scratch, as needs to be done with current systems. We describe how the fixpoint algorithms in current generic analysis engines can be extended to support incremental analysis. The possible changes to a program are classified into three types: addition, deletion, and arbitrary change. For each one of these, we provide one or more algorithms for identifying the parts of the analysis that must be recomputed and for performing the actual recomputation. The potential benefits and drawbacks of these algorithms are discussed. Finally, we present some experimental results obtained with an implementation of the algorithms in the PLAI generic abstract interpretation framework. The results show significant benefits when using the proposed incremental analysis algorithms.", "num_citations": "55\n", "authors": ["265"]}
{"title": "Independence in Constraint Logic Programs.\n", "abstract": " Studying independence of literals, variables, and substitutions has proven very useful in the context of logic programming (LP). Here we study independence in the broader context of constraint logic programming (CLP). We show that a naive extrapolation of the LP de nitions of independence to CLP is unsatisfactory (in fact, wrong) for two reasons. First, because interaction between variables through constraints is more complex than in the case of logic programming. Second, in order to ensure the e ciency of several optimizations not only must independence of the search space be considered, but also an orthogonal issue {\\independence of constraint solving.\" We clarify these issues by proposing various types of search independence and constraint solver independence, and show how they can be combined to allow di erent independence-related optimizations, from parallelism to intelligent backtracking. Su cient conditions for independence which can be evaluated\\a-priori\" at run-time are also proposed. Our results suggest that independence, provided a suitable de nition is chosen, is even more useful in CLP than in LP.", "num_citations": "55\n", "authors": ["265"]}
{"title": "Relating Goal-Scheduling, Precedence, and Memory Management in AND-Parallel Execution of Logic Programs.\n", "abstract": " The interactions among three important issues involved in the implementation of logic programs in parallel (goal scheduling, precedence, and memory management) are discussed. A simplified, parallel memory management model and an efficient, load-balancing goal scheduling strategy are presented. It is shown how, for systems which support\" don't know\" non-determinism, special care has to be taken during goal scheduling if the space recovery characteristics of sequential systems are to be preserved. A solution based on selecting only\" newer\" goals for execution is described, and an algorithm is proposed for efficiently maintaining and determining precedence relationships and variable ages across parallel goals. It is argued that the proposed schemes and algorithms make it possible to extend the storage performance of sequential systems to parallel execution without the considerable overhead previously associated with it. The results are applicable to a wide class of parallel and coroutining systems, and they represent an efficient alternative to\" all heap\" or\" spaghetti stack\" allocation models.", "num_citations": "53\n", "authors": ["265"]}
{"title": "Towards data-aware qos-driven adaptation for service orchestrations\n", "abstract": " Several activities in service oriented computing can benefit from knowing properties of a given service composition ahead of time. We will focus here on properties related to computational cost and resource usage, in a wide sense, as they can be linked to QoS characteristics. In order to attain more accuracy, we formulate computational cost / resource usage as functions on input data (or appropriate abstractions thereof) and show how these functions can be used to make more informed decisions when performing composition, proactive adaptation, and predictive monitoring. We present an approach to, on one hand, automatically synthesize these functions from orchestrations and, on the other hand, to effectively use them to increase the quality of non-trivial service-based systems with data-dependent behavior. We validate our approach by means of simulations with runtime selection of services and adaptation due\u00a0\u2026", "num_citations": "52\n", "authors": ["265"]}
{"title": "User-definable resource usage bounds analysis for Java bytecode\n", "abstract": " Automatic cost analysis of programs has been traditionally concentrated on a reduced number of resources such as execution steps, time, or memory. However, the increasing relevance of analysis applications such as static debugging and/or certification of user-level properties (including for mobile code) makes it interesting to develop analyses for resource notions that are actually application-dependent. This may include, for example, bytes sent or received by an application, number of files left open, number of SMSs sent or received, number of accesses to a database, money spent, energy consumption, etc. We present a fully automated analysis for inferring upper bounds on the usage that a Java bytecode program makes of a set of application programmer-definable resources. In our context, a resource is defined by programmer-provided annotations which state the basic consumption that certain program\u00a0\u2026", "num_citations": "52\n", "authors": ["265"]}
{"title": "Program development using abstract interpretation (and the Ciao system preprocessor)\n", "abstract": " The technique of Abstract Interpretation has allowed the development of very sophisticated global program analyses which are at the same time provably correct and practical. We present in a tutorial fashion a novel program development framework which uses abstract interpretation as a fundamental tool. The framework uses modular, incremental abstract interpretation to obtain information about the program. This information is used to validate programs, to detect bugs with respect to partial specifications written using assertions (in the program itself and/or in system libraries), to generate and simplify run-time tests, and to perform high-level program transformations such as multiple abstract specialization, parallelization, and resource usage control, all in a provably correct way. In the case of validation and debugging, the assertions can refer to a variety of program points such as procedure entry, procedure\u00a0\u2026", "num_citations": "52\n", "authors": ["265"]}
{"title": "Resource usage analysis of logic programs via abstract interpretation using sized types\n", "abstract": " We present a novel general resource analysis for logic programs based on sized types. Sized types are representations that incorporate structural (shape) information and allow expressing both lower and upper bounds on the size of a set of terms and their subterms at any position and depth. They also allow relating the sizes of terms and subterms occurring at different argument positions in logic predicates. Using these sized types, the resource analysis can infer both lower and upper bounds on the resources used by all the procedures in a program as functions on input term (and subterm) sizes, overcoming limitations of existing resource analyses and enhancing their precision. Our new resource analysis has been developed within the abstract interpretation framework, as an extension of the sized types abstract domain, and has been integrated into the Ciao preprocessor, CiaoPP. The abstract domain\u00a0\u2026", "num_citations": "51\n", "authors": ["265"]}
{"title": "Using Attributed Variables in the Implementation of Concurrent and Parallel Logic Programming Systems.\n", "abstract": " Incorporating the possibility of attaching attributes to variables in a logic programming system has been shown to allow the addition of general constraint solving capabilities to it. This approach is very attractive in that by adding a few primitives any logic programming system can be turned into a generic constraint logic programming system in which constraint solving can be user de ned, and at source level {an extreme example of the\\glass box\" approach. In this paper we propose a di erent and novel use for the concept of attributed variables: developing a generic parallel/concurrent (constraint) logic programming system, using the same\\glass box\" avor. We argue that a system which implements attributed variables and a few additional primitives can be easily customized at source level to implement many of the languages and execution models of parallelism and concurrency currently proposed, in both shared memory and distributed systems. We illustrate this through examples and report on an implementation of our ideas.", "num_citations": "48\n", "authors": ["265"]}
{"title": "Automatic compile-time parallelization of logic programs for restricted, goal level, independent and parallelism\n", "abstract": " A framework for the automatic parallelization of (constraint) logic programs is proposed and proved correct. Intuitively, the parallelization process replaces conjunctions of literals with parallel expressions. Such expressions trigger at run-time the exploitation of restricted, goal-level, independent and parallelism. The parallelization process performs two steps. The first one builds a conditional dependency graph (which can be simplified using compile-time analysis information), while the second transforms the resulting graph into linear conditional expressions, the parallel expressions of the &-Prolog language. Several heuristic algorithms for the latter (\u201cannotation\u201d) process are proposed and proved correct. Algorithms are also given which determine if there is any loss of parallelism in the linearization process with respect to a proposed notion of maximal parallelism. Finally, a system is presented which implements the\u00a0\u2026", "num_citations": "47\n", "authors": ["265"]}
{"title": "A syntactic approach to combining functional notation, lazy evaluation, and higher-order in LP systems\n", "abstract": " Nondeterminism and partially instantiated data structures give logic programming expressive power beyond that of functional programming. However, functional programming often provides convenient syntactic features, such as having a designated implicit output argument, which allow function call nesting and sometimes results in more compact code. Functional programming also sometimes allows a more direct encoding of lazy evaluation, with its ability to deal with infinite data structures. We present a syntactic functional extension, used in the Ciao system, which can be implemented in ISO-standard Prolog systems and covers function application, predefined evaluable functors, functional definitions, quoting, and lazy evaluation. The extension is also composable with higher-order features and can be combined with other extensions to ISO-Prolog such as constraints. We also highlight the features of\u00a0\u2026", "num_citations": "42\n", "authors": ["265"]}
{"title": "Inferring parametric energy consumption functions at different software levels: ISA vs. LLVM IR\n", "abstract": " The static estimation of the energy consumed by program executions is an important challenge, which has applications in program optimization and verification, and is instrumental in energy-aware software development. Our objective is to estimate such energy consumption in the form of functions on the input data sizes of programs. We have developed a tool for experimentation with static analysis which infers such energy functions at two levels, the instruction set architecture (ISA) and the intermediate code (LLVM IR) levels, and reflects it upwards to the higher source code level. This required the development of a translation from LLVM IR to an intermediate representation and its integration with existing components, a translation from ISA to the same representation, a resource analyzer, an ISA-level energy model, and a mapping from this model to LLVM IR. The approach has been applied to programs\u00a0\u2026", "num_citations": "41\n", "authors": ["265"]}
{"title": "Abstraction carrying code and resource-awareness\n", "abstract": " Proof-Carrying Code (PCC) is a general approach to mobile code safety in which the code supplier augments the program with a certificate (or proof). The intended benefit is that the program consumer can locally validate the certificate wrt the\" untrusted\" program by means of a certificate checker---a process which should be much simpler, efficient, and automatic than generating the original proof. Abstraction Carrying Code (ACC) is an enabling technology for PCC in which an abstract model of the program plays the role of certificate. The generation of the certificate, ie, the abstraction, is automatically carried out by an abstract interpretation-based analysis engine, which is parametric wrt different abstract domains. While the analyzer on the producer side typically has to compute a semantic fixpoint in a complex, iterative process, on the receiver it is only necessary to check that the certificate is indeed a fixpoint of the\u00a0\u2026", "num_citations": "41\n", "authors": ["265"]}
{"title": "A documentation generator for (C) LP systems\n", "abstract": " We describe lpdoc, a tool which generates documentation manuals automatically from one or more logic program source files, written in Ciao, ISO-Prolog, and other (C)LP languages. It is particularly useful for documenting library modules, for which it automatically generates a rich description of the module interface. However, it can also be used quite successfully to document full applications. A fundamental advantage of using lpdoc is that it helps maintaining a true correspondence between the program and its documentation, and also identifying precisely to what version of the program a given printed manual corresponds. The quality of the documentation generated can be greatly enhanced by including within the program text assertions (declarations with types, modes, etc....) for the predicates in the program, and machine-readable comments. One of the main novelties of lpdoc is that these assertions\u00a0\u2026", "num_citations": "41\n", "authors": ["265"]}
{"title": "A Simulation Study of Or-and and Independent And-parallelism.\n", "abstract": " Although studies of a number of parallel implementations of logic programming languages are now available, the results are difficult to interpret due to the multiplicity of factors involved, the effect of each of which is difficult to sep\u00e1rate. In this paper we present the results of a highlevel simulation study of or-and independent and-parallelism with a wide selection of Prolog programs that aims to facil\u00edtate this separation. We hope this study will be instrumental in better understanding and comparing results from actual implementations, as shown by an example in the paper. In addition, the paper examines some of the issues and tradeoffs associated with the combination of and-and or-parallelism and proposes reasonable solutions based on the simulation data.", "num_citations": "41\n", "authors": ["265"]}
{"title": "Some Paradigms for Visualizing Parallel Execution of Logic Programs.\n", "abstract": " This paper addresses the design of visual paradigms for studying the parallel execution of logic programs. First, an intuitive method is proposed for arriving at the design of a paradigm and its implementation as a tool for a given model of parallelism. This method is based on stepwise refinement starting from the definition of basic notions such as events and observablcs and of some precedence relationships among events which hold for the given model of parallelism. The method is then applied to several types of parallel execution models for logic programs (Or-parallelism, Restricted And-parallclism, Determinate Dependent And parallelism) for which visuali/ation paradigms arc designed. Finally, VisAndOr, a tool which implements all of these paradigms is presented, together with a discussion of its usefulness through examples.", "num_citations": "40\n", "authors": ["265"]}
{"title": "A generic preprocessor for program validation and debugging\n", "abstract": " We present a generic preprocessor for combined static/dynamic validation and debugging of constraint logic programs. Passing programs through the preprocessor prior to execution allows detecting many bugs automatically. This is achieved by performing a repertoire of tests which range from simple syntactic checks to much more advanced checks based on static analysis of the program. Together with the program, the user may provide a series of assertions which trigger further automatic checking of the program. Such assertions are written using the assertion language presented in Chapter 1, which allows expressing a wide variety of properties. These properties extend beyond the predefined set which may be understandable by the available static analysers and include properties defined by means of user programs. In addition to user-provided assertions, in each particular CLP system assertions may\u00a0\u2026", "num_citations": "39\n", "authors": ["265"]}
{"title": "Safe upper-bounds inference of energy consumption for Java bytecode applications\n", "abstract": " Many space applications such as sensor networks, on-board satellite-based platforms, on-board vehicle monitoring systems, etc. handle large amounts of data and analysis of such data is often critical for the scientific mission. Transmitting such large amounts of data to the remote control station for analysis is usually too expensive for time-critical applications. Instead, modern space applications are increasingly relying on autonomous on-board data analysis. All these applications face many resource constraints. A key requirement is to minimize energy consumption. Several approaches have been developed for estimating the energy consumption of such applications (e.g. [3, 1]) based on measuring actual consumption at run-time for large sets of random inputs. However, this approach has the limitation that it is in general not possible to cover all possible inputs. Using formal techniques offers the potential for inferring safe energy consumption bounds, thus being specially interesting for space exploration and safety-critical systems. We have proposed and implemented a general frame- work for resource usage analysis of Java bytecode [2]. The user defines a set of resource(s) of interest to be tracked and some annotations that describe the cost of some elementary elements of the program for those resources. These values can be constants or, more generally, functions of the input data sizes. The analysis then statically derives an upper bound on the amount of those resources that the program as a whole will consume or provide, also as functions of the input data sizes. This article develops a novel application of the analysis of [2] to inferring safe\u00a0\u2026", "num_citations": "37\n", "authors": ["265"]}
{"title": "A model for inter-module analysis and optimizing compilation\n", "abstract": " Recent research into the implementation of logic programming languages has demonstrated that global program analysis can be used to speed up execution by an order of magnitude. However, currently such global program analysis requires the program to be analysed as a whole: separate compilation of modules is not supported. We describe and empirically evaluate a simple model for extending global program analysis to support separate compilation of modules. Importantly, our model supports context-sensitive program analysis and multi-variant specialization of procedures in the modules.", "num_citations": "37\n", "authors": ["265"]}
{"title": "Extracting non-strict independent and-parallelism using sharing and freeness information\n", "abstract": " Logic programming systems which exploit and-parallelism among non-deterministic goals rely on notions of independence among those goals in order to ensure certain efficiency properties. \u201cNon-strict\u201d independence (NSI) is a more relaxed notion than the traditional notion of \u201cstrict\u201d independence (SI) which still ensures the relevant efficiency properties and can allow considerable more parallelism than SI. However, all compilation technology developed to date has been based on SI, because of the intrinsic complexity of exploiting NSI. This paper fills this gap by developing a technique for compile-time detection of NSI. It also proposes algorithms for combined compile-time/run-time detection, presenting novel run-time checks for this type of parallelism. The approach is based on the knowledge of certain properties regarding the run-time instantiations of program variables \u2014sharing and freeness\u2014 for which\u00a0\u2026", "num_citations": "37\n", "authors": ["265"]}
{"title": "Improved compilation of Prolog to C using moded types and determinism information\n", "abstract": " We describe the current status of and provide performance results for a prototype compiler of Prolog to C, ciaocc. ciaocc is novel in that it is designed to accept different kinds of high-level information, typically obtained via an automatic analysis of the initial Prolog program and expressed in a standardized language of assertions. This information is used to optimize the resulting C code, which is then processed by an off-the-shelf C compiler. The basic translation process essentially mimics the unfolding of a bytecode emulator with respect to the particular bytecode corresponding to the Prolog program. This is facilitated by a flexible design of the instructions and their lower-level components. This approach allows reusing a sizable amount of the machinery of the bytecode emulator: predicates already written in C, data definitions, memory management routines and areas, etc., as well as mixing emulated\u00a0\u2026", "num_citations": "36\n", "authors": ["265"]}
{"title": "Some issues in analysis and specialization of modular Ciao-Prolog programs\n", "abstract": " Separating programs into modules is a well-known technique which has proven very useful in program development and maintenance. Starting by introducing a number of possible scenarios, in this paper we study different issues which appear when developing analysis and specialization techniques for modular logic programming. We discuss a number of design alternatives and their consequences for the different scenarios considered and describe where applicable the decisions made in the Ciao system analyzer and specializer. In our discussion we use the module system of Ciao Prolog. This is both for concreteness and because Ciao Prolog is a second-generation Prolog system which has been designed with global analysis and specialization in mind, and which has a strict module system. The aim of this work is not to provide a theoretical basis on modular analysis and specialization, but rather to discuss\u00a0\u2026", "num_citations": "35\n", "authors": ["265"]}
{"title": "Complete and Efficient Methods for Supporting Side-effects in Independent/Restricted AND-Parallelism.\n", "abstract": " It has been shown that it is possible to exploit Independent/Restricted And-parallelism in logic programs while retaining the conventional \u201cdon\u2019t know\u201d semantics of such programs. In particular, it is possible to parallelize pure Prolog programs while maintaining the semantics of the language. However, when builtin side-effects (such as write or assert) appear in the program, if an identical observable behaviour to that of sequential Prolog implementations is to be preserved, such side-effects have to be properly sequenced. Previously proposed solutions to this problem are either incomplete (lacking, for example, backtracking semantics) or they force sequentialization of significant portions of the execution graph which could otherwise run in parallel. In this paper a series of side-effect synchronization methods are proposed which incur lower overhead and allow more parallelism than those previously proposed. Most importantly, and unlike previous proposals, they have well-defined backward execution behaviour and require only a small modification to a given (And-parallel) Prolog implementation.", "num_citations": "35\n", "authors": ["265"]}
{"title": "The CIAO prolog system: reference manual\n", "abstract": " Ciao is a public domain, next generation multi-paradigm programming environment with a unique set of features:\u2022 Ciao offers a complete Prolog system, supporting ISO-Prolog, but its novel modular design allows both restricting and extending the language. As a result, it allows working with fully declarative subsets of Prolog and also to extend these subsets (or ISO-Prolog) both syntactically and semantically. Most importantly, these restrictions and extensions can be activated separately on each program module so that several extensions can coexist in the same application for different modules.", "num_citations": "32\n", "authors": ["265"]}
{"title": "Distributed concurrent constraint execution in the CIAO system\n", "abstract": " This paper describes the current prototype of the distributed CIAO system. It introduces the concepts of\\teams\" and\\active modules\"(or active objects), which conveniently encapsulate di erent types of functionalities desirable from a distributed system, from parallelism for achieving speedup to client-server applications. It presents the user primitives available and describes their implementation. This implementation uses attributed variables and, as an example of a communication abstraction, a blackboard that follows the Linda model. The functionalities of the system are illustrated through examples, using the implemented primitives. The implementation of most of the primitives is also described in detail.", "num_citations": "32\n", "authors": ["265"]}
{"title": "Abstract interpretation with specialized definitions\n", "abstract": " The relationship between abstract interpretation and partial evaluation has received considerable attention and (partial) integrations have been proposed starting from both the partial evaluation and abstract interpretation perspectives. In this work we present what we argue is the first generic algorithm for efficient and precise integration of abstract interpretation and partial evaluation from an abstract interpretation perspective. Taking as starting point state-of-the-art algorithms for context-sensitive, polyvariant abstract interpretation and (abstract) partial evaluation of logic programs, we present an algorithm which combines the best of both worlds. Key ingredients include the accurate success propagation inherent to abstract interpretation and the powerful program transformations achievable by partial deduction. In our algorithm, the calls which appear in the analysis graph are not analyzed w.r.t.\u00a0the original\u00a0\u2026", "num_citations": "31\n", "authors": ["265"]}
{"title": "Determinacy analysis for logic programs using mode and type information\n", "abstract": " We propose an analysis for detecting procedures and goals that are deterministic (i.e. that produce at most one solution), or predicates whose clause tests are mutually exclusive (which implies that at most one of their clauses will succeed) even if they are not deterministic (because they call other predicates that can produce more than one solution). Applications of such determinacy information include detecting programming errors, performing certain high-level program transformations for improving search efficiency, optimizing low level code generation and parallel execution, and estimating tighter upper bounds on the computational costs of goals and data sizes, which can be used for program debugging, resource consumption and granularity control, etc. We have implemented the analysis and integrated it in the CiaoPP system, which also infers automatically the mode and type information that our\u00a0\u2026", "num_citations": "31\n", "authors": ["265"]}
{"title": "WWW programming using computational logic systems (and the PiLLoW/CIAO library)\n", "abstract": " We discuss from a practical point of view a number of issues involved in writing Internet and WWW applications using LP/CLP systems. We describe PiLLoW, a public-domain Internet and WWW programming library for LP/CLP systems which we argue signi cantly simpli es the process of writing such applications. PiLLoW provides facilities for generating HTML structured documents, producing HTML forms, writing form handlers, accessing and parsing WWW documents, and accessing code posted at HTTP addresses. We also describe the architecture of some application classes, using a high-level model of client-server interaction, active modules. We then propose an architecture for automatic LP/CLP code downloading for local execution, using generic browsers. Finally, we also provide an overview of related work on the topic. The PiLLoW library has been developed in the context of the &-Prolog and CIAO systems, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality.", "num_citations": "31\n", "authors": ["265"]}
{"title": "And-Or parallel Prolog: A recomputation based approach\n", "abstract": " We argue that in order to exploit both Independent And-and Or-parallelism in Prolog programs there is advantage in recomputing some of the independent goals, as opposed to all their solutions being reused. We present an abstract model, called the Composition-tree, for representing and-or parallelism in Prolog programs. The Composition-tree closely mirrors sequential Prolog execution by recomputing some independent goals rather than fully re-using them. We also outline two environment representation techniques for And-Or parallel execution offull Prolog based on the Composition-tree model abstraction. We argue that these techniques have advantages over earlier proposals for exploiting and-or parallelism in Prolog.", "num_citations": "31\n", "authors": ["265"]}
{"title": "An overview of the ciao multiparadigm language and program development environment and its design philosophy\n", "abstract": " We describe some of the novel aspects and motivations behind the design and implementation of the Ciao multiparadigm programming system. An important aspect of Ciao is that it provides the programmer with a large number of useful features from different programming paradigms and styles, and that the use of each of these features can be turned on and off at will for each program module. Thus, a given module may be using e.g. higher order functions and constraints, while another module may be using objects, predicates, and concurrency. Furthermore, the language is designed to be extensible in a simple and modular way. Another important aspect of Ciao is its programming environment, which provides a powerful preprocessor (with an associated assertion language) capable of statically finding non-trivial bugs, verifying that programs comply with specifications, and performing many types of\u00a0\u2026", "num_citations": "30\n", "authors": ["265"]}
{"title": "Combining static analysis and profiling for estimating execution times\n", "abstract": " Effective static analyses have been proposed which infer bounds on the number of resolutions. These have the advantage of being independent from the platform on which the programs are executed and have been shown to be useful in a number of applications, such as granularity control in parallel execution. On the other hand, in distributed computation scenarios where platforms with different capabilities come into play, it is necessary to express costs in metrics that include the characteristics of the platform. In particular, it is specially interesting to be able to infer upper and lower bounds on actual execution times. With this objective in mind, we propose an approach which combines compile-time analysis for cost bounds with a one-time profiling of a given platform in order to determine the values of certain parameters for that platform. These parameters calibrate a cost model which, from then on, is able to\u00a0\u2026", "num_citations": "30\n", "authors": ["265"]}
{"title": "A Practical Approach to the Global Analysis of CLP Programs.\n", "abstract": " This paper presents and illustrates with an example a practical approach to the data ow analysis of programs written in constraint logic programming (CLP) languages using abstract interpretation. It is rst argued that, from the framework point of view, it su ces to propose relatively simple extensions of traditional analysis methods which have already been proved useful and practical and for which e cient xpoint algorithms have been developed. This is shown by proposing a simple but quite general extension of Bruynooghe's traditional framework to the analysis of CLP programs. In this extension constraints are viewed not as\\suspended goals\" but rather as new information in the store, following the traditional view of CLP. Using this approach, and as an example of its use, a complete, constraint system independent, abstract analysis is presented for approximating de niteness information. The analysis is in fact of quite general applicability. It has been implemented and used in the analysis of CLP (R) and Prolog-III applications. Results from the implementation of this analysis are also presented.", "num_citations": "30\n", "authors": ["265"]}
{"title": "Tools for search-tree visualisation: The apt tool\n", "abstract": " The control part of the execution of a constraint logic program can be conceptually shown as a search-tree, where nodes correspond to calls, and whose branches represent conjunctions and disjunctions. This tree represents the search space traversed by the program, and has also a direct relationship with the amount of work performed by the program. The nodes of the tree can be used to display information regarding the state and origin of instantiation of the variables involved in each call. This depiction can also be used for the enumeration process. These are the features implemented in APT, a tool which runs constraint logic programs while depicting a (modified) searchtree, keeping at the same time information about the state of the variables at every moment in the execution. This information can be used to replay the execution at will, both forwards and backwards in time. These views can be\u00a0\u2026", "num_citations": "29\n", "authors": ["265"]}
{"title": "The Ciao Modular, Standalone Compiler and Its Generic Program Processing Library\n", "abstract": " Ciao Prolog incorporates a module system which allows separate compilation and sensible creation of standalone executables. We describe some of the main aspects of the Ciao modular compiler, ciaoc, which takes advantage of the characteristics of the Ciao Prolog module system to automatically perform separate and incremental compilation and efficiently build small, standalone executables with competitive run-time performance, ciaoc can also detect statically a larger number of programming errors. We also present a generic code processing library for handling modular programs, which provides an important part of the functionality of ciaoc. This library allows the development of program analysis and transformation tools in a way that is to some extent orthogonal to the details of module system design, and has been used in the implementation of ciaoc and other Ciao system tools. We also describe the\u00a0\u2026", "num_citations": "29\n", "authors": ["265"]}
{"title": "Towards execution time estimation in abstract machine-based languages\n", "abstract": " Abstract machines provide a certain separation between platform-dependent and platform-independent concerns in compilation. Many of the differences between architectures are encapsulated in the specific abstract machine implementation and the bytecode is left largely architecture independent. Taking advantage of this fact, we present a framework for estimating upper and lower bounds on the execution times of logic programs running on a bytecode-based abstract machine. Our approach includes a one-time, program-independent profiling stage which calculates constants or functions bounding the execution time of each abstract machine instruction. Then, a compile-time cost estimation phase, using the instruction timing information, infers expressions giving platform-dependent upper and lower bounds on actual execution time as functions of input data sizes for each program. Working at the abstract\u00a0\u2026", "num_citations": "28\n", "authors": ["265"]}
{"title": "A holistic approach for resource-aware adaptive data stream mining\n", "abstract": " Mining data streams is a field of increasing interest due to the importance of its applications and dissemination of data stream sources. Most of the streaming techniques developed so far have not addressed the need for resource-aware computing in data stream analysis. The fact that streaming information is often generated or received onboard resource-constrained computational devices such as sensor nodes and mobile devices motivates the need for resource-awareness in data stream processing systems. In this paper, we propose a generic framework that enables resource-awareness in streaming computation using algorithm granularity settings in order to change the resource consumption patterns periodically. This generic framework is applied to a novel threshold-based micro-clustering algorithm to test its validity and feasibility. We have termed this algorithm as RA-Cluster. RA-Custer is the first\u00a0\u2026", "num_citations": "28\n", "authors": ["265"]}
{"title": "An integration of partial evaluation in a generic abstract interpretation framework\n", "abstract": " Information generated by abstract interpreters has long been used to perform program specialization. Additionally, if the abstract interpreter generates a multivariant analysis, it is also possible to perform multiple specialization. Information about values of variables is propagated by simulating program execution and performing xpoint computations for recursive calls. In contrast, traditional partial evaluators (mainly) use unfolding for both propagating values of variables and transforming the program. It is known that abstract interpretation is a better technique for propagating success values than unfolding. However, the program transformations induced by unfolding may lead to important optimizations which are not directly achievable in the existing frameworks for multiple specialization based on abstract interpretation. The aim of this work is to devise a specialization framework which integrates the better information propagation of abstract interpretation with the powerful program transformations performed by partial evaluation, and which can be implemented via small modi cations to existing generic abstract interpreters. With this aim, we will relate top-down abstract interpretation with traditional concepts in partial evaluation and sketch how the sophisticated techniques developed for controlling partial evaluation can be adapted to the proposed specialization framework. We conclude that there can be both practical and conceptual advantages in the proposed integration of partial evaluation and abstract interpretation.", "num_citations": "28\n", "authors": ["265"]}
{"title": "An improved continuation call-based implementation of tabling\n", "abstract": " Tabled evaluation has been proved an effective method to improve several aspects of goal-oriented query evaluation, including termination and complexity. Several \u201cnative\u201d implementations of tabled evaluation have been developed which offer good performance, but many of them require significant changes to the underlying Prolog implementation, including the compiler and the abstract machine. Approaches based on program transformation, which tend to minimize changes to both the Prolog compiler and the abstract machine, have also been proposed, but they often result in lower efficiency. We explore some techniques aimed at combining the best of these worlds, i.e., developing an extensible implementation which requires minimal modifications to the compiler and the abstract machine, and with reasonably good performance. Our preliminary experiments indicate promising results.", "num_citations": "27\n", "authors": ["265"]}
{"title": "High-level languages for small devices: a case study\n", "abstract": " In this paper we study, through a concrete case, the feasibility of using a high-level, general-purpose logic language in the design and implementation of applications targeting wearable computers. The case study is a\" sound spatializer\" which, given real-time signals for monaural audio and heading, generates stereo sound which appears to come from a position in space. The use of advanced compile-time transformations and optimizations made it possible to execute code written in a clear style without efficiency or architectural concerns on the target device, while meeting strict existing time and memory constraints. The final executable compares favorably with a similar implementation written in C. We believe that this case is representative of a wider class of common pervasive computing applications, and that the techniques we show here can be put to good use in a range of scenarios. This points to the\u00a0\u2026", "num_citations": "27\n", "authors": ["265"]}
{"title": "A generic framework for context-sensitive analysis of modular programs\n", "abstract": " Context-sensitive analysis provides information which is potentially more accurate than that provided by context-free analysis. Such information can then be applied in order to validate/debug the program and/or to specialize the program obtaining important improvements. Unfortunately, context-sensitive analysis of modular programs poses important theoretical and practical problems. One solution, used in several proposals, is to resort to context-free analysis. Other proposals do address context-sensitive analysis, but are only applicable when the description domain used satisfies rather restrictive properties. In this paper, we argue that a general framework for context-sensitive analysis of modular programs, i.e., one that allows using all the domains which have proved useful in practice in the non-modular setting, is indeed feasible and very useful. Driven by our experience in the design and implementation\u00a0\u2026", "num_citations": "27\n", "authors": ["265"]}
{"title": "A technique for recursive invariance detection and selective program specialization\n", "abstract": " This paper presents a technique for achieving a class of optimizations related to the reduction of checks within cycles. The technique uses both Program Transformation and Abstract Interpretation. After a first pass of an abstract interpreter which detects simple invariants, program transformation is used to build a hypothetical situation that simplifies some predicates that should be executed within the cycle. This transformation implements the heuristic hypothesis that once conditional tests hold they may continue doing so recursively. Specialized versions of predicates are generated to detect and exploit those cases in which the invariance may hold. Abstract interpretation is then used again to verify the truth of such hypotheses and confirm the proposed simplification. This allows optimizations that go beyond those possible with only one pass of the abstract interpreter over the original program, as is normally the\u00a0\u2026", "num_citations": "27\n", "authors": ["265"]}
{"title": "Multivariant non-failure analysis via standard abstract interpretation\n", "abstract": " Non-failure analysis aims at inferring that predicate calls in a program will never fail. This type of information has many applications in functional/logic programming. It is essential for determining lower bounds on the computational cost of calls, useful in the context of program parallelization, instrumental in partial evaluation and other program transformations, and has also been used in query optimization. In this paper, we re-cast the non-failure analysis proposed by Debray et al. as an abstract interpretation, which not only allows to investigate it from a standard and well understood theoretical framework, but has also several practical advantages. It allows us to incorporate non-failure analysis into a standard, generic abstract interpretation engine. The analysis thus benefits from the fixpoint propagation algorithm, which leads to improved information propagation. Also, the analysis takes advantage of the multi\u00a0\u2026", "num_citations": "26\n", "authors": ["265"]}
{"title": "Tools for constraint visualisation: The VIFID/TRIFID tool\n", "abstract": " Visualisation of program executions has been used in applications which include education and debugging. However, traditional visualisation techniques often fall short of expectations or are altogether inadequate for new programming paradigms, such as Constraint Logic Programming (CLP), whose declarative and operational semantics differ in some crucial ways from those of other paradigms. In particular, traditional ideas regarding the behaviour of data often cannot be lifted in a straightforward way to (C)LP from other families of programming languages. In this chapter we discuss techniques for visualising data evolution in CLP. We briefly review some previously proposed visualisation paradigms, and also propose a number of (to our knowledge) novel ones. The graphical representations have been chosen based on the perceived needs of a programmer trying to analyse the behaviour and\u00a0\u2026", "num_citations": "26\n", "authors": ["265"]}
{"title": "Structured interactive musical scores\n", "abstract": " Interactive Scores is a formalism for the design and performance of interactive scenarios that provides temporal relations (TRs) among the objects of the scenario. We can model TRs among objects in Time Stream Petri nets, but it is difficult to represent global constraints. This can be done explicitly in the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We want to formalize a heterogeneous system that controls in one subsystem the concurrent execution of the objects using ntcc, and audio and video processing in the other. We also plan to develop an automatic verifier for ntcc.", "num_citations": "25\n", "authors": ["265"]}
{"title": "Abstraction-carrying code: a model for mobile code safety\n", "abstract": " Proof-Carrying Code (PCC) is a general approach to mobile code safety in which programs are augmented with a certificate (or proof). The intended benefit is that the program consumer can locally validate the certificate w.r.t. the \u201cuntrusted\u201d program by means of a certificate checker\u2014a process which should be much simpler, efficient, and automatic than generating the original proof. The practical uptake of PCC greatly depends on the existence of a variety of enabling technologies which allow both proving programs correct and replacing a costly verification process by an efficient checking procedure on the consumer side. In this work we propose Abstraction-Carrying Code (ACC), a novel approach which uses abstract interpretation as enabling technology. We argue that the large body of applications of abstract interpretation to program verification is amenable to the overall PCC scheme. In\u00a0\u2026", "num_citations": "25\n", "authors": ["265"]}
{"title": "An abstract interpretation-based approach to mobile code safety\n", "abstract": " Recent approaches to mobile code safety, like proof-carrying code, involve associating safety information to programs. The code supplier provides a program and also includes with it a certificate (or proof) whose validity entails compliance with a predefined safety policy. The intended benefit is that the program consumer can locally validate the certificate w.r.t. the \u201cuntrusted\u201d program by means of a certificate checker\u2014a process which should be much simpler, efficient, and automatic than generating the original proof. We herein introduce a novel approach to mobile code safety which follows a similar scheme, but which is based throughout on the use of abstract interpretation techniques. In our framework the safety policy is specified by using an expressive assertion language defined over abstract domains. We identify a particular slice of the abstract interpretation-based static analysis results which is especially\u00a0\u2026", "num_citations": "25\n", "authors": ["265"]}
{"title": "A framework for assertion-based debugging in constraint logic programming\n", "abstract": " As constraint logic programming matures and larger applications are built, an increased need arises for advanced development and debugging environments. Assertions are linguistic constructions which allow expressing properties of programs. Classical examples of assertions are type declarations. However, herein we are interested in supporting a more general setting [3, 1] in which, on one hand assertions can be of a more general nature, including properties which are statically undecidable, and, on the other, only a small number of assertions may be present in the program, i.e., the assertions are optional. In particular, we do not wish to limit the programming language or the language of assertions unnecessarily in order to make the assertions statically decidable. Consequently, the proposed framework needs to deal throughout with approximations [2].               The framework we propose (see [4]) is\u00a0\u2026", "num_citations": "25\n", "authors": ["265"]}
{"title": "Hiord: A Type-Free Higher-Order Logic Programming Language with Predicate Abstraction\n", "abstract": " A new formalism, called Hiord, for defining type-free higher-order logic programming languages with predicate abstraction is introduced. A model theory, based on partial combinatory algebras, is presented, with respect to which the formalism is shown sound. A programming language built on a subset of Hiord, and its implementation are discussed. A new proposal for defining modules in this framework is considered, along with several examples.", "num_citations": "24\n", "authors": ["265"]}
{"title": "Efficient local unfolding with ancestor stacks for full Prolog\n", "abstract": " The integration of powerful partial evaluation methods into practical compilers for logic programs is still far from reality. This is related both to 1) efficiency issues and to 2) the complications of dealing with practical programs. Regarding efficiency, the most successful unfolding rules used nowadays are based on structural orders applied over (covering) ancestors, i.e., a subsequence of the atoms selected during a derivation. Unfortunately, maintaining the structure of the ancestor relation during unfolding introduces significant overhead. We propose an efficient, practical local unfolding rule based on the notion of covering ancestors which can be used in combination with any structural order and allows a stack-based implementation without losing any opportunities for specialization. Regarding the second issue, we propose assertion-based techniques which allow our approach to deal with real programs that\u00a0\u2026", "num_citations": "24\n", "authors": ["265"]}
{"title": "From eventual to atomic and locally atomic CC programs: A concurrent semantics\n", "abstract": " We present a concurrent semantics (i.e. a semantics where concurrency is explicitely represented) for CC programs with atomic tells. This allows to derive concurrency, dependency, and nondeterminism information for such languages. The ability to treat failure information puts CLP programs also in the range of applicability of our semantics: although such programs are not concurrent, the concurrency information derived in the semantics may be interpreted as possible parallelism, thus allowing to safely parallelize those computation steps which appear to be concurrent in the net. Dually, the dependency information may also be interpreted as necessary sequentialization, thus possibly exploiting it to schedule CC programs. The fact that the semantical structure contains dependency information suggests a new tell operation, which checks for consistency only the constraints it depends on, achieving a\u00a0\u2026", "num_citations": "24\n", "authors": ["265"]}
{"title": "Timed definite clause omega-grammars\n", "abstract": " We propose timed context-free grammars (TCFGs) and show how parsers for such grammars can be developed using definite clause grammars (DCGs) coupled with constraints over reals (CLP (R)). Timed context-free grammars describe timed context-free languages (TCFLs). We next extend timed context-free grammars to timed context-free omega-grammars (omega-TCFGs for brevity) and incorporate co-inductive logic programming in DCGs to obtain parsers for them. Timed context-free omega-grammars describe timed context-free languages containing infinite-sized words, and are a generalization of timed omega-regular languages recognized by timed automata. We show a practical application of omega-TCFGs to the well-known generalized railroad crossing problem.", "num_citations": "23\n", "authors": ["265"]}
{"title": "Independence in CLP languages\n", "abstract": " Studying independence of goals has proven very useful in the context of logic programming. In particular, it has provided a formal basis for powerful automatic parallelization tools, since independence ensures that two goals may be evaluated in parallel while preserving correctness and efficiency. We extend the concept of independence to constraint logic programs (CLP) and prove that it also ensures the correctness and efficiency of the parallel evaluation of independent goals. Independence for CLP languages is more complex than for logic programming as search space preservation is necessary but no longer sufficient for ensuring correctness and efficiency. Two additional issues arise. The first is that the cost of constraint solving may depend upon the order constraints are  encountered. The second is the need to handle dynamic scheduling. We clarify these issues by proposing various types of search\u00a0\u2026", "num_citations": "23\n", "authors": ["265"]}
{"title": "The ciao logic programming environment: A tutorial\n", "abstract": " Lazily: bytecode loaded when a predicate of the module is called. Useful when not all capabilities of an application are used in every run.\u00a6 Not possible for every module. The compiler has to produce stump code. Executables may be compressed: smaller but (sometimes) slower startup. Stand-alone architecture-dependent executables may also be created.", "num_citations": "23\n", "authors": ["265"]}
{"title": "Towards independent and-parallelism in CLP\n", "abstract": " In this paper we propose a complete scheme for automatic exploitation of independent and-parallelism in CLP programs. We first discuss the new problems involved because of the different properties of the independence notions applicable to CLP. We then show how independence can be derived from a number of standard analysis domains for CLP. Finally, we perform a preliminary evaluation of the efficiency, accuracy, and effectiveness of the approach by implementing a parallelizing compiler for CLP based on the proposed ideas and applying it on a number of CLP benchmarks.", "num_citations": "23\n", "authors": ["265"]}
{"title": "Abstract specialization and its application to program parallelization\n", "abstract": " Program specialization optimizes programs for known values of the input. It is often the case that the set of possible input values is unknown, or this set is infinite. However, a form of specialization can still be performed in such cases by means of abstract interpretation, specialization then being with respect to abstract values (substitutions), rather than concrete ones. This paper reports on the application of abstract multiple specialization to automatic program parallelization in the &-Prolog compiler. Abstract executability, the main concept underlying abstract specialization, is formalized, the design of the specialization system presented, and a non-trivial example of specialization in automatic parallelization is given.", "num_citations": "22\n", "authors": ["265"]}
{"title": "Implementing Distributed Concurrent Constraint Execution in the CIAO System.\n", "abstract": " This paper describes the current prototype of the distributed CIAO system. It introduces the concepts of\\teams\" and\\active modules\"(or active objects), which conveniently encapsulate di erent types of functionalities desirable from a distributed system, from parallelism for achieving speedup to client-server applications. The user primitives available are presented and their implementation described. This implementation uses attributed variables and, as an example of a communication abstraction, a blackboard that follows the Linda model. Finally, the CIAO WWW interface is also brie y described. The functionalities of the system are illustrated through examples, using the implemented primitives.", "num_citations": "22\n", "authors": ["265"]}
{"title": "Improving the efficiency of nondeterministic independent and-parallel systems\n", "abstract": " We present the design and implementation of the and-parallel component of ACE. ACE is a computational model for the full Prolog language that simultaneously exploits both or-parallelism and independent and-parallelism. A high-performance implementation of the ACE model has been realized and its performance reported in this paper. We discuss how some of the standard problems which appear when implementing and-parallel systems are solved in ACE. We then propose a number of optimizations aimed at reducing the overheads and the increased memory consumption which occur in such systems when using previously proposed solutions. Finally, we present results from an implementation of ACE which includes the optimizations proposed. The results show that ACE exploits and-parallelism with high efficiency and high speedups. Furthermore, they also show that the proposed optimizations, which are\u00a0\u2026", "num_citations": "22\n", "authors": ["265"]}
{"title": "Using generalized annotated programs to solve social network diffusion optimization problems\n", "abstract": " There has been extensive work in many different fields on how phenomena of interest (e.g., diseases, innovation, product adoption) \u201cdiffuse\u201d through a social network. As social networks increasingly become a fabric of society, there is a need to make \u201coptimal\u201d decisions with respect to an observed model of diffusion. For example, in epidemiology, officials want to find a set of k individuals in a social network which, if treated, would minimize spread of a disease. In marketing, campaign managers try to identify a set of k customers that, if given a free sample, would generate maximal \u201cbuzz\u201d about the product. In this article, we first show that the well-known Generalized Annotated Program (GAP) paradigm can be used to express many existing diffusion models. We then define a class of problems called Social Network Diffusion Optimization Problems (SNDOPs). SNDOPs have four parts: (i) a diffusion model expressed\u00a0\u2026", "num_citations": "21\n", "authors": ["265"]}
{"title": "Automatic fragment identification in workflows based on sharing analysis\n", "abstract": " In Service-Oriented Computing (SOC), fragmentation and merging of workflows are motivated by a number of concerns, among which we can cite design issues, performance, and privacy. Fragmentation emphasizes the application of design and runtime methods for clustering workflow activities into fragments and for checking the correctness of such fragment identification w.r.t. to some predefined policy. We present a fragment identification approach based on sharing analysis and we show how it can be applied to abstract workflow representations that may include descriptions of data operations, logical link dependencies based on logical formulas, and complex control flow constructs, such as loops and branches. Activities are assigned to fragments (to infer how these fragments are made up or to check their well-formedness) by interpreting the sharing information obtained from the analysis according to\u00a0\u2026", "num_citations": "21\n", "authors": ["265"]}
{"title": "An efficient, parametric fixpoint algorithm for analysis of Java bytecode\n", "abstract": " Abstract interpretation has been widely used for the analysis of object-oriented languages and, in particular, Java source and bytecode. However, while most existing work deals with the problem of finding expressive abstract domains that track accurately the characteristics of a particular concrete property, the underlying fixpoint algorithms have received comparatively less attention. In fact, many existing (abstract interpretation based\u2013) fixpoint algorithms rely on relatively inefficient techniques for solving inter-procedural call graphs or are specific and tied to particular analyses. We also argue that the design of an efficient fixpoint algorithm is pivotal to supporting the analysis of large programs. In this paper we introduce a novel algorithm for analysis of Java bytecode which includes a number of optimizations in order to reduce the number of iterations. The algorithm is parametric -in the sense that it is independent of\u00a0\u2026", "num_citations": "21\n", "authors": ["265"]}
{"title": "A generic persistence model for (C) LP systems (and two useful implementations)\n", "abstract": " This paper describes a model of persistence in (C)LP languages and two different and practically very useful ways to implement this model in current systems. The fundamental idea is that persistence is a characteristic of certain dynamic predicates (i.e., those which encapsulate state). The main effect of declaring a predicate persistent is that the dynamic changes made to such predicates persist from one execution to the next one. After proposing a syntax for declaring persistent predicates, a simple, file-based implementation of the concept is presented and some examples shown. An additional implementation is presented which stores persistent predicates in an external database. The abstraction of the concept of persistence from its implementation allows developing applications which can store their persistent predicates alternatively in files or databases with only a few simple changes to a declaration\u00a0\u2026", "num_citations": "21\n", "authors": ["265"]}
{"title": "Partial order and contextual net semantics for atomic and locally atomic CC programs\n", "abstract": " We present two concurrent semantics (i.e. semantics where concurrency is explicitly represented) for CC programs with atomic tells. One is based on simple partial orders of computation steps, while the other one is based on contextual nets and it is an extension of a previous one for eventual CC programs. Both such semantics allow us to derive concurrency, dependency, and nondeterminism information for the considered languages. We prove some properties about the relation between the two semantics, and also about the relation between them and the operational semantics. Moreover, we discuss how to use the contextual net semantics in the context of CLP programs. More precisely, by interpreting concurrency as possible parallelism, our semantics can be useful for a safe parallelization of some CLP computation steps. Dually, the dependency information may also be interpreted as necessary\u00a0\u2026", "num_citations": "21\n", "authors": ["265"]}
{"title": "A practical approach to the global analysis of constraint logic programs\n", "abstract": " This paper presents and illustrates with an example a practical approach to the data ow analysis of programs written in constraint logic programming (CLP) languages using abstract interpretation. It is rst argued that, from the framework point of view, it su ces to propose quite simple extensions of traditional analysis methods which have already been proved useful and practical and for which e cient xpoint algorithms have been developed. This is shown by proposing a simple but quite general extension to the analysis of CLP programs of Bruynooghe's traditional framework. In this extension constraints are viewed not as\\suspended goals\" but rather as new information in the store, following the traditional view of CLP. Using this approach, a complete, constraint system independent, abstract analysis is presented for approximating de niteness information. The analysis is of quite general applicability since it uses in its implementation only constraints over the Herbrand domain. Some results from the implementation of this analysis are also presented.", "num_citations": "21\n", "authors": ["265"]}
{"title": "Independent and-parallel implementation of narrowing\n", "abstract": " We present a parallel graph narrowing machine, which is used to implement a functional logic language on a shared memory multiprocessor. It is an extension of an abstract machine for a purely functional language. The result is a programmed graph reduction machine which integrates the mechanisms of unification, backtracking, and independent and-parallelism. In the machine, the subexpressions of an expression can run in parallel. In the case of backtracking, the structure of an expression is used to avoid the reevaluation of subexpressions as far as possible. Deterministic computations are detected. Their results are maintained and need not be reevaluated after backtracking.", "num_citations": "21\n", "authors": ["265"]}
{"title": "Reducing the overhead of assertion run-time checks via static analysis\n", "abstract": " In order to aid in the process of detecting incorrect program behaviors, a number of approaches have been proposed which include a combination of language-level constructs (such as procedure-level assertions/contracts, program-point assertions, gradual types, etc.) and associated tools (such as code analyzers and run-time verification frameworks). However, it is often the case that these constructs and tools are not used to their full extent in practice due to a number of limitations such as excessive run-time overhead and/or limited expressiveness. Verification frameworks that combine static and dynamic techniques offer the potential to bridge this gap. In this paper we explore the effectiveness of abstract interpretation in detecting parts of program specifications that can be statically simplified to true or false, as well as the impact of such analysis in reducing the cost of the run-time checks required for the remaining\u00a0\u2026", "num_citations": "20\n", "authors": ["265"]}
{"title": "Annotation algorithms for unrestricted independent AND-parallelism in logic programs\n", "abstract": " We present two new algorithms which perform automatic parallelization via source-to-source transformations. The objective is to exploit goal-level, unrestricted independent and-parallelism. The proposed algorithms use as targets new parallel execution primitives which are simpler and more flexible than the well-known &/2 parallel operator. This makes it possible to generate better parallel expressions by exposing more potential parallelism among the literals of a clause than is possible with &/2. The difference between the two algorithms stems from whether the order of the solutions obtained is preserved or not. We also report on a preliminary evaluation of an implementation of our approach. We compare the performance obtained to that of previous annotation algorithms and show that relevant improvements can be obtained.", "num_citations": "20\n", "authors": ["265"]}
{"title": "Reduced certificates for abstraction-carrying code\n", "abstract": " Abstraction-Carrying Code (ACC) has recently been proposed as a framework for mobile code safety in which the code supplier provides a program together with an abstraction whose validity entails compliance with a predefined safety policy. The abstraction plays thus the role of safety certificate and its generation is carried out automatically by a fixed-point analyzer. The advantage of providing a (fixed-point) abstraction to the code consumer is that its validity is checked in a single pass of an abstract interpretation-based checker. A main challenge is to reduce the size of certificates as much as possible while at the same time not increasing checking time. We introduce the notion of reduced certificate which characterizes the subset of the abstraction which a checker needs in order to validate (and re-construct) the full certificate in a single pass. Based on this notion, we instrument a generic\u00a0\u2026", "num_citations": "20\n", "authors": ["265"]}
{"title": "Efficient top-down set-sharing analysis using cliques\n", "abstract": " We study the problem of efficient, scalable set-sharing analysis of logic programs. We use the idea of representing sharing information as a pair of abstract substitutions, one of which is a worst-case sharing representation called a clique set, which was previously proposed for the case of inferring pair-sharing. We use the clique-set representation for (1) inferring actual set-sharing information, and (2) analysis within a top-down framework. In particular, we define the new abstract functions required by standard top-down analyses, both for sharing alone and also for the case of including freeness in addition to sharing. We use cliques both as an alternative representation and as widening, defining several widening operators. Our experimental evaluation supports the conclusion that, for inferring set-sharing, as it was the case for inferring pair-sharing, precision losses are limited, while useful efficiency gains are\u00a0\u2026", "num_citations": "20\n", "authors": ["265"]}
{"title": "Towards a Concurrent Semantics based Analysis of CC and CLP\n", "abstract": " We present in an informal way some preliminary results on the investigation of efficient compile-time techniques for Constraint Logic [JL87] and Concurrent Constraint [Sar89] Programming. These techniques are viewed as sourceto-source program transformations between the two programming paradigms and are based on a concurrent semantics of CC programs [MR91]. Previous work [BH92] showed that it is possible to perform program transformations from Prolog to AKL 1 [JH91], allowing the latter to fully exploit the Independent And-Parallelism (IAP)[ItR93] present in Prolog programs. However, when extending the transformation techniques to the CLP paradigm [JL87, Co190, VanH89], some issues have to be initially solved. First, the notion of independence has to be extended [GttM93]. Second, compile-time tools based on the extended notions have to be developed in order to capture the independence of\u00a0\u2026", "num_citations": "20\n", "authors": ["265"]}
{"title": "ENTRA: Whole-systems energy transparency\n", "abstract": " Promoting energy efficiency to a first class system design goal is an important research challenge. Although more energy-efficient hardware can be designed, it is software that controls the hardware; for a given system the potential for energy savings is likely to be much greater at the higher levels of abstraction in the system stack. Thus the greatest savings are expected from energy-aware software development, which is the vision of the EU ENTRA project. This article presents the concept of energy transparency as a foundation for energy-aware software development. We show how energy modelling of hardware is combined with static analysis to allow the programmer to understand the energy consumption of a program without executing it, thus enabling exploration of the design space taking energy into consideration. The paper concludes by summarising the current and future challenges identified in the ENTRA\u00a0\u2026", "num_citations": "19\n", "authors": ["265"]}
{"title": "HEX programs with action atoms\n", "abstract": " HEX programs were originally introduced as a general framework for extending declarative logic programming, under the stable model semantics, with the possibility of bidirectionally accessing external sources of knowledge and/or computation. The original framework, however, does not deal satisfactorily with stateful external environments: the possibility of predictably influencing external environments has thus not yet been considered explicitly. This paper lifts HEX programs to ACTHEX programs: ACTHEX programs introduce the notion of action atoms, which are associated to corresponding functions capable of actually changing the state of external environments. The execution of specific sequences of action atoms can be declaratively programmed. Furthermore, ACTHEX programs allow for selecting preferred actions, building on weights and corresponding cost functions. We introduce syntax and semantics of acthex programs; ACTHEX programs can successfully be exploited as a general purpose language for the declarative implementation of executable specifications, which we illustrate by encodings of knowledge bases updates, action languages, and an agent programming language. A system capable of executing ACTHEX programs has been implemented and is publicly available.", "num_citations": "19\n", "authors": ["265"]}
{"title": "Precise set sharing analysis for Java-style programs\n", "abstract": " Finding useful sharing information between instances in object-oriented programs has recently been the focus of much research. The applications of such static analysis Applications are multiple: by knowing which variables definitely do not share in memory we can apply conventional compiler optimizations, find coarse-grained parallelism opportunities, or, more importantly, verify certain correctness aspects of programs even in the absence of annotations. In this paper we introduce a framework for deriving precise sharing information based on abstract interpretation for a Java-like language.Our analysis achieves precision in various ways, including supporting multivariance, which allows separating different contexts. We propose a combined Set Sharing + Nullity + Classes domain which captures which instances do not share and which ones are definitively null, and which uses the classes to refine the static\u00a0\u2026", "num_citations": "19\n", "authors": ["265"]}
{"title": "Some Design Issues in the Visualization of Constraint Logic Program Execution.\n", "abstract": " Visualization of program executions has been found useful in applications which include education and debugging. However, traditional visualization techniques often fall short of expectations or are altogether inadequate for new programming paradigms, such as Constraint Logic Programming (CLP), whose declarative and operational semantics di er in some crucial ways from those of other paradigms. In particular, traditional ideas regarding ow control and the behavior of data often cannot be lifted in a straightforward way to (C) LP from other families of programming languages. In this paper we discuss techniques for visualizing program execution and data evolution in CLP. We brie y review some previously proposed visualization paradigms, and also propose a number of (to our knowledge) novel ones. The graphical representations have been chosen based on the perceived needs of a programmer trying to analyze the behavior and characteristics of an execution. In particular, we concentrate on the representation of the program execution behavior (control), the runtime values of the variables, and the runtime constraints. Given our interest in visualizing large executions, we also pay attention to abstraction techniques, ie, techniques which are intended to help in reducing the complexity of the visual information.", "num_citations": "19\n", "authors": ["265"]}
{"title": "Automatic inference of determinacy and mutual exclusion for logic programs using mode and type analyses\n", "abstract": " We propose an analysis for detecting procedures and goals that are deterministic (i.e., that produce at most one solution at most once), or predicates whose clause tests are mutually exclusive (which implies that at most one of their clauses will succeed) even if they are not deterministic. The analysis takes advantage of the pruning operator in order to improve the detection of mutual exclusion and determinacy. It also supports arithmetic equations and disequations, as well as equations and disequations on terms, for which we give a complete satisfiability testing algorithm, w.r.t. available type information. Information about determinacy can be used for program debugging and optimization, resource consumption and granularity control, abstraction carrying code, etc. We have implemented the analysis and integrated it in the CiaoPP system, which also infers automatically the mode and type information that our\u00a0\u2026", "num_citations": "18\n", "authors": ["265"]}
{"title": "Abstract specialization and its applications\n", "abstract": " The aim of program specialization is to optimize programs by exploiting certain knowledge about the context in which the program will execute. There exist many program manipulation techniques which allow specializing the program in different ways. Among them, one of the best known techniques is partial evaluation, often referred to simply as program specialization, which optimizes programs by specializing them for (partially) known input data. In this work we describe abstract specialization, a technique whose main features are:(1) specialization is performed with respect to\" abstract\" values rather than\" concrete\" ones, and (2) abstract interpretation rather than standard interpretation of the program is used in order to propagate information about execution states. The concept of abstract specialization is at the heart of the specialization system in CiaoPP, the Ciao system preprocessor. In this paper we present a\u00a0\u2026", "num_citations": "18\n", "authors": ["265"]}
{"title": "Recomputation based Implementations of And-Or Parallel Prolog.\n", "abstract": " We argue that in order to exploit both Independent And-and Or-parallelism in Prolog programs there is advantage in recomputing some of the independent goals, as opposed to all their solutions being reused. We present an abstract model, called the Composition-Tree, for representing and-or parallelism in Prolog Programs. The Composition-tree closely mirrors sequen-tial Prolog execution by recomputing some indepen-dent goals rather than fully re-using them. We also outline two environment representation techniques for And-Or parallel execution of full Prolog based on the Composition-tree model abstraction. We argue that these techniques have advantages over earlier proposals for exploiting and-or parallelism in Prolog.", "num_citations": "18\n", "authors": ["265"]}
{"title": "Interval-based resource usage verification by translation into Horn clauses and an application to energy consumption\n", "abstract": " Many applications require conformance with specifications that constrain the use of resources, such as execution time, energy, bandwidth, etc. We present a configurable framework for static resource usage verification where specifications can include data size-dependent resource usage functions, expressing both lower and upper bounds. Ensuring conformance with respect to such specifications is an undecidable problem. Therefore, to statically check such specifications, our framework infers the same type of resource usage functions, which safely approximate the actual resource usage of the program, and compares them against the specification. We review how this framework supports several languages and compilation output formats by translating them to an intermediate representation based on Horn clauses and using the configurability of the framework to describe the resource semantics of the input\u00a0\u2026", "num_citations": "17\n", "authors": ["265"]}
{"title": "Efficient negation using abstract interpretation\n", "abstract": " While negation has been a very active area of research in logic programming, comparatively few papers have been devoted to implementation issues. Furthermore, the negation-related capabilities of current Prolog systems are limited. We recently presented a novel method for incorporating negation in a Prolog compiler which takes a number of existing methods (some modified and improved by us) and uses them in a combined fashion. The method makes use of information provided by a global analysis of the source code. Our previous work focused on the systematic description of the techniques and the reasoning about correctness and completeness of the method, but provided no experimental evidence to evaluate the proposal. In this paper, we provide experimental data which indicates that the method is not only feasible but also quite promising from the efficiency point of view. In addition, the tests\u00a0\u2026", "num_citations": "17\n", "authors": ["265"]}
{"title": "Automatic parallelization of irregular and pointer-based computations: Perspectives from logic and constraint programming\n", "abstract": " Irregular computations pose some of the most interesting and challenging problems in automatic parallelization. Irregularity appears in certain kinds of numerical problems and is pervasive in symbolic applications. Such computations often use dynamic data structures which make heavy use of pointers. This complicates all the steps of a parallelizing compiler, from independence detection to task partitioning and placement. In the past decade there has been significant progress in the development of parallelizing compilers for logic programming and, more recently, constraint programming. The typical applications of these paradigms frequently involve irregular computations, which arguably makes the techniques used in these compilers potentially interesting. In this paper we introduce in a tutorial way some of the problems faced by parallelizing compilers for logic and constraint programs. These include the\u00a0\u2026", "num_citations": "17\n", "authors": ["265"]}
{"title": "Relating data-parallelism and (and-) parallelism in logic programs\n", "abstract": " Much work has been done in the areas of and-parallelism and data-parallelism in Logic Programs. Such work has proceeded to a certain extent in an independent fashion. Both types of parallelism offer advantages and disadvantages. Traditional (and-) parallel models offer generality, being able to exploit parallelism in a large class of programs (including that exploited by data-parallelism techniques). Data-parallelism techniques on the other hand offer increased performance for a restricted class of programs. The thesis of this paper is that these two forms of parallelism are not fundamentally different and that relating them opens the possibility of obtaining the advantages of both within the same system. Some relevant issues are discussed and solutions proposed. The discussion is illustrated through visualizations of actual parallel executions implementing the ideas proposed.", "num_citations": "17\n", "authors": ["265"]}
{"title": "A transformational approach to parametric accumulated-cost static profiling\n", "abstract": " Traditional static resource analyses estimate the total resource usage of a program, without executing it. In this paper we present a novel resource analysis whose aim is instead the static profiling of accumulated cost, i.e., to discover, for selected parts of the program, an estimate or bound of the resource usage accumulated in each of those parts. Traditional resource analyses are parametric in the sense that the results can be functions on input data sizes. Our static profiling is also parametric, i.e., our accumulated cost estimates are also parameterized by input data sizes. Our proposal is based on the concept of cost centers and a program transformation that allows the static inference of functions that return bounds on these accumulated costs depending on input data sizes, for each cost center of interest. Such information is much more useful to the software developer than the traditional resource usage\u00a0\u2026", "num_citations": "16\n", "authors": ["265"]}
{"title": "Context-sensitive multivariant assertion checking in modular programs\n", "abstract": " We propose a modular, assertion-based system for verification and debugging of large logic programs, together with several interesting models for checking assertions statically in modular programs, each with different characteristics and representing different trade-offs. Our proposal is a modular and multivariant extension of our previously proposed abstract assertion checking model and we also report on its implementation in the CiaoPP system. In our approach, the specification of the program, given by a set of assertions, may be partial, instead of the complete specification required by traditional verification systems. Also, the system can deal with properties which cannot always be determined at compile-time. As a result, the proposed system needs to work with safe approximations: all assertions proved correct are guaranteed to be valid and all errors actual errors. The use of modular, context-sensitive\u00a0\u2026", "num_citations": "16\n", "authors": ["265"]}
{"title": "A generator of efficient abstract machine implementations and its application to emulator minimization\n", "abstract": " The implementation of abstract machines involves complex decisions regarding, e.g., data representation, opcodes, or instruction specialization levels, all of which affect the final performance of the emulator and the size of the bytecode programs in ways that are often difficult to foresee. Besides, studying alternatives by implementing abstract machine variants is a time-consuming and error-prone task because of the level of complexity and optimization of competitive implementations, which makes them generally difficult to understand, maintain, and modify. This also makes it hard to generate specific implementations for particular purposes. To ameliorate those problems, we propose a systematic approach to the automatic generation of implementations of abstract machines. Different parts of their definition (e.g., the instruction set or the internal data and bytecode representation) are kept separate and\u00a0\u2026", "num_citations": "16\n", "authors": ["265"]}
{"title": "Exploiting goal independence in the analysis of logic programs\n", "abstract": " This paper illustrates the use of a top-down framework to obtain goal independent analyses of logic programs, a task which is usually associated with the bottom-up approach. While it is well known that the bottom-up approach can be used, through the magic set transformation, for goal dependent analysis, it is less known that the top-down approach can be used for goal independent analysis. The paper describes two ways of doing the latter. We show how the results of a goal independent analysis can be used to speed up subsequent goal dependent analyses. However this speed-up may result in a loss of precision. The influence of domain characteristics on this precision is discussed and an experimental evaluation using a generic top-down analyzer is described. Our results provide intuition regarding the cases where a two phase analysis might be worth-while.", "num_citations": "16\n", "authors": ["265"]}
{"title": "Flexible scheduling for non-deterministic, And-parallel execution of logic programs\n", "abstract": " We summarise our study of an important (but rarely examined) aspect of parallel execution in logic programming (LP): memory management, and the closely related issue of scheduling. We examine these issues in the context of implicit and-parallelism in nondeterministic programs, because it presents some of the most general problems (see [8] for justifications). This abstract is a highly condensed version of [8], and the reader is referred to that paper for details.We use the\" sub-tree\"(or\" multi-sequential\" approach), where the computation is divided into\" chunks\"(tasks) which are worked on by individual processing agents (workers) cooperatively in parallel. To retain much of the sequential efficiency of state of the art sequential LP systems while achieving performance improvements through parallelism, each task is executed by the worker in much the same way as in a sequential implementation, except that parallel\u00a0\u2026", "num_citations": "16\n", "authors": ["265"]}
{"title": "An Automatic Translation Scheme from Prolog to the Andorra Kernel Language.\n", "abstract": " The Andorra family of languages (which includes the Andorra Kernel Language {AKL) is aimed, in principle, at simultaneously supporting the programming styles of Prolog and committed choice languages. On the other hand, AKL requires a somewhat detailed speci cation of control by the user. This could be avoided by programming in Prolog to run on AKL. However, Prolog programs cannot be executed directly on AKL. This is due to a number of factors, from more or less trivial syntactic di erences to more involved issues such as the treatment of cut and making the exploitation of certain types of parallelism possible. This paper provides basic guidelines for constructing an automatic compiler of Prolog programs into AKL, which can bridge those di erences. In addition to supporting Prolog, our style of translation achieves independent and-parallel execution where possible, which is relevant since this type of parallel execution preserves, through the translation, the user-perceived\\complexity\" of the original Prolog program.", "num_citations": "16\n", "authors": ["265"]}
{"title": "Informatics Research Evaluation\n", "abstract": " Evaluation can be highly effective in improving research quality and productivity. To achieve the intended effects, research evaluation should follow established principles, benchmarked against appropriate criteria, and sensitive to disciplinary differences.", "num_citations": "15\n", "authors": ["265"]}
{"title": "Practical run-time checking via unobtrusive property caching\n", "abstract": " The use of annotations, referred to as assertions or contracts, to describe program properties for which run-time tests are to be generated, has become frequent in dynamic programing languages. However, the frameworks proposed to support such run-time testing generally incur high time and/or space overheads over standard program execution. We present an approach for reducing this overhead that is based on the use of memoization to cache intermediate results of check evaluation, avoiding repeated checking of previously verified properties. Compared to approaches that reduce checking frequency, our proposal has the advantage of being exhaustive (i.e., all tests are checked at all points) while still being much more efficient than standard run-time checking. Compared to the limited previous work on memoization, it performs the task without requiring modifications to data structure representation or checking\u00a0\u2026", "num_citations": "15\n", "authors": ["265"]}
{"title": "Sized type analysis for logic programs\n", "abstract": " We present a novel analysis for relating the sizes of terms and subterms occurring at different argument positions in logic predicates. We extend and enrich the concept of sized type as a representation that incorporates structural (shape) information and allows expressing both lower and upper bounds on the size of a set of terms and their subterms at any position and depth. For example, expressing bounds on the length of lists of numbers, together with bounds on the values of all of their elements. The analysis is developed using abstract interpretation and the novel abstract operations are based on setting up and solving recurrence relations between sized types. It has been integrated, together with novel resource usage and cardinality analyses, in the abstract interpretation framework in the Ciao preprocessor, CiaoPP, in order to assess both the accuracy of the new size analysis and its usefulness in the resource usage estimation application. We show that the proposed sized types are a substantial improvement over the previous size analyses present in CiaoPP, and also benefit the resource analysis considerably, allowing the inference of equal or better bounds than comparable state of the art systems.", "num_citations": "15\n", "authors": ["265"]}
{"title": "A general implementation framework for tabled CLP\n", "abstract": " This paper describes a framework to combine tabling evaluation and constraint logic programming (TCLP). While this combination has been studied previously from a theoretical point of view and some implementations exist, they either suffer from a lack of efficiency, flexibility, or generality, or have inherent limitations with respect to the programs they can execute to completion (either with success or failure). Our framework addresses these issues directly, including the ability to check for answer / call entailment, which allows it to terminate in more cases than other approaches. The proposed framework is experimentally compared with existing solutions in order to provide evidence of the mentioned advantages.", "num_citations": "15\n", "authors": ["265"]}
{"title": "An initial proposal for data-aware resource analysis of orchestrations with applications to predictive monitoring\n", "abstract": " Several activities in service oriented computing can benefit from knowing ahead of time future properties of a given service composition. In this paper we focus on how statically inferred computational cost functions on input data, which represent safe upper and lower bounds, can be used to predict some QoS-related values at runtime. In our approach, BPEL processes are translated into an intermediate language which is in turn converted into a logic program. Cost and resource analysis tools are applied to infer functions which, depending on the contents of some initial incoming message, return safe upper and lower bounds of some resource usage measure. Actual and predicted time characteristics are used to perform predictive monitoring. A validation is performed through simulation.", "num_citations": "15\n", "authors": ["265"]}
{"title": "Parallelizing irregular and pointer-based computations automatically: Perspectives from logic and constraint programming\n", "abstract": " Irregular computations pose some of the most interesting and challenging problems in automatic parallelization. Irregularity appears in certain kinds of numerical problems and is pervasive in symbolic applications. Such computations often use dynamic data structures, which make heavy use of pointers. This complicates all the steps of a parallelizing compiler, from independence detection to task partitioning and placement. Starting in the mid 80s there has been significant progress in the development of parallelizing compilers for logic programming (and more recently, constraint programming) resulting in quite capable parallelizers. The typical applications of these paradigms frequently involve irregular computations, and make heavy use of dynamic data structures with pointers, since logical variables represent in practice a well-behaved form of pointers. This arguably makes the techniques used in these compilers\u00a0\u2026", "num_citations": "15\n", "authors": ["265"]}
{"title": "A comparative study of methods for automatic compile-time parallelization of logic programs\n", "abstract": " This paper presents a study of the effectiveness of three different algorithms for the parallelization of logic programs based on compile-time detection of independence among goals. The algorithms are embedded in a complete parallelizing compiler, which incorporates different abstract interpretation-based program analyses. The complete system shows the task of automatic program parallelization to be practical. The trade-offs involved in using each of the algorithms in this task are studied experimentally, weaknesses of these identified, and possible improvements discussed.", "num_citations": "15\n", "authors": ["265"]}
{"title": "Goal dependent vs. goal independent analysis of logic programs\n", "abstract": " Goal independent analysis of logic programs is commonly discussed in the context of the bottom-up approach. However, while the literature is rich in descriptions of top-down analysers and their application, practical experience with bottom-up analysis is still in a preliminary stage. Moreover, the practical use of existing top-down frameworks for goal independent analysis has not been addressed in a practical system. We illustrate the efficient use of existing goal dependent, top-down frameworks for abstract interpretation in performing goal independent analyses of logic programs much the same as those usually derived from bottom-up frameworks. We present several optimizations for this flavour of top-down analysis. The approach is fully implemented within an existing top-down framework. Several implementation tradeoffs are discussed as well as the influence of domain characteristics. An experimental\u00a0\u2026", "num_citations": "15\n", "authors": ["265"]}
{"title": "Designing a high performance parallel logic programming system\n", "abstract": " Compilation techniques such as those portrayed by the Warren Abstract Machine (WAM) have greatly improved the speed of execution of logic programs. The research presented herein is geared towards providing additional performance to logic programs through the use of parallelism, while preserving the conventional semantics of logic languages. Two areas to which special attention is given are the preservation of sequential performance and storage efficiency, and the use of low overhead mechanisms for controlling parallel execution. Accordingly, the techniques used for supporting parallelism are efficient extensions of those which have brought high inferencing speeds to sequential implementations. At a lower level, special attention is also given to design and simulation detail and to the architectural implications of the execution model behavior. This paper offers an overview of the basic concepts and\u00a0\u2026", "num_citations": "15\n", "authors": ["265"]}
{"title": "A general framework for static profiling of parametric resource usage\n", "abstract": " For some applications, standard resource analyses do not provide the information required. Such analyses estimate the total resource usage of a program (without executing it) as functions on input data sizes. However, some applications require knowing how such total resource usage is distributed over selected parts of a program. We propose a novel, general, and flexible framework for setting up cost equations/relations which can be instantiated for performing a wide range of resource usage analyses, including both static profiling and the inference of the standard notion of cost. We extend and generalize standard resource analysis techniques, so that the relations generated include additional Boolean control variables for switching on or off different terms in the relations, as required by the desired resource usage profile. We also instantiate our framework to perform static profiling of accumulated cost (also\u00a0\u2026", "num_citations": "14\n", "authors": ["265"]}
{"title": "Profiling for run-time checking of computational properties and performance debugging in logic programs\n", "abstract": " Although several profiling techniques for identifying performance bottlenecks in logic programs have been developed, they are generally not automatic and in most cases they do not provide enough information for identifying the root causes of such bottlenecks. This complicates using their results for guiding performance improvement. We present a profiling method and tool that provides such explanations. Our profiler associates cost centers to certain program elements and can measure different types of resource-related properties that affect performance, preserving the precedence of cost centers in the call graph. It includes an automatic method for detecting procedures that are performance bottlenecks. The profiling tool has been integrated in a previously developed run-time checking framework to allow verification of certain properties when they cannot be verified statically. The approach allows\u00a0\u2026", "num_citations": "14\n", "authors": ["265"]}
{"title": "Experiments in context-sensitive analysis of modular programs\n", "abstract": " Several models for context-sensitive analysis of modular programs have been proposed, each with different characteristics and representing different trade-offs. The advantage of these context-sensitive analyses is that they provide information which is potentially more accurate than that provided by context-free analyses. Such information can then be applied to validating/debugging the program and/or to specializing the program in order to obtain important performance improvements. Some very preliminary experimental results have also been reported for some of these models, providing some initial evidence on their potential. However, further experimentation, needed in order to understand the many issues left open and to show that the proposed modes scale and are usable in the context of large, real-life modular programs, was left as future work. The aim of this paper is twofold. On one hand we\u00a0\u2026", "num_citations": "14\n", "authors": ["265"]}
{"title": "Some techniques for automated, resource-aware distributed and mobile computing in a multi-paradigm programming system\n", "abstract": " Distributed parallel execution systems speed up applications by splitting tasks into processes whose execution is assigned to different receiving nodes in a high-bandwidth network. On the distributing side, a fundamental problem is grouping and scheduling such tasks such that each one involves sufficient computational cost when compared to the task creation and communication costs and other such practical overheads. On the receiving side, an important issue is to have some assurance of the correctness and characteristics of the code received and also of the kind of load the particular task is going to pose, which can be specified by means of certificates. In this paper we present in a tutorial way a number of general solutions to these problems, and illustrate them through their implementation in the Ciao multi-paradigm language and program development environment. This system includes facilities for\u00a0\u2026", "num_citations": "14\n", "authors": ["265"]}
{"title": "Programming with Global Analysis\n", "abstract": " Global data-flow analysis of (constraint) logic programs, which is generally based on abstract interpretation [7], is reaching a comparatively high level of maturity. A natural question is whether it is time for its routine incorporation in standard compilers, something which, beyond a few experimental systems, has not happened to date. Such incorporation arguably makes good sense only if:\u2022 the range of applications of global analysis is large enough to justify the additional complication in the compiler, and\u2022 global analysis technology can deal with all the features of\" practical\" languages (eg, the ISO-Prolog built-ins) and\" scales up\" for large programs.We present a tutorial overview of a number of concepts and techniques directly related to the issues above, with special emphasis on the first one. In particular, we concentrate on novel uses of global analysis during program development and debugging, rather than on the more traditional application area of program optimization.", "num_citations": "14\n", "authors": ["265"]}
{"title": "Towards integrating partial evaluation in a specialization framework based on generic abstract interpretation\n", "abstract": " Partial evaluation JGS93, DGT96] specializes programs for known values of the input. Partial evaluation of logic programs has received considerable attention Neu90, LS91, Sah93, Gal93, Leu97] and several algorithms parameterized by di erent control strategies have been proposed which produce useful partial evaluations of programs. Regarding the correctness of such transformations, two conditions, de ned on the set of atoms to be partially evaluated, have been identi ed which which ensure correctness of the transformation:\\closedness\" and\\independence\" LS91]. From a practical point of view, e ectiveness, that is, nding suitable control strategies which provide an appropriate level of specialization while ensuring termination, is a crucial problem which has also received considerable attention. Much work has been devoted to the study of such control strategies in the context of\\on-line\" partial evaluation of logic programs MG95, LD97, LM96]. Usually, control is divided into components:\\local control,\" which controls the unfolding for a given atom, and\\global control,\" which ensures that the set of atoms for which a partial evaluation is to be computed remains nite.In most of the practical program specialization algorithms, the above mentioned control strategies use, to a greater or lesser degree, information generated by static program analysis. One of the most widely used techniques for static analysis is abstract interpretation CC77, CC92]. Some of the relations between abstract interpretation and partial evaluation have been identi ed before GCS88, GH91, Gal92, CK93, PH95, LS96]. However, the role of analysis is so fundamental that it can be\u00a0\u2026", "num_citations": "14\n", "authors": ["265"]}
{"title": "Assertion-based debugging of higher-order (C) LP programs\n", "abstract": " Higher-order constructs extend the expressiveness of first-order (Constraint) Logic Programming ((C) LP) both syntactically and semantically. At the same time assertions have been in use for some time in (C) LP systems helping programmers detect errors and validate programs. However, these assertion-based extensions to (C) LP have not been integrated well with higher-order to date. This paper contributes to filling this gap by extending the assertion-based approach to error detection and program verification to the higher-order context within (C) LP. We propose an extension of properties and assertions as used in (C) LP in order to be able to fully describe arguments that are predicates. The extension makes the full power of the assertion language available when describing higher-order arguments. We provide syntax and semantics for (higher-order) properties and assertions, as well as for programs which\u00a0\u2026", "num_citations": "13\n", "authors": ["265"]}
{"title": "Interval-based resource usage verification: formalization and prototype\n", "abstract": " In an increasing number of applications (e.g., in embedded, real-time, or mobile systems) it is important or even essential to ensure conformance with respect to a specification expressing the use of some resource, such as execution time, energy, or user-defined resources. In previous work we have presented a novel framework for data size-dependent, static resource usage verification (which can also be combined with run-time tests). Specifications can include both lower and upper bound resource usage functions. In order to statically check such specifications, both upper- and lower-bound resource usage functions (on input data sizes) approximating the actual resource usage of the program are automatically inferred and compared against the specification. The outcome of the static checking of assertions can express intervals for the input data sizes such that a given specification can be proved for some\u00a0\u2026", "num_citations": "13\n", "authors": ["265"]}
{"title": "Comparing tag scheme variations using an abstract machine generator\n", "abstract": " In this paper we study, in the context of a WAM-based abstract machine for Prolog, how variations in the encoding of type information in tagged words and in their associated basic operations impact performance and memory usage. We use a high-level language to specify encodings and the associated operations. An automatic generator constructs both the abstract machine using this encoding and the associated Prolog-to-bytecode compiler. Annotations in this language make it possible to impose constraints on the final representation of tagged words, such as the effectively addressable space (fixing, for example, the word size of the target processor/architecture), the layout of the tag and value bits inside the tagged word, and how the basic operations are implemented. We evaluate a large number of combinations of the different parameters in two scenarios: a) trying to obtain an optimal general-purpose abstract\u00a0\u2026", "num_citations": "13\n", "authors": ["265"]}
{"title": "A practical type analysis for verification of modular prolog programs\n", "abstract": " Regular types are a powerful tool for computing very precise descriptive types for logic programs. However, in the context of real-life, modular Prolog programs, the accurate results obtained by regular types often come at the price of efficiency. In this paper we propose a combination of techniques aimed at improving analysis efficiency in this context. As a first technique we allow optionally reducing the accuracy of inferred types by using only the types defined by the user or present in the libraries. We claim that, for the purpose of verifying type signatures given in the form of assertions the precision obtained using this approach is sufficient, and show that analysis times can be reduced significantly. Our second technique is aimed at dealing with situations where we would like to limit the amount of reanalysis performed, especially for library modules. Borrowing some ideas from polymorphic type systems, we show how\u00a0\u2026", "num_citations": "13\n", "authors": ["265"]}
{"title": "The ciao module system: A new module system for prolog\n", "abstract": " It is now widely accepted that separating programs into modules has proven very useful in program development and maintenance. While many Prolog implementations include useful module systems, we feel that these systems can be improved in a number of ways, such as, for example, being more amenable to effective global analysis and transformation and allowing separate compilation or sensible creation of standalone executables. We discuss a number of issues related to the design of such an improved module system for Prolog. Based on this, we present the choices made in the Ciao module system, which has been designed to meet a number of objectives: allowing separate compilation, extensibility in features and in syntax, amenability to modular global analysis and transformation, etc.", "num_citations": "13\n", "authors": ["265"]}
{"title": "Interfacing Prolog and VRML and its application to constraint visualization\n", "abstract": " A number of data description languages initially designed as standards for trie WWW are currently being used to implement user interfaces to programs. This is done independently of whether such programs are executed in the same or a different host as trie one running the user interface itself. The advantage of this approach is that it provides a portable, standardized, and easy to use solution for the application programmer, and a familiar behavior for the user, typically well versed in the use of WWW browsers. Among the proposed standard description languages, VRML is a aimed at representing three dimensional scenes including hyperlink capabilities. VRML is already used as an  import/export format in many 3-D packages and tools, and has been shown effective in displaying complex objects and scenarios. We propose and describe a Prolog library which allows parsing and checking VRML code, transforming it, and writing it out as VRML again. The library converts such code to an internal representation based on first order terms which can then be arbitrarily manipulated. We also present as an example application the use of this library to implement a novel 3-D visualization for examining and understanding certain aspects of the behavior of  CLP(FD) programs.", "num_citations": "13\n", "authors": ["265"]}
{"title": "Some challenges for constraint programming\n", "abstract": " We propose a number of challenges for future constraint programming systems, including improvements in implementation technology (using global analysis based optimization and parallelism), debugging facilities, and the extension of the application domain to distributed, global programming. We also briefly discuss how we are exploring techniques to meet these challenges in the context of the development of the CIAO constraint logic programming system.", "num_citations": "13\n", "authors": ["265"]}
{"title": "Relating data-parallelism and (And-) parallelism in logic programs\n", "abstract": " Much work has been done in the areas of and-parallelism and data-parallelism in Logic Programs. Both types of parallelism offer advantages and disadvantages: traditional (and-) parallel models offer generality, whereas data-parallelism techniques offer increased performance for a restricted class of programs. The thesis of this paper is that these two forms of parallelism are not fundamentally different and that relating them opens the possibility of obtaining the advantages of both within the same system. Some relevant issues are discussed and solutions proposed. The discussion is illustrated through visualizations of actual parallel executions implementing the ideas proposed.", "num_citations": "13\n", "authors": ["265"]}
{"title": "Towards description and optimization of abstract machines in an extension of prolog\n", "abstract": " Competitive abstract machines for Prolog are usually large, intricate, and incorporate sophisticated optimizations. This makes them difficult to code, optimize, and, especially, maintain and extend. This is partly due to the fact that efficiency considerations make it necessary to use low-level languages in their implementation. Writing the abstract machine (and ancillary code) in a higher-level language can help harness this inherent complexity. In this paper we show how the semantics of basic components of an efficient virtual machine for Prolog can be described using (a variant of) Prolog which retains much of its semantics. These descriptions are then compiled to C and assembled to build a complete bytecode emulator. Thanks to the high level of the language used and its closeness to Prolog the abstract machine descriptions can be manipulated using standard Prolog compilation and optimization\u00a0\u2026", "num_citations": "12\n", "authors": ["265"]}
{"title": "Inferring energy bounds statically by evolutionary analysis of basic blocks\n", "abstract": " devices, including in some cases mission critical systems, for which there is a need to optimize their energy consumption and verify that they will perform their function within the available energy budget. In this work we propose a novel parametric approach to estimating tight energy bounds (both upper and lower) that are practical for energy verification and optimization applications in embedded systems. Our approach consists in dividing a program into basic (\u201cbranchless\u201d) blocks, establishing the maximal (resp. minimal) energy consumption for each block using an evolutionary algorithm, and combining the obtained values according to the program control flow, using static analysis, to produce energy bound functions. Such functions depend on input data sizes, and return upper or lower bounds on the energy consumption of the program for any given set of input values of those sizes, without running the program. The approach has been tested on XMOS chips, but is general enough to be applied to any microprocessor and programming language. Our experimental results show that the bounds obtained by our prototype tool can be tight while remaining on the safe side of budgets in practice.", "num_citations": "11\n", "authors": ["265"]}
{"title": "O\u2019CIAO: An Object Oriented Programming Model Using CIAO Prolog\n", "abstract": " There have been several previous proposals for the integration of Object Oriented Programming features into Logic Programming, resulting in much support theory and several language proposals. However, none of these proposals seem to have made it into the mainstream. Perhaps one of the reasons for these is that the resulting languages depart too much from the standard logic programming languages to entice the average Prolog programmer. Another reason may be that most of what can be done with object-oriented programming can already be done in Prolog through the meta-and higher-order programming facilities that the language includes, albeit sometimes in a more cumbersome way. In light of this, in this paper we propose an alternative solution which is driven by two main objectives. The first one is to include only those characteristics of object-oriented programming which are cumbersome to implement in standard Prolog systems. The second one is to do this in such a way that there is minimum impact on the syntax and complexity of the language, ie, to introduce the minimum number of new constructs, declarations, and concepts to be learned. Finally, we would like the implementation to be as straightforward as possible, ideally based on simple source to source expansions.", "num_citations": "11\n", "authors": ["265"]}
{"title": "The Ciao System\n", "abstract": " This is the Reference Manual for the Ciao development system. It contains basic information on how to install Ciao and how to write, debug, and run Ciao programs from the command line, from inside GNU emacs, or from a windowing desktop. It also documents all the libraries available in the standard distribution.This manual has been generated using the LPdoc semi-automatic documentation generator for LP/CLP programs [HC97, Her00]. lpdoc processes Ciao files (and files in Prolog and other CLP languages) adorned with assertions and machine-readable comments, which should be written in the Ciao assertion language [PBH97, PBH00]. From these, it generates manuals in many formats including postscript, pdf, texinfo, info, HTML, man, etc., as well as on-line help, ascii README files, entries for indices of manuals (info, WWW,...), and maintains WWW distribution sites.", "num_citations": "11\n", "authors": ["265"]}
{"title": "Divided We Stand: Parallel Distributed Stack Memory Management\n", "abstract": " We present an overview of the stack-based memory management techniques that we used in our non-deterministic and-parallel Prolog systems: &-Prolog and DASWAM. We believe that the problems associated with non-deterministic and-parallel systems are more general than those encountered in or-parallel and deterministic and-parallel systems, which can be seen as subsets of this more general case. We develop on the previously proposed \u201cmarker scheme\u201d, lifting some of the restrictions associated with the selection of goals while keeping (virtual) memory consumption down. We also review some of the other problems associated with the stack-based management scheme, such as handling of forward and backward execution, cut, and rollbacks.", "num_citations": "11\n", "authors": ["265"]}
{"title": "The &\u2013Prolog Compiler System\u2014Automatic Parallelization Tools for LP\n", "abstract": " This report presents an overview of the current work performed by us in the context of the efficient parallel implementation of traditional logic programming systems. The work is based on the &-Prolog System, a system for the automatic parallelization and execution of logic programming languages within the Independent And-parallelism model, and the global analysis and parallelization tools which have been developed for this system. In order to make the report self-contained, we first describe the\" classical\" tools of the &-Prolog system. We then explain in detail the work performed in improving and generalizing the global analysis and parallelization tools. Also, we describe the objectives which will drive our future work in this area.", "num_citations": "11\n", "authors": ["265"]}
{"title": "B-Log: A Branch and Bound Methodology for the Parallel Execution of Logic Programs.\n", "abstract": " We propose a computational methodology-\" B-LOG\"-, which offers the potential for an effective implementation of Logic Programming in a parallel computer. We also propose a weighting scheme to guide the search process through the graph and we apply the concepts of parallel\" branch and bound\" algorithms in order to perform a\" best-first\" search using an information theoretic bound. The concept of\" session\" is used to speed up the search process in a succession of similar queries. Within a session, we strongly modify the bounds in a local database, while bounds kept in a global database are weakly modified to provide a better initial condition for other sessions. We also propose an implementation scheme based on a database machine using\" semantic paging\", and the\" B-LOG processor\" based on a scoreboard driven controller.", "num_citations": "11\n", "authors": ["265"]}
{"title": "Cost analysis of smart contracts via parametric resource analysis\n", "abstract": " The very nature of smart contracts and blockchain platforms, where program execution and storage are replicated across a large number of nodes, makes resource consumption analysis highly relevant. This has led to the development of analyzers for specific platforms and languages. However, blockchain platforms present significant variability in languages and cost models, as well as over time. Approaches that facilitate the quick development and adaptation of cost analyses are thus potentially attractive in this context. We explore the application of a generic approach and tool for cost analysis to the problem of static inference of gas consumption bounds in smart contracts. The approach is based on Parametric Resource Analysis, a method that simplifies the implementation of analyzers for inferring safe bounds on different resources and with different resource consumption models. In addition, to support\u00a0\u2026", "num_citations": "10\n", "authors": ["265"]}
{"title": "From big-step to small-step semantics and back with interpreter specialisation\n", "abstract": " We investigate representations of imperative programs as constrained Horn clauses. Starting from operational semantics transition rules, we proceed by writing interpreters as constrained Horn clause programs directly encoding the rules. We then specialise an interpreter with respect to a given source program to achieve a compilation of the source language to Horn clauses (an instance of the first Futamura projection). The process is described in detail for an interpreter for a subset of C, directly encoding the rules of big-step operational semantics for C. A similar translation based on small-step semantics could be carried out, but we show an approach to obtaining a small-step representation using a linear interpreter for big-step Horn clauses. This interpreter is again specialised to achieve the translation from big-step to small-step style. The linear small-step program can be transformed back to a big-step non-linear program using a third interpreter. A regular path expression is computed for the linear program using Tarjan's algorithm, and this regular expression then guides an interpreter to compute a program path. The transformation is realised by specialisation of the path interpreter. In all of the transformation phases, we use an established partial evaluator and exploit standard logic program transformation to remove redundant data structures and arguments in predicates and rename predicates to make clear their link to statements in the original source program.", "num_citations": "10\n", "authors": ["265"]}
{"title": "Inferring energy bounds via static program analysis and evolutionary modeling of basic blocks\n", "abstract": " The ever increasing number and complexity of energy-bound devices (such as the ones used in Internet of Things applications, smart phones, and mission critical systems) pose an important challenge on techniques to optimize their energy consumption and to verify that they will perform their function within the available energy budget. In this work we address this challenge from the software point of view and propose a novel approach to estimating accurate parametric bounds on the energy consumed by program executions that are practical for their application to energy verification and optimization. Our approach divides a program into basic (branchless) blocks and performs a best effort modeling to estimate upper and lower bounds on the energy consumption for each block using an evolutionary algorithm. Then it combines the obtained values according to the program control flow, using a safe static\u00a0\u2026", "num_citations": "10\n", "authors": ["265"]}
{"title": "Semantic code browsing\n", "abstract": " Programmers currently enjoy access to a very high number of code repositories and libraries of ever increasing size. The ensuing potential for reuse is however hampered by the fact that searching within all this code becomes an increasingly difficult task. Most code search engines are based on syntactic techniques such as signature matching or keyword extraction. However, these techniques are inaccurate (because they basically rely on documentation) and at the same time do not offer very expressive code query languages. We propose a novel approach that focuses on querying for semantic characteristics of code obtained automatically from the code itself. Program units are pre-processed using static analysis techniques, based on abstract interpretation, obtaining safe semantic approximations. A novel, assertion-based code query language is used to express desired semantic characteristics of the code as\u00a0\u2026", "num_citations": "10\n", "authors": ["265"]}
{"title": "Towards a high-level implementation of execution primitives for unrestricted, independent and-parallelism\n", "abstract": " Most efficient implementations of parallel logic programming rely on complex low-level machinery which is arguably difficult to implement and modify. We explore an alternative approach aimed at taming that complexity by raising core parts of the implementation to the source language level for the particular case of and-parallelism. We handle a significant portion of the parallel implementation at the Prolog level with the help of a comparatively small number of concurrency-related primitives which take care of lower-level tasks such as locking, thread management, stack set management, etc. The approach does not eliminate altogether modifications to the abstract machine, but it does greatly simplify them and it also facilitates experimenting with different alternatives. We show how this approach allows implementing both restricted and unrestricted (i.e., non fork-join) parallelism. Preliminary experiments show\u00a0\u2026", "num_citations": "10\n", "authors": ["265"]}
{"title": "The Ciao Preprocessor\n", "abstract": " CiaoPP is the abstract interpretation-based preprocessor of the Ciao multi-paradigm program development environment. CiaoPP can perform a number of program debugging, analysis, and source-to-source transformation tasks on (Ciao) Prolog programs. These tasks include:\u2022 Inference of properties of the predicates and literals of the program, including types, modes and other variable instantiation properties, non-failure, determinacy, bounds on computational cost, bounds on sizes of terms in the program, etc.\u2022 Certain kinds of static debugging and verification, finding errors before running the program. This includes checking how programs call system library predicates and also checking the assertions present in the program or in other modules used by the program. Such assertions represent essentially partial specifications of the program.\u2022 Several kinds of source to source program transformations such as program specialization, slicing, partial evaluation of a program, program parallelization (taking granularity control into account), inclusion of run-time tests for assertions which cannot be checked completely at compile-time, etc.", "num_citations": "10\n", "authors": ["265"]}
{"title": "Abstract verification and debugging of constraint logic programs\n", "abstract": " The technique of Abstract Interpretation [13] has allowed the development of sophisticated program analyses which are provably correct and practical. The semantic approximations produced by such analyses have been traditionally applied to optimization during program compilation. However, recently, novel and promising applications of semantic approximations have been proposed in the more general context of program verification and debugging [3],[10],[7].", "num_citations": "10\n", "authors": ["265"]}
{"title": "IDRA (IDeal resource allocation): Computing ideal speedups in parallel logic programming\n", "abstract": " We present a technique to estimate accurate speedups for parallel logic programs with relative independence from characteristics of a given implementation or underlying parallel hardware. The proposed technique is based on gathering accurate data describing one execution at run-time, which is fed to a simulator. Alternative schedulings are then simulated and estimates computed for the corresponding speedups. A tool implementing the aforementioned techniques is presented, and its predictions are compared to the performance of real systems, showing good correlation.", "num_citations": "10\n", "authors": ["265"]}
{"title": "A constraint-based approach to quality assurance in service choreographies\n", "abstract": " The knowledge about the quality characteristics (QoS) of service compositions is crucial for determining their usability and economic value; the quality of service compositions is usually regulated using Service Level Agreements (SLAs). While end-to-end SLAs are well suited for request-reply interactions, more complex, decentralized, multi-participant compositions (service choreographies) typically need multiple message exchanges between stateful parties and the corresponding SLAs thus involve several cooperating parties with interdependent QoS. The usual approaches to determining QoS ranges structurally (which are by construction easily composable) are not applicable in this scenario. Additionally, the intervening SLAs may depend on the exchanged data. We present an approach to data-aware QoS assurance in choreographies through the automatic derivation of composable QoS models from\u00a0\u2026", "num_citations": "9\n", "authors": ["265"]}
{"title": "Automated attribute inference in complex service workflows based on sharing analysis\n", "abstract": " The properties of data and activities in business processes can be used to greatly facilitate several relevant tasks performed at design-and run-time, such as fragmentation, compliance checking, or top-down design. Business processes are often described using workflows, and we present an approach to mechanically infer business domain-specific attributes of workflow components, including data items, activities, and elements of sub-workflows, from known attributes of workflow inputs and the structure of the workflow by modeling these components as concepts and applying sharing analysis applied to a Horn clause representation of the workflow. The analysis is applicable to workflows featuring complex control and data dependencies, embedded control constructs, such as loops and branches, and embedded component services.", "num_citations": "9\n", "authors": ["265"]}
{"title": "The Ciao approach to the dynamic vs. static language dilemma\n", "abstract": " The environment in which much software needs to be developed nowadays (decoupled software development, use of components and services, increased interoperability constraints, need for dynamic update or self-reconfiguration, mash-up development, etc.) is posing requirements which align with the classical arguments for dynamic languages and which in fact go beyond them. Examples of often required dynamic features include making it possible to (partially) test and verify applications which are partially developed and which will never be \u201ccomplete\u201d or \u201cfinal,\u201d or which evolve over time in an asynchronous, decentralized fashion (eg, software service-based systems). These requirements, coupled with the intrinsic agility in development of dynamic programming languages such as Python, Ruby, Lua, JavaScript, Perl, PHP, etc.(with Scheme or Prolog also in this class) have made such languages a very attractive option for a number of purposes that go well beyond simple scripting. Parts written in these languages often become essential components (or even the whole implementation) of full, mainstream applications.At the same time, detecting errors at compile-time and inferring many properties required in order to optimize programs are still important issues in realworld applications. Thus, strong arguments are still also made in favor of static languages. For example, many modern logic and functional languages (such as, eg, Mercury [24] or Haskell [12]) impose strong type-related requirements such as that all types (and, when relevant, modes) have to be defined explicitly or", "num_citations": "9\n", "authors": ["265"]}
{"title": "Memory Performance of AND-parallel Prolog on Shared-Memory Architectures\n", "abstract": " The goal of the RAP-WAM AND-parallel Prolog abstract architecture is to provide inference speeds significantly beyond those of sequential systems, while supporting Prolog semantics and preserving sequential performance and storage efficiency. This paper presents simulation results supporting these claims with special emphasis on memory performance on a two-level sharedmemory multiprocessor organization. Several solutions to the cache coherency problem are analyzed. It is shown that RAP-WAM offers good locality and storage efficiency and that it can effectively take advantage of broadcast caches. It is argued that speeds in excess of 2 ML IPS on real applications exhibiting medium parallelism can be attained with current technology.", "num_citations": "9\n", "authors": ["265"]}
{"title": "Incremental and Modular Context-sensitive Analysis\n", "abstract": " Context-sensitive global analysis of large code bases can be expensive, which can make its use impractical during software development. However, there are many situations in which modifications are small and isolated within a few components, and it is desirable to reuse as much as possible previous analysis results. This has been achieved to date through incremental global analysis fixpoint algorithms that achieve cost reductions at fine levels of granularity, such as changes in program lines. However, these fine-grained techniques are neither directly applicable to modular programs nor are they designed to take advantage of modular structures. This paper describes, implements, and evaluates an algorithm that performs efficient context-sensitive analysis incrementally on modular partitions of programs. The experimental results show that the proposed modular algorithm shows significant improvements, in\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Incremental analysis of logic programs with assertions and open predicates\n", "abstract": " Generic components are a further abstraction over the concept of modules, introducing dependencies on other (not necessarily available) components implementing specified interfaces. They have become a key concept in large and complex software applications. Despite undeniable advantages, generic code is also anti-modular. Precise analysis (e.g., for detecting bugs or optimizing code) requires such code to be instantiated with concrete implementations, potentially leading to expensive combinatorial explosion. In this paper we claim that incremental, whole program analysis can be very beneficial in this context, and alleviate the anti-modularity nature of generic code. We propose a simple Horn-clause encoding of generic programs, using open predicates and assertions, and we introduce a new incremental, multivariant analysis algorithm that reacts incrementally not only to changes in program clauses, but\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Computing abstract distances in logic programs\n", "abstract": " Abstract interpretation is a well-established technique for performing static analyses of logic programs. However, choosing the abstract domain, widening, fixpoint, etc. that provides the best precision-cost trade-off remains an open problem. This is in a good part because of the challenges involved in measuring and comparing the precision of different analyses. We propose a new approach for measuring such precision, based on defining distances in abstract domains and extending them to distances between whole analyses of a given program, thus allowing comparing precision across different analyses. We survey and extend existing proposals for distances and metrics in lattices or abstract domains, and we propose metrics for some common domains used in logic program analysis, as well as extensions of those metrics to the space of whole program analyses. We implement those metrics within the CiaoPP\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Static Performance Guarantees for Programs with Runtime Checks\n", "abstract": " Instrumenting programs for performing runtime checking of properties, such as regular shapes, is a common and useful technique that helps programmers detect incorrect program behaviors. This is specially true in dynamic languages such as Prolog. However, such runtime checks inevitably introduce runtime overhead (in execution time, memory, energy, etc.). Several approaches have been proposed for reducing this overhead, such as eliminating the checks that can statically be proved to always succeed, and/or optimizing the way in which the (remaining) checks are performed. However, there are cases in which it is not possible to remove all checks statically (eg, open libraries which must check their interfaces, complex properties, unknown code, etc.) and in which, even after optimizations, these remaining checks may still introduce an unacceptable level of overhead. It is thus important for programmers to be\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Some trade-offs in reducing the overhead of assertion run-time checks via static analysis\n", "abstract": " A number of approaches for helping programmers detect incorrect program behaviors are based on combining language-level constructs (such as procedure-level assertions/contracts, program-point assertions, or gradual types) with a number of associated tools (such as code analyzers and run-time verification frameworks) that automatically check the validity of such constructs. However, these constructs and tools are often not used to their full extent in practice due to excessive run-time overhead, limited expressiveness, and/or limitations in the effectiveness of the tools. Verification frameworks that combine static and dynamic verification techniques and are based on abstraction offer the potential to bridge this gap. In this paper we explore the effectiveness of abstract interpretation in detecting parts of program specifications that can be statically simplified to true or false, as well as in reducing the cost of the run\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Towards energy consumption verification via static analysis\n", "abstract": " In this paper we leverage an existing general framework for resource usage verification and specialize it for verifying energy consumption specifications of embedded programs. Such specifications can include both lower and upper bounds on energy usage, and they can express intervals within which energy usage is to be certified to be within such bounds. The bounds of the intervals can be given in general as functions on input data sizes. Our verification system can prove whether such energy usage specifications are met or not. It can also infer the particular conditions under which the specifications hold. To this end, these conditions are also expressed as intervals of functions of input data sizes, such that a given specification can be proved for some intervals but disproved for others. The specifications themselves can also include preconditions expressing intervals for input data sizes. We report on a prototype implementation of our approach within the CiaoPP system for the XC language and XS1-L architecture, and illustrate with an example how embedded software developers can use this tool, and in particular for determining values for program parameters that ensure meeting a given energy budget while minimizing the loss in quality of service.", "num_citations": "8\n", "authors": ["265"]}
{"title": "Certificate size reduction in abstraction-carrying code\n", "abstract": " Abstraction-Carrying Code (ACC) has recently been proposed as a framework for mobile code safety in which the code supplier provides a program together with an abstraction (or abstract model of the program) whose validity entails compliance with a predefined safety policy. The abstraction plays thus the role of safety certificate and its generation is carried out automatically by a fixpoint analyzer. The advantage of providing a (fixpoint) abstraction to the code consumer is that its validity is checked in a single pass (i.e., one iteration) of an abstract interpretation-based checker. A main challenge to make ACC useful in practice is to reduce the size of certificates as much as possible while at the same time not increasing checking time. The intuitive idea is to only include in the certificate information that the checker is unable to reproduce without iterating. We introduce the notion of reduced certificate which\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Abductive inference in probabilistic logic programs\n", "abstract": " Action-probabilistic logic programs (ap-programs) are a class of probabilistic logic programs that have been extensively used during the last few years for modeling behaviors of entities. Rules in ap-programs have the form\" If the environment in which entity E operates satisfies certain conditions, then the probability that E will take some action A is between L and U\". Given an ap-program, we are interested in trying to change the environment, subject to some constraints, so that the probability that entity E takes some action (or combination of actions) is maximized. This is called the Basic Probabilistic Logic Abduction Problem (Basic PLAP). We first formally define and study the complexity of Basic PLAP and then provide an exact (exponential) algorithm to solve it, followed by more efficient algorithms for specific subclasses of the problem. We also develop appropriate heuristics to solve Basic PLAP efficiently.", "num_citations": "8\n", "authors": ["265"]}
{"title": "Methods and methodologies for developing answer-set programs-Project description\n", "abstract": " Answer-set programming (ASP) is a well-known formalism for declarative problem solving, enjoying a continuously increasing number of diverse applications. However, arguably one of the main challenges for a wider acceptance of ASP is the need of tools, methods, and methodologies that support the actual programming process. In this paper, we review the main goals of a project, funded by the Austrian Science Fund (FWF), which aims to address this aspect in a systematic manner. The project is planned for a duration of three years and started in September 2009. Generally, the focus of research will be on methodologies for systematic program development, program testing, and debugging. In particular, in working on these areas, special emphasis shall be given to the ability of the developed techniques to respect the declarative nature of ASP. To support a sufficient level of usability, solutions are planned to be compatible not only for the core language of ASP but also for important extensions thereof that are commonly used and realised in various answer-set solvers. Ultimately, the methods resulting from the project shall form the basis of an integrated development environment (IDE) for ASP that is envisaged to combine straightforward as well as advanced techniques, realising a convenient tool for developing answer-set programs.", "num_citations": "8\n", "authors": ["265"]}
{"title": "Towards a complete scheme for tabled execution based on program transformation\n", "abstract": " The advantages of tabled evaluation regarding program termination and reduction of complexity are well known \u2014as are the significant implementation, portability, and maintenance efforts that some proposals (especially those based on suspension) require. This implementation effort is reduced by program transformation-based continuation call techniques, at some efficiency cost. However, the traditional formulation of this proposal\u00a0[1] limits the interleaving of tabled and non-tabled predicates and thus cannot be used as-is for arbitrary programs. In this paper we present a complete translation for the continuation call technique which, while requiring the same runtime support as the traditional approach, solves these problems and makes it possible to execute arbitrary tabled programs. We also present performance results which show that the resulting CCall approach offers a useful tradeoff that can be\u00a0\u2026", "num_citations": "8\n", "authors": ["265"]}
{"title": "Customizable resource usage analysis for java bytecode\n", "abstract": " Automatic cost analysis of programs has been traditionally studied in terms of a number of concrete, predefined resources such as execution steps, time, or memory. However, the increasing relevance of analysis applications such as static debugging and/or certification of user-level properties (including for mobile code) makes it interesting to develop analyses for resource notions that are actually applicationdependent. This may include, for example, bytes sent or received by an application, number of files left open, number of SMSs sent or received, number of accesses to a database, money spent, energy consumption, etc. We present a fully automated analysis for inferring upper bounds on the usage that a Java bytecode program makes of a set of application programmer-definable resources. In our context, a resource is defined by programmer-provided annotations which state the basic consumption that certain program elements make of that resource. From these definitions our analysis derives functions which return an upper bound on the usage that the whole program (and individual blocks) make of that resource for any given set of input data sizes. The analysis proposed is independent of the particular resource. We also present some experimental results from a prototype implementation of the approach covering an ample set of interesting resources.", "num_citations": "8\n", "authors": ["265"]}
{"title": "A simple approach to distributed objects in prolog\n", "abstract": " We present the design of a distributed object system for Prolog, based on adding remote execution and distribution capabilities to a previously existing object system. Remote execution brings RPC into a Prolog system, and its semantics is easy to express in terms of well-known Prolog builtins. The final distributed object design features state mobility and user-transparent network behavior. We sketch an implementation which provides distributed garbage collection and some degree of tolerance to network failures. We provide a preliminary study of the overhead of the communication mechanism for some test cases.", "num_citations": "8\n", "authors": ["265"]}
{"title": "WebDB: A Database WWW Interface\n", "abstract": " WebDB is at the same time:\u2022 A tool which allows browsing and modifying interactively and remotely using any WWW browser many kinds of relational (SQL) databases which may reside in different locations and operating systems.\u2022 A customizable relational (logic) database system with a WWW-based user-interface. In this case the internal databases are kept as WebDB native tables, which are stored as Prolog persistent predicates and can also be consulted/edited from a Prolog program or even by using a text editor.", "num_citations": "8\n", "authors": ["265"]}
{"title": "IDRA (IDeal Resource Allocation): A Tool for Computing Ideal Speedups.\n", "abstract": " Performance studies of actual parallel systems usually tend to conc\u00e9ntrate on the effectiveness of a given implementation. This is often done in the absolute, without quantitave reference to the potential parallelism contained in the programs from the point of view of the execution paradigm. We feel that studying the parallelism inherent to the programs is interesting, as it gives information about the best possible behavior of any implementation and thus allows contrasting the results obtained. We propose a method for obtaining ideal speedups for programs through a combination of sequential or parallel execution and simulation, and the algorithms that allow implementing the method. Our approach is novel and, we arg\u00fce, more accurate than previously proposed methods, in that a crucial part of the data-the execution times of tasks-is obtained from actual executions, while speedup is computed by simulation. This allows obtaining speedup (and other) data under controlled and ideal assumptions regarding issues such as number of processor, scheduling algorithm and overheads, etc. The results obtained can be used for example to eval\u00faate the ideal parallelism that a program contains for a given model of execution and to compare such\" perfect\" parallelism to that obtained by a given implementation of that model. We also present a tool, IDRA, which implements the proposed method, and results obtained with IDRA for benchmark programs, which are then compared with those obtained in actual executions on real parallel systems.", "num_citations": "8\n", "authors": ["265"]}
{"title": "Multivariant assertion-based guidance in abstract interpretation\n", "abstract": " Approximations during program analysis are a necessary evil, as they ensure essential properties, such as soundness and termination of the analysis, but they also imply not always producing useful results. Automatic techniques have been studied to prevent precision loss, typically at the expense of larger resource consumption. In both cases (i.e., when analysis produces inaccurate results and when resource consumption is too high), it is necessary to have some means for users to provide information to guide analysis and thus improve precision and/or performance. We present techniques for supporting within an abstract interpretation framework a rich set of assertions that can deal with multivariance/context-sensitivity, and can handle different run-time semantics for those assertions that cannot be discharged at compile time. We show how the proposed approach can be applied to both improving\u00a0\u2026", "num_citations": "7\n", "authors": ["265"]}
{"title": "Towards a high-level implementation of flexible parallelism primitives for symbolic languages\n", "abstract": " The advent of multicore processors is bringing renewed interest in parallelism and, accordingly, in the development of languages and tools to simplify the task of writing parallel programs. This is especially important for the complex, non-regular algorithms often found in software which performs non-trivial symbolic tasks. Such software can benefit from being written in a high-level language whose nature is symbolic as well, since this narrows the gap between the conceptual definition of the task to be performed and the code which executes it. In our case we will use for concreteness a logic-based multiparadigm language, Ciao [1], which is based on a logic-programming kernel and a flexible mechanism whereby multiple extensions are built supporting Prolog, functional programming, constraint programming, and other system-and user-level languages. The base language and system features dynamic typing, higher\u00a0\u2026", "num_citations": "7\n", "authors": ["265"]}
{"title": "Conferences vs. journals in CS, what to do? Evolutionary ways forward and the ICLP/TPLP model\n", "abstract": " CS Other sciences publish mostly in conferences journals are the norm proceedings fully refereed just communication publications vehicles, abstracts perceived to be of better quality, conference papers more prestigious than most journals worthless top-level CS researchers publish all publications are sparsely in journals in journals journal papers very long (40-50 pages!) short journal publication takes years takes a few months no problem: results already speed important, no published in conference other venue! role of journal paper is to complete done in monographs work: proofs, comprehensive/books? experimental results, etc.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Lightweight compilation of (C) LP to JavaScript\n", "abstract": " We present and evaluate a compiler from Prolog (and extensions) to JavaScript which makes it possible to use (constraint) logic programming to develop the client side of web applications while being compliant with current industry standards. Targeting JavaScript makes (C)LP programs executable in virtually every modern computing device with no additional software requirements from the point of view of the user. In turn, the use of a very high-level language facilitates the development of high-quality, complex software. The compiler is a back end of the Ciao system and supports most of its features, including its module system and its rich language extension mechanism based on packages. We present an overview of the compilation process and a detailed description of the run-time system, including the support for modular compilation into separate JavaScript code. We demonstrate the maturity of the compiler\u00a0\u2026", "num_citations": "6\n", "authors": ["265"]}
{"title": "CLP projection for constraint handling rules\n", "abstract": " This paper introduces and studies the notion of CLP projection for Constraint Handling Rules (CHR). The CLP projection consists of a naive translation of CHR programs into Constraint Logic Programs (CLP). We show that the CLP projection provides a safe operational and declarative approximation for CHR programs. We demonstrate moreover that a confluent CHR program has a least model, which is precisely equal to the least model of its CLP projection (closing hence a ten year-old conjecture by Abdennadher et al.). Finally, we illustrate how the notion of CLP projection can be used in practice to apply CLP analyzers to CHR. In particular, we show results from applying AProVE to prove termination, and CiaoPP to infer both complexity upper bounds and types for CHR programs.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Non-strict independence-based program parallelization using sharing and freeness information\n", "abstract": " The current ubiquity of multi-core processors has brought renewed interest in program parallelization. Logic programs allow studying the parallelization of programs with complex, dynamic data structures with (declarative) pointers in a comparatively simple semantic setting. In this context, automatic parallelizers which exploit and-parallelism rely on notions of independence in order to ensure certain efficiency properties.\u201cNon-strict\u201d independence is a more relaxed notion than the traditional notion of \u201cstrict\u201d independence which still ensures the relevant efficiency properties and can allow considerable more parallelism. Non-strict independence cannot be determined solely at run-time (\u201ca priori\u201d) and thus global analysis is a requirement. However, extracting non-strict independence information from available analyses and domains is non-trivial. This paper provides on one hand an extended presentation of our classic\u00a0\u2026", "num_citations": "6\n", "authors": ["265"]}
{"title": "Universidad Polit\u00e9cnica de Madrid\n", "abstract": " This project exposes a particular experience concerning software development processes implementation, using the CMMI model level 2. Organization was in a basic level or level 1.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Towards Execution Time Estimation for Logic Programs via Static Analysis and Profiling\n", "abstract": " Effective static analyses have been proposed which infer bounds on the number of resolutions or reductions. These have the advantage of being independent from the platform on which the programs are executed and have been shown to be useful in a number of applications, such as granularity control in parallel execution. On the other hand, in distributed computation scenarios where platforms with different capabilities come into play, it is necessary to express costs in metrics that include the characteristics of the platform. In particular, it is specially interesting to be able to infer upper and lower bounds on actual execution times. With this objective in mind, we propose an approach which combines compile-time analysis for cost bounds with a one-time profiling of the platform in order to determine the values of certain parameters for a given platform. These parameters calibrate a cost model which, from then on, is able to compute statically time bound functions for procedures and to predict with a significant degree of accuracy the execution times of such procedures in the given platform. The approach has been implemented and integrated in the CiaoPP system.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Using combined static analysis and profiling for logic program execution time estimation\n", "abstract": " Predicting statically the running time of programs has many applications ranging from task scheduling in parallel execution to proving the ability of a program to meet strict time constraints. A starting point in order to attack this problem is to infer the computational complexity of such programs (or fragments thereof). This is one of the reasons why the development of static analysis techniques for inferring cost-related properties of programs (usually upper and/or lower bounds of actual costs) has received considerable attention.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Removing superfluous versions in polyvariant specialization of prolog programs\n", "abstract": " Polyvariant specialization allows generating multiple versions of a procedure, which can then be separately optimized for different uses. Since allowing a high degree of polyvariance often results in more optimized code, polyvariant specializers, such as most partial evaluators, can generate a large number of versions. This can produce unnecessarily large residual programs. Also, large programs can be slower due to cache miss effects. A possible solution to this problem is to introduce a minimization step which identifies sets of equivalent versions, and replace all occurrences of such versions by a single one. In this work we present a unifying view of the problem of superfluous polyvariance. It includes both partial deduction and abstract multiple specialization. As regards partial deduction, we extend existing approaches in several ways. First, previous work has dealt with pure logic programs and a very\u00a0\u2026", "num_citations": "6\n", "authors": ["265"]}
{"title": "High-level characteristics of OR-and Independent AND-parallelism in Prolog\n", "abstract": " Although studies of a number of parallel implementations of logic programming languages are now available, their results are difficult to interpret due to the multiplicity of factors involved, the effect of each of which is difficult to separate. In this paper we present the results of a high-level simulation study of or- and independent and-parallelism with a wide selection of Prolog programs that aims to determine the intrinsic amount of parallelism, independently of implementation factors, thus facilitating this separation. We expect this study will be instrumental in better understanding and comparing results from actual implementations, as shown by some examples provided in the paper. In addition, the paper examines some of the issues and tradeoffs associated with the combination of and- and or-parallelism and proposes reasonable solutions based on the simulation data obtained.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Efficient term size computation for granularity control\n", "abstract": " Knowing the size of the terms to which program variables are bound at run-time in logic programs is required in a class of optimizations which includes granularity control and recursion elimination. Such size is difficult to even approximate at compile time and is thus generally computed at run-time by using (possibly prede\u00f1ned) predicates which traverse the terms involved. We propose a technique which has the potential of performing this computation much more efficiently. The technique is based on \u00f1nding program procedures which are called before those in which knowledge regarding term sizes is needed and which traverse the terms whose size is to be determined, and transforming such procedures so that they compute term sizes \"on the fly\". We present a systematic way of determining whether a given program can be transformed in order to compute a given term size at a given program point without additional term traversal. Also, if several such transformations are  possible our approach allows \u00f1nding minimal transformations under certain criteria. We also discuss the advantages and applications of our technique (specifically in the task of granularity control) and present some performance results.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Automatic Exploitation of Non-Determinate Independent And-Parallelism in the Basic Andorra Model\n", "abstract": " The Andorra Principle states that determinate goals can (and should) be run before other goals, possibly in parallel. This principle has been applied in a framework called the basic Andorra model, which allows (dependent) and-parallelism among determinate goals as well as or-parallelism. We show that it is possible to extend this model in order to also allow and-parallelism among independent, nondeterminate goals, thus supporting full independent and-parallelism, without greatly modifying the underlying implementation machinery. We propose to realize such an extension by making each (nondeterminate) independent goal determinate using a special all-solutions built-in predicate. We also show that this can be achieved automatically by compile-time translation from source Prolog programs by proposing a transformation that fulfills this objective. Finally, we provide some implementation results.", "num_citations": "6\n", "authors": ["265"]}
{"title": "Towards CIAO-Prolog-A parallel concurrent constraint system\n", "abstract": " We present an informal discussion on some methodological aspects regarding the efBcient parallel implementation of (concurrent)(constraint) logic programming systems, as well as an overview of some of the current work performed by our group in the context of such systems. These efforts represent our first steps towards the development of what we cali the CIAO (Concurrent, Independence-based And/Or parallel) system-a platform which we expect will provide efBcient implementations of a series of non-deterministic, concurrent, constraint logic'programming\u00a1 anguages, on sequential and multiprocessor machines. CIAO can be in some ways seen as an evolution of the &-Prolog [17] system concepts: it builds on &-Prolog ideas such as parallelization and optimization heavily based on compile-time global analysis and efBcient abstract machine design. On the other hand, CIAO is aimed at adding several important extensions, such as or-parallelism, constraints, more direct support for explicit concurrency in the source language, as well as other ideas inspired by proposals such as Muse [1] and Aurora [27], GHC [39], PNU-Prolog [30], IDIOM [16], DDAS [32], Andorra-I [31], AKL [20], and the extended Andorra model [40]. One of the objectives of CIAO is to offer at the same time all the user-level models provided by these systems. More than a precisely defined design, at this point the CIAO system should be seen as a target which serves to motivate and direct our current research efforts. This impreciseness is purposely based on our belief that, in order to develop an efBcient system with the characteristics that we desire, a number of technologies\u00a0\u2026", "num_citations": "6\n", "authors": ["265"]}
{"title": "A Practical Application of Sharing and Freeness Inference.\n", "abstract": " Abstract Interpretation [6] of logic programs ([1],[14],[8],[19],[2],[4],[13],[5],[12],[15]...) is currently proposed as a means for obtaining characteristics of the program at compile-time, tiras allowing several types of optimizations. However, only few studies have been reported analyzing the practicality of analyzers in the task they were designed for [23, 12, 22, 21, 3]. This paper offers a preliminary analysis of effectiveness of an analyzer which contributes to fifi this gap and is novel in both the domain and the application: results are provided for an abstract interpreter based on the sharing+ freeness domain presented in [17] and [7] in the application of automatic program parallelization.The analyzer under study was designed to accurately and concisely infer at compile-time variable groundness, sharing, and freeness information for a program and a given query form. The abstract domain approximates this information by combining two components: one provides information on sharing (aliasing, independence) and groundness; the other encodes freeness information. Briefly, the former is essentially the abstract domain of Jacobs and Langen [11](for efficiency and increased precisi\u00f3n, however, the analyzer under study uses the efficient abstract unification and topdown driven abstract interpretation algorithms defined by Muthukumar and Hermenegildo [18] instead of the pur\u00e9 bottom-up approach used by Jacobs and Langen). The latter is represented as a list of those program variables which are known to be free.", "num_citations": "6\n", "authors": ["265"]}
{"title": "An integrated approach to assertion-based random testing in Prolog\n", "abstract": " We present an approach for assertion-based random testing of Prolog programs that is tightly integrated within an overall assertion-based program development scheme. Our starting point is the Ciao model, a framework that unifies unit testing and run-time verification, as well as static verification and static debugging, using a common assertion language. Properties which cannot be verified statically are checked dynamically. In this context, the idea of generating random test values from assertion preconditions emerges naturally since these preconditions are conjunctions of literals, and the corresponding predicates can in principle be used as generators. Our tool generates valid inputs from the properties that appear in the assertions shared with other parts of the model, and the run time-check instrumentation of the Ciao framework is used to perform a wide variety of checks. This integration also facilitates the\u00a0\u2026", "num_citations": "5\n", "authors": ["265"]}
{"title": "Conferences vs. Journals in CS, what to do? Evolutionary ways forward and the ICLP/TPLP Model\n", "abstract": " We computer scientists seem to do it differently to other sciences: we publish mostly in conferences and our conferences are of a different nature. Our journal papers are long and take a long time to review and publish whereas often their papers are short and published quickly. And all this interacts with the tendency to evaluate researchers or departments frequently and in a mechanical way (via paper numbers and citation counts) instead of infrequently and deeply (by actually reading papers) and the fact that the current way in which bibliometry is done makes our papers invisible to the world. This position paper offers my viewpoint on what the problems are and why they are important, and also elaborates on some realistic ways forward. In particular, regarding the issue of conferences vs. journals, it proposes the model adopted a few years back by the logic programming community for its main conference (ICLP) and journal (TPLP, Cambridge U. Press). This model is based on the assumption that CS journal papers can be of two types: rapid publication papers (similar to those of other sciences and also close to our conference papers) as well as the longer journal papers that are traditional in CS. Then, the concrete proposal is to, instead of publishing the traditional conference proceedings, have the papers submitted instead to a special issue of an (indexed) journal which is ready online in time for the conference. The traditional conference reviewing process is also improved (following journal standards for rapid publication papers and special issues) to include at least two full rounds of refereeing and a final copy editing step. I argue that this\u00a0\u2026", "num_citations": "5\n", "authors": ["265"]}
{"title": "Towards a general argumentation system based on answer-set programming\n", "abstract": " Within the last years, especially since the work proposed by Dung in 1995, argumentation has emerged as a central issue in Artificial Intelligence. With the so called argumentation frameworks (AFs) it is possible to represent statements (arguments) together with a binary attack relation between them. The conflicts between the statements are solved on a semantical level by selecting acceptable sets of arguments. An increasing amount of data requires an automated computation of such solutions. Logic Programming in particular Answer-Set Programming (ASP) turned out to be adequate to solve problems associated to such AFs. In this work we use ASP to design a sophisticated system for the evaluation of several types of argumentation frameworks.", "num_citations": "5\n", "authors": ["265"]}
{"title": "Focused proof search for linear logic in the calculus of structures\n", "abstract": " The proof-theoretic approach to logic programming has benefited from the introduction of focused proof systems, through the non-determinism reduction and control they provide when searching for proofs in the sequent calculus. However, this technique was not available in the calculus of structures, known for inducing even more non-determinism than other logical formalisms. This work in progress aims at translating the notion of focusing into the presentation of linear logic in this setting, and use some of its specific features, such as deep application of rules and fine granularity, in order to improve proof search procedures. The starting point for this research line is the multiplicative fragment of linear logic, for which a simple focused proof system can be built.", "num_citations": "5\n", "authors": ["265"]}
{"title": "Improving the compilation of Prolog to C using type and determinism information: Preliminary results\n", "abstract": " We describe the current status of and provide preliminary performance results for a compiler of Prolog to C. The compiler is novel in that it is designed to accept different kinds of high-level information (typically obtained via an analysis of the initial Prolog program and expressed in a standardized language of assertions) and use this information to optimize the resulting C code, which is then further processed by an off-the-shelf C compiler. The basic translation process used essentially mimics an unfolding of a C-coded bytecode emulator with respect to the particular bytecode corresponding to the Prolog program. Optimizations are then applied to this unfolded program. This is facilitated by a more flexible design of the bytecode instructions and their lower-level components. This approach allows reusing a sizable amount of the machinery of the bytecode emulator: ancillary pieces of C code, data definitions, memory management routines and areas, etc., as well as mixing bytecode emulated code with natively compiled code in a relatively straightforward way. We report on the performance of programs compiled by the current version of the system, both with and without analysis information.", "num_citations": "5\n", "authors": ["265"]}
{"title": "Specialization and Optimization of Constraint Programs with Dynamic Scheduling\n", "abstract": " Abstract Interpretation, Logic Programming, Constraint Logic Programming, Compile-time Analysis, Multiple Program Specialization, Optimization, Program Specialization.", "num_citations": "5\n", "authors": ["265"]}
{"title": "On The Uses of Attributed Variables in Parallel and Conciirrent Logic Programming Systems*\n", "abstract": " Incorporating the possibility of attaching attributes to variables in a logic programming system has been shown to allow the addition of general constraint solving capabilities to it. This approach is very attractive in that by adding a few primitives any logic programming system can be turned into a generic constraint logic programming system in which constraint solving can be user defined, and at source level-an extreme example of the\" glass box\" approach. In this paper we propose a different and novel use for the concept of attributed variables: developing a generic parallel/concurrent (constraint) logic programming system, using the same\" glass box\" flavor. We arg\u00fce that a system which implements attributed variables and a few additional primitives can be easily customized at source level to implement many of the languages and execution models of parallelism and concurrency currently proposed, in both shared memory and distributed systems. We illustrate this through examples.", "num_citations": "5\n", "authors": ["265"]}
{"title": "Experimenting with Independent And-Parallel Prolog using Standard Prolog\n", "abstract": " This paper presents an approximation to the study of parallel systems using sequential tools. The Independent And-parallelism in Prolog is an example of parallel processing paradigm in the framework of logic programming, and implementations like< fc-Prolog uncover the potential performance of parallel processing. But this potential can also be explored using only sequential systems. Being the spirit of this paper to show how this can be done with a standard system, only standard Prolog will be used in the implementations included. Such implementations include tests for parallelism in And-Prolog, a correctnesschecking meta-interpreter of< fc-Prolog and a simulator of parallel execution for< fc-Prolog.", "num_citations": "5\n", "authors": ["265"]}
{"title": "A general framework for static cost analysis of parallel logic programs\n", "abstract": " The estimation and control of resource usage is now an important challenge in an increasing number of computing systems. In particular, requirements on timing and energy arise in a wide variety of applications such as internet of things, cloud computing, health, transportation, and robots. At the same time, parallel computing, with (heterogeneous) multi-core platforms in particular, has become the dominant paradigm in computer architecture. Predicting resource usage on such platforms poses a difficult challenge. Most work on static resource analysis has focused on sequential programs, and relatively little progress has been made on the analysis of parallel programs, or more specifically on parallel logic programs. We propose a novel, general, and flexible framework for setting up cost equations/relations which can be instantiated for performing resource usage analysis of parallel logic programs for a wide range\u00a0\u2026", "num_citations": "4\n", "authors": ["265"]}
{"title": "Towards incremental and modular context-sensitive analysis\n", "abstract": " 2012 ACM Subject Classification Theory of computation\u2192 Invariants, Theory of computation\u2192 Pre-and post-conditions, Theory of computation\u2192 Program analysis, Theory of computation\u2192 Program semantics, Theory of computation\u2192 Abstraction", "num_citations": "4\n", "authors": ["265"]}
{"title": "Towards run-time checks simplification via term hiding\n", "abstract": " One of the most attractive features of untyped languages for programmers is the flexibility in term creation and manipulation. However, with such power comes the responsibility of ensuring correctness of operations. A solution is adding run-time checks to the program via assertions, but this can introduce overheads that are in many cases impractical. While such overheads can be greatly reduced with static analysis, the gains depend strongly on the quality of the information inferred. Reusable libraries, ie, library modules that are pre-compiled independently of the client, pose special challenges in this context. We propose a relaxed form of atom-based module system (which hides only a selected set of functor symbols but still provides a strict mechanism to prevent breaking visibility rules across modules) that can enrich significantly the shape information that can be inferred in reusable modular programs. We also propose an improved run-time checking approach that takes advantage of the proposed mechanisms to achieve large reductions in overhead, closer to those of static languages even in the reusable-library context. While the approach is general and system-independent, we present it for concreteness in the context of the Ciao assertion language and combined static/dynamic checking framework. Our method maintains full expressiveness of the checks in this context. Contrary to other approaches it does not introduce the need to switch the language to (static) type systems, which is known to change the semantics in languages like Prolog. We also study the approach experimentally and evaluate the overhead reduction achieved in the run\u00a0\u2026", "num_citations": "4\n", "authors": ["265"]}
{"title": "Towards pre-indexed terms\n", "abstract": " Indexing of terms and clauses is a well-known technique used in Prolog implementations (as well as automated theorem provers) to speed up search. In this paper we show how the same mechanism can be used to implement efficient reversible mappings between different term representations, which we call pre-indexings. Based on user-provided term descriptions, these mappings allow us to use more efficient data encodings internally, such as prefix trees. We show that for some classes of programs, we can drastically improve the efficiency by applying such mappings at selected program points.", "num_citations": "4\n", "authors": ["265"]}
{"title": "Bisimilarity in concurrent constraint programming\n", "abstract": " In this doctoral work we aim at developing a new approach to labelled semantics and equivalences for the Concurrent Constraint Programming (CCP) which will enable a broader capture of processes behavioural equivalence. Moreover, we work towards exploiting the strong connection between rst order logic and CCP. Something which will allow us to represent logical formulae in terms of CCP processes and verify its logical equivalence by means of our notion of bisimilarity. Finally, following the lines of the Concurrecy Workbench we plan to implement a CCP Workbench based on our theoretical structure", "num_citations": "4\n", "authors": ["265"]}
{"title": "A program transformation for continuation call-based tabled execution\n", "abstract": " The advantages of tabled evaluation regarding program termination and reduction of complexity are well known --as are the significant implementation, portability, and maintenance efforts that some proposals (especially those based on suspension) require. This implementation effort is reduced by program transformation-based continuation call techniques, at some efficiency cost. However, the traditional formulation of this proposal by Ramesh and Cheng limits the interleaving of tabled and non-tabled predicates and thus cannot be used as-is for arbitrary programs. In this paper we present a complete translation for the continuation call technique which, using the runtime support needed for the traditional proposal, solves these problems and makes it possible to execute arbitrary tabled programs. We present performance results which show that CCall offers a useful tradeoff that can be competitive with state-of-the-art implementations.", "num_citations": "4\n", "authors": ["265"]}
{"title": "Negative ternary set-sharing\n", "abstract": " The Set-Sharing domain has been widely used to infer at compile-time interesting properties of logic programs such as occurs-check reduction, automatic parallelization, and finite-tree analysis. However, performing abstract unification in this domain requires a closure operation that increases the number of sharing groups exponentially. Much attention has been given to mitigating this key inefficiency in this otherwise very useful domain. In this paper we present a novel approach to Set-Sharing: we define a new representation that leverages the complement (or negative) sharing relationships of the original sharing set, without loss of accuracy. Intuitively, given an abstract state  over the finite set of variables of interest , its negative representation is . Using this encoding during analysis dramatically reduces the number of elements that need to be represented in the abstract states and during\u00a0\u2026", "num_citations": "4\n", "authors": ["265"]}
{"title": "Efficient Set Sharing using ZBDDs\n", "abstract": " Set sharing is an abstract domain in which each concrete object is represented by the set of local variables from which it might be reachable. It is a useful abstraction to detect parallelism opportunities, since it contains definite information about which variables do not share in memory, i.e., about when the memory regions reachable from those variables are disjoint. Set sharing is a more precise alternative to pair sharing, in which each domain element is a set of all pairs of local variables from which a common object may be reachable. However, the exponential complexity of some set sharing operations has limited its wider application. This work introduces an efficient implementation of the set sharing domain using Zero-supressed Binary Decision Diagrams (ZBDDs). Because ZBDDs were designed to represent sets of combinations (i.e., sets of sets), they naturally represent elements of the set sharing\u00a0\u2026", "num_citations": "4\n", "authors": ["265"]}
{"title": "Towards high-level execution primitives for and-parallelism: Preliminary results\n", "abstract": " Most implementations of parallel logic programming rely on complex low-level machinery which is arguably difflcult to implement and modify. We explore an alternative approach aimed at taming that complexity by raising core parts of the implementation to the source language level for the particular case of and-parallelism. Therefore, we handle a signiflcant portion of the parallel implementation mechanism at the Prolog level with the help of a comparatively small number of concurrency-related primitives which take care of lower-level tasks such as locking, thread management, stack set management, etc. The approach does not eliminate altogether modiflcations to the abstract machine, but it does greatly simplify them and it also facilitates experimenting with different alternatives. We show how this approach allows implementing both restricted and unrestricted (ie, non fork-join) parallelism. Preliminary experiments show that the amount of performance sacriflced is reasonable, although granularity control is required in some cases. Also, we observe that the availability of unrestricted parallelism contributes to better observed speedups.", "num_citations": "4\n", "authors": ["265"]}
{"title": "A generic framework for the analysis and specialization of logic programs\n", "abstract": " The relationship between abstract interpretation [2] and partial evaluation [5] has received considerable attention and (partial) integrations have been proposed starting from both the partial deduction (see e.g. [6] and its references) and abstract interpretation perspectives. Abstract interpretation-based analyzers (such as the CiaoPP analyzer [9,4]) generally compute a program analysis graph [1] in order to propagate (abstract) call and success information by performing fixpoint computations when needed. On the other hand, partial deduction methods [7] incorporate powerful techniques for on-line specialization including (concrete) call propagation and unfolding.", "num_citations": "4\n", "authors": ["265"]}
{"title": "CP Debugging Needs and Tools\n", "abstract": " Conventional programming techniques are not well suited for solving many highly combinatorial industrial problems, like scheduling, decision making, resource allocation or planning. Constraint Programming (CP), an emerging software technology, offers an original approach allowing for efficient and flexible solving of complex problems, through combined implementation of various constraint solvers and expert heuristics. Its applications are increasingly elded in various industries.One of the main features of CP is a new approach to software production: the same program is progressively improved at each step of the development cycle, from the first prototype until the final product. This makes debugging the cornerstone of CP technology. Existing debugging tools reveal to be ineffective in most industrial situations and tools developed for imperative or functional programming are not adapted to the context of CP. The main reasons are that the huge numbers of variables and constraints makes the computation state difficult to understand, and that the non deterministic execution increases drastically the number of computation states which must be analysed. As recognised in [CFGG95]\u201cconstraint program are particularly hard to debug, in that it is practically impossible to follow constraint propagation step by step for any non-toy program\u201d.", "num_citations": "4\n", "authors": ["265"]}
{"title": "Effectiveness of combined sharing and freeness analysis using abstract interpretation\n", "abstract": " This paper presents improved unification algorithms, an implementation, and an analysis of the effectiveness of an abstract interpreter based on the sharing+ freeness domain presented in a previous paper, which was designed to accurately and concisely represent combined freeness and sharing information for program variables. We first briefly review this domain and the unification algorithms previously proposed. We then improve these algorithms and correct them to deal with some cases which were not well analyzed previously, illustrating the improvement with an example. We then present the implementation of the improved algorithm and evaluate its performance by comparing the effectiveness of the information inferred to that of other interpreters available to us for an application (program parallelization) that is common to all these interpreters. All these systems have been embedded in a real parallelizing compiler. Effectiveness of the analysis is measured in terms of actual final performance of the system: ie in terms of the actual speedups obtained. The results show good performance for the combined domain in that it improves the accuracy of both types of information and also in that the analyzer using the combined domain is more effective in the application than any of the other analyzers it is compared to.", "num_citations": "4\n", "authors": ["265"]}
{"title": "Implementation of an event driven scheme for visualizing parallel execution of logic programs\n", "abstract": " This article presents in an informal way some early results on the design of a series of paradigms for visualization of the parallel execution of logic programs. The results presented here refer to the visualization of or-parallelism, as in MUSE and Aurora, deterministic dependent and-parallelism, as in Andorra-I, and independent and-parallelism as in &-Prolog. A tool has been implemented for this purpose and has been interfaced with these systems. Results are presented showing the visualization of executions from these systems and the usefulness of the resulting tool is briefly discussed.", "num_citations": "4\n", "authors": ["265"]}
{"title": "Memory referencing characteristics and caching performance of AND-parallel Prolog on shared-memory multiprocessors\n", "abstract": " This paper presents the performance analysis results for the RAP-WAM AND-Parallel Prolog architecture on shared-memory multiprocessor organizations. The goal of this parallel model is to provide inference speeds beyond those attainable in sequential systems, while supporting conventional logic programming semantics. Special emphasis is placed on sequential performance, storage efficiency, and low control overhead. First, the concepts and techniques used in the parallel execution model are described, along with the general methodology, benchmarks, and simulation tools used for its evaluation. Results are given both at the memory reference level and at the memory organization level. A two-level shared-memory architecture model is presented together with an analysis of various solutions to the cache coherency problem. Finally, RAP-WAM shared-memory simulation results are presented. It is\u00a0\u2026", "num_citations": "4\n", "authors": ["265"]}
{"title": "Paving the Roadmaps: Enabling and Integration Technologies\n", "abstract": " integration technology challenging future eits dimension constraint programming systematic program development eit concern specific supportive enabling technology topical roadmap chapter computational logic research global concern particular computational logic topic ass achievement reliable road construction", "num_citations": "4\n", "authors": ["265"]}
{"title": "Towards Static Performance Guarantees for Programs with Run-time Checks\n", "abstract": " 2012 ACM Subject Classification Theory of computation\u2192 Program semantics, Theory of computation\u2192 Program analysis, Theory of computation\u2192 Pre-and post-conditions, Theory of computation\u2192 Invariants", "num_citations": "3\n", "authors": ["265"]}
{"title": "Exploring the impact of inaccuracy and imprecision of qos assumptions on proactive constraint-based QoS prediction for service orchestrations\n", "abstract": " Constraint-based Quality of Service (QoS) prediction is a method for predicting violations of Service Level Agreements (SLAs) in an executing instance of a service orchestration. It uses assumptions about the ranges of QoS values for component services in the orchestration. Experiments suggest that the method, when given correct component QoS assumptions, produces highly accurate predictions according to a series of quality-of-prediction metrics, and that it does so well ahead of the time when the prediction is to happen. We study the behavior of this method when the component QoS assumptions become incorrect or too vague. We conclude that the effect is a graceful deterioration in prediction quality, unless gross (order-of-magnitude) imprecisions are introduced. However, the method is very sensitive to the loss of information on the lower bounds for component QoS values, since the knowledge of the upper\u00a0\u2026", "num_citations": "3\n", "authors": ["265"]}
{"title": "An overview of the ciao system\n", "abstract": " Ciao is a logic-based, multi-paradigm programming system. One of its most distinguishing features is that it supports a large number of semantic and syntactic language features which can be selectively activated or deactivated for each program module. As a result, a module can be written in, for example, ISO-Prolog plus constraints and higher order, while another can be a pure logic module with a different control rule such as iterative deepening and/or tabling, and perhaps using constructive negation. A powerful and modular extension mechanism allows user-level design and implementation of such features and sub-languages.               Another distinguishing feature of Ciao is its powerful assertion language, which allows expressing many kinds of program properties (ranging from, e.g., moded types to resource consumption), as well as tests and documentation. The compiler is capable of statically\u00a0\u2026", "num_citations": "3\n", "authors": ["265"]}
{"title": "Modular Extensions for Modular (Logic) Languages\n", "abstract": " We address the problem of developing mechanisms for easily implementing modular extensions to modular (logic) languages. By (language) extensions we refer to different groups of syntactic definitions and translation rules that extend a language. Our use of the concept of modularity in this context is twofold. We would like these extensions to be modular, in the sense above, i.e., we should be able to develop different extensions mostly separately. At the same time, the sources and targets for the extensions are modular languages, i.e., such extensions may take as input separate pieces of code and also produce separate pieces of code. Dealing with this double requirement involves interesting challenges to ensure that modularity is not broken: first, combinations of extensions (as if they were a single extension) must be given a precise meaning. Also, the separate translation of multiple sources (as if they\u00a0\u2026", "num_citations": "3\n", "authors": ["265"]}
{"title": "Verification, Model Checking, and Abstract Interpretation: 11th International Conference, VMCAI 2010, Madrid, Spain, January 17-19, 2010, Proceedings\n", "abstract": " This volume contains the proceedings of the 11th International Conference on Veri? cation, Model Checking, and Abstract Interpretation (VMCAI 2010), held in Madrid, Spain, January 17\u201319, 2010. VMCAI 2010 was the 11th in a series of meetings. Previous meetings were held in Port Je? erson (1997), Pisa (1998), Venice (2002), New York (2003), Venice (2004), Paris (2005), Charleston (2006), Nice (2007), SanFrancisco (2008), and Savannah (2009). VMCAI centers on state-of-the-art research relevant to analysis of programs and systems and drawn from three research communities: veri? cation, model checking, and abstract interpretation. A goal is to facilitate interaction, cro-fertilization, and the advance of hybrid methods that combine two or all three areas. Topics covered by VMCAI include program veri? cation, program cert-cation, model checking, debugging techniques, abstract interpretation, abstract domains, static analysis, type systems, deductive methods, and optimization. The Program Committee selected 21 papers out of 57 submissions based on anonymous reviews and discussions in an electronic Program Committee me-ing. The principal selection criteria were relevance and quality.", "num_citations": "3\n", "authors": ["265"]}
{"title": "Realizing the dependently typed lambda calculus\n", "abstract": " Dependently typed lambda calculi such as the Edinburgh Logical Framework (LF) can encode relationships between terms in types and can naturally capture correspondences between formulas and their proofs. Such calculi can also be given a logic programming interpretation: the system is based on such an interpretation of LF. We have considered whether a conventional logic programming language can also provide the benefits of a Twelf-like system for encoding type and term dependencies through dependent typing, and whether it can do so in an efficient manner. In particular, we have developed a simple mapping from LF specifications to a set of formulas in the higher-order hereditary Harrop (hohh) language, that relates derivations and proof-search between the two frameworks. We have shown that this encoding can be improved by exploiting knowledge of the well-formedness of the original LF specifications to elide much redundant type-checking information. The resulting logic program has a structure that closely follows the original specification, thereby allowing LF specifications to be viewed as meta-programs that generate hohh programs. We have proven that this mapping is correct, and, using the Teyjus implementation of lprolog, we have shown that our translation provides an efficient means for executing LF specifications, complementing the ability the Twelf system provides for reasoning about them. In addition, the translation offers new avenues for reasoning about such specifications, via reasoning over the generated hohh programs.", "num_citations": "3\n", "authors": ["265"]}
{"title": "Description and Optimization of Abstract Machines in a Dialect of Prolog\n", "abstract": " Abstract Machines in an Extension of Prolog\u201d published in the proceedings of the 2006 International Symposium on Logic-based Program Synthesis and Transformation (LOPSTR\u201906)[MCH06].", "num_citations": "3\n", "authors": ["265"]}
{"title": "Functional notation and lazy evaluation in Ciao\n", "abstract": " Certain aspects of functional programming provide syntactic convenience, such as having a designated implicit output argument, which allows function cali nesting and sometimes results in more compact code. Functional programming also sometimes allows a more direct encoding of lazy evaluation, with its ability to deal with infinite data structures. We present a syntactic functional extensi\u00f3n of Prolog covering function application, predefined evaluable functors, functional definitions, quoting, and lazy evaluation. The extensi\u00f3n is also composable with higher-order features. We also highlight the Ciao features which help implementation and present some data on the overhead of using lazy evaluation with respect to eager evaluation.", "num_citations": "3\n", "authors": ["265"]}
{"title": "Grid-based histogram arithmetic for the probabilistic analysis of functions\n", "abstract": " The selection of predefined analytic grids (partitions of the numeric ranges) to represent input and output functions as histograms has been proposed as a mechanism of approximation in order to control the tradeoff between accuracy and computation times in several areas ranging from simulation to constraint solving. In particular, the applicati on of interval methods for probabilistic function characterization has been shown to have advantages over other methods based on the simulati on of random samples. However, standard interval arithmetic has always been used for the computation steps. In this paper, we introduce an alternative approximate arithmetic aimed at controlling the cost of the interval operations. Its distinctive feature is that grids are taken into account by the operators. We apply the technique in the context of probability density functions in order to improve the accuracy of the probability\u00a0\u2026", "num_citations": "3\n", "authors": ["265"]}
{"title": "Analysis and transformation of constrained Horn clauses for program verification\n", "abstract": " This paper surveys recent work on applying analysis and transformation techniques that originate in the field of constraint logic programming (CLP) to the problem of verifying software systems. We present specialisation-based techniques for translating verification problems for different programming languages, and in general software systems, into satisfiability problems for constrained Horn clauses (CHCs), a term that has become popular in the verification field to refer to CLP programs. Then, we describe static analysis techniques for CHCs that may be used for inferring relevant program properties, such as loop invariants. We also give an overview of some transformation techniques based on specialisation and fold/unfold rules, which are useful for improving the effectiveness of CHC satisfiability tools. Finally, we discuss future developments in applying these techniques.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Towards a general framework for static cost analysis of parallel logic programs\n", "abstract": " The estimation and control of resource usage is now an important challenge in an increasing number of computing systems. In particular, requirements on timing and energy arise in a wide variety of applications such as internet of things, cloud computing, health, transportation, and robots. At the same time, parallel computing, with (heterogeneous) multi-core platforms in particular, has become the dominant paradigm in computer architecture. Predicting resource usage on such platforms poses a difficult challenge. Most work on static resource analysis has focused on sequential programs, and relatively little progress has been made on the analysis of parallel programs, or more specifically on parallel logic programs. We propose a novel, general, and flexible framework for setting up cost equations/relations which can be instantiated for performing resource usage analysis of parallel logic programs for a wide range of resources, platforms and execution models. The analysis estimates both lower and upper bounds on the resource usage of a parallel program (without executing it) as functions on input data sizes. In addition, it also infers other meaningful information to better exploit and assess the potential and actual parallelism of a system. We develop a method for solving cost relations involving the max function that arise in the analysis of parallel programs. Finally, we instantiate our general framework for the analysis of logic programs with Independent And-Parallelism, report on an implementation within the CiaoPP system, and provide some experimental results. To our knowledge, this is the first approach to the cost analysis of parallel logic\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "An Approach to Static Performance Guarantees for Programs with Run-time Checks\n", "abstract": " Instrumenting programs for performing run-time checking of properties, such as regular shapes, is a common and useful technique that helps programmers detect incorrect program behaviors. This is specially true in dynamic languages such as Prolog. However, such run-time checks inevitably introduce run-time overhead (in execution time, memory, energy, etc.). Several approaches have been proposed for reducing such overhead, such as eliminating the checks that can statically be proved to always succeed, and/or optimizing the way in which the (remaining) checks are performed. However, there are cases in which it is not possible to remove all checks statically (e.g., open libraries which must check their interfaces, complex properties, unknown code, etc.) and in which, even after optimizations, these remaining checks still may introduce an unacceptable level of overhead. It is thus important for programmers to be able to determine the additional cost due to the run-time checks and compare it to some notion of admissible cost. The common practice used for estimating run-time checking overhead is profiling, which is not exhaustive by nature. Instead, we propose a method that uses static analysis to estimate such overhead, with the advantage that the estimations are functions parameterized by input data sizes. Unlike profiling, this approach can provide guarantees for all possible execution traces, and allows assessing how the overhead grows as the size of the input grows. Our method also extends an existing assertion verification framework to express \"admissible\" overheads, and statically and automatically checks whether the instrumented\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Exploiting term hiding to reduce run-time checking overhead\n", "abstract": " One of the most attractive features of untyped languages is the flexibility in term creation and manipulation. However, with such power comes the responsibility of ensuring the correctness of these operations. A solution is adding run-time checks to the program via assertions, but this can introduce overheads that are in many cases impractical. While static analysis can greatly reduce such overheads, the gains depend strongly on the quality of the information inferred. Reusable libraries, i.e., library modules that are pre-compiled independently of the client, pose special challenges in this context. We propose a technique which takes advantage of module systems which can hide a selected set of functor symbols to significantly enrich the shape information that can be inferred for reusable libraries, as well as an improved run-time checking approach that leverages the proposed mechanisms to achieve large\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Term hiding and its impact on run-time check simplification\n", "abstract": " One of the most attractive features of untyped languages for programmers is the flexibility in term creation and manipulation. However, with such power comes the responsibility of ensuring correctness of operations. A solution is adding run-time checks to the program via assertions, but this can introduce overheads that are in many cases impractical. While such overheads can be greatly reduced with static analysis, the gains depend strongly on the quality of the information inferred. Reusable libraries, i.e., library modules that are pre-compiled independently of the client, pose special challenges in this context. We propose a relaxed form of atom-based module system (which hides only a selected set of functor symbols but still provides a strict mechanism to prevent breaking visibility rules across modules) that can enrich significantly the shape information that can be inferred in reusable modular programs. We also propose an improved run-time checking approach that takes advantage of the proposed mechanisms to achieve large reductions in overhead, closer to those of static languages even in the reusable-library context. While the approach is general and systemindependent, we present it for concreteness in the context of the Ciao assertion language and combined static/dynamic checking framework. Our method maintains full expressiveness of the checks in this context. Contrary to other approaches it does not introduce the need to switch the language to (static) type systems, which is known to change the semantics in languages like Prolog. We also study the approach experimentally and evaluate the overhead reduction achieved in the run\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Description and optimization of abstract machines in a dialect of Prolog\n", "abstract": " In order to achieve competitive performance, abstract machines for Prolog and related languages end up being large and intricate, and incorporate sophisticated optimizations, both at the design and at the implementation levels. At the same time, efficiency considerations make it necessary to use low-level languages in their implementation. This makes them laborious to code, optimize, and, especially, maintain and extend. Writing the abstract machine (and ancillary code) in a higher-level language can help tame this inherent complexity. We show how the semantics of most basic components of an efficient virtual machine for Prolog can be described using (a variant of) Prolog. These descriptions are then compiled to C and assembled to build a complete bytecode emulator. Thanks to the high-level of the language used and its closeness to Prolog, the abstract machine description can be manipulated using\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Parallel backtracking with answer memoing for independent and-parallelism\n", "abstract": " Goal-level Independent and-parallelism (IAP) is exploited by scheduling for simultaneous execution of two or more goals, which will not interfere with each other at run time. This can be done safely even if such goals can produce multiple answers. The most successful IAP implementations to date have used recomputation of answers and sequentially ordered backtracking. While in principle simplifying the implementation, recomputation can be very inefficient if the granularity of the parallel goals is large enough and they produce several answers, while sequentially ordered backtracking limits parallelism. And, despite the expected simplification, the implementation of the classic schemes has proved to involve complex engineering, with the consequent difficulty for system maintenance and extension, while still frequently running into the well-known trapped goal and garbage slot problems. This work presents an\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "From relational specifications to logic programs\n", "abstract": " This paper presents a compiler from expressive, relational specifications to logic programs. Specifically, the compiler translates the Imperative Alloy specification language to Prolog. Imperative Alloy is a declarative, relational specification language based on first-order logic and extended with imperative constructs; Alloy specifications are traditionally not executable. In spite of this theoretical limitation, the compiler produces useful prototype implementations for many specifications.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Datalog for enterprise software: from industrial applications to research (invited talk)\n", "abstract": " LogicBlox is a platform for the rapid development of enterprise applications in the domains of decision automation, analytics, and planning. Although the LogicBlox platform embodies several components and technology decisions (eg, an emphasis on software-as-a-service), the key substrate and glue is an implementation of the Datalog language. All application development on the LogicBlox platform is done declaratively in Datalog: The language is used to query large data sets, but also to develop web and desktop GUIs (with the help of pre-defined libraries), to interface with solvers, statistics tools, and optimizers for complex analytics solutions, and to express the overall business logic of the application. The goal of this talk is to present both the business case for Datalog and the fruitful interaction of research and industrial applications in the LogicBlox context.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Tight semantics for logic programs\n", "abstract": " We define the Tight Semantics (TS), a new semantics for all NLPs complying with the requirements of: 2-valued semantics; preserving the models of SM; guarantee of model existence, even in face of Odd Loops Over Negation (OLONs) or infinite chains; relevance cumulativity; and compliance with the Well-Founded Model. When complete models are unnecessary, and top-down querying (\u00e0 la Prolog) is desired, TS provides the 2-valued option that guarantees model existence, as a result of its relevance property. Top-down querying with abduction by need is rendered available too by TS. The user need not pay the price of computing whole models, nor that of generating all possible abductions, only to filter irrelevant ones subsequently. A TS model of a NLP P is any minimal model M of P that further satisfies P^---the program remainder of P---in that each loop in P^ has a MM contained in M, whilst respecting the constraints imposed by the MMs of the other loops so-constrained too. The applications afforded by TS are all those of Stable Models, which it generalizes, plus those permitting to solve OLONs for model existence, plus those employing OLONs for productively obtaining problem solutions, not just filtering them (like Integrity Constraints).", "num_citations": "2\n", "authors": ["265"]}
{"title": "Constraint answer set programming systems\n", "abstract": " We present an integration of answer set programming and constraint processing as an interesting approach to constraint logic programming. Although our research is in a very early stage, we motivate constraint answer set programming and report on related work, our research objectives, preliminary results we achieved, and future work.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Logic programming foundations of cyber-physical systems\n", "abstract": " Cyber-physical systems (CPS) are becoming ubiquitous. Almost every device today has a controller that reads inputs through sensors, does some processing and then performs actions through actuators. These controllers are discrete digital systems whose inputs are continuous physical quantities and whose outputs control physical (analog) devices. Thus, CPS involve both digital and analog data. In addition, CPS are assumed to run forever, and many CPS may run concurrently with each other. we will develop techniques for faithfully and elegantly modeling CPS. Our approach is based on using constraint logic programming over reals, co-induction, and coroutining.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Contractibility and contractible approximations of soft global constraints\n", "abstract": " We study contractibility and its approximation for two very general classes of soft global constraints. We introduce a general formulation of decomposition-based soft constraints and provide a sufficient condition for contractibility and an approach to approximation. For edit-based soft constraints, we establish that the tightest contractible approximation cannot be expressed in edit-based terms, in general.", "num_citations": "2\n", "authors": ["265"]}
{"title": "A tabling implementation based on variables with multiple bindings\n", "abstract": " Suspension-based tabling systems have to save and restore computation states belonging to OR branches. Stack freezing combined with (forward) trailing is among the better-known implementation approaches for this purpose. Resuming a goal using this technique reinstalls the bindings for all the variables in the environment where the goal was suspended. In this paper we explore an alternative approach where variables can keep track of several bindings, associated with suspensions. Resuming a goal boils down to determining which suspension has to be resumed, in order to select, when dereferencing, the bindings which were active at the moment of suspending. We present the ideas behind this approach, highlight several advantages over other suspension-based implementations, and perform an experimental evaluation. We also recall the similarity between OR-parallelism and suspension-based\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Towards structured state threading in Prolog\n", "abstract": " It is very often the case that programs require passing, maintaining, and updating some notion of state. Prolog programs often implement such stateful computations by carrying this state in predicate arguments (or, alternatively, in the internal dat\u00e1base). This often causes code obfuscation, complicates code reuse, introduces dependencies on the data model, and is prone to incorrect propagation of the state information among predicate calis. To partly solve these problems, we introduce contexts as a consistent mechanism for specifying implicit arguments and its threading in clause goals. We propose a notation and an interpretation for contexts, ranging from single goals to complete programs, give an intuitive semantics, and describe a translation into standard Prolog. We also discuss a particular light-weight implementation in Ciao Prolog, and we show the usefulness of our proposals on a series of examples and applications, including code directiy using contexts, DCGs, extended DCGs, logical loops and other custom control structures.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Automatic binding-related error diagnosis in logic programs\n", "abstract": " This paper proposes a diagnosis algorithm for locating a certain kind of errors in logic programs: variable binding errors that result in abstract symptoms during compile-time checking of assertions based on abstract interpretation. The diagnoser analyzes the graph generated by the abstract interpreter, which is a provably safe approximation of the program semantics. The proposed algorithm traverses this graph to find the point where the actual error originates (a reason of the symptom), leading to the point the error has been reported (the symptom). The procedure is fully automatic, not requiring any interaction with the user. A prototype diagnoser has been implemented and preliminary results are encouraging.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Unified memory analysis\n", "abstract": " The ability to accurately model the state of program memory and how it evolves during program execution is critical to many optimization and verification techniques. Current memory analysis techniques either provide very accurate information but run prohibitively slowly or run in an acceptable time but produce very conservative results. This paper presents an analysis method that is capable of accurately modeling many important program properties (aliasing, shape, logical data structures, sharing of data elements in collections) while maintaining an acceptable level of performance.Using several examples we show how our abstract model is able to provide detailed information on the properties of interest in the concrete domain. We demonstrate that the model is a safe approximation of the concrete heap and outline the required data flow operations. Finally, we show that the asymptotic runtime of this method is polynomial in the size of the abstract heap and that in practice it is efficient enough to be used in an optimizing compiler.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Why Ciao?\u2013An Overview of the Ciao System\u2019s Design Philosophy\n", "abstract": " Our intention in this note is not to provide a listing of the many features of the Ciao system: this can in part be found for example in the brochures announcing upcoming versions in the Ciao website or in more feature-oriented descriptions such as [9]). Instead in this document we would like to describe the objectives and reasoning followed in our design of the Ciao system [12, 11] as well as the fundamental characteristics that in our opinion make Ciao quite unique and hopefully really useful to you as a Ciao user.Prolog? Yes,\u201cmore and less.\u201d When you start the Ciao top level and load a standard Prolog program you experience Ciao as a modern Prolog system. And Ciao is certainly a high-quality, high-performance, fully featured, public domain (and widely used!) implementation of Prolog. So, if you were looking for a truly great Prolog you came to the right place! But Ciao is also much, much more than a Prolog system. It essentially represents our best effort at designing and implementing what we believe a truly \u201cnext-generation,\u201d multi-paradigm programming language and program development environment should be. You may ask then,\u201cIf that is your intention, why even bother to support Prolog? All other \u201cnext-generation\u201d logic programming systems decided to depart from Prolog and give up on running Prolog programs!\u201d The answer is because, first, as Ciao shows, Prolog can be supported without giving up on having also a truly next-generation system (we explain how below). And second, because supporting Prolog is clearly of practical interest: it is an industry standard and it is the principal logic language taught to students, so there is\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Combining Static Analysis and Profiling for Estimating Execution Times in Logic Programs\n", "abstract": " Effective static analyses have been proposed which allow inferring functions which bound the number of resolutions or reductions. These have the advantage of being independent from the platform on which the programs are executed and such bounds have been shown useful in a number of applications, such as granularity control in parallel execution. On the other hand, in certain distributed computation scenarios where different platforms come into play, with each platform having different capabilities, it is more interesting to express costs in metrics that include the characteristics of the platform. In particular, it is specially interesting to be able to infer upper and lower bounds on actual execution time. With this objective in mind, we propose a method which allows inferring upper and lower bounds on the execution times of procedures of a program in a given execution platform. The approach combines compile-time cost bounds analysis with a one-time profiling of the platform in order to determine the values of certain constants for that platform. These constants calibrate a cost model which from then on is able to compute statically time bound functions for procedures and to predict with a significant degree of accuracy the execution times of such procedures in the given platform. The approach has been implemented and integrated in the CiaoPP system.", "num_citations": "2\n", "authors": ["265"]}
{"title": "A generic persistence model for (C) LP systems\n", "abstract": " Mutable state is traditionally implemented in Prolog and other (C)LP systems by performing dynamic modifications to predicate definitions at runtime, i.e. to dynamic predicates of the internal database. Dynamic facts are often used to store information accessible per module or globally and which can be preserved through backtracking. These database updates, despite the obvious drawback of their non-declarative nature, have practical applications and they are given a practical semantics by the so-called logical view of (internal) database updates.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Static Analysis: 9th International Symposium, SAS 2002, Madrid, Spain, September 17-20, 2002. Proceedings\n", "abstract": " Staticanalysisisaresearchareaaimedatdevelopingprinciplesandtoolsfor-ri? cation and semantics-based manipulation of programs and high-performance implementation of programming languages. The series of Static Analysis S-posia is a forum for the presentation and discussion of advances in the area. This volume contains the papers accepted for presentation at the Ninth-ternational Static Analysis Symposium (SAS 2002), which was held Sept-ber 17-20, 2002 in Madrid, Spain. Previous SAS symposia were held in-ris, France (LNCS 2126), Santa Barbara, CA, USA (LNCS 1824), Venice, Italy (LNCS1694), Pisa, Italy (LNCS1503), Paris, France (LNCS1302), Aachen, G-many (LNCS 1145), Glasgow, UK (LNCS 983), Namur, Belgium (LNCS 864), followingtheinternationalworkshopWSAinPadova, Italy (LNCS724), Bor-aux, France (BigreVol. 81-82) andJTASPEFL/WSA, Bordeaux, France (Bigre Vol. 74). In response to the call for papers, 86 contributions were submitted from 12 di? erent countries. Following on-line discussions, the Program Committee met in Madrid on June 22, 2002, and selected 32 papers, basing this choice on their scienti? cqualityandrelevancetothesymposium. Eachpaperwasreviewedbyat leastthreeProgramCommitteemembersorexternalreferees. Inadditiontothe contributed papers, this volume includes abstracts of invited talks by Thomas Reps (UniversityofWisconsin, USA), RobertHall (AT&TLabsResearch, USA), and Javier Esparza (University of Edinburgh, United Kingdom). On behalf of the Program Committee, the Program Chairs would like to thank all the authors who submitted papers and all the external referees for\u00a0\u2026", "num_citations": "2\n", "authors": ["265"]}
{"title": "Program debugging and validation using semantic approximations and partial specifications\n", "abstract": " The technique of Abstract Interpretation [11] has allowed the development of sophisticated program analyses which are provably correct and practical. The semantic approximations produced by such analyses have been traditionally applied to optimization during program compilation. However, recently, novel and promising applications of semantic approximations have been proposed in the more general context of program validation and debugging [3,9,7].", "num_citations": "2\n", "authors": ["265"]}
{"title": "Visualization designs for constraint logic programming\n", "abstract": " We address the design and implementation of visual paradigms for observing the execution of constraint logic programs, aiming at debugging, tuning and optimization, and teaching. We focus on the display of data in CLP executions, where representation for constrained variables and for the constrains themselves are seeked. Two tools, VIFID and TRIFID, exemplifying the devised depictions, have been implemented, and are used to showcase the usefulness of the visualizations developed.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Debugging of constraint programs: the DiSCiPl methodology and tools\n", "abstract": " This introduction gives a general perspective of the debugging methodology and the tools developed in the ESPRIT IV project DiSCiPl Debugging Systems for Constraint Programming. It has been prepared by the editors of this volume by substantial rewriting of the DiSCiPl deliverable CP Debugging Tools.[1]               This introduction is organised as follows. Section 1 outlines the DiSCiPl view of debugging, its associated debugging methodology, and motivates the kinds of tools proposed: the assertion based tools, the declarative diagnoser and the visualisation tools. Sections 2 through 4 provide a short presentation of the tools of each kind. Finally, Section 5 presents a summary of the tools developed in the project. This introduction gives only a general view of the DiSCiPl debugging methodology and tools. For details and for specific bibliographic references the reader is referred to the subsequent chapters.", "num_citations": "2\n", "authors": ["265"]}
{"title": "CP Debugging Tools\n", "abstract": " The purpose of this Part 2 (\\CP Debugging Tools\") of deliverable D. WP1. 1. M1. 1 is to present the constraints debugging tools to be realised in the DiSCiPl project.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Independence in dynamically scheduled logic languages\n", "abstract": " The notion of independence has been used in conventional logic programming as the basis for several optimizations, including program parallelization, intelligent backtracking, and goal reordering. In this paper we extend this notion to logic programming languages with dynamic scheduling. This type of scheduling, in which some calls are dynamically \u201cdelayed\u201d until their arguments are sufficiently instantiated, is provided in most practical implementations because it offers advantages for both programming power and declarativeness. We focus on the notion of independence required for ensuring correctness and efficiency of parallelization within the independent and-parallel model. We also provide sufficient conditions for independence which can be evaluated \u201ca-priori\u201d (i.e., at run-time), as is needed in many practical applications.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Programming Languages: Implementations, Logics and Programs: 7th International Symposium, PLILP'95, Utrecht, The Netherlands, September 20-22, 1995. Proceedings\n", "abstract": " This book constitutes the proceedings of the Seventh International Symposium on Programming Languages: Implementations, Logics and Programs, PLILP'95, held in Utrecht, The Netherlands, in September 1995. The book presents 26 refereed full papers selected from 84 submissions; they report research on declarative programming languages and provide insights in the relation between the logic of those languages, implementation techniques, and the use of these languages in constructing real programs. In addition there are abstracts or full presentations of three invited talks as well as eight posters and demonstrations.", "num_citations": "2\n", "authors": ["265"]}
{"title": "&ACE: the And-parallel Component of ACE (A Progress Report on ACE)\n", "abstract": " ACE is a computational model for full Prolog capable of concurrently exploiting both Or and Independent And-parallelism. In this paper we focus on the specific implementation of the And-parallel component of the system, describing its internal organization, some optimizations to the basic model, and finally presenting some performance figures. Keywords: Independent And-parallelism, Orparallelism, implementation issues. 1 Introduction The ACE (And-Or/Parallel Copying-based Execution) model [6] uses stack-copying [1] and recomputation [5] to efficiently support combined Or-and Independent And-parallel execution of logic programs. ACE represents an efficient combination of Or-and independent And-parallelism in the sense that penalties for supporting either form of parallelism are paid only when that form of parallelism is actually exploited. Thus, in the presence of only Or-parallelism, execution in ACE is exactly as in the MUSE [2] system---a stack-copying based purely Or-parallel s...", "num_citations": "2\n", "authors": ["265"]}
{"title": "ACE: And/Or-parallel Copying-based Execution of Logic Programs\n", "abstract": " In this paper we present a novel execution model for parallel implementation of logic programs which is capable of exploiting both independent and-parallelism and or-parallelism in an efficient way. This model extends the stack copying approach, which has been successfully applied in the Muse system to implement or-parallelism, by integrating it with proven techniques used to support independent and-parallelism. We show how all solutions to non-deterministic andparallel goals are found without repetitions. This is done through recomputation as in Prolog (and in various and-parallel systems, like &-Prolog and DDAS), ie, solutions of and-parallel goals are not shared. We propose a scheme for the efficient management of the address space in a way that is compatible with the apparently incompatible requirements of both and-and or-parallelism. We also show how the full Prolog language, with all its extra-logical features, can be supported in our and-or parallel system so that its sequential semantics is preserved. The resulting system retains the advantages of both purely or-parallel systems as well as purely and-parallel systems. The stack copying scheme together with our proposed memory management scheme can also be used to implement models that combine dependent and-parallelism and or-parallelism, such as Andorra and Prometheus.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Some Considerations on the Compile-Time Analysis of Constraint Logic Programs\n", "abstract": " This paper discusses some issues which arise in the dataflow analysis of constraint logic programming (CLP) languages. The basic technique applied is that of abstract interpretation. First, some types of optimizations possible in a number of CLP systems (including efficient parallelization) are presented and the information that has to be obtained at compile-time in order to be able to implement such optimizations is considered. Two approaches are then proposed and discussed for obtaining this information for a CLP program: one based on an analysis of a CLP metainterpreter using standard Prolog analysis tools, and a second one based on direct analysis of the CLP program. For the second approach an abstract domain which approximates groundness (also referred to as\" definiteness\") information (ie constraint to a single valu\u00e9) and the related abstraction functions are presented.", "num_citations": "2\n", "authors": ["265"]}
{"title": "Experiments in Context-Sensitive Incremental and Modular Static Analysis in CiaoPP\n", "abstract": " Objectives. We have recently been working on combining these two techniques within CiaoPP in order to achieve incrementality both at the intra-and intermodular levels. Extending the context-sensitive, fine-grained, incremental analysis techniques to the modular setting requires dealing with the fact that the analysis of a module depends on the analysis of other modules in complex ways, through several paths to different versions (summaries) of the procedures. In order to bridge this gap we have developed a framework that analyzes separately the modules of a modular program, using context-sensitive fixpoint analysis while achieving both inter-modular (coarse-grain) and intra-modular (fine-grain) incrementality. Our objective is to give an overview (and demo) of the approach and, specially, report on the results (ie, incremental gains) obtained so far.Algorithm. The essence of the algorithm is that the concrete (possibly infinite) program execution trees are abstracted as graphs (essentially, regular trees), with the analysis information split by procedure (predicate), and partitioned by module, while tracking the dependencies between predicates and modules. We solve the problems related to the propagation of the fine-grain change information across module", "num_citations": "2\n", "authors": ["265"]}
{"title": "VeriFly: On-the-fly Assertion Checking via Incrementality\n", "abstract": " Assertion checking is an invaluable programmer's tool for finding many classes of errors or verifying their absence in dynamic languages such as Prolog. For Prolog programmers this means being able to have relevant properties such as modes, types, determinacy, non-failure, sharing, constraints, cost, etc., checked and errors flagged without having to actually run the program. Such global static analysis tools are arguably most useful the earlier they are used in the software development cycle, and fast response times are essential for interactive use. Triggering a full and precise semantic analysis of a software project every time a change is made can be prohibitively expensive. In our static analysis and verification framework this challenge is addressed through a combination of modular and incremental (context- and path-sensitive) analysis that is responsive to program edits, at different levels of granularity. We describe how the combination of this framework within an integrated development environment (IDE) takes advantage of such incrementality to achieve a high level of reactivity when reflecting analysis and verification results back as colorings and tooltips directly on the program text -- the tool's VeriFly mode. The concrete implementation that we describe is Emacs-based and reuses in part off-the-shelf \"on-the-fly\" syntax checking facilities (flycheck). We believe that similar extensions are also reproducible with low effort in other mature development environments. Our initial experience with the tool shows quite promising results, with low latency times that provide early, continuous, and precise assertion checking and other semantic\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Testing your (static analysis) truths\n", "abstract": " Static analysis is nowadays an essential component of many software development toolsets. Despite some notorious successes in the validation of compilers, comparatively little work exists on the systematic validation of static analyzers, whose correctness and reliability is critical if they are to be inserted in production environments. Contributing factors may be the intrinsic difficulty of formally verifying code that is quite complex and of finding suitable oracles for testing it. In this paper, we propose a simple, automatic method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach is its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "An evolutionary scheduling approach for trading-off accuracy vs. verifiable energy in multicore processors\n", "abstract": " This work addresses the problem of energy-efficient scheduling and allocation of tasks in multicore environments, where the tasks can allow a certain loss in accuracy in the output, while still providing proper functionality and meeting an energy budget. This margin for accuracy loss is exploited by using computing techniques that reduce the work load, and thus can also result in significant energy savings. To this end, we use the technique of loop perforation, that transforms loops to execute only a subset of their original iterations, and integrate this technique into our existing optimization tool for energy-efficient scheduling. To verify that a schedule meets an energy budget, both safe upper and lower bounds on the energy consumption of the tasks involved are needed. For this reason, we use a parametric approach to estimate safe (and tight) energy bounds that are practical for energy verification (and\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Pre-indexed Terms for Prolog\n", "abstract": " Indexing of terms and clauses is a well-known technique used in Prolog implementations (as well as automated theorem provers) to speed up search. In this paper we show how the same mechanism can be used to implement efficient reversible mappings between different term representations, which we call pre-indexings. Based on user-provided term descriptions, these mappings allow us to use more efficient data encodings internally, such as prefix trees. We show that for some classes of programs, we can drastically improve the efficiency by applying such mappings at selected program points.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Towards Assertion-based Debugging of Higher-Order (C) LP Programs\n", "abstract": " Higher-order constructs extend the expressiveness of first-order (Constraint) Logic Programming ((C)LP) both syntactically and semantically. At the same time assertions have been in use for some time in (C)LP systems helping programmers detect errors and validate programs. However, these assertion-based extensions to (C)LP have not been integrated well with higher order to date. Our work contributes to filling this gap by extending the assertion-based approach to error detection and program validation to the higher-order context, within (C)LP. It is based on an extension of properties and assertions as used in (C)LP in order to be able to fully describe arguments that are predicates.", "num_citations": "1\n", "authors": ["265"]}
{"title": "An Approach to Assertion-based Debugging of Higher-Order (C) LP Programs\n", "abstract": " Higher-order constructs extend the expressiveness of first-order (Constraint) Logic Programming ((C)LP) both syntactically and semantically. At the same time assertions have been in use for some time in (C)LP systems helping programmers detect errors and validate programs. However, these assertion-based extensions to (C)LP have not been integrated well with higher-order to date. This paper contributes to filling this gap by extending the assertion-based approach to error detection and program validation to the higher-order context within (C)LP. We propose an extension of properties and assertions as used in (C)LP in order to be able to fully describe arguments that are predicates. The extension makes the full power of the assertion language available when describing higher-order arguments. We provide syntax and semantics for (higher-order) properties and assertions, as well as for programs which contain such assertions, including the notions of error and partial correctness and provide some formal results. We also discuss several alternatives for performing run-time checking of such programs.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A sharing-based approach to supporting adaptation in service compositions\n", "abstract": " Data-related properties of the activities involved in a service composition can be used to facilitate several design-time and run-time adaptation tasks, such as service evolution, distributed enactment, and instance-level adaptation. A number of these properties can be expressed using a notion of sharing. We present an approach for automated inference of data properties based on sharing analysis, which is able to handle service compositions with complex control structures, involving loops and sub-workflows. The properties inferred can include data dependencies, information content, domain-defined attributes, privacy or confidentiality levels, among others. The analysis produces characterizations of the data and the activities in the composition in terms of minimal and maximal sharing, which can then be used to verify compliance of potential adaptation actions, or as supporting information in their\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "The Ciao clp (FD) Library. A Modular CLP Extension for Prolog\n", "abstract": " We present a new free library for Constraint Logic Programming over Finite Domains, included with the Ciao Prolog system. The library is entirely written in Prolog, leveraging on Ciao's module system and code transformation capabilities in order to achieve a highly modular design without compromising performance. We describe the interface, implementation, and design rationale of each modular component. The library meets several design goals: a high level of modularity, allowing the individual components to be replaced by different versions; high-efficiency, being competitive with other FD implementations; a glass-box approach, so the user can specify new constraints at different levels; and a Prolog implementation, in order to ease the integration with Ciao's code analysis components. The core is built upon two small libraries which implement integer ranges and closures. On top of that, a finite domain variable datatype is defined, taking care of constraint reexecution depending on range changes. These three libraries form what we call the FD kernel of the library. This FD kernel is used in turn to implement several higher-level finite domain constraints, specified using indexicals. Together with a labeling module this layer forms what we name \\emph{the FD solver}. A final level integrates the clp(FD) paradigm with our FD solver. This is achieved using attributed variables and a compiler from the clp(FD) language to the set of constraints provided by the solver. It should be noted that the user of the library is encouraged to work in any of those levels as seen convenient: from writing a new range module to enriching the set of FD constraints by\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Supporting pruning in tabled LP\n", "abstract": " This paper analyzes issues which appear when supporting pruning operators in tabled LP. A version of the once/1 control predicate tailored for tabled predicates is presented, and an implementation analyzed and evaluated. Using once/1 with answer-on-demand strategies makes it possible to avoid computing unneeded solutions for problems which can benefit from tabled LP but in which only a single solution is needed, such as model checking and planning. The proposed version of once/1 is also directly applicable to the efficient implementation of other optimizations, such as early completion, cut-fail loops (to, e.g., prune at the toplevel), if-then-else, and constraint-based branch-and-bound optimization. Although once/1 still presents open issues such as dependencies of tabled solutions on program history, our experimental evaluation confirms that it provides an arbitrarily large efficiency improvement in\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Reversible language extensions and their application in debugging\n", "abstract": " A range of methodologies and techniques are available to guide the design and implementation of language extensions and domain-specific languages on top of a base language. A simple yet powerful technique to this end is to formulate the extension via source-to-source transformation rules that are interleaved across the different compilation passes of the base language. Despite being a very successful approach, it has the main drawback that the input source code is lost in the process. As a result, during the whole workflow of program development (warning and error reporting, source-level debugging, or even program analysis) the tools involved report in terms of the base language, which is confusing to users. In this paper, we propose an augmented approach to language extensions for Prolog, where symbolic annotations are included in the target program. These annotations allow the selective\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "A segment-swapping approach for executing trapped computations\n", "abstract": " We consider the problem of supporting goal-level, independent and-parallelism (IAP) in the presence of non-determinism. IAP is exploited when two or more goals which will not interfere at run time are scheduled for simultaneous execution. Backtracking over non-deterministic parallel goals runs into the well-known trapped goal and garbage slot problems. The proposed solutions for these problems generally require complex low-level machinery which makes systems difficult to maintain and extend, and in some cases can even affect sequential execution performance. In this paper we propose a novel solution to the problem of trapped nondeterministic goals and garbage slots which is based on a single stack reordering operation and offers several advantages over previous proposals. While the implementation of this operation itself is not simple, in return it does not impose constraints on the scheduler. As\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Towards Parameterized Regular Type Inference Using Set Constraints\n", "abstract": " We propose a method for inferring \\emph{parameterized regular types} for logic programs as solutions for systems of constraints over sets of finite ground Herbrand terms (set constraint systems). Such parameterized regular types generalize \\emph{parametric} regular types by extending the scope of the parameters in the type definitions so that such parameters can relate the types of different predicates. We propose a number of enhancements to the procedure for solving the constraint systems that improve the precision of the type descriptions inferred. The resulting algorithm, together with a procedure to establish a set constraint system from a logic program, yields a program analysis that infers tighter safe approximations of the success types of the program than previous comparable work, offering a new and useful efficiency vs. precision trade-off. This is supported by experimental results, which show the feasibility of our analysis.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Sampler programs: The stable model semantics of abstract constraint programs revisited\n", "abstract": " Abstract constraint atoms provide a general framework for the study of aggregates utilized in answer set programming. Such primitives suitably increase the expressive power of rules and enable more concise representation of various domains as answer set programs. However, it is non-trivial to generalize the stable model semantics for programs involving arbitrary abstract constraint atoms. For instance, a nondeterministic variant of the immediate consequence operator is needed, or the definition of stable models cannot be stated directly using primitives of logic programs. In this paper, we propose sampler programs as a relaxation of abstract constraint programs that better lend themselves to the program transformation involved in the definition of stable models. Consequently, the declarative nature of stable models can be restored for sampler programs and abstract constraint programs are also covered if decomposed into sampler programs. Moreover, we study the relationships of the classes of programs involved and provide a characterization in terms of abstract but essentially deterministic computations. This result indicates that all nondeterminism related with abstract constraint atoms can be resolved at the level of program reduct when sampler programs are used as the intermediate representation.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Learning domain-specific heuristics for answer set solvers\n", "abstract": " In spite of the recent improvements in the performance of Answer Set Programming (ASP) solvers, when the search space is sufficiently large, it is still possible for the search algorithm to mistakenly focus on areas of the search space that contain no solutions or very few. When that happens, performance degrades substantially, even to the point that the solver may need to be terminated before returning an answer. This prospect is a concern when one is considering using such a solver in an industrial setting, where users typically expect consistent performance. To overcome this problem, in this paper we propose a technique that allows learning domain-specific heuristics for ASP solvers. The learning is done off-line, on representative instances from the target domain, and the learned heuristics are then used for choice-point selection. In our experiments, the introduction of domain-specific heuristics improved performance on hard instances by up to 3 orders of magnitude (and 2 on average), nearly completely eliminating the cases in which the solver had to be terminated because the wait for an answer had become unacceptable.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Runtime addition of integrity constraints in an abductive proof procedure\n", "abstract": " Abductive Logic Programming is a computationally founded representation of abductive reasoning. In most ALP frameworks, integrity constraints express domainspecific logical relationships that abductive answers are required to satisfy. Integrity constraints are usually known a priori. However, in some applications (such as interactive abductive logic programming, multi-agent interactions, contracting) it makes sense to relax this assumption, in order to let the abductive reasoning start with incomplete knowledge of integrity constraints, and to continue without restarting when new integrity constraints become known. In this paper, we propose a declarative semantics for abductive logic programming with addition of integrity constraints during the abductive reasoning process, an operational instantiation (with formal termination, soundness and completeness properties) and an implementation of such a framework based on the SCIFF language and proof procedure.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Dynamic Magic Sets for Disjunctive Datalog Programs\n", "abstract": " Answer set programming (ASP) is a powerful formalism for knowledge representation and common sense reasoning that allows disjunction in rule heads and nonmonotonic negation in bodies. Magic Sets are a technique for optimizing query answering over logic programs and have been originally defined for standard Datalog, that is, ASP without disjunction and negation. Essentially, the input program is rewritten in order to identify a subset of the program instantiation which is sufficient for answering the query. Dynamic Magic Sets (DMS) are an extension of this technique to ASP. The optimization provided by DMS can be exploited also during the nondeterministic phase of ASP systems. In particular, after some assumptions have been made during the computation, parts of the program may become irrelevant to a query (because of these assumptions). This allows for dynamic pruning of the search space, which may result in exponential performance gains. DMS has been implemented in the dlv system and experimental results confirm the effectiveness of the technique.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Program parallelization using synchronized pipelining\n", "abstract": " While there are well-understood methods for detecting loops whose iterations are independent and parallelizing them, there are comparatively fewer proposals that support parallel execution of a sequence of loops or nested loops in the case where such loops have dependencies among them. This paper introduces a refined notion of independence, called eventual independence, that in its simplest form considers two loops, say loop1 and loop2, and captures the idea that for every i there exists k such that the i\u2009+\u20091-th iteration of loop2 is independent from the j-th iteration of loop1, for all j\u2009\u2265\u2009k. Eventual independence provides the foundation of a semantics-preserving program transformation, called synchronized pipelining, that makes execution of consecutive or nested loops parallel, relying on a minimal number of synchronization events to ensure semantics preservation. The practical benefits of\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Inferring Determinacy and Mutual Exclusion in Logic Programs Using Mode and Type Analysis\n", "abstract": " We propose an analysis for detecting procedures and goals that are deterministic (ie, that produce at most one solution at most once), or predicates whose clause tests are mutually exclusive (which implies that at most one of their clauses will succeed) even if they are not deterministic. The analysis takes advantage of the pruning operator in order to improve the detection of mutual exclusion and determinacy. It also supports arithmetic equations and disequations, as well as equations and disequations on terms, for which we give a complete satisfiability testing algorithm, wrt available type information. We have implemented the analysis and integrated it in the CiaoPP system, which also infers automatically the mode and type information that our analysis takes as input. Experiments performed on this implementation show that the analysis is fairly accurate and efficient.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Program analysis with write invariant properties\n", "abstract": " This paper introduces a general purpose method, write invariant properties, for improving the precision of heap analysis techniques at a minimal computational cost. This method is specifically focused on eliminating the imprecision introduced when program states from multiple call paths are merged at call sites when using partially call-context sensitive interprocedural analysis techniques. The concept of write invariant properties allows the recovery of many important classes of information such as collection sizes, null pointer properties and object allocation sites.The concept of write invariant properties is based on the identification of heap object properties that are invariant during a method call provided certain parts of various objects are unmodified. By using a heap domain that can track this write information during the analysis we can extract the information for a given write invariant property at call entry and then, at the return of the call, we can assert that these properties must still hold (provided the required parts of the object are not modified). This paper presents a definition for write invariant properties in the concrete heap, translates this definition in a form usable in the abstract heap domain and integrates this into a basic partially call-sensitive analysis framework.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A sketch of a complete scheme for tabled execution based on program transformation\n", "abstract": " Tabled evaluation has proved to be an effective method to improve several aspects of goal-oriented query evaluation, including termination and complexity. \u201cNative\u201d implementations of tabled evaluation offer good performance, but also require significant implementation effort, affecting compiler and abstract machine. Alternatively, program transformation-based implementations, such as the original continuation call (CCall) technique, offer lower implementation burden at some efficiency cost. A limitation of the original CCall proposal is that it limits the interleaving of tabled and non-tabled predicates and thus cannot be used for arbitrary programs. In this work we present an extension of the CCall technique that allows the execution of arbitrary tabled programs, as well as some performance results. Our approach offers a useful tradeoff that can be competitive with state-of-the-art implementations, while\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Efficient Representations for Set-Sharing Analysis\n", "abstract": " The Set-Sharing domain has been widely used to infer at compile-time interesting properties of logic programs such as occurs-check reduction, automatic parallelization, and finite-tree analysis. However, performing abstract unification in this domain requires a closure operation that increases the number of sharing groups exponentially. Much attention has been given in the literature to mitigating this key inefficiency in this otherwise very useful domain. In this paper we present a novel approach to Set-Sharing: we define a new representation that leverages the complement (or negative) sharing relationships of the original sharing set, without loss of accuracy. Intuitively, given an abstract state shy over the finite set of variables of interest V, its negative representation is p (V)\\shy. Using this encoding during analysis dramatically reduces the number of elements that need to be represented in the abstract states and during abstract unification as the cardinality of the original set grows toward 2^. To further compress the number of elements, we express the set-sharing relationships through a set of ternary strings that compacts the representation by eliminating redundancies among the sharing sets. Our experimental evaluation shows that our approach can compress the number of relationships, reducing significantly the memory usage and running time of all abstract operations, including abstract unification.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Some Improvements over the Continuation Call Tabling Implementation Technique\n", "abstract": " Tabled evaluation has been proved an effective method to improve several aspects of goal-oriented query evaluation, including termination and complexity. Several \u201cnative\u201d implementations of tabled evaluation have been developed which offer good performance, but many of them need significant changes to the underlying Prolog implementation. More portable approaches, generally using program transformation, have been proposed but they often result in lower efficiency. We explore some techniques aimed at combining the best of these worlds, ie, developing a portable and extensible implementation, with minimal modifications at the abstract machine level, and with reasonably good performance. Our preliminary results indicate promising results.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A generic, context sensitive analysis framework for object oriented programs\n", "abstract": " Abstract interpreters rely on the existence of a fixpoint algorithm that calculates a least upper bound approximation of the semantics of the program. Usually, that algorithm is described in terms of the particular language in study and therefore it is not directly applicable to programs written in a different source language. In this paper we introduce a generic, block-based, and uniform representation of the program control flow graph and a language-independent fixpoint algorithm that can be applied to a variety of languages and, in particular, Java. Two major characteristics of our approach are accuracy (obtained through a topdown, context sensitive approach) and reasonable efficiency (achieved by means of memoization and dependency tracking techniques). We have also implemented the proposed framework and show some initial experimental results for standard benchmarks, which further support the feasibility of the solution adopted.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Optimizing Prolog for Small Devices: A Case Study\n", "abstract": " In this paper we present the design and implementation of a wearable application in Prolog. The application program is a \u201csound spatializer.\u201d Given an audio signal and real time data from a head-mounted compass, a signal is generated for stereo headphones that will appear to come from a position in space. We describe high-level and low-level optimizations and transformations that have been applied in order to fit this application on the wearable device. The end application operates comfortably in real-time on a wearable computer, and has a memory foot print that remains constant over time enabling it to run on continuous audio streams. Comparison with a version hand-written in C shows that the C version is no more than 20-40% faster; a small price to pay for a high level description. iv", "num_citations": "1\n", "authors": ["265"]}
{"title": "Authors\n", "abstract": " CiteSeerX \u2014 Authors Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Authors (2006) Cached Download as a PDF Download Links [clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [clip.dia.fi.upm.es] [www.clip.dia.fi.upm.es] [clip.dia.fi.upm.es] Save to List Add to Collection Correct Errors Monitor Changes by M. Carro , J. Morales , Henk L. Muller , G. Puebla , M. Hermenegildo , M. Carro , J. Morales , Henk L. Muller , G. Puebla , M. Hermenegildo Citations: 1 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract facultad de inform\u00e1tica universidad polit\u00e9cnica de tica : -\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "On abstraction-carrying code and certificate-size reduction\n", "abstract": " Abstraction-Carrying Code (ACC) is a framework for mobile code safety in which the code supplier provides a program together with an abstraction (or abstract model of the program) whose validity entails compliance with a predefined safety policy. The abstraction plays thus the role of safety certificate and its generation is carried out automatically by a fixed-point analyzer. The advantage of providing a (fixed-point) abstraction to the code consumer is that its validity is checked in a single pass (i.e., one iteration) of an abstract interpretation-based checker. A main challenge to make ACC useful in practice is to reduce the size of certificates as much as possible, while at the same time not increasing checking time. Intuitively, we only include in the certificate the information which the checker is unable to reproduce without iterating. We introduce the notion of reduced certif\u00edcate which characterizes the subset of the abstraction which a checker needs in order to validate (and re-construct) the full certificate in a single pass. Based on this notion, we show how to instrument a generic analysis algorithm with the necessary extensions in order to identify the information relevant to the checker.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Generation of reduced certificates in abstraction-carrying code\n", "abstract": " Abstraction-Carrying Code (ACC) has recently been proposed as a framework for mobile code safety in which the code supplier provides a program together with an abstraction whose validity entails compliance with a predefined safety policy. The abstraction plays thus the role of safety certif\u00edcate and its generation is carried out automatically by a fixed-point analyzer. The advantage of providing a (fixedpoint) abstraction to the code consumer is that its validity is checked in a single pass of an abstract interpretation-based checker. A main challenge is to reduce the size of certificates as much as possible while at the same time not increasing checking time. In this paper, we first introduce the notion of reduced certif\u00edcate which characterizes the subset of the abstraction which a checker needs in order to validate (and re-construct) the full certif\u00edcate in a single pass. Based on this notion, we then instrument a generic analysis algorithm with the necessary extensions in order to identify the information relevant to the checker.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A study of set-sharing analysis via cliques\n", "abstract": " We study the problem of efficient, scalable set-sharing analysis of logic programs. We use the idea of representing sharing information as a pair of abstract substitutions, one of which is a worst-case sharing representation called a clique set, which was previously proposed for the case of inferring pair-sharing. We use the clique-set representation for (1) inferring actual set-sharing information, and (2) analysis within a top-down framework. In particular, we define the abstract functions required by standard top-down analyses, both for sharing alone and also for the case of including freeness in addition to sharing. Our experimental evaluation supports the conclusion that, for inferring set-sharing, as it was the case for inferring pair-sharing, precision losses are limited, while useful efficiency gains are obtained. At the limit, the clique-set representation allowed analyzing some programs that exceeded memory capacity using classical sharing representations.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Experiments in abstract interpretation-based code certification for pervasive systems\n", "abstract": " Proof-carrying code (PCC) is a general methodology for certifying that the execution of a untrusted mobile code is safe. The basic idea is that the code supplier attaches a certificate to the mobile code which the consumer checks in order to ensure that the code is indeed safe. The potential benefit is that the consumer's task is reduced from the level of proving to the level of checking. Recently, the abstract interpretation techniques developed in logic programming have been proposed as a basis for PCC. This extended abstract reports on experiments which illustrate several issues involved in abstract interpretation-based certification. First, we describe the implementation of our system in the context of CiaoPP: the preprocessor of the Ciao multi-paradigm programming system. Then, by means of some experiments, we show how code certification is aided in the implementation of the framework. Finally, we discuss the\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Abstract interpretation-based mobile code certification\n", "abstract": " Current approaches to mobile code safety \u2013 inspired by the technique of Proof-Carrying Code (PCC) [4] \u2013 associate safety information (in the form of a certificate) to programs. The certificate (or proof) is created by the code supplier at compile time, and packaged along with the untrusted code. The consumer who receives the code+certificate package can then run a checker which, by a straightforward inspection of the code and the certificate, is able to verify the validity of the certificate and thus compliance with the safety policy. The main practical difficulty of PCC techniques is in generating safety certificates which at the same time: i) allow expressing interesting safety properties, ii) can be generated automatically and, iii) are easy and efficient to check.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Abstract specialization and its applications\n", "abstract": " The aim of program specialization is to optimize programs by exploiting certain knowledge about the context in which the program will execute. There exist many program manipulation techniques which allow specializing the program in different ways. Among them, one of the best known techniques is partial evaluation, often referred to simply as program specialization, which optimizes programs by specializing them for (partially) known input data. In this work we describe abstract specialization, a technique whose main features are: (1) specialization is performed with respect to \"abstract\" val\u00faes rather than \"concrete\" ones, and (2) abstract interpretation rather than standard interpretation of the program is used in order to prop\u00e1gate information about execution states. The concept of abstract specialization is at the heart of the specialization system in CiaoPP, the Ciao system preprocessor. In this paper we present a unifying view of the different specialization techniques used in CiaoPP and discuss their potential applications by means of examples. The applications discussed include program parallelization, optimization of dynamic scheduling (concurreney), and integration of partial evaluation techniques.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A Generic Model for Persistence in (C) LP Systems (and two useful implementations)\n", "abstract": " This paper describes a model of persistence in (C) LP languages and two different and practically very useful ways to implement this model in current systems. The fundamental idea is that persistence is a characteristic of certain dynamic predicates (which encapsulate state). The main effect of declaring a predicate persistent is that the dynamic changes made to such predicates persist from one execution to the next one. After proposing a syntax for declaring persistent predicates, a simple, file-based implementation of the concept is presented and some examples shown. It is then argued that the concept developed provides the most natural way to interface with databases. Such an interface to a relational database is then developed as simply one more implementation alternative to the simple, file-based approach. The abstraction of the concept of persistence from its implementation allows developing applications which can store data alternatively on files or databases with only a few simple changes to a declaration stating the location and modality used for persistent storage. The paper presents the model, the implementation approach in both the cases of using files and relational databases, a number of optimizations of the process (using information obtained from static global analysis and goal clustering), and performance results from an implementation of these ideas.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Automatic Unrestricted Independent And-Parallelism in Logic Programs\n", "abstract": " We present new algorithms which perform automatic parallelization via source-to-source transformations. The objective is to exploit goal-level, unrestricted independent andparallelism. The proposed algorithms use as targets new parallel execution primitives which are simpler and more flexible than the well-known &/2 parallel operator, which makes it possible to generate better parallel expressions by exposing more potential parallelism among the literals of a clause than is possible with &/2. The main differences between the algorithms stem from whether the order of the solutions obtained is preserved or not, and on the use of determinacy information. We briefly describe the environment where the algorithms have been implemented and the runtime platform in which the parallelized programs are executed. We also report on an evaluation of an implementation of our approach. We compare the performance obtained to that of previous annotation algorithms and show that relevant improvements can be obtained.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Agent Programming in Ciao Prolog\n", "abstract": " The agent programming landscape has been revealed as a natural framework for developing \u201cintelligence\u201d in AI. This can be seen from the extensive use of the agent concept in presenting (and developing) AI systems, the proliferation of agent theories, and the evolution of concepts such as agent societies (social intelligence) and coordination.", "num_citations": "1\n", "authors": ["265"]}
{"title": "An integration of partial evaluation in a generic abstract interpretation framework\n", "abstract": " Information generated by abstract interpreters has long been used to perform program specialization. Additionally, if the abstract interpreter generates a multivariant analysis, it is also possible to perform m\u00faltiple specialization. Information about val\u00faes of variables is propagated by simulating program execution and performing fixpoint computations for recursive calis. In contrast, traditional partial evaluators (mainly) use unfolding for both propagating val\u00faes of variables and transforming the program. It is known that abstract interpretation is a better technique for propagating success val\u00faes than unfolding. However, the program transformations induced by unfolding may lead to important optimizations which are not directly achievable in the existing frameworks for m\u00faltiple specialization based on abstract interpretation. The aim of this work is to devise a specialization framework which integrates the better information propagation of abstract interpretation with the powerful program transformations performed by partial evaluation, and which can be implemented via small modifications to existing generic abstract interpreters. With this aim, we will relate top-down abstract interpretation with traditional concepts in partial evaluation and sketch how the sophisticated techniques developed for controlling partial evaluation can be adapted to the proposed specialization framework. We conclude that there can be both practical and conceptual advantages in the proposed integration of partial evaluation and abstract interpretation.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Guest editors' introduction Special Issue: synthesis, transformation and analysis of logic programs 2\n", "abstract": " Guest editors' introduction Special Issue: synthesis, transformation and analysis of logic programs 2 \u041a\u041e\u0420\u0417\u0418\u041d\u0410 \u041f\u041e\u0418\u0421\u041a \u041d\u0410\u0412\u0418\u0413\u0410\u0422\u041e\u0420 \u0421\u0415\u0421\u0421\u0418\u042f \u041a\u041e\u041d\u0422\u0410\u041a\u0422\u042b \u0418\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0418\u042f \u041e \u041f\u0423\u0411\u041b\u0418\u041a\u0410\u0426\u0418\u0418 eLIBRARY ID: 170562 GUEST EDITORS' INTRODUCTION SPECIAL ISSUE: SYNTHESIS, TRANSFORMATION AND ANALYSIS OF LOGIC PROGRAMS 2 BOSSI A. , DEVILLE Y. 1 A. Bossi, Universita Ca' Foscari di Venezia, Via Torino 155, 30173, Mestre-Venezia, Italy \u0422\u0438\u043f: \u0441\u0442\u0430\u0442\u044c\u044f \u0432 \u0436\u0443\u0440\u043d\u0430\u043b\u0435 - \u043d\u0430\u0443\u0447\u043d\u0430\u044f \u0441\u0442\u0430\u0442\u044c\u044f \u042f\u0437\u044b\u043a: \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u0422\u043e\u043c: 41\u041d\u043e\u043c\u0435\u0440: 2-3 \u0413\u043e\u0434: 1999 \u0421\u0442\u0440\u0430\u043d\u0438\u0446\u044b: 139-140 \u0416\u0423\u0420\u041d\u0410\u041b: THE JOURNAL OF LOGIC PROGRAMMING \u0418\u0437\u0434\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e: Elsevier Science Publishing Company, Inc. ISSN: 0743-1066 \u0411\u0418\u0411\u041b\u0418\u041e\u041c\u0415\u0422\u0420\u0418\u0427\u0415\u0421\u041a\u0418\u0415 \u041f\u041e\u041a\u0410\u0417\u0410\u0422\u0415\u041b\u0418: \u0412\u0445\u043e\u0434\u0438\u0442 \u0432 \u0420\u0418\u041d\u0426 \u00ae : \u0434\u0430 \u0426\u0438\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0439 \u0432 \u0420\u0418\u041d\u0426 \u00ae : 0 \u0412\u0445\u043e\u0434\u0438\u0442 \u0432 \u044f\u0434\u0440\u043e \u0420\u0418\u041d\u0426 \u00ae : \u043d\u0435\u0442 \u0426\u0438\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0439 \u0438\u0437 \u044f\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "High Performance Parallel Logic Programming: The ACE Parallel Prolog System.\n", "abstract": " In recem years a lot of research bas been invested in parallel processing of numerical applications. However, parallel processing of Symbolic and Artificinl lnteHigence applications has largely been ignored. This talk presems a high performance system for parallel symbolic computing based on the logic programming paradigm. Logic programming is a paradigm of r> rogramming ba.~ ed on a 8Ubset of first order logic. An important property of logic programming languages 18 that their declarative semantics is largely intlependent of the order in which different operations are performed during execution of the program. The different, operations, lhtL~, can also be performed in parallel. As a rt $ Ult, it is possible to exploit substantial parallelism from Prolog programs, that have been wntten for sequential machines, without requiring the user to make any modifications to them. In this talk we present a model called ACE that accomplishes this. ACE is a computational model for the full Prolog language, capable of cooCUITelltly exploiting Or-parallelism, Independent And\u00b7 r> arallelism. and Dependent And-parallelism. In th1s talk we will give an overview of the ACE systPm. The ACE system is operational and has shown good results. The ped ormance figures for ACE are presented. Runtime optimizations techniques that allow us to considen\u2022 bly improve our parallel engine are also briefly described.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Automatic optimization of dynamic scheduling in logic programs\n", "abstract": " Many modern logic programming languages provide more flexible scheduling than the Prolog traditional left-to-right computation rule. Computation generally also proceeds following some fixed scheduling rule but certain goals are dynamically\" delayed\" until their arguments are sufficiently instantiated to allow the call to run efficiently. This general form of scheduling is referred to as dynamic scheduling. Languages with dynamic scheduling also include constraint programming languages in which constraints which are\" too hard\" are delayed. In addition, most implementations of concurrent (constraint) programming languages essentially also follow a fixed left to right scheduling rule with suspension, where such suspension is controlled by the conditions in the ask guards. In fact, it has been shown that many such languages can be directly translated into (constraint) logic programs with dynamic scheduling with\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "Improving the E ciency of Nondeterministic Independent And {parallel Systems\n", "abstract": " We present the design and implementation of the and-parallel component of ACE. ACE is a computational model for the full Prolog language that simultaneously exploits both or-parallelism and independent and-parallelism. A high performance implementation of the ACE model has been realized and its performance reported in this paper. We discuss how some of the standard problems which appear when implementing and-parallel systems are solved in ACE. We then propose a number of optimizations aimed at reducing the overheads and the increased memory consumption which occur in such systems when using previously proposed solutions. Finally, we present results from an implementation of ACE which includes the optimizations proposed. The results show that ACE exploits and-parallelism with high e ciency and high speedups. Furthermore, they also show that the proposed optimizations, which are applicable to many other and-parallel systems, signi cantly decrease memory consumption and increase speedups and absolute performance both in forwards execution and during backtracking.", "num_citations": "1\n", "authors": ["265"]}
{"title": "The CIAO Multi-Dialect Compiler and System: A Demo and Status Report\n", "abstract": " This paper presents a brief overview of the CIAO system, the capabilities of its compiler and the techniques used for supporting them. It essentially provides an overview of current work at the UPM CLIP group, performed in collaboration with the U. of Arizona, KU Leuven, Melbourne U., Monash U., and New Mexico State U. More details and references on this project can be found in the full version of the paper [Her]. CIAO is a compiler, run-time, and program development system which supports several programming paradigms and execution models in a unified framework. It was initially conceived as a compiler writer's workbench, ie, a testbed for advanced compilation techniques, but has since proven also quite useful for application development. Programming styles supported include those of Prolog/Logic Programming, CLP, and CC. In the belief that network applications are a good target for computational logic systems, the styles above are combined in CIAO with distributed and network-wide execution capabilities. Such capabilities allow both the transparent execution of parallel and concurrent code written for a multiprocessor in a distributed environment [CH95] and the straightforward generation of WWW-based applications [CH96a, CH96b, CHV96]. CIAO also supports several computation rules, including standard left-to-right SLD resolution and the determinate-first principle (as in the Andorra model [SCWY90, dMSC93]). The implementation of CIAO is based on the observation that, under certain assumptions, a large number of currently proposed LP, CLP, and CC programming and execution models can be explained and implemented\u00a0\u2026", "num_citations": "1\n", "authors": ["265"]}
{"title": "A Technique for Dynamic Term Size Computation via Program Transformation\n", "abstract": " Knowing the size of the terms to which program variables are bound at run-time in logic programs is required in a class of applications related to program optimization such as, for example, granularity analysis and selection among different algorithms or control rules whose performance may be dependent on such size. Such size is difficult to even approximate at compile time and is thus generally computed at run-time by using (possibly predefined) predicates which traverse the terms involved. We propose a technique based on program transformation which has the potential of performing this computation much more efficiently. The technique is based on finding program procedures which are called before those in which knowledge regarding term sizes is needed and which traverse the terms whose size is to be determined, and transforming such procedures so that they compute term sizes\" on the fly\". We present a systematic way of determining whether a given program can be transformed in order to compute a given term size at a given program point without additional term traversal. Also, if several such transformations are possible our approach allows finding minimal transformations under certain criteria. We also discuss the advantages and applications of our technique and present some performance results.", "num_citations": "1\n", "authors": ["265"]}
{"title": "A Note on Data-Parallelism and (And-Parallel) Prolog.\n", "abstract": " The term data parallelism is generally used to refer to a parallel semantics for (de nite) iteration in a programming language such that all iterations are performed simultaneously, synchronizing before any event that directly or indirectly involves communication among iterations. It is often also allowed that the results of the iterations be combined by reduction with an associative operator. In this context a de nite iteration as an iteration where the number of repetitions is known before the iteration is initiated. Data parallelism has been exploited in many languages, including Fortran-90 33], C* 42], Data Parallel C 20],* LISP 41], etc. Recently, much progress has been reported in the application of concepts from data-parallelism to logic programming, both from the theoretical and practical points of view, including the design of programming constructs and the development of many implementation techniques 43, 37, 5, 8, 28, 47, 34, 4, 6, 7].On the other hand, much progress has also been made (and continues to be made) in the exploitation of parallelism in logic programs based on control-derived notions such as and-parallelism and or-parallelism 11, 13, 14, 27, 21, 30, 31, 44, 32, 1, 2, 18, 19, 17, 16, 29, 40, 45, 38]. It appears interesting to explore, even if only informally, the relation between these two at rst sight di erent approaches to the exploitation of parallelism in logic programs. This informal exploration is one of the purposes of this note (the other being to explore the related issue of fast task startup).", "num_citations": "1\n", "authors": ["265"]}
{"title": "Independent AND-Parallel Narrowing\n", "abstract": " We present a parallel graph narrowing machine, which is used to implement a functional logic language on a shared memory multiprocessor. It is an extension of an abstract machine for a purely functional language. The result is a programmed graph reduction machine which integrates the mechanisms of uni cation, backtracking, and independent and-parallelism. In the machine, the subexpressions of an expression can run in parallel. In the case of backtracking, the structure of an expression is used to avoid the reevaluation of subexpressions as far as possible. Deterministic computations are detected. Their results are maintained and need not be reevaluated after backtracking.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Towards a translation algorithm from Prolog to the Andorra Kernel Language\n", "abstract": " The Andorra family of languages (which includes the Andorra Kernel Language-AKL) offers the advantage of supporting simultaneously the programming styles of Prolog and committed choice languages. However, Prolog programs cannot be executed directly on the AKL. This is due to a number of factors, from more or less trivial syntactic differences to more involved issues such as the treatment of cut and making the exploitation of certain types of parallelism possible. These differences can be bridged, however, through program analysis and transformation. This paper provides basic guidelines for constructing an automatic compiler of Prolog programs into AKL. First we revisit the Andorra Kernel Language flow of control and then introduce the basic translation paradigms for dealing with each type of transformation needed. Finally, we present a basic translation algorithm. The translation process benefits from an abstract interpretation-based global analysis of the program. We also put special attention on a style of translation which attempts to achieve independent and-parallel execution where possible, since this type of parallel execution preserves through the translation the user-perceived\" complexity\" of the originai Prolog program. This process is simplified in part by making use of some compile-time analysis technology developed in the context of the &-Pro} og compiler.", "num_citations": "1\n", "authors": ["265"]}
{"title": "Constructs and evaluations strategies for intelligent speculative parallelism\u2014armageddon revisited\n", "abstract": " This report addresses speculative parallelism (the assignment of spare processing resources to tasks which are not known to be strictly required for the successful completion of a computation) at the user and application level. At this level, the execution of a program is seen as a (dynamic) tree\u2014a graph, in general. A solution for a problem is a traversal of this graph from the initial state to a node known to be the answer. Speculative parallelism then represents the assignment of resources to multiple branches of this graph even if they are not positively known to be on the path to a solution.", "num_citations": "1\n", "authors": ["265"]}
{"title": "PL Estimating the computacional cost of logic programs\n", "abstract": " Information about the computational cost of programs is potentially useful for a variety of purposes, including selecting among dierent algorithms, guiding program transformations, in granularity control and mapping decisions in parallelizing compilers, and query optimization in deductive databases. Cost analysis of logic programs is complicated by nondeterminism: on the one hand, procedures can return multiple solutions, making it necessary to estimate the number of solutions in order to give nontrivial upper bound cost estimates; on the other hand, the possibility offailure has to betaken into account while estimating lower bounds. Here we discuss techniques to address these problems to some extent.", "num_citations": "1\n", "authors": ["265"]}
{"title": "On the virtues of spaghetti\n", "abstract": " In this paper, we examine the issue of memory management in the parallel execution of logic programs. We concentrate on non-deterministic and-parallel schemes which we believe present a relatively general set of problems to be solved, including most of those encountered in the memory management of or-parallel systems. We present a distributed stack memory management model which allows exible scheduling of goals. Previously proposed models (based on the\\Marker model\") are lacking in that they impose restrictions on the selection of goals to be executed or they may require consume a large amount of virtual memory. This paper rst presents results which imply that the above mentioned shortcomings can have signi cant performance impacts. An extension of the Marker Model is then proposed which allows exible scheduling of goals while keeping (virtual) memory consumption down. Measurements are presented which show the advantage of this solution. Methods for handling forward and backward execution, cut and roll back are discussed in the context of the proposed scheme. In addition, the paper shows how the same mechanism for exible scheduling can be applied to allow the e cient handling of the very general form of suspension that can occur in systems which combine several types of and-parallelism and more sophisticated methods of executing logic programs. We believe that the results are applicable to many and-and or-parallel systems.", "num_citations": "1\n", "authors": ["265"]}