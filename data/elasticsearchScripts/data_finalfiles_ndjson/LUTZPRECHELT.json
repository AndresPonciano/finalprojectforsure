{"title": "Early stopping-but when?\n", "abstract": " Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (\u201cearly stopping\u201d). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using difierent 12 problems and 24 difierent network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4% on average), but cost much more\u00a0\u2026", "num_citations": "1310\n", "authors": ["161"]}
{"title": "Proben1: A set of neural network benchmark problems and benchmarking rules\n", "abstract": " Proben1 is a collection of problems for neural network learning in the realm of pattern classification and function approximation plus a set of rules and conventions for carrying out benchmark tests with these or similar problems. Proben1 contains 15 data sets from 12 di erent domains. All datasets represent realistic problems which could be called diagnosis tasks and all but one consist of real world data. The datasets are all presented in the same simple format, using an attribute representation that can directly be used for neural network training. Along with the datasets, Proben1 defines a set of rules for how to conduct and how to document neural network benchmarking. The purpose of the problem and rule collection is to give researchers easy access to data for the evaluation of their algorithms and networks and to make direct comparison of the published results feasible. This report describes the datasets and the benchmarking rules. It also gives some basic performance measures indicating the di culty of the various problems. These measures can be used as baselines for comparison.", "num_citations": "1105\n", "authors": ["161"]}
{"title": "Finding plagiarisms among a set of programs with JPlag\n", "abstract": " JPlag is a web service that finds pairs of similar programs among a given set of programs. It has successfully been used in practice for detecting plagiarisms among student Java program submissions. Support for the languages C, C++ and Scheme is also available. We describe JPlag\u2019s architecture and its comparsion algorithm, which is based on a known one called Greedy String Tiling. Then, the contribution of this paper is threefold: First, an evaluation of JPlag\u2019s performance on several rather different sets of Java programs shows that JPlag is very hard to deceive. More than 90 percent of the 77 plagiarisms within our various benchmark program sets are reliably detected and a majority of the others at least raise suspicion. The run time is just a few seconds for submissions of 100 programs of several hundred lines each. Second, a parameter study shows that the approach is fairly robust with respect to its\u00a0\u2026", "num_citations": "881\n", "authors": ["161"]}
{"title": "Automatic early stopping using cross validation: quantifying the criteria\n", "abstract": " Cross validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (`early stopping'). The exact criterion used for cross validation based early stopping, however, is chosen in an ad-hoc fashion by most researchers or training is stopped interactively. To aid a more well-founded selection of the stopping criterion, 14 different automatic stopping criteria from three classes were evaluated empirically for their efficiency and effectiveness in 12 different classification and approximation tasks using multi-layer perceptrons with RPROP training. The experiments show that, on average, slower stopping criteria allow for small improvements in generalization (in the order of 4%), but cost about a factor of 4 longer in training time.", "num_citations": "812\n", "authors": ["161"]}
{"title": "Design recovery by automated search for structural design patterns in object-oriented software\n", "abstract": " The object-oriented design community has recently begun to collect so-called design patterns: cliches plus hints to their recommended use in software construction. The structural design patterns Adapter, Bridge, Composite, Decorator, and Proxy represent packaged problem/context/solution/properties descriptions to common problems in object-oriented design. Localizing instances of these patterns in existing software produced without explicit use of patterns can improve the maintainability of software. In the authors' approach, called the Pat system, design information is extracted directly from C++ header files and stored in a repository. The patterns are expressed as PROLOG rules and the design information is translated into facts. A single PROLOG query is then used to search for all patterns. They examined four applications, including the popular class libraries zApp and LEDA, with Pat. With some restrictions all\u00a0\u2026", "num_citations": "369\n", "authors": ["161"]}
{"title": "PROBEN 1-a set of benchmarks and benchmarking rules for neural network training algorithms\n", "abstract": " Proben1 is a collection of problems for neural network learning in the realm of pattern classification and function approximation plus a set of rules and conventions for carrying out benchmark tests with these or similar problems. Proben1 contains 15 data sets from 12 different domains. All datasets represent realistic problems which could be called diagnosis tasks and all but one consist of real world data. The datasets are all presented in the same simple format, using an attribute representation that can directly be used for neural network training. Along with the datasets, Proben1 defines a set of rules for how to conduct and how to document neural network benchmarking. The purpose of the problem and rule collection is to give researchers easy access to data for the evaluation of their algorithms and networks and to make direct comparison of the published results feasible. This report describes the datasets and the benchmarking rules. It also gives some basic performance measures indicating the difficulty of the various problems. These measures can be used as baselines for comparison.", "num_citations": "352\n", "authors": ["161"]}
{"title": "Investigation of the CasCor family of learning algorithms\n", "abstract": " Six learning algorithms are investigated and compared empirically. All of them are based on variants of the candidate training idea of the Cascade Correlation method. The comparison was performed using 42 different datasets from the PROBEN1 benchmark collection. The results indicate: (1) for these problems it is slightly better not to cascade the hidden units; (2) error minimization candidate training is better than covariance maximization for regression problems but may be a little worse for classification problems; (3) for most learning tasks, considering validation set errors during the selection of the best candidate will not lead to improved networks, but for a few tasks it will. \u00a9 1997 Elsevier Science Ltd.", "num_citations": "151\n", "authors": ["161"]}
{"title": "A quantitative study of experimental evaluations of neural network learning algorithms: Current research practice\n", "abstract": " In all, 190 articles about neural network learning algorithms published in 1993 and 1994 are examined for the amount of experimental evaluation they contain. Some 29% of them employ not even a single realistic or real learning problem. Only 8% of the articles present results for more than one problem using real world data. Furthermore, one third of all articles do not present any quantitative comparison with a previously known algorithm. These results suggest that we should strive for better assessment practices in neural network learning algorithm research. For the long-term benefit of the field, the publication standards should be raised in this respect and easily accessible collections of benchmark problems should be built.", "num_citations": "151\n", "authors": ["161"]}
{"title": "An interface for melody input\n", "abstract": " We present a software system, called Tunserver, which recognizes a musical tune whistled by the user, finds it in a database, and returns its name, composer, and other information. Such a service is useful for track retrieval at radio stations, music stores, etc., and is also a step toward the long-term goal of communicating with a computer much like one would with a human being. Tuneserver is implemented as a public Java-based WWW service with a database of approximately 10,000 motifs. Tune recognition is based on a highly error-resistant encoding, proposed by Parsons, that uses only the direction of the melody, ignoring the size of intervals as well as rhythm. We present the design and implementation of the tune recognition core, outline the design of the Web service, and describe the results obtained in an empirical evaluation of the new interface, including the derivation of suitable system parameters\u00a0\u2026", "num_citations": "115\n", "authors": ["161"]}
{"title": "An experiment measuring the effects of personal software process (PSP) training\n", "abstract": " The personal software process is a process improvement methodology aimed at individual software engineers. It claims to improve software quality (in particular defect content), effort estimation capability, and process adaptation and improvement capabilities. We have tested some of these claims in an experiment comparing the performance of participants who had just previously received a PSP course to a different group of participants who had received other technical training instead. Each participant of both groups performed the same task. We found the following positive effects: the PSP group estimated their productivity (though not their effort) more accurately, made fewer trivial mistakes, and their programs performed more careful error-checking; further, the performance variability was smaller in the PSP group in various respects. However, the improvements are smaller than the PSP proponents usually\u00a0\u2026", "num_citations": "92\n", "authors": ["161"]}
{"title": "A coding scheme development methodology using grounded theory for qualitative analysis of pair programming\n", "abstract": " A number of quantitative studies of pair programming (the practice of two programmers working together using just one computer) have partially conflicting results. Qualitative studies are needed to explain what is really going on. We support such studies by taking a grounded theory (GT) approach for deriving a coding scheme for the objective conceptual description of specific pair programming sessions independent of a particular research goal. The present article explains why our initial attempts at using GT failed and describes how to avoid these difficulties by a predetermined perspective on the data, concept naming rules, an analysis results metamodel, and pair coding. These practices may be helpful in all GT situations, particularly those involving very rich data such as video data. We illustrate the operation and usefulness of these practices by real examples derived from our coding work and present a few preliminary hypotheses regarding pair programming that have surfaced.", "num_citations": "76\n", "authors": ["161"]}
{"title": "Technical opinion: comparing Java vs. C/C++ efficiency differences to interpersonal differences\n", "abstract": " For each metric of interest (such as the runtime of the program) the values could be represented for each language group by their means, but in doing so a lot of interesting information would be thrown away. Therefore, the entire distribution is represented by so-called boxplots. The figures in this sidebar contain four boxplots: one for the values obtained from the Java programs, one for the C++ programs, one for the C programs, and one for the union of the C and C++ programs. The box indicates the location and extent of the \u201cmiddle half\u201d of the values, for instance, the left edge is positioned so that only 25% of the values are smaller (the 25% quantile), the right edge so that only 25% of the values are larger (75% quantile). The T-shaped whiskers indicate the 10% and 90% quantiles, respectively. The fat dot inside the box (Figures 1 and 2) is the 50% quantile\u2014the median. The letter M in the plot represents the\u00a0\u2026", "num_citations": "72\n", "authors": ["161"]}
{"title": "Some notes on neural learning algorithm benchmarking\n", "abstract": " New neural learning algorithms are often benchmarked only poorly. This article gathers some important DOs and DON'Ts for researchers in order to improve on that situation. The essential requirements are (1) Volume: benchmarking has to be broad enough, i.e. must use several problems; (2) Validity: common errors that invalidate the results have to be avoided; (3) Reproducibility: benchmarking has to be documented well enough to be completely reproducible; and (4) Comparability: benchmark results should, if possible, be directly comparable with the results achieved by others using different algorithms.", "num_citations": "71\n", "authors": ["161"]}
{"title": "Are scripting languages any good? A validation of Perl, Python, Rexx, and Tcl against C, C++, and Java.\n", "abstract": " Four scripting languages are introduced shortly and their theoretical and purported characteristics are discussed and related to three more conventional programming languages. Then the comparison is extended to an objective empirical one using 80 implementations of the same set of requirements, created by 74 different programmers. The limitations of the empirical data are laid out and discussed and then the 80 implementations are compared for several properties, such as run time, memory consumption, source text length, comment density, program structure, reliability, and the amount of effort required for writing them. The results indicate that, for the given programming problem,\u201cscripting languages\u201d(Perl, Python, Rexx, Tcl) are more productive than conventional languages. In terms of run time and memory consumption, they often turn out better than Java and not much worse than C or C++. In general, the differences between languages tend to be smaller than the typical differences due to different programmers within the same language.", "num_citations": "68\n", "authors": ["161"]}
{"title": "Functionality versus practicality: Employing existing tools for recovering structural design patterns\n", "abstract": " The object-oriented design community has recently begun to collect socalled software design patterns: descriptions of proven solutions common software design problems, packaged in a description that includes a problem, a context, a solution, and its properties. Design pattern information can improve the maintainability of software, but is often absent in program documentation. We present a system called Pat for localizing instances of structural design patterns in existing C++ software. It relies extensively on a commercial CASE tool and a PROLOG interpreter, resulting in a simple and robust architecture that cannot solve the problem completely, but is industrial-strength; it avoids much of the brittleness that many reverse engineering tools exhibit when applied to realistic software. The contribution of our work is not so much in the engineering value represented by this concrete system, but in its methodological\u00a0\u2026", "num_citations": "59\n", "authors": ["161"]}
{"title": "Connection pruning with static and adaptive pruning schedules\n", "abstract": " Neural network pruning methods on the level of individual network parameters (e.g. connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in the pruning methods known today (e.g. OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping\u00a0\u2026", "num_citations": "46\n", "authors": ["161"]}
{"title": "A study of experimental evaluations of neural network learning algorithms: Current research practice\n", "abstract": " A large body of research in arti cial neural networks is concerned with nding good learning algorithms to solve practical application problems. Such work tries to improve for instance the quality of found solutions (generalization), the probability of convergence, the ease of use, the learning speed, or some combination thereof. Currently, there exists no theory that quantitatively predicts the behavior of a new algorithm compared to other algorithms for any of these criteria. Consequently, experimental evaluation1 is needed to validate any claims of improvement made for a new algorithm or to characterize under which circumstances improvements can be expected.It seems that such evaluation is often not performed thoroughly enough, even in articles published by leading journals. Motivated by this impression, I decided to investigate this hypothesis by studying the current research practice empirically. In a recent study of experimental evaluation in computer science publications, the journal Neural Computation produced quite good results, far above average 1]. However, the only measure used in that work was the fraction of article space devoted to the evaluation and the articles considered were not only those about learning algorithms. The approach taken in the present study is more concrete at assessing the quality of an evaluation. I review the set of all articles presenting learning algorithms for practical problems that appeared in two renown neural network journals in 1993 and the rst half of 1994. In each article, the number of problems used in the algorithm evaluation and the number of other algorithms used for comparison were counted. While\u00a0\u2026", "num_citations": "46\n", "authors": ["161"]}
{"title": "The impact of inheritance depth on maintenance tasks: Detailed description and evaluation of two experiment replications\n", "abstract": " Inheritance is one of the main concepts of object-oriented technology. It is claimed that the use of inheritance improves productivity and decreases development time.John Daly et al. reported on two experiments evaluating the effects of inheritance depth on program maintenance. They found that maintenance was performed significantly quicker for software using three levels of inheritance, compared to equivalent \u2018flattened\u2019software without inheritance. A second experiment found that maintenance for software using five levels of inheritance tended to be slightly slower than for equivalent software without inheritance.", "num_citations": "43\n", "authors": ["161"]}
{"title": "An experiment on the usefulness of design patterns: Detailed description and evaluation\n", "abstract": " Advocates of software design patterns claim that using design patterns improves communication between software people. The controlled experiment that we describe in this report tests the hypotheses that software maintainers of well-structured, well-documented software containing design patterns can make changes (1) faster and (2) with less errors if the use of patterns is explicitly documented in the software.", "num_citations": "43\n", "authors": ["161"]}
{"title": "---A Set of Neural Network Benchmark Problems and Benchmarking Rules\n", "abstract": " Proben1 is a collection of problems for neural network learning in the realm of pattern classification and function approximation plus a set of rules and conventions for carrying out benchmark tests with these or similar problems. Proben1 contains 15 data sets from 12 different domains. All datasets represent realistic problems which could be called diagnosis tasks and all but one consist of real world data. The datasets are all presented in the same simple format, using an attribute representation that can directly be used for neural network training. Along with the datasets, Proben1 defines a set of rules for how to conduct and how to document neural network benchmarking. The purpose of the problem and rule collection is to give researchers easy access to data for the evaluation of their algorithms and networks and to make direct comparison of the published results feasible. This report describes the datasets and the benchmarking rules. It also gives some basic performance measures...", "num_citations": "42\n", "authors": ["161"]}
{"title": "Plat_Forms: A web development platform comparison by an exploratory experiment searching for emergent platform properties\n", "abstract": " Background: For developing Web-based applications, there exist several competing and widely used technological platforms (consisting of a programming language, framework(s), components, and tools), each with an accompanying development culture and style. Research question: Do Web development projects exhibit emergent process or product properties that are characteristic and consistent within a platform, but show relevant substantial differences across platforms or do team-to-team individual differences outweigh such differences, if any? Such a property could be positive (i.e., a platform advantage), negative, or neutral, and it might be unobvious which is which. Method: In a nonrandomized, controlled experiment, framed as a public contest called \u201cPlat_Forms,\u201d top-class teams of three professional programmers competed to implement the same requirements for a Web-based application within 30 hours\u00a0\u2026", "num_citations": "40\n", "authors": ["161"]}
{"title": "The 28: 1 grant-sackman legend is misleading, or: How large is interpersonal variation really\n", "abstract": " How long do different programmers take to solve the same task? In 1967, Grant and Sackman published their now famous number of 28: 1 interpersonal performance differences, which is both incorrect and misleading.This report presents the analysis of a much larger dataset of software engineering work time data with respect to the same question. It corrects the false 28: 1 value, proposes more appropriate metrics, presents the results for the larger dataset, and presents results of several further analyses: distribution shapes, effect sizes, and the performance of various significance tests.", "num_citations": "37\n", "authors": ["161"]}
{"title": "Measurements of MasPar MP-1216A Communicati on Operati ons\n", "abstract": " The MasPar MP-1 is a SIMD parall el computer wi th hi gh throughput on ne-grai ni rregular interprocessor communication. This report presents measurements of communication on a MP-1216A machine with 16384 processors. The timings cover all classes of communicati on operati ons provi ded in the standard MPL li brary pl us the router and xnet statements wi th a vari ety of communi cati on and processor acti vi ty patterns. Thi s report al so di scusses the results of these measurements, some of which are rather surprising.", "num_citations": "36\n", "authors": ["161"]}
{"title": "Adaptive parameter pruning in neural networks\n", "abstract": " Neural network pruning methods on the level of individual network parameters (eg connection weights) can improve generalization. An open problem in the pruning methods known today (OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This paper presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. The results of extensive experimentation indicate that lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required. Results of statistical signi cance tests comparing autoprune to the new method lprune as well as to backpropagation with early stopping are given for 14 di erent problems. prechelt@ icsi. berkeley. edu; permanent address: prechelt@ ira. uka. de", "num_citations": "34\n", "authors": ["161"]}
{"title": "Accelerating learning from experience: Avoiding defects faster\n", "abstract": " All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques.", "num_citations": "33\n", "authors": ["161"]}
{"title": "Why software repositories are not used for defect-insertion circumstance analysis more often: A case study\n", "abstract": " ContextRoot-cause analysis is a data-driven technique for developing software process improvements in mature software organizations. The search for individual process correlates of high defect densities, which we call defect insertion circumstance analysis (DICA), is potentially both effective and cost-efficient as one approach to be used when attempting a general defect root cause analysis. In DICA, data from existing repositories (version archive, bug tracker) is evaluated largely automatically in order to determine conditions (such as the people, roles, components, or time-periods involved) that correlate with higher-than-normal defect insertion frequencies. Nevertheless, no reports of industrial use of DICA have been published.ObjectiveDetermine the reasons why DICA is not used more often by practitioners.MethodWe use a single-case, typical-case, revelatory-type case study to evaluate in parallel the\u00a0\u2026", "num_citations": "32\n", "authors": ["161"]}
{"title": "Distributed-pair programming can work well and is not just distributed pair-programming\n", "abstract": " Background: Distributed Pair Programming can be performed via screensharing or via a distributed IDE. The latter offers the freedom of concurrent editing (which may be helpful or damaging) and has even more awareness deficits than screen sharing.Objective: Characterize how competent distributed pair programmers may handle this additional freedom and these additional awareness deficits and characterize the impacts on the pair programming process.Method: A revelatory case study, based on direct observation of a single, highly competent distributed pair of industrial software developers during a 3-day collaboration. We use recordings of these sessions and conceptualize the phenomena seen.Results: 1. Skilled pairs may bridge the awareness deficits without visible obstruction of the overall process. 2. Skilled pairs may use the additional editing freedom in a useful limited fashion, resulting in potentially\u00a0\u2026", "num_citations": "30\n", "authors": ["161"]}
{"title": "The onion has cancer: Some social network analysis visualizations of open source project communication\n", "abstract": " Background: People contribute to OSS projects in wildly different degrees, from reporting a single defect once and never coming back to spending many hours each workday on the project over several years-or anything in between. It is a common conception that these degrees of participation sort the participants into a number of similar groups which are layered like the peels of an onion: The onion model. Objective: We check whether this model of gradually different degrees of participation is valid with respect to the participation in OSS project mailing-list traffic. Methods: We perform social network analysis based on replies to mailing-list messages and use visualization to check the nature of three different groups of participants. Results: There appears to be a discontinuity with respect to core members: The degree to which very active core members (as opposed to less active co-developers) react to e-mails of\u00a0\u2026", "num_citations": "29\n", "authors": ["161"]}
{"title": "On knowledge transfer skill in pair programming\n", "abstract": " Context: General knowledge transfer is often considered a valuable effect or side-effect of pair programming, but even more important is its role for the success of the pair programming session itself: The partners often need to explain an idea to carry the process forward. Goal: Understand the mechanisms at work when knowledge is transferred during a pair programming session; provide practical advice for constructive behavior. Method: Qualitative data analysis of recordings of actual industrial pair programming sessions. Results: Some pairs are much more efficient in their knowledge transfer than others. These pairs manage to (1) not attempt to explain multiple things at once,(2) not lose sight of a topic,(3) clarify difficult points in stages. Conclusions: Pair programming requires skill beyond software development skill. To be able to identify knowledge needs and then push such knowledge to or pull it from the\u00a0\u2026", "num_citations": "28\n", "authors": ["161"]}
{"title": "Jtourbus: Simplifying program understanding by documentation that provides tours through the source code\n", "abstract": " Many small and medium-sized systems have little or no design documentation, which makes program understanding during maintenance enormously more difficult when performed by outsiders. Thus, if only minimal design documentation is available, which form should it take to maximize its usefulness? We suggest that it is helpful if the documentation describes a tour through the source code, leading the user directly to relevant details. This work reports an evaluation of this conceptual idea in the form of a controlled experiment with 59 student subjects working on a difficult program understanding task in the context of the 27 KLOC JHotDraw graphics framework. One group received a plain text documentation, the other received tour-structured documentation which they navigated by using an Eclipse plugin called JTourBus that we constructed for the experiment. The results indicate that program understanding can\u00a0\u2026", "num_citations": "27\n", "authors": ["161"]}
{"title": "A Controlled Experiment in Maintenance Comparing Design Patterns to Simple Solutions\n", "abstract": " CiNii \u8ad6\u6587 - A Controlled Experiment in Maintenance Comparing Design Patterns to Simple Solutions CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66 \u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248 \u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 A Controlled Experiment in Maintenance Comparing Design Patterns to Simple Solutions PRECHELT Lutz \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 PRECHELT Lutz \u53ce\u9332\u520a\u884c\u7269 IEEE Trans. on SE IEEE Trans. on SE 27, 12, 2001 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30c7\u30b6\u30a4\u30f3 \u30d1\u30bf\u30fc\u30f3\u4fdd\u5b88\u306e\u305f\u3081\u306eDP\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5236\u7d04\u6761\u4ef6\u306b\u95a2\u3059\u308b\u8003\u5bdf \u8868 \u79c0\u548c , \u5b88\u7530 \u7acb , \u6d77\u8c37 \u6cbb\u5f66 , \u6d77\u5c3b \u8ce2\u4e8c \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. KBSE, \u77e5\u80fd\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u5de5\u5b66 106(382), 13-18, 2006-11-17 \u53c2\u8003\u6587\u732e16\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 80012883091 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf/\u2026", "num_citations": "27\n", "authors": ["161"]}
{"title": "A series of controlled experiments on design patterns: Methodology and results\n", "abstract": " Software design patterns are an idea that is intuitively appealing and has found many advocates. However, as scientists we must be concerned about gathering hard evidence for the claims of bene cial consequences of design patterns. This article describes the major claims and derives the corresponding research questions. It discusses the methodology of a research programme for investigating these questions and sketches the practical constraints that make this research di cult. It then shortly summarizes three controlled experiments that were successfully carried out within these constraints and lists the main results and their consequences, such as: One should document design patterns when they are used and one must not apply design patterns without judgement of alternatives. Finally, design considerations of a fourth experiment are discussed.", "num_citations": "26\n", "authors": ["161"]}
{"title": "Replication of the first controlled experiment on the usefulness of design patterns: Detailed description and evaluation\n", "abstract": " Advocates of software design patterns claim that using design patterns improves communication between software developers. The controled experiment that we describe in this report tests the hypothesis that software maintainers of well-structured, well-documented software containing design patterns can make changes (1) faster and (2) with less errors if the use of patterns is explicitly documented in the software.", "num_citations": "25\n", "authors": ["161"]}
{"title": "Quality experience: a grounded theory of successful agile projects without dedicated testers\n", "abstract": " Context: While successful conventional software development regularly employs separate testing staff, there are successful agile teams with as well as without separate testers. Question: How does successful agile development work without separate testers? What are advantages and disadvantages? Method: A case study, based on Grounded Theory evaluation of interviews and direct observation of three agile teams; one having separate testers, two without. All teams perform long-term development of parts of e-business web portals. Results: Teams without testers use a quality experience work mode centered around a tight field-use feedback loop, driven by a feeling of responsibility, supported by test automation, resulting in frequent deployments. Conclusion: In the given domain, hand-overs to separate testers appear to hamper the feedback loop more than they contribute to quality, so working without testers is\u00a0\u2026", "num_citations": "22\n", "authors": ["161"]}
{"title": "Design patterns in software maintenance: An experiment replication at Freie Universit\u00e4t Berlin\n", "abstract": " An article published in 2001 reported a controlled experiment that compared maintenance of small programs using design patterns with maintenance of equivalent programs using simplified design solutions. A replication of that experiment was published in 2004. In 2010, a group of researchers from multiple countries picked this experiment as the subject of an attempt to perform a joint replication: Many groups performing an experiment using the same setup, each contributing a few data points to a larger overall data set. This article reports on one of those sub-replications. Only one of the results is statistically significant; it confirms the result of the original experiment stating that the simplified version of the GR program could be extended more quickly than the pattern version which used the Abstract Factory and Composite patterns. The article's main contributions, however, are (a) its description of the peculiarities of\u00a0\u2026", "num_citations": "22\n", "authors": ["161"]}
{"title": "Why we need an explicit forum for negative results\n", "abstract": " Current Computer Science CS research is primarily focused on solving engineering problems. Often though, promising attempts for solving a particular problem fail for non-avoidable reasons. This is what I call a negative result: something that should have worked does not. Due to the current CS publication climate such negative results today are usually camouflaged as positive results by non-evaluating or mis-evaluating the research or by redefining the problem to fit the solution. Such publication behavior hampers progress in CS by suppressing some valuable insights, producing spurious understanding, and misleading further research efforts. Specific examples given below illustrate and back up these claims. This paper is the announcement of a partial remedy: a permanent publication forum explicitly for negative CS research results, called the Forum for Negative Results, FNR. FNR will be a regular part of J. UCS.", "num_citations": "22\n", "authors": ["161"]}
{"title": "Efficient parallel execution of irregular recursive programs\n", "abstract": " Programs whose parallelism stems from multiple recursion form an interesting subclass of parallel programs with many practical applications. The highly irregular shape of many recursion trees makes it difficult to obtain good load balancing with small overhead. We present a system, called REAPAR, that executes recursive C programs in parallel on SMP machines. Based on data from a single profiling run of the program, REAPAR selects a load-balancing strategy that is both effective and efficient and it generates parallel code implementing that strategy. The performance obtained by REAPAR on a diverse set of benchmarks matches that published for much more complex systems requiring high-level problem-oriented explicitly parallel constructs. A case study even found REAPAR to be competitive to handwritten (low-level, machine-oriented) thread-parallel code.", "num_citations": "21\n", "authors": ["161"]}
{"title": "Documenting design patterns in code eases program maintenance\n", "abstract": " Software design patterns are a promising idea with many advocates. While subjective reports of their usefulness are available, scientific proof is still missing. We consider the case of programmers using design pattern documentation (in the form of comments in the source program) during maintenance. Is such pattern documentation (PD) helpful for understanding a program more quickly and designing better solutions for given maintenance tasks? We describe a controlled experiment for investigating this question, present its results, and conclude that design pattern documentation can speed up program changes as well as improve their quality.", "num_citations": "21\n", "authors": ["161"]}
{"title": "A controlled experiment on the effects of PSP training: Detailed description and evaluation\n", "abstract": " The Personal Software Process (PSP) is a methodology for systematic and continuous improvement of an individual software engineer\u2019s software production capabilities. The proponents of the PSP claim that the PSP methods improve in particular the program quality and the capability for accurate estimation of the development time, but do not impair productivity.We have performed a controlled experiment for assessing these and related claims. The experiment compares the performance of a group of students that have just previously participated in a PSP course to a comparable set of students from a \u201cnormal\u201d programming course. This report presents in detail the experiment design and setup, the results of the experiment, and our interpretation of the results.", "num_citations": "19\n", "authors": ["161"]}
{"title": "A community\u2019s perspective on the status and future of peer review in software engineering\n", "abstract": " Context:Pre-publication peer review of scientific articles is considered a key element of the research process in software engineering, yet it is often perceived as not to work fully well.Objective:We aim at understanding the perceptions of and attitudes towards peer review of authors and reviewers at one of software engineering\u2019s most prestigious venues, the International Conference on Software Engineering (ICSE).Method:We invited 932 ICSE 2014/15/16 authors and reviewers to participate in a survey with 10 closed and 9 open questions.Results:We present a multitude of results, such as: Respondents perceive only one third of all reviews to be good, yet one third as useless or misleading; they propose double-blind or zero-blind reviewing regimes for improvement; they would like to see showable proofs of (good) reviewing work be introduced; attitude change trends are weak.Conclusion:The perception of the\u00a0\u2026", "num_citations": "18\n", "authors": ["161"]}
{"title": "Liberating pair programming research from the oppressive driver/observer regime\n", "abstract": " The classical definition of pair programming (PP) describes it via two obvious roles: driver (the person currently having the keyboard) and observer (the other, alternatively called navigator). Although prior research has found some assumptions regarding these roles to be false, so far no alternative PP role model took hold. Instead, most PP research tacitly assumes the classical model to be true and thus PP to be no more difficult than solo programming. We perform qualitative research (using Grounded Theory Methodology) to find a more realistic role model, and have uncovered a suprising complexity: There are more than two roles, they are assumed and unassumed gradually, multiple roles can be held by one person at the same time, and some of their facets are subtle. Mastering this complexity requires specific PP skills beyond mere programming and communication skills. By ignoring such skills, previous PP\u00a0\u2026", "num_citations": "18\n", "authors": ["161"]}
{"title": "What Happens During Pair Programming.\n", "abstract": " Successful qualitative analysis of pair programming requires a terminology (such as a set of concepts or a coding scheme) that represents the observed phenomena on an appropriate abstraction level. On the one hand, different analysis goals will require different specialized terminology, on the other hand it would be helpful, if different studies used common terminology so that comparing and combining their results will be easier. We suggest to define terminology in layers: a PP foundation layer that is common to all analyses and more specialized study-specific layers on top. The present article presents this foundation layer which we have derived from audio/video analysis of pair programming sessions in Grounded Theory investigation style. Its concepts describe the individual observable human-to-human utterances and human-to-computer activities that occur during pair programming.", "num_citations": "18\n", "authors": ["161"]}
{"title": "Observations on knowledge transfer of professional software developers during pair programming\n", "abstract": " Context: Software development is knowledge-intense work, and so is pair programming. However, the importance of knowledge transfer in pair programming is usually only stressed for expert-novice constellations. Goal: Understand how knowledge transfer during pair programming works and eventually provide guidance for practitioners. Method: Detailed qualitative data analysis of full-length recordings of industrial pair programming sessions. Results: Expert software developers need to transfer knowledge, too, in order to conduct productive pair programming sessions. There is a diversity of beneficial and potentially problematic patterns, which even good pairs do not steadily apply or avoid, respectively. Conclusions: Pair programming is a versatile practice that even experts can profit from. Knowledge transfer skills do not automatically emerge from good software development skills, but can probably be learned.", "num_citations": "17\n", "authors": ["161"]}
{"title": "CuPit-2: A portable parallel programming language for artificial neural networks\n", "abstract": " CuPit-2 is a programming language specifically designed to express neural network learning algorithms. It provides most of the flexibility of general-purpose languages like C/C++, but results in much clearer and more elegant programs due to higher expressiveness, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). Furthermore, CuPit-2 programs can be compiled into efficient code for parallel machines; no changes are required in the source program. This article presents a description of the language constructs and reports performance results for an implementation of CuPit-2 on symmetric multiprocessors (SMPs).", "num_citations": "17\n", "authors": ["161"]}
{"title": "Cupit-a parallel language for neural algorithms: Language reference and tutorial\n", "abstract": " CuPit is a parallel programming language with two main design goals: 1. to allow the simple, problem-adequate formulation of learning algorithms for neural networks with focus on algorithms that change the topology of the underlying neural network during the learning process and2. to allow the generation of e cient code for massively parallel machines from a completely machine-independent program description, in particular to maximize both data locality and load balancing even for irregular neural networks. The idea to achieve these goals lies in the programming model: CuPit programs are objectcentered, with connections and nodes of a graph (which is the neural network) being the objects. Algorithms are based on parallel local computations in the nodes and connections and communication along the connections (plus broadcast and reduction operations). This report describes the design considerations and the resulting language de nition and discusses in detail a tutorial example program.", "num_citations": "16\n", "authors": ["161"]}
{"title": "7 types of cooperation episodes in Side-by-Side programming\n", "abstract": " In side-by-side programming, two programmers (typically working on related aspects of a project) move their computers so close to one another that they can effortlessly change between working alone and working together, where working alone is the primary mode. The technique was proposed in order to obtain some of the advantages of pair programming at much lower overhead. As a first step towards understanding how and when to use side-by-side programming, the present study aims at describing when and for what purpose side-by-side programmers get together to cooperate. The main result is a classification of the cooperation episodes by purpose and content into different types: Exchange project details, Exchange general knowledge, Discuss strategy, Discuss step, Debug work product, Integrate work products, and Make remark. These types were derived via the Grounded Theory method and are described conceptually in terms of the types of events of which they consist. All concepts used in these descriptions are grounded in actual observations.", "num_citations": "14\n", "authors": ["161"]}
{"title": "Understanding Pair Programming: The Base Layer\n", "abstract": " There has been and still is a lot of controversy on whether pair programming is a useful engineering technique-as if this would not strongly depend on the specific goals, task, and the pair's pair programming skill. Rather than providing still more bottom-line, quantitative results on pair programming, a research group at Freie Universit\u00e4t Berlin set out to decipher what is the actual process of pair programming and what is pair programming skill. This book contains a set of concepts that serves as the infrastructure for studies of pair programming that focus on qualitative data analysis. It promises to connect the results of such studies to one another. The book is oriented towards researchers only, not towards practitioners.", "num_citations": "13\n", "authors": ["161"]}
{"title": "Expanding the base for teaching of percutaneous coronary interventions: the explicit approach\n", "abstract": " Objectives: Accelerate and improve the training and learning process of operators performing percutaneous coronary interventions (PCI).   Background: Operator cognitive, in particular decision\u2010making skills and technical skills are a major factor for the success of coronary interventions. Currently, cognitive skills are commonly developed by three methods: (1) Cognitive learning of rules for which statistical evidence is available. This is very incomprehensive and isolates cognitive learning from skill acquisition. (2) Informal tutoring received from experienced operators, and (3) personal experience by trial\u2010and\u2010error are both very slow.   Methods: We propose in this concept article a conceptual framework to elicit, capture, and transfer expert PCI skills to complement the current approach. This includes the development of an in\u2010depth understanding of the nature of PCI skills, terminology, and nomenclature needed to\u00a0\u2026", "num_citations": "13\n", "authors": ["161"]}
{"title": "On understanding how to introduce an innovation to an Open Source project\n", "abstract": " We propose to research the introduction of Software Engineering inventions into Open Source projects (1) to help researchers with creating opportunities for evaluating their tools, methods and process designs in real-life settings, and (2) to help Open Source projects with improving their processes based on state-of-the-art knowledge. Such research will go beyond diffusion and dissemination of inventions to active introduction, and thus increase the chances of adoption. We will discuss the research approach, our preliminary insights, limitations of the approach, and why researchers interested in evaluating their own inventions should be interested in this research.", "num_citations": "13\n", "authors": ["161"]}
{"title": "A quantitative study of experimental neural network learning algorithm evaluation practices\n", "abstract": " 113 articles about neural network learning algorithms published in 1993 and 1994 are examined for the amount of experimental evaluation they contain. Every third of them does employ not even a single realistic or real learning problem. Only 6% of all articles present results for more than one problem using real world data. Furthermore, one third of all articles does not present any quantitative comparison with a previously known algorithm. These results indicate that the quality of research in the area of neural network learning algorithms needs improvement. The publication standards should be raised and easily accessible collections of example problems be built.", "num_citations": "12\n", "authors": ["161"]}
{"title": "Difficulty factors of obtaining access for empirical studies in industry\n", "abstract": " Context: The difficulty (not just effort) of obtaining access for software engineering empirical studies in industry varies greatly. Supposedly, some of this variance in difficulty is particular, stemming from properties of individual contexts (the industrial partners and their work), while the rest is repeatable, related to properties of the research question and research design. Question: What are these recurring difficulty factors that arise from research question and research design? What mechanisms produce their influence? Method: We use ideation and knowledge extraction from research experience to identify potential difficulty factors, use expert discussion to understand their mechanisms, and use concept analysis to arrange them into a taxonomy. We evaluate the result by comparatively applying it to two research efforts pursued by the same research group. Results: We find six scope factors, five problematic intervention\u00a0\u2026", "num_citations": "11\n", "authors": ["161"]}
{"title": "The search for a research method for studying OSS process innovation\n", "abstract": " Medium-sized, open-participation Open Source Software (OSS) projects do not usually perform explicit software process improvement on any routine basis. It would be useful to understand how to get such a project to accept a process improvement proposal and hence to perform process innovation. We want to determine an effective and feasible qualitative research method for studying the above question. We present (narratively) a case study of how we worked towards and eventually found such a research method. The case involves four attempts at collecting suitable data about innovation episodes (direct participation (twice), polling developers for episodes, manually finding episodes in mailing list archives) and the adaptation of the Grounded Theory data analysis methodology. Direct participation allows gathering rather rich data, but does not allow for observing a sufficiently large number of innovation\u00a0\u2026", "num_citations": "11\n", "authors": ["161"]}
{"title": "An empirical study of working speed differences between software engineers for various kinds of task\n", "abstract": " How long do different software engineers take to solve the same task? In 1967, Grant and Sackman published their now famous number of 28: 1 interpersonal performance differences, which is both incorrect and misleading. This article presents the analysis of a larger dataset of software engineering work time data taken from various controlled experiments. It corrects the false 28: 1 value, proposes more appropriate metrics, presents the results for the larger dataset, and further analyzes the data for distribution shapes and effect sizes.", "num_citations": "11\n", "authors": ["161"]}
{"title": "Some reasons why actual cross-fertilization in cross-functional agile teams is difficult\n", "abstract": " Background: Agile teams are supposed to be cross-functional in order to be complete (so they can work without external help). Cross-functionality is also supposed to produce cross-fertilization: Better ideas and solutions, problems prevented or detected earlier, etc. Question: What is motivating or demotivating team members to work in a cross-functional manner? Method: We conceptualize observations from five agile teams (work observations, interviews, group discussion) and from interviews with five agile consultants/coaches by applying Grounded Theory Methodology. Results: The inclination to interact cross functionally is moderated by at least six factors such as perceived inefficiency, a sense of responsibility for one's own functional domain, or the difficulty to find a level of detail that is suitable for the collaboration. Conclusion: Cross-fertilization is harder to get than one might expect and teams need to develop\u00a0\u2026", "num_citations": "10\n", "authors": ["161"]}
{"title": "Comparing adaptive and non-adaptive connection pruning with pure early stopping\n", "abstract": " Neural network pruning methods on the level of individual network parameters eg connection weights can improve generalization, as is shown in this empirical study. However, an open problem in the pruning methods known today OBD, OBS, autoprune, epsiprune is the selection of the number of parameters to be removed in each pruning step pruning strength. This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune which is superior to OBD on diagnosis tasks unless severe pruning early in the training process is required.", "num_citations": "10\n", "authors": ["161"]}
{"title": "Software testing\n", "abstract": " A scotish student of American History has to write a paper on'Frontier mentality in the American West from 1840 to 1880'. She logs on to the LIBSYS system and uses the search facility to discover if she can access original documents from that time. She discovers sources in various US university libraries and downloads copies of some of these. However, for one document, her university must confirm that she is a genuine student and that use is for non-commercial purposes. The student then uses the facility in LIBSYS that can request such permission and registers her request. If granted, the document will be downloaded to the registered library\u2019s server and printed for her. She receives a message from LIBSYS telling her that she will receive an e-mail message when the printed document is available for collection.", "num_citations": "9\n", "authors": ["161"]}
{"title": "Plat_Forms 2007: the web development platform comparison; evaluation and results\n", "abstract": " \u201cPlat_Forms\u201d is a competition in which top-class teams of three professional programmers competed to implement the same requirements for a web-based system within 30 hours, each team using a different technology platform (Java EE, PHP, or Perl). Plat_Forms intends to provide new insights into the real (rather than purported) pros, cons, and emergent properties of each platform. This report describes the evaluation of the solutions delivered by the participants and of the development process and presents the evaluation methodology as well as the results in great detail. It analyzes many aspects of each solution, both external (usability, functionality, reliability, robustness, etc.) and internal (size, structure, flexibility, modifiability, etc.). The many results we obtained cover a wide spectrum: First, there are results that many people would have called \u201cobvious\u201d or \u201cwell known\u201d, say, that Perl solutions tend to be more compact than Java solutions. Second, there are results that contradict conventional wisdom, say, that our PHP solutions appear in some (but not all) respects to be actually at least as secure as the others. Finally, one result makes a statement we have not seen discussed previously: The amount of variation between the teams tends to be smaller for PHP than for the other platforms in a whole variety of different respects. See Section 1.8 for the best ways to get an overview of this long document.", "num_citations": "9\n", "authors": ["161"]}
{"title": "Software cost estimation\n", "abstract": " CiNii \u8ad6\u6587 - Software Cost Estimation CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e \u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d \u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec\u7248\u306e\u516c\u958b\u306b\u3064\u3044\u3066 Software Cost Estimation SOMMERVILLE I. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMMERVILLE I. \u53ce\u9332\u520a\u884c\u7269 Software Engineering Software Engineering, 513-531, 1989 Addison-Wesley \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30de\u30cd\u30b8\u30e1\u30f3\u30c8\uff1a\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30b3\u30b9\u30c8\u898b\u7a4d\u308a\u6280\u8853 \u5927\u7b46 \u8c4a \u60c5\u5831\u51e6\u7406 33(8), 906-911, 1992-08-15 \u53c2\u8003\u6587\u732e24\u4ef6 \u88ab\u5f15\u7528\u6587\u732e2\u4ef6 CiNii\u5229\u7528\u8005\u30a2\u30f3\u30b1\u30fc\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10006726992 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u306e\u4e00\u90e8 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 \u306b\u306b/\u3067| (\u2026", "num_citations": "9\n", "authors": ["161"]}
{"title": "Actual process: A research program\n", "abstract": " Most process research relies heavily on the use of terms and concepts whose validity depends on a variety of assumptions to be met. As it is difficult to guarantee that they are met, such work continually runs the risk of being invalid. We propose a different and complementary approach to understanding process: Perform all description bottom-up and based on hard data alone. We call the approach actual process and the data actual events. Actual events can be measured automatically. This paper describes what has been done in this area already and what are the core problems to be solved in the future.", "num_citations": "8\n", "authors": ["161"]}
{"title": "INCA: A multi-choice model of cooperation under restricted communication\n", "abstract": " The advantage of the Iterated Prisoner's Dilemma as a model of interaction is its simplicity, which allows deep mathematical analysis. On the other hand, it is unrealistic to assume that there are only two possible actions, that one always knows in advance what constitutes cooperative actions, and that successive actions are independent of each other. The present work proposes an extended interaction model with multiple action choices, only vague a priori notions of cooperativeness with action selection constraints from previous choices and the possibility of gradual cooperation. The model assumes that the partners cannot communicate except for through their action choices. The model was explored experimentally. The results indicate that cooperation may not always be achieved as easily as the iterated prisoner's dilemma model suggests and that extended interaction models should be used as additional tools\u00a0\u2026", "num_citations": "8\n", "authors": ["161"]}
{"title": "The CuPit Compiler for the MasPar MP-A Literate Programming Document\n", "abstract": " This document contains the complete source code of the CuPit compiler for the MasPar MP-1/MP-2 SIMD parallel machines. The compiler is presented as a FunnelWeb literate programming document that contains definitions for the various specification files needed by the Eli compiler construction system. The exactly same set of files that enabled FunnelWeb to produce this document also enable Eli to produce the complete executable compiler, run time system, and standard library. In this document the source code is complemented by interspersed documentation text and several larger introduction text blocks and appendices, in particular a description of all errors found in the compiler during its development and use. The compiler takes CuPitsource code as input and produces MPL source code as output. CuPit is a special purpose language for neural network algorithms which dynamically change the topology of the neural network. The compiler is designed to optimize the irregular problems that arise when executing such algorithms for both data locality and load balancing. The compiler can produce several different versions of code:(1) a plain do-as-good-as-you-can-without-any-tricks one (unoptimized),(2) one that uses a better data distribution (statically optimized),(3) one that contains additional instructions to collect information about program behavior at run time, also known as the rti version, meaning\\run time information version\"(dynamically optimized).", "num_citations": "8\n", "authors": ["161"]}
{"title": "Konstruktive neuronale Lernverfahren auf Parallelrechnern\n", "abstract": " Sachensucher\\zu werden und mich nie in irgendeine Richtung dr angen wollten. Mein Chef, Prof. Walter Tichy, hat mir beigebracht, wie man Wissenschaft macht. Er hat mich gelehrt, da keine Panik angebracht ist, weil Forschung zun achst immer so unklar aussieht, und er hat f ur hervorragende Arbeitsbedingungen gesorgt. Die dritte conditio sine qua non war meine Freundin Sonja Gelhaus. Ohne ihre N ahe und Aufmunterung h atte ich vermutlich irgendwann aufgegeben oder einen Nervenzusammenbruch gehabt.", "num_citations": "8\n", "authors": ["161"]}
{"title": "Does Pair Programming Pay Off?\n", "abstract": " Pair programming means two developers working together closely on the same software development task on a single computer. It is not obvious whether this is an efficient practice: While a pair can often work faster and better, it also occupies two team members. We have studied this for several years by analyzing recordings of dozens of actual industrial pair programming sessions. Our observations suggest that pair programming that pays off will typically (need to) exhibit high process fluency. They also suggest that it is going to pay off when the pair members\u2019 knowledge is highly complementary.", "num_citations": "7\n", "authors": ["161"]}
{"title": "Spelling out risk reduction strategies for intracoronary stenting\n", "abstract": " Pascal 002 Biological and medical sciences/002B Medical sciences/002B26 Radiotherapy. Instrumental treatment. Physiotherapy. Reeducation. Rehabilitation, orthophony, crenotherapy. Diet therapy and various other treatments (general aspects)/002B26E Diseases of the cardiovascular system", "num_citations": "7\n", "authors": ["161"]}
{"title": "Configuration management\n", "abstract": " \u25cf To describe key CM activities namely CM planning, change management, version management and system building", "num_citations": "7\n", "authors": ["161"]}
{"title": "Plat_Forms 2007 Task: PbT\n", "abstract": " This document contains the requirements for the system to be built by the participants of the Plat_Forms 2007 contest. The system is called PbT (People by Temperament). It is to be written within 30 hours by a team of three people. For further details about the contest, please see www.plat-forms.org.", "num_citations": "7\n", "authors": ["161"]}
{"title": "Methodik und ergebnisse einer experimentreihe \u00fcber entwurfsmuster\n", "abstract": " Software-Entwurfsmuster sind eine intuitiv einleuchtende Idee, die viele Bef\u00fcrworter hat. Als Forscher d\u00fcrfen wir die behaupteten Wirkungen von Entwurfsmustern jedoch nicht einfach glauben, sondern m\u00fcssen sie gr\u00fcndlich pr\u00fcfen. Dieser Artikel beschreibt die wichtigsten Behauptungen und entwickelt daraus zugeh\u00f6rige Forschungsfragen. Wir diskutieren die Methodik f\u00fcr ein Forschungsprogramm, das die Fragen beantworten soll, und skizzieren praktische Beschr\u00e4nkungen, unter denen diese Forschung ablaufen mu\u00df. Es folgt eine kurze Beschreibung von drei kontrollierten Experimenten und den Hauptfolgerungen aus ihren Ergebnissen. So sollte man zum Beispiel die Benutzung von Entwurfsmustern in einem Entwurf genau dokumentieren und Entwurfsmuster nicht einsetzen, ohne alternative Entw\u00fcrfe zu pr\u00fcfen. Abschlie\u00dfend diskutieren wir ein im Rahmen des Forschungsprogramms geplantes\u00a0\u2026", "num_citations": "7\n", "authors": ["161"]}
{"title": "On understanding the power of judgement in percutaneous coronary intervention\n", "abstract": " Aim                                  Explain how research can advance the state-ofthe- practice in percutaneous coronary intervention (PCI).                                                                  Methods and results                                  Identifying the success factors of PCI; identifying decision- making performance (power of judgement) as the factor that could be advanced faster than is currently the case; explaining why and how such advancement needs a different research approach than those currently pursued in medical research; presenting initial results of this approach in the form of a set of basic concepts (pivoting around risk) that are useful for describing the decision-making process during a PCI.                                                                  Conclusion                                  Building a terminology (ontology) of PCI decision-making concepts and then eliciting expert knowledge about the decision-making process itself are\u00a0\u2026", "num_citations": "6\n", "authors": ["161"]}
{"title": "Exploiting domain-specific properties: Compiling parallel dynamic neural network algorithms into efficient code\n", "abstract": " Domain-specific constraints can be exploited to implement compiler optimizations that are not otherwise feasible. Compilers for neural network learning algorithms can achieve near-optimal colocality of data and processes and near-optimal balancing of load over processors, even for dynamically irregular problems. This is impossible for general programs, but restricting programs to the neural algorithm domain allows for the exploitation of domain-specific properties. The operations performed by neural algorithms are broadcasts, reductions, and object-local operations only; the load distribution is regular with respect to the (perhaps irregular) network topology; changes of network topology occur only from time to time. A language, compilation techniques, and a compiler implementation on the MasPar MP-1 are described and quantitative results for the effects of various optimizations used in the compiler are shown\u00a0\u2026", "num_citations": "6\n", "authors": ["161"]}
{"title": "Explaining pair programming session dynamics from knowledge gaps\n", "abstract": " Background: Despite a lot of research on the effectiveness of Pair Programming (PP), the question when it is useful or less useful remains unsettled. Method: We analyze recordings of many industrial PP sessions with Grounded Theory Methodology and build on prior work that identified various phenomena related to within-session knowledge build-up and transfer. We validate our findings with practitioners. Result: We identify two fundamentally different types of required knowledge and explain how different constellations of knowledge gaps in these two respects lead to different session dynamics. Gaps in project-specific systems knowledge are more hampering than gaps in general programming knowledge and are dealt with first and foremost in a PP session. Conclusion: Partner constellations with complementary knowledge make PP a particularly effective practice. In PP sessions, differences in system\u00a0\u2026", "num_citations": "5\n", "authors": ["161"]}
{"title": "PP-ind: A repository of industrial pair programming session recordings\n", "abstract": " PP-ind is a repository of audio-video-recordings of industrial pair programming sessions. Since 2007, our research group has collected data in 13 companies. A total of 57 developers worked together (mostly in groups of two, but also three or four) in 67 sessions with a mean length of 1: 35 hours. In this report, we describe how we collected the data and provide summaries and characterizations of the sessions.", "num_citations": "5\n", "authors": ["161"]}
{"title": "Ausgew\u00e4hlte Kapitel der Softwaretechnik\n", "abstract": " This was a project in which we had good domain experience and about six years of metrics, both team productivity and other analogous software of similar scope and functionality. The di erence with this project was that we switched from a functional design methodology to oo.", "num_citations": "5\n", "authors": ["161"]}
{"title": "A parallel programming model for irregular dynamic neural networks\n", "abstract": " The compilation of high-level programming languages for parallel machines faces two challenges: maximizing data/process locality and balancing load. No solutions for the general case are known that solve both problems at once. The present paper describes a programming model that allows to solve both problems for the special case of neural network learning algorithms, even for irregular networks with dynamically changing topology (constructive neural algorithms). The model is based on the observation that such algorithms predominantly execute local operations (on nodes and connections of the network), reductions, and broadcasts. The model is concretized in an object-centered procedural language called CuPit. The language is completely abstract: No aspects of the parallel implementation such as number of processors, data distribution, process distribution, execution model etc. are visible in user\u00a0\u2026", "num_citations": "5\n", "authors": ["161"]}
{"title": "Experience report: Teaching and using the personal software process (PSP)\n", "abstract": " PSP is a methodology for an individual software engineer's continuous self-improvement. Currently, few PSP experience reports are available from non-US sources, and hardly any from people other than the PSP inventor Watts Humphrey. We describe independent experiences with PSP. We find that PSP is a viable and useful approach and has quantifiable, positive impact. Problems in teaching PSP are in keeping students motivated and keeping them focused on general ideas instead of details. Problems in using a personal software process are keeping enough self-discipline and finding proper tool support.", "num_citations": "5\n", "authors": ["161"]}
{"title": "Comparison of MasPar MP-1 and MP-2 communication operations\n", "abstract": " Report 01/93 Pre93] describes the ndings of a series of communication measurements performed on a MasPar MP-1 series MP-1216A machine. The current report covers the same measurements performed on a MP-2 series MP-2216 machine. It compares the results and outlines and discusses the main di erences. In these measurements, raw router communication was sometimes faster, sometimes slower on the MP-2 than on the MP-1, depending on the parameters of the communication requested. The relative performance of the MP-2 varied between 93% and 120%. Xnet communication was faster in all cases (performance 100% to 175%). Complex functions from the communication library were also always faster (performance 100% to 180%). Some of these results contradict technical speci cations for the MP-1 and MP-2 published by MasPar.", "num_citations": "5\n", "authors": ["161"]}
{"title": "Experimental evaluation and modelling of the comprehension of indirect anaphors in a programming language\n", "abstract": " In this thesis I evaluate experimentally, how programmers understand indirect anaphors in source code and attempt to predict the comprehension of indirect anaphors in a cognitive model. Because indirect anaphors underspecify a relation, they might ease comprehension, but they will be hard to comprehend for those who do not know the underspecified relation. Indirect anaphors could thus be used as an input method only. Alternatively, indirect anaphors could also be shown during reading, if (a) they benefit readers sufficiently often, and (b) such cases can to be identified before the code is presented to the reader. Prior experiments in psycholinguistics indicate that condition (a) can be fulfilled with regard to on-line and off-line effects comprehension and the design of the experiment in this thesis follows the design of the existing studies. Existing cognitive models of text comprehension render the prediction of the comprehension of indirect anaphors in source code achievable based on the activation values computed in such models. An experiment was performed, in which programmers read source code while having their eye movements tracked and answered comprehension questions afterwards. Extensive postprocessing of the eye tracking data was performed to validate the data and to reduce the error in the data. It seems advisable to use required fixations in the experimental materials next time, to have a better basis for post-hoc error correction. Even after error correction exceptionally many data had to be removed. The ANOVA for the eye tracking data could not be performed as within-subject analysis, probably due to missing data. The\u00a0\u2026", "num_citations": "4\n", "authors": ["161"]}
{"title": "Plat_Forms 2011: Finding emergent properties of web application development platforms\n", "abstract": " Empirical evidence on emergent properties of different web development platforms when used in a non-trivial setting is rare to non-existent. In this paper we report on an experiment called Plat_Forms 2011 where teams of professional software developers implemented the same specification of a small to medium sized web application using different web development platforms, with 3 to 4 teams per platform. We define platforms by the main programming language used, in our case Java, Perl, PHP, or Ruby. In order to find properties that are similar within a web development platform but different across platforms, we analyzed several characteristics of the teams and their solutions, such as completeness, robustness, structure and aspects of the team's development process. We found certain characteristics that can be attributed to the platforms used but others that cannot. Our findings also indicate that for some\u00a0\u2026", "num_citations": "4\n", "authors": ["161"]}
{"title": "Four generic issues for tools-as-plugins illustrated by the distributed editor Saros\n", "abstract": " Saros is an Eclipse plugin for multi-writer, real-time, distributed collaborative text editing that also includes VoIP, chat, whiteboard, and screen sharing functionality. We present four problematic issues we encountered in the development of Saros: Providing portability, choosing a metaphor, handling clashes in display markups, and attributing incompatibilities correctly to their source. These issues will apply to many other plugins similarly. For three of them, no generic solution approach yet exists but should be worked out.", "num_citations": "4\n", "authors": ["161"]}
{"title": "Some non-usage data for a distributed editor: the saros outreach\n", "abstract": " After contacting more than 40 companies and 11 OSS projects regarding using our distributed editor Saros, we find that almost all of those many who have a use case for it, are reluctant to even try it out. It appears that distance matters even by anticipatory obedience.", "num_citations": "4\n", "authors": ["161"]}
{"title": "Credibility, or why should I insist on being convinced?\n", "abstract": " As software engineers, we all have our opinions about what works and what does not (or not so well), and we share stories of practice and experience that are eventually distilled into cultural knowledge and received wisdom. The trouble is that \u201cwhat everyone knows\u201d is often wrong, and although we are continually collecting information, we may not be assessing and integrating that information critically, or even giving ourselves the wherewithal to do so.", "num_citations": "4\n", "authors": ["161"]}
{"title": "The decision-making process in percutaneous coronary interventions\n", "abstract": " As the range and quality of endovascular instruments improve, the industry supplying these tools is increasingly trying to create the impression that percutaneous coronary intervention (PCI) is a simple and straightforward procedure. It is nothing of the sort. In PCI, in fact, an operator performs high-risk procedures under time pressure, with only indirect and incomplete angiographic information about the interventional site, and is constantly trading off one risk against another. Mastering this complex process requires years of experience and an attitude characterized by the constant willingness to learn and reflect when faced with each unexpected development or new situation. Unfortunately, to the best of our knowledge, the complexity of this repetitive and evolving process, consisting of the triad of angiographic imaging, image interpretation, and interventional action, has not yet been fully described. Furthermore, there is no data on the factors determining the decision trade-offs or their relative weights in various common situations. Consequently, physicians learning PCI today need a gifted teacher or have to gain experience the hard way, by trial and error, which exposes patients to avoidable risks and complications. Existing materials on learning PCI (eg,(1-4)) only describe formal training requirements and the parameters of institutional and operator competence. They provide no help on how to make hard decisions during an intervention.", "num_citations": "4\n", "authors": ["161"]}
{"title": "Object-Oriented Design\n", "abstract": " Objectives\u25cf To explain how a software design may be represented as a set of interacting objects that manage their own state and operations", "num_citations": "4\n", "authors": ["161"]}
{"title": "The co-evolution of a hype and a software architecture: experience of component-producing large-scale EJB early adopters\n", "abstract": " abaXX. components was one of the first AN software products fully based on Enterprise JavaBeans/spl trade/ (EJB) technology. We describe the evolution of its architecture as it moved from simply taking the initial EJB hype for the truth, through several intermediate stages, to using EJB simply as one of several encapsulated implementation techniques. So far, the public perception of how to use EJB properly evolved along a similar path, lagging 6 to 12 months behind.", "num_citations": "4\n", "authors": ["161"]}
{"title": "Quality Management\n", "abstract": " \u25cf To explain how measurement may be used in assessing software quality and the limitations of software measurement", "num_citations": "4\n", "authors": ["161"]}
{"title": "Transportable natural language interfaces for taxonomic knowledge representation systems\n", "abstract": " A general approach is presented for building transportable natural language interfaces for question answering systems based on a KL-ONE-like knowledge representation. An example system is described. The YAKS knowledge representation of concepts and relations is annotated with minimal syntactic information to generate a semantic case frame grammar with inheritance of cases. The generated grammar directs a case frame parser, which processes written input into instantiated case frames. These instantiations are easily translated into knowledge base queries. The same method is applicable to other object-oriented knowledge bases and other parsing techniques. The key property aimed for with this approach was cost-effective knowledge acquisition for the natural language interface. This property was achieved by basically exploiting as much as possible the knowledge about syntactic and semantic\u00a0\u2026", "num_citations": "4\n", "authors": ["161"]}
{"title": "The SIS project: Software reuse with a natural language approach\n", "abstract": " The SIS (Software Information System) project investigated a new approach to one part of the software reuse problem. The problem is how to nd and use a reusable component from a repository. The approach is (1) to provide a knowledge representation system that allows for modeling the components in the repository with user-de ned semantic categories,(2) to provide deductive capabilities in this knowledge representation system, to perform powerful retrieval operations, and (3) to complement the formal query facility with a natural language user interface in order to achieve ease of use and freedom from terminology constraints imposed by the names used in the knowledge base.In the SIS project, both the knowledge representation system YAKS and the natural language processing component SARA have been designed and built. We have developed a general approach for building transportable natural language interfaces for databases using KL-ONE-like knowledge representation languages. A prototype system called YAKR was built for which this approach was used to integrate YAKS and SARA. This report describes the philosophy, design, and implementation of YAKR, the rst experiences we have gathered with it, and the limitations we expect.", "num_citations": "4\n", "authors": ["161"]}
{"title": "Praxis versus Praktika: Die Naivit at der SE-Ausbildung in der Hochschule\n", "abstract": " Ein in der SE-Ausbildung gern zitiertes Faktum ist, da zwischen 60 und 90 Prozent der Kosten eines Softwareprodukts erst in der Wartungsphase anfallen, also dann, wenn es eigentlich schon\\fertig\" ist. Sollte es irgendwie gelingen, dank Anwendung von SE-Methoden den anf anglichen Herstellungsproze noch zu verbilligen, steigt dieser Anteil (ceteris paribus) sogar noch an. Das Hauptziel von SE mu es folglich sein, die Wartung von Software zu erleichtern.Diese Sichtweise sollte die SE-Ausbildung nat urlich widerspiegeln. Zwar ist es wahr, da die meisten SE-Methoden in etwa gleichem Ma e die Herstellung wie die Wartung eines Produkts erleichtern, aber dies ist durchaus nicht f ur alle Techniken richtig und der Wartungsgesichtspunkt wird im SE allzu oft nur in Nebens atzen ber ucksichtigt, anstatt gerade auf solche Techniken abzuheben, die sich vor allem auch in der Wartung positiv bemerkbar machen. Es\u00a0\u2026", "num_citations": "4\n", "authors": ["161"]}
{"title": "Double-blind is good but open would be better: Perceptions of peer review in the SE community\n", "abstract": " Peer review in software engineering is considered, same as for other disciplines, to be a key element of the research process, yet it is often perceived as not to work fully well. To understand the pains and gains in the peer review system, we ran a survey with open and closed questions with the authors and PC members of ICSE 2014/2015/2016. We received 241 responses (29% response rate). 67% of the respondents identified themselves as professors. We analyzed the responses quantitatively and qualitatively (with open coding). All questions were optional. Agreement scales had 10 points, so mild levels of agreement could be expressed but there was no undecided middle point. The resulting article appeared in Information and Software Technology in 2018 [1] and we also disclosed the anonymized data set [2].", "num_citations": "3\n", "authors": ["161"]}
{"title": "Role clarity deficiencies can wreck agile teams\n", "abstract": " Background                One of the twelve agile principles is to build projects around motivated individuals and trust them to get the job done. Such agile teams must self-organize, but this involves conflict, making self-organization difficult. One area of difficulty is agreeing on everybody\u2019s role.                                          Background                What dynamics arise in a self-organizing team from the negotiation of everybody\u2019s role?                                          Method                We conceptualize observations from five agile teams (work observations, interviews) by Charmazian Grounded Theory Methodology.                                          Results                We define role as something transient and implicit, not fixed and named. The roles are characterized by the responsibilities and expectations of each team member. Every team member must understand and accept their own roles (Local role clarity) and everbody else\u2019s roles (Team-wide role clarity). Role clarity allows a team to work smoothly and effectively and to develop its members\u2019 skills fast. Lack of role clarity creates friction that not only hampers the day-to-day work, but also appears to lead to high employee turnover. Agile coaches are critical to create and maintain role clarity.                                          Conclusions                Agile teams should pay close attention to the levels of Local role clarity of each member and Team-wide role clarity overall, because role clarity deficits are highly detrimental.", "num_citations": "3\n", "authors": ["161"]}
{"title": "Agile offsharing: Using pair work to overcome nearshoring difficulties\n", "abstract": " A major problem in distributed development situations, in particular offshoring situations, is often creating a proper understanding of the requirements at the remote site. It is difficult even if such understanding is available at the local site. This note argues why cross-site, synchronous, closely-coupled pair work at an engineering level, such as pair programming, may be a method for solving this problem and that corresponding research should be carried out.", "num_citations": "3\n", "authors": ["161"]}
{"title": "Two Comparisons of Programming Languages\n", "abstract": " At parties, stereotypical programmers tend to be the quiet kind. There is one topic, however, that is sure to produce not only their rapt attention, but also plenty of verbal contribution: programming languages! Everyone has lots of factual knowledge of several languages, accompanied by plenty of opinion regarding which one is best and why. Any evidence, too? Yes, seasoned programmers will also have a number of war stories to tell where something went particularly well or not well because of the language used:\u201cI estimated this to be a tenor fifteen-hour job, but then I decided to use language X instead and had it all up and running after just three hours\u2014and mighty readable, too!\u201d", "num_citations": "3\n", "authors": ["161"]}
{"title": "Plat_Forms: is there one best web development technology?\n", "abstract": " Introduction Software developers during their work face several fundamental choices with a multitude of options. There are methodological choices, where one selects among possible development processes, and technological choices regarding for instance development tools or base technologies for the software product. While a number of solid research results are available on some methodological topics (such as review or testing techniques), there is hardly any high-quality information on how specific technologies influence development success. There are narrow benchmarking studies comparing the execution performance of, for example, dababase management systems or application servers, there are feature lists comparing the functionality of base technology products, but there are essentially no holistic studies that compare directly how competing technologies shape a realistic product and how that may\u00a0\u2026", "num_citations": "3\n", "authors": ["161"]}
{"title": "On acquiring decision making skills for endovascular interventions.\n", "abstract": " To improve interventional training we propose a staged rational approach for decision making and skill acquisition. Education and training for endovascular interventions should start to develop the learners' decision-making skills by learning from explicit representations of master interventionist's tacit decision-making knowledge through implementation of the notions of generic interventional modules, interventional strategic and tactical designs. We hope that these suggestions will encourage action, stimulate dialogue and advance the precision of our learning, procedures, practice and patient care.", "num_citations": "3\n", "authors": ["161"]}
{"title": "Information management as an explicit role in OSS projects: A case study\n", "abstract": " Globally distributed teams of volunteers and communication by electronic means are at the core of Open Source Software development. To help projects in managing their information, we defined a light-weight, role-based process improvement and observed its use in a longitudinal case study. Results gathered by mailing-list analysis give insights into the different types of information managed and their relative importance: While technical content such as how-tos and to-dos is most frequent, the amount of information regarding decision making is surprisingly low.", "num_citations": "3\n", "authors": ["161"]}
{"title": "Plat_Forms--a contest: The web development platform comparison\n", "abstract": " \"Plat_Forms\" is a competition in which top-class teams of three programmers compete to implement the same requirements for a web-based system within 30 hours, each team using a different technology platform (Java EE, .NET, PHP, Perl, Python, or Ruby on Rails). The results will provide new insights into the real (rather than purported) pros, cons, and emergent properties of each platform. The evaluation will analyze many aspects of each solution, both external (usability, functionality, reliability, performance, etc.) and internal (structure, understandability, flexibility, etc.).", "num_citations": "3\n", "authors": ["161"]}
{"title": "How does individual variability influence schedule risk?: A small simulation with experiment data\n", "abstract": " The present report is a follow-up to our report on an experiment for investigating the effects from Personal Software Process (PSP) training [1]. It uses the work time data from the experiment plus several simplifying assumptions in order to assess by stochastic simulation how much a reduction in the performance variability of individual programmers might reduce the uncertainty of project time requirements.", "num_citations": "3\n", "authors": ["161"]}
{"title": "CuPit-2: a parallel language for neural algorithms; language reference and tutorial\n", "abstract": " CuPit-2 is a parallel programming language with two main design goals: 1. to allow the simple, problem-adequate formulation of learning algorithms for neural networks with focus on algorithms that change the topology of the underlying neural network during the learning process and2. to allow the generation of efficient code for massively parallel machines from a completely machine-independent program description, in particular to maximize both data locality and load balancing even for irregular neural networks. The idea to achieve these goals lies in the programming model: CuPit-2 programs are objectcentered, with connections and nodes of a graph (which is the neural network) being the objects. Algorithms are based on parallel local computations in the nodes and connections and communication along the connections (plus broadcast and reduction operations). This report describes the design considerations and the resulting language definition and discusses in detail a tutorial example program.", "num_citations": "3\n", "authors": ["161"]}
{"title": "A motivating example problem for teaching adaptive systems design\n", "abstract": " There are some general lessons to be learned about the design of adaptive systems and the best method to learn them is an appropriate exercise. This paper lists these lessons, discusses why it is difficult to use examples from real applications for the exercise, and suggests a game to be used as an alternative example problem.", "num_citations": "3\n", "authors": ["161"]}
{"title": "On the status and future of peer review in software engineering\n", "abstract": " Context: Pre-publication peer review of scientific articles is considered a key element of the research process in software engineering, yet it is often perceived as not to work fully well. Objective: We aim at understanding the perceptions of and attitudes towards peer review of authors and reviewers at one of software engineering\u2019s most prestigious venues, the International Conference on Software Engineering (ICSE). Method: We invited 932 ICSE 2014/15/16 authors and reviewers to participate in a survey with 10 closed and 9 open questions. Results: We present a multitude of results, such as: Respondents perceive only one third of all reviews to be good, yet one third as useless or misleading; they propose double-blind or zero-blind reviewing regimes for improvement; they would like to see showable proofs of (good) reviewing work be introduced; attitude change trends are weak. Conclusion: The perception of the current state of software engineering peer review is fairly negative. Also, we found hardly any trend that suggests reviewing will improve by itself over time; the community will have to make explicit efforts. Fortunately, our (mostly senior) respondents appear more open for trying different peer reviewing regimes than we had expected.", "num_citations": "2\n", "authors": ["161"]}
{"title": "A Controlled Experiment Measuring the E ects of Personal Software Process (PSP) Training\n", "abstract": " The Personal Software Process is a process improvement methodology aiming at individual software engineers. It claims to improve software quality (in particular defect content), effort estimation capability, and process adaptation and improvement capabilities. We have tested some of these claims in a controlled experiment comparing the performance of participants who had just previously received a PSP course to a control group of participants who had received other technical training instead. Each participant of both groups performed the same task.The results indicate that the improvements are smaller than the PSP proponents usually assume. We could not nd improved e ort estimation capabilities and only a small improvement of program reliability. Further important observations are the low actual usage of PSP techniques in the PSP group and the group's smaller variability in performance compared to the control group. We conjecture that PSP training does not typically realize its potential bene ts (as seen in some industrial PSP success stories) when programmers are left alone with motivating themselves to actually use the PSP techniques.", "num_citations": "2\n", "authors": ["161"]}
{"title": "Data locality and load balancing for parallel neural network learning\n", "abstract": " Compilers for neural network learning algorithms can achieve near-optimal co-locality of data and processes and near-optimal balancing of load over processors for irregular problems. This is impossible for general programs, but restricting programs to that particular problem domain allows for the exploitation of domain-speci c properties: The operations performed by neural algorithms are broadcasts, reductions, and object-local operations only; the load distribution is regular with respect to the (perhaps irregular) network topology; changes of network topology occur only from time to time.Compilation techniques and a compiler implementation for the MasPar MP-1 is described and quantitative results for the e ects of various optimizations used in the compiler are given. Experiments with weight pruning algorithms yielded speedups of 28% due to load balancing, and of 195% due to data locality. Two other optimizations, connection allocation and selecting the number of replicates, speed programs up by about 50% or 100%, respectively.", "num_citations": "2\n", "authors": ["161"]}
{"title": "Ziele und Wege f\u00fcr Softwaretechnik-Praktika\n", "abstract": " Bei der Planung eines Praktikums in Software-Engineering gilt es zun\u00e4chst zu entscheiden, was die hauptsachliche Zielsetzung des Praktikums sein soli. Die zahlreichen Lernaspekte, die von einem solchen Praktikum angesprochen werden k\u00f6nnen, sind n\u00e4mlich mit verschiedenen Durchf\u00fchrungsvarianten unterschiedlich gut abzudecken. Eine bewu\u00dfte Auswahl der Schwerpunktsetzung ist daher Voraussetzung f\u00fcr richtige Planung des Praktikums. Dieser Beitrag f\u00fchrt m\u00f6gliche Ausbildungsziele an und zeigt grunds\u00e4tzliche Merkmale von Praktika zu ihrer Erreichung auf. Aus der teilweisen Widerspr\u00fcchlichkeit der Ziele ergibt sich eine Liste von fundamentalen Praktikums ausrichtungen.", "num_citations": "2\n", "authors": ["161"]}
{"title": "Grund uberlegungen zur Ausbildung in Software-Engineering im Studiengang Informatik\n", "abstract": " Begr undung: Da die Software-Krise noch l angst nicht behoben ist, besteht immer die Neigung, da wer programmieren kann auch dazu herangezogen wird. Wer kein SE gelernt hat, bringt es nur schwer fertig, dabei wohlstrukturiert zu arbeiten. Unstrukturiertes Arbeiten f uhrt jedoch zumindest im Falle l angerer Lebensdauer der Software Wartung! schon bei relativ kleinen Komponenten zu unbeherrschbaren Zust anden.Folgerung: Die Grundausbildung in SE mu f ur alle erfolgen, die eine Grundausbildung in Programmieren bekommen also nicht nur in der Informatik!. Um der Bildung von schlechten Angewohnheiten von vornherein vorzubeugen, sollte sie auch zeitlich mit den ersten Programmier ubungen einhergehen. These 2:", "num_citations": "2\n", "authors": ["161"]}
{"title": "How layered reuse can support harmful micropolitics: SAP ERP in surgery planning\n", "abstract": " Background: Enterprise Resource Planning (ERP) software was originally conceived for manufacturing contexts and has later been generalized to cover service contexts as well. In hospitals, adoption was accelerated by an increasing commodification and marketization of health care.Questions: What impact do the manufacturing roots of SAP ERP have when the software is applied for service delivery in a hospital?Method: A typical-case Yin Case Study about the surgery planning and execution processes in a large university hospital.Results: One goal of the ERP implementation was to optimize the utilization of the operation rooms (ORs). This goal was largely missed because of too-low quality of planning data effectuated intentionally by the surgeons in order to pursue micropolitical agendas. This was possible because the software paid no attention to micropolitics at all, largely due to its roots in manufacturing\u00a0\u2026", "num_citations": "1\n", "authors": ["161"]}
{"title": "Four presumed gaps in the software engineering research community's knowledge\n", "abstract": " Background: The state of the art in software engineering consists of a myriad of contributions and the gaps between them; it is difficult to characterize. Questions: In order to help understanding the state of the art, can we identify gaps in our knowledge that are at a very general, widely relevant level? Which research directions do these gaps suggest? Method: 54 expert interviews with senior members of the ICSE community, evaluated qualitatively using elements of Grounded Theory Methodology. Results: Our understanding of complexity, of good-enoughness, and of developers' strengths is underdeveloped. Some other relevant factors' relevance is apparently not clear. Software engineering is not yet an evidence-based discipline. Conclusion: More software engineering research should concern itself with emergence phenomena, with how engineering tradeoffs are made, with the assumptions underlying research works, and with creating certain taxonomies. Such work would also allow software engineering to become more evidence-based.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Tracking Innovation in Open Source Software Projects\n", "abstract": " This thesis is a long term study on the management and communication behavior of Open Source Communities. In particular the strategies and techniques towards changing software engineering processes were investigated. This thesis is based on previous research on this topic, in which one year (2007) of email communication of 13 mid-sized Open Source projects was analyzed. This thesis presents a long term analysis of the same projects, but over the course of the years 2007 to 2016. The theory that was grounded on the data collected in one year was extended and enriched with more data and thus more insights.A qualitative research approach, inspired by Grounded Theory Methodology was used. It was shown that it is possible to perform a full scale search for the change events that were continuing across the ten year observation period while being started in 2007. Also the theory for such process changes was adapted to fit new insights, such as the influence of an outside event on the motivation of the developer who attempts a change.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Methoden zur kontinuierlichen Performancemessung bei der agilen Entwicklung von Webanwendungen\n", "abstract": " Die vorliegende Diplomarbeit entstand bei der Firma Infopark AG im Zeitraum von April bis September 2009. Das Thema wurde zusammen mit Herrn Thomas Witt, Produktmanager der Infopark AG, erarbeitet. Gegenstand ist die Entwicklung von Methoden und eines Prototypen zur Durchf\u00fchrung kontinuierlicher Leistungstests. Sie sollen im Zuge der agilen Entwicklung von Webanwendungen verwendet werden. Die fortw\u00e4hrende Performanceoptimierung soll Teil des agilen Prozesses werden. Durch den andauernden Leistungstest erkannte Engp\u00e4sse sollen durch Refaktorisierung behoben und die Leistung in der zu entwickelnden Webanwendungen gesteigert werden.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Message from the RESER 2013 workshop chairs\n", "abstract": " The RESER workshop provides a venue in which empirical software engineering researchers can discuss the theoretical foundations and methods of replication, as well as present the results of specific replicated studies.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Plat_Forms: Contests as an alternative approach to SE empirical studies in industry\n", "abstract": " Plat_Forms is a set of three studies that search for process and product properties emerging from the use of specific web development platforms. Its first key property is the use of a contest format; this leads to easier, broader, and more uniform data collection compared to on-site industrial studies. As a second key property, the three instances do not merely serve to extend or corroborate each others' results, they also serve as a time series for characterizing changes in the underlying software development reality over the course of a few years - an important aspect that has received little attention in empirical software engineering research so far.", "num_citations": "1\n", "authors": ["161"]}
{"title": "The Forum for Negative Results (FNR)\n", "abstract": " In September 1997, J. UCS published an article titled\" Why we Need an Explicit Forum for Negative Results\"[Prechelt, 1997]. It argued that when a plausible approach for solving a computer science or software engineering problem had failed to work out, it was silly for the scientific system not to publish the attempt iff a useful insight had been gained along the way nevertheless. Due to the strong bias of essentially all Computer Science publication venues towards\" successful\" research results, it was thus required to call for such negative results explicitly in order to avoid that those results would either be misleadingly disguised as successes or disappear in some closet. The article declared that J. UCS had thus agreed to create the\" Forum for Negative Results (FNR)\" as a permanent special section of J. UCS. To be submitted to FNR, an article would explain an idea, argue why it was plausible to lead to success\u00a0\u2026", "num_citations": "1\n", "authors": ["161"]}
{"title": "Errors and Defects\n", "abstract": " \u2022 Quite a bit is known about defects\u2022 Little is known about errors in software engineering yet", "num_citations": "1\n", "authors": ["161"]}
{"title": "Optimizing Return-On-Investment (ROI) for Empirical Software Engineering Studies Working Group Results\n", "abstract": " Return-on-investment (ROI) is a concept from the financial world. In the dynamic view, ROI describes the periodically recurring profits (returns) from fixed financial capital (investment). In the static view, ROI describes the one-time income or saving (return) realized as a consequence of a one-time expenditure (investment). In this case, if the return does not occur within a short time, later parts of the return may be discounted for interest. For our purposes, we will use the static view and ignore discounting. We will call the return benefit and the investment cost.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Rapid Software Development\n", "abstract": " Objectives\u25cf To explain how an iterative, incremental development process leads to faster delivery of more useful software", "num_citations": "1\n", "authors": ["161"]}
{"title": "Integrating a Tool into Multiple Different IDEs\n", "abstract": " Abstract abaXX Technology produces component-based platform products that help in the construction of Webbased systems, in particular process portals, using Java2 Enterprise Edition (J2EE) technology. Most parts of these products are API-based and hence require support by appropriate construction tools. Much support is available in leading J2EE IDEs, but some specialized tools have to be provided in addition. Since the products are platformindependent, the tools should work in many different IDEs, too.", "num_citations": "1\n", "authors": ["161"]}
{"title": "CuPit-2: portable and efficient high-level parallel programming of neural networks\n", "abstract": " CuPit-2 is a special-purpose programming language designed for expressing dynamic neural network learning algorithms. It provides most of the flexibility of general-purpose languages such as C or C++, but is more expressive. It allows writing much clearer and more elegant programs, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). In contrast to other languages, CuPit-2 programs can be compiled into efficient code for parallel machines without any changes in the source program, thus providing an easy start for using parallel platforms. This article analyzes the circumstances under which the CuPit-2 approach is the most useful one, presents a description of most language constructs and reports performance results for CuPit-2 on symmetric multiprocessors (SMPs). It concludes that in many cases CuPit-2 is a good basis for neural learning algorithm research on small-scale parallel machines.", "num_citations": "1\n", "authors": ["161"]}
{"title": "The surprising dynamics of a simple year 2000 bug\n", "abstract": " These are the reactions (and an analysis of their reasons) of a very simple program containing a rather simple form of century-dependent code. These reactions are extremely surprising and emerge from an interesting daisy-chain of effects.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Connection pruning with static and adaptive pruning schedules\n", "abstract": " Neural network pruning methods on the level of individual network parameters (eg connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in the pruning methods known today (eg OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required.", "num_citations": "1\n", "authors": ["161"]}
{"title": "PROBEN1-A standardized benchmark collection for neural network training algorithms, 1994\n", "abstract": " CiNii \u8ad6\u6587 - PROBEN1-A standardized benchmark collection for neural network training algorithms, 1994 CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587 \u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 PROBEN1-A standardized benchmark collection for neural network training algorithms, 1994 PRECHELT L. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 PRECHELT L. \u53ce\u9332\u520a\u884c\u7269 ftp://ftp.ira.uka.de/pub/neuron/proben1.tar.gz or ftp://ftp.cs.cmu.edu/afs/cs/project/contrib/prechelt/proben1.tar.gz ftp://ftp.ira.uka.de/pub/neuron/proben1.tar.gz or ftp://ftp.cs.cmu.edu/afs/cs/project/contrib/prechelt/proben1.tar.gz, 1994 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 A Modified General Regression Neural Network (MGRNN) with new, as a \u2026", "num_citations": "1\n", "authors": ["161"]}
{"title": "Methodik und Ergebnisse einer Experimentreihe uber Entwurfsmuster\n", "abstract": " Software-Entwurfsmuster sind eine intuitiv einleuchtende Idee, die viele Bef urworter hat. Als Forscher d urfen wir die behaupteten Wirkungen von Entwurfsmustern jedoch nicht einfach glauben, sondern m ussen sie gr undlich pr ufen. Dieser Artikel beschreibt die wichtigsten Behauptungen und entwickelt daraus zugeh orige Forschungsfragen. Wir diskutieren die Methodik f ur ein Forschungsprogramm, das die Fragen beantworten soll, und skizzieren praktische Beschr ankungen, unter denen diese Forschung ablaufen mu.Es folgt eine kurze Beschreibung von drei kontrollierten Experimenten und den Hauptfolgerungen aus ihren Ergebnissen. So sollte man zum Beispiel die Benutzung von Entwurfsmustern in einem Entwurf genau dokumentieren und Entwurfsmuster nicht einsetzen, ohne alternative Entw urfe zu pr ufen. Abschlie end diskutieren wir ein im Rahmen des Forschungsprogramms geplantes viertes Experiment. Der Beitrag dieses Artikels liegt vorrangig in einer Beschreibung und Diskussion wichtiger methodischer Aspekte kontrollierter Experimente in der Softwaretechnik.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Optimizing Return-On-Investment (ROI) for Empirical Software Engineering Studies\n", "abstract": " Return-on-investment (ROI) is a concept from the financial world. In the dynamic view, ROI describes the periodically recurring profits (returns) from fixed financial capital (investment). In the static view, ROI describes the one-time income or saving (return) realized as a consequence of a one-time expenditure (investment). In this case, if the return does not occur within a short time, later parts of the return may be discounted for interest. For our purposes, we will use the static view and ignore discounting. We will call the return benefit and the investment cost. The notion of ROI can be generalized to any domain (in particular engineering) in which both cost and benefit can be quantified on a rational scale and both scales use the same units. Most commonly, the scale is either money or time. ROI can be used for retrospective analysis of the performance of an investment (controlling) or as an aid for decision-making about potential investments (planning). In the former case, both cost and benefit may be known and the ROI is a single fixed number. Values greater than 1 indicate successful investments, values smaller than 1 indicate failed ones. In the latter case, however, there is usually some uncertainty about the cost and almost always significant uncertainty about the benefit; both expected cost and expected benefit are best described by probability distributions and hence the ROI is also a distribution.", "num_citations": "1\n", "authors": ["161"]}
{"title": "Vorlesung\" Softwaretechnik\n", "abstract": " \u2022\" Contains all of the classes for creating user interfaces and for painting graphics and images. A user interface object such as a button or a scrollbar is called, in AWT terminology, a component. The Component class is the root of all AWT components. See Component for a detailed description of properties that all AWT components share.\u2022 Some components fire events when a user interacts with the components. The AWTEvent class and its subclasses are used to represent the events that AWT components can fire. See AWTEvent for a description of the AWT event model.\u2022 A container is a component that can contain components and other containers. A container can also have a layout manager that controls the visual placement of components in the container. The AWT package contains several layout manager classes and an interface for building your own layout manager. See Container and LayoutManager for more information.\"", "num_citations": "1\n", "authors": ["161"]}
{"title": "Plat_Forms: Is there a single best web development technology?\n", "abstract": " While a number of solid research results are available on some methodological topics (such as review or testing techniques), there is hardly any high-quality information on how specific technologies influence development success. There are narrow benchmarking studies comparing the execution performance of, eg, dababase management systems or application servers, there are feature lists comparing the functionality of base technology products, but there are essentially no holistic studies that compare directly how competing technologies shape a realistic product and how that may influence the project. Those few such studies that exist are very small-scale in terms of the system considered, a good example being [jccpprt], and despite their small scale some also lack the methodological rigor required for high credibility, a popular example being [betterwebapp].", "num_citations": "1\n", "authors": ["161"]}