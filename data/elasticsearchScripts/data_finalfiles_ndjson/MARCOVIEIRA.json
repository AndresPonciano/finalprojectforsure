{"title": "Testing and comparing web vulnerability scanning tools for SQL injection and XSS attacks\n", "abstract": " Web applications are typically developed with hard time constraints and are often deployed with security vulnerabilities. Automatic web vulnerability scanners can help to locate these vulnerabilities and are popular tools among developers of web applications. Their purpose is to stress the application from the attacker's point of view by issuing a huge amount of interaction within it. Two of the most widely spread and dangerous vulnerabilities in web applications are SQL injection and cross site scripting (XSS), because of the damage they may cause to the victim business. Trusting the results of web vulnerability scanning tools is of utmost importance. Without a clear idea on the coverage and false positive rate of these tools, it is difficult to judge the relevance of the results they provide. Furthermore, it is difficult, if not impossible, to compare key figures of merit of web vulnerability scanners. In this paper we propose a\u00a0\u2026", "num_citations": "251\n", "authors": ["345"]}
{"title": "On the emulation of software faults by software fault injection\n", "abstract": " This paper presents an experimental study on the emulation of software faults by fault injection. In a first experiment, a set of real software faults has been compared with faults injected by a SWIFI tool (Xception) to evaluate the accuracy of the injected faults. Results revealed the limitations of Xception (and other SWIFI tools) in the emulation of different classes of software faults (about 44% of the software faults cannot be emulated). The use of field data about real faults was discussed and software metrics were suggested as an alternative to guide the injection process when field data is nor available. In a second experiment, a set of rules for the injection of errors meant to emulate classes of software faults was evaluated. The fault triggers used seem to be the cause for the observed strong impact of the faults in the target system and in the program results. The results also show the influence in the fault emulation of\u00a0\u2026", "num_citations": "193\n", "authors": ["345"]}
{"title": "Using web security scanners to detect vulnerabilities in web services\n", "abstract": " Although Web services are becoming business-critical components, they are often deployed with critical software bugs that can be maliciously explored. Web vulnerability scanners allow detecting security vulnerabilities in Web services by stressing the service from the point of view of an attacker. However, research and practice show that different scanners have different performance on vulnerabilities detection. In this paper we present an experimental evaluation of security vulnerabilities in 300 publicly available Web services. Four well known vulnerability scanners have been used to identify security flaws in Web services implementations. A large number of vulnerabilities has been observed, which confirms that many services are deployed without proper security testing. Additionally, the differences in the vulnerabilities detected and the high number of false-positives (35% and 40% in two cases) and low\u00a0\u2026", "num_citations": "189\n", "authors": ["345"]}
{"title": "Evaluating computer intrusion detection systems: A survey of common practices\n", "abstract": " The evaluation of computer intrusion detection systems (which we refer to as intrusion detection systems) is an active research area. In this article, we survey and systematize common practices in the area of evaluation of such systems. For this purpose, we define a design space structured into three parts: workload, metrics, and measurement methodology. We then provide an overview of the common practices in evaluation of intrusion detection systems by surveying evaluation approaches and methods related to each part of the design space. Finally, we discuss open issues and challenges focusing on evaluation methodologies for novel intrusion detection systems.", "num_citations": "175\n", "authors": ["345"]}
{"title": "Choosing the right NoSQL database for the job: a quality attribute evaluation\n", "abstract": " For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have emerged, namely NoSQL technology. The rising interest in NoSQL technology, as well as the growth in the number of use case scenarios, over the last few years resulted in an increasing number of evaluations and comparisons among competing NoSQL technologies. While most research work mostly focuses on performance evaluation using standard benchmarks, it is important to notice that the architecture of real world systems is not only driven by performance requirements, but has to comprehensively include many other quality attribute requirements. Software quality attributes form the basis from which software engineers and architects develop software and make design decisions. Yet, there has been no quality attribute focused survey or classification of NoSQL databases where databases are compared with regards to their suitability for quality attributes common on the design of enterprise systems. To fill this gap, and aid software engineers and architects, in this article, we survey and create a concise and up-to-date comparison of NoSQL engines, identifying their most beneficial use case scenarios from the software engineer point of view and the quality attributes that each of them is most suited to.", "num_citations": "135\n", "authors": ["345"]}
{"title": "A dependability benchmark for OLTP application environments\n", "abstract": " Publisher SummaryThis chapter proposes a new dependability benchmark for on-line transaction processing (OLTP) systems\u2014the DBench-OLTP dependability benchmark. The goal of this benchmark is to provide a practical way to measure both performance and dependability features of OLTP systems. These systems constitute the kernel of the information systems used today to support the daily operations of most of the businesses and comprise some of the best examples of business-critical applications. This benchmark specifies the measures and all the steps required to evaluate both the performance and key dependability features of OLTP systems. The DBench-OLTP uses the basic setup, the workload, and the performance measures specified in the TPC-C performance benchmark, and adds two new elements: (1) measures related to dependability; and (2) a faultload based on operator faults. The chapter\u00a0\u2026", "num_citations": "135\n", "authors": ["345"]}
{"title": "Comparing the effectiveness of penetration testing and static code analysis on the detection of sql injection vulnerabilities in web services\n", "abstract": " Web services are becoming business-critical components that must provide a non-vulnerable interface to the client applications. However, previous research and practice show that many web services are deployed with critical vulnerabilities. SQL injection vulnerabilities are particularly relevant, as Web services frequently access a relational database using SQL commands. Penetration testing and static code analysis are two well-know techniques often used for the detection of security vulnerabilities. In this work we compare how effective these two techniques are on the detection of SQL injection vulnerabilities in Web services code. To understand the strengths and limitations of these techniques, we used several commercial and open source tools to detect vulnerabilities in a set of vulnerable services. Results suggest that, in general, static code analyzers are able to detect more SQL injection vulnerabilities than\u00a0\u2026", "num_citations": "118\n", "authors": ["345"]}
{"title": "Effective detection of SQL/XPath injection vulnerabilities in web services\n", "abstract": " This paper proposes a new automatic approach for the detection of SQL Injection and XPath Injection vulnerabilities, two of the most common and most critical types of vulnerabilities in Web services. Although there are tools that allow testing Web applications against security vulnerabilities, previous research shows that the effectiveness of those tools in Web services environments is very poor. In our approach a representative workload is used to exercise the Web service and a large set of SQL/XPath injection attacks are applied to disclose vulnerabilities. Vulnerabilities are detected by comparing the structure of the SQL/XPath commands issued in the presence of attacks to the ones previously learned when running the workload in the absence of attacks. Experimental evaluation shows that our approach performs much better than known tools (including commercial ones), achieving extremely high detection\u00a0\u2026", "num_citations": "97\n", "authors": ["345"]}
{"title": "Evaluation of web security mechanisms using vulnerability & attack injection\n", "abstract": " In this paper we propose a methodology and a prototype tool to evaluate web application security mechanisms. The methodology is based on the idea that injecting realistic vulnerabilities in a web application and attacking them automatically can be used to support the assessment of existing security mechanisms and tools in custom setup scenarios. To provide true to life results, the proposed vulnerability and attack injection methodology relies on the study of a large number of vulnerabilities in real web applications. In addition to the generic methodology, the paper describes the implementation of the Vulnerability & Attack Injector Tool (VAIT) that allows the automation of the entire process. We used this tool to run a set of experiments that demonstrate the feasibility and the effectiveness of the proposed methodology. The experiments include the evaluation of coverage and false positives of an intrusion detection\u00a0\u2026", "num_citations": "96\n", "authors": ["345"]}
{"title": "Mapping software faults with web security vulnerabilities\n", "abstract": " Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security), showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause\u00a0\u2026", "num_citations": "94\n", "authors": ["345"]}
{"title": "Detecting SQL injection vulnerabilities in web services\n", "abstract": " Web services are often deployed with critical software bugs that can be maliciously exploited. Web vulnerability scanners are regarded as an easy way to test Web applications against security vulnerabilities. However, previous research shows that the effectiveness of these tools in Web services environments is very poor. In fact, the high number of false-positives and the low coverage observed in practice highlight the strong limitations of these tools. The goal of this paper is to demonstrate that it is possible to develop a vulnerability scanner for Web services that performs much better than the commercial ones currently available. Thus, we propose an approach to detect SQL injection vulnerabilities, one of the most common and most critical types of vulnerabilities in web environments. Experimental evaluation shows that our approach performs much better than well-known commercial tools, achieving very high\u00a0\u2026", "num_citations": "89\n", "authors": ["345"]}
{"title": "Assessing robustness of web-services infrastructures\n", "abstract": " Web-services are supported by a complex software infrastructure that must provide a robust service to the client applications. This practical experience report presents a practical approach for the evaluation of the robustness of Web-services infrastructures. A set of robustness tests (i.e., invalid web-services call parameters) is applied during Web-services execution in order to reveal possible robustness problems in the Web-services code and in the application server infrastructure. The approach is illustrated using two different implementations of the Web-services specified by the TPC-App performance benchmark running on top of the JBoss application server. The proposed approach is generic and can be used to evaluate the robustness of Web-services implementations (relevant for programmers) and application server infrastructures (relevant for administrators and system integrators).", "num_citations": "79\n", "authors": ["345"]}
{"title": "Defending against web application vulnerabilities\n", "abstract": " \u2022 Implementation. During coding, developers must use best practices that avoid the most critical vulnerabilities in the specific application domain. Example practices include input and output validation, the identification of malicious characters, and the use of parameterized commands. 4 Although these techniques are usually effective in avoiding most Web security vulnerabilities, developers do not always apply them or they apply them incorrectly because they lack security-related knowledge. The \u201cWhy Don\u2019t Developers Use Secure Coding Practices?\u201d sidebar addresses this issue in more detail.\u2022 Testing. Many techniques are available for identifying security vulnerabilities during testing, including penetration testing (by far the most popular technique), static analysis, dynamic analysis, and runtime anomaly detection. 4 The problem is that developers often focus on testing functional requirements and disregard security aspects. Furthermore, existing automated tools usually provide poor results\u2014either low vulnerability detection coverage or too many false positives.\u2022 Deployment. At runtime, it is possible to include different attack detection mechanisms in the environment. These mechanisms can operate at different levels and use various detection approaches. Obstacles to their use relate to performance overhead and to the false positives that disrupt normal system behavior.", "num_citations": "78\n", "authors": ["345"]}
{"title": "Benchmarking vulnerability detection tools for web services\n", "abstract": " Vulnerability detection tools are frequently considered the silver-bullet for detecting vulnerabilities in web services. However, research shows that the effectiveness of most of those tools is very low and that using the wrong tool may lead to the deployment of services with undetected vulnerabilities. In this paper we propose a benchmarking approach to assess and compare the effectiveness of vulnerability detection tools in web services environments. This approach was used to define a concrete benchmark for SQL Injection vulnerability detection tools. This benchmark is demonstrated by a real example of benchmarking several widely used tools, including four penetration-testers, three static code analyzers, and one anomaly detector. Results show that the benchmark accurately portrays the effectiveness of vulnerability detection tools and suggest that the proposed approach can be applied in the field.", "num_citations": "77\n", "authors": ["345"]}
{"title": "Vulnerability & attack injection for web applications\n", "abstract": " In this paper we propose a methodology to inject realistic attacks in Web applications. The methodology is based on the idea that by injecting realistic vulnerabilities in a Web application and attacking them automatically we can assess existing security mechanisms. To provide true to life results, this methodology relies on field studies of a large number of vulnerabilities in Web applications. The paper also describes a set of tools implementing the proposed methodology. They allow the automation of the entire process, including gathering results and analysis. We used these tools to conduct a set of experiments to demonstrate the feasibility and effectiveness of the proposed methodology. The experiments include the evaluation of coverage and false positives of an intrusion detection system for SQL injection and the assessment of the effectiveness of two Web application vulnerability scanners. Results show that the\u00a0\u2026", "num_citations": "72\n", "authors": ["345"]}
{"title": "Assessing and comparing vulnerability detection tools for web services: Benchmarking approach and examples\n", "abstract": " Selecting a vulnerability detection tool is a key problem that is frequently faced by developers of security-critical web services. Research and practice shows that state-of-the-art tools present low effectiveness both in terms of vulnerability coverage and false positive rates. The main problem is that such tools are typically limited in the detection approaches implemented, and are designed for being applied in very concrete scenarios. Thus, using the wrong tool may lead to the deployment of services with undetected vulnerabilities. This paper proposes a benchmarking approach to assess and compare the effectiveness of vulnerability detection tools in web services environments. This approach was used to define two concrete benchmarks for SQL Injection vulnerability detection tools. The first is based on a predefined set of web services, and the second allows the benchmark user to specify the workload that best\u00a0\u2026", "num_citations": "69\n", "authors": ["345"]}
{"title": "Benchmarking the robustness of web services\n", "abstract": " This paper proposes an approach for the evaluation of the robustness of web services, which are complex software components that must provide a robust interface to the client applications. However, although web services are becoming business-critical components, there is no practical way to assess the robustness of the code or to compare alternative implementations concerning robustness. The approach proposed is based on a set of robustness tests (i.e., invalid web services call parameters) that is applied in order to discover both programming and design errors. The web services are classified based on the failures observed during the execution of the tests. The approach is illustrated by evaluating several web services publicly available in the Internet and two different implementations of the web services specified by the standard TPC-App performance benchmark. The proposed approach is useful for both\u00a0\u2026", "num_citations": "68\n", "authors": ["345"]}
{"title": "Analysis of field data on web security vulnerabilities\n", "abstract": " Most web applications have critical bugs (faults) affecting their security, which makes them vulnerable to attacks by hackers and organized crime. To prevent these security problems from occurring it is of utmost importance to understand the typical software faults. This paper contributes to this body of knowledge by presenting a field study on two of the most widely spread and critical web application vulnerabilities: SQL Injection and XSS. It analyzes the source code of security patches of widely used Web applications written in weak and strong typed languages. Results show that only a small subset of software fault types, affecting a restricted collection of statements, is related to security. To understand how these vulnerabilities are really exploited by hackers, this paper also presents an analysis of the source code of the scripts used to attack them. The outcomes of this study can be used to train software developers\u00a0\u2026", "num_citations": "67\n", "authors": ["345"]}
{"title": "Detection of malicious transactions in DBMS\n", "abstract": " A major difficulty faced by organizations is the protection of data against malicious access or corruption. Database management systems (DBMS) are a key component in the information infrastructure of most organizations and represent the ultimate layer in preventing unauthorized data accesses. Several mechanisms needed to protect data, such as authentication, user privileges, encryption, and auditing, have been implemented in commercial DBMS. However, typical database security mechanisms are not able to detect and handle many data security attacks. In fact, malicious transactions executed by unauthorized users that may gain access to the database by exploring system vulnerabilities and unauthorized database transactions executed by authorized users cannot be detected and stopped by typical security mechanisms. In this paper we propose a new mechanism for the detection of malicious transactions\u00a0\u2026", "num_citations": "60\n", "authors": ["345"]}
{"title": "Comparing SQL injection detection tools using attack injection: An experimental study\n", "abstract": " System administrators frequently rely on intrusion detection tools to protect their systems against SQL Injection, one of the most dangerous security threats in database-centric web applications. However, the real effectiveness of those tools is usually unknown, which may lead administrators to put an unjustifiable level of trust in the tools they use. In this paper we present an experimental evaluation of the effectiveness of five SQL Injection detection tools that operate at different system levels: Application, Database and Network. To test the tools in a realistic scenario, Vulnerability and Attack Injection is applied in a setup based on three web applications of different sizes and complexities. Results show that the assessed tools have a very low effectiveness and only perform well under specific circumstances, which highlight the limitations of current intrusion detection tools in detecting SQL Injection attacks. Based on\u00a0\u2026", "num_citations": "56\n", "authors": ["345"]}
{"title": "The OLAP and Data Warehousing Approaches for Analysis and Sharing of Results from Dependability Evaluation Experiments.\n", "abstract": " Two important questions on experimental dependability evaluation remain largely unanswered: 1) how to analyze the usually large amount of raw data produced in dependability evaluation experiments and 2) how to compare results from different experiments or results from similar experiments across different systems. These problems are also common to other dependability evaluation techniques such as the ones based on simulation, or even to the analysis of field data on computer faults. We propose the use of data warehousing technologies to store raw results from different experiments/setups in a common multidimensional structure where raw data can be analyzed and shared world wide by means of web-enabled OLAP (On-Line Analytical Processing) tools. This paper describes how to use the proposed approach in a concrete example of dependability evaluation experiment.", "num_citations": "56\n", "authors": ["345"]}
{"title": "Fog orchestration for the Internet of Everything: state-of-the-art and research challenges\n", "abstract": " Recent developments in telecommunications have allowed drawing new paradigms, including the Internet of Everything, to provide services by the interconnection of different physical devices enabling the exchange of data to enrich and automate people\u2019s daily activities; and Fog computing, which is an extension of the well-known Cloud computing, bringing tasks to the edge of the network exploiting characteristics such as lower latency, mobility support, and location awareness. Combining these paradigms opens a new set of possibilities for innovative services and applications; however, it also brings a new complex scenario that must be efficiently managed to properly fulfill the needs of the users. In this scenario, the Fog Orchestrator component is the key to coordinate the services in the middle of Cloud computing and Internet of Everything. In this paper, key challenges in the development of the Fog Orchestrator\u00a0\u2026", "num_citations": "53\n", "authors": ["345"]}
{"title": "Enhancing penetration testing with attack signatures and interface monitoring for the detection of injection vulnerabilities in web services\n", "abstract": " Web services are often deployed with critical software bugs that may be maliciously exploited. Developers often trust on penetration testing tools to detect those vulnerabilities but the effectiveness of such technique is limited by the lack of information on the internal state of the tested services. This paper proposes a new approach for the detection of injection vulnerabilities in web services. The approach uses attack signatures and interface monitoring to increase the visibility of the penetration testing process, yet without needing to access web service's internals (as these are frequently not available). To demonstrate the feasibility of the approach we implemented a prototype tool to detect SQL Injection vulnerabilities in SOAP. An experimental evaluation comparing this prototype with three commercial penetration testers was conducted. Results show that our prototype is able to achieve much higher detection\u00a0\u2026", "num_citations": "50\n", "authors": ["345"]}
{"title": "Towards a security benchmark for database management systems\n", "abstract": " One of the main problems faced by organizations is the protection of their data against unauthorized access or corruption due to malicious actions. Database management systems (DBMS) constitute the kernel of the information systems used today to support the daily operations of most organizations and represent the ultimate layer in preventing unauthorized access to data stored in information systems. Nevertheless, in spite of the key role played by the DBMS in the overall data security, no practical way has been proposed so far to characterize the security in such systems or to compare alternative solutions concerning security features. This paper proposes an approach to characterize the security mechanisms in database systems and database applications, according to a set of security classes. The proposed approach is generic and can be applied to both DBMS (relevant for system integrators) and real\u00a0\u2026", "num_citations": "50\n", "authors": ["345"]}
{"title": "Towards fault tolerance in web services compositions\n", "abstract": " Many businesses are now moving towards the use of composite web services that are based on a collection of web services working together to achieve an objective. Although they are becoming business-critical elements, current development support tools do not provide a practical way to include fault tolerance characteristics in web services compositions. This paper proposes a mechanism that allows programmers to easily develop fault tolerant compositions using diverse web services. The mechanism allows programmers to specify alternative web services for each operation and offers a set of artifacts that simplify the coding process, by automatically dealing with all the aspects related to the redundant web services invocation and responses voting. The mechanism is also able to perform a continuous evaluation of the services based on their behavior during operation. The approach is illustrated using\u00a0\u2026", "num_citations": "49\n", "authors": ["345"]}
{"title": "phpSAFE: A security analysis tool for OOP web application plugins\n", "abstract": " There is nowadays an increasing pressure to develop complex Web applications at a fast pace. The vast majority is built using frameworks based on third-party server-side plugins that allow developers to easily add new features. However, as many plugin developers have limited programming skills, there is a spread of security vulnerabilities related to their use. Best practices advise the use of systematic code review for assure security, but free tools do not support OOP, which is how most Web applications are currently developed. To address this problem we propose phpSAFE, a static code analyzer that identifies vulnerabilities in PHP plugins developed using OOP. We evaluate phpSAFE against two well-known tools using 35 plugins for a widely used CMS. Results show that phpSAFE clearly outperforms other tools, and that plugins are being shipped with a considerable number of vulnerabilities, which tends to\u00a0\u2026", "num_citations": "43\n", "authors": ["345"]}
{"title": "Online detection of malicious data access using dbms auditing\n", "abstract": " This paper proposes a mechanism that allows concurrent detection of malicious data access through the online analysis of the Database Management Systems (DBMS) audit trail. The proposed mechanism uses a directed graph representing the profile of valid transactions to detect illegal accesses to data, which are seen as unauthorized sequences of Structured Query Language (SQL) commands. The paper proposes a generic algorithm that learns the graph representing the profile of the transactions executed by the users. This mechanism can be used to protect traditional database applications from data attacks as well as web based applications from SQL injection types of attacks. The proposed mechanism is generic and can be used in most commercial DBMS, adding concurrent detection of malicious data access to classical database security mechanisms. The paper presents a practical example of the\u00a0\u2026", "num_citations": "41\n", "authors": ["345"]}
{"title": "On the metrics for benchmarking vulnerability detection tools\n", "abstract": " Research and practice show that the effectiveness of vulnerability detection tools depends on the concrete use scenario. Benchmarking can be used for selecting the most appropriate tool, helping assessing and comparing alternative solutions, but its effectiveness largely depends on the adequacy of the metrics. This paper studies the problem of selecting the metrics to be used in a benchmark for software vulnerability detection tools. First, a large set of metrics is gathered and analyzed according to the characteristics of a good metric for the vulnerability detection domain. Afterwards, the metrics are analyzed in the context of specific vulnerability detection scenarios to understand their effectiveness and to select the most adequate one for each scenario. Finally, an MCDA algorithm together with experts' judgment is applied to validate the conclusions. Results show that although some of the metrics traditionally used\u00a0\u2026", "num_citations": "38\n", "authors": ["345"]}
{"title": "Resilience benchmarking\n", "abstract": " Computer benchmarks are standard tools that allow evaluating and comparing different systems or components according to specific characteristics (performance, dependability, security, etc). Resilience encompasses all attributes of the quality of \u2018working well in a changing world that includes faults, failures, errors and attacks\u2019. This way, resilience benchmarking merges concepts from performance, dependability, and security. This chapter presents an overview on the state-of-the-art on benchmarking performance, dependability and security. The goal is to identify the existing approaches, techniques and problems relevant to the resilience-benchmarking problem.", "num_citations": "33\n", "authors": ["345"]}
{"title": "Classification of defect types in requirements specifications: Literature review, proposal and assessment\n", "abstract": " Requirements defects have a major impact throughout the whole software lifecycle. Having a specific defects classification for requirements is important to analyse the root causes of problems, build checklists that support requirements reviews and to reduce risks associated with requirements problems. In our research we analyse several defects classifiers; select the ones applicable to requirements specifications, following rules to build defects taxonomies; and assess the classification validity in an experiment of requirements defects classification performed by graduate and undergraduate students. Not all subjects used the same type of defect to classify the same defect, which suggests that defects classification is not consensual. Considering our results we give recommendations to industry and other researchers on the design of classification schemes and treatment of classification results.", "num_citations": "33\n", "authors": ["345"]}
{"title": "Looking at web security vulnerabilities from the programming language perspective: A field study\n", "abstract": " This paper presents a field study on Web security vulnerabilities from the programming language type system perspective. Security patches reported for a set of 11 widely used Web applications written in strongly typed languages (Java, C#, VB.NET) were analyzed in order to understand the fault types that are responsible for the vulnerabilities observed (SQL injection and XSS). The results are analyzed and compared with a similar work on Web applications written using a weakly typed language (PHP). This comparison points out that some of the types of defects that lead to vulnerabilities are programming language independent, while others are strongly related to the language used. Strongly typed languages do reduce the frequency of vulnerabilities, as expected, but there still is a considerable number of vulnerabilities observed in the field. The characterization of those vulnerabilities shows that they are caused\u00a0\u2026", "num_citations": "33\n", "authors": ["345"]}
{"title": "Benchmarking the Dependability of Different OLTP Systems.\n", "abstract": " On-Line Transaction Processing (OLTP) systems constitute the kernel of the information systems used today to support the daily operations of most organizations. Although these systems comprise the best examples of complex business-critical systems, no practical way has been proposed so far to characterize the impact of faults in such systems or to compare alternative solutions concerning dependability features. This paper presents a practical example of benchmarking key dependability features of four different transactional systems using a first proposal of dependability benchmark for OLTP application environments. This dependability benchmark is an extension to the TPC-C standard performance benchmark, and specifies the measures and all the steps required to evaluate both the performance and dependability features of OLTP systems. Two different versions of the Oracle transactional engine running over two different operating systems were evaluated and compared. The results show that dependability benchmarking can be successfully applied to OLTP application environments.", "num_citations": "33\n", "authors": ["345"]}
{"title": "wsrbench: An on-line tool for robustness benchmarking\n", "abstract": " Testing Web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess Web services robustness in the presence of erroneous inputs. Previous works proposed that Web services robustness testing should be based on a set of robustness tests (i.e., invalid Web services call parameters) that are applied in order to discover both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for Web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their Web services code) and consumers (to select the services that best fit their requirements). The tool is demonstrated by testing several Web services available in the\u00a0\u2026", "num_citations": "32\n", "authors": ["345"]}
{"title": "Penetration testing for web services\n", "abstract": " Web services are often deployed with critical software security faults that open them to malicious attack. Penetration testing using commercially available automated tools can help avoid such faults, but new analysis of several popular testing tools reveals significant failings in their performance. The Web extra at http://youtu.be/COgKs9e679o is an audio interview in which authors Nuno Antunes and Marco Vieira describe how their analysis of popular testing tools revealed significant performance failures and provided important insights for future improvement.", "num_citations": "31\n", "authors": ["345"]}
{"title": "Benchmarking the resilience of self-adaptive software systems: perspectives and challenges\n", "abstract": " Self-adaptive systems are widely recognized as the future of computer systems. Due to their dynamic and evolving nature, the characterization of self-adaptation and resilience attributes is of utmost importance, but also presents itself as a huge challenge. In fact, currently there is no practical way to characterize self-daptation capabilities, especially when comparing alternative systems concerning resilience. In this position paper we discuss the problem of resilience benchmarking of self-adaptive software systems. We identify a set of key challenges and propose a roadmap to tackle those challenges. At the same time, we present some perspectives on the development of such a benchmark, taking Autonomic Database Management Systems (ADBMS) as an illustrative case.", "num_citations": "31\n", "authors": ["345"]}
{"title": "The web attacker perspective-a field study\n", "abstract": " Web applications are a fundamental pillar of today's globalized world. Society depends and relies on them for business and daily life. However, web applications are under constant attack by hackers that exploit their vulnerabilities to access valuable assets and disrupt business. Many studies and reports on web application security problems analyze the victim's perspective by detailing the vulnerabilities publicly disclosed. In this paper we present a field study on the attacker's perspective by looking at over 300 real exploits used by hackers to attack web applications. Results show that SQL injection and Remote File Inclusion are the two most frequently used exploits and that hackers prefer easier rather than complicated attack techniques. Exploit and vulnerability data are also correlated to show that, although there are many types of vulnerabilities out there, only few are interesting enough for attackers to obtain\u00a0\u2026", "num_citations": "31\n", "authors": ["345"]}
{"title": "Recovery and Performance Balance of a COTS DBMS in the Presence of Operator Faults\n", "abstract": " A major cause of failures in large database management systems (DBMS) is operator faults. Although most of the complex DBMS have comprehensive recovery mechanisms, the effectiveness of these mechanisms is difficult to characterize. On the other hand, the tuning of a large database is very complex and database administrators tend to concentrate on performance tuning and disregard the recovery mechanisms. Above all, database administrators seldom have feedback on how good a given configuration is concerning recovery. This paper proposes an experimental approach to characterize both the performance and the recoverability in DBMS. Our approach is presented through a concrete example of benchmarking the performance and recovery of an Oracle DBMS running the standard TPC-C benchmark, extended to include two new elements: a fault load based on operator faults and measures related to\u00a0\u2026", "num_citations": "30\n", "authors": ["345"]}
{"title": "Designing vulnerability testing tools for web services: approach, components, and tools\n", "abstract": " This paper proposes a generic approach for designing vulnerability testing tools for web services, which includes the definition of the testing procedure and the tool components. Based on the proposed approach, we present the design of three innovative testing tools that implement three complementary techniques (improved penetration testing, attack signatures and interface monitoring, and runtime anomaly detection) for detecting injection vulnerabilities, thus offering an extensive support for different scenarios. A case study has been designed to demonstrate the tools for the particular case of SQL Injection vulnerabilities. The experimental evaluation demonstrates that the tools can effectively be used in different scenarios and that they outperform well-known commercial tools by achieving higher detection coverage and lower false-positive rates.", "num_citations": "29\n", "authors": ["345"]}
{"title": "Experience report: an analysis of hypercall handler vulnerabilities\n", "abstract": " Hypervisors are becoming increasingly ubiquitous with the growing proliferation of virtualized data centers. As a result, attackers are exploring vectors to attack hypervisors, against which an attack may be executed via several attack vectors such as device drivers, virtual machine exit events, or hyper calls. Hyper calls enable intrusions in hypervisors through their hyper call interfaces. Despite the importance, there is very limited publicly available information on vulnerabilities of hyper call handlers and attacks triggering them, which significantly hinders advances towards monitoring and securing these interfaces. In this paper, we characterize the hyper call attack surface based on analyzing a set of vulnerabilities of hyper call handlers. We systematize and discuss the errors that caused the considered vulnerabilities, and activities for executing attacks triggering them. We also demonstrate attacks triggering the\u00a0\u2026", "num_citations": "29\n", "authors": ["345"]}
{"title": "Improving web services robustness\n", "abstract": " Developing robust web services is a difficult task. Field studies show that a large number of web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Several techniques for the identification of robustness problems have been proposed in the past. This paper proposes a mechanism that automatically fixes the problems detected. The approach consists of using robustness testing to detect robustness issues and then mitigate those issues by applying inputs verification based on well-defined parameter domains, including domain dependencies between different parameters. This integrated and fully automatable methodology has been used to improve three different implementations of the TPC-App web services. Results show that this tool can be easily used by developers to improve the robustness of web services implementations.", "num_citations": "29\n", "authors": ["345"]}
{"title": "A practical approach for generating failure data for assessing and comparing failure prediction algorithms\n", "abstract": " Failure Prediction allows improving the dependability of computer systems, but its use is still uncommon due to scarcity of failure-related data that can be used for training, assessing and comparing alternative failure predictors. As failures are rare events and the characteristics of failure data varies from system to system, in this paper we propose the use of realistic software fault injection to facilitate the generation of failure data on a particular system installation. In practice, we propose a comprehensive experimental approach that allows generating failure data in short time and we study the applicability and limitations of such process in assessing and comparing alternative failure prediction algorithms. A case study is presented comparing four algorithms for predicting failures in a system based on a Windows OS. Results show that using fault injection allows to dramatically speed up the generation of failure data and\u00a0\u2026", "num_citations": "28\n", "authors": ["345"]}
{"title": "A survey on data security in data warehousing: Issues, challenges and opportunities\n", "abstract": " Data Warehouses (DWs) are the enterprise's most valuable assets in what concerns critical business information, making them an appealing target for malicious inside and outside attackers. Given the volume of data and the nature of DW queries, most of the existing data security solutions for databases are inefficient, consuming too many resources and introducing too much overhead in query response time, or resulting in too many false positive alarms (i.e., incorrect detection of attacks) to be checked. In this paper, we present a survey on currently available data security techniques, focusing on specific issues and requirements concerning their use in data warehousing environments. We also point out challenges and opportunities for future research work in this field.", "num_citations": "28\n", "authors": ["345"]}
{"title": "An appraisal to assess the security of database configurations\n", "abstract": " Database management systems (DBMS) have a long tradition in high security and several mechanisms needed to protect data have been proposed/consolidated in the database arena. However, the effectiveness of those mechanisms is very dependent on the actual configuration chosen by the database administrator. Tuning a large database is quite complex and achieving high security is a very difficult task that requires a lot of expertise and continuous and proactive work. In this paper we present an assessment tool aimed at evaluating the security of DBMS configurations. The proposed tool is simple and effective, and can be used by administrators with very little security knowledge. We evaluate the tool by performing the assessment of four different real database installations based on four well-known and widely used DBMS engines.", "num_citations": "28\n", "authors": ["345"]}
{"title": "Towards assessing the security of DBMS configurations\n", "abstract": " Database management systems (DBMS) have a long tradition in high security. Several mechanisms needed to protect data have been proposed/consolidated in the database arena. However, the effectiveness of those mechanisms is very dependent on the actual configuration chosen by the database administrator. Tuning a large database is quite complex and achieving high security is a very difficult task that requires a lot of expertise and continuous and proactive work. In this paper we analyze the security best practices behind the many configuration options available in several well-known DBMS. These security best practices are then generalized in order to be applicable to practically any DBMS available today. Finally, we use these best practices to define a set of configuration tests, which have been successfully used to evaluate four real database installations based in four well-known and widely used DBMS.", "num_citations": "28\n", "authors": ["345"]}
{"title": "Nosql databases: A software engineering perspective\n", "abstract": " For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have started being developed, namely NoSQL technology. With increased interest in NoSQL technology, as well as more use case scenarios, over the last few years these databases have been more frequently evaluated and compared. It is necessary to find if all the possibilities and characteristics of non-relational technology have been disclosed. While most papers perform mostly performance evaluation using standard benchmarks, it is nevertheless important to notice that real world scenarios, with real enterprise data, do not function solely based on performance. In this paper, we have gathered a concise and up-to-date comparison of NoSQL engines, their most beneficial use case scenarios from the\u00a0\u2026", "num_citations": "27\n", "authors": ["345"]}
{"title": "Detecting malicious SQL\n", "abstract": " Web based applications often have vulnerabilities that can be exploited to launch SQL-based attacks. In fact, web application developers are normally concerned with the application functionalities and can easily neglect security aspects. The increasing number of web attacks reported every day corroborates that this attack-prone scenario represents a real danger and is not likely to change favorably in the future. However, the main problem resides in the fact that most of the SQL-based attacks cannot be detected by typical intrusion detection systems (IDS) at network or operating system level. In this paper we propose a database level IDS to concurrently detect malicious database operations. The proposed IDS is based on a comprehensive anomaly detection scheme that checks SQL commands to detect SQL injection and analyses transactions to detect more elaborate data-centric attacks, including insider\u00a0\u2026", "num_citations": "27\n", "authors": ["345"]}
{"title": "An integrated big and fast data analytics platform for smart urban transportation management\n", "abstract": " Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to\u00a0\u2026", "num_citations": "26\n", "authors": ["345"]}
{"title": "A data masking technique for data warehouses\n", "abstract": " Data Warehouses (DWs) are the enterprise's most valuable asset in what concerns critical business information, making them an appealing target for attackers. Packaged database encryption solutions are considered the best solution to protect sensitive data. However, given the volume of data typically processed by DW queries, the existing encryption solutions heavily increase storage space and introduce very large overheads in query response time, due to decryption costs. In many cases, this performance degradation makes encryption unfeasible for use in DWs. In this paper we propose a transparent data masking solution for numerical values in DWs based on the mathematical modulus operator, which can be used without changing user application and DBMS source code. Our solution provides strong data security while introducing small overheads in both storage space and database performance. Several\u00a0\u2026", "num_citations": "26\n", "authors": ["345"]}
{"title": "A practical experience on the impact of plugins in web security\n", "abstract": " In an attempt to support customization, many web applications allow the integration of third-party server-side plugins that offer diverse functionality, but also open an additional door for security vulnerabilities. In this paper we study the use of static code analysis tools to detect vulnerabilities in the plugins of the web application. The goal is twofold: 1) to study the effectiveness of static analysis on the detection of web application plugin vulnerabilities, and 2) to understand the potential impact of those plugins in the security of the core web application. We use two static code analyzers to evaluate a large number of plugins for a widely used Content Manage-ment System. Results show that many plugins that are current-ly deployed worldwide have dangerous Cross Site Scripting and SQL Injection vulnerabilities that can be easily exploited, and that even widely used static analysis tools may present disappointing\u00a0\u2026", "num_citations": "25\n", "authors": ["345"]}
{"title": "A robustness testing approach for SOAP Web services\n", "abstract": " The use of Web services in enterprise applications is quickly increasing. In a Web services environment, providers supply a set of services for consumers. However, although Web services are being used in business-critical environments, there are no practical means to test or compare their robustness to invalid and malicious inputs. In fact, client applications are typically developed with the assumption that the services being used are robust, which is not always the case. Robustness failures in such environments are particularly dangerous, as they may originate vulnerabilities that can be maliciously exploited, with severe consequences for the systems under attack. This paper addresses the problem of robustness testing in Web services environments. The proposed approach is based on a set of robustness tests (including both malicious and non-malicious invalid call parameters) that is used to discover\u00a0\u2026", "num_citations": "25\n", "authors": ["345"]}
{"title": "Protecting database centric web services against SQL/XPath injection attacks\n", "abstract": " Web services represent a powerful interface for back-end database systems and are increasingly being used in business critical applications. However, field studies show that a large number of web services are deployed with security flaws (e.g., having SQL Injection vulnerabilities). Although several techniques for the identification of security vulnerabilities have been proposed, developing non-vulnerable web services is still a difficult task. In fact, security-related concerns are hard to apply as they involve adding complexity to already complex code. This paper proposes an approach to secure web services against SQL and XPath Injection attacks, by transparently detecting and aborting service invocations that try to take advantage of potential vulnerabilities. Our mechanism was applied to secure several web services specified by the TPC-App benchmark, showing to be 100% effective in stopping attacks\u00a0\u2026", "num_citations": "25\n", "authors": ["345"]}
{"title": "An analysis of openstack vulnerabilities\n", "abstract": " Cloud management frameworks provide an effective way to deploy and manage the hardware, storage and network resources for supporting critical cloud infrastructures. OpenStack is used in the context of business critical systems and frequently deals with highly sensitive resources, where a security breach may result in severe damage, including information theft or financial losses. Despite this, there is little information on how much security is a concern during design and implementation of OpenStack components. This work analyses 5 years of security reports on OpenStack and the corresponding patches, with the goal of characterizing the most frequent vulnerabilities, how they can be exploited, and their root causes. The goal is to identify vulnerability trends, characterize frequent threats, and shed some light on the overall security of OpenStack. Special focus is placed on the framework component for\u00a0\u2026", "num_citations": "24\n", "authors": ["345"]}
{"title": "Approaches and challenges in database intrusion detection\n", "abstract": " Databases often support enterprise business and store its secrets. This means that securing them from data damage and information leakage is critical. In order to deal with intrusions against database systems, Database Intrusion Detection Systems (DIDS) are frequently used. This paper presents a survey on the main database intrusion detection techniques currently available and discusses the issues concerning their application at the database server layer. The identified weak spots show that most DIDS inadequately deal with many characteristics of specific database systems, such as ad hoc workloads and alert management issues in data warehousing environments, for example. Based on this analysis, research challenges are presented, and requirements and guidelines for the design of new or improved DIDS are proposed. The main finding is that the development and benchmarking of specifically tailored\u00a0\u2026", "num_citations": "24\n", "authors": ["345"]}
{"title": "Fault injection for failure prediction methods validation\n", "abstract": " Failure prediction methods are becoming sine qua non conditions for effective availability enhancement in complex computer and communication systems. Therefore, there is a growing need for validation, benchmarking and assessment of such methods on real industrial data. Our thesis is that the effectiveness of such methods can be significantly enhanced when combined with fault injection. Then, not only failures can be predicted but also potential root causes can be identified based on symptoms, which can be observed at runtime on the system. We first briefly introduce failure prediction and fault injection methods and then present a methodology for improving and validating failure prediction methods using fault injection. We anticipate that with our approach we will be able to more efficiently predict forthcoming outages and also identify root causes which in turn will enable effective recovery or failure avoidance\u00a0\u2026", "num_citations": "24\n", "authors": ["345"]}
{"title": "Experimental robustness evaluation of JMS middleware\n", "abstract": " The use of Java Message Service (JMS) for enterprise applications communication and integration is increasing very quickly. However, although JMS is frequently used in business-critical environments, applications are typically developed with the assumption that the middleware being used is robust, which is not always the case. Robustness failures in such environments are particularly dangerous, as they may originate vulnerabilities that can be maliciously exploited with severe consequences for the systems subject of attack. This paper proposes an approach for the evaluation of the robustness of JMS middleware. Our approach is presented through a concrete example of evaluating the robustness of three well-known JMS solutions (JBoss MQ 3.2.8.SP1, JBoss MQ 4.2.1.GA, and Active MQ 4.1.1), in which several robustness and critical security related problems have been disclosed (including specification\u00a0\u2026", "num_citations": "24\n", "authors": ["345"]}
{"title": "ESFFI-A novel technique for the emulation of software faults in COTS components\n", "abstract": " The paper presents and evaluates a methodology for the emulation of software faults in COTS components using software implemented fault injection (SWIFI) technology. ESFFI (Emulation of Software Faults by Fault Injection) leverages matured fault injection techniques, which have been used so far for the emulation of hardware faults, and adds new features that make possible the insertion of errors mimicking those caused by real software faults. The major advantage of ESFFI over other techniques that also emulate software faults (mutations, for instance) is making fault locations ubiquitous; every software module can be targeted, no matter if it is a device driver running in operating kernel mode or a third party component whose source code is not available. Experimental results have shown that for specific fault classes, e.g. assignment and checking, the accuracy obtained by this technique is quite good.", "num_citations": "24\n", "authors": ["345"]}
{"title": "Finding SQL injection and cross site scripting vulnerabilities with diverse static analysis tools\n", "abstract": " The use of Static Analysis Tools (SATs) is mandatory when developing secure software and searching for vulnerabilities in legacy software. However, the performance of the various SATs concerning the detection of vulnerabilities and false alarm rate is usually unknown and depends on many factors. The simultaneous use of several tools should increase the detection capabilities, but also the number of false alarms. In this paper, we study the problem of combining several SATs to best meet the developer needs. We present results of analyzing the performance of diverse static analysis tools, based on a previously published dataset that resulted from the use of five diverse SATs to find two types of vulnerabilities, namely SQL Injections (SQLi) and Cross-Site Scripting (XSS), in 132 plugins of the WordPress Content Management System (CMS). We present the results based on well-established measures for binary\u00a0\u2026", "num_citations": "22\n", "authors": ["345"]}
{"title": "Software metrics as indicators of security vulnerabilities\n", "abstract": " Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively\u00a0\u2026", "num_citations": "22\n", "authors": ["345"]}
{"title": "Balancing security and performance for enhancing data privacy in data warehouses\n", "abstract": " Data Warehouses (DWs) store the golden nuggets of the business, which makes them an appealing target. To ensure data privacy, encryption solutions have been used and proven efficient in their security purpose. However, they introduce massive storage space and performance overheads, making them unfeasible for DWs. We propose a data masking technique for protecting sensitive business data in DWs that balances security strength with database performance, using a formula based on the mathematical modular operator. Our solution manages apparent randomness and distribution of the masked values, while introducing small storage space and query execution time overheads. It also enables a false data injection method for misleading attackers and increasing the overall security strength. It can be easily implemented in any DataBase Management System (DBMS) and transparently used, without\u00a0\u2026", "num_citations": "22\n", "authors": ["345"]}
{"title": "Applying data mining for detecting anomalies in satellites\n", "abstract": " Telemetry data is the only source for identifying/predicting anomalies in artificial satellites. Human specialists analyze these data in real time, but its large volume, makes this analysis extremely difficult. In this experience paper we study the hypothesis of using clustering algorithms to help operators and analysts to perform telemetry analysis. Two real cases of satellite anomalies in Brazilian space missions are considered, allowing assessing and comparing the effectiveness of two clustering algorithms (K-means and Expectation Maximization), which showed to be effective in the case study where several telemetry channels tended to deliver outlier values and, in these cases, could support the satellite operators by allowing the anticipation of anomalies. However for silent problems, where there was just a small variation in a single telemetry, the algorithms were not as efficient.", "num_citations": "20\n", "authors": ["345"]}
{"title": "Training security assurance teams using vulnerability injection\n", "abstract": " Writing secure Web applications is a complex task. In fact, a vast majority of Web applications are likely to have security vulnerabilities that can be exploited using simple tools like a common Web browser. This represents a great danger as the attacks may have disastrous consequences to organizations, harming their assets and reputation. To mitigate these vulnerabilities, security code inspections and penetration tests must be conducted by well-trained teams during the development of the application. However, effective code inspections and testing takes time and cost a lot of money, even before any business revenue. Furthermore, software quality assurance teams typically lack the knowledge required to effectively detect security problems. In this paper we propose an approach to quickly and effectively train security assurance teams in the context of web application development. The approach combines a novel\u00a0\u2026", "num_citations": "20\n", "authors": ["345"]}
{"title": "Integrated intrusion detection in databases\n", "abstract": " Database management systems (DBMS), which are the ultimate layer in preventing malicious data access or corruption, implement several security mechanisms to protect data. However these mechanisms cannot always stop malicious users from accessing the data by exploiting system vulnerabilities. In fact, when a malicious user accesses the database there is no effective way to detect and stop the attack in due time. This practical experience report presents a tool that implements concurrent intrusion detection in DBMS. This tool analyses the transactions the users execute and compares them with the profile of the authorized transactions that were previously learned in order to detect potential deviations. The tool was evaluated using the transactions from a standard database benchmark (TPC-W) and a real database application. Results show that the proposed intrusion detection tool can effectively\u00a0\u2026", "num_citations": "20\n", "authors": ["345"]}
{"title": "A technique for deploying robust web services\n", "abstract": " Developing robust web services is a difficult task. Field studies show that a large number of web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Although several techniques for the identification of robustness problems have been proposed in the past, there is no practical approach to automatically fix those problems. This paper proposes a mechanism that automatically fixes robustness problems in web services. The approach consists of using robustness testing to detect robustness issues and then mitigate those issues by applying inputs verification based on well-defined parameter domains, including domain dependencies between different parameters. This integrated and fully automated methodology has been used to improve three different implementations of the TPC-App web services and several services publicly available on the\u00a0\u2026", "num_citations": "19\n", "authors": ["345"]}
{"title": "24/7 real-time data warehousing: A tool for continuous actionable knowledge\n", "abstract": " Technological evolution has redefined many business models. Many decision makers are now required to act near real-time, instead of periodically, given the latest transactional information. Decision-making occurs much more frequently and considers the latest business data. Since data warehouses (DWs) are the core of business intelligence, decision support systems need to deal with 24/7 real-time requirements. Thus, the ability to deal with continuous data loading and decision support availability simultaneously is critical, for producing continuous actionable knowledge. The main challenge in this context is to efficiently manage the DW's refreshment, when data sources change, to recapture consistency and accuracy with those sources, while maintaining OLAP availability and database performance. This paper proposes a simple, fast and efficient solution based on database replication and temporary tables to\u00a0\u2026", "num_citations": "19\n", "authors": ["345"]}
{"title": "Comparing web services performance and recovery in the presence of faults\n", "abstract": " Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.", "num_citations": "19\n", "authors": ["345"]}
{"title": "Changeloads for resilience benchmarking of self-adaptive systems: a risk-based approach\n", "abstract": " Benchmarking self-adaptive software systems calls for a new model that takes into account a distinctive characteristic of such systems: alterations over time (i.e., self-achieved modifications or adjustments triggered by changes in the external or internal contexts of the system). Changes are thus a fundamental component of a resilience benchmark, raising an intrinsic research problem: how to identify and select the most realistic and relevant (sequences of) changes to be included in the benchmarking procedure. The problem is that defining a representative change load would require access to a large amount of field data, which is not available for most systems. In this paper we propose an approach based on risk analysis to tackle this key issue, debating its effectiveness and usability with a simple case study. The procedure, that combines field data with expert knowledge and experimental data, allows moving from\u00a0\u2026", "num_citations": "18\n", "authors": ["345"]}
{"title": "A testing service for lifelong validation of dynamic SOA\n", "abstract": " Service Oriented Architectures (SOAs) are increasingly being used to support the information infrastructures of organizations. SOAs are dynamic and evolve after deployment in order to adapt to changes in the requirements and infrastructure. Consequently, traditional validation approaches based on offline testing conducted before deployment are not adequate anymore, demanding for new techniques that allow testing the SOA during its whole lifecycle. In this paper we propose a SOA testing approach based on a composite service that is able to trace SOA evolution and automatically test the various services according to specific testing policies. The paper describes the architecture of the testing service and presents a concrete implementation focused on robustness testing. Results from a case study demonstrate the effectiveness of the proposed approach in discovering and testing the robustness of SOA services.", "num_citations": "18\n", "authors": ["345"]}
{"title": "Combining energy and wavelet transform for epileptic seizure prediction in an advanced computational system\n", "abstract": " Seizure prediction in epileptic patients will allow a deep improvement in their quality of life. In the paper a new method using energy relative measures in wavelet coefficients is proposed and tested in several patients. The results show the potential of the technique, but also its limitations, stressing the needs for further work in a larger number of patients, using multimodal information and an advanced database with a large features set to be used in seizure prediction An advanced computational framework is under development, using multisensorial information to build a large set of features to be used in a classification system supporting seizure prediction. This system is composed of two main parts the algorithms base and the database, briefly described.", "num_citations": "18\n", "authors": ["345"]}
{"title": "Evaluation of intrusion detection systems in virtualized environments using attack injection\n", "abstract": " The evaluation of intrusion detection systems (IDSes) is an active research area with many open challenges, one of which is the generation of representative workloads that contain attacks. In this paper, we propose a novel approach for the rigorous evaluation of IDSes in virtualized environments, with a focus on IDSes designed to detect attacks leveraging or targeting the hypervisor via its hypercall interface. We present hInjector, a tool for generating IDS evaluation workloads by injecting such attacks during regular operation of a virtualized environment. We demonstrate the application of our approach and show its practical usefulness by evaluating a representative IDS designed to operate in virtualized environments. The virtualized environment of the industry-standard benchmark SPECvirt_sc2013 is used as a testbed, whose drivers generate workloads representative of workloads seen in production\u00a0\u2026", "num_citations": "17\n", "authors": ["345"]}
{"title": "Efficient data distribution for DWS\n", "abstract": " The DWS (Data Warehouse Striping) technique is a data partitioning approach especially designed for distributed data warehousing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost computers and the queries are executed in parallel by all the computers, guarantying a nearly optimal speed up and scale up. Data loading in data warehouses is typically a heavy process that gets even more complex when considering distributed environments. Data partitioning brings the need for new loading algorithms that conciliate a balanced distribution of data among nodes with an efficient data allocation (vital to achieve low and uniform response times and, consequently, high performance during the execution of queries). This paper evaluates several alternative algorithms and proposes a generic approach for the evaluation of data distribution algorithms in the context of DWS\u00a0\u2026", "num_citations": "17\n", "authors": ["345"]}
{"title": "Practical evaluation of static analysis tools for cryptography: Benchmarking method and case study\n", "abstract": " The incorrect use of cryptography is a common source of critical software vulnerabilities. As developers lack knowledge in applied cryptography and support from experts is scarce, this situation is frequently addressed by adopting static code analysis tools to automatically detect cryptography misuse during coding and reviews, even if the effectiveness of such tools is far from being well understood. This paper proposes a method for benchmarking static code analysis tools for the detection of cryptography misuse, and evaluates the method in a case study, with the goal of selecting the most adequate tools for specific development contexts. Our method classifies cryptography misuse in nine categories recognized by developers (weak cryptography, poor key management, bad randomness, etc.) and provides the workload, metrics and procedure needed for a fair assessment and comparison of tools. We found that all\u00a0\u2026", "num_citations": "16\n", "authors": ["345"]}
{"title": "Evaluating and improving penetration testing in web services\n", "abstract": " Developers often rely on penetration testing tools to detect vulnerabilities in web services, although frequently without really knowing their effectiveness. In fact, the lack of information on the internal state of the tested services and the complexity and variability of the responses analyzed, limits the effectiveness of such technique, highlighting the importance of evaluating and improving existing tools. The goal of this paper is to investigate if attack signatures and interface monitoring can be an effective mean to assess and improve the performance of penetration testing tools in web services environments. In practice, attacks performed by such tools are signed and the interfaces between the target application and external resources are monitored (e.g., between services and a database server), allowing gathering additional information on existing vulnerabilities. A prototype was implemented focusing on SQL injection\u00a0\u2026", "num_citations": "16\n", "authors": ["345"]}
{"title": "Applying text classification algorithms in web services robustness testing\n", "abstract": " Testing web services for robustness is an effective way of disclosing software bugs. However, when executing robustness tests, a very large amount of service responses has to be manually classified to distinguish regular responses from responses that indicate robustness problems. Besides requiring a large amount of time and effort, this complex classification process can easily lead to errors resulting from the human intervention in such a laborious task. Text classification algorithms have been applied successfully in many contexts (e.g., spam identification, text categorization, etc) and are considered a powerful tool for the successful automation of several classification-based tasks. In this paper we present a study on the applicability of five widely used text classification algorithms in the context of web services robustness testing. In practice, we assess the effectiveness of Support Vector Machines, Na\u00efve Bayes\u00a0\u2026", "num_citations": "16\n", "authors": ["345"]}
{"title": "SOA-Scanner: an integrated tool to detect vulnerabilities in service-based infrastructures\n", "abstract": " Service Oriented Architectures are nowadays used in a wide range of organizations to support critical daily operations. Although the underlying services should behave in a secure manner, they are often deployed with bugs that can be maliciously exploited. The characteristics of service-based environments open the door to security challenges that must be handled properly, including services under the control of multiple providers and dynamism of interactions and compositions. This paper presents an extensible tool able to widely test such infrastructures for vulnerabilities. The tool is based in an iterative process that uses interface monitoring to automatically monitor and discover the existing services, resources and interactions, and applies different testing approaches depending on the level of access to each existing services. Two case studies has been developed do demonstrate the tool, and results show that\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "A survey on secure software development lifecycles\n", "abstract": " This chapter presents a survey on the most relevant software development practices that are used nowadays to build software products for the web, with security built in. It starts by presenting three of the most relevant Secure Software Development Lifecycles, which are complete solutions that can be adopted by development companies: the CLASP, the Microsoft Secure Development Lifecycle, and the Software Security Touchpoints. However it is not always feasible to change ongoing projects or replace the methodology in place. So, this chapter also discusses other relevant initiatives that can be integrated into existing development practices, which can be used to build and maintain safer software products: the OpenSAMM, the BSIMM, the SAFECode, and the Securosis. The main features of these security development proposals are also compared according to their highlights and the goals of the target software\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "Experimental evaluation of web service frameworks in the presence of security attacks\n", "abstract": " Web services are increasingly being used to provide critical operations in business-to-business and safety-critical environments. In these environments the exploitation of security vulnerabilities may result in major damages in the services infrastructures, financial or reputation losses to the organizations involved, and other catastrophic consequences for the users and the environment. Web services frameworks are the basis for developers to create and deploy web services, and must provide a robust and secure environment, so that an application can deliver its service, even when in presence of security attacks. In this paper we study the behavior of well-known web services frameworks in the presence of security attacks targeting the core web services specifications, i.e., those enabling basic message exchange functionalities. Results show that frameworks are quite resistant to attacks. However, they also indicate\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "Selecting secure web applications using trustworthiness benchmarking\n", "abstract": " The multiplicity of existing software and component alternatives for web applications, especially in open source communities, has boosted interest in suitable benchmarks, able to assist in the selection of candidate solutions, concerning several quality attributes. However, the huge success of performance and dependability benchmarking contrasts the small advances in security benchmarking. Traditional vulnerability/attack detection techniques can hardly be used alone to benchmark security, as security depends on hidden vulnerabilities and subtle properties of the system and its environment. A comprehensive security benchmarking process should consist of a two-step process: elimination of flawed alternatives followed by trustworthiness benchmarking. In this paper, the authors propose a trustworthiness benchmark based on the systematic collection of evidences that can be used to select one among several\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "A learning-based approach to secure web services from SQL/XPath Injection attacks\n", "abstract": " Business critical applications are increasingly being deployed as web services that access database systems, and must provide secure operations to its clients. Although the open web environment emphasizes the need for security, several studies show that web services are still being deployed with command injection vulnerabilities. This paper proposes a learning-based approach to secure web services against SQL and XPath Injection attacks. Our approach is able to transparently learn valid request patterns (learning phase) and then detect and abort potentially harmful requests (protection phase). When it is not possible to have a complete learning phase, a set of heuristics can be used to accept/discard doubtful cases. Our mechanism was applied to secure TPC-App services and open source services. It showed to be extremely effective in stopping all tested attacks, while introducing a negligible performance\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "Definition of fault loads based on operator faults for DMBS recovery benchmarking\n", "abstract": " The characterization of database management system (DBMS) recovery mechanisms and the comparison of recovery features of different DBMS require a practical approach to benchmark the effectiveness of recovery in the presence of faults. Existing performance benchmarks for transactional and database areas include two major components: a workload and a set of performance measures. The definition of a benchmark to characterize DBMS recovery needs a new component the faultload. A major cause of failures in large DBMS is operator faults, which make them an excellent starting point for the definition of a generic faultload. This paper proposes the steps for the definition of generic faultloads based on operator faults for DBMS recovery benchmarking. A classification for operator faults in DBMS is proposed and a comparative analysis among three commercially DBMS is presented. The paper ends with a\u00a0\u2026", "num_citations": "15\n", "authors": ["345"]}
{"title": "The perils of android security configuration\n", "abstract": " Recent analysis of the popular Android platform highlights common security misconfigurations as well as the respective roles that users, manufacturers, and researchers play in protecting devices against cyberattacks.", "num_citations": "14\n", "authors": ["345"]}
{"title": "Understanding interoperability issues of web service frameworks\n", "abstract": " Web Services are a set of technologies designed to support the invocation of remote services by client applications, with the key goal of providing interoperable application-to-application interaction while supporting vendor and platform independence. The goal of this work is to study the real level of interoperability provided by these technologies through a massive experimental campaign involving a wide set of very popular frameworks for web services, implemented using seven different programming languages. We have tested the inter-operation of eleven client-side framework subsystems with three of the most widely used server-side implementations, each one hosting thousands of different services. The results highlight numerous situations where the goal of interoperability between different frameworks is not met due to problems both on the client and the server side. Moreover, we have identified issues also\u00a0\u2026", "num_citations": "14\n", "authors": ["345"]}
{"title": "The time dimension in predicting failures: A case study\n", "abstract": " Online Failure Prediction is a cutting-edge technique for improving the dependability of software systems. It makes extensive use of machine learning techniques applied to variables monitored from the system at regular intervals of time (e.g. mutexes/s, paged bytes/s, etc.). The goal of this work is to assess the impact of considering the time dimension in failure prediction, through the use of sliding windows. The state-of-the-art SVM (Support Vector Machine) classifier is used to support the study, predicting failure events occurring in a Windows XP machine. An extensive comparative analysis is carried out, in particular using a software fault injection technique to speed up the failure data generation process.", "num_citations": "14\n", "authors": ["345"]}
{"title": "Untrustworthiness: A trust-based security metric\n", "abstract": " Quantifying security is very hard and, although there are many proposals of security metrics in the literature, no consensual quantitative security metric has been proposed so far. A key difficulty is that security is, usually, more influenced by what is unknown about a system than by what is known about it. In this paper we present the idea of trust-based metrics, which are based on the idea of quantifying and exposing the trustworthiness relationship between a system and its owner. We defend that they represent a powerful alternative to traditional security metrics and are much easier to obtain. As an instantiation, we propose minimum untrustworthiness as a low-cost high-reward trust-based metric that can be easily used to assess and compare security aspects. We discuss what does it express, show how it can be computed and what are its advantages. Finally, we present preliminary work on the definition of an\u00a0\u2026", "num_citations": "14\n", "authors": ["345"]}
{"title": "PRIVAaaS: privacy approach for a distributed cloud-based data analytics platforms\n", "abstract": " Data privacy is a key challenge that is exacerbated by Big Data storage and analytics processing requirements. Big Data and Cloud Computing are related and allow the users to access data from any device, making data privacy essential as the data sets are exposed through the web. Organizations care about data privacy as it directly affects the confidence that clients have that their personal data are safe. This paper presents a data privacy approach - PRIVAaaS - and its inte-gration to the LEMONADE Web-based platform, developed to compose ETL (Extract, Transform, Load) process and Machine Learning workflows. The 3-level approach of PRIVAaaS, based on data anonymization policies, is implemented in a software toolkit that provides a set of libraries and tools which allows controlling and reducing data leakage in the context of Big Data processing.", "num_citations": "13\n", "authors": ["345"]}
{"title": "Risk assessment of user-defined security configurations for android devices\n", "abstract": " The wide spreading of mobile devices, such as smartphones and tablets, and their advancing capabilities, ranging from taking photos to accessing banking accounts, make them an attractive target for attackers. This, together with the fact that users frequently store critical information in such devices and that many organizations allow employees to use their personal devices to access the enterprise information infrastructure and applications, makes security assessment a key need. This paper proposes an approach for assessing the security risk posed by user-defined configurations in Android devices. The approach is based on the analysis of the risk (impact and likelihood) of user misconfiguration to harm the device or the user. The impact and likelihood values are defined based on a Multiple-Criteria Decision Analysis (MCDA) performed on the inputs provided by a set of security experts. A case study considering\u00a0\u2026", "num_citations": "13\n", "authors": ["345"]}
{"title": "Assessing the security of web service frameworks against Denial of Service attacks\n", "abstract": " Web services frequently provide business\u2013critical functionality over the Internet, being widely exposed and thus representing an attractive target for security attacks. In particular, Denial of Service (DoS) attacks may inflict severe damage to web service providers, including financial and reputation losses. This way, it is vital that the software supporting services deployment (i.e., the web service framework) is able to provide a secure environment, so that the services can be delivered even when facing attacks. In this paper, we present an experimental approach that allows understanding how well a given web service framework is prepared to handle DoS attacks. The approach is based on a set of phases that include the execution of a large number of well-known DoS attacks against a target framework and the classification of the observed behavior. Results show that four out of the six frameworks tested are vulnerable\u00a0\u2026", "num_citations": "13\n", "authors": ["345"]}
{"title": "Fault injection\n", "abstract": " Resilient systems are designed to operate at acceptable levels even in the presence of faults and other adverse events. Assessing the resilience of a given system therefore requires that the effects of such events can be measured and examined in detail, which in turn requires the ability to introduce faults and observe the subsequent behaviour of the system. Fault injection is therefore a fundamental method for resilience assessment. It allows us to study the effect of faults on a system, and can thus be used to identify weaknesses in fault handling, to assess the effectiveness of error detectors and fault tolerance mechanisms, and to quantify the effect of faults on the quality of service achieved by the system. This chapter provides an introduction to fault injection for resilience assessment. We start with a brief overview of the area of fault injection, including the necessary terminology. We then discuss common\u00a0\u2026", "num_citations": "13\n", "authors": ["345"]}
{"title": "A service discovery approach for testing dynamic SOAs\n", "abstract": " Service Oriented Architectures (SOAs) are composed of distributed services that interact through standard interfaces, and evolve transparently to other services and users. Although such dynamicity makes SOA a promising architectural style, it prevents organizations from having complete knowledge of the SOA and of its (possibly untrusted) services. This constitutes an important restriction to the applicability of the SOA architectural style to real-time and (business-)critical services, consequently requiring new solutions to automatically discover the services that compose the SOA and to assess their behavior during execution. In this paper we present an approach for services discovery in dynamic SOAs. The discovery algorithm we propose merges information provided by different services providers and is implemented in a testing service that becomes an intrinsic part of the SOA. The practical case study presented\u00a0\u2026", "num_citations": "13\n", "authors": ["345"]}
{"title": "Extending test-driven development for robust web services\n", "abstract": " Research and practice show that a large number of Web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Test-driven development, particularly suitable for Web service environments, is a software development technique based on predefined test cases that are used during development to validate the desired software improvements or new functionalities. However, building representative test cases is quite difficult and developers often miss the test cases for robustness validation. This paper demonstrates how robustness testing can be integrated in the test driven development process to improve Web services robustness. To demonstrate the proposed approach we have invited experienced developers to implement three different versions of the Web services specified by the standard TPC-App performance benchmark, two following the\u00a0\u2026", "num_citations": "13\n", "authors": ["345"]}
{"title": "Towards assessing representativeness of fault injection-generated failure data for online failure prediction\n", "abstract": " Online Failure Prediction allows improving system dependability by foreseeing incoming failures at runtime, enabling mitigation actions to be taken in advance, though prediction systems' learning and assessing is hard due to the scarcity of failure data. Realistic software fault injection has been identified as a valid solution for addressing the scarcity of failure data, as injecting software faults (the most occurring on computer systems) increases the probability of a system to fail, hence allowing the collection of failure-related data in short time. Moreover, realistic injection permits the emulation of software faults likely to exist in the target system after its deployment. However, besides the representativeness of the software faults injected is recognized as a necessary condition for generating valid failure data, studies on the representativeness of generated failure-related data has still not been addressed. In this work we\u00a0\u2026", "num_citations": "12\n", "authors": ["345"]}
{"title": "Application of SOA in safety-critical embedded systems\n", "abstract": " Service-Oriented Architecture (SOA) are having a widespread use in enterprise computing applications, being Web services the most common implementation. The use of SOA has also been proposed for embedded systems, although very little could be found in the literature on the use of SOA for Safety-Critical Embedded Systems. This paper discusses the use of SOA for the development of this class of systems. Safety-critical embedded systems have specific requirements such as high reliability and real time response, making the use of SOA more challenging than for standard applications. To make concepts clear, a case study on Avionics for Unmanned Aerial Vehicles (UAVs) is presented. This is a complex application based on a reference model proposed by the authors. SOA shows to be a promising approach to implement parts of this reference model, especially in what concerns the missions played\u00a0\u2026", "num_citations": "12\n", "authors": ["345"]}
{"title": "Moving target defense in cloud computing: A systematic mapping study\n", "abstract": " Moving Target Defense (MTD) consists of applying system reconfiguration (e.g., VM migration, IP shuffling) to dynamically change the available attack surface. MTD makes use of reconfiguration to confuse attackers and nullify their knowledge about the system state. It also can be used as an attack reaction (e.g., using Virtual Machine (VM) migration to move VMs away from a compromised host). Thus, MTD seems to be a promising technique to tackle some of the cloud computing security challenges. In this systematic mapping study, we aim to investigate the current research state of Moving Target Defense in the cloud computing context, and to identify potential research gaps in the literature. Considering five major scientific databases in the computer science domain, we collected 224 papers related to the area. After disambiguation and filtering, we selected 95 papers for analysis. The outcome of such analysis\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Exploratory study of machine learning techniques for supporting failure prediction\n", "abstract": " The growing complexity of software makes it difficult or even impossible to detect all faults before deployment, and such residual faults eventually lead to failures at runtime. Online Failure Prediction (OFP) is a technique that attempts to avoid or mitigate such failures by predicting their occurrence based on the analysis of past data and the current state of a system. Given recent technological developments, Machine Learning (ML) algorithms have shown their ability to adapt and extract knowledge in a variety of complex problems, and thus have been used for OFP. Still, they are highly dependent on the problem at hand, and their performance can be influenced by different factors. The problem with most works using ML for OFP is that they focus only on a small set of prediction algorithms and techniques, although there is no comprehensive study to support their choice. In this paper, we present an exploratory analysis\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "A security configuration assessment for android devices\n", "abstract": " The wide spreading of mobile devices, such as smartphones and tablets, and their always-advancing capabilities makes them an attractive target for attackers. This, together with the fact that users frequently store critical personal information in such devices and that many organizations currently allow employees to use their personal devices to access the enterprise information infrastructure and applications, makes the assessment of the security of mobile devices a key issue. This paper proposes an approach supported by a tool that allows assessing the security of Android devices based on the user-defined settings, which are known to be a key source of security vulnerabilities. The tool automatically extracts 41 settings from the mobile devices under testing, 14 of which defined and proposed in this work and the remaining adapted from the well-known CIS benchmarks. The paper discusses the settings that are\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Engineering secure web services\n", "abstract": " Web services are key components in the implementation of Service Oriented Architectures (SOA), which must satisfy proper security requirements in order to be able to support critical business processes. Research works show that a large number of web services are deployed with significant security flaws, ranging from code vulnerabilities to the incorrect use of security standards and protocols. This chapter discusses state of the art techniques and tools for the deployment of secure web services, including standards and protocols for the deployment of secure services, and security assessment approaches. The chapter also discusses how relevant security aspects can be correlated into practical engineering approaches.", "num_citations": "11\n", "authors": ["345"]}
{"title": "HInjector: injecting hypercall attacks for evaluating VMI-based intrusion detection systems\n", "abstract": " Virtual machine introspection (VMI) is a mechanism for monitoring the states of guest virtual machines (VMs) from a virtualization host. Its use for intrusion detection is an emerging trend that brings many benefits, such as the possibility to monitor guest VMs in a transparent manner for attackers. However, a major issue is the evaluation of the attack detection accuracy of VMI-based intrusion detection systems (IDSes), eg,[1], for detecting virtualization-related attacks, such as attacks targeting virtual machine monitors (VMMs). VMMs\u2019 attack surfaces are very narrow making them difficult to penetrate. As a result there are only a few publicly available exploits that demonstrate attacks against VMMs. We believe that the automated artificial injection of malicious system activities with respect to representative attack models is a promising approach towards overcoming this issue.A malicious VM user may execute an attack against the underlying VMM via several attack vectors such as device drivers, VM exit events, or hypercalls. Hypercalls are software traps from a kernel of a paravirtualized guest VM to the VMM. They enable the intrusion in VMMs, initiated from a malicious guest VM kernel, in a procedural manner through VMMs\u2019 well-defined hypercall interfaces. This makes the automated injection of hypercall attacks feasible. Further, given the potential high severity of hypercall attacks, the evaluation of the effectiveness of IDSes for detecting and preventing such attacks is crucial. The exploitation of a vulnerability in a VMM\u2019s hypercall handler typically leads to altering the VMM\u2019s memory enabling, for example, the execution of malicious code with VMM\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Securing data warehouses from web-based intrusions\n", "abstract": " Decision support for 24/7 enterprises requires 24/7 available Data Warehouses (DWs). In this context, web-based connections to DWs are used by business management applications demanding continuous availability. Given that DWs store highly sensitive business data, a web-based connection provides a door for outside attackers and thus, creates a main security issue. Database Intrusion Detection Systems (DIDS) deal with intrusions in databases. However, given the distinct features of DW environments most DIDS either generate too many false alarms or too low intrusion detection rates. This paper proposes a real-time DIDS explicitly tailored for web-access DWs, functioning at the SQL command level as an extension of the DataBase Management System, using an SQL-like rule set and predefined checkups on well-defined DW features, which enable wide security coverage. We also propose a risk\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Leveraging 24/7 availability and performance for distributed real-time data warehouses\n", "abstract": " Real-time Data Warehouses (DWs) must be able to deal with continuous updates while ensuring 24/7 availability. To improve their performance, distributing data using round-robin algorithms on clusters of shared-nothing machines is normally used. This paper proposes a solution for distributed DW databases that ensures its continuous availability and deals with frequent data loading requirements, while adding small performance overhead. We use a data striping and replication architecture to distribute portions of each fact table among pairs of slave nodes, where each slave node is an exact replica of its partner. This allows balancing query execution and replacing any defective node, ensuring the system's continuous availability. The size of each portion in a given node depends on its individual features, namely performance benchmark measures and dedicated database RAM. The estimated cost for executing\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Integrating GQM and Data Warehousing for the Definition of Software Reuse Metrics\n", "abstract": " Software reuse is the practice of using existing artifacts (code, architecture, requirements, etc.) in new projects. The advantages of using previously developed software in new projects are easily understood. However, reusing artifacts is usually done in an ad-hoc and incipient way, requiring an important effort of adaptation, so developers frequently prefer to develop components from scratch. In this paper we present a strategy that is being adopted by Critical Software, a medium-sized company, to promote software reuse. This strategy starts by assuming that the success of software reuse is dependent on the ability of measuring its advantages. We have thus proposed the use of the Goal-Question-Metric (GQM) technique, extended with Data Warehousing data model design concepts to extract a set of reuse-specific metrics for measuring the gains of reuse. We show that it is very easy to measure the productivity\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Service-oriented architectures for complex safety-critical embedded systems: A case study on uavs\n", "abstract": " Jo\u00e3o Batista Camargo Jr and M\u00e1rio Corr\u00eaa are with the Universidade de S\u00e3o Paulo, S\u00e3o Paulo, Brazil (e-mail: joao. camargo@ poli. usp. br). Julio Estrella, Kalinka RLJC Branco, and OT Junior are with the Instituto de Ci\u00eancias Matem\u00e1ticas e de Computa\u00e7\u00e3o-USP, S\u00e3o Carlos, SP, Brazil (e-mails: jcezar, kalinka, otjunior@ icmc. usp. br). problem, allowing the use of different paradigms to solve different parts of a complex system, avoiding the disadvantages already mentioned. As a case study, an UAS (Unmanned Aerial System) reference model is proposed. The system was modeled (a layered reference model), showing its critical and non-critical sections. Services and protocols between layers are under development. The advantages of the use of SOA are discussed, easing the implementation of many high-level functions. SOA shows to be a promising approach to implement parts of this reference model, especially in what concerns the missions played by the aircraft. This paper also proposed a framework KBF (Knowledge Based Framework) to allow the use of SOA in the non-critical parts of Safety-Critical Embedded Systems, helping the implementation of the SSI (Smart Sensor Interface) and SSP (Smart Sensor Protocol), as they are good first candidates for application of the ideas presented in this paper.", "num_citations": "11\n", "authors": ["345"]}
{"title": "From performance to dependability benchmarking: A mandatory path\n", "abstract": " The work on performance benchmarking has started long ago. Ranging from simple benchmarks that target a very specific system or component to very complex benchmarks for complex infrastructures, performance benchmarks have contributed to improve successive generations of systems. However, the fact that nowadays most systems need to guarantee high availability and reliability shows that it is necessary to shift the focus from measuring pure performance to the measurement of both performance and dependability. Research on dependability benchmarking has started in the beginning of this decade, having already led to the proposal of several benchmarks. However, no dependability benchmark has yet achieved the status of a real benchmark endorsed by a standardization body or corporation. In this paper we argue that standardization bodies must shift focus and start including dependability\u00a0\u2026", "num_citations": "11\n", "authors": ["345"]}
{"title": "Plug and play fault injector for dependability benchmarking\n", "abstract": " Dependability benchmarks must include fault injectors that are very simple to install and use. They should be downloadable from the web together with the other components of the benchmark and be able to target all system components. Since the existing injectors did not fulfill this requirement, we have developed DBench-FI. Presently this SWIFI tool targets the Linux OS on Intel processors, and uses a flexible runtime kernel upgrading algorithm to allow access to either user or system spaces. It does not require the availability of the source code of any system component or user process. It is not based on any debug mode either, being able to inject faults even in tasks that were already running when it is installed, irrespective of their complexity. It currently only injects memory faults, but there are plans to include other fault models in the future. Results of complex fault injection campaigns are presented.", "num_citations": "11\n", "authors": ["345"]}
{"title": "Interacting srn models for availability evaluation of vm migration as rejuvenation on a system under varying workload\n", "abstract": " This paper presents a set of analytical models for availability evaluation of a virtualized system with VMM software rejuvenation enabled by VM migration schedule. The proposed models consider aspects of varying workload. We adopted the interacting models approach to reduce time and cost for models' evaluation. Our metrics of interest are steady-state availability and annual downtime. In our first case study, we compare the monolithic model and interacting models approach. Obtained results show a significant reduction in time for model evaluation in the interacting models approach. Our second case study presents a variety of scenarios with a varying workload. We consider the workload variation with two stages: i) peak - when the workload submitted is high; and ii) off-peak - when the workload submitted is low. The evaluation results comprise from systems without high workload demand (peakDuration = 0h\u00a0\u2026", "num_citations": "10\n", "authors": ["345"]}
{"title": "No SQL in practice: A write-heavy enterprise application\n", "abstract": " The continuous information growth in current organizations has created a need for adaptation and innovation in the field of data storage. Alternative technologies such as NoSQL have been heralded as the solution to the ever-growing data requirements of the corporate world, but these claims have not been backed by many real world studies. Current benchmarks evaluate database performance by executing specific queries over mostly synthetic data. These artificial scenarios, then, prevent us from easily drawing conclusions for the real world and appropriately characterize the performance of databases in a real system. To counter this, we used a real world enterprise system with real corporate data to evaluate the performance characteristics of popular NoSQL databases and compare them to SQL counterparts. In particular, we present one of the first write-heavy evaluations using enterprise software and big data\u00a0\u2026", "num_citations": "10\n", "authors": ["345"]}
{"title": "Portable faultloads based on operator faults for DBMS dependability benchmarking\n", "abstract": " Databases play a central role in the information infrastructure of most organizations. The characterization of DBMS (database management systems) dependability is then of utmost importance. Existing performance benchmarks for transactional and database areas include two major components: a workload and a set of performance measures. The definition of a benchmark to characterize dependability needs a new component - the faultload. Operator faults represent a major cause of failures in large DBMS. This paper proposes three approaches for the definition of portable faultloads based on operator faults to benchmark the dependability of DBMS and shows a benchmarking example of a commercial (Oracle) and an open source (PostgreSQL) database.", "num_citations": "10\n", "authors": ["345"]}
{"title": "To benchmark or not to benchmark security: That is the question\n", "abstract": " The multiplicity of available software and component alternatives has boosted the interest in suitable benchmarks, able to assist in the selection of candidate solutions from the existing diversity, concerning several attributes. The huge success of performance and dependability benchmarking, however, markedly contrasts with the small advances on security benchmarking, which has only sparsely been studied in recent years. In this position paper we discuss the difficulties involved in applying the dependability benchmarking approach to the security context, and propose and discuss an appealing alternative: trustworthiness benchmarking.", "num_citations": "9\n", "authors": ["345"]}
{"title": "A trust-based benchmark for DBMS configurations\n", "abstract": " Database management systems (DBMS), the central component of many computers applications, are typically immersed in very complex environments. Protecting the DBMS from security attacks requires evaluating a long list of complex configuration characteristics that may impact, in a variety of ways, the applications and people that interact with the database system. Effectively, understanding the impact of different configuration alternatives in terms of security is one of the most difficult problems faced by database administrators nowadays (DBA). In this paper we propose a benchmark that allows DBAs to assess and compare database configurations. The benchmark provides a trust-based security metric, named minimum untrustworthiness, that expresses the minimum level of distrust the DBA should have in a given configuration regarding its ability to prevent attacks. The practical application of the benchmark in\u00a0\u2026", "num_citations": "9\n", "authors": ["345"]}
{"title": "Towards data security in affordable data warehouse\n", "abstract": " The Data Warehouse Striping (DWS) technique is a round-robin data partitioning approach especially designed for affordable data warehousing environments based on clusters of low-cost computers running low-cost open-source software, which guaranties a nearly optimal speed up and scale up when new nodes are added to the cluster. However, low-cost software does not provide the security capabilities needed to protect critical business data. For example, most open-source Database Management Systems (DBMS) lack in providing efficient data encryption mechanisms essential to guarantee data confidentiality. This paper presents our current work on developing an architecture for affordable data warehouses with extended data protection capabilities and tolerance against nodes Denial of Service (DoS) attacks. The approach uses data encryption, spurious data, signatures, and redundancy to guarantee full data protection even when an attacker gets administrative access to one or more cluster nodes.", "num_citations": "9\n", "authors": ["345"]}
{"title": "Understanding how to use static analysis tools for detecting cryptography misuse in software\n", "abstract": " The use of cryptography is nowadays common in software systems, with cryptographic libraries widely available to software developers. As such, the likely weakest link in sensitive software has moved from cryptographic function implementations to the application code surrounding such functions. Ordinary developers usually lack knowledge in practical cryptography, and support from specialists is rare. Frequently, these difficulties are addressed by running static analysis tools to automatically detect cryptography misuse during coding and reviews. However, the effectiveness of such tools is not yet well understood. This article studies how well programmatic misuse of cryptography is detected by free static code analysis tools. The performance of such tools in detecting misuse is correlated to coding tasks and use cases commonly found in development efforts; also, cryptography misuse is classified in comprehensive\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "A model for availability and security risk evaluation for systems with VMM rejuvenation enabled by VM migration scheduling\n", "abstract": " Most companies and organizations rely nowadays on virtualized environments to host and run their applications. Some of these applications have stringent availability and security requirements. An important challenge for high availability in virtualized systems is software aging, which can lead the system to hangs or other types of failures. Software rejuvenation is applied to cope with software aging problems, whereas previous research suggests the use of Virtual Machine (VM) migration to reduce the downtime related to Virtual Machine Monitor (VMM) software rejuvenation. However, there is still a gap regarding the security implications of applying VM migration scheduling as support for VMM software rejuvenation. In this paper, we propose a security evaluation approach based on an availability model for virtualized systems with VM migration for VMM rejuvenation. The goal is to find the proper rejuvenation\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "An experimental study of software aging and rejuvenation in dockerd\n", "abstract": " Virtualized containers are being extensively used to host applications as they substantially reduce the overhead caused by conventional virtualization techniques. Therefore, as containers adoption grows, the need for dependability also increases. Dockerd, the process that is in charge of Docker containers management, is supposed to support long-running systems, which makes it prone to the well-known problem of software aging. This paper presents an experimental study of software aging and rejuvenation targeting the dockerd daemon. We used the SWARE approach to conduct the experimentation, which encompasses three phases: i) stress - stress environment with the accelerated workload to induce bugs activation; ii) wait - stop the workload submission to observe possible accumulated effects and; iii) rejuvenation - submit a rejuvenation action to perceive changes in the internal software state. The\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "Towards an approach for trustworthiness assessment of software as a service\n", "abstract": " Trust and trustworthiness assessment in cloud computing, as in other areas, is anything but simple, mainly due to the complex and dynamic nature of the cloud, variety of services (e.g., safety-critical or business-critical), large number of relevant quality attributes (e.g., security and performance), and last, but foremost, due to the subjective notion of trust and trustworthiness. In this paper, we first review the concepts related with trust in cloud computing. Then, we present a context-aware approach to benchmark cloud software services (SaaS) from a trustworthiness perspective. This approach helps providers to assess and improve their services and customers to select more trustworthy services.", "num_citations": "8\n", "authors": ["345"]}
{"title": "Service-oriented architectures for a flexible and safe use of unmanned aerial vehicles\n", "abstract": " Unmanned Aerial Vehicles (UAVs) are becoming widely used for supporting multiple heterogeneous missions, requiring continuous evolution and adaptation of the implemented services. This scenario calls for the definition of architectural abstractions that support the integration of service-variants, in particular the notion of SOA (Service-Oriented Architecture) for services composition, to fulfill application-specified performance and dependability requirements. In this paper, we discuss the problems of using SOA (a non-deterministic approach) in UAVs (a real-time based embedded system) and the safety issues that such environment may raise. We then put forward a framework to tackle the identified challenges and propose potential solutions considering the opportunities and threats of using SOA in UAVs.", "num_citations": "8\n", "authors": ["345"]}
{"title": "Test-Based Interoperability Certification for Web Services\n", "abstract": " Web Services are designed with the key goal of providing interoperable application-to-application interaction, regardless of the platforms involved. Although experience shows that interoperability is difficult to achieve, developers still have limited tools to assess the interoperability of their services and, to the best of our knowledge, none able to support end-to-end interoperability certification. In this paper, we lay the foundations of an interoperability certification process for Web services, which allows testing the interoperability level of a given Web service and also identifying possible interoperability issues. In practice, the process can be used by developers or providers to certify a given web service for interoperability, ensuring successful interaction with client-side platforms. We show the effectiveness of the process by conducting a large experimental evaluation to certify five different implementations of the services\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "A specific encryption solution for data warehouses\n", "abstract": " Protecting Data Warehouses (DWs) is critical, because they store the secrets of the business. Although published work state encryption is the best way to assure the confidentiality of sensitive data and maintain high performance, this adds overheads that jeopardize their feasibility in DWs. In this paper, we propose a Specific Encryption Solution tailored for DWs (SES-DW), using a numerical cipher with variable mixes of eXclusive Or (XOR) and modulo operators. Storage overhead is avoided by preserving each encrypted column\u2019s datatype, while transparent SQL rewriting is used to avoid I/O and network bandwidth bottlenecks by discarding data roundtrips for encryption and decryption purposes. The experimental evaluation using the TPC-H benchmark and a real-world sales DW with Oracle 11g and Microsoft SQL Server 2008 shows that SES-DW achieves better response time in both inserting and\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "Security testing in SOAs: Techniques and tools\n", "abstract": " Web Applications and Services are often deployed with critical software bugs that may be maliciously exploited. The adoption of Service Oriented Architectures (SOAs) in a wide range of organizations, including business-critical systems, opens the door to new security challenges. The problem is that developers are frequently not specialized on security and the common time-to-market constraints limit an in depth test for vulnerabilities. Additionally, research and practice shows that the effectiveness of existing vulnerability detection tools is very poor. This highlights the need for tools capable of efficiently detecting vulnerabilities in SOAs. This chapter discusses these problems and proposes new techniques and tools to improve services security by detecting vulnerabilities in a SOA in an automated manner.", "num_citations": "8\n", "authors": ["345"]}
{"title": "WSFAggressor: an extensible web service framework attacking tool\n", "abstract": " This paper presents a tool for testing the security of web service frameworks. The tool implements a large set of attack types, defined based on previous security research studies, existing testing tools, and field experience. The motivation is that developers frequently build web services based on the assumption that the underlying frameworks are secure, which is not always the case. Despite the evident need for security in the platforms that support services, existing security testing tools are very limited. In practice, most tools focus on application level vulnerabilities, and the few that allow testing platforms implement a very limited set of attack types. To the best of our knowledge, our tool includes more attacks than any other existing tool. Furthermore, by implementing an extensible architecture (based on plug-ins), the tool can be easily extended with additional attacks, supporting also a large variety of testing\u00a0\u2026", "num_citations": "8\n", "authors": ["345"]}
{"title": "Monitoring Database Application Behavior for Intrusion Detection\n", "abstract": " Database management systems (DBMS) represent the ultimate layer in preventing malicious data access or corruption and implement several security mechanisms to protect data. However these mechanisms cannot always stop malicious users from accessing data by exploiting system vulnerabilities. The aim of this paper is to propose an intrusion detection mechanism for DBMS to fill this gap. Our approach consists of a comprehensive representation of user database utilization profiles to perform concurrent intrusion detection. Prior to the detection it is necessary to define and learn these utilization profiles. Profiles are defined using a three level abstraction and learned directly from monitoring the database utilization in real conditions. The proposed mechanism is generic and can be easily implemented in commercial and open-source DBMS", "num_citations": "8\n", "authors": ["345"]}
{"title": "An exploratory study on machine learning to combine security vulnerability alerts from static analysis tools\n", "abstract": " Due to time-to-market needs and cost of manual validation techniques, software systems are often deployed with vulnerabilities that may be exploited to gain illegitimate access/control, ultimately resulting in non-negligible consequences. Static Analysis Tools (SATs) are widely used for vulnerability detection, where the source code is analyzed without executing it. However, the performance of SATs varies considerably and a high detection rate usually comes with significant false alarms. Recent studies considered combining various SATs to improve the overall detection ability, but they do not allow exploring different performance trade-offs, as basic and rigid rules are normally followed. Machine Learning (ML) algorithms have shown promising results in several complex problems, due to their ability to fit specific needs. This paper presents an exploratory study on the combination of the output of SATs through ML\u00a0\u2026", "num_citations": "7\n", "authors": ["345"]}
{"title": "Quantifying the attack detection accuracy of intrusion detection systems in virtualized environments\n", "abstract": " With the widespread adoption of virtualization, intrusion detection systems (IDSes) are increasingly being deployed in virtualized environments. When securing an environment, IT security officers are often faced with the question of how accurate deployed IDSes are at detecting attacks. To this end, metrics for assessing the attack detection accuracy of IDSes have been developed. However, these metrics are defined with respect to a fixed set of hardware resources available to the tested IDS. Therefore, IDSes deployed in virtualized environments featuring elasticity (i.e., on-demand allocation or deallocation of virtualized hardware resources during system operation) cannot be evaluated in an accurate manner using existing metrics. In this paper, we demonstrate the impact of elasticity on IDS attack detection accuracy. In addition, we propose a novel metric and measurement methodology for accurately quantifying\u00a0\u2026", "num_citations": "7\n", "authors": ["345"]}
{"title": "Requirements, design and evaluation of a privacy reference architecture for web applications and services\n", "abstract": " Organizational Information Systems (IS) collect, store, and manage personal and business information through web applications and services. Due to regulation laws and to protect the privacy of users, clients, and business partners, such information must be kept private. This paper proposes a privacy reference architecture that can serve as foundation for the analysis, design and development of web applications with privacy concerns. Using the proposed reference architecture, these applications can manage personal information in a more secure manner, protecting such information from different sources of privacy violation. Also, it can be used as a standardization model that facilitates system integration and communication. The architecture was evaluated regarding four key quality attributes: completeness, applicability, usability and feasibility. Results show that it brings values for the stakeholders and is an\u00a0\u2026", "num_citations": "7\n", "authors": ["345"]}
{"title": "Technical information on vulnerabilities of hypercall handlers\n", "abstract": " Modern virtualized service infrastructures expose attack vectors that enable attacks of high severity, such as attacks targeting hypervisors. A malicious user of a guest VM (virtual machine) may execute an attack against the underlying hypervisor via hypercalls, which are software traps from a kernel of a fully or partially paravirtualized guest VM to the hypervisor. The exploitation of a vulnerability of a hypercall handler may have severe consequences such as altering hypervisor's memory, which may result in the execution of malicious code with hypervisor privilege. Despite the importance of vulnerabilities of hypercall handlers, there is not much publicly available information on them. This significantly hinders advances towards securing hypercall interfaces. In this work, we provide in-depth technical information on publicly disclosed vulnerabilities of hypercall handlers. Our vulnerability analysis is based on reverse engineering the released patches fixing the considered vulnerabilities. For each analyzed vulnerability, we provide background information essential for understanding the vulnerability, and information on the vulnerable hypercall handler and the error causing the vulnerability. We also show how the vulnerability can be triggered and discuss the state of the targeted hypervisor after the vulnerability has been triggered.", "num_citations": "7\n", "authors": ["345"]}
{"title": "Benchmarking software requirements documentation for space application\n", "abstract": " Poorly written requirements are a common source of software defects. In application areas like space systems, the cost of malfunctioning software can be very high. This way, assessing the quality of software requirements before coding is of utmost importance. This work proposes a systematic procedure for assessing software requirements for space systems that adopt the European Cooperation for Space Standardization (ECSS) standards. The main goal is to provide a low-cost, easy-to-use benchmarking procedure that can be applied during the software requirements review to guarantee that the requirements specifications comply with the ECSS standards. The benchmark includes two checklists that are composed by a set of questions to be applied to the requirements specification. It was applied to the software requirements specification for one of the services described in the ECSS Packet Utilization\u00a0\u2026", "num_citations": "7\n", "authors": ["345"]}
{"title": "Benchmarking Untrustworthiness: An Alternative to Security Measurement\n", "abstract": " Benchmarking security is hard and, although there are many proposals of security metrics in the literature, no consensual quantitative security metric has been previously proposed. A key difficulty is that security is usually more influenced by what is unknown about a system than by what is known. In this paper, the authors propose the use of an untrustworthiness metric for benchmarking security. This metric, based on the idea of quantifying and exposing the trustworthiness relationship between a system and its owner, represents a powerful alternative to traditional security metrics. As an example, the authors propose a benchmark for Database Management Systems (DBMS) that can be easily used to assess and compare alternative database configurations based on minimum untrustworthiness, which is a low-cost and high-reward trust-based metric. The practical application of the benchmark in four real large\u00a0\u2026", "num_citations": "7\n", "authors": ["345"]}
{"title": "Predicting timing failures in web services\n", "abstract": " Web services are increasingly being used in business critical environments, enabling uniform access to services provided by distinct parties. In these environments, an operation that does not execute on due time may be completely useless, which may result in service abandonment, and reputation or monetary losses. However, existing web services environments do not provide mechanisms to detect or predict timing violations. This paper proposes a web services programming model that transparently allows temporal failure detection and uses historical data for temporal failure prediction. This enables providers to easily deploy time-aware web services and consumers to express their timeliness requirements. Timing failures detection and prediction can be used by client applications to select alternative services in runtime and by application servers to optimize the resources allocated to each service.", "num_citations": "7\n", "authors": ["345"]}
{"title": "Deploying fault tolerant web service compositions\n", "abstract": " Many businesses are now moving towards the use of composite web services. These consist of a collection of web services working together to achieve an objective. Although they are becoming business-critical elements, current development tools do not provide a practical way to include fault tolerance characteristics in web services compositions. This paper proposes a mechanism that allows programmers to easily develop fault tolerant compositions using diverse services. The mechanism allows programmers to specify alternative web services for each operation and offers a set of artifacts that simplify the coding process, by automatically dealing with all aspects related to the redundant web services invocation and responses voting. The mechanism is also able to perform a continuous evaluation of the services based on their behavior during operation. The approach is illustrated using compositions based on services publicly available in the Internet and on the services specified by the standard TPC-App performance benchmark.", "num_citations": "7\n", "authors": ["345"]}
{"title": "Availability and reliability modeling of vm migration as rejuvenation on a system under varying workload\n", "abstract": " Cloud computing serves as a platform for diverse types of applications, from low-priority to critical. Some of these applications require high levels of system availability and reliability. Developing methods for cloud computing availability and reliability evaluation is of utmost importance. In this paper, we propose a set of models for availability and reliability evaluation of a virtualized system with VMM software rejuvenation enabled by VM migration scheduling. To improve models fidelity with a real environment, we added a specific sub-model to represent the aspects of workload variation. Our main goal is to find the proper VM migration schedule to maximize system availability and to analyze the impact of such a schedule on the system reliability. Our results include the following: (1) the appropriate rejuvenation schedule to maximize availability in each proposed scenario; (2) downtime reduction when comparing the\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "An approach for trustworthiness benchmarking using software metrics\n", "abstract": " Trustworthiness is a paramount concern for users and customers in the selection of a software solution, specially in the context of complex and dynamic environments, such as Cloud and IoT. However, assessing and benchmarking trustworthiness (worthiness of software for being trusted) is a challenging task, mainly due to the variety of application scenarios (e.g., businesscritical, safety-critical), the large number of determinative quality attributes (e.g., security, performance), and last, but foremost, due to the subjective notion of trust and trustworthiness. In this paper, we present trustworthiness as a measurable notion in relative terms based on security attributes and propose an approach for the assessment and benchmarking of software. The main goal is to build a trustworthiness assessment model based on software metrics (e.g., Cyclomatic Complexity, CountLine, CBO) that can be used as indicators of software\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "Diversity with intrusion detection systems: An empirical study\n", "abstract": " Defence-in-depth is a term often used in security literature to denote architectures in which multiple security protection systems are deployed to defend the valuable assets of an organization (e.g. the data and the services). In this paper we present an approach for analysing defence-in-depth, and illustrate the use of the approach with an empirical study in which we have assessed the detection capabilities of intrusion detection systems when deployed in diverse, two-version, parallel defence-in-depth configurations. The configurations have been assessed in settings that favour detection of attacks (reducing false negatives), as well as settings that favour legitimate traffic (reducing false positives).", "num_citations": "6\n", "authors": ["345"]}
{"title": "A benchmarking process to assess software requirements documentation for space applications\n", "abstract": " Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "On benchmarking intrusion detection systems in virtualized environments\n", "abstract": " Modern intrusion detection systems (IDSes) for virtualized environments are deployed in the virtualization layer with components inside the virtual machine monitor (VMM) and the trusted host virtual machine (VM). Such IDSes can monitor at the same time the network and host activities of all guest VMs running on top of a VMM being isolated from malicious users of these VMs. We refer to IDSes for virtualized environments as VMM-based IDSes. In this work, we analyze state-of-the-art intrusion detection techniques applied in virtualized environments and architectures of VMM-based IDSes. Further, we identify challenges that apply specifically to benchmarking VMM-based IDSes focussing on workloads and metrics. For example, we discuss the challenge of defining representative baseline benign workload profiles as well as the challenge of defining malicious workloads containing attacks targeted at the VMM. We also discuss the impact of on-demand resource provisioning features of virtualized environments (e.g., CPU and memory hotplugging, memory ballooning) on IDS benchmarking measures such as capacity and attack detection accuracy. Finally, we outline future research directions in the area of benchmarking VMM-based IDSes and of intrusion detection in virtualized environments in general.", "num_citations": "6\n", "authors": ["345"]}
{"title": "A Field Perspective on the Interoperability of Web Services\n", "abstract": " In a typical web services environment, a web service framework supports the client and server interaction by, among other tasks, announcing the services interfaces and translating application-level service calls to SOAP messages. Although designed to support inter-operation, research and practice suggest that existing client-side and server-side frameworks, many times, cannot fully inter-operate. The problem is that, as web services are increasingly being deployed to support business-critical environments, interoperability issues may prevent or impact business transactions, potentially resulting in huge financial and reputation losses. In this paper we present an experimental evaluation of the interoperability of 1024 publicly available web services, against a set of diverse and well-known client-side web service frameworks. We have detected at least one severe interoperability issue in over 53% of the services\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "HLA middleware robustness and scalability evaluation in the context of satellite simulators\n", "abstract": " Satellite simulators are used to support the tasks of space mission analysis and satellite verification and operation, having high dependability requirements. When used for different missions or different phases of the same mission, robustness and scalability are two particularly important properties. This practical experience report presents the results of a robustness and scalability evaluation of two open-source Runtime Libraries implementing HLA, a standard widely used in the context of simulators development. Results show that the tested implementations present a large number of robustness problems and that scalability is quite limited.", "num_citations": "6\n", "authors": ["345"]}
{"title": "Changeloads: A fundamental piece on the saso systems benchmarking puzzle\n", "abstract": " Benchmarks have been traditionally tailored to static, unchangeable systems, functioning in well-known and controlled environments. Thus, established benchmarks (and benchmarking approaches) are becoming progressively less representative of real world scenarios, as change is gaining emphasis as a fundamental player in computing systems runtime conditions. As today's systems are becoming, at the very least, reactive to changes (either endogenous or exogenous) at some level, if not even proactive in reaching their goals more efficiently and effectively, we believe that benchmarks must also evolve, becoming applicable to systems that react to change, adapt, evolve, and have the capability to improve their own performance. In this position paper, we argue that representative change loads are now as vital as representative workloads, and that change load-based benchmarks will become a key part in the\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "Evaluating the feasibility issues of data confidentiality solutions from a data warehousing perspective\n", "abstract": " Data Warehouses (DWs) are the core of enterprise sensitive data, which makes protecting confidentiality in DWs a critical task. Published research and best practice guides state that encryption is the best way to achieve this and maintain high performance. However, although encryption algorithms strongly fulfill their security purpose, we demonstrate that they introduce massive storage space and response time overheads, which mostly result in unacceptable security-performance tradeoffs, compromising their feasibility in DW environments. In this paper, we enumerate state-of-the-art data masking and encryption solutions and discuss the issues involving their use from a data warehousing perspective. Experimental evaluations using the TPC-H decision support benchmark and a real-world sales DW support our remarks, implemented in Oracle 11g and Microsoft SQL Server 2008. We conclude that the\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "Implementing software effort estimation in a medium-sized company\n", "abstract": " Effort estimation in software development projects is far from being an easy task. In fact, despite the several effort estimation techniques available in the literature and the need for companies to perform such task in a daily basis, most small and medium-sized companies still suffer from the problem of incorrect estimations that often result in losing the contract bid or in failure during project execution. In this paper we present and discuss the implementation of a software effort estimation process in a medium-sized company, currently recognized as CMMI Level 5. The paper contextualizes the problem and the company, introduces the estimation techniques used, and presents some preliminary results, showing that software effort estimation can be successfully applied in medium-sized companies at low cost, allowing the reduction of project uncertainty and increasing the probability of success during bidding and execution.", "num_citations": "6\n", "authors": ["345"]}
{"title": "Robustness validation in service-oriented architectures\n", "abstract": " The use of Service Oriented Architecture (SOA) in enterprise applications development is increasing very quickly. In a SOA environment providers supply a set of services that must be robust. Although SOA is being used in business-critical environments, there is no practical means to measure or compare the robustness of services. Robustness failures in such environments are dangerous, as they can be maliciously exploited with severe consequences for the attacked systems. This chapter addresses the problem of robustness validation in SOA environments. The approach proposed is based on a set of robustness tests that is used to discover both programming and design errors. Two concrete examples are presented: one focusing on web services and the other targeting Java Message Service (JMS) middleware. The proposed approach is useful for both providers (to validate the robustness of deployed\u00a0\u2026", "num_citations": "6\n", "authors": ["345"]}
{"title": "Data Processing on Edge and Cloud: A Performability Evaluation and Sensitivity Analysis\n", "abstract": " Nowadays, the Internet of Things (IoT) allows monitoring and automation in diverse contexts, such as hospitals, homes, or even smart cities, just to name a few examples. IoT data processing may occur, at the edge of the network or in the cloud, but frequently the processing must be divided between the two layers. Aiming to guarantee that the IoT systems works efficiently, it is essential to evaluate the system even in initial design stages. However, evaluating hybrid systems composed by multiple layers is not an easy task as a myriad of parameters are involved in the process. Thus, this paper presents two SPN models (one base and extended one) that can represent an abstract distributed system composed of IoT, edge and cloud layers. The models are highly configurable to be used in diverse simulation scenarios. Besides a sensitivity analysis evidenced the most impacting components in the studied architecture\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "An approach for benchmarking the security of web service frameworks\n", "abstract": " Web services are a popular technology for deploying applications on the Web. They are supported by frameworks, the middleware that handles most communication aspects. Security in the Web is a key concern as the exposure to attacks is high and may result in catastrophic consequences for the deployed services. Selecting the most secure framework is challenging, especially considering their diversity and the complexity involved in any security assessment. This paper is an initial contribution aiming at the definition of a security benchmark for assessing and comparing the security of web service frameworks. The proposed benchmark is based on two phases: Security Qualification and Trustworthiness Assessment. In the first phase, state-of-the-art techniques are used to detect vulnerabilities in the frameworks. If vulnerabilities are found, the framework is disqualified. In the second phase, the qualified frameworks\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "A platform to enable self-adaptive cloud applications using trustworthiness properties\n", "abstract": " Self-Adaptive Systems (SASs) reflect on both their state and on the environment and change their behavior to satisfy the expected objectives. Cloud systems are self-adaptive by nature, especially considering the resources used in a pay-as-you-go manner. Satisfying trustworthiness (worthiness of a service based on evidences of its trust) properties also demands self-adaptation capabilities. Unfortunately, developers lack an easy-to-use platform to support the assessment of such properties and to execute the required adaptions. This paper presents TMA, a platform that implements a MAPE-K control loop for cloud systems, supported by a distributed monitoring system based on probes. Quality Models are used to express trustworthiness properties, resulting in scores, which are used to plan adaptations through evaluation rules. These plans are executed by actuators. A demo shows the scaling up/down of the\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "Iaas cloud availability planning using models and genetic algorithms\n", "abstract": " One of the main goals of cloud customers is to improve the availability levels of their applications. Thus, Cloud Providers usually offer Service Level Agreements (SLAs) to meet the availability requirements of the customers. However, setting up reasonable availability SLAs is a challenging task due to the cloud environment complexity. High availability is also a challenge for small private cloud environments, which nowadays have to provide a high availability platform for the hosted applications. In this paper, we propose an approach to support the design of Infrastructure-as-a-Service (IaaS) Cloud architectures aiming at the desired levels of system availability. Our fundamental architecture considers the main four components of virtualized environments: Front-End, Physical Machines, Virtual Machines and sStorage Area Network. We designed an availability model for IaaS architectures based on these components\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "Propheticus: Machine learning framework for the development of predictive models for reliable and secure software\n", "abstract": " The growing complexity of software calls for innovative solutions that support the deployment of reliable and secure software. Machine Learning (ML) has shown its applicability to various complex problems and is frequently used in the dependability domain, both for supporting systems design and verification activities. However, using ML is complex and highly dependent on the problem in hand, increasing the probability of mistakes that compromise the results. In this paper, we introduce Propheticus, a ML framework that can be used to create predictive models for reliable and secure software systems. Propheticus attempts to abstract the complexity of ML whilst being easy to use and accommodating the needs of the users. To demonstrate its use, we present two case studies (vulnerability prediction and online failure prediction) that show how it can considerably ease and expedite a thorough ML workflow.", "num_citations": "5\n", "authors": ["345"]}
{"title": "Development of complex software with agile method\n", "abstract": " Agile Software Development (ASD) has been on mainstream through methodologies such as XP and Scrum enabling them to be applied in the development of complex and reliable software systems. This paper is the end result of the Master's dissertation of the main author, and proposes a solution to guide the development of complex systems based on components by adding exceptional behavior modeling practices to Scrum, resulting in the Scrum+CE method (Scrum with Exceptional Behavior). In order to evaluate the proposed method, a synthetic controlled experiment was conducted with three groups. We compared the efficiency of the new process in relation to plain Scrum and the results were the production of a better quality software but with less features implemented during the same amount of time.", "num_citations": "5\n", "authors": ["345"]}
{"title": "Studying the propagation of failures in SOAs\n", "abstract": " Although Service Oriented Architectures (SOAs) are being increasingly used in business-critical scenarios, the applicability of Verification and Validation (V&V) is still very limited. The problem is that V&V activities have to be implemented at runtime to fit the characteristics of SOA. Recent proposals of runtime V&V techniques specific to SOA domain are far from being complete and a key issue lies in understanding how the \"failures propagate\" in a dynamic system and how to continuously verify its evolving elements. This paper introduces an approach to deal with the propagation of failures in a SOA environment. The proposed technique is based on three key steps: estimating the failure rate of the individual services, using fault injection to find the exposure of each service to failures from the invoked services, and estimating the impact of each service in the overall architecture. The overall approach is presented with\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "Towards evaluating the impact of data quality on service applications\n", "abstract": " Service applications frequently make use of a relational database to store and retrieve data and rely on the correctness of this data to deliver service to clients. Despite this, relational databases do not provide support for complex data integrity restrictions, which have to be controlled by the application. As such, bugs present in service applications can easily lead to the storage of incorrect data that, at random instants can cause applications to fail and stop delivering service, which can severely impact clients, other applications, and even the reputation or finance of the service provider. The goal of this work is to set the basis for an approach that is able to assess how vulnerable a service application can be to incorrect data. We expect that the results can also be used to suggest solutions for applications showing failures in presence of poor data and to define problem prevention techniques during the development of\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "Trustworthiness Benchmarking of Web Applications Using Static Code Analysis\n", "abstract": " Benchmarking the security of web applications is complex and, although there are many proposals of metrics, no consensual quantitative security metric has been proposed so far. Static analysis is an effective approach for detecting vulnerabilities, but the complexity of applications and the large variety of vulnerabilities prevent any single tool from being foolproof. In this application paper we investigate the hypothesis of combining the output of multiple static code analyzers to define metrics for comparing the trustworthiness of web applications. Various experiments, including a benchmarking campaign over seven distinct open source web forums, show that the raw number of vulnerabilities reported by a set of tools allows rough trustworthiness comparison. We also study the use of normalization and false positive rate estimation to calibrate the output of each tool. Results show that calibration allows computing a\u00a0\u2026", "num_citations": "5\n", "authors": ["345"]}
{"title": "Testing Web Services for Robustness: A Tool Demo\n", "abstract": " Web services represent a powerful interface for back-end systems that must provide a robust interface to client applications, even in the presence of invalid inputs. However, developing robust services is a difficult task. In this paper we demonstrate wsrbench, an online tool that facilitates web services robustness testing. Additionally, we present two scenarios to motivate robustness testing and to demonstrate the power of robustness testing in web services environments.", "num_citations": "5\n", "authors": ["345"]}
{"title": "On the Use of Open-Source C/C++ Static Analysis Tools in Large Projects\n", "abstract": " Software applications are frequently deployed with security vulnerabilities that may open the door to attacks. In business-critical scenarios, such attacks may lead to significant financial and reputation losses. Static Analysis Tools (SATs), which analyze the source code without executing it, can be used to detect potential faults in the source code, including security vulnerabilities. However, many false alarms are normally reported, leading teams to discard the use of such tools, especially on large software projects. Existing works have dealt with the evaluation of SATs, but they are mostly based on small pieces of code designed to support the evaluation. In this paper, we present and discuss the results of the execution of two Open-Source C/C++ SATs (CPPCheck and Flawfinder) on the large open-source project Mozilla. Our goal is to study the applicability of SATs in a large project and the vulnerability categories they\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "A hierarchical model for virtualized data center availability evaluation\n", "abstract": " Availability events like hangs, failures, and interruptions are usually unexpected and unpredictable. Due to the involving uncertainty, setting up a consolidated approach for availability evaluation is a hard endeavor. The availability evaluation becomes even more complicated in complex systems like cloud computing. Previous works fill such a gap using availability models. However, the majority of those papers proposes models for specific scenarios, which impairs their scalability and flexibility. In this paper, we present a scalable availability model for virtualized datacenters. From the model's design point-of-view, we provide: i) a hierarchical model based on Reliability Block Diagrams and Stochastic Reward Nets, and ii) an evaluation of the time to compute the model's results. We tested different architectures regarding system availability, capacity-oriented availability, and power consumption. As our model\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "INTENSE: INteroperability TEstiNg as a SErvice\n", "abstract": " The web services technology has been created to support communication between heterogeneous platforms. Despite its maturity, built upon more than a decade of experience, research and practice show that the technology still fails to connect web service client applications to servers, even when the programming languages involved are the same. This is especially troubling for service providers, as a failure in the inter-operation of web services may lead to disastrous consequences for the services involved, which frequently support businesses. In this paper, we present INTENSE, a service deployed as an on-line web application, designed to test the interoperability of a web service against specific client-side platforms. The tool is able to test the pre-runtime steps involving code generation and the end-to-end runtime communication, present in a web service interaction with a client. We used INTENSE to test a set\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "ITWS: An extensible tool for interoperability testing of web services\n", "abstract": " Web services are supported by a set of protocols that have been designed with the main goal of providing interoperable communication to applications. In typical business-critical services environments the occurrence of interoperability issues can have disastrous consequences, including direct financial costs, reputation, and client fidelity losses. Despite this, experience suggests that interoperability is still quite difficult to achieve, since the heterogeneity of frameworks for providing web services is quite large. In addition, current tools have limited testing capabilities and, in many cases do not specialize in this problem. In this paper we present ITWS, an extensible Interoperability Testing tool for Web Services that is able to assess the interoperability of a web service, supported by any given framework. We have used ITWS to test the interoperability of a set of home-implemented TPC-App web services and a set of\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "Lessons Learnt in the Implementation of CMMI\u00ae Maturity Level 5\n", "abstract": " CMMI \u00ae  has proven benefits in software process improvement. Typically, organisations that achieve a CMMI level rating improve their performance. However, CMMI implementation is not trivial, in particular for high maturity levels, and not all organisations achieve the expected results. Certain CMMI implementation problems may remain undetected by SCAMPISM since only a sample of the organisation is analysed during the appraisal and assessing the quality of implementation of some practices may be difficult. In this paper we present the case of three CMMI level 5 organisations. From the lessons learnt and based on an extensive bibliographic research, we identify a set of problems and difficulties that organisations willing to implement CMMI should be aware of and provide a set of recommendations to help avoid them. As future research we will develop a framework to help to evaluate the quality of\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "Benchmarking the Resilience of Self-Adaptive Systems: A New Research Challenge\n", "abstract": " Self-adaptive systems are widely recognized as the future of computer systems. Due to their dynamic and evolving nature, the characterization of self-adaptation and resilience attributes is of upmost importance. The problem is that nowadays there is no practical way to characterize self-adaptation capabilities or to compare alternative solutions concerning resilience. In this paper we discuss the problem of resilience benchmarking of self-adaptive systems. We start by identifying a set of key challenges and then propose a research roadmap to tackle those challenges.", "num_citations": "4\n", "authors": ["345"]}
{"title": "Supporting fraud analysis in mobile telecommunications using case-based reasoning\n", "abstract": " Fraud in mobile telecommunications is a complex and dynamic problem for Telecom operators. These companies have developed and are exploring new ways of making the fraud detection process more efficient. Most of these attempts are based in fraud management systems, capable of detecting fraudulent communications. In this paper, we present a case-based reasoning system that aids fraud analysts in the classification of potential fraud cases. The system developed, presents to the analyst the most similar past cases, representing suspicious communication episodes that were previously investigated. We also describe an example of how the system is used.", "num_citations": "4\n", "authors": ["345"]}
{"title": "Joint evaluation of recovery and performance of a COTS DBMS in the presence of operator faults\n", "abstract": " A major cause of failures in large database management systems (DBMS) is operator/administrator faults. Although most of the complex DBMS available today have comprehensive recovery mechanisms, the effectiveness of these mechanisms is difficult to characterize. On the other hand, the tuning of a large database is very complex and database administrators tend to concentrate on performance tuning and disregard the recovery mechanisms. Above all, database administrators seldom have feedback on how good a given configuration is concerning recovery. This paper proposes an experimental approach to characterize both the performance and the recoverability of DBMS. Our approach is presented through a concrete example of benchmarking the performance and recovery of an Oracle DBMS running the standard TPC-C benchmark, extended to include two new elements: a faultload based on operator\u00a0\u2026", "num_citations": "4\n", "authors": ["345"]}
{"title": "Security and Availability Modeling of VM Migration as Moving Target Defense\n", "abstract": " Moving Target Defense (MTD) is a defensive mechanism based on dynamic system reconfiguration to prevent or thwart cyberattacks. In the last years, considerable progress has been made regarding MTD approaches for virtualized environments, and Virtual Machine (VM) migration is the core of most of these approaches. However, VM migration produces system downtime, meaning that each MTD reconfiguration affects system availability. Therefore, a method for a combined evaluation of availability and security is of utmost importance for VM migration-based MTD design. In this paper, we propose a Stochastic Reward Net (SRN) for the probability of attack success and availability evaluation of an MTD based on VM migration scheduling. We study the MTD system under different conditions regarding 1) VM migration scheduling, 2) VM migration failure probability, and 3) attack success rate. Our results highlight\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "Evaluating the applicability of robustness testing in virtualized environments\n", "abstract": " Virtualization provides many benefits as server consolidation and cost reduction, but it also introduces new challenges like security and isolation. Thus, trust is still one of the roadblocks in their adoption in critical systems. Virtualized systems are governed by a hypervisor and resources are shared amongst virtual machines. Paravirtualization improves the performance of the costly I/O operations, by providing an hypercall interface to the guests' kernel. Hypercalls must be robust and secure, as their abuse leads to harmful effects. This paper presents an assessment of the applicability of robustness testing to the Xen hypercall interface. For this, we devised a testing campaign by mutating valid hypercall invocations with invalid values. The campaign was then executed from a compromised machine inserted in a representative virtualization environment. The results revealed the compromised machine being crashed\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "Toward characterizing HTML defects on the Web\n", "abstract": " HTML is being massively used as an interface to provide services to users. Web developers are producing and changing sites at a high pace while trying to support the latest HTML standards. In this context, it is common to find websites that do not comply with the standards and fail to be correctly processed by browsers. Considering this dynamic environment and the increasingly large diversity of browsers with frequent updates, the appearance of problems in web pages is a common, sometimes severe, and hard\u2010to\u2010track problem. In this short communication, we describe the initial design of an approach that will be used to obtain information regarding the characteristics of HTML documents on the Web and extract indicators of representative errors made by their developers. Preliminary results show nearly 90% of the pages analyzed having at least one type of error and the prevalence of a small number of error types.", "num_citations": "3\n", "authors": ["345"]}
{"title": "GIS and Data: Three applications to enhance Mobility\n", "abstract": " The increasing urban population sets new demands for mobility solutions. The impacts of traffic congestions or inefficient transit connectivity directly affect public health (eg emissions and stress) and the city economy (eg deaths in road accidents, productivity, and commuting). In parallel, the advance of technology has made it easier to obtain data about the systems which make up the city information systems. This paper takes advantage of GIS and real-time data to present: 1) a web application integrating multiple services; 2) an android application for bus visualization and prediction and 3) a dashboard focused on applying exploratory data analysis techniques on ticketing data.", "num_citations": "3\n", "authors": ["345"]}
{"title": "A practical approach towards automatic testing of web services interoperability\n", "abstract": " Web Services are a technology designed to support the invocation of remote elements by client applications, with the goal of providing interoperable application-to-application interaction while assuring vendor and platform independence. In business-critical environments, the occurrence of interoperability issues can have disastrous consequences, including direct financial costs, reputation, and client fidelity losses. Despite this, experience shows that services interoperability is still quite difficult to achieve. The goal of this paper is to propose a practical testing process to understand the real level of interoperability provided by web services platforms. An extensible tool, that implements the proposed approach, has been used to run a large campaign during which we have tested the interoperability of a large number of web services, comprising both home-implemented and publicly available services, deployed on top\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "Characterizing the performance of web service frameworks under security attacks\n", "abstract": " Web Services are a platform-independent technology that enables business processes to be accessible worldwide. In this context, web service frameworks play a key role as intermediate layers, offering support for interoperability and communication between providers and consumers. Due to their relevance in the overall infrastructure, frameworks are a frequent target of attacks trying to impact the performance of the provided services. This paper proposes an experimental approach for assessing the performance of web service frameworks under the presence of security attacks. The approach is based on a client that exchanges non-malicious messages with an infrastructure that includes the framework being assessed and several web service applications. Attacks are performed by a malicious client application and performance measures are collected during the process (from the point-of-view of legitimate users\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "A monitoring and testing framework for critical off-the-shelf applications and services\n", "abstract": " One of the biggest verification and validation challenges is the definition of approaches and tools to support system assessment while minimizing costs and delivery time. This includes the integration of OTS software components in critical systems that must undergo proper certification or approval processes. In the particular case of testing, due to the differences and peculiarities of components, developers often build ad-hoc and poorly-reusable testing tools, which results in increased time and costs. This paper introduces a framework for testing and monitoring of critical OTS applications and services. The framework includes i) a box that is instrumented for monitoring OS and application level variables, ii) an adaptable toolset for testing the target components, and iii) tools for data storing, retrieval and analyzes. A prototype of the framework is under development, and future testing scenarios are designed to show the\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "SCoRe: An across-the-board metric for computer systems resilience benchmarking\n", "abstract": " Resilience benchmarking is currently the focus of many research initiatives. Assessing and comparing computer systems under changing environments is becoming crucial due to the dynamic characteristics of modern computing environments. Although several metrics have been proposed over the years, there is no universally accepted resilience metric, which hampers the definition of representative, useful, and accepted benchmarks. In this paper we propose a resilience metric, the Specific Corrected Resilience (SCoRe), which portrays the ability of the system to keep operating with a desired level of quality, in spite of the imposed stress, opening the way for benchmarking the resilience of computer systems.", "num_citations": "3\n", "authors": ["345"]}
{"title": "How to advance TPC benchmarks with dependability aspects\n", "abstract": " Transactional systems are the core of the information systems of most organizations. Although there is general acknowledgement that failures in these systems often entail significant impact both on the proceeds and reputation of companies, the benchmarks developed and managed by the Transaction Processing Performance Council (TPC) still maintain their focus on reporting bare performance. Each TPC benchmark has to pass a list of dependability-related tests (to verify ACID properties), but not all benchmarks require measuring their performances. While TPC-E measures the recovery time of some system failures, TPC-H and TPC-C only require functional correctness of such recovery. Consequently, systems used in TPC benchmarks are tuned mostly for performance. In this paper we argue that nowadays systems should be tuned for a more comprehensive suite of dependability tests, and that a\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "An application for electroencephalogram mining for epileptic seizure prediction\n", "abstract": " A computational framework to support seizure predictions in epileptic patients is presented. It is based on mining and knowledge discovery in Electroencephalogram (EEG) signal. A set of features is extracted and classification techniques are then used to eventually derive an alarm signal predicting a coming seizure. The epileptic patient may then take steps in order to prevent accidents and social exposure.", "num_citations": "3\n", "authors": ["345"]}
{"title": "Towards timely ACID transactions in DBMS\n", "abstract": " On-time data management is becoming a key difficulty faced by the information infrastructure of most organizations. In fact, database applications for critical areas are increasingly giving more importance to the timely execution of transactions. Database applications with timeliness requirements have to deal with the possible occurrence of timing failures, when the operations specified in the transaction do not complete within the expected deadlines. In spite of the importance of timeliness requirements in database applications, typical commercial DBMS do not assure any temporal properties, not even the detection of the cases when the transaction takes longer than the expected/desired time. This paper discusses the problem of timing failure detection in database applications and proposes a transaction programming approach to help developers in programming database applications with time constraints\u00a0\u2026", "num_citations": "3\n", "authors": ["345"]}
{"title": "TACID transactions\n", "abstract": " Developing database applications with timeliness requirements is a difficult problem. During the execution of transactions, database applications with timeliness requirements have to deal with the possible occurrence of timing failures, when the operations specified in the transaction do not complete within the expected deadlines. In spite of the importance of timeliness requirements in database applications, database management systems (DBMS) do not assure any temporal property, not even the detection of the cases when the transaction takes longer than the expected/desired time. Our goal is to investigate ways to add timeliness properties to the typical ACID (Atomicity, Consistency, Isolation, and Durability) properties supported by most DBMS.", "num_citations": "3\n", "authors": ["345"]}
{"title": "Vulnerable Code Detection Using Software Metrics and Machine Learning\n", "abstract": " Software metrics are widely-used indicators of software quality and several studies have shown that such metrics can be used to estimate the presence of vulnerabilities in the code. In this paper, we present a comprehensive experiment to study how effective software metrics can be to distinguish the vulnerable code units from the non-vulnerable ones. To this end, we use several machine learning algorithms (Random Forest, Extreme Boosting, Decision Tree, SVM Linear, and SVM Radial) to extract vulnerability-related knowledge from software metrics collected from the source code of several representative software projects developed in C/C++ (Mozilla Firefox, Linux Kernel, Apache HTTPd, Xen, and Glibc). We consider different combinations of software metrics and diverse application scenarios with different security concerns (e.g., highly critical or non-critical systems). This experiment contributes to\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "On configuring a testbed for dependability experiments: Guidelines and fault injection case study\n", "abstract": " Several techniques have been developed to experimentally assess the dependability of computer systems, such as fault injection and robustness testing. Given the growing complexity of systems, such approaches often require a large set of experiments to be performed in order to achieve statistical relevance, thus leading to extremely long experimental campaigns. Due to recent developments, there are now various technologies (e.g., multithreading, virtualization) that maximize the use of computer resources. However, taking advantage of such technologies to implement a testbed that accelerates the experimental process is complex and, to the best of our knowledge, no guidelines or examples are easily accessible. This practical experience report overviews the attributes and requirements that should be considered when implementing a testbed to accelerate dependability experiments and presents our\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Improving failure prediction by ensembling the decisions of machine learning models: A case study\n", "abstract": " The complexity of software has grown considerably in recent years, making it nearly impossible to detect all faults before pushing to production. Such faults can ultimately lead to failures at runtime. Recent works have shown that using Machine Learning (ML) algorithms it is possible to create models that can accurately predict such failures. At the same time, methods that combine several independent learners (i.e., ensembles) have proved to outperform individual models in various problems. While some well-known ensemble algorithms (e.g Bagging) use the same base learners (i.e., homogeneous), using different algorithms (i.e., heterogeneous) may exploit the different biases of each algorithm. However, this is not a trivial task, as it requires finding and choosing the most adequate base learners and methods to combine their outputs. This paper presents a case study on using several ML techniques to create\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Towards models for availability and security evaluation of cloud computing with moving target defense\n", "abstract": " Security is one of the most relevant concerns in cloud computing. With the evolution of cyber-security threats, developing innovative techniques to thwart attacks is of utmost importance. One recent method to improve cloud computing security is Moving Target Defense (MTD). MTD makes use of dynamic reconfiguration in virtualized environments to \"confuse\" attackers or to nullify their knowledge about the system state. However, there is still no consolidated mechanism to evaluate the trade-offs between availability and security when using MTD on cloud computing. The evaluation through measurements is complex as one needs to deal with unexpected events as failures and attacks. To overcome this challenge, we intend to propose a set of models to evaluate the availability and security of MTD in cloud computing environments. The expected results include the quantification of availability and security levels under different conditions (e.g., different software aging rates, varying workloads, different attack intensities).", "num_citations": "2\n", "authors": ["345"]}
{"title": "A Europe-Brazil Context for Secure Data Analytics in the Cloud\n", "abstract": " Intercontinental data processing cloud systems raise stringent security and privacy challenges, particularly due to legislation differences. We propose solutions for these challenges with elastic AAA, efficient privacy and anonymization techniques in multiple phases, and security assessment for trustworthiness estimation.", "num_citations": "2\n", "authors": ["345"]}
{"title": "A research agenda for benchmarking the resilience of software defined networks\n", "abstract": " Software Defined Networking (SDN) has recently emerged as a hot topic deserving strong interest both from academia and industry. The change it advises of logically centralizing the overall network control on software platforms and applications, turns networks more and more into software-based systems. Such paradigm shift enables to build more manageable, agile and smarter data communication infrastructures. Despite the interest it is deserving on a multitude of physical and virtualized infrastructures, currently there are no systematic approaches of characterizing and comparing alternative SDN-based solutions regarding their resilience -- a characteristic of utmost importance on such critical environments. In this paper we present a tentative path to bridge this gap, by proposing a research agenda to advance resilience benchmarks for SDNs. We believe that with such class of tools and methodologies\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Introduction to Software Security Concepts\n", "abstract": " The main problem faced by system administrators nowadays is the protection of data against unauthorized access or corruption due to malicious actions. In fact, due to the impressive growth of the Internet, software security has become one vital concern in any information infrastructure. Unfortunately, software security is still commonly misunderstood. This chapter presents key concepts on security, also providing the basis for understanding existing challenges on developing and deploying secure software systems.", "num_citations": "2\n", "authors": ["345"]}
{"title": "Detecting Vulnerabilities in Service Oriented Architectures\n", "abstract": " The adoption of Service Oriented Architectures (SOAs) in a wide range of organizations, including business-critical systems, opens the door to new security challenges. Although the services used should be secure and reliable, they are often deployed with security bugs that can be maliciously exploited. The problem is that developers are frequently not specialized on security and the common time-to-market constraints limits an in depth test for vulnerabilities. Additionally, research and practice shows that the effectiveness of existing vulnerability detection tools is very poor. The goal of this work is to advance the state-of-the-art by investigating new techniques and tools to effectively detect vulnerabilities in SOAs in an automated manner. Instrumental in this work is to propose a benchmarking approach that allows assessing and comparing vulnerability detection tools, thus helping guiding tools development and\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Cost-effective data allocation in data warehouse striping\n", "abstract": " The Data Warehouse Striping (DWS) technique is a data partitioning approach especially designed for distributed data warehousing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost computers and each query is executed in parallel by all the computers, guarantying a nearly optimal speed up and scale up. Data loading in distributed data warehouses is typically a heavy process and brings the need for loading algorithms that conciliate a balanced distribution of data among nodes with an efficient data allocation. These are fundamental aspects to achieve low and uniform response times and, consequently, high performance during the execution of queries. This paper proposes a generic approach for the evaluation of data distribution algorithms and assesses several alternative algorithms in the context of DWS. The experimental results show that the effective loading of the nodes must consider complementary effects, minimizing the number of distinct keys of any large dimension in the fact tables in each node, as well as splitting correlated rows among the nodes.", "num_citations": "2\n", "authors": ["345"]}
{"title": "Selecting software packages for secure database installations\n", "abstract": " Security is one of the biggest concerns of database administrators. Most marketed software products announce a variety of features and mechanisms designed to improve security. However, that same variety largely complicates the process of selecting the adequate set of software products (i.e., a software package) for a given installation. In this paper we propose an approach that can be used to fairly compare alternative software packages regarding security capabilities in database environments. We focus specifically on the two main software systems required for a new installation: the Operating System and the Database Management System (DBMS). We carefully explain and discuss our method, which is based on the idea of evaluating the characteristics of software packages against a comprehensive list of security concerns that are universally accepted as vital to any database installation. We created an actual\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "A composed approach for automatic classification of web services robustness\n", "abstract": " Testing Web Services (WS) for robustness is a lengthy and arduous process. After testing a set of services, there is typically a very large quantity and variety of test results to be analyzed, which poses a challenge to the developer that has to manually process all results and identify the out-puts that indicate the presence of bugs in the code. Previous research indicates that well-known automatic classification algorithms can be used to automate this step. However, the applicability of such algorithms is also limited, as they are frequently unable to deal with the large diversity of outputs present in typical WS scenarios, thus producing incorrect results. In this paper we propose an approach that allows the automatic classification of the results of WS robustness tests. The technique integrates rule-based classification (including domain rules) and conventional machine-learning algorithms trained using generic data. The\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Towards benchmarking the trustworthiness of web applications code\n", "abstract": " Comparing the security of web applications is very hard and, although there are many proposals of security metrics in the literature, no consensual quantitative security metric has been proposed so far. In this paper we study the use of trust-based metrics as an alternative for benchmarking the security of web applications code. The approach consists of quantifying and exposing evidences that show that developers applied valuable best practices to prevent potential security vulnerabilities, thus improving the trustworthiness that can be justifiably put in the application. The idea is that the metrics should portray the relative level of trust users can put in an application regarding its ability to prevent attacks. To demonstrate the idea we conducted a preliminary experimental evaluation using two implementations of a complex Web Service. Although further research is needed, preliminary results suggest that trust-based\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Benchmarking Untrustworthiness in DBMS Configurations\n", "abstract": " Database Management Systems (DBMS) are usually immersed in a so complex environment that assessing the security impact of any particular configuration choice is an extremely hard task. DBMS configuration untrustworthiness can be defined as a measure of how much one should distrust a given configuration to be able to prevent the manifestation of the most common security threats as real attacks. In this paper we propose an approach to benchmark untrustworthiness in DBMS configurations. This benchmark allows database administrators to compare the trustworthiness of individual configuration choices from several perspectives and taking into account the threats that are meaningful for a particular environment. The paper discusses the characteristics of this type of tools and presents a preliminary untrustworthiness comparison of four real database installations (based on four different DBMS engines\u00a0\u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Correlating security vulnerabilities with software faults\n", "abstract": " Web applications are typically developed with hard time constraints and are often deployed with software bugs. One important type of bug is the one that creates security vulnerabilities. Knowing what kind of programming mistakes usually lead to security vulnerabilities can be an important tool to help in the detection and prevention of security flaws. In this paper we describe ongoing work on correlating software faults and security vulnerabilities like Cross Site Scripting (XSS) and SQL injection.", "num_citations": "2\n", "authors": ["345"]}
{"title": "Testes padronizados de confiabilidade para sistemas transaccionais\n", "abstract": " Testes padronizados de confiabilidade para sistemas transaccionais | Estudo Geral Skip navigation PT EN Sign on to: Home Communities & Collections Research Outputs Researchers Organizations Projects Explore by Research Outputs Researchers Organizations Projects About About the repository University of Coimbra OA Policy FCT Policy Open Access Useful links Help How to deposit CreativeCommons FAQ's Provas Acad\u00e9micas 1.Estudo Geral 2.Faculdade de Ci\u00eancias e Tecnologia 3.Departamento de Engenharia Inform\u00e1tica 4.FCTUC Eng.Inform\u00e1tica - Teses de Doutoramento ESTUDO GERAL Reposit\u00f3rio cient\u00edfico da UC Home Communities & Collections Research Outputs Researchers Organizations Projects Explore by Research Outputs Researchers Organizations Projects About About the repository University of Coimbra OA Policy FCT Policy Open Access Useful links Help How to deposit FAQ's use : \u2026", "num_citations": "2\n", "authors": ["345"]}
{"title": "Timely ACID Transactions in DBMS\n", "abstract": " Developing database applications with timeliness requirements is a difficult problem. During the execution of transactions, database applications (with timeliness requirements) have to deal with the possible occurrence of timing failures, when the operations specified in the transaction do not complete within the expected deadlines.", "num_citations": "2\n", "authors": ["345"]}
{"title": "ESFFI-a novel technique for the emulation of software faults\n", "abstract": " This paper presents and evaluates a methodology for the emulation of software faults in COTS components using software implemented fault injection (SWIFI) technology. ESFFI (Emulation of Software Faults by Fault Injection) leverages matured fault injection techniques, which have been used so far for the emulation of hardware faults, and adds new features that make possible the insertion of errors mimicking those caused by real software faults. The major advantage of ESFFI over other techniques that also emulate software faults (mutations, for instance) is making fault locations ubiquitous; every software module can be targeted no matter if it is a device driver running in operating kernel mode or a 3 rd party component whose source code is not available. Experimental results have shown that for specific fault classes, eg assignment and checking, the accuracy obtained by this technique is quite good. 1.", "num_citations": "2\n", "authors": ["345"]}
{"title": "Analysis of VM migration scheduling as moving target defense against insider attacks\n", "abstract": " As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges. Unfortunately, due to resource sharing, cloud computing platforms open the door for insider attacks, which consist of malicious actions from cloud authorized users (eg, clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users or the underlying provider environment. Virtual machine (VM) migration is a Moving Target Defense (MTD) technique to mitigate insider attacks effects, as it provides VMs positioning manageability. However, there is a clear demand for studies quantifying the security benefits of VM migration-based MTD considering different system architecture configurations. This paper tries to fill such a gap by presenting a Stochastic Reward Net model for the security evaluation of a VM migration-based MTD. The security metric of interest is the probability of attack success. We consider\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Security requirements and solutions for iot gateways: A comprehensive study\n", "abstract": " The need for improving the security level of Internet-of-Things (IoT) systems is growing. Users may refrain from using such systems if they realize that security measures are not in place. In this context, one of the most important IoT components has not received the necessary attention by the community: the gateway. IoT gateways play a central role in an IoT system as they solve heterogeneity issues. However, if compromised, gateways can be a source of security threats, and these threats become more relevant due to the gateway\u2019s central role. Gateways connect IoT devices and cloud services, and successful attacks on this component may exploit this fact. Using a well-known security requirements (SRs) engineering approach, this article evaluates and prioritizes SRs specifically for IoT gateways. The prioritization highlights a set of SRs that must be observed when evaluating and improving the gateway security\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "XSX: Lightweight Encryption for Data Warehousing Environments\n", "abstract": " The use of data security solutions like encryption significantly reduces database performance in data-intensive processing environments such as Data Warehouses. To overcome this issue this paper proposes a transparent lightweight encryption technique based on alternating sequences of eXclusive Or (XOR) and bit switching operations. The technique aims to achieve high performance while providing significant levels of security strength, attaining better security-performance tradeoffs than most commercial encryption solutions and those proposed by the research community. The experimental evaluation conducted using the TPC-DS benchmark shows that the proposed technique adds lower database performance overhead when comparing with the AES standard encryption algorithm and with other encryption solutions such as OPES, Salsa20, TEA and SES-DW.", "num_citations": "1\n", "authors": ["345"]}
{"title": "The 2016 IEEE Services Emerging Technology Track on Dependable and Secure Services (DSS 2016)\n", "abstract": " This emerging technology track focuses on key topics regarding dependability and security of software and services. Service-based systems are being used in business, safety, and mission-critical environments to achieve operational goals and possess special characteristics that bring in difficult challenges to the research and industry communities. Among these challenges, dependability and security have been widely identified as critical aspects that need to be addressed, especially when considering that many services are also nowadays being deployed on the web, used over unreliable networks, and potentially exposed to security threats. The goal of the Emerging Technology Track On Dependable and Secure Services is to bring together researchers and practitioners to present original research and industrial practice regarding techniques to improve the dependability and security of services. Services hold\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "An XML-Based Policy Model for Access Control in Web Applications\n", "abstract": " Organizational Information Systems (IS) collect, store, and manage personal and business data. Due to regulation laws and to protect the privacy of users, clients, and business partners, these data must be kept private. This paper proposes a model and a mechanism that allows defining access control policies based on the user profile, the time period, the mode and the location from where data can be accessed. The proposed policy model is simple enough to be used by a business manager, yet it has the flexibility to define complex restrictions. At runtime, a protection layer monitors data accesses and enforces existing policies. A prototype tool was implemented to run an experimental evaluation, which showed that the tool is able to enforce access control with minimal performance impact, while assuring scalability both in terms of the number of users and the number of policies.", "num_citations": "1\n", "authors": ["345"]}
{"title": "Adapting Test-Driven Development to Build Robust Web Services\n", "abstract": " Web services are increasingly being used in business critical environments as a mean to provide a service or integrate distinct software services. Research indicates that, in many cases, services are deployed with robustness issues (ie, displaying unexpected behaviors when in presence of invalid input conditions). Recently, Test-Driven Development (TDD) emerged as software development technique based on test cases that are defined before development, as a way to validate functionalities. However, programmers typically disregard the verification of limit conditions, such as the ones targeted by robustness testing. Moreover, in TDD, tests are created before developing the functionality, conflicting with the typical robustness testing approach. This chapter discusses the integration of robustness testing in TDD for improving the robustness of web services during development. The authors requested three\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "A Hypotension Surveillance and Prediction System for Critical Care\n", "abstract": " The sudden fall of blood pressure (hypotension) is a common and serious complication in medical care. In critical care patients, hypotension may induce severe or even lethal events. Moreover, recent studies report an increase of mortality in critical care patients. By predicting hypotension (HT) in advance, medical staff can take action to minimize its effects or even avoid its occurrence. Typically, most medical systems have focused on monitoring current patient status, rather than predicting a patient\u2019s future status. Therefore, predicting HT episodes in advance remains a challenge. In this chapter, the authors present a solution for continuous monitoring and prediction of HT episodes, using Heart Rate (HR) and mean Blood Pressure (BP) biosignals. They propose an architecture for a HT Predictor (HTP) Tool, presenting a set of applications and a real-time database capable of continuously storing, and real-time\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "DBMS application layer intrusion detection for data warehouses\n", "abstract": " Data Warehouses (DWs) are used for producing business knowledge and aiding decision support. Since they store the secrets of the business, securing their data is critical. To accomplish this, several Database Intrusion Detection Systems (DIDS) have been proposed. However, when using DIDS in DWs, most solutions produce either too many false-positives (i.e., false alarms) that must be verified or too many false-negatives (i.e., true intrusions that pass undetected). Moreover, many approaches detect intrusions a posteriori which, given the sensitivity of DW data, may result in irreparable cost. To the best of our knowledge, no DIDS specifically tailored for DWs has been proposed. This paper examines intrusion detection from a data warehousing perspective and the reasons why traditional database security methods are not sufficient to avoid intrusions. We define the specific requirements for a DW DIDS and\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Towards runtime V&V for service oriented architectures\n", "abstract": " The widespread use of SOAs and their specific char-acteristics raise new challenges for V&V practices. This paper presents some of these challenges and introduces Runtime V&V as a possible future solution.", "num_citations": "1\n", "authors": ["345"]}
{"title": "Using Data Masking for Balancing Security and Performance in Data Warehousing\n", "abstract": " Data Warehouses (DWs) are the core of sensitive business information, which makes them an appealing target. Encryption solutions are accepted as the best way to ensure strong security in data confidentiality while keeping high database performance. However, this work shows that they introduce massive storage space and performance overheads to a magnitude that makes them unfeasible for DWs. This work proposes a data masking technique for protecting sensitive business data in DWs which balances security strength with database performance, using a Formula based on the mathematical modular operator and simple arithmetic operations. The proposed solution provides apparent randomness in the generation and distribution of the masked values, while introducing small storage space and query execution time overheads. It also enables a false data injection method for misleading attackers and\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "M\u00e9todo \u00c1gil aplicado ao desenvolvimento de software confi\u00e1vel baseado em componentes\n", "abstract": " Os M\u00e9todos \u00c1geis, ou Desenvolvimento \u00c1gil de Software (DAS), tem se popularizado, na \u00faltima d\u00e9cada, por meio de m\u00e9todos como Extreme Programming (XP) e Scrum e isso fez com que fossem aplicadas no desenvolvimento de sistemas computacionais de diversos tamanhos, complexidades t\u00e9cnica e de dom\u00ednio, e de rigor quanto \u00e0 confiabilidade. Esse fato evidencia a necessidade de processos de desenvolvimento de software que sejam mais rigorosos e que possuam uma quantidade adequada de modelagem e documenta\u00e7\u00e3o, em especial no que concerne ao projeto arquitetural, com o objetivo de garantir maior qualidade no seu resultado final. A confiabilidade pode ser alcan\u00e7ada adicionando elementos de tratamento de exce\u00e7\u00f5es \u00e0s fases iniciais do processo de desenvolvimento e \u00e0 reutiliza\u00e7\u00e3o de componentes. O tratamento de exce\u00e7\u00f5es tem sido uma t\u00e9cnica muito utilizada na verifica\u00e7\u00e3o e na depura\u00e7\u00e3o de erros em sistemas de software. O MDCE+ \u00e9 um m\u00e9todo que auxilia a modelagem do comportamento excepcional de sistemas baseados em componentes que, por ser centrado na arquitetura, melhora a defini\u00e7\u00e3o e a an\u00e1lise do fluxo de exce\u00e7\u00f5es entre os componentes do sistema. Este trabalho prop\u00f5e uma solu\u00e7\u00e3o para guiar o desenvolvimento de sistemas confi\u00e1veis baseados em componentes por meio da adi\u00e7\u00e3o de pr\u00e1ticas do MDCE+ ao Scrum, resultando no m\u00e9todo Scrum+CE (Scrum com Comportamento Excepcional). Esse processo passa a expor os requisitos excepcionais em n\u00edvel das Est\u00f3rias de Usu\u00e1rio, adiciona testes de aceita\u00e7\u00e3o mais detalhados, obriga a cria\u00e7\u00e3o do artefato de Arquitetura Inicial e adiciona\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Incorporating Recovery from Failures into a Data Integration Benchmark\n", "abstract": " The proposed TPC-DI benchmark measures the performance of Data Integration systems (a.k.a. ETL systems) given the task of integrating data from an OLTP system and other data sources to create a data warehouse.This paper describes the scenario, structure and timing principles used in TPC-DI. Although failure recovery is very important in real deployments of Data Integration systems, certain complexities made it difficult to specify in the benchmark. Hence failure recovery aspects have been scoped out of the current version of TPC-DI. The issues around failure recovery are discussed in detail and some options are described. Finally the audience is invited to offer additional suggestions.", "num_citations": "1\n", "authors": ["345"]}
{"title": "From Performance to Resilience Benchmarking\n", "abstract": " In this paper we discuss the resilience benchmarking concept. First we present a brief historical overview on benchmarking. Then we discuss the key components of a resilience benchmarking. Finally, we propose a research path for tackling the resilience benchmarking challenge.", "num_citations": "1\n", "authors": ["345"]}
{"title": "BIRF: Keeping Software Development under Control across the Organization\n", "abstract": " Many organizations have to manage an increasingly large number of software projects. In many cases, these projects are outsourced to different companies or developed across several departments. This creates a problem because it is increasingly difficult for a project manager, responsible for several projects, to have an accurate view of all activities underway, being able to act proactively before problems occur. In this paper we present an approach and corresponding implementation for effectively tracking and managing multiple projects across an organization, offering an integrated view of the state of projects being implemented. This approach allows to effectively monitoring risks, progress, budget, deliveries, and other critical aspects, allowing responding in real-time. The system was implemented for the European Space Agency, being now successfully used in a number of projects. This paper also presents\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Appraisals Based on Security Best Practices for Software Configurations\n", "abstract": " Protecting systems and data from malicious access and corruption requires the existence of effective security mechanisms and the correct configuration of those mechanisms. Configuring large software systems for security is a complex task, entailing a lot of expertise that many administrators do not have. This paper proposes a generic methodology to condense widespread information about security best practices into easy-to-use appraisals for three scenarios: 1) to assess how effective software configurations are in terms of fulfilling best practices; 2) to understand the set of best practices that can be implemented when using a given software product; and 3) to evaluate how well a system administrator knows existing security best practices. Following this methodology we defined an appraisal for database systems configurations, which was used to evaluate four real installations. Experimental results show the\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Timing Failures Detection in Web Services\n", "abstract": " Current business critical environments increasingly rely on SOA standards to execute business operations. These operations are frequently based on Web service compositions that use several Web services over the internet and have to fulfill specific timing constraints. In these environments, an operation that does not conclude in due time may have a high cost as it can easily turn into service abandonment with financial and prestige losses to the service provider. In fact, at certain points, carrying on with the execution of an operation may be useless as a timely response will be impossible to obtain. This paper proposes a time-aware programming model for Web services that provides transparent timing failure detection. The paper illustrates the proposed model using a set of services specified by the TPC-App performance benchmark.", "num_citations": "1\n", "authors": ["345"]}
{"title": "Redundant array of inexpensive nodes for DWS\n", "abstract": " The DWS (Data Warehouse Striping) technique is a round-robin data partitioning approach especially designed for distributed data warehousing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost computers and the queries are executed in parallel by all the computers, guarantying a nearly optimal speed up and scale up. However, the use of a large number of inexpensive nodes increases the risk of having node failures that impair the computation of queries. This paper proposes an approach that provides Data Warehouse Striping with the capability of answering to queries even in the presence of node failures. This approach is based on the selective replication of data over the cluster nodes, which guarantees full availability when one or more nodes fail. The proposal was evaluated using the newly TPC-DS benchmark and the results show that the approach is quite\u00a0\u2026", "num_citations": "1\n", "authors": ["345"]}
{"title": "Using experimental measurements to assess dependable adaptation support mechanisms for timed transactions\n", "abstract": " Recently, we proposed a framework that allows to analyze stochastic data, namely end-to-end measurements of distributed interactions, and hence characterize the temporal probabilistic behavior of underlying communication or transactional services, to gather fundamental information for adaptation and dependability purposes. In particular, we argue that this framework can be used to provide support for some classes of timed transactions and it allows to satisfy important dependability objectives. This paper studies the framework in the scope of transactional environments, assessing the applicability of the proposed techniques for monitoring stochastic data, which is observed in these environments. We apply the framework using several sets of experimental field data that was collected with different kinds of transactions and in different load conditions. We compare the results with a baseline and conservative probabilistic approach and we show that the framework presents some benefits and may indeed be a useful tool to achieve more dependable timed transactions.", "num_citations": "1\n", "authors": ["345"]}
{"title": "A Survey on Data Security in Data Warehousing\n", "abstract": " Data Warehouses (DWs) are the enterprise\u2019s most valuable assets in what concerns critical business information, making them an appealing target for malicious inside and outside attackers. Given the volume of data and the nature of DW queries, most of the existing data security solutions for databases are inefficient, consuming too many resources and introducing too much overhead in query response time, or resulting in too many false positive alarms (ie, incorrect detection of attacks) to be checked. In this paper, we present a survey on currently available data security techniques, focusing on specific issues and requirements concerning their use in data warehousing environments. We also point out challenges and opportunities for future research work in this field.", "num_citations": "1\n", "authors": ["345"]}