{"title": "A systematic review of studies on use case points and expert\u2010based estimation of software development effort\n", "abstract": " In recent years, due to significant evolution in adopting new technologies and development methodologies in the field of software engineering, the developers and researchers are striving to optimize the accuracy of software effort estimation (SEE). The overestimation and underestimation both are the key challenges for software progress; hence, there is a continuous need for an accurate SEE. This paper highlights a systematic review of studies associated with the best practices of use case point (UCP) and expert judgment\u2013based software development effort estimation techniques. The primary aim and contribution of this paper are to support the researchers through an extensive review to ease to other researcher's search for effort estimation studies. We have performed state\u2010of\u2010the\u2010art review from five viewpoints of reference: (a) review of studies concerning UCPs and expert judgment\u2013based effort estimation, (b\u00a0\u2026", "num_citations": "11\n", "authors": ["2262"]}
{"title": "A trust model using edge nodes and a cuckoo filter for securing VANET under the NLoS condition\n", "abstract": " Trust, as a key element of security, has a vital role in securing vehicular ad-hoc networks (VANETs). Malicious and selfish nodes by generating inaccurate information, have undesirable impacts on the trustworthiness of the VANET environment. Obstacles also have a negative impact on data trustworthiness by restricting direct communication between nodes. In this study, a trust model based on plausibility, experience, and type of vehicle is presented to cope with inaccurate, incomplete and uncertainty data under both line of sight (LoS) and none-line of sight (NLoS) conditions. In addition, a model using the k-nearest neighbor (kNN) classification algorithm based on feature similarity and symmetry is developed to detect the NLoS condition. Radio signal strength indicator (RSSI), packet reception rate (PDR) and the distance between two vehicle nodes are the features used in the proposed kNN algorithm. Moreover, due to the big data generated in VANET, secure communication between vehicle and edge node is designed using the Cuckoo filter. All obtained results are validated through well-known evaluation measures such as precision, recall, overall accuracy, and communication overhead. The results indicate that the proposed trust model has a better performance as compared to the attack-resistant trust management (ART) scheme and weighted voting (WV) approach. Additionally, the proposed trust model outperforms both ART and WV approaches under different patterns of attack such as a simple attack, opinion tampering attack, and cunning attack. Monte-Carlo simulation results also prove validity of the proposed trust model. View Full-Text", "num_citations": "10\n", "authors": ["2262"]}
{"title": "An IoT based home automation integrated approach: impact on society in sustainable development perspective\n", "abstract": " In recent years, due to substantial evolution in the field of consumer electronics, the society is striving to optimize efficiency, energy savings, green technology and environmental sustainability in their daily lives at homes. Most of the people are controlling and monitoring home appliances manually and therefore, facing lots of problems in managing natural resources, cost, effort and security which lead towards an un-comfortable and un-reliable life. Numerous \u2018intelligent\u2019devices such as smartphones, tablets, air-conditioners, etc. have promoted the key concept of the Internet of Things (IoT) based home automation. Entrenched with technology, these devices can be distantly monitored and controlled over the Internet at home and anywhere in the world. Over the past few decades, global warming has become a severe worldwide challenge. However, sustainable development and green technology play an important role in climate change. The primary purpose of this study is to save natural resources, reduce energy consumption, and to understand the impact of home automation on the society in order to achieve the goal of green technology and environmental sustainability. In this paper, IoT based home automation approach integrated with the smart meter, solar, wind, geothermal renewable energy resources and government green awareness program to extensively optimize the need of energy consumption, security, cost, convenience and cleaner environment for the society is proposed. In addition, a survey was conducted among the target audience for the purpose of identifying and evaluating its least impact on the environment and society in a\u00a0\u2026", "num_citations": "6\n", "authors": ["2262"]}
{"title": "Course recommendation system using fuzzy logic approach\n", "abstract": " Course selection is a key for success in student\u2019s academic path. In today\u2019s education environment, various courses offered by different academic institutions required the students to explore the course outline manually. Most of them are lacking in knowledge, having dilemma and making blind selections to choose the right course. Therefore, it is essential to have a course recommendation to provide guidance to a student to choose the course related with their interest and skill. This paper proposed to develop a course recommendation system using fuzzy logic approach. The development methodology of this system involves several phases include requirements planning, user design and construction for prototyping, testing and cutover. This study used the fuzzy rules technique in order to calculate each associated student\u2019s skill and interest level based on Mamdani fuzzy inference system method. Then, the rules will generate final outcome which recommend the suitable course path and provide the details to a user based on their course test. The result shows the functionality of this system has been achieved and works well. This study is significantly helping the students to choose their course based on the interest and skill.", "num_citations": "5\n", "authors": ["2262"]}
{"title": "Data quality issues in big data: a review\n", "abstract": " Data with good quality has precedence when analyzing and using big data to deduce value from such tremendous volume of data in today\u2019s business environments. Decisions and insights derived from poor data has a negative and unpredictable consequences to organizations. At present, due to the lack of comprehensive and intensive research in the field of data quality, especially large data, there is an urgent need to address this issue by researchers to reach the optimal way to estimate and evaluate the quality of large data. Thus, enabling institutions to make rational decisions based on evaluation outputs. In this paper, the current research on the quality of large data was reviewed and summarized by exploring the basic characteristics of large data. The main challenges facing the quality of information were also discussed in the context of large data. Some of the initiatives suggested by the researchers\u00a0\u2026", "num_citations": "5\n", "authors": ["2262"]}
{"title": "Master data definition and the privacy classification in government agencies: Case studies of local government\n", "abstract": " The Master Data Management (MDM) empowers government agencies to consolidate and integrate multiple master data sources to a single source of truth. With MDM, the master data from different agencies that are valuable across agencies, applications and services will be identified and managed in a central repository as a high quality enterprise master data. However, the MDM establishment usually hindered by a data policy where most of the government agencies are reluctant to share their master data due to widespread privacy concerns. Hence, this study aims to define master data and its privacy classification in each government agencies by using a qualitative and quantitative data analysis approach. It involves participative case studies from seven (7) Malaysia\u2019s local government agencies. The study identifies 36 sets of master data which generally grouped into three domains which are; (1) customers\u00a0\u2026", "num_citations": "5\n", "authors": ["2262"]}
{"title": "Test management traceability model to support software testing documentation\n", "abstract": " Software Documentation is one of the key quality factors in software development. However, many developers are still putting less effort and less priority on documentation. To them writing documentation during project development is very tedious and time consuming. As a result, the documentation tends to be significantly out-dated, poor quality and difficult to access that will certainly lead to poor software maintenance. Current studies have proved that the key point to this problem is software traceability. Traceability relates to an ability to trace all related software components within a software system that includes requirements, test cases, test results and other artefacts. This research reveals some issues related to current software traceability and attempts to suggest a new software traceability model that focuses on software test documentation for test management. This effort leads to a new software test\u00a0\u2026", "num_citations": "5\n", "authors": ["2262"]}
{"title": "Enhancing Performance Aspect in Usability Guidelines for Mobile Web Application\n", "abstract": " The rapid growth of technology has boosted the usage of mobile devices to access a web application. These devices have various specifications such as screen sizes and resolutions. Responsive Web Design (RWD) approach bridges the gap of these differences by adapting the flexibility concept in screen sizes and resolutions, thus ensuring better website usability. Poor performance in website usability resulting in declination in number of users. Therefore several existing Usability Guidelines (UG) were invented mainly for a desktop-based website and unfortunately lesser studies have been carried out for a Mobile Web Application (MWA) especially the performance aspect of it. Henceforth, this paper focuses on enhancing the performance element of an existing UG by analysing, identifying, proposing, implementing, and measuring the new additional performance attributes in UG for MWA. We used First\u00a0\u2026", "num_citations": "4\n", "authors": ["2262"]}
{"title": "Risk management framework for distributed software team: A case study of telecommunication company\n", "abstract": " Distributed software development (DSD) has grown rapidly over the last few years and present definite risk to the software industry which need to be carefully analyzed and managed. The risks relating to DSD include lack of trust, ineffective communication, time zone difference, cultural differences such as language and corporate culture along with knowledge sharing challenges. Software risk management approach in DSD, however, is still inadequate and needed further attention. The aim of this paper is to identify the components involved in risk management process related to DSD, and finally to enhance existing risk management framework used in the organization to accommodate the distributed nature of the team. The quantitative approach which is survey method has been chosen as an appropriate research method to achieve the objectives of this paper. The results show that communication is the most\u00a0\u2026", "num_citations": "4\n", "authors": ["2262"]}
{"title": "CO-DEPENDENCE RELATIONSHIP BETWEEN MASTER DATA MANAGEMENT AND DATA QUALITY: A REVIEW.\n", "abstract": " Abstract Master Data Management refers to the consolidation, integration and standardization of master data from multiple data sources into a centralized system to support data quality improvement in an organization. Nevertheless, while Master Data Management came into prominence in the information systems field of study, there is a lack of review papers for this topic have been published. Hence, this paper reports the results of a systematic literature review on the Master Data Management research topic. It aims to summarize the research progress of Master Data Management since 2000 to July 2016 and to review the association of Master Data Management and Data Quality. Search strategies with relevant keywords were used to identify literature from seven prestigious academic databases, namely 1) ACM Digital Library; 2) Emerald; 3) IEEE; 4) Science Direct; 5) Scopus; 6) Springer Link; 7) Web of Science\u00a0\u2026", "num_citations": "4\n", "authors": ["2262"]}
{"title": "Improving estimation accuracy prediction of software development effort: A proposed ensemble model\n", "abstract": " Software effort estimation is an essential feature of software engineering for effective planning, controlling and delivering successful software projects. The overestimation and underestimation both are the key challenges for future software development. The failure to acknowledge the effort estimation accuracy may lead to customer disappointment, inaccurate estimation and hence, contribute to either poor software development process or project failure. The main aim of this research is to optimize the estimation accuracy prediction of software development effort to support software development firms and practitioners. In this paper, we propose an ensemble software effort estimation model based on Use Case Points (UCP), expert judgment and Case-Based Reasoning (CBR) techniques. This research is conducted through primary (a multi-case involving software companies) study to make an ensemble model. The\u00a0\u2026", "num_citations": "3\n", "authors": ["2262"]}
{"title": "Analysing Log Files For Web Intrusion Investigation Using Hadoop\n", "abstract": " The process of analyzing large amount of data from the log file helps organization to identify the web intruders' activities as well as the vulnerabilities of the website. However, analyzing them is totally a great challenge as the process is time consuming and sometimes can be inefficient. Existing or traditional log analyzers may not able to analyze such big chunk of data. Therefore, the aim of this research is to produce an analysis result for web intrusion investigation in Big Data environment. In this study, web log was analyzed based on attacks that are captured through web server log files. The web log was cleaned and refined through a log-preprocessing program before it was analyzed. An experimental simulation was conducted using Hadoop framework to produce the required analysis results. The results of this experimental simulation indicate that Hadoop application is able to produce analysis results from large\u00a0\u2026", "num_citations": "3\n", "authors": ["2262"]}
{"title": "Enhancement Data Integrity Checking Using Combination MD5 and SHA1 Algorithm in Hadoop Architecture\n", "abstract": " The use of Big Data in decision-making is critical, in line with the growing size of data storage, either online or offline. However, there are only a few software applications that are capable to process large-capacity data such as Hadoop. Hadoop is open-source software for Big Data processing including several components joined together where one of its main components is Hadoop User Experience (Hue). Hue is being used to upload the data into Hadoop databases using Graphical User Interface (GUI). However, Hue is not equipped with a function to evaluate whether the downloaded data has changed or not, resulting in the processing of incorrect data that leads to false decisions. Therefore, this study aims to improve the functions available in Hue using MD5 and SHA1 cryptographic functions for data verification purposes. These cryptographic functions have been chosen due to their acceptance worldwide and with the added functionality of data verification in Hue, data validation can be performed during uploading process to prevent users from processing erroneous data. The result of this study will ensure the integrity of the data by validation in any means of changes of data before being stored to the Hadoop in offline mode.", "num_citations": "3\n", "authors": ["2262"]}
{"title": "Information security management system implementation success factors: a review\n", "abstract": " Information Security Management System (ISMS) is an information security compliance standards implemented in organization to provide specifications and controls for protecting information assets and to increase the integrity and confidence of clients over the respective organization. The cognizance in preserving and securing the critical information has led many organization to adopt ISMS. The objective of this study is to review and identify success factors that could influence the ISMS self-implementation in organizations. Based on twenty most relevant and recent studies, ten factors are extracted which can be considered important in the ISMS implementation. In particular, staff awareness and training and top management support are found to be the most crucial factors in determining the successful implementation of ISMS and the rating of the importance for both factors are almost equal. All factors identified can\u00a0\u2026", "num_citations": "3\n", "authors": ["2262"]}
{"title": "Formulating a Software Traceability Model for IntegratedTest Documentation: a Case Study\n", "abstract": " Software Documentation is a key of quality factor in software development but many among developers failed to produce document during their project. The purpose of documentation is to aid the developers in understanding the project. Unfortunately, the documentation is out dated, poor quality, difficult to access and therefore cannot be trusted. The key point of the problems is software traceability. This research is trying to develop a model of software traceability that can generate a software testing documentation to support test management. This will led to new software testing documentation generation process model based on software engineering standards. Information retrieval technique will be utilized to trace link between software artefacts.", "num_citations": "3\n", "authors": ["2262"]}
{"title": "An Efficient and Secure Session Key Management Scheme in Wireless Sensor Network\n", "abstract": " Wireless Sensor Network (WSN) is a particular network built from small sensor nodes. These sensor nodes have unique features. That is, it can sense and process data in WSN. WSN has tremendous applications in many fields. Despite the significance of WSN, this kind of network faced several issues. The biggest problems rising in WSN are energy consumption and security. Robust security development is needed to cope with WSN applications. For security purposes in WSN, cryptography techniques are very favorable. However, WSN has resource limitations, which is the main problem in applying any security scheme. Hence, if we are using the cryptography scheme in WSN, we must first guarantee that it must be energy-efficient. Thus, we proposed a secure hybrid session key management scheme for WSN. In this scheme, the major steps of public key cryptography are minimized, and much of the operations are based on symmetric key cryptography. This strategy extensively reduces the energy consumption of WSN and ensures optimum security. The proposed scheme is implemented, and their analysis is performed using different parameters with benchmark schemes. We concluded that the proposed scheme is energy-efficient and outperforms the available benchmark schemes. Furthermore, it provides an effective platform for secure key agreements and management in the WSN environment.", "num_citations": "2\n", "authors": ["2262"]}
{"title": "Software effort estimation accuracy prediction of machine learning techniques: A systematic performance evaluation\n", "abstract": " Software effort estimation accuracy is a key factor in effective planning, controlling, and delivering a successful software project within budget and schedule. The overestimation and underestimation both are the key challenges for future software development, henceforth there is a continuous need for accuracy in software effort estimation. The researchers and practitioners are striving to identify which machine learning estimation technique gives more accurate results based on evaluation measures, datasets and other relevant attributes. The authors of related research are generally not aware of previously published results of machine learning effort estimation techniques. The main aim of this study is to assist the researchers to know which machine learning technique yields the promising effort estimation accuracy prediction in software development. In this article, the performance of the machine learning ensemble\u00a0\u2026", "num_citations": "2\n", "authors": ["2262"]}
{"title": "Recent Analysis of Forged Request Headers Constituted by HTTP DDoS\n", "abstract": " Application Layer Distributed Denial of Service (DDoS) attacks are very challenging to detect. The shortfall at the application layer allows formation of HTTP DDoS as the request headers are not compulsory to be attached in an HTTP request. Furthermore, the header is editable, thus providing an attacker with the advantage to execute HTTP DDoS as it contains almost similar request header that can emulate a genuine client request. To the best of the authors\u2019 knowledge, there are no recent studies that provide forged request headers pattern with the execution of the current HTTP DDoS attack scripts. Besides that, the current dataset for HTTP DDoS is not publicly available which leads to complexity for researchers to disclose false headers, causing them to rely on old dataset rather than more current attack patterns. Hence, this study conducted an analysis to disclose forged request headers patterns created by HTTP DDoS. The results of this study successfully disclose eight forged request headers patterns constituted by HTTP DDoS. The analysis was executed by using actual machines and eight real attack scripts which are capable of overwhelming a web server in a minimal duration. The request headers patterns were explained supported by a critical analysis to provide the outcome of this paper. View Full-Text", "num_citations": "2\n", "authors": ["2262"]}
{"title": "Customer churn prediction in telecommunication industry using machine learning classifiers\n", "abstract": " Customer churn is one of the main problems in telecommunication industry. This study aims to identify the factors that influence customer churn and develop an effective churn prediction model as well as provide best analysis of data visualization results. The dataset has been collected from Kaggle open data website. The proposed methodology for analysis of churn prediction covers several phases: data pre-processing, analysis, implementing machine learning algorithms, evaluation of the classifiers and choose the best one for prediction. Data preprocessing process involved three major action, which are data cleaning, data transformation and feature selection. Machine learning classifiers was chosen are Logistic Regression, Artificial Neural Network and Random Forest. Then, classifiers were evaluated by using performance measurement which are accuracy, precision, recall and error rate in order to find the\u00a0\u2026", "num_citations": "2\n", "authors": ["2262"]}
{"title": "Master data identification in public sector organisations\n", "abstract": " The Master Data Management (MDM) establishment in public sector improves the government administration efficiency by encouraging the resource sharing, data collaboration and citizen-centeredness. Nevertheless, defining master data in public sector organisations during initial activity in establishing the MDM is a very challenging process. Organisations had difficulties in identifying the master data in their organisations due to the unclear definition of master data in the public sector. Furthermore, less attention is given in the existing literatures in the MDM area for public sector. Hence, this study aims to identify and categorise the master data in public sector organisations by using a qualitative approach. It involves participative case studies from nine Malaysia\u2019s public sector organisations and experts\u2019 opinion to determine and categorise the master data in Malaysia\u2019s public sector organisations. The result of this\u00a0\u2026", "num_citations": "2\n", "authors": ["2262"]}
{"title": "Specification of a Hybrid Effort Estimation System using UML\n", "abstract": " Software effort estimation (SEE) predicts the amount of effort needed to develop software and this information is important for a project manager to make good decisions when confronted with requirement change requests during the development. However, SEE can predict accurate development effort only if the project data is complete and stable. In real project, this is not the case and becomes a very challenging issue. Inaccurate effort estimation will result in lots of difficulties in managing a software project, hence the project's failure rate increases. Both algorithmic and nonalgorithmic methods in SEE have their respective advantages and limitation. This paper presents a hybrid approach combining algorithmic and non-algorithmic SEE methods to propose a more accurate decision to the project manager when a requirement change is requested by a stakeholder. This hybrid model is translated into an effort\u00a0\u2026", "num_citations": "1\n", "authors": ["2262"]}
{"title": "Development of Agronomist Station System for Water Table Management at Peatland\n", "abstract": " The palm oil sector has been criticized globally for being the source of diverse negative impacts on the surrounding environment due to the increasing rate of land development in the peatland. Scientific evidence shows that the peatland becomes less tolerant of subsidence once the existing water is over drained for agriculture development. To overcome this problem, planters had established a water gate system in the vegetation area so that the distribution of water is possible to recover an area with a low water table. Commonly, a piezometer is deployed to evaluate the water table at the problematic zone. Unfortunately, this approach has limitations in which the manual reading assessment is prone to human error and requires a lot of time and effort. Therefore, the Agronomist Station (AGROST) is developed to counter these drawbacks. The purpose of this study is to improve water table management in the\u00a0\u2026", "num_citations": "1\n", "authors": ["2262"]}
{"title": "Experiences in Software Development Model Selection and MVC Design Pattern Implementation for Kuala Lumpur City Hall Traffic Offence Management System\n", "abstract": " Traffic Offence Management system is a crucial system in facilitating information about traffic offenders. As highlighted by Kuala Lumpur City Hall (DBKL), repeated traffic offenders are exist due to semi-auto and manual management of information. Moreover the data is decentralised and scattered, resulting difficult for cross referencing and data retrieval on traffic offenders record. The existing system is less transparent and plausible to breed unfortunate circumstances such as mismanagement at both ends between officers and offenders. Towards a new development of Traffic Offence Management System, this paper findings focus on selecting software development model as the Software Development Life-Cycle (SDLC) and implementation of Model-View Controller (MVC) design pattern for a new enhanced system which will be highlighted through comparative study.", "num_citations": "1\n", "authors": ["2262"]}
{"title": "External attacks on automotive system through wireless communication channels\n", "abstract": " The reliance of today\u2019s automotive system on electronics control system is expected to make the cars to be state-of-the-art vehicle. However, this technology dependency results in the cars to be exposed to attacks by the hacker through the manipulation of electronics system. Previously, for the attacker to compromise car\u2019s system, he/she must access the car directly and internally. However, with the incorporation of wireless technologies such as Bluetooth and cellular into automotive system for example in its telematic units, the attacks are evolved from internal attacks into remote attack where the adversary does not have to internally access the car\u2019s system. This paper analyses the vulnerabilities of the automotive system by the remote attacks performed through Bluetooth and cellular. Once the vulnerabilities were analyzed, the threats imposed by these vulnerabilities are accessed. Two scenarios namely theft and surveillance are used to exemplify the threats that are carried by the vulnerability of the automotive system to the remote attacks. From the vulnerability analysis and threat assessment, it can be deduced that the automotive system is vulnerable to attacks and proper countermeasure must be taken to curb the implication from the attacks.", "num_citations": "1\n", "authors": ["2262"]}
{"title": "Malware log files for Internet investigation using hadoop: A review\n", "abstract": " On the Internet, malware is one of the most serious threats to system security. Major complex issues and problems on some software systems are oftentimes made by malware. Malware can infect any computer software that causes connection to Internet infrastructure. There are many types of malware and some of the popular malware are Botnet, Trojans, Viruses, Spyware and Adware. Internet users with lesser knowledge of the malware threats are susceptible to this issue. To protect and prevent the computer and internet users from exposing themselves towards malware attacks, identifying the attacks through investigating malware log file is an essential step to curb this threat. The log file exposes crucial information in identifying the malware, such as algorithm and functional characteristic, the network interaction between the source and the destination, and type of malware. By nature, the log file size is humongous\u00a0\u2026", "num_citations": "1\n", "authors": ["2262"]}
{"title": "A study of authentication protocols for security of mobile RFID (M-RFID) system\n", "abstract": " Radio-frequency identification (RFID) uses electromagnetic fields for wireless communication that automatically recognized target devices with no physical interaction with the purpose of information transmission. The RFID system composed of transponder or tag, interrogator and back-end host system. The services of mobile RFID (M-RFID) networks can be setup by consolidating with current wireless networking for mobile devices and RFID networks whereby these services are aiming for individual and private users using Mobile smart phone or PDA. Even though these systems have the benefits of RFID and mobile technology, it also promotes crucial security and privacy issues. M-RFID inherits all issues that exist in the traditional RFID system, for example, replay attack, denial of service, spoofing, traceability and impersonation. However the severity of these issues intensifies because M-RFID reader is used in the\u00a0\u2026", "num_citations": "1\n", "authors": ["2262"]}
{"title": "A Document-based Traceability Model: An Evaluation by Using Feature Analysis\n", "abstract": " Software traceability is one of the key quality factors in software development. However, many developers are still not putting enough effort and priority on traceability. In order to verify and validate the completeness and consistency with the requirement, traceability is crucial. This study aims to evaluate the developed traceability model and to compare it with other traceability model with respect to their abilities to support the test management. For this evaluation, the DESMET method will be used, and a feature analysis of the models will be conducted. The evaluation shows that each of the models has some strengths and some weaknesses. Based on the evaluation, it was found that the proposed model meets its expectations to support the test management and its traceability has achieved some promising output and remarkable understanding compared to existing models.", "num_citations": "1\n", "authors": ["2262"]}