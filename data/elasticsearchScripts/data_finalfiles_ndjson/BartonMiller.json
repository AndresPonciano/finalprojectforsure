{"title": "An empirical study of the reliability of UNIX utilities\n", "abstract": " The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.", "num_citations": "1356\n", "authors": ["1027"]}
{"title": "The Paradyn parallel performance measurement tool\n", "abstract": " Paradyn is a tool for measuring the performance of large-scale parallel programs. Our goal in designing a new performance tool was to provide detailed, flexible performance information without incurring the space (and time) overhead typically associated with trace-based tools. Paradyn achieves this goal by dynamically instrumenting the application and automatically controlling this instrumentation in search of performance problems. Dynamic instrumentation lets us defer insertion until the moment it is needed (and remove it when it is no longer needed); Paradyn's Performance Consultant decides when and where to insert instrumentation.< >", "num_citations": "1161\n", "authors": ["1027"]}
{"title": "What are race conditions? Some issues and formalizations\n", "abstract": " In shared-memory parallel programs that use explicit synchronization, race conditions result when accesses to shared memory are not properly synchronized. Race conditions are often considered to be manifestations of bugs, since their presence can cause the program to behave unexpectedly.  Unfortunately, there has been little agreement in the literature as to precisely what constitutes a race condition. Two different notions have been implicitly considered: one pertaining to programs intended to be deterministic (which we call general races) and the other to nondeterministic programs containing critical sections (which we call data races). However, the differences between general races and data races have not yet been recognized. This    paper examines these differences by characterizing races using a formal model and exploring their properties. We show that two variations of each type of race exist: feasible\u00a0\u2026", "num_citations": "642\n", "authors": ["1027"]}
{"title": "An empirical study of the robustness of macos applications using random testing\n", "abstract": " We report on the fourth in a series of studies on the reliability of application programs in the face of random input. Over the previous 15 years, we have studied the reliability of UNIX command line and X-Window based (GUI) applications and Windows applications. In this study, we apply our fuzz testing techniques to applications running on the Mac OS X operating system. We continue to use a simple, or even simplistic technique: unstructured black-box random testing, considering a failure to be a crash or hang. As in the previous three studies, the technique is crude but seems to be effective in locating bugs in real programs. We tested the reliability of 135 command-line UNIX utilities and thirty graphical applications on Mac OS X by feeding random input to each. We report on application failures--crashes (dumps core) or hangs (loops indefinitely)--and, where source code is available, we identify the causes of these\u00a0\u2026", "num_citations": "465\n", "authors": ["1027"]}
{"title": "Fuzz revisited: A re-examination of the reliability of UNIX utilities and services\n", "abstract": " We have tested the reliability of a large collection of basic UNIX utility programs, X-Window applications and servers, and network services. We used a simple testing method of subjecting these programs to a random input stream. Our testing methods and tools are largely automatic and simple to use. We tested programs on nine versions of the UNIX operating system, including seven commercial systems and the freely-available GNU utilities and Linux. We report which programs failed on which systems, and identify and categorize the causes of these failures.The result of our testing is that we can crash (with core dump) or hang (infinite loop) over 40%(in the worst case) of the basic programs and over 40% of the X-Window applications. We were not able to crash any of the network services that we tested nor any of X-Window servers. This study parallels our 1990 study (that tested only the basic UNIX utilities); all\u00a0\u2026", "num_citations": "429\n", "authors": ["1027"]}
{"title": "Optimal tracing and replay for debugging message-passing parallel programs\n", "abstract": " A common debugging strategy involves reexecuting a program (on a given input) over and over, each time gaining more information about bugs. Such techniques can fail on message-passing parallel programs. Because of nondeterminacy, different runs on the given input may produce different results. This nonrepeatability is a serious debugging problem, since an execution cannot always be reproduced to track down bugs. This paper presents a technique for tracing and replaying message-passing programs. By tracing the order in which messages are delivered, a reexecution can be forced to deliver messages in their original order, reproducing the original execution. To reduce the overhead of such a scheme, we show that the delivery'order of only messages involved inraces need be traced (and not every message). Our technique makes run-time decisions to detect and trace racing messages and is\u00a0\u2026", "num_citations": "290\n", "authors": ["1027"]}
{"title": "IPS-2: The second generation of a parallel program measurement system\n", "abstract": " IPS is a performance measurement system for parallel and distributed programs. IPS's model of parallel programs uses knowledge about the semantics of a program's structure to provide two important features. First, IPS provides a large amount of performance data about the execution of a parallel program, and this information is organized so that access to it is easy and intuitive. Second, DPS provides performance analysis techniques that help to automatically guide the programmer to the location of program bottlenecks.IPS is currently running on its second implementation. The first implementation was a testbed for the basic design concepts, providing experience with a hierarchical program and measurement model, interactive program analysis, and automatic guidance techniques. This implementation was built on the Charlotte Distributed Operating System. The second implementation, IPS-2, extends the basic\u00a0\u2026", "num_citations": "289\n", "authors": ["1027"]}
{"title": "MRNet: A software-based multicast/reduction network for scalable tools\n", "abstract": " We present MRNet, a software-based multicast/reduction network for building scalable performance and system administration tools. MRNet supports multiple simultaneous, asynchronous collective communication operations. MRNet is flexible, allowing tool builders to tailor its process network topology to suit their tool's requirements and the underlying system's capabilities. MRNet is extensible, allowing tool builders to incorporate custom data reductions to augment its collection of built-in reductions. We evaluated MRNet in a simple test tool and also integrated into an existing, real-world performance tool with up to 512 tool back-ends. In the real-world tool, we used MRNet not only for multicast and simple data reductions but also with custom histogram and clock skew detection reductions. In our experiments, the MRNet-based tools showed significantly better performance than the tools without MRNet for average\u00a0\u2026", "num_citations": "262\n", "authors": ["1027"]}
{"title": "Fine-grained dynamic instrumentation of commodity operating system kernels\n", "abstract": " We have developed a technology, fine-grained dynamic instrumentation of commodity kernels, which can splice (insert) dynamically generated code before almost any machine code instruction of a completely unmodified running commodity operating system kernel. This technology is well-suited to performance profiling, debugging, code coverage, security auditing, runtime code optimizations, and kernel extensions. We have designed and implemented a tool called KernInst that performs dynamic instrumentation on a stock production Solaris kernel running on an UltraSPARC. On top of KernInst, we have implemented a kernel performance profiling tool, and used it to understand kernel and application performance under a Web proxy server workload. We used this information to make two changes (one to the kernel, one to the proxy) that cumulatively reduce the percentage of elapsed time that the proxy spends opening disk cache files from 40% to 7%.", "num_citations": "255\n", "authors": ["1027"]}
{"title": "Dynamic program instrumentation for scalable performance tools\n", "abstract": " Presents a new technique called 'dynamic instrumentation' that provides efficient, scalable, yet detailed data collection for large-scale parallel applications. Our approach is unique because it defers inserting any instrumentation until the application is in execution. We can insert or change instrumentation at any time during execution by modifying the application's binary image. Only the instrumentation required for the currently selected analysis or visualization is inserted. As a result, our technique collects several orders of magnitude less data than traditional data collection approaches. We have implemented a prototype of our dynamic instrumentation on the CM-5, and present results for several real applications. In addition, we include recommendations to operating system designers, compiler writers, and computer architects about the features necessary to permit efficient monitoring of large-scale parallel systems.< >", "num_citations": "253\n", "authors": ["1027"]}
{"title": "Hybrid analysis and control of malware\n", "abstract": " Malware attacks necessitate extensive forensic analysis efforts that are manual-labor intensive because of the analysis-resistance techniques that malware authors employ. The most prevalent of these techniques are code unpacking, code overwriting, and control transfer obfuscations. We simplify the analyst\u2019s task by analyzing the code prior to its execution and by providing the ability to selectively monitor its execution. We achieve pre-execution analysis by combining static and dynamic techniques to construct control- and data-flow analyses. These analyses form the interface by which the analyst instruments the code. This interface simplifies the instrumentation task, allowing us to reduce the number of instrumented program locations by a hundred-fold relative to existing instrumentation-based methods of identifying unpacked code. We implement our techniques in SD-Dyninst and apply them to a large\u00a0\u2026", "num_citations": "241\n", "authors": ["1027"]}
{"title": "Reliable network connections\n", "abstract": " We present two systems, reliable sockets (rocks) and reliable packets (racks), that provide transparent network connection mobility using only user-level mechanisms. Each system can detect a connection failure within seconds of its occurrence, preserve the endpoint of a failed connection in a suspended state for an arbitrary period of time, and automatically reconnect, even when one end of the connection changes IP address, with correct recovery of in-flight data. To allow rocks and racks to interoperate with ordinary clients and servers, we introduce a general user-level Enhancement Detection Protocol that enables the remote detection of rocks and racks, or any other socket enhancement system, but does not affect applications that use ordinary sockets. Rocks and racks provide the same functionality but have different implementation models: rocks intercept and modify the behavior of the sockets API by using an\u00a0\u2026", "num_citations": "235\n", "authors": ["1027"]}
{"title": "Improving the accuracy of data race detection\n", "abstract": " For shared-memory parallel programs that use explicit synchronization, data race detection is an important part of debugging. A data race exists when concurrently executing sections of code access common shared variables. In programs intended to be data race free, they are sources of nondeterminism usually considered bugs. Previous methods for detecting data races in executions of parallel programs can determine when races occurred, but can report many data races that are artifacts of others and not direct manifestations of program bugs. Artifacts exist because some races can cause others and can also make false races appear real. Such artifacts can overwhelm the programmer with information irrelevant for debugging. This paper presents results showing how to identify nonartifact data races by validation and ordering.Data race validation attempts to determine which races involve events that either did\u00a0\u2026", "num_citations": "200\n", "authors": ["1027"]}
{"title": "Critical path analysis for the execution of parallel and distributed programs\n", "abstract": " In our research of performance measurement tools for parallel and distributed programs, we have developed techniques for automatically guiding the programmer to performance problems in their application programs. One example of such techniques finds the critical path through a graph of a program\u2019s execution history.This paper presents the design, implementation and testing of the critical path analysis technique on the IPS performance measurement tool for parallel and distributed programs. We create a precedence graph of a program\u2019s activities (Program Activity Graph) with the data collected during the execution of a program. The critical path, the longest path in the program activity graph, represents the sequence of the program activities that take the longest time to execute. Various algorithms are developed to track the critical path from this graph. The events in this path are associated with the entities in the source program and the statistical results are displayed based on the hierarchical structure of the IPS. The test results from the measurement of sample programs show that the knowledge of the critical path in a program\u2019s execution helps users identify performance problems and better understand the behavior of a program.", "num_citations": "192\n", "authors": ["1027"]}
{"title": "Anywhere, any-time binary instrumentation\n", "abstract": " The Dyninst binary instrumentation and analysis framework distinguishes itself from other binary instrumentation tools through its abstract, machine independent interface; its emphasis on anywhere, any-time binary instrumentation; and its low overhead that is proportional to the number of instrumented locations. Dyninst represents the program in terms of familiar control flow structures such as functions, loops, and basic blocks, and users manipulate these representations to insert instrumentation anywhere in the binary. We use graph transformation techniques to insure that this instrumentation executes when desired even when instrumenting highly optimized (or malicious) code that other instrumenters cannot correctly instrument. Unlike other binary instrumenters, Dyninst can instrument at any time in the execution continuum, from static instrumentation (binary rewriting) to instrumenting actively executing code\u00a0\u2026", "num_citations": "175\n", "authors": ["1027"]}
{"title": "Detecting data races on weak memory systems\n", "abstract": " For shared-memory systems, the most commonly assumed programmer\u2019s model of memory is sequential consistency. The weaker models of weak ordering, release consistency with sequentially consistent synchronization operations, data-race-free-O, and data-race-free-1 provide higher performance by guaranteeing sequential consistency to only a restricted class of programs-mainly programs that do not exhibit data races. To allow programmers to use the intuition and algorithms already developed for sequentially consistent systems, it is impontant to determine when a program written for a weak system exhibits no data races. In this paper, we investigate the extension of dynamic data race detection techniques developed for sequentially consistent systems to weak systems. A potential problem is that in the presence of a data race, weak systems fail to guarantee sequential consistency and therefore dynamic\u00a0\u2026", "num_citations": "174\n", "authors": ["1027"]}
{"title": "MDL: A language and compiler for dynamic program instrumentation\n", "abstract": " We use a form of dynamic code generation, called dynamic instrumentation, to collect data about the execution of an application program. Dynamic instrumentation allows us to instrument running programs to collect performance and other types of information. The instrumentation code is generated incrementally and can be inserted and removed at any time. Our instrumentation currently runs on the SPARC, PA-RISC, Power 2, Alpha, and x86 architectures. Specification of what data to collect are written in a specialized language called the Metric Description Language, that is part of the Paradyn Parallel Performance Tools. This language allows platform independent descriptions of how to collect performance data. It also provides a concise way to specify, how to constrain performance data to particular resources such as modules, procedures, nodes, files, or message channels (or combinations of these resources\u00a0\u2026", "num_citations": "150\n", "authors": ["1027"]}
{"title": "Dynamic control of performance monitoring on large scale parallel systems\n", "abstract": " Performance monitoring of large scale parallel computers creates a dilemma: we need to collect detailed information to find performance bottlenecks, yet collecting all this data can introduce serious data collection bottlenecks. At the same time, users are being inundated with volumes of complex graphs and tables that require a performance expert to interpret. We present a new approach called the W 3 Search Model, that addresses both these problems by combining dynamic on-the-fly selection of what performance data to collect with decision support to assist users with the selection and presentation of performance data. We present a case study describing how a prototype implementation of our technique was able to identify the bottlenecks in three real programs. In addition, we were able to reduce the amount of performance data collected by a factor ranging from 13 to 700 compared to traditional sampling and\u00a0\u2026", "num_citations": "148\n", "authors": ["1027"]}
{"title": "Practical analysis of stripped binary code\n", "abstract": " Executable binary code is the authoritative source of information about program content and behavior. The compile, link, and optimize steps can cause a program's detailed execution behavior to differ substantially from its source code. Binary code analysis is used to provide information about a program's content and structure, and is therefore a foundation of many applications, including binary modification[3,12,22,31], binary translation[5,29], binary matching[30], performance profiling[13,16,18], debugging, extraction of parameters for performance modeling, computer security[7,8] and forensics[23,26]. Ideally, binary analysis should produce information about the content of the program's code (instructions, basic blocks, functions, and modules), structure (control and data flow), and data structures (global and stack variables). The quality and availability of this information affects applications that rely on binary analysis.", "num_citations": "138\n", "authors": ["1027"]}
{"title": "Who wrote this code? identifying the authors of program binaries\n", "abstract": " Program authorship attribution\u2014identifying a programmer based on stylistic characteristics of code\u2014has practical implications for detecting software theft, digital forensics, and malware analysis. Authorship attribution is challenging in these domains where usually only binary code is available; existing source code-based approaches to attribution have left unclear whether and to what extent programmer style survives the compilation process. Casting authorship attribution as a machine learning problem, we present a novel program representation and techniques that automatically detect the stylistic features of binary code. We apply these techniques to two attribution problems: identifying the precise author of a program, and finding stylistic similarities between programs by unknown authors. Our experiments provide strong evidence that programmer style is preserved in program binaries.", "num_citations": "137\n", "authors": ["1027"]}
{"title": "Binary-code obfuscations in prevalent packer tools\n", "abstract": " The first steps in analyzing defensive malware are understanding what obfuscations are present in real-world malware binaries, how these obfuscations hinder analysis, and how they can be overcome. While some obfuscations have been reported independently, this survey consolidates the discussion while adding substantial depth and breadth to it. This survey also quantifies the relative prevalence of these obfuscations by using the Dyninst binary analysis and instrumentation tool that was recently extended for defensive malware analysis. The goal of this survey is to encourage analysts to focus on resolving the obfuscations that are most prevalent in real-world malware.", "num_citations": "121\n", "authors": ["1027"]}
{"title": "Binary code is not easy\n", "abstract": " Binary code analysis is an enabling technique for many applications. Modern compilers and run-time libraries have introduced significant complexities to binary code, which negatively affect the capabilities of binary analysis tool kits to analyze binary code, and may cause tools to report inaccurate information about binary code. Analysts may hence be confused and applications based on these tool kits may have degrading quality. We examine the problem of constructing control flow graphs from binary code and labeling the graphs with accurate function boundary annotations. We identified several challenging code constructs that represent hard-to-analyze aspects of binary code, and show code examples for each code construct. As part of this discussion, we present new code parsing algorithms in our open source Dyninst tool kit that support these constructs, including a new model for describing jump tables that\u00a0\u2026", "num_citations": "116\n", "authors": ["1027"]}
{"title": "On the complexity of event ordering for shared-memory parallel program executions\n", "abstract": " This paper presents results on the complexity of computing event orderings for sharedmemory parallel program executions. Given a program execution, we formally define the problem of computing orderings that the execution must have exhibited or could have exhibited, and prove that computing such orderings is an intractable problem. We present a formal model of a shared-memory parallel program execution on a sequentially consistent processor, and discuss event orderings in terms of this model. Programs are considered that use fork/join and either counting semaphores or event style synchronization. We define a feasible program execution to be an execution of the program that performs the same events as an observed execution, but which may exhibit different orderings among those events. Any program execution exhibiting the same data dependences among the shared data as the observed execution\u00a0\u2026", "num_citations": "112\n", "authors": ["1027"]}
{"title": "Problem diagnosis in large-scale computing environments\n", "abstract": " We describe a new approach for locating the causes of anomalies in distributed systems. Our target environment is a distributed application that contains multiple identical processes performing similar activities. We use a new, lightweight form of dynamic instrumentation to collect function-level traces from each process. If the application fails, the traces are automatically compared to each other. We find anomalies by identifying processes that stopped earlier than the rest (sign of a fail-stop problem) or processes that behaved different from the rest (sign of a non-fail-stop problem). Our algorithm does not require reference data to distinguish anomalies from normal behaviors. However, it can make use of such data when available to reduce the number of false positives. Ultimately, we identify a function that is likely to explain the anomalous behavior. We demonstrated the efficacy of our approach by finding two\u00a0\u2026", "num_citations": "111\n", "authors": ["1027"]}
{"title": "Detecting data races in parallel program executions\n", "abstract": " Several methods currently exist for detecting data races in an execution of a shared-memory parallel program.  Although these methods address an important aspect of parallel program debugging, they do not precisely define the notion of a data race.  As a result, it is not possible to precisely state which data races are detected, nor is the meaning of the reported data races always clear.  Furthermore, these methods can sometimes generate false data race reports.  They can determine whether a data race was exhibited during an execution, but when more than one data race is reported, only limited indication is given as to which ones are real.  This paper addresses these two issues.  We first present a model for reasoning about data races, and then present a two-phase approach to data race detection that attempts to validate the accuracy of each detected data race.  Our model of data races distinguishes among those data races that actually occurred during an execution (actual data races), those that could have occurred because of timing variations (feasible data races), and those that appeared to have occurred (apparent data races).  The first phase of our two-phase approach to data race detection is similar to previous methods and detects a set of data race candidates (the apparent data races).  We prove that this set always contains all actual data races, although it may contain other data races, both feasible and infeasible.  Unlike previous methods, we then employ a second phase which validates the apparent data races by attempting to determine which ones are feasible.  This second phase requires no more information than previous\u00a0\u2026", "num_citations": "104\n", "authors": ["1027"]}
{"title": "A distributed programs monitor for Berkeley UNIX\n", "abstract": " Writing and debugging distributed programs can be difficult. When a program is working, it can be difficult to achieve reasonable execution performance. A major cause of these difficulties is a lack of tools for the programmer. We use a model of distributed computation and measurement to implement a program monitoring system for programs running on the Berkeley UNIX 4.2BSD operating system. The model of distributed computation describes the activities of the processes within a distributed program in terms of computation (internal events) and communication (external events). The measurement model focuses on external events and separates the detection of external events, event record selection and data analysis. The implementation of the measurement tools involved changes to the Berkeley UNIX kernel, and the addition of daemon processes to allow the monitoring activity to take place across machine\u00a0\u2026", "num_citations": "104\n", "authors": ["1027"]}
{"title": "Process hijacking\n", "abstract": " Process checkpointing is a basic mechanism required for providing high throughput computing service on distributively owned resources. We present a new process checkpoint and migration technique, called process hijacking, that uses dynamic program re-writing techniques to add checkpointing capability to a running program. Process hijacking makes it possible to checkpoint and migrate proprietary applications that cannot be re-linked with a checkpoint library, and it makes it possible to dynamically hand off an ordinary running process to a distributed resource management system such as Condor. We discuss the problems of adding checkpointing capability to a program already in execution: loading new code into the running process; and replacing functions of the process with calls to dynamically loaded functions. We use the DynInst API process editing library, augmented with a new call for replacing\u00a0\u2026", "num_citations": "97\n", "authors": ["1027"]}
{"title": "Learning to Analyze Binary Computer Code.\n", "abstract": " We present a novel application of structured classification: identifying function entry points (FEPs, the starting byte of each function) in program binaries. Such identification is the crucial first step in analyzing many malicious, commercial and legacy software, which lack full symbol information that specifies FEPs. Existing pattern-matching FEP detection techniques are insufficient due to variable instruction sequences introduced by compiler and link-time optimizations. We formulate the FEP identification problem as structured classification using Conditional Random Fields. Our Conditional Random Fields incorporate both idiom features to represent the sequence of instructions surrounding FEPs, and control flow structure features to represent the interaction among FEPs. These features allow us to jointly label all FEPs in the binary. We perform feature selection and present an approximate inference method for massive program binaries. We evaluate our models on a large set of real-world test binaries, showing that our models dramatically outperform two existing, standard disassemblers.", "num_citations": "96\n", "authors": ["1027"]}
{"title": "What to draw? When to draw? An essay on parallel program visualization\n", "abstract": " 2. WHY IS VISUALIZATION IMPORTANT\"? Almost everyone seems to agree that visualization is an important tool in understanding complex processes. Physical scientists have been using visualization successfully for several years. In the classroom, computer scientists have been using it to teach such subjects as data structures. As motivation, I present two examples from my experiences where pictures were worth much more than a thousand words (or even a megabyte). The first example comes from the world of aviation. As a pilot flying in stormy weather, you will occasionally hear the air traffic controllers broadcast a message such as: This work was supported in part by National Science Foundation grants CCR-8815928 and CCR-9100968, and Office of Naval Research grant N00014-89-J-1222. Copyright O 1992 Barton P. Miller", "num_citations": "93\n", "authors": ["1027"]}
{"title": "Method or Madness? Inside the\" SNWR\" College Rankings.\n", "abstract": " This paper examines why Americans are so preoccupied with the\" US News and World Report\"(\" USNWR\") annual rankings of colleges and universities and why higher education institutions have become equally preoccupied with them. It discusses the rankings categories (academic reputation, student selectivity, faculty resources, graduation and retention rate, alumni giving, financial resources, and graduation rate performance), and it notes how the rankings methodology allows colleges and universities to take actions to manipulate their rankings and the effects that such actions have on higher education. The paper questions why colleges and universities continue to participate in the rankings if they are flawed, discussing some of the major problems with the rankings. The paper concludes that the problem with\" USNWR\" rankings is not its presentation of information on individual data elements but rather its effort to aggregate these elements into a single index, noting that if it stopped doing this, many of the objections. that people have about the ratings would stop. Finally, the paper offers thoughts about how the\" USNWR\" could alter its rating formula in ways that would be more socially desirable.(Contains 20 references.)(SM)", "num_citations": "89\n", "authors": ["1027"]}
{"title": "On-line automated performance diagnosis on thousands of processes\n", "abstract": " Performance analysis tools are critical for the effective use of large parallel computing resources, but existing tools have failed to address three problems that limit their scalability:(1) management and processing of the volume of performance data generated when monitoring a large number of application processes,(2) communication between a large number of tool components, and (3) presentation of performance data and analysis results for applications with a large number of processes. In this paper, we present a novel approach for finding performance problems in applications with a large number of processes that leverages our multicast and data aggregation infrastructure to address these three performance tool scalability barriers. First, we show how to design a scalable, distributed performance diagnosis facility. We demonstrate this design with an on-line, automated strategy for finding performance\u00a0\u2026", "num_citations": "86\n", "authors": ["1027"]}
{"title": "Paging tradeoffs in distributed-shared-memory multiprocessors\n", "abstract": " Massively parallel processors have begun using commodity operating systems that support demand-paged virtual memory. To evaluate the utility of virtual memory, we measured the behavior of seven shared-memory parallel application programs on a simulated distributed-shared-memory machine. Our results (1) confirm the importance of gang CPU scheduling, (2) show that a page-faulting processor should spin rather than invoke a parallel context switch, (3) show that our parallel programs frequently touch most of their data, and (4) indicate that memory, not just CPUs, must be \u201cgang scheduled.\u201d Overall, our experiments demonstrate that demand paging has limited value on current parallel machines because of the applications' synchronization and memory reference patterns and the machines' high page-fault and parallel context-switch overheads.", "num_citations": "80\n", "authors": ["1027"]}
{"title": "Labeling library functions in stripped binaries\n", "abstract": " Binary code presents unique analysis challenges, particularly when debugging information has been stripped from the executable. Among the valuable information lost in stripping are the identities of standard library functions linked into the executable; knowing the identities of such functions can help to optimize automated analysis and is instrumental in understanding program behavior. Library fingerprinting attempts to restore the names of library functions in stripped binaries, using signatures extracted from reference libraries. Existing methods are brittle in the face of variations in the toolchain that produced the reference libraries and do not generalize well to new library versions. We introduce semantic descriptors, high-level representations of library functions that avoid the brittleness of existing approaches. We have extended a tool, unstrip, to apply this technique to fingerprint wrapper functions in the GNU C\u00a0\u2026", "num_citations": "74\n", "authors": ["1027"]}
{"title": "IPS: An Interactive and Automatic Performance Measurement Tool for Parallel and Distributed Programs.\n", "abstract": " We have designed an interactive tool, called IPS, for perfor mance measurement and analysis of parallel and distributed pro grams. IPS is based on two main principles. First, programmers should be supplied with the maximum information about the execu tion of their program. This information should be available from all levels of abstraction-from the statement level up to the process interaction level. To prevent the programmer from being inundated with irrelevant details, there must be a logical and intuitive organi zation to this data. Second, programmers should be supplied with answers, not numbers. The performance tool should be able to guide the programmer to the location of the performance problem, and describe the problem in terms of the source program. IPS uses a hierarchical model as the framework for perfor mance measurement. The hierarchical model maps program's behavior to different levels of abstraction, and unifies performance data from the whole program level down to procedure and statement level. IPS allows the programmer to interactively evaluate the per formance history of a distributed program. Users are able to maneuver through the hierarchy and analyze the program measure ment results at various levels of detail. The regular organization of the hierarchy allows us to easily reason about the program's execu tion and provides information to automatically guide the program mer to the cause of performance bottlenecks. Critical path analysis, in conjunction with hierarchically organized performance metrics, is one method used to direct the programmer in identifying bottlenecks.", "num_citations": "73\n", "authors": ["1027"]}
{"title": "Recovering the toolchain provenance of binary code\n", "abstract": " Program binaries are an artifact of a production process that begins with source code and ends with a string of bytes representing executable code. There are many reasons to want to know the specifics of this process for a given binary---for forensic investigation of malware, to diagnose the role of the compiler in crashes or performance problems, or for reverse engineering and decompilation---but binaries are not generally annotated with such provenance details. Intuitively, the binary code should exhibit properties specific to the process that produced it, but it is not at all clear how to find such properties and map them to specific elements of that process.", "num_citations": "71\n", "authors": ["1027"]}
{"title": "Extracting compiler provenance from program binaries\n", "abstract": " We present a novel technique that identifies the source compiler of program binaries, an important element of program provenance. Program provenance answers fundamental questions of malware analysis and software forensics, such as whether programs are generated by similar tool chains; it also can allow development of debugging, performance analysis, and instrumentation tools specific to particular compilers. We formulate compiler identification as a structured learning problem, automatically building models to recognize sequences of binary code generated by particular compilers. We evaluate our techniques on a large set of real-world test binaries, showing that our models identify the source compiler of binary code with over 90% accuracy, even in the presence of interleaved code from multiple compilers. A case study demonstrates the use of inferred compiler provenance to augment stripped binary\u00a0\u2026", "num_citations": "70\n", "authors": ["1027"]}
{"title": "Distributed active catalogs and meta-data caching in descriptive name services\n", "abstract": " Today's global internetworks challenge the ability of name services and other information services to locate data quickly. The authors introduce distributed active catalog and meta-data caching for optimizing queries in this environment. The active catalog constrains the search space for a query by returning a list of data repositories where the answer to the query is likely to be found. Meta-data caching improves performance by keeping frequently used characterizations of the search space close to the user, and eliminating active catalog communication and processing costs. When searching for query responses, the techniques contact only the small percentage of the data repositories with actual responses, resulting in search times of a few seconds. A distributed active catalog and meta-data caching method was implemented in a prototype descriptive name service called Nomenclator. Performance results for\u00a0\u2026", "num_citations": "68\n", "authors": ["1027"]}
{"title": "A comparison of interactivity in the Linux 2.6 scheduler and an MLFQ scheduler\n", "abstract": " We implemented a simple multilevel feedback queue scheduler in the Linux 2.6 kernel and compared its response to interactive tasks with that of the new Linux 2.6 scheduler. Our objectives were to evaluate whether Linux 2.6 accomplished its goal of improved interactivity, and to see whether a simpler model could do as well without all of the special cases and exceptions that the new Linux 2.6 scheduler acquired. We describe the two algorithms in detail, report their average interactive response times under different kinds of background workloads, and compare their methods of deciding whether a task is interactive. The MLFQ scheduler performs comparably to the Linux 2.6 scheduler in all response time tests and displays some inadvertent improvements in turnaround time, while avoiding the complex task of explicitly defining interactivity. We maintain an inverse relationship between priority and time slice length\u00a0\u2026", "num_citations": "65\n", "authors": ["1027"]}
{"title": "Dynamic instrumentation of threaded applications\n", "abstract": " The use of threads is becoming commonplace in both sequential and parallel programs. This paper describes our design and initial experience with non-trace based performance instrumentation techniques for threaded programs. Our goal is to provide detailed performance data while maintaining control of instrumentation costs. We have extended Paradyn's dynamic instrumentation (which can instrument programs without recompiling or relinking) to handle threaded programs.Controlling instrumentation costs means efficient instrumentation code and avoiding locks in the instrumentation. Our design is based on low contention data structures. To associate performance data with individual threads, we have all threads share the same instrumentation code and assign each thread with its own private copy of performance counters or timers. The asynchrony in a threaded program poses a major challenge to dynamic\u00a0\u2026", "num_citations": "64\n", "authors": ["1027"]}
{"title": "Performance measurement for parallel and distributed programs: a structured and automatic approach\n", "abstract": " Novel approaches are presented for designing performance measurement systems for parallel and distributed programs. The first approach involves unifying performance information into a single, regular structure that reflects the structure of programs under measurement. The authors define a hierarchical model for the execution of parallel and distributed programs as a framework for the performance measurement. A complete different levels of detail in the hierarchy. The second approach is based on the development of automatic guidance techniques that can direct users to the location of performance problems in the program. Guidance information from such techniques supplies facts about problems in the program and provides possible answers for the further improvement of program efficiency. A performance measurement system, called IPS, has been developed as a prototype of the authors' model and design\u00a0\u2026", "num_citations": "63\n", "authors": ["1027"]}
{"title": "Parallel program performance metrics: A comparison and validation\n", "abstract": " There are many metrics designed to assist in the performance debugging of large-scale parallel applications. We describe a new technique, called True Zeroing, that permits direct quantitative comparison of the guidance supplied by these metrics on real applications. We apply this technique to three programs that include both numeric and symbolic applications. We compare three existing metrics: Gprof, Critical Path, and Quartz/NPT, and several new variations. Critical Path provided the best overall guidance, but it was not infallible. We also include a set of recommendations to tool builders based on the experience gained during our case study.", "num_citations": "62\n", "authors": ["1027"]}
{"title": "Mr. scan: Extreme scale density-based clustering using a tree-based network of gpgpu nodes\n", "abstract": " Density-based clustering algorithms are a widely-used class of data mining techniques that can find irregularly shaped clusters and cluster data without prior knowledge of the number of clusters it contains. DBSCAN is the most wellknown density-based clustering algorithm. We introduce our version of DBSCAN, called Mr. Scan, which uses a hybrid parallel implementation that combines the MRNet tree-based distribution network with GPGPU-equipped nodes. Mr. Scan avoids the problems of existing implementations by effectively partitioning the point space and by optimizing DBSCAN's computation over dense data regions. We tested Mr. Scan on both a geolocated Twitter dataset and image data obtained from the Sloan Digital Sky Survey. At its largest scale, Mr. Scan clustered 6.5 billion points from the Twitter dataset on 8,192 GPU nodes on Cray Titan in 17.3 minutes. All other parallel DBSCAN implementations\u00a0\u2026", "num_citations": "59\n", "authors": ["1027"]}
{"title": "Incremental call\u2010path profiling\n", "abstract": " Profiling is a key technique for achieving high performance. Call\u2010path profiling is a refinement of this technique that classifies a function's behavior based on the path taken to reach the function. This information is particularly useful when optimizing programs that use libraries, such as those for communication (MPI or PVM), linear algebra (ScaLAPACK), or threading. We present a new method for call\u2010path profiling called incremental call\u2010path profiling. We profile only a subset of the functions in the program, allowing the use of more complex metrics while lowering the overhead. This combination of call\u2010path information and complex metrics is particularly useful for localizing bottlenecks in frequently called functions. We also describe the implementation and application of iPath, an incremental call\u2010path profiler. iPath was used to profile two real\u2010world applications: the MILC su3_rmd QCD distributed simulation and\u00a0\u2026", "num_citations": "56\n", "authors": ["1027"]}
{"title": "The integration of application and system based metrics in a parallel program performance tool\n", "abstract": " The IPS-2 parallel program measurement tools provide performance data from application programs, the operating system, hardware, network, and other sources. Previous versions of IPS-2 allowed programmers to collect information about an application based only on what could be collected by software instrumentation inserted into the program (and system call libraries). We have developed an open interface, called the \u201cexternal time histogram\u201d, providing a graceful way to include external data from many sources. The user can tell IPS-2 of new sources of performance data through an extensible metric description language. The data from these external sources is automatically collected when the application program is run. IPS-2 provides a library to simplify constructing the external data collectors.The new version of IPS-2 can measure sharedmemory and message-passing parallel programs running on a\u00a0\u2026", "num_citations": "56\n", "authors": ["1027"]}
{"title": "Using dynamic kernel instrumentation for kernel and application tuning\n", "abstract": " The authors have designed a new technology\u2014fine-grained dynamic                 instrumentation of commodity operating system kernels\u2014that can insert                 runtime-generated code at almost any machine code instruction of an unmodified                 operating system kernel. This technology is ideally suited for kernel performance                 profiling, debugging, code coverage, runtime optimization, and extensibility. They                 have written a tool called KernInst that implements dynamic instrumentation on a                 stock production Solaris 2.5.1 kernel running on an UltraSparc CPU. They have                 written a kernel performance profiler on top of KernInst. Measuring kernel                 performance has a two-way benefit: it can suggest optimizations to both the kernel                 and applications that spend much of their time in kernel code. In this paper, the                 authors present their experiences using KernInst\u00a0\u2026", "num_citations": "53\n", "authors": ["1027"]}
{"title": "Development of a standardized large river bioassessment protocol (LR\u2010BP) for macroinvertebrate assemblages\n", "abstract": " Efforts to develop benthic macroinvertebrate sampling protocols for the bioassessment of lotic ecosystems have been focused largely on wadeable systems. As these methods became increasingly refined and accepted, a growing number of monitoring agencies expanded their work and are now developing sampling protocols for non\u2010wadeable large rivers. Large rivers can differ from wadeable streams in many ways that preclude the use of some wadeable stream sampling protocols. Hence, resource managers need clear and consistent large river bioassessment protocols for measuring ecological integrity that are cost effective, logistically feasible, and meet or are adaptable to the multi\u2010purpose sampling needs of researchers and managers. We conducted a study using an experimental macroinvertebrate sampling method that was designed to overcome limitations of several methods currently in use. Our\u00a0\u2026", "num_citations": "50\n", "authors": ["1027"]}
{"title": "Experiment management support for performance tuning\n", "abstract": " The development of a high performance parallel system or application is an evolutionary process -- both the code and the environment go through many changes during a program's lifetime -- and at each change, a key question for developers is: how and how much did the performance change? No existing performance tool provides the necessary functionality to answer this question. We report on the design and preliminary implementation of a tool that views each execution as a scientific experiment and provides the functionality to answer questions about a program's performance that span more than a single execution or environment.", "num_citations": "49\n", "authors": ["1027"]}
{"title": "A callgraph-based search strategy for automated performance diagnosis\n", "abstract": " We introduce a new technique for automated performance diagnosis, using the program\u2019s callgraph. We discuss our implementation of this diagnosis technique in the Paradyn Performance Consultant. Our implementation includes the new search strategy and new dynamic instrumentation to resolve pointer-based dynamic call sites at run-time. We compare the effectiveness of our new technique to the previous version of the Performance Consultant for several sequential and parallel applications. Our results show that the new search method performs its search while inserting dramatically less instrumentation into the application, resulting in reduced application perturbation and consequently a higher degree of diagnosis accuracy.", "num_citations": "46\n", "authors": ["1027"]}
{"title": "Manual vs. automated vulnerability assessment: A case study\n", "abstract": " The dream of every software development team is to assess the security of their software using only a tool. In this paper, we attempt to evaluate and quantify the effectiveness of automated source code analysis tools by comparing such tools to the results of an in-depth manual evaluation of the same system. We present our manual vulnerability assessment methodology, and the results of applying this to a major piece of software. We then analyze the same software using two commercial products, Coverity Prevent and Fortify SCA, that perform static source code analysis. These tools found only a few of the fifteen serious vulnerabilities discovered in the manual assessment, with none of the problems found by these tools requiring a deep understanding of the code. Each tool reported thousands of defects that required human inspection, with only a small number being security related. And, of this small number of security-related defects, there did not appear to be any that indicated significant vulnerabilities beyond those found by the manual assessment.", "num_citations": "45\n", "authors": ["1027"]}
{"title": "Improving online performance diagnosis by the use of historical performance data\n", "abstract": " Accurate performance diagnosis of parallel and distributed programs is a difficult and time-consuming task. We describe a new technique that uses historical performance data, gathered in previous executions of an application, to increase the effectiveness of automated performance diagnosis. We incorporate several different types of historical knowledge about the application\u2019s performance into an existing profiling tool, the Paradyn Parallel Performance Tool. We gather performance and structural data from previous executions of the same program, extract knowledge useful for diagnosis from this collection of data in the form of search directives, then input the directives to an enhanced version of Paradyn, which conducts a directed online diagnosis. Compared to existing approaches, incorporating historical data shortens the time required to identify bottlenecks, decreases the amount of unhelpful instrumentation\u00a0\u2026", "num_citations": "44\n", "authors": ["1027"]}
{"title": "Performance debugging using parallel performance predicates\n", "abstract": " The use of parallelism in a program pr\u0119sents many new opportunities for performance degradation. Most parallel programmers are aware of these new sources of poor performance, and group them into general categories, such as load imbalance, commu-", "num_citations": "42\n", "authors": ["1027"]}
{"title": "Dpm: A measurement system for distributed programs\n", "abstract": " A framework for measuring the performance of distributed programs is presented. This framework includes a model of distributed programs, a description of the measurement principles and methods, and a guideline for implementing these ideas. The author describes a measurement system called the Distributed Programs Monitor (DPM), which he has constructed on the basis of these concepts. DPM has been implemented and used for measurement studies on two different operating systems, DEMOS/MP and Berkeley Unix.< >", "num_citations": "42\n", "authors": ["1027"]}
{"title": "Mining software repositories for accurate authorship\n", "abstract": " Code authorship information is important for analyzing software quality, performing software forensics, and improving software maintenance. However, current tools assume that the last developer to change a line of code is its author regardless of all earlier changes. This approximation loses important information. We present two new line-level authorship models to overcome this limitation. We first define the repository graph as a graph abstraction for a code repository, in which nodes are the commits and edges represent the development dependencies. Then for each line of code, structural authorship is defined as a sub graph of the repository graph recording all commits that changed the line and the development dependencies between the commits, weighted authorship is defined as a vector of author contribution weights derived from the structural authorship of the line and based on a code change measure\u00a0\u2026", "num_citations": "40\n", "authors": ["1027"]}
{"title": "Virtual machine-provided context sensitive page mappings\n", "abstract": " Context sensitive page mappings provide different mappings from virtual addresses to physical page frames depending on whether a memory reference occurs in a data or instruction context. Such differences can be used to modify the behavior of programs that reference their executable code in a data context. Previous work has demonstrated several applications of context sensitive page mappings, including protection against buffer-overrun attacks and circumvention of self-checksumming codes. We extend context sensitive page mappings to the virtual machine monitor, allowing operation independent of the guest operating system. Our technique takes advantage of the VMM's role in enforcing protection between guest operating systems to interpose on guest OS memory management operations and selectively introduce context sensitive page mappings.", "num_citations": "38\n", "authors": ["1027"]}
{"title": "First principles vulnerability assessment\n", "abstract": " Clouds and Grids offer significant challenges to providing secure infrastructure software. As part of a our effort to secure such middleware, we present First Principles Vulnerability Assessment (FPVA), a new analyst-centric (manual) technique that aims to focus the analyst's attention on the parts of the software system and its resources that are most likely to contain vulnerabilities that would provide access to high-value assets. FPVA finds new threats to a system and is not dependent on a list of known threats.", "num_citations": "37\n", "authors": ["1027"]}
{"title": "An adaptive cost system for parallel program instrumentation\n", "abstract": " We present a new data collection cost system that provides programmers with feedback about the impact data collection is having on their application. We allow programmers to define the level of perturbation their application can tolerate and then we regulate the amount of instrumentation to ensure that threshold is not exceeded. Our approach is unique in that the type of data gathered remains constant; instead we regulate when it is collected. This permits programmers to trade speed of isolation of a performance problem for less application perturbation. We implemented this cost system in the Paradyn Performance Tools and present case studies demonstrating the accuracy of the cost system.", "num_citations": "37\n", "authors": ["1027"]}
{"title": "DEMOS/MP: the development of a distributed operating system\n", "abstract": " The DEMOS/MP operating system has moved from a supercomputer with a simple addressing structure to a network of microcomputers. This transformation was done without significant changes to the semantics of the original DEMOS, i.e. existing DEMOS programs should run on DEMOS/MP. The changes to DEMOS were simplified by the structure of its primitive objects and the functions over those objects. The structure of DEMOS links and processes were the major contributors to the simplicity. The changes made to produce DEMOS/MP involved the internal structure of link, modification to parts of the kernel, and limited changes to the various system processes.", "num_citations": "37\n", "authors": ["1027"]}
{"title": "Toward the deconstruction of Dyninst\n", "abstract": " There are two problems that hinder the development of binary tools: a lack of code sharing and a lack of portability. Binary tools, whether static or dynamic, depend on similar analysis and apply similar modification techniques. However, implementations of these techniques are not shared between tools, forcing a developer to reinvent the wheel rather than leverage existing functionality. Tools are often limited to a small range of platforms, preventing users from using existing tools on new platforms.This research describes the deconstruction of the Dyninst dynamic instrumentation library. Dyninst possesses powerful analysis and modification techniques, but these capabilities are hidden underneath the instrumentation-focused API. We are creating a suite of component libraries that will provide a platform-independent interface to a core piece of Dyninst functionality. These libraries will allow other tool developers to access the capabilities of Dyninst. In addition to a programmatic interface, we are also creating representations for interchange of data between tools. The deconstruction of Dyninst poses several interesting challenges, including the design of general interfaces between components that are also abstract and portable.", "num_citations": "35\n", "authors": ["1027"]}
{"title": "Playing inside the black box: Using dynamic instrumentation to create security holes\n", "abstract": " Programs running on insecure or malicious hosts have often been cited as ripe targets for security attacks. The enabling technology for these attacks is the ability to easily analyze and control the running program. Dynamic instrumentation provides the necessary technology for this analysis and control. As embodied in the DynInst API library, dynamic instrumentation allows easy construction of tools that can: (1) inspect a running process, obtaining structural information about the program; (2) control the execution of the program, (3) cause new libraries to be dynamically loaded into the process' address space; (4) splice new code sequences into the running program and remove them; and (5) replace individual call instructions or entire functions.         With this technology, we have provided two demonstrations of its use: exposing vulnerabilities in a distributed scheduling system (Condor), and bypassing access to a\u00a0\u2026", "num_citations": "35\n", "authors": ["1027"]}
{"title": "Nomenclator descriptive query optimization for large X. 500 environments\n", "abstract": " Nomenclator is an architecture for providing efficient descriptive(attribute-based) naming in a large internet environment. As a test of the basic design, we have built a Nomenclator prototype that uses X. 500 as its underlying data repository. X. 500 SEARCH queries that previously took several minutes, can, in many cases, be answered in a matter of seconds. Our system improves descriptive query performance by trimming branches of the X. 500 direetory tree from the search. These tree-trimming techniques are part of an active catalog that constrains the search space as needed during query processing. The active catalog provides information about the data distribution (meta-&ta) to constrain query processing on demand. Nomenclator caches both data (responses to querim) and meta-data (data distribution information, tree-trimming techniques, data access techniques) to speed future queries. Nomenclator\u00a0\u2026", "num_citations": "35\n", "authors": ["1027"]}
{"title": "Integrated visualization of parallel program performance data\n", "abstract": " Performance tuning a parallel application involves integrating performance data from many components of the system, including the message passing library, performance monitoring tool, resource manager, operating system, and the application itself. The current practice of visualizing these data streams using a separate, customized tool for each source is inconvenient from a usability perspective, and there is no easy way to visualize the data in an integrated fashion. We demonstrate a solution to this problem using Devise, a generic visualization tool which is designed to allow an arbitrary number of different but related data streams to be integrated and explored visually in a flexible manner. We display data emanating from a variety of sources side by side in three case studies. First we interface the Paradyn parallel performance tool and Devise, using two simple data export modules and Paradyn's simple\u00a0\u2026", "num_citations": "34\n", "authors": ["1027"]}
{"title": "Binary wrapping: A technique for instrumenting object code\n", "abstract": " We present a technique, called binary wrapping, that allows object code routines to be instrumented. Binary wrapping allows tracing to be placed around (and sometimes within) proprietary code, when source code access is difficult or impossible. This technique is based on wrapping user-written code around the object code routine. No modifications are needed to the programs that call the object code routine. Binary wrapping has proven itself invaluable in instrumenting proprietary libraries, and may well be useful in other similar circumstances.", "num_citations": "34\n", "authors": ["1027"]}
{"title": "Reliable communication in an unreliable environment\n", "abstract": " From: nelly@corto.inria.fr Received(Date): Mon, 16 Mar 92 16:29:18 +0100 Newsgroups: comp.os.research Subject: OOOS bibliography update Approved: comp-os-research@ftp.cse.ucsc.edu This is an update of the \"Object-Oriented and Operating Systems\" bibliography file, maintained by the SOR project at INRIA Rocquencourt (France), and provided to interested parties as a public service. It is created by applying \"diff -b -c1\" between the current version and the latest version which you have previously received. The \"patch\" program allows you to create the new version by automatically applying this diff to the old version. The latest version can be retrieved by anonymous ftp from ftp.cse.ucsc.edu (128.114.134.19) under the name 'pub/bib/ooos.bib', or from nuri.inria.fr (128.93.1.26) as 'INRIA/bib/ooos.bib.Z' (compressed). --------------------------------------------------------------------- *** 1.29 1991/10/01 10:10:27 --- bib.bib 1992\u2026", "num_citations": "34\n", "authors": ["1027"]}
{"title": "Identifying multiple authors in a binary program\n", "abstract": " Knowing the authors of a binary program has significant application to forensics of malicious software (malware), software supply chain risk management, and software plagiarism detection. Existing techniques assume that a binary is written by a single author, which does not hold true in real world because most modern software, including malware, often contains code from multiple authors. In this paper, we make the first step toward identifying multiple authors in a binary. We present new fine-grained techniques to address the tougher problem of determining the author of each basic block. The decision of attributing authors at the basic block level is based on an empirical study of three large open source software, in which we find that a large fraction of basic blocks can be well attributed to a single author. We present new code features that capture programming style at the basic block level, our approach\u00a0\u2026", "num_citations": "33\n", "authors": ["1027"]}
{"title": "Performance measurement of dynamically compiled Java executions\n", "abstract": " With the development of dynamic compilers for Java, Java's performance promises to rival that of equivalent C/C++ binary executions. This should ensure that Java will become the platform of choice for ubiquitous Web\u2010based supercomputing. Therefore, being able to build performance tools for dynamically compiled Java executions will become increasingly important. In this paper we discuss those aspects of dynamically compiled Java executions that make performance measurement difficult: (i) some Java application methods may be transformed from byte\u2010code to native code at run\u2010time; (ii) even in native form, application code may interact with the Java virtual machine. We describe Paradyn\u2010J, an experimental version of the Paradyn Parallel Performance Tool that addresses this environment by describing performance data from dynamically compiled executions in terms of the multiple execution forms\u00a0\u2026", "num_citations": "33\n", "authors": ["1027"]}
{"title": "Detecting code reuse attacks with a model of conformant program execution\n", "abstract": " Code reuse attacks circumvent traditional program protection mechanisms such as  by constructing exploits from code already present within a process. Existing techniques to defend against these attacks provide ad hoc solutions or lack in features necessary to provide comprehensive and adoptable solutions. We present a systematic approach based on first principles for the efficient, robust detection of these attacks; our work enforces expected program behavior instead of defending against anticipated attacks. We define conformant program execution () as a set of requirements on program states. We demonstrate that code reuse attacks violate these requirements and thus can be detected; further, new exploit variations will not circumvent . To provide an efficient and adoptable solution, we also define observed conformant program execution, which validates program state at system call\u00a0\u2026", "num_citations": "30\n", "authors": ["1027"]}
{"title": "A framework for scalable, parallel performance monitoring\n", "abstract": " Performance monitoring of HPC applications offers opportunities for adaptive optimization based on the dynamic performance behavior, unavailable in purely post\u2010mortem performance views. However, a parallel performance monitoring system must have low overhead and high efficiency to make these opportunities tangible. We describe a scalable parallel performance monitor called TAUoverMRNet (ToM), created from the integration of the TAU performance system and the Multicast Reduction Network (MRNet). The integration is achieved through a plug\u2010in architecture in TAU that allows the selection of different transport substrates to offload the online performance data. A method to establish the transport overlay structure of the monitor from within TAU, one that requires no added support from the job manager or application, is presented. We demonstrate the distribution of performance analysis from the sink to\u00a0\u2026", "num_citations": "30\n", "authors": ["1027"]}
{"title": "Diagnosing distributed systems with self-propelled instrumentation\n", "abstract": " We present a three-part approach for diagnosing bugs and performance problems in production distributed environments. First, we introduce a novel execution monitoring technique that dynamically injects a fragment of code, the agent, into an application process on demand. The agent inserts instrumentation ahead of the control flow within the process and propagates into other processes, following communication events, crossing host boundaries, and collecting a distributed function-level trace of the execution. Second, we present an algorithm that separates the trace into user-meaningful activities called flows. This step simplifies manual examination and enables automated analysis of the trace. Finally, we describe our automated root cause analysis technique that compares the flows to help the analyst locate an anomalous flow and identify a function in that flow that is a likely cause of the anomaly. We\u00a0\u2026", "num_citations": "30\n", "authors": ["1027"]}
{"title": "The paradyn parallel performance tools and pvm\n", "abstract": " Paradyn is a performance tool for large-scale parallel applications. By using dynamic instrumentation and automating the search for bottlenecks, it can measure long running applications on production-sized data sets. Paradyn has recently been ported to measure native PVM applications.Programmers run their unmodified PVM application programs with Paradyn. Paradyn automatically inserts and modifies instrumentation during the execution of the application, systematically searching for the causes of performance problems. In most cases, Paradyn can isolate major causes of performance problems, and the part of the program that is responsible the problem. Paradyn currently runs on the Thinking Machine CM-5, Sun workstations, and PVM (currently only on Suns). It can measure heterogeneous programs across any of these platforms.", "num_citations": "28\n", "authors": ["1027"]}
{"title": "Performance characterization of distributed programs (debugging)\n", "abstract": " Writing distributed programs is difficult for at least two reasons. The first reason is that distributed computing environments present new problems caused by asynchrony, independent time bases, and communications delays. The second reason is that there is a lack of tools available to help the programmer understand the program he/she has written. The tools we use for single machine environments do not easily generalize to a distributed environment. There has been only limited success with previous systems that have tried to help the programmer in developing, debugging, and measuring distributed programs.", "num_citations": "27\n", "authors": ["1027"]}
{"title": "Efficient, sensitivity resistant binary instrumentation\n", "abstract": " Binary instrumentation allows users to inject new code into programs without requiring source code, symbols, or debugging information. Instrumenting a binary requires structural modifications such as moving code, adding new code, and overwriting existing code; these modifications may unintentionally change the program's semantics. Binary instrumenters attempt to preserve the intended semantics of the program by further transforming the code to compensate for these structural modifications. Current instrumenters may fail to correctly preserve program semantics or impose significant unnecessary compensation cost because they lack a formal model of the impact of their structural modifications on program semantics. These weaknesses are particularly acute when instrumenting highly optimized or malicious code, making current instrumenters less useful as tools in the security or high-performance domains. We\u00a0\u2026", "num_citations": "26\n", "authors": ["1027"]}
{"title": "CrossWalk: A tool for performance profiling across the user-kernel boundary\n", "abstract": " Publisher SummaryThis chapter discusses a tool for performance profiling across the user-kernel boundary, CrossWalk. CrossWalk starts profiling at the user level, profiles the main function, then its callees and walks further down the application call graph, refining the performance problem to a particular function. If it determines that this function is a system call, it walks into the kernel code and starts traversing the kernel call graph until it locates the ultimate bottleneck. The key technologies in CrossWalk are dynamic application instrumentation and dynamic kernel instrumentation. For the former, one can use an existing library called Dyninst API. For the latter, a new framework called Kerninst API is designed with an interface modeled after Dyninst. When combined, the two libraries provide a unified and powerful interface for building cross-boundary tools. The chapter describes the usefulness of the cross-boundary\u00a0\u2026", "num_citations": "26\n", "authors": ["1027"]}
{"title": "Tree-based overlay networks for scalable applications\n", "abstract": " The increasing availability of high-performance computing systems with thousands, tens of thousands, and even hundreds of thousands of computational nodes is driving the demand for programming models and infrastructures that allow effective use of such large-scale environments. Tree-based overlay networks (TBO~Ns) have proven to provide such a model for distributed tools like performance profilers, parallel debuggers, system monitors and system administration tools. We demonstrate that the extensibility and flexibility of the TBO~N distributed computing model, along with its performance characteristics, make it surprisingly general, particularly for applications outside the tool domain. We describe many interesting applications and commonly-used algorithms for which TBO~Ns are well-suited and provide a new (non-tool) case study, a distributed implementation of the mean-shift algorithm commonly used\u00a0\u2026", "num_citations": "25\n", "authors": ["1027"]}
{"title": "Autonomous analysis of interactive systems with self-propelled instrumentation\n", "abstract": " Finding the causes of intermittent bugs and performance problems in modern systems is a challenging task. Conventional profilers focus on improving aggregate performance metrics in an application and disregard many problems that are highly visible to users but are deemed statistically insignificant. Finding intermittent bugs is also hard -- breakpoint debuggers change the timing of events, often masking the problem. To address these limitations, we propose a novel approach called self-propelled instrumentation -- using an autonomous agent to perform self-directed exploration of the system. We inject the agent into a running application, and the agent starts propagating through the code, carried by the application's flow of control. As it propagates, it inserts instrumentation dynamically to collect and analyze detailed execution information. The key feature of this approach lies in its ability to meet three requirements\u00a0\u2026", "num_citations": "25\n", "authors": ["1027"]}
{"title": "Clam-an open system for graphical user interfaces\n", "abstract": " CLAM is an object-oriented system designed to support the building of extensible graphical user interfaces. CLAM provides a basic windowing environment with the ability to extend its functions using dynamically loaded C++ classes. The dynamically loaded classes allow for performance tuning (by transparently loading the class in either the client or the CLAM server) and for sharing of new functions.", "num_citations": "25\n", "authors": ["1027"]}
{"title": "Techniques for performance measurement of parallel programs\n", "abstract": " Programmers of parallel systems require high-level tools to aid in analyzing the performance of applications. Performance tuning of parallel programs di ers substantially from the analogous processes on sequential architectures for two main reasons: the inherent complexity of concurrent systems is greater, and the observability of concurrent systems is complicated by the e ects instrumentation can have on the behavior of the system. The complexity of parallel architectures combined with non-determinism can make performance di cult to predict and analyze. Many approaches to help users to understand parallel programs have been proposed. This paper summarizes the problems associated with creating parallel performance measurement tools and describes some of the systems that have been built to solve these problems.", "num_citations": "24\n", "authors": ["1027"]}
{"title": "Structured binary editing with a CFG transformation algebra\n", "abstract": " Binary modification allows users to alter existing code or inject new code into programs without requiring source code, symbols, or debugging information. It is critically important that such modification not accidentally create a structurally invalid binary that has illegal control flow or executes invalid instructions. Unfortunately, current modification tools do not make this guarantee, instead relying on the user to manually ensure the modified binary is valid. In addition, they fail to provide high-level abstractions of the binary (e.g., functions), instead requiring the user to have a deep understanding of the idiosyncrasies of the instruction set and the behavior of the program. We present structured binary editing, which allows users to modify a program binary by modifying its control flow graph (CFG). We define an algebra of CFG transformations that is closed under a CFG validity constraint, thus ensuring that users can\u00a0\u2026", "num_citations": "23\n", "authors": ["1027"]}
{"title": "A taxonomy of race conditions\n", "abstract": " Parallel programs are frequently nondeterministic, meaning they can give different results even when using the same input. These different results arise because variations in the timing of the multiple threads cause the threads to access shared resources in different orders. The phenomena that cause the nondeterministic behavior have been (and continue to be) variously referred to as access anomalies, race conditions, or just races. In a 1992 paper, Netzer and Miller (Netzer, R. H. B., and Miller, B. P.ACM Lett. Programming Languages Systems(Mar. 1992), 74\u201388.) made an important contribution to formalizing and standardizing adjectives that can be applied to \u201craces\u201d (e.g., data race, actual race). In this paper, we continue this effort by presenting a refined taxonomy for races in parallel programs. The terminology we suggest is not always consistent with that used previously, and we describe why we believe our\u00a0\u2026", "num_citations": "22\n", "authors": ["1027"]}
{"title": "Database challenges in global information systems\n", "abstract": " The global Intemet provides users with an information labyrinth-rich in resourees, yet confusing and difficult to navigate. Many researchers are responding with a desire to integrate all~ es, indeed all information, into one global (file) tree. We know (and wonder how the rest of the world fails to see) that hierarchical navigation is an inadequate query facility. Non-pmeedural languages, like relational query languages, offer users the hope of breaking through the labyrinth to aeeess information quickly and directly. The door is open for the database community to make a major impact on the structure of global computing, but current technology is inadequate to the task. New research must overcome the problems of scale, autonomy and availability to make global information systems a reality.Traditionally, distributed name services have addressed the problems of locating resources, people, and information in a network\u00a0\u2026", "num_citations": "22\n", "authors": ["1027"]}
{"title": "Automating risk analysis of software design models\n", "abstract": " The growth of the internet and networked systems has exposed software to an increased amount of security threats. One of the responses from software developers to these threats is the introduction of security activities in the software development lifecycle. This paper describes an approach to reduce the need for costly human expertise to perform risk analysis in software, which is common in secure development methodologies, by automating threat modeling. Reducing the dependency on security experts aims at reducing the cost of secure development by allowing non-security-aware developers to apply secure development with little to no additional cost, making secure development more accessible. To automate threat modeling two data structures are introduced, identification trees and mitigation trees, to identify threats in software designs and advise mitigation techniques, while taking into account specification requirements and cost concerns. These are the components of our model for automated threat modeling, AutSEC. We validated AutSEC by implementing it in a tool based on data flow diagrams, from the Microsoft security development methodology, and applying it to VOMS, a grid middleware component, to evaluate our model's performance.", "num_citations": "21\n", "authors": ["1027"]}
{"title": "A framework for scalable, parallel performance monitoring using tau and mrnet\n", "abstract": " Performance monitoring of HPC applications offers opportunities for adaptive optimization based on dynamic performance behavior, unavailable in purely post-mortem performance views. However, a parallel performance monitoring system must have low overhead and high efficiency to make these opportunities tangible. We describe a scalable parallel performance monitor called TAUoverMRNet (ToM), created from the integration of the TAU performance system and the Multicast Reduction Network (MRNet). The integration is achieved through a plug-in architecture in TAU that allows selection of different transport substrates to offload online performance data. A method to establish the transport overlay structure of the monitor from within TAU, one that requires no added support from the job manager or application, is presented. We demonstrate the distribution of performance analysis from the sink to the overlay nodes and the reduction in large-scale profile data that could otherwise overwhelm any single sink. Results show low perturbation and significant savings accrued from reduction at large processor-counts.", "num_citations": "21\n", "authors": ["1027"]}
{"title": "Performance measurement of interpreted programs\n", "abstract": " In an interpreted execution there is an interdependence between the interpreter\u2019s execution and the interpreted application\u2019s execution; the implementation of the interpreter determines how the application is executed, and the application triggers certain activities in the interpreter. We present a representational model for describing performance data from an interpreted execution that explicitly represents the interaction between the interpreter and the application in terms of both the interpreter and application developer\u2019s view of the execution. We present results of a prototype implementation of a performance tool for interpreted Java programs that is based on our model. Our prototype uses two techniques, dynamic instrumentation and transformational instrumentation, to measure Java programs starting with unmodified Java class files and an unmodified Java virtual machine. We use performance data from\u00a0\u2026", "num_citations": "21\n", "authors": ["1027"]}
{"title": "Slack: a new performance metric for parallel programs\n", "abstract": " Critical Path Profiling is a technique that provides guidance to help programmers try to improve the running time of their program. However, Critical Path Profiling provides only an upper bound estimate of the improvement possible in a parallel program execution. In this paper, we present a new metric, called Slack, to complement Critical Path and provide additional information to parallel programmers about the potential impact of making improvements along the critical path.", "num_citations": "21\n", "authors": ["1027"]}
{"title": "Scalable failure recovery for high-performance data aggregation\n", "abstract": " Many high-performance tools, applications and infrastructures, such as Paradyn, STAT, TAU, Ganglia, SuperMon, Astrolabe, Borealis, and MRNet, use data aggregation to synthesize large data sets and reduce data volumes while retaining relevant information content. Hierarchical or tree-based overlay networks (TBONs) are often used to execute data aggregation operations in a scalable, piecewise fashion. In this paper, we present state compensation, a scalable failure recovery model for high-bandwidth, low-latency TBON computations. By leveraging inherently redundant state information found in many TBON computations, state compensation avoids explicit state replication (for example, process checkpoints and message logging) and incurs no overhead in the absence of failures. Further, when failures do occur, state compensation uses a weak data consistency model and localized protocols that allow\u00a0\u2026", "num_citations": "20\n", "authors": ["1027"]}
{"title": "Mapping performance data for high-level and data views of parallel program performance\n", "abstract": " Programs written inhigh-level parallel languages need profiling tools that provide performance data in terms of the semantics of the high-level language. But high-level performance data can be incomplete when the cause of a performance problem camot be explained in terms of the semantics of the language. We also need the ability to view the performance of the underlying mechanisms used by the language and correlate the underlying activity to the language source code. The key techniques for providing these performance views is the ability to map low-level performance data up to the language abstractions. We describe how we use this information to produce performance data at the higher levels, and how we present this data in terms of both the code and parallel data structures.We have developed an implementation of these mapping techniques for the data parallel CM Fortran language running on the\u00a0\u2026", "num_citations": "20\n", "authors": ["1027"]}
{"title": "A performance tool for high-level parallel programming languages\n", "abstract": " Users of high-level parallel programming languages require accurate performance information that is relevant to their source code. Furthermore, when their programs experience performance problems at the lowest levels of their hardware and software systems, programmers need to be able to peel back layers of abstraction to examine low-level problems while maintaining references to the high-level source code that ultimately caused the problem. In this paper, we present NV, a model for the explanation of performance information for programs built on multiple levels of abstraction. In NV, a level of abstraction includes a collection of nouns (code and data objects), verbs (activities), and performance information measured for the nouns and verbs. Performance information is mapped from level to level to maintain the relationships between low-level activities and high-level code, even when such relationships\u00a0\u2026", "num_citations": "20\n", "authors": ["1027"]}
{"title": "Automated tracing and visualization of software security structure and properties\n", "abstract": " Visualizing a program's structure and security characteristics is the intrinsic part of in-depth software security assessment. Such an assessment is typically an analyst-driven task. The visualization for security analysis is usually labor-intensive, since analysts need to read documents and source code, synthesize trace data from multiple sources (eg, system utilities like lsof or strace). To help address this problem, we propose SecSTAR, a tool that dynamically collects the key information from a system and automatically produces the necessary diagrams to support the first steps of widely-used security analysis methodologies, such as Microsoft Threat Modeling and UW/UAB First Principles Vulnerability Assessment (FPVA). SecSTAR uses an efficient dynamic binary instrumentation technique, self-propelled instrumentation, to collect trace data from production systems during runtime then automatically produces\u00a0\u2026", "num_citations": "19\n", "authors": ["1027"]}
{"title": "Deep start: A hybrid strategy for automated performance problem searches\n", "abstract": " We present Deep Start, a new algorithm for automated performance diagnosis that uses stack sampling to augment our search-based automated performance diagnosis strategy. Our hybrid approach locates performance problems more quickly and finds problems hidden from a more straightforward search strategy. Deep Start uses stack samples collected as a by-product of normal search instrumentation to find deep starters, functions that are likely to be application bottlenecks. Deep starters are examined early during a search to improve the likelihood of finding performance problems quickly.We implemented the Deep Start algorithm in the Performance Consultant, Paradyn\u2019s automated bottleneck detection component. Deep Start found half of our test applications\u2019 known bottlenecks 32% to 59% faster than the Performance Consultant\u2019s current call graphbased search strategy, and finished finding\u00a0\u2026", "num_citations": "19\n", "authors": ["1027"]}
{"title": "MRNet: A scalable infrastructure for the development of parallel tools and applications\n", "abstract": " MRNet is a customizable, high-throughput communication software system for parallel tools and applications. It reduces the cost of these tools\u2019 activities by incorporating a tree-based overlay network (TBON) of processes between the tool\u2019s front-end and back-ends. MRNet was recently ported and released for Cray XT systems. In this paper we describe the main features that make MRNet well-suited as a general facility for building scalable parallel tools. We present our experiences with MRNet and examples of its use.", "num_citations": "17\n", "authors": ["1027"]}
{"title": "Multiapplication support in a parallel-program performance tool\n", "abstract": " We added new features for analyzing multiple programs to the IPS-2 parallel-program performance tools and were surprised at the wide range of performance problems for which this modified IPS-2 can be used. With multiapplication IPS-2, programmers can simultaneously run and analyze cooperating or contending applications; combine performance displays and metrics of multiple applications or multiple versions of the same application to directly compare performance; analyze critical paths of execution for individual applications, for a single application and the applications with which it interacts, or for entire workloads; study how the application workload performance affects the hardware, operating system, and network performance; study an application's evolution through multiple versions, hardware platforms, or input sets; study a workload's aggregate behavior, how applications interact, or how individual\u00a0\u2026", "num_citations": "16\n", "authors": ["1027"]}
{"title": "Xos: an Operating System for the X-tree Architecture\n", "abstract": " This paper describes the fundamentals of the X-TREE Operating System (XOS), a system developed to investigate the effects of the X-TREE architecture on operating system design. It outlines the goals and constraints of the project and describes the major features and modules of XOS. Two concepts are of special interest: The first is demand paging across the network of nodes and the second is separation of the global object space and the directory structure used to reference it. Weaknesses in the model are discussed along with directions for future research.", "num_citations": "16\n", "authors": ["1027"]}
{"title": "Dynamic kernel i-cache optimization\n", "abstract": " We have developed a facility for run-time optimization of a commodity operating system kernel. This facility is a first step towards an evolving operating system, one that adapts and changes over time without need for rebooting. Our infrastructure, currently implemented on UltraSPARC Solaris 7, includes the ability to do a detailed analysis of the running kernel's binary code, dynamically insert and remove code patches, and dynamically install new versions of kernel functions. As a first use of this technology, we have implemented a run-time kernel version of the code positioning I-cache optimizations, and obtained noticeable speedups in kernel performance. As a first case study, we performed run-time code positioning on the kernel\u2019s TCP read-side processing routine while running a Web client benchmark. We found that the code positioning optimizations reduced this function\u2019s execution time by 17.6%, resulting in an end-to-end benchmark speedup of 7%.The primary contributions of this paper are the first run-time kernel implementation of code positioning, and an infrastructure for turning an unmodified commodity kernel into an evolving one. Two further contributions are made in kernel performance measurement. First, we provide a simple and effective algorithm for deriving control flow edge execution counts from basic block execution counts, which contradicts the widely held belief that edge counts cannot be derived from block counts. Second, we describe a means for converting wall time instrumentation-based kernel measurements into virtual (ie, CPU) time measurements via instrumentation of the kernel\u2019s context switch handlers.", "num_citations": "15\n", "authors": ["1027"]}
{"title": "Why do software assurance tools have problems finding bugs like heartbleed\n", "abstract": " The nature of the Heartbleed vulnerability [1] has been well described by several people including Matthew Green[2], Chris Williams [3], Troy Hunt[4], Eric Limer [5], and Sean Cassidy [6]. We appreciate their clear descriptions of the vulnerability, the code, and its consequences. We want to take a different slant: what makes the Heartbleed vulnerability difficult for automated tools to discover? First, we provide our thoughts on why both static and dynamic assurance tools have a difficult time discovering the vulnerability and why current defensive run-\u2010time measures were not successful. Second, we discuss how in the future the SWAMP (Software Assurance Marketplace)[7] could serve as a valuable resource for tool researchers and developers to facilitate the discovery of weaknesses that are not found by existing tools, and to facilitate testing of new innovative tools.", "num_citations": "13\n", "authors": ["1027"]}
{"title": "A framework for multi-execution performance tuning\n", "abstract": " This paper describes a design and prototype implementation of a performance tool designed to answer performance questions that span multiple program executions from all stages of the lifespan of an application. We use the scientific experimentation archetype as a basis for designing an Experiment Management environment for parallel performance. In our model, information from all experiments for one application, including the components of the code executed, the execution environment, and the performance data collected, is gathered in a Program Space. Our Experiment Management tool enables exploration of this space with a simple naming mechanism, a selection and query facility, and a set of visualizations. A key component of this work is the ability to automatically describe the differences between two runs of a program, both the structural differences (differences in program source code and the\u00a0\u2026", "num_citations": "13\n", "authors": ["1027"]}
{"title": "Delphi: An integrated, language-directed performance prediction, measurement and analysis environment\n", "abstract": " Despite construction of powerful parallel systems and networked computational grids, achieving a large fraction of peak performance for a range of applications has proven very difficult. In this paper, we describe the components of Delphi, an integrated performance measurement and prediction environment that places system design on a solid performance engineering basis.", "num_citations": "13\n", "authors": ["1027"]}
{"title": "Group file operations for scalable tools and middleware\n", "abstract": " Group file operations are a new, intuitive idiom for tools and middleware - including parallel debuggers and runtimes, performance measurement and steering, and distributed resource management - that require scalable operations on large groups of distributed files. The idiom provides new semantics for using file groups in standard file operations to eliminate costly iteration. A file-based idiom promotes conciseness and portability, and eases adoption. With explicit semantics for aggregation of group results, the idiom addresses a key scalability barrier. We have designed TBON-FS, a new distributed file system that provides scalable group file operations by leveraging tree-based overlay networks (TBONs) for scalable communication and data aggregation. We integrated group file operations into several tools: parallel versions of common utilities including cp, grep, rsync, tail, and top, and the Ganglia Distributed\u00a0\u2026", "num_citations": "12\n", "authors": ["1027"]}
{"title": "Machine learning-assisted binary code analysis\n", "abstract": " Binary code analysis is a foundational technique in the areas of computer security, performance modeling, and program instrumentation. In computer security, such analysis can provide the basis for detecting, understanding and controlling malicious code. Any analysis of malicious program requires as a first step precisely locating the Function Entry Points (FEPs, the starting byte of each function) within the binary. When full symbol information is available this is a trivial step. Malicious software authors, however, are not known for helpfully providing debugging symbols along with virus payloads. In addition, commodity software is often distributed without symbols, such as in many Linux distributions.In this paper, we consider the machine learning problem of identifying FEPs in binaries where symbols indicating function location are stripped. Our work is targeted at the processing of binaries on a large scale such as is needed in both network-and host-based analysis tools. As a result, we must keep false positive rates extremely low while trying to maximize recall. We consider binaries for both Linux and Windows on the Intel IA32 architecture.", "num_citations": "12\n", "authors": ["1027"]}
{"title": "The tool daemon protocol (TDP)\n", "abstract": " Run-time tools are crucial to program development. In our desktop computer environments, we take for granted the availability of tools for operations such as debugging, profiling, tracing, checkpointing, and visualization. When programs move into distributed or Grid environments, it is difficult to find such tools. This difficulty is caused by the complex interactions necessary between application program, operating system and layers of job scheduling and process management software. As a result, each run-time tool must be individually ported to run under a particular job management system; for m tools and n environments, the problem becomes an m \\times n effort, rather than the hoped-for m + n effort. Variations in underlying operating systems can make this problem even worse. The consequence of this situation is a paucity of tools in distributed and Grid computing environments. In response to the problem, we\u00a0\u2026", "num_citations": "12\n", "authors": ["1027"]}
{"title": "Integrating a Debugger and a Performance Tool for Steering\n", "abstract": " Steering is a performance optimization idiom applicable to many problem domains. It allows control and performance tuning to take place during program execution. Steering emphasizes the optimization and control of the performance of a program using mechanisms that are external to the program. Performance measurement tools and symbolic debuggers already independently provide some of the mechanisms needed to implement a steering tool. In this paper we describe a con guration that integrates a performance tool, Paradyn, and a debugger to build a steering environment.", "num_citations": "12\n", "authors": ["1027"]}
{"title": "The Photochemical Preparation of Dimethyl-N-(2-cyano-2-propyl)ketenimine from 2,2'-Azobisisobutyronitrile1\n", "abstract": " Allylamine and III.\u2014To a round bottom flask equipped with a stirrer, reflux condenser, gas inlet tube, and gas exit tube leading from the top of the condenser to a trap containing 95% ethanol kept at\u201478 were added 32.2 g.(0.092 mole) of triphenyltin hydride and 2.6 g.(0.046 mole) of allylamine. Evolution of a gas was observed on mixing. The reaction mixture was stirred under a nitrogen atmosphere til solidification was complete. The contents of the trap gave a negative test with Nessler\u2019s reagent for ammonia, no characteristic brown-red color being observed. The solid in the flask was recrystallized from benzene, mp 230-232. It did not depress the melting point on admixture with an authentic sample of IV. In a second experiment, using the same procedure as described above, 20 g.(0.057 mole) of III and 1.62 g.(0.029 mole) of I were used. Any possible propylene being evolved was led from the top of the condenser\u00a0\u2026", "num_citations": "12\n", "authors": ["1027"]}
{"title": "Dyninst and MRNet: Foundational infrastructure for parallel tools\n", "abstract": " Parallel tools require common pieces of infrastructure: the ability to control, monitor, and instrument programs, and the ability to massively scale these operations as the application program being studied scales. The Paradyn Project has a long history of developing new technologies in these two areas and producing ready-to-use tool kits that embody these technologies: Dyninst, which provides binary program control, instrumentation, and modification, and MRNet, which provides a scalable and extensible infrastructure to simplify the construction of massively parallel tools, middleware and applications. We will discuss new techniques that we have developed in these areas, and present examples of current use of these tool kits in a variety of tool and middleware projects. In addition, we will discuss features in these tool kits that have not yet been fully exploited in parallel tool development, and that could lead\u00a0\u2026", "num_citations": "11\n", "authors": ["1027"]}
{"title": "Binary-code obfuscations in prevalent packer tools\n", "abstract": " Security analysts\u2019 understanding of the behavior and intent of malware samples depends on their ability to build high-level analysis products from the raw bytes of program binaries. Thus, the first steps in analyzing defensive malware are understanding what obfuscations are present in real-world malware binaries, how these obfuscations hinder analysis, and how they can be overcome. To this end, we present a thorough examination of the obfuscation techniques used by the packer tools that are most popular with malware authors [Bustamante 2008]. Though previous studies have discussed the current state of binary packing [Yason 2007], anti-debugging [Falliere 2007], and anti-unpacking [Ferrie 2008a] techniques, there have been no comprehensive studies of the obfuscation techniques that are applied to binary code. While some of the individual obfuscations that we discuss have been reported independently, this paper consolidates the discussion while adding substantial depth and breadth to it. We describe obfuscations that make binary code difficult to discover (eg, control-transfer obfuscations, exception-based control transfers, incremental code unpacking, code overwriting); to accurately disassemble into instructions (eg, ambiguous code and data, disassembler fuzz-testing, non-returning calls); to structure into functions and basic blocks (eg, obfuscated calls and returns, call-stack tampering, overlapping functions and basic blocks); to understand (eg, obfuscated constants, calling-convention violations, chunked control-flow, do-nothing code); and to manipulate (eg, self-checksumming, anti-relocation, stolen-bytes techniques). We also\u00a0\u2026", "num_citations": "11\n", "authors": ["1027"]}
{"title": "Foreword for fuzz testing book\n", "abstract": " Sitting in my apartment in Madison in the Fall of 1988, there was a wild midwest thunderstorm pouring rain and lighting up the late night sky. That night, I was logged on to the Unix system in my office via a dial-up phone line over a 1200 baud modem. With the heavy rain, there was noise on the line and that noise was interfering with my ability to type sensible commands to the shell and programs that I was running. It was a race to type an input line before the noise overwhelmed the command.This fighting with the noisy phone line was not surprising. After all, this was just before error-correcting modems were available. What did surprise me was the fact that the noise seemed to be causing programs to crash. And more surprising to me were the programs that were crashing--common Unix utilities that we all use everyday.", "num_citations": "11\n", "authors": ["1027"]}
{"title": "Comparing interactive scheduling in Linux\n", "abstract": " We implemented a multilevel feedback queue scheduler in the Linux 2.6 kernel and compared its response to interactive tasks to that of the new Linux 2.6 scheduler. Our objectives were to evaluate whether Linux 2.6 accomplished its goal of improved interactivity, and to see whether our simpler model could do as well without all the special cases and exceptions that the new Linux 2.6 scheduler acquired. We describe the two algorithms in detail, report their average interactive response times under different kinds of background workloads, and compare their methods of deciding whether a task is interactive. Our MLFQ scheduler performs comparably to the Linux 2.6 scheduler in all response time tests and displays some inadvertent improvements in turnaround time, while avoiding the complex task of explicitly defining interactivity. We maintain an inverse relationship between priority and time slice length, and this\u00a0\u2026", "num_citations": "11\n", "authors": ["1027"]}
{"title": "The anatomy of mr. scan: a dissection of performance of an extreme scale gpu-based clustering algorithm\n", "abstract": " The emergence of leadership class systems with GPU-equipped nodes has the potential to vastly increase the performance of existing distributed applications. However, the inclusion of GPU computation into existing extreme scale distributed applications can reveal scalability issues that were absent in the CPU version. The issues exposed in scaling by a GPU can become limiting factors to overall application performance. We developed an extreme scale GPU-based application to perform data clustering on multi-billion point datasets. In this application, called Mr. Scan, we ran into several of these performance limiting issues. Through the use of complete end-to-end benchmarking of Mr. Scan (measuring time from reading and distribution to final output), we were able to identify three major sources of real world performance issues: data distribution, GPU load balancing, and system specific issues such as start-up\u00a0\u2026", "num_citations": "10\n", "authors": ["1027"]}
{"title": "Checkpoints of GUI-based Applications.\n", "abstract": " We describe a new system, called guievict, that enables the graphical user interface (GUI) of any application to be transparently migrated to or replicated on another display without premeditative steps such as re-linking the application program binary or re-directing the application process\u2019s window system communication through a proxy. Guievict is based on a small X window server extension that enables an application to retrieve its window session, a transportable representation of its GUI, from the window server and a library of GUI migration functionality that is injected in the application process at run time. We discuss the underlying technical issues: controlling and synchronizing the communication between the application and the window system, identifying and retrieving the GUI resources that form the window session, regenerating the window session in a new window system, and maintaining application transparency. We have implemented guievict for the XFree86 implementation of the X window system. The GUI migration performance of guievict is measurably but not perceptibly worse than that of a proxy-based system.", "num_citations": "10\n", "authors": ["1027"]}
{"title": "Reliable sockets\n", "abstract": " Reliable Sockets (rocks) are a portable, user-level replacement to the sockets interface that transparently protects applications from network connection failures. Rocks preserve TCP connections and UDP sessions across failures that commonly arise in mobile computing, including host movement, network reconfiguration, link failures, and extended periods of disconnection. Rocks detect connection failures within seconds of their occurrence and automatically recover, reconnecting within a couple seconds once connectivity is restored. Failures may occur at any time, even while data is in flight. To interoperate with ordinary sockets, rocks use a new rock detection protocol that is safe, efficient and suitable for general use in other sockets enhancing systems. Rocks do not require any modifications to application binaries or to the operating system kernel. The data transfer performance of rocks is comparable to that of ordinary sockets for all but small (less than 64 byte) data transfers, and connection latency, while greater than that of ordinary sockets, is on the order of a millisecond.", "num_citations": "10\n", "authors": ["1027"]}
{"title": "A Visual Process Connector for Unix\n", "abstract": " Upconn a tool that lets Unix programmers visually describe the connections between the processes in a distributed program and then execute the distributed program, is described. Upconn consists of several modules and can be extended by adding to a library of tools rather than by adding many special features to Upconn itself. Upconn has three main uses. First, it lets researchers study distributed processing in common Unix environments, reducing the dependence on specialized environments. Second, it can be used for rapid prototyping of distributed applications. Third, Upconn is a learning tool that can help students focus on writing distributed programs without dealing with the complexities of the communication links.< >", "num_citations": "10\n", "authors": ["1027"]}
{"title": "Parallelism in distributed programs: Measurement and prediction\n", "abstract": " We have constructed a system for monitoring the behavior of distributed programs. The meas-urement system provides traces of the execution of programs and has been implemented on both the DEMOS/MP and Berkeley UNIX 4.2 BSD operating systems. Among the analysis techniques that have been applied to the measurement traces is the computation of the amount of parallelism. or concurrent activity. in a program. There are a number of interesting features of our parallelism measurements. First, the programs being measured can be run on a system that is concurrently running other programs. For most cases. system loading does not affect the measurements. Second, the measurement data can be used to predict the performance of the program in other configurations (eg, different assignments of processes to processors). We can also predict the performance of the program in an ideal distributed\u00a0\u2026", "num_citations": "10\n", "authors": ["1027"]}
{"title": "A callgraph\u2010based search strategy for automated performance diagnosis\n", "abstract": " We introduce a new technique for automated performance diagnosis, using the program's callgraph. We discuss our implementation of this diagnosis technique in the Paradyn Performance Consultant. Our implementation includes the new search strategy and new dynamic instrumentation to resolve pointer\u2010based dynamic call sites at run\u2010time. We compare the effectiveness of our new technique to the previous version of the Performance Consultant for several sequential and parallel applications. Our results show that the new search method performs its search while inserting dramatically less instrumentation into the application, resulting in reduced application perturbation and consequently a higher degree of diagnosis accuracy. Copyright \u00a9 2002 John Wiley & Sons, Ltd.", "num_citations": "9\n", "authors": ["1027"]}
{"title": "Instrumentation and measurement\n", "abstract": " Instrumentation and measurement | The grid ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksThe grid: blueprint for a new computing infrastructureInstrumentation and measurement chapter Instrumentation and measurement Share on Authors: Jeffrey Kenneth Hollingsworth profile image Jeffrey K. Hollingsworth View Profile , BP Miller profile image Bart Miller View Profile Authors Info & Affiliations Publication: The grid: blueprint for a new computing infrastructureOctober 1998 Pages 339\u2013365 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert \u2026", "num_citations": "9\n", "authors": ["1027"]}
{"title": "Diogenes: Looking for an honest cpu/gpu performance measurement tool\n", "abstract": " GPU accelerators have become common on today's leadership-class computing platforms. Exploiting the additional parallelism offered by GPUs is fraught with challenges. A key performance challenge faced by developers is how to limit the time consumed by synchronization and memory transfers between the CPU and GPU. We introduce the feed-forward measurement (FFM) performance tool model that automates the identification of unnecessary or inefficient synchronization and memory transfer, providing an estimate of potential benefit if the problem were fixed. FFM uses a new multi-stage/multi-run instrumentation model that adjusts instrumentation based application behavior on prior runs, guiding FFM to problematic GPU operations that were previously unknown. The collected data feeds a new analysis model that gives an accurate estimate of potential benefit of fixing the problem. We created an\u00a0\u2026", "num_citations": "8\n", "authors": ["1027"]}
{"title": "Targeting resilience and profitability in African smallholder agriculture: Insights from ICRISAT-led research programs\n", "abstract": " We reviewed the strategy for Agricultural Research for Development (AR4D) adopted by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT). The objective was to examine ICRISAT\u2019s research strategy related to the twin challenges of resilience and profitability in developing technologies aimed at improving the livelihoods of smallholder farmers in the drylands of Africa. To do this, we examined the expected impact on resilience and profitability of its present program and the realized impact of ICRISAT\u2019s previous research. We argue that the current CGIAR Research Programs led by ICRISAT envisage separate product lines for resilience and profitability, targeted at two groups, i.e., subsistence- and market-oriented smallholders. This approach, expected to make technology more appropriate for farmers\u2019 needs, risks overlooking the interconnectedness of the two targets if they are too\u00a0\u2026", "num_citations": "8\n", "authors": ["1027"]}
{"title": "The Relevance of Classic Fuzz Testing: Have We Solved This One?\n", "abstract": " As fuzz testing has passed its 30th anniversary, and in the face of the incredible progress in fuzz testing techniques and tools, the question arises if the classic, basic fuzz technique is still useful and applicable? In that tradition, we have updated the basic fuzz tools and testing scripts and applied them to a large collection of Unix utilities on Linux, FreeBSD, and MacOS. As before, our failure criteria was whether the program crashed or hung. We found that 9 crash or hang out of 74 utilities on Linux, 15 out of 78 utilities on FreeBSD, and 12 out of 76 utilities on MacOS. A total of 24 different utilities failed across the three platforms. We note that these failure rates are somewhat higher than our in previous 1995, 2000, and 2006 studies of the reliability of command line utilities. In the basic fuzz tradition, we debugged each failed utility and categorized the causes the failures. Classic categories of failures, such as pointer\u00a0\u2026", "num_citations": "7\n", "authors": ["1027"]}
{"title": "Bad and good news about using software assurance tools\n", "abstract": " Software assurance tools \u2013 tools that scan the source or binary code of a program to find weaknesses \u2013 are the first line of defense in assessing the security of a software project. Even though there are a plethora of such tools available, with multiple tools for almost every programming language, adoption of these tools is spotty at best. And even though different tools have distinct abilities to find different kinds of weaknesses, the use of multiple tools is even less common. And when the tools are used (or attempted to be used), they are often used in ways that reduce their effectiveness. We present a step\u2010by\u2010step discussion of how to use a software assurance tool, describing the challenges that can occur in this process. We also present quantitative evidence about the effects that can occur when assurance tools are applied in a simplistic or naive way. We base this presentation on our direct experiences with using a\u00a0\u2026", "num_citations": "7\n", "authors": ["1027"]}
{"title": "Long-term effects of public low-income housing vouchers on work, earnings, and neighborhood quality\n", "abstract": " This analysis of the federal Section 8 housing voucher program finds that voucher recipients live in better neighborhoods within five years of receiving vouchers. The results also show that voucher receipt initially causes lower earnings, but these dissipate over time. The work and earnings effects from voucher receipt differ substantially across demographic groups.", "num_citations": "7\n", "authors": ["1027"]}
{"title": "A scalable failure recovery model for tree-based overlay networks\n", "abstract": " We present a scalable failure recovery model for data aggregations in large scale tree-based overlay networks (TBONs). A TBON is a network of hierarchically organized processes that exploits the logarithmic scaling properties of trees to provide scalable data multicast, gather, and in-network aggregation. TBONs are commonly used in debugging and performance tools, system monitoring, information management systems, stream processing, and mobile ad hoc networks. Our recovery model leverages inherent information redundancies in TBON computations. This redundant information is gathered from non-failed processes to compensate for computation and communication state lost due to failures. This state compensation strategy is attractive because: (1) it avoids the time and resource overheads of previous reliability approaches, which rely on explicit replication; (2) recovery is rapid and only involves a small subset of the network; and (3) it applies to many useful, complex computations. In this paper, we formalize the TBON model and its fundamental properties to prove that our state compensation model properly preserves computational semantics across TBON process failures. These properties lead to an efficient implementation of state compensation, which we use to empirically validate and evaluate recovery performance. We show that state compensation can recover from failures in extremely large TBONs in milliseconds rendering practically no application service interruption.", "num_citations": "7\n", "authors": ["1027"]}
{"title": "Benchmarking the MRNet distributed tool infrastructure: lessons learned\n", "abstract": " Summary form only given. MRNet is an infrastructure that provides scalable multicast and data aggregation functionality for distributed tools. While evaluating MRNet's performance and scalability, we learned several important lessons about benchmarking large-scale, distributed tools and middleware. First, automation is essential for a successful benchmarking effort, and should be leveraged whenever possible during the benchmarking process. Second, micro-benchmarking is invaluable not only for establishing the performance of low-level functionality, but also for design verification and debugging. Third, resource management systems need substantial improvements in their support for running tools and applications together. Finally, the most demanding experiments should be attempted early and often during a benchmarking effort to increase the chances of detecting problems with the tool and experimental\u00a0\u2026", "num_citations": "7\n", "authors": ["1027"]}
{"title": "The traveling salesman problem: The development of a distributed computation\n", "abstract": " The Traveling Salesman Problem (TSP) is computationally expensive to evaluate. It can, however, be readily decomposed into subproblems that can be computed in parallel. Developing a distributed program taking advantage of such a decomposition, however, remains a difficult problem.We developed such a distributed program to compute the TSP solutions, using a new set of distributed program performance tools to better understand our TSP program. These tools allowed us to discover the performance bottlenecks in our program and to revise the program to significantly improve its execution speed.", "num_citations": "7\n", "authors": ["1027"]}
{"title": "Exposing hidden performance opportunities in high performance gpu applications\n", "abstract": " Leadership class systems with nodes containing many-core accelerators, such as GPUs, have the potential to increase the performance of applications. Effectively exploiting the parallelism provided by many-core accelerators requires developers to identify where accelerator parallelization would provide benefit and ensuring efficient interaction between the CPU and accelerator. In the abstract, these issues appear straightforward and well understood. However, we have found that significant untapped performance opportunities exist in these areas even in well-known, heavily optimized, real world applications created by experienced GPU developers. These untapped performance opportunities exist because accelerated libraries can create unexpected synchronization delay and memory transfer requests, interaction between accelerated libraries can cause unexpected inefficiencies when combined, and\u00a0\u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "A lightweight library for building scalable tools\n", "abstract": " MRNet is a software-based multicast reduction network for building scalable tools. Tools face communication and computation issues when used on large systems; MRNet alleviates these issues by providing multicast communication and data aggregation functionalities. Until now, the MRNet API has been entirely in C++. We present a new, lightweight library that provides a C interface for MRNet back-ends, making MRNet accessible to a wide range of new tools. Further, this library is single threaded to accommodate even more platforms and tools where this is a limitation.This new library provides the same abstractions as the C++ library, using an API that can be derived by applying a standard translation template to the C++ API.", "num_citations": "6\n", "authors": ["1027"]}
{"title": "In search of sweet-spots in parallel performance monitoring\n", "abstract": " Parallel performance monitoring extends parallel measurement systems with infrastructure and interfaces for online performance data access, communication, and analysis. At the same time it raises concerns for the impact on application execution from monitor overhead. The application monitoring scheme parameterized by performance events to monitor, access frequency and the type of data analysis operation defines a set of monitoring requirements. The monitoring infrastructure presents its own choices, particularly the amount and configuration of resources devoted explicitly to monitoring. The key to scalable, low-overhead parallel performance monitoring is to match the application monitoring demands to the effective operating range of the monitoring system (or vice-versa). A poor match can result in over-provisioning (wasted resources) or in under-provisioning (lack of scalability, high overheads and poor\u00a0\u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "How to Open a File and Not Get Hacked\n", "abstract": " Careless attention to opening files, often caused by problems with path traversal or shared directories, can expose applications to attacks on the file names that they use. In this paper we present criteria to determine if a path is safe from attack and how previous algorithms are not sufficient to protect against such attacks. We then describe an algorithm to safely open a file when in the presence of an attack (and how to detect the presence of such an attack), and provide a new library of file open routines that embodies our algorithm. These routines can be used as one-for-one substitutes for conventional POSIX open and fopen calls.", "num_citations": "6\n", "authors": ["1027"]}
{"title": "Paradyn parallel performance tools\n", "abstract": " Paradyn Parallel Performance Tools Page 1 Paradyn Overview \u00a9 2000 Barton P. Miller Paradyn Parallel Performance Tools Barton P. Miller, Jeff Hollingsworth bart@cs.wisc.edu, hollings@cs.umd.edu Mehmet Altinel (UMD) Drew Bernat Bryan Buck (UMD) Trey Cain Chris Chambreau Mihai Christodorescu Kyong Dong (UMD) Nick Rasmussen Philip Roth Brandon Schendel Ari Tamches Mustafa Tikir Roland Wism\u00fcller (TUM) Brian Wylie Zhichen Xu Victor Zandy Wei Zhang Page 2 \u2013 2 \u2013 Paradyn Overview \u00a9 2000 Barton P. Miller Some History and Motivation Experience with IPS-2 tools project: \u25a1Trace-based tool running on workstations, SMP (Sequent Symmetry), Cray Y-MP. \u25a1Commercial Success: In Sun SPARCWorks, Informix OnView, NSF Supercomputer Centers. \u25a1Many real scientific and database/transaction users. Page 3 \u2013 3 \u2013 Paradyn Overview \u00a9 2000 Barton P. Miller More Motivation and History A 1992 \u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "Optimizing array distributions in data-parallel programs\n", "abstract": " Data parallel programs are sensitive to the distribution of data across processor nodes. We formulate the reduction of inter-node communication as an optimization on a colored graph. We present a technique that records the run time inter-node communication caused by the movement of array data between nodes during execution and builds the colored graph, and provide a simple algorithm that optimizes the coloring of this graph to describe new data distributions that would result in less inter-node communication. From the distribution information, we write compiler pragmas to be used in the application program.             Using these techniques, we traced the execution of a real data-parallel application (written in CM Fortran) and collected the array access information. We computed new distributions that should provide an overall reduction in program execution time. However, compiler optimizations and\u00a0\u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "The frequency of dynamic pointer references in \u201cC\u201d programs\n", "abstract": " A collection of\" C\" programs was measured for the number of dynamic references to pointers. The number of dynamic references to pointers is presenteded with respect to the total number of instructions a program executes, giving the percentage of pointer references executed in a\" C\" program. The measurements were done on a VAX 11/780 running the Berkeley UNIX operating system. The measured programs were selected by examining the most commonly run programs on the Computer Sciences Department UNIX machines. The measurement process was performed in two steps:(1) the dynamic counting of pointer references, and (2) the counting of the total number of instructions executed by the program.There are several uses for the results presented in this report. One use was for a study of how well\" C\" programs would run on a CPU that did not easily support 32 bit pointers. Each time a pointer was used, a\u00a0\u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "A Reliable and Secure UNIX Connection Service\n", "abstract": " Distributed programs require a method for processes residing on different machines to identify each other and establish communication. One method is to provide a special connection service to perform this task. A good connection service should be easy to use. It should allow arbitrary processes to connect to each other as well as helping client processes to connect to server processes. It should provide location transparency; that is, the programmer should not have to know the network address of a process to connect to it. The connection service should be reliable. It should provide a way for a process to establish the identity of the user associated with the process to which it has connected, and to communicate securely with that process.We have implemented a connection service for Berkeley UNIX that is reliable, available, secure, and easy to use. The connection service achieves ease of use through a simple\u00a0\u2026", "num_citations": "6\n", "authors": ["1027"]}
{"title": "The Case for an Open and Evolving Software Assurance Framework\n", "abstract": " \u25aa Access to powerful computing capabilities (700 cores, 5 TB of RAM, 104 TB of HDD space, off site backup, industry leading networking technologies)\u25aa Provides a hub for software assurance projects\u25aa Supports managed access to tools, packages and results\u25aa Maintains confidentiality of software and results at the discretion of the user", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Automating threat modeling through the software development life-cycle\n", "abstract": " Fixing software security issues early in the development life-cycle reduces its cost dramatically. Companies doing software development know this reality, and they have introduced risk assessment methodologies in their development processes. Unfortunately, these methodologies require engineers to have deep software security skills to carry out some of the most important steps of this process, and training them on security is expensive. In this scenario, we propose a new automated approach to analyze software designs to identify, risk rank and mitigate potential threats to the system. We designed a new data structure to detect threats in software designs called Identification Tree. We also defined a new one for describing countermeasures to threats, called Mitigation Trees. Our automated approach relies on Identification Trees and Mitigation Trees to integrate a guided risk assessment process through the development life-cycle. It does not require developers to have any security training, and was integrated in the current Threat Modeling process of Microsoft.", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Relative exercise intensity and caloric expenditure of hooping\n", "abstract": " The purpose of this study was to determine the relative exercise intensity and caloric expenditure of hooping. Sixteen apparently healthy females, aged 16-59 years, from the Hooked on Hooping studio in Green Bay, Wisconsin volunteered to be tested. Subjects completed a 30-minute hooping trial by following a video created by the founder of Hooked on Hooping. To measure oxygen consumption, subjects wore a portable metabolic analyzer. Heart Rate (HR) was recorded using a Polar telemetric unit and ratings of perceived exertion (RPE) were determined using the Borg 6-20 scale. Results for the study showed that the average HR was 151 bpm, corresponding to 84% HRmax. Throughout the session, the average V02 measured was 20.6 + 3.31 ml/kg/min. The average caloric expenditure of hooping was calculated to be 7.0 + 1.44 Kcal/min, equivalent to 210 + 43.3 calories for 30 minutes of hooping. Average RPE was 13.0 + 1.51, which corresponds to \"somewhat hard\" on the Borg scale. Based on these results, hooping was shown to meet the recommended guidelines from ACSM for improving cardiovascular fitness and controlling body weight.", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Mechanisms for Mapping High-Level Parallel Performance Data.\n", "abstract": " A primary problem in the performance measurement of high-level parallel programming languages is to map low-level events to high-level programming constructs. We discuss several aspects of this problem and present three methods with which performance tools can map performance data and provide accurate performance information to programmers. In particular, we discuss static mapping, dynamic mapping, and a new technique that uses a data structure called the set of active sentences. Because each of these methods requires cooperation between compilers and performance tools, we describe the nature and amount of cooperation required. The three mapping methods are orthogonal; we describe how they should be combined in a complete tool. Although we concentrate on mapping upward through layers of abstraction, our techniques are independent of mapping direction.", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Summary of ACM/ONR workshop on parallel and distributed debugging\n", "abstract": " Robert Netzer presented an adaptive trace strategy that records the minimal number of shared memory references required for execution replay. Execution replay is a fundamental debugging technique but for reasons of efficiency we cannot afford to trace every shared memory reference. The algorithm proposed by Netzer makes runtime decisions about what not to trace. The goal is to trace just enough data during execution to reproduce the program execution in exact detail. The algorithm is based on\" frontier\" race detection to do adaptive tracing, which allows nondeterministic references to be located and traced dynamically. A frontier race is the transitive reduction of a shared-data dependence relation between two different processes. Tracing each frontier race is sufficient and necessary for replay: enforcing the frontier races ensures that all operations are in the right order during the execution replay.The race\u00a0\u2026", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Experience with techniques for refining data race detection\n", "abstract": " Dynamic data race detection is a critical part of debugging shared-memory parallel programs. The races that can be detected must be refined to filter out false alarms and pinpoint only those that are direct manifestations of bugs. Most race detection methods can report false alarms because of imprecise run-time information and because some races are caused by others. To overcome this problem, race refinement uses whatever run-time information is available to speculate on which of the detected races should be reported. In this paper we report on experimental tests of two refinement techniques previously developed by us. Our goal was to determine whether good refinement is possible, and how much run-time information is required. We analyzed two sets of programs, one set written by others (which they had tested and believed to be race-free but which in fact had subtle races) and another set written by\u00a0\u2026", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Lost in a labyrinth of workstations\n", "abstract": " Two efforts at descriptive name service for workstations are discussed. The Networked Resource Discovery Project proposes to use signposts to locate resources. This approach is effective in the search for one of many instances of a service. Nomenclator is a system that combines maps with signs. Early Nomenclator results indicate that there are significant performance advantages to be had by using a combination of signs and maps to locate resources. If this approach to descriptive name services is pursued, it is argued that users will not become lost in a labyrinth of workstations.< >", "num_citations": "5\n", "authors": ["1027"]}
{"title": "Optimal processor assignment for parallel database design\n", "abstract": " The computing time bene ts of parallelism in database systems (achieved by using multiple processors to execute a query) must be weighed against communication, startup, and termination overhead costs that increase as a function of the number of processors used. We consider problems of minimizing overhead subject to allocating data among the processors according to speci ed loads. We present lower bounds for these combinatorial problems and demonstrate how processors may be optimally assigned for some problem classes.1. Introduction. In highly-parallel database machines (eg, Gamma 2], Bubba 1], Non-Stop SQL 12], XPRS 11] and Volcano 6]) relations are partitioned across multiple processors.(Livny et al 9] and Ries and Epstein 10] introduced the related concept of\\horizontal\" partitioning.) This allows each processor to execute a portion of a query in parallel with the other processors, resulting in a lower response time for the query. However, there is communication overhead associated with initiating and terminating a query on multiple processors, and this overhead increases as a function of the number of processors used to execute a query 1. In order to minimize overhead while balancing the workload among the processors, Multi-Attribute GrId deClustering (MAGIC) introduced by Ghandeharizadeh 3] partitions a relation by assigning ranges of several attribute values to each processor in the system. To illustrate MAGIC consider the partitioning of the Employee relation EMP in gure 1. For parallel computation, MAGIC partitions the EMP relation by establishing ranges of Salary and Age attribute values. Each cell of this grid\u00a0\u2026", "num_citations": "5\n", "authors": ["1027"]}
{"title": "An In-Depth Security Assessment of Maritime Container Terminal Software Systems\n", "abstract": " Attacks on software systems occur world-wide on a daily basis targeting individuals, corporations, and governments alike. The systems that facilitate maritime shipping are at risk of serious disruptions, and these disruptions can stem from vulnerabilities in the software and processes used in these systems. These vulnerabilities leave such systems open to cyber-attack. Assessments of the security of maritime shipping systems have focused on identifying risks but have not taken the critical (and expensive) next step of actually identifying vulnerabilities present in these systems. While such risk assessments are important, they have not provided the detailed identification of security issues in the systems that control these ports and their terminals. In response, we formed a key collaboration between an experienced academic cybersecurity team and a well-known commercial software provider that manages maritime\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Trusted ci experiences in cybersecurity and service to open science\n", "abstract": " This article describes experiences and lessons learned from the Trusted CI project, funded by the US National Science Foundation (NSF) to serve the community as the NSF Cybersecurity Center of Excellence (CCoE). Trusted CI is an effort to address cybersecurity for the open science community through a single organization that provides leadership, training, consulting, and knowledge to that community. The article describes the experiences and lessons learned of Trusted CI regarding both cybersecurity for open science and managing the process of providing centralized services to a broad and diverse community.", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Using SchedFlow for performance evaluation of workflow applications\n", "abstract": " Computational science increasingly relies on the execution of workflows in distributed networks to solve complex applications. However, the heterogeneity of resources in these environments complicates resource management and the scheduling of such applications. Sophisticated scheduling policies are being developed for workflows, but they have had little impact in practice because their integration into existing workflow engines is complex and time consuming as each policy has to be individually ported to a particular workflow engine. In addition, choosing a particular scheduling policy is difficult, as factors like machine availability, workload, and communication volume between tasks are difficult to predict. In this paper, we describe SchedFlow, a tool that integrates scheduling policies into workflow engines such as Taverna, DAGMan or Karajan. We show how SchedFlow was used to take advantage of different\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "TAUoverMRNet (ToM): A framework for scalable parallel performance monitoring\n", "abstract": " TAU Performance System Sameer Shende, Allen D. Malony University of Oregon {sameer, malony}@cs.uoregon.edu Workshop Jan 9 Page 1 Aroon Nataraj, Allen D. Malony, Alan Morris University of Oregon {anataraj,malony,amorris}@cs.uoregon.edu TAUoverMRNet (ToM): A Framework for Scalable Parallel Performance Monitoring Dorian C. Arnold, Barton P. Miller University of Wisconsin, Madison dorian.arnold@gmail.com bart@cs.wisc.edu Page 2 TAUoverMRNet (ToM) STHEC 2008, Kos, Greece Motivation \u2752 Performance problem analysis increasingly complex \u274d Multi-core, heterogeneous, and extreme scale computing \u2752 Shift of performance measurement and analysis perspective \u274d Static, offline dynamic, online \u274d Support for performance monitoring (measurement + query) \u274d Enabling of adaptive applications \u2752 Prerequisites for performance measurement \u274d Low overhead and low perturbation \u274d Runtime \u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "The effects of metadata corruption on nfs\n", "abstract": " Distributed file systems need to be robust in the face of failures. In this work, we study the failure handling and recovery mechanisms of a widely used distributed file system, Linux NFS. We study the behavior of NFS under corruption of important metadata through fault injection. We find that the NFS protocol behaves in unexpected ways in the presence of these corruptions. On some occasions, incorrect errors are communicated to the client application; inothers, the system hangs applications or crashes outright; in a few cases, success is falsely reported when an operation has failed. We use the results of our study to draw lessons for future designs and implementations of the NFS protocol.", "num_citations": "4\n", "authors": ["1027"]}
{"title": "A loop-aware search strategy for automated performance analysis\n", "abstract": " Automated online search is a powerful technique for performance diagnosis. Such a search can change the types of experiments it performs while the program is running, making decisions based on live performance data. Previous research has addressed search speed and scaling searches to large codes and many nodes. This paper explores using a finer granularity for the bottlenecks that we locate in an automated online search, i.e., refining the search to bottlenecks localized to loops. The ability to insert and remove instrumentation on-the-fly means an online search can utilize fine-grain program structure in ways that are infeasible using other performance diagnosis techniques. We automatically detect loops in a program\u2019s binary control flow graph and use this information to efficiently instrument loops. We implemented our new strategy in an existing automated online performance tool, Paradyn\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Deep Start: a hybrid strategy for automated performance problem searches\n", "abstract": " To attack the problem of scalability of performance diagnosis tools with respect to application code size, we have developed the Deep Start search strategy\u2014a new technique that uses stack sampling to augment an automated search for application performance problems. Our hybrid approach locates performance problems more quickly and finds performance problems hidden from a more straightforward search strategy. The Deep Start strategy uses stack samples collected as a by\u2010product of normal search instrumentation to select deep starters, functions that are likely to be application bottlenecks. With priorities and careful control of the search refinement, our strategy gives preference to experiments on the deep starters and their callees. This approach enables the Deep Start strategy to find application bottlenecks more efficiently and more effectively than a more straightforward search strategy. We implemented\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Using cost to control instrumentation overhead\n", "abstract": " We present a new data collection cost system that provides programmers with feedback about the impact data collection is having on their application. We allow programmers to define the level of perturbation their application can tolerate, and then regulate the amount of instrumentation to ensure that the threshold is not exceeded. Our approach is unique in that the type of data gathered remains constant; instead, we regulate when data are collected. This permits programmers to trade speed of isolation of a performance problem for less application perturbation. We implemented this cost system in the Paradyn Performance Tools and present several case studies demonstrating the accuracy of the cost system.", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Performance of the new vertex detector at SLD\n", "abstract": " During the past year, the SLD collaboration completed the construction and began the operation of a new vertex detector (VXD3) employing 307 million pixels. This detector, based on 96 CCDs of 13 cm/sup 2/ area each, is an upgrade of the original vertex detector of SLD (VXD2), made possible by advances in the technology of CCD detectors. Its improved impact parameter resolution, larger solid angle coverage and virtually error-free track linking will enhance the SLD measurement of the polarization-enhanced forward-backward asymmetry for b and c-quarks, increase the precision of the measurement of the b-fraction in hadronic Z decays, and open the possibility to observe B/sub s//sup 0/-mixing. Full separation of primary, secondary and tertiary vertices is accessible. A description of the mechanics and electronics of VXD3 are presented along with results from the first data.", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Specification and verification of network managers for large internets\n", "abstract": " Large internet environments are increasing the difficulty of network management. Integrating increasing numbers of autonomous subnetworks (each with an increasing number of hosts) makes it more difficult to determine if the network managers of the subnetworks will interoperate correctly. We propose a high level, formal specification language, NMSL, as an aid in solving this problem. NMSL has two aspects of operation, a descriptive aspect and a prescriptive aspect. In its descriptive aspect, NMSL specifies abstractions of the network components and their instantiations, and verifies the consistency of such a specification. The abstractions include the data objects and processes in a network management system. These abstractions are instantiated on network elements. Network elements are grouped together in the specification of domains of administration. An extension mechanism is provided to allow for the\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Distributed upcalls: a mechanism for layering asynchronous abstractions\n", "abstract": " A design for structuring asynchronous upward calls, called upcalls, was described by Clark [2]. Upcalls allow a programmer to specify, for each layer in a system, a procedure that will be called by a lower layer in response to asynchronous events. Upcalls are implemented between layers that reside in the same address space. This paper describes a design for distributed upcalls, a mechanism for propagating upcalls across address space boundaries. Distributed upcalls extend the programmer's flexibility in using layers, and allow asynchronous actions to propagate upwards through any of these layers\u2013in the server's address space and then in the client's. Distributed upcalls provide a natural complement to remote pro-cedure calls.We have implemented a server structuring system called CLAM [3]. CLAM allows clients to dynamically load new layers (object modules) in the server and then access these modules\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "LTC\n", "abstract": " Group \ufb01le operations are a new, intuitive idiom for tools and mid~ dleware-including parallel debuggers and runtimes, performance measure\u00bb ment and steering, and distributed resource management~ that require scala~ ble operations over large groups of distributed \ufb01les. The group \ufb01le operation idiom provides new semantics for using \ufb01le groups as operands in standard \ufb01le operations, thus eliminating costly iteration. A \ufb01le-based idiom promotes conciseness and portability, and eases adoption. With explicit semantics for aggregation of group data results, the idiom addresses a key scalability barrier. We have designed TBON~ FS, a new distributed \ufb01le system that provides scalable group \ufb01le operations by leveraging tree-based overlay networks (TBONs) for scalable distribution of group \ufb01le operation requests and aggregation of group status and data results. Using a prototype TBON-FS system, we have\u00a0\u2026", "num_citations": "4\n", "authors": ["1027"]}
{"title": "Data Reduction and Partitioning in an Extreme Scale GPU-Based Clustering Algorithm\n", "abstract": " The emergence of leadership-class systems with GPU-equipped nodes has the potential to vastly increase the performance of existing distributed applications. An increasing number of applications that are converted to run on these systems are reliant on algorithms that perform computations on spatial data. Algorithms that operate on spatial data, such as density-based clustering algorithms, present unique challenges in data partitioning and result aggregation when porting to extreme scale environments. These algorithms require that spatially-near data points are partitioned to the same node and that the output from individual nodes needs to be aggregated to ensure that relationships between partition boundaries are discovered. In the development of an extreme scale density-based clustering algorithm, called Mr. Scan, we leveraged the increased computational power provided by GPUs to overcome these challenges. Three main techniques allowed Mr. Scan to cluster 6.5 billion points from the Twitter dataset using 8,192 GPU nodes on Cray Titan in 7.5 minutes:(1) a data partitioner scheme that ensures correctness by using shadow regions to selectively duplicate computation between nodes to detect clusters that lie in-between partition boundaries,(2) a dense box algorithm that reduces the computational complexity of clustering dense spatial regions, and (3) a new tree based method for merging results with spatial dependencies that requires only a fraction of the intermediate results to produce an accurate final result. The pure computational power of the GPUs allowed us to effectively summarize the initial data, making the scalable use of\u00a0\u2026", "num_citations": "3\n", "authors": ["1027"]}
{"title": "From Continuous Integration to Continuous Assurance\n", "abstract": " Continuous assurance extends the concept of continuous integration into the software assurance space. The goal is to naturally integrate the security assessment of software into the software development workflow. The Software Assurance Marketplace (SWAMP) [1] was established to support continuous assurance, helping to simplify and automate the process of running code analysis tools, especially static code analysis (SCA) tools. We describe how the SWAMP can be integrated easily into the continuous assurance workflow, providing direct access from integrated development environments (IDEs) such as Eclipse, source code management systems such as git and Subversion, and continuous integration systems such as Jenkins.", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Addressing the cyber-security of Maritime shipping\n", "abstract": " Attacks on software occur world-wide on a daily basis targeting individuals, corporations and governments alike. The computer systems that control maritime shipping are at risk from serious disruptions, and these disruptions can stem from vulnerabilities in the software and processes used in these computer systems. These vulnerabilities leave such information systems open to cyber-attack. Disruption of those systems could have disastrous consequences at worldwide level.", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Vulnerability assessment of open source wireshark and chrome browser\n", "abstract": " The objective of this effort was to conduct an in-depth vulnerability assessment of the Wireshark network protocol monitoring system. An in-depth assessment using First Principles Vulnerability Assessment FPVA methodology was performed that produced architectural, resource, privilege and trust analyses of the code, which, in turn, identified several verified security vulnerabilities. In addition, a similar analysis on the Google ChromeChromium web browser was performed, producing similar products and a vulnerability report. Other accomplishments included new work on tools to speed the task of analyst-driven vulnerability assessment of code, new techniques for statically analyzing source code for defects, and useful collaborations with industry and academia.Descriptors:", "num_citations": "3\n", "authors": ["1027"]}
{"title": "FINAL: Flexible and scalable composition of file system name spaces\n", "abstract": " Group file operations enable tools and middleware to operate upon a large group of files located across thousands of independent servers in a scalable fashion, a necessary requirement for effective use of today's largest distributed systems. Our initial prototype of group file operations showed scalability benefits for several tools, but also revealed the importance of having a scalable method for defining useful groups. We have developed a language called FINAL for describing name space composition in a flexible and scalable manner. Clients of our TBON-FS distributed file system can use this language to compose a single-system image (SSI) name space that automatically creates useful groups in a scalable fashion. We provide many examples of traditional and SSI name space compositions that can be described using FINAL, and report how TBON-FS can compose a global name space from tens of thousands\u00a0\u2026", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Program development for extreme-scale computing\n", "abstract": " Portal - Seminar 10181 Dagstuhl Seminar Proceedings 10181 Program Development for Extreme-Scale Computing J. Labarta, BP Miller, B. Mohr, M. Schulz (Eds.) published by LZI Host ISSN 1862 - 4405 Dagstuhl Seminar 10181, 02.05. - 07.05.2010 Additional Information Seminar Homepage License Search Publication Server Authors Labarta, Jesus Miller, Barton P. Mohr, Bernd Schulz, Martin 10181 Abstracts Collection -- Program Development for Extreme-Scale Computing Authors: Labarta, Jesus ; Miller, Barton P. ; Mohr, Bernd ; Schulz, Martin Abstract | Document (192 KB) | BibTeX 10181 Executive Summary -- Program Development for Extreme-Scale Computing Authors: Labarta, Jesus ; Miller, Barton P. ; Mohr, Bernd ; Schulz, Martin Abstract | Document (120 KB) | BibTeX DROPS-Home | Imprint | Privacy Published by LZI \u2026", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Binary code patching: An ancient art refined for the 21st century\n", "abstract": " \u2022 Only one or a few platforms supported.\u2022 Often requires \u201cmanaged\u201d code.\u2022 Scarcity of tools. Examples of use:\u2022 Performance profiling.\u2022 Tracing: addresses, basic blocks, functions...\u2022 Sandboxing: modifying a binary to restrict its behavior (IDS).\u2022 Simulation: intercepting operations to simulate different ones.\u2022 Memory debugging: checking dynamic allocate/free and memory references for correct operation.", "num_citations": "3\n", "authors": ["1027"]}
{"title": "User-defined reductions for communication in data-parallel languages\n", "abstract": " Parallel programming and parallel computers, have been a gleam in the eye of computer science for three or four decades. Rapid advances in semiconductor technology have led to high-performance, low-cost microprocessors that are appropriate components for a parallel machine. Unfortunately, this progress has left parallel software far behind. The difficulty of programming parallel computers is now, by far, the largest obstacle to their widespread use.Improved parallel programming languages could reduce the difficulty of programming parallel computers by making programs less error prone and less machine specific. One promising approach is data-parallel languages, such as HPF [9], C*[18], or NESL [2], which provide high-level abstractions for the three key facets of parallel programming: concurrency, synchronization, and communication. In these languages, programmers express parallelism by invoking a\u00a0\u2026", "num_citations": "3\n", "authors": ["1027"]}
{"title": "The Paradyn Parallel Performance Tools and PVM\n", "abstract": " Paradyn is a performance tool for large-scale parallel applications. By using dynamic instrumentation and automating the search for bottlenecks, it can measure long running applications on production-sized data sets Paradyn has recently been ported to measure native PVM applications.Programmers run their unmodified PVM application programs with Paradyn. Paradyn automatically inserts and modifies instrumentation during the execution of the application, systematically searching for the causes of performance problems. In most cases, Paradyn can isolate major causes of performance problems, and the part of the program that is responsible the problem. Paradyn currently runs on the Thinking Machine CM-5, Sun workstations, and PVM (currently only on Suns). It can measure heterogeneous programs across any of these platforms.", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Optimal Resource Allocation and Binding of Non-Pipelined Designs\n", "abstract": " In this paper we give an integer linear program (ILP) formulation of the resource allocation and binding problem of high-level synthesis. Given a behavioral specification and a time-step schedule for operations, the formulation minimizes the number of wiring nets and multiplexers used in the design. This is the first time that an ILP model for minimizing multiplexers and wiring nets has been mathematically formulated and optimally solved. The model handles chaining, multi-cycle operations and tradeoffs chip area between wiring and resources.", "num_citations": "3\n", "authors": ["1027"]}
{"title": "\u2018IPS User\u2019s Guide,\u2019\u2019\n", "abstract": " IPS [3] is an interactive, trace driven performance measurement system for parallel and distributed programs. It runs on DECstation, Sun, Vax, Sequent Symmetry, and Cray Y-MP computers (or a heterogeneous combination of these computers). IPS provides a large amount of performance data about the execution of a parallel program, and this information is organized hierarchically so that access to it is easy and intuitive. Performance data is organized hierarchically from the whole application down to particular procedures inside the application. This model guides the user in finding performance bottle necks in their applications. Multiple applications (or multiple runs of the same application) can be measured in a single IPS session. Many performance metrics are available from IPS and most of these metrics can be presented in simple tables, profiles, or in graphs plotting the metric over time. These metrics include elapsed CPU time, synchronization blocking time (for both messages and busy waiting), I/O rates, procedure calls, and message rates. IPS also includes high-level analyses, such as Critical Path analysis [4], to help the user more quickly find performance bottlenecks. IPS has open interfaces to allow easy support of new sources of performance data and new visualization modules. External performance data can include data collected from the operating system, network, and hardware. The external data facility is designed so that it is a simple task for users to add new metrics and modify old ones. Similarly, it is a simple task to add new types of visualizations. There are two parts to IPS, an X Window based user interface called the master\u00a0\u2026", "num_citations": "3\n", "authors": ["1027"]}
{"title": "Identifying and (automatically) remedying performance problems in CPU/GPU applications\n", "abstract": " GPU accelerators have become common on today's leadership-class computing platforms. Effective exploitation of the additional parallelism offered by GPUs is fraught with challenges. A key performance challenge faced by developers is how to limit the time consumed by synchronizations between the CPU and GPU. We introduce the extended feed-forward measurement (FFM) performance tool that provides an automated detection of synchronization problems, identifies if the synchronization problem is a component of a larger construct that exhibits a problem beyond an individual synchronization operation, identifies remedies that can correct the issue, and in some cases automatically applies remedies to problems exhibited by larger constructs. The extended FFM performance tool identifies three causes of unnecessary synchronizations: a problem caused by a single operation, a problem caused by memory\u00a0\u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Structured random differential testing of instruction decoders\n", "abstract": " Decoding binary executable files is a critical facility for software analysis, including debugging, performance monitoring, malware detection, cyber forensics, and sandboxing, among other techniques. As a foundational capability, binary decoding must be consistently correct for the techniques that rely on it to be viable. Unfortunately, modern instruction sets are huge and the encodings are complex, so as a result, modern binary decoders are buggy. In this paper, we present a testing methodology that automatically infers structural information for an instruction set and uses the inferred structure to efficiently generate structured-random test cases independent of the instruction set being tested. Our testing methodology includes automatic output verification using differential analysis and reassembly to generate error reports. This testing methodology requires little instruction-set-specific knowledge, allowing rapid testing\u00a0\u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Binary code multi-author identification in multi-toolchain scenarios\n", "abstract": " Knowing the authors of a binary program has significant application to forensic analysis of malicious software (malware), software supply chain risk management, and software plagiarism detection. Since modern software is typically the result of team efforts and there are multiple compilers and optimization levels available to use, it is essential to be able to reliably identify multiple authors across multiple toolchains. However, existing multi-author identification studies were only evaluated with code compiled by a single toolchain, leaving open the question whether existing techniques can work in multi-toolchain scenarios. In this paper, we explore how toolchains impact programming style at real-world scale and present new techniques for multi-author identification in multi-toolchain scenarios. We show that existing techniques do not work well on real world code in multi-toolchain scenarios. Instead of manually designing code features, we apply deep learning to automatically extract features from the code. Our techniques can achieve 71% accuracy, and 82% top-5 accuracy for classifying 700 authors, which can greatly help analysts prioritize their investigation. In addition to the new authorship identification results, we provide an in-depth analysis of our learning models and our results. This analysis is essential since authorship identification is not a task easily accomplished by a person. For example, our results show a counterintuitive phenomena: unoptimized code is more difficult to attribute than optimized code. Our investigation shows strong evidence that compiler optimizations such as function inlining can help learning authorship style.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Building on Lessons Learned From Over a Decade of MRNet Research and Development\n", "abstract": " Building on Lessons Learned from over a Decade of MRNet Research and Development Page 1 Building on Lessons Learned from over a Decade of MRNet Research and Development (SC2003 \u21e8 SC2013) Barton P. Miller University of Wisconsin bart@cs.wisc.edu Dorian C. Arnold (UNM), Michael J. Brim (ORNL), Philip C. Roth (ORNL), Evan Samanas (UW), Benjamin Welton (UW), Bill Williams (UW) Page 2 \u00a9 Barton P. Miller 2012 2 \u201cI think that I shall never see A poem lovely as a tree.\u201d Trees by Joyce Kilmer (1919) If you can formulate the problem so that it is hierarchically decomposed, you can probably make it run fast. \u201cI think that I shall never see An algorithm lovely as a tree.\u201d Trees by Joyce Kilmer (1919) 25And Moses chose able men out of all Israel, and made them heads over the people, rulers of thousands, rulers of hundreds, rulers of fifties, and rulers of tens. \u05d4\u05db \u05d9\u05b5\u05e9\u05c1\u05b0\u05e0\u05d0\u05b7 \u05d4\u05b6\u05e9\u05c1\u05b9\u05de \u05e8\u05b7\u05d7\u05b0\u05d1\u05b4\u05d9\u05bc\u05b7\u05d5 \u05dc\u05b8\u05db\u05bc\u05b4\u05de \u05dc\u05b4\u05d9\u05b7\u05d7 \u05dc\u05b5\u05d0 \u05b8\u05e8\u05b0\u05e9\u05c2\u05b4\u05d9 \u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Increasing Automated Vulnerability Assessment Accuracy on Cloud and Grid Middleware\n", "abstract": " The fast adaptation of Cloud computing has led to an increase in novel information technology threats. The targets of these new threats range from large scale distributed system, such as the Large Hadron Collider by the CERN, to industrial (water, power, electricity, oil, gas, etc.) distributed systems, i.e. SCADA systems. The use of automated tools for vulnerability assessment is quite attractive, but while these tools can find common problems in a program\u2019s source code, they miss a significant number of critical and complex vulnerabilities. In addition, middleware systems frequently base their security on mechanisms such as authentication, authorization, and delegation. While these mechanisms have been studied in depth and can control key resources, they are not enough to assure that all application\u2019s resources are safe. Therefore, security of distributed systems have been placed under the watchful eye of\u00a0\u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Vulnerability assessment enhancement for middleware\n", "abstract": " Security on Grid computing is often an afterthought. However assessing security of middleware systems is of the utmost importance because they manage critical resources owned by different organizations. To fulfill this objective we use First Principles Vulnerability Assessment (FPVA), an innovative analystic-centric (manual) methodology that goes beyond current automated vulnerability tools. FPVA involves several stages for characterizing the analyzed system and its components. Based on the evaluation of several middleware systems, we have found that there is a gap between the initial and the last stages of FPVA, which is filled with the security practitioner expertise. We claim that this expertise is likely to be systematically codified in order to be able to automatically indicate which, and why, components should be assessed. In this paper we introduce key elements of our approach: Vulnerability graphs, Vulnerability Graph Analyzer, and a Knowledge Base of security configurations.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Vulnerability assessment enhancement for middleware for computing and informatics\n", "abstract": " Security on Grid computing is often an afterthought. However assessing security of middleware systems is of the utmost importance because they manage critical resources owned by different organizations. To fulfill this objective we use First Principles Vulnerability Assessment (FPVA), an innovative analystic-centric (manual) methodology that goes beyond current automated vulnerability tools. FPVA involves several stages for characterizing the analyzed system and its components. Based on the evaluation of several middleware systems, we have found that there is a gap between the initial and the last stages of FPVA, which is filled with the security practitioner expertise. We claim that this expertise is likely to be systematically codified in order to be able to automatically indicate which, and why, components should be assessed. In this paper we introduce key elements of our approach: Vulnerability graphs, Vulnerability Graph Analyzer, and a Knowledge Base of security configurations.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Body image perception in athletes versus non-athletes\n", "abstract": " The purpose of this study was to find if there was a relationship when measuring body image perception between athletes and non-athletes.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "A Framework for Binary Code Analysis, and Static and Dynamic Patching\n", "abstract": " A Framework for Binary Code Analysis, and Static and Dynamic Patching Page 1 Binary Code Analysis and Editing \u00a9 2007 Barton P. Miller July 2007 A Framework for Binary Code Analysis, and Static and Dynamic Patching Barton P. Miller University of Wisconsin bart@cs.wisc.edu Page 2 \u2013 2 \u2013 Binary Code Analysis and Editing \u00a9 2007 Barton P. Miller Motivation \u2022 Multi-platform \u2022 Open architecture \u2022 Extensible \u2022 Open source \u2022 Testable \u2022 Suitable for batch processing \u2022 Accurate \u2022 Efficient \u25aa Binary code analysis is a basic tool of security analysts, application developers, system designers and tool developers. \u25aa We are designing and building a new foundation to support such analysis. \u25aa Existing binary analysis tools have significant limitations. Page 3 \u2013 3 \u2013 Binary Code Analysis and Editing \u00a9 2007 Barton P. Miller Why Binary Code? \u25aa Access to the source code often is not possible: \u2022 Proprietary software packages. \u2022 \u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "A tool for converting Linux device drivers into Solaris compatible binaries\n", "abstract": " The Linux operating system is quickly becoming a standard, attracting a wide user community and supporting a broad variety of applications and devices. Other vendors, such as Sun, have provided Linux\u2010compatible system call interfaces to their kernels, but are constrained by the lack of device support. To address this problem, we present a system (called PITS) to build device drivers, in this case for Solaris x86, from Linux source code. To accomplish this goal, we designed tools and Linux kernel emulation code to handle the myriad incompatibilities. These incompatibilities require the ability to resolve symbol conflicts, emulate internal Linux kernel data structures, handle module initialization, and generate module dependencies. With our method, we show that converting Linux device drivers is possible, but has a few technical difficulties. Issues arise with sparse documentation, external user interfaces, and\u00a0\u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Western women doctors in an Eastern land: the work of female medical missionaries in India, 1870-1947\n", "abstract": " Western women doctors in an Eastern land : the work of female medical missionaries in India, 1870-1947 Toggle navigation Repositorio Institucional de la UIA Le\u00f3n English espa\u00f1ol English English espa\u00f1ol Login Toggle navigation View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item Western women doctors in an Eastern land : the work of female medical missionaries in India, 1870-1947 Thumbnail Date 2012-06-11 Author Cowan, Catherine N. Metadata Show full item record URI https://repositorio.leon.uia.mx/xmlui/123456789/45242 Collections University of Wisconsin, Madison DSpace software copyright \u00a9 2002-2016 DuraSpace Contact Us | Send \u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Slack: A New Performance Metric for Parallel Programs\n", "abstract": " Abstract Critical Path Profiling is a technique that provides guidance to help programmers try to improve the running time of their program. However, Critical Path Profiling provides only an upper bound estimate of the improvement possible in a parallel program execution. In this paper, we present a new metric, called Slack, to complement Critical Path and provide additional information to parallel programmers about the potential impact of making improvements along the critical path.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Summary of ACM/ONR workshop on parallel and distributed debugging\n", "abstract": " Allen Malony (Center for Supercomputing Research and Development) described models for performance perturbation analysis. When collecting performance information, perturbations introduce errors into the performance measurements. Perturbation analysis subtracts out the effects of perturbation from the measured traces, ideally allowing error-free performance information to be recovered. Two models, time-based and event-based, were described. The time-based model assumes that all parallel threads of execution are independent in that they do not affect each other (either via synchronization or shared data). Under this assumption, you can simply subtract the time overhead incurred by all instrumentation points. Once perturbations are subtracted out, the concurrent thread with the longest resulting execution time is assumed to comprise the actual critical path. The event-based model relaxes the\u00a0\u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Breaking bounds: the Brigadas Femeninas of the Cristero rebellion\n", "abstract": " Breaking bounds : the Brigadas Femeninas of the Cristero rebellion Toggle navigation Login Toggle navigation View Item MINDS@UW Home MINDS@UW Madison University of Wisconsin-Madison Libraries UW-Madison Closed Dissertations and Theses View Item MINDS@UW Home MINDS@UW Madison University of Wisconsin-Madison Libraries UW-Madison Closed Dissertations and Theses View Item Breaking bounds : the Brigadas Femeninas of the Cristero rebellion File(s) 35759559.pdf (5.409Mb) Date 1988 Author Findlay, Eileen J. Metadata Show full item record Permanent Link http://digital.library.wisc.edu/1793/28662 Part of UW-Madison Closed Dissertations and Theses Contact Us | Send Feedback Search MINDS@UW This Collection Browse All of MINDS@UWCommunities & CollectionsBy Issue DateAuthorsTitlesSubjectsThis CollectionBy Issue DateAuthorsTitlesSubjects My Account LoginRegister \u2026", "num_citations": "2\n", "authors": ["1027"]}
{"title": "A Technique for Manuscript Collection Development Analysis\n", "abstract": " To better focus manuscript collection activity, staff of the State Archives and Historical Research Library at the State Historical Society of North Dakota have developed a technique for manuscript collection development analysis. This technique employs a system to assess subject content and research values of manuscript collections and includes a statistical analysis method to determine various collection attributes. The results of the analysis are data indicating collection strengths and collection development needs. With this information archivists are better able to build and strengthen a manuscript collection.", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Frequency Domain Analysis of AC Dipole-dipole Electromagnetic Soundings\n", "abstract": " The subject of this report is the development and application of frequency domain techniques for the analysis of AC dipole-dipole electromagnetic soundings of the earth. In chapter I, the AC dipole-dipole apparent resistivity is defined and its nature in terms of electromagnetic waves within the earth is explored as a function of frequency and dipole separation. The relation of AC dipole-dipole apparent resistivity to", "num_citations": "2\n", "authors": ["1027"]}
{"title": "Parallel binary code analysis\n", "abstract": " Binary code analysis is widely used to help assess a program's correctness, performance, and provenance. Binary analysis applications often construct control flow graphs, analyze data flow, and use debugging information to understand how machine code relates to source lines, inlined functions, and data types. To date, binary analysis has been single-threaded, which is too slow for convenient use in performance tuning workflows where it is used to help attribute performance to complex applications with large binaries.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Poster: Deployment-quality and accessible solutions for cryptography code development\n", "abstract": " Cryptographic API misuses seriously threaten software security. Automatic screening of cryptographic misuse vulnerabilities has been a popular and important line of research over the years. However, the vision of producing a scalable detection tool that developers can routinely use to screen millions of line of code has not been achieved yet. Our main technical goal is to attain a high precision and high throughput approach based on specialized program analysis. Specifically, we design inter-procedural program slicing on top of a new on-demand flow-, context-and field-sensitive data flow analysis. Our current prototype named CryptoGuard can detect a wide range of Java cryptographic API misuses with a precision of 98.61%,, when evaluated on 46 complex Apache Software Foundation projects (including, Spark, Ranger, and Ofbiz). Our evaluation on 6,181 Android apps also generated many security insights\u00a0\u2026", "num_citations": "1\n", "authors": ["1027"]}
{"title": "The Trusted CI Vision for an NSF Cybersecurity Ecosystem And Five-year Strategic Plan (2019-2023).\n", "abstract": " The National Science Foundation (NSF) funds over 11,000 awards each year with an annual budget of about $7.5 billion. These awards, in aggregate, create a science and engineering research community whose size and diversity is unparalleled. Implementing appropriate cybersecurity across this NSF community\u2013in a way that is both effective in managing cybersecurity risks and enables this community to achieve its science objectives\u2013is a complex interdisciplinary research challenge. Adding to this challenge is the reality that NSF awards are hosted in institutions (eg, universities, research laboratories) that have their own diverse histories, policies, and risk tolerances with which the projects must comply.Trusted CI, the NSF Cybersecurity Center of Excellence (CCoE), has been working to overcome this 1 challenge for over five years. Its success has been noted both by the NSF community, with the 2017 Report from the NSF Large Facilities Cyberinfrastructure Workshop[1] citing it as a model for future NSF centers and the director of the NSF Office of Advanced Cyberinfrastructure referring to Trusted CI as \u201ca very innovative model in providing cybersecurity expertise to NSF large projects such as the NSF Facilities and has been extremely successful.\"[2](34: 26)", "num_citations": "1\n", "authors": ["1027"]}
{"title": "In-Depth Software Vulnerability Assessment of Container Terminal Systems\n", "abstract": " Attacks on software systems occur world-wide on a daily basis targeting individuals, corporations and governments alike. The systems that control maritime shipping are at risk of serious disruptions, and these disruptions can stem from vulnerabilities in the software and processes used in these systems. These vulnerabilities leave such information systems open to cyber-attack. Disruption of these systems could have disastrous consequences on a global scale.The assessment of the security of maritime shipping systems has faced two significant limitations. First, existing studies have been directed at identifying risks but have not taken the critical (and expensive) next step of actually identifying vulnerabilities present in these systems. Second, these studies have focused on overall port operations. While such an overview is important, and has resulted on overall recommendations for changes in policy, they have not provided an evaluation of security issues in the computer systems that control these ports and their terminals.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Benchmark Creation for Multi-Personality and Content-Aware Circuit Partitioning Algorithms\n", "abstract": " This report documents efforts to assemble and modify a set of benchmark circuits for testing a new class of circuit partitioning algorithms designed for heterogeneous FPGAs. Often, computations can be implemented using different types of resources within these devices; the new partitioning algorithms incorporate the circuit mapping step, where computations are mapped to specific resource types, into the partitioning algorithms themselves. We elaborate on the details of this new form of partitioning called ?multi-personality? partitioning. While remapping provides a great deal of flexibility to the partitioner to modify the implementation of circuit nodes in order to meet the desired partitioning criteria, testing this new partitioner requires large, heterogeneous netlists. In this work we develop a list of requirements for the needed benchmarks, investigate existing benchmarks to determine their suitability, and document how we adapt the chosen benchmarks for use in testing multi-personality partitioning algorithms. Finally, we also discuss initial efforts in assembling and developing a set of benchmarks for testing another new form of partitioning called ?content-aware? partitioning.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Mr. scan: A hybrid/hybrid extreme scale density based clustering algorithm\n", "abstract": " Density-based clustering algorithms are a widely-used class of data mining techniques that can find irregularly shaped clusters and cluster data without prior knowledge of the number of clusters the data contains. DBSCAN is the most well-known density-based clustering algorithm. We introduce our extension of DBSCAN, called Mr. Scan, which uses a hybrid/hybrid parallel implementation that combines the MRNet tree-based distribution network with GPU-equipped nodes. Mr. Scan avoids the problems encountered in other parallel versions of DBSCAN, such as scalability limits, reduction in output quality at large scales, and the inability to effectively process dense regions of data. Mr. Scan uses effective data partitioning and a new merging technique to allow data sets to be broken into independently processable partitions without the reduction in quality or large amount of node-to-node communication seen in other parallel versions of DBSCAN. The dense box algorithm designed as part of Mr. Scan allows for dense regions to be detected and clustered without the need to individually compare all points in these regions to one another. Mr. Scan was tested on both a geolocated Twitter dataset and image data obtained from the Sloan Digital Sky Survey. In testing Mr. Scan we performed end-to-end benchmarks measuring complete application run time from reading raw unordered input point data from the file system to writing the final clustered output to the file system. The use of end-to-end benchmarking gives a clear picture of the performance that can be expected from Mr. Scan in real world use cases. At its largest scale, Mr. Scan clustered 6.5\u00a0\u2026", "num_citations": "1\n", "authors": ["1027"]}
{"title": "A path to operating system and runtime support for extreme scale tools\n", "abstract": " In this project, we cast distributed resource access as operations on files in a global name space and developed a common, scalable solution for group operations on distributed processes and files. The resulting solution enables tool and middleware developers to quickly create new scalable software or easily improve the scalability of existing software. The cornerstone of the project was the design of a new programming idiom called group file operations that eliminates iterative behavior when a single process must apply the same set of file operations to a group of related files. To demonstrate our novel and scalable ideas for group file operations and global name space composition, we developed a group file system called TBON-FS that leverages a tree-based overlay network (TBON), specifically MRNet, for logarithmic communication and distributed data aggregation. We also developed proc++, a new synthetic file system co-designed for use in scalable group file operations. Over the course of the project, we evaluated the utility and performance of group file operations, global name space composition, TBON-FS, and proc++ in three case studies. The first study focused on the ease in using group file operations and TBON-FS to quickly develop several new scalable tools for distributed system more\u00bb", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Software Analysis Techniques to Approximate Data Centric Direct Measurements\n", "abstract": " Data centric analysis using direct measurements has been established as a successful performance analysis technique. The information gathered with this technique can be used to address data locality problems and other issues. Existing approaches rely on special hardware support which is needed to negate a \u2018skid\u2019factor. Our approach is viable on hardware where the skid factor is an issue. Prior methods also rely on maintaining runtime information about memory allocation addresses for variables, which may lead to program perturbation. Our approach uses software analysis to eliminate the need for maintaining allocation and free records. We show that by using heuristics our technique can attribute data centric values to program variables and maintain the approximate rank-order found by using traditional techniques.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "\" Breaking away\"--the effect of non-uniform pacing on power output and RPE growth\n", "abstract": " The purpose of this study was to determine the effects of a short burst in power output midway through a 10 km cycle time trial and its relationship to RPE growth. We hypothesized that a break-away effort in non-normal pacing will cause a disruption in normal pacing strategy. However, a return to the cyclists pre-determined template would occur prior to completion of the time trial. Ten subjects performed a maximal incremental cycling test followed by a total of four 10 km cycling time trials. The first two trials were to habituate the subjects and to establish a baseline performance. The final two trials were randomized and consisted of a spontaneously paced 10 km cycling time trial and a 10 km cycling time trial during which subjects were required to increase velocity (maximum effort) for 1 km. The results of the study indicate that a burst in power output during a 10 km time trial does have an effect on overall power output and RPE growth. During a breakaway effort, power output was significantly increased compared to the no burst time trial. RPE growth was significantly faster during the burst time trial compared to the no burst time trial.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Double-crested cormorants on Southern Green Bay: ecology and food habits with emphasis on the yellow perch fishery, dietary overlap with smympatric American white pelicans, and\u00a0\u2026\n", "abstract": " Double-crested cormorants on Southern Green Bay : ecology and food habits with emphasis on the yellow perch fishery, dietary overlap with smympatric American white pelicans, and seasonal contaminant uptake Toggle navigation Repositorio Institucional de la UIA Le\u00f3n English espa\u00f1ol English English espa\u00f1ol Login Toggle navigation View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item Double-crested cormorants on Southern Green Bay : ecology and food habits with emphasis on the yellow perch fishery, dietary overlap with smympatric American white pelicans, and seasonal contaminant uptake Thumbnail Date 2012-06-22 Author Meadows, Sarah \u2026", "num_citations": "1\n", "authors": ["1027"]}
{"title": "The optimal control of ice-storage air-conditioning systems\n", "abstract": " An ice-storage air-conditioning system involves making ice at night and melting the ice for cooling during the day. The driving force in using an ice-storage system is that some electric companies charge a fraction of the daytime rate for usage during the night to try to reduce peak demand. By reducing peak demand, the utility may not need to fire up a less efficient gas fired generator and possibly not have to build more power plants. An additional incentive is often implemented in the form of utility rebate on the first cost of the ice system.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "The Integration of Application and System Based Metrics in a Parallel Program Performance Tools\n", "abstract": " The IPS-2 parallel program measurement tools provide performance data from application programs, the operating system, hardware, network, and other sources. Previous versions of IPS-2 allowed programmers to collect information about an application based only on what could be collected by software instrumentation inserted into the program (and system call libraries). We have developed an open interface, called the \u2018\u2018external time histogram\u2019\u2019, providing a graceful way to include external data from many sources. The user can tell IPS-2 of new sources of performance data through an extensible metric description language. The data from these external sources is automatically collected when the application program is run. IPS-2 provides a library to simplify constructing the external data collectors.The new version of IPS-2 can measure sharedmemory and message-passing parallel programs running on a heterogeneous collection of machines. Data from C or Fortran programs, and data from simulations can be processed by the same tool. As a result of including the new external performance data, IPS-2 now can report on a whole new set of performance problems. We describe the results of using IPS-2 on two real applications: a shared-memory database join utility, and a multi-processor interconnection network simulator. Even though these applications previously went through careful tuning, we were able to precisely identify performance problems and extract additional performance improvements of about 30%.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Verification of Network Management System Configurations\n", "abstract": " This paper presents a model for network management systems, and a method for verifying specifications of these systems. We divide the verification problem into three parts: capacity, protection, and configuration verification. Capacity verification determines if the processes in the network management system are configured to handle the load that their clients place on them. Protection verification determines if access permissions are being violated. Configuration verification determines if other general requirements on the specification are being met. We also provide a way to distribute the verification process, and a way to summarize information that needs to be propagated across domain boundaries. We discuss the performance of our implementation of this system, and describe our future research directions.", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Implementation of a Visual UNIX Process Connector\n", "abstract": " ABSTRACT UPCONN is a tool used by a programmer to visually describe the connections between the processes in a distributed program and to execute the distributed program. With UPCONN, the implementation of processes in a program is separated from the description of the connections between them. The programmer is provided with a screen editor which enables processes to be described and allows the connections between these processes to be specified. When a distributed program is described, UPCONN allows the user to execute the program or save the program for later use.UPCONN has a modular design which makes it easily extendible and allows its parts to be used independently. A library of processes and procedures is included to perform specialized tasks, such as message monitoring and file access. The library provides a method for adding new features to UPCONN. Several existing\u00a0\u2026", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Further Optimism in Optimistic Methods of Concurrency Control\n", "abstract": " \u00d7 MINDS@ UW policy documentation links are not currently working. MINDS@ UW documentation can be found at the following link in the meantime: https://www. library. wisc. edu/research-support/minds/getting-started/", "num_citations": "1\n", "authors": ["1027"]}
{"title": "Impingement and entrainment of fishes at Dairyland Power Cooperative's Genoa site\n", "abstract": " The impact to the fishery of Navigation Pool No. 9 of the Mississippi River from impingement and entrainment processes by two Dairyland Power Cooperative power stations in Genoa, Wisconsin, was determined. Impingement samples were taken once each week for a 24 hour period from 8 August 1978 through 30 June 1980 at both plants. Sampling devices, comprised of galvanized hardware cloth, were used to strain screen wash water to determine impingement rates. Entrainment sampling was conducted once each week for a 24 hour period from 26 February 1979 through 10 September 1979 and 6 February 1980 through 30 June 1980. Entrainment rates were determined by straining water from tapped intake pipes with plankton nets. Impingement survival tests, stress tests, and ichthyoplankton surveys were also taken to further define the impact of entrainment and impingement.   An estimated 8,390 (127.1 kg) and 54,349 (2915.7 kg) fish were impinged during this study at La Crosse Boiling Water Reactor. (LACBWR)and Genoa #3 (G-3), respectively. The two power stations collectively impinged representatives of 53 species and 19 families of fish. The most frequently impinged species of fish were bluegill (Lepomis macrochirus), freshwater drum (Aplodinotus grunniens)., gizzard shad(Dorosoma cepedianum) and channel catfish (Ictalurus punctatus). Young-of-the-year fish density, behavioral responses of some species of fish and intake structure design were determined to be the most influential factors causing impingement at these plants. The impact to the Pool 9 fishery from impingement by these two power stations was judged\u00a0\u2026", "num_citations": "1\n", "authors": ["1027"]}
{"title": "WG Bleyer and the development of journalism education\n", "abstract": " Contents include: the youth, student, English teacher; curriculum beginnings; press bureau, public relations; curriculum growth under Bleyer; teacher organization, publisher cooperation; elevation of journalism as a profession; and end of a career.", "num_citations": "1\n", "authors": ["1027"]}