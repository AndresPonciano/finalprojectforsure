{"title": "Feedback effects between similarity and social influence in online communities\n", "abstract": " A fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties. People are similar to their neighbors in a social network for two distinct reasons: first, they grow to resemble their current friends due to social influence; and second, they tend to form new links to others who are already like them, a process often termed selection by sociologists. While both factors are present in everyday social processes, they are in tension: social influence can push systems toward uniformity of behavior, while selection can lead to fragmentation. As such, it is important to understand the relative effects of these forces, and this has been a challenge due to the difficulty of isolating and quantifying them in real settings.", "num_citations": "818\n", "authors": ["2087"]}
{"title": "A model of computation for mapreduce\n", "abstract": " In recent years the MapReduce framework has emerged as one of the most widely used parallel computing platforms for processing data on terabyte and petabyte scales. Used daily at companies such as Yahoo!, Google, Amazon, and Facebook, and adopted more recently by several universities, it allows for easy parallelization of data intensive computations over many machines. One key feature of MapReduce that differentiates it from previous models of parallel computation is that it interleaves sequential and parallel computation. We propose a model of efficient computation using the MapReduce paradigm. Since MapReduce is designed for computations over massive data sets, our model limits the number of machines and the memory per machine to be substantially sublinear in the size of the input. On the other hand, we place very loose restrictions on the computational power of of any individual machine\u00a0\u2026", "num_citations": "610\n", "authors": ["2087"]}
{"title": "Inferring social ties from geographic coincidences\n", "abstract": " We investigate the extent to which social ties between people can be inferred from co-occurrence in time and space: Given that two people have been in approximately the same geographic locale at approximately the same time, on multiple occasions, how likely are they to know each other? Furthermore, how does this likelihood depend on the spatial and temporal proximity of the co-occurrences? Such issues arise in data originating in both online and offline domains as well as settings that capture interfaces between online and offline behavior. Here we develop a framework for quantifying the answers to such questions, and we apply this framework to publicly available data from a social media site, finding that even a very small number of co-occurrences can result in a high empirical likelihood of a social tie. We then present probabilistic models showing how such large probabilities can arise from a natural\u00a0\u2026", "num_citations": "554\n", "authors": ["2087"]}
{"title": "Cooperation and contagion in web-based, networked public goods experiments\n", "abstract": " A longstanding idea in the literature on human cooperation is that cooperation should be reinforced when conditional cooperators are more likely to interact. In the context of social networks, this idea implies that cooperation should fare better in highly clustered networks such as cliques than in networks with low clustering such as random networks. To test this hypothesis, we conducted a series of web-based experiments, in which 24 individuals played a local public goods game arranged on one of five network topologies that varied between disconnected cliques and a random regular graph. In contrast with previous theoretical work, we found that network topology had no significant effect on average contributions. This result implies either that individuals are not conditional cooperators, or else that cooperation does not benefit from positive reinforcement between connected neighbors. We then tested both of these possibilities in two subsequent series of experiments in which artificial seed players were introduced, making either full or zero contributions. First, we found that although players did generally behave like conditional cooperators, they were as likely to decrease their contributions in response to low contributing neighbors as they were to increase their contributions in response to high contributing neighbors. Second, we found that positive effects of cooperation were contagious only to direct neighbors in the network. In total we report on 113 human subjects experiments, highlighting the speed, flexibility, and cost-effectiveness of web-based experiments over those conducted in physical labs.", "num_citations": "491\n", "authors": ["2087"]}
{"title": "Ghost work: How to stop Silicon Valley from building a new global underclass\n", "abstract": " In the spirit ofNickel and Dimed, a necessary and revelatory expose of the invisible human workforce that powers the web--and that foreshadows the true future of work. Hidden beneath the surface of the web, lost in our wrong-headed debates about AI, a new menace is looming. Anthropologist Mary L. Gray and computer scientist Siddharth Suri team up to unveil how services delivered by companies like Amazon, Google, Microsoft, and Uber can only function smoothly thanks to the judgment and experience of a vast, invisible human labor force. These people doing\" ghost work\" make the internet seem smart. They perform high-tech piecework: flagging X-rated content, proofreading, designing engine parts, and much more. An estimated 8 percent of Americans have worked at least once in this\" ghost economy,\" and that number is growing. They usually earn less than legal minimums for traditional work, they have no health benefits, and they can be fired at any time for any reason, or none. There are no labor laws to govern this kind of work, and these latter-day assembly lines draw in--and all too often overwork and underpay--a surprisingly diverse range of workers: harried young mothers, professionals forced into early retirement, recent grads who can't get a toehold on the traditional employment ladder, and minorities shut out of the jobs they want. Gray and Suri also show how ghost workers, employers, and society at large can ensure that this new kind ofwork creates opportunity--rather than misery--for those who do it.", "num_citations": "490\n", "authors": ["2087"]}
{"title": "Counting triangles and the curse of the last reducer\n", "abstract": " The clustering coefficient of a node in a social network is a fundamental measure that quantifies how tightly-knit the community is around the node. Its computation can be reduced to counting the number of triangles incident on the particular node in the network. In case the graph is too big to fit into memory, this is a non-trivial task, and previous researchers showed how to estimate the clustering coefficient in this scenario. A different avenue of research is to to perform the computation in parallel, spreading it across many machines. In recent years MapReduce has emerged as a de facto programming paradigm for parallel computation on massive data sets. The main focus of this work is to give MapReduce algorithms for counting triangles which we use to compute clustering coefficients.", "num_citations": "476\n", "authors": ["2087"]}
{"title": "On graph problems in a semi-streaming model\n", "abstract": " We formalize a potentially rich new streaming model, the semi-streaming model, that we believe is necessary for the fruitful study of efficient algorithms for solving problems on massive graphs whose edge sets cannot be stored in memory. In this model, the input graph, G=(V, E), is presented as a stream of edges (in adversarial order), and the storage space of an algorithm is bounded by O (n\u00b7 polylog n), where n=| V|. We are particularly interested in algorithms that use only one pass over the input, but, for problems where this is provably insufficient, we also look at algorithms using constant or, in some cases, logarithmically many passes. In the course of this general study, we give semi-streaming constant approximation algorithms for the unweighted and weighted matching problems, along with a further algorithmic improvement for the bipartite case. We also exhibit log n/log log n semi-streaming approximations to\u00a0\u2026", "num_citations": "371\n", "authors": ["2087"]}
{"title": "An experimental study of the coloring problem on human subject networks\n", "abstract": " Theoretical work suggests that structural properties of naturally occurring networks are important in shaping behavior and dynamics. However, the relationships between structure and behavior are difficult to establish through empirical studies, because the networks in such studies are typically fixed. We studied networks of human subjects attempting to solve the graph or network coloring problem, which models settings in which it is desirable to distinguish one9s behavior from that of one9s network neighbors. Networks generated by preferential attachment made solving the coloring problem more difficult than did networks based on cyclical structures, and \u201csmall worlds\u201d networks were easier still. We also showed that providing more information can have opposite effects on performance, depending on network structure.", "num_citations": "321\n", "authors": ["2087"]}
{"title": "Filtering: a method for solving graph problems in mapreduce\n", "abstract": " The MapReduce framework is currently the de facto standard used throughout both industry and academia for petabyte scale data analysis. As the input to a typical MapReduce computation is large, one of the key requirements of the framework is that the input cannot be stored on a single machine and must be processed in parallel. In this paper we describe a general algorithmic design technique in the MapReduce framework called filtering. The main idea behind filtering is to reduce the size of the input in a distributed fashion so that the resulting, much smaller, problem instance can be solved on a single machine. Using this approach we give new algorithms in the MapReduce framework for a variety of fundamental graph problems for sufficiently dense graphs. Specifically, we present algorithms for minimum spanning trees, maximal matchings, approximate weighted matchings, approximate vertex and edge\u00a0\u2026", "num_citations": "280\n", "authors": ["2087"]}
{"title": "The crowd is a collaborative network\n", "abstract": " The main goal of this paper is to show that crowdworkers collaborate to fulfill technical and social needs left by the platform they work on. That is, crowdworkers are not the independent, autonomous workers they are often assumed to be, but instead work within a social network of other crowdworkers. Crowdworkers collaborate with members of their networks to 1) manage the administrative overhead associated with crowdwork, 2) find lucrative tasks and reputable employers and 3) recreate the social connections and support often associated with brick and mortar-work environments. Our evidence combines ethnography, interviews, survey data and larger scale data analysis from four crowdsourcing platforms, emphasizing the qualitative data from the Amazon Mechanical Turk (MTurk) platform and Microsoft's proprietary crowdsourcing platform, the Universal Human Relevance System (UHRS). This paper draws\u00a0\u2026", "num_citations": "209\n", "authors": ["2087"]}
{"title": "Cooperation and assortativity with dynamic partner updating\n", "abstract": " The natural tendency for humans to make and break relationships is thought to facilitate the emergence of cooperation. In particular, allowing conditional cooperators to choose with whom they interact is believed to reinforce the rewards accruing to mutual cooperation while simultaneously excluding defectors. Here we report on a series of human subjects experiments in which groups of 24 participants played an iterated prisoner\u2019s dilemma game where, critically, they were also allowed to propose and delete links to players of their own choosing at some variable rate. Over a wide variety of parameter settings and initial conditions, we found that dynamic partner updating significantly increased the level of cooperation, the average payoffs to players, and the assortativity between cooperators. Even relatively slow update rates were sufficient to produce large effects, while subsequent increases to the update rate had\u00a0\u2026", "num_citations": "185\n", "authors": ["2087"]}
{"title": "Monopsony in online labor markets\n", "abstract": " Despite the seemingly low switching and search costs of on-demand labor markets like Amazon Mechanical Turk, we find substantial monopsony power, as measured by the elasticity of labor supply facing the requester (employer). We isolate plausibly exogenous variation in rewards using a double machine learning estimator applied to a large dataset of scraped MTurk tasks. We also reanalyze data from five MTurk experiments that randomized payments to obtain corresponding experimental estimates. Both approaches yield uniformly low labor supply elasticities, around 0.1, with little heterogeneity. Our results suggest monopsony might also be present even in putatively \u201cthick\u201d labor markets. (JEL C44, J22, J23, J42)", "num_citations": "159\n", "authors": ["2087"]}
{"title": "Strategic network formation with structural holes\n", "abstract": " A fundamental principle in social network research is that individuals can benefit from serving as intermediaries between others who are not directly connected. Through such intermediation, they potentially can broker the flow of information and synthesize ideas arising in different parts of the network. These principles form the underpinning for the theory of structural holes, which studies the ways in which individuals, particularly in organizational settings, fill the\" holes\" between people or groups that are not otherwise interacting.", "num_citations": "149\n", "authors": ["2087"]}
{"title": "Graph distances in the streaming model: the value of space.\n", "abstract": " We investigate the importance of space when solving problems based on graph distance in the streaming model. In this model, the input graph is presented as a stream of edges in an arbitrary order. The main computational restriction of the model is that we have limited space; therefore, we cannot remember all the streamed data but rather are forced to make space-efficient summaries of the data as we go along. For a graph of n vertices and m edges, we show that testing many graph properties, including connectivity (ergo any reasonable decision problem about distances) and bipartiteness, requires \u2126 (n) bits of space. Given this, we then investigate how the power of the model increases as we relax our space restriction. Our main result is an efficient randomized algorithm that constructs a (2t+ 1)-spanner in one pass. With high probability, it uses O (t\u00b7 n1+ 1/t log2 n) bits of space and processes each edge in the stream in O (t2\u00b7 n1/t log n) time. We find approximations to diameter and girth via the constructed spanner. For t= \u2126 (log n log log n), the space requirement of the algorithm is O (n\u00b7 polylog n), and the per-edge processing time is O (polylog n). We also show a corresponding lower bound of t for the approximation ratio achievable when the space restriction is O (t\u00b7 n1+ 1/t log2 n). We then consider the scenario in which we are allowed multiple passes of the input stream. Here we investigate whether allowing these extra passes will compensate for a given space restriction. We show that finding vertices d away from a particular vertex will always take d passes, for all d\u2208{1,..., t/2}, when the space restriction is o (n1+ 1/t). For girth, we show the\u00a0\u2026", "num_citations": "137\n", "authors": ["2087"]}
{"title": "Incentivizing high quality crowdwork\n", "abstract": " We study the causal effects of financial incentives on the quality of crowdwork. We focus on performance-based payments (PBPs), bonus payments awarded to workers for producing high quality work. We design and run randomized behavioral experiments on the popular crowdsourcing platform Amazon Mechanical Turk with the goal of understanding when, where, and why PBPs help, identifying properties of the payment, payment structure, and the task itself that make them most effective. We provide examples of tasks for which PBPs do improve quality. For such tasks, the effectiveness of PBPs is not too sensitive to the threshold for quality required to receive the bonus, while the magnitude of the bonus must be large enough to make the reward salient. We also present examples of tasks for which PBPs do not improve quality. Our results suggest that for PBPs to improve quality, the task must be effort-responsive\u00a0\u2026", "num_citations": "131\n", "authors": ["2087"]}
{"title": "The economic and cognitive costs of annoying display advertisements\n", "abstract": " Some online display advertisements are annoying. Although publishers know the payment they receive to run annoying ads, little is known about the cost that such ads incur (e.g., causing website abandonment). Across three empirical studies, the authors address two primary questions: (1) What is the economic cost of annoying ads to publishers? and (2) What is the cognitive impact of annoying ads to users? First, the authors conduct a preliminary study to identify sets of more and less annoying ads. Second, in a field experiment, they calculate the compensating differential, that is, the amount of money a publisher would need to pay users to generate the same number of impressions in the presence of annoying ads as it would generate in their absence. Third, the authors conduct a mouse-tracking study to investigate how annoying ads affect reading processes. They conclude that in plausible scenarios, the\u00a0\u2026", "num_citations": "130\n", "authors": ["2087"]}
{"title": "Economic properties of social networks\n", "abstract": " We examine the marriage of recent probabilistic generative models for social networks with classical frameworks from mathematical economics. We are particularly interested in how the statistical structure of such networks influences global economic quantities such as price variation. Our findings are a mixture of formal analysis, simulation, and experiments on an international trade data set from the United Nations.", "num_citations": "114\n", "authors": ["2087"]}