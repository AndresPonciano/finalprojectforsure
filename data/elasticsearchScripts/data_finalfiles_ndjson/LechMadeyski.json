{"title": "Towards identifying software project clusters with regard to defect prediction\n", "abstract": " Background: This paper describes an analysis that was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary study perfomed before showed the need for a further in-depth analysis in order to identify project clusters.Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency.Method: Hierarchical and k-means clustering, as well as Kohonen's neural network was used to find groups of similar projects. The obtained clusters were investigated with the discriminant analysis. For each of the identified group a\u00a0\u2026", "num_citations": "444\n", "authors": ["1151"]}
{"title": "Robust statistical methods for empirical software engineering\n", "abstract": " There have been many changes in statistical theory in the past 30 years, including increased evidence that non-robust methods may fail to detect important results. The statistical advice available to software engineering researchers needs to be updated to address these issues. This paper aims both to explain the new results in the area of robust analysis methods and to provide a large-scale worked example of the new methods. We summarise the results of analyses of the Type 1 error efficiency and power of standard parametric and non-parametric statistical tests when applied to non-normal data sets. We identify parametric and non-parametric methods that are robust to non-normality. We present an analysis of a large-scale software engineering experiment to illustrate their use. We illustrate the use of kernel density plots, and parametric and non-parametric methods using four different software\u00a0\u2026", "num_citations": "166\n", "authors": ["1151"]}
{"title": "Which process metrics can significantly improve defect prediction models? An empirical study\n", "abstract": " The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one\u00a0\u2026", "num_citations": "163\n", "authors": ["1151"]}
{"title": "Overcoming the equivalent mutant problem: A systematic literature review and a comparative experiment of second order mutation\n", "abstract": " Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated\u00a0\u2026", "num_citations": "157\n", "authors": ["1151"]}
{"title": "Judy\u2013a mutation testing tool for Java\n", "abstract": " Popular code coverage measures, such as branch coverage, are indicators of the thoroughness rather than the fault detection capability of test suites. Mutation testing is a fault-based technique that measures the effectiveness of test suites for fault localisation. Unfortunately, use of mutation testing in the software industry is rare because generating and running vast numbers of mutants against the test cases is time-consuming and difficult to do without an automated, fast and reliable tool. Our objective is to present an innovative approach to mutation testing that takes advantage of a novel aspect-oriented programming mechanism, called \u2018pointcut and advice\u2019, to avoid multiple compilation of mutants and, therefore, to speed up mutation testing. An empirical comparison of the performance of the developed tool, called Judy, with the MuJava mutation testing tool on 24 open-source projects demonstrates the value of the\u00a0\u2026", "num_citations": "123\n", "authors": ["1151"]}
{"title": "Test-driven development: An empirical evaluation of agile practice\n", "abstract": " Agile methods are gaining more and more interest both in industry and in research. Many industries are transforming their way of working from traditional waterfall projects with long duration to more incremental, iterative and agile practices. At the same time, the need to evaluate and to obtain evidence for different processes, methods and tools has been emphasized. Lech Madeyski offers the first in-depth evaluation of agile methods. He presents in detail the results of three different experiments, including concrete examples of how to conduct statistical analysis with meta analysis or the SPSS package, using as evaluation indicators the number of acceptance tests passed (overall and per hour) and design complexity metrics. The book is appropriate for graduate students, researchers and advanced professionals in software engineering. It proves the real benefits of agile software development, provides readers with in-depth insights into experimental methods in the context of agile development, and discusses various validity threats in empirical studies.", "num_citations": "123\n", "authors": ["1151"]}
{"title": "The impact of test-first programming on branch coverage and mutation score indicator of unit tests: An experiment\n", "abstract": " Abstract Background Test-First programming is regarded as one of the software development practices that can make unit tests to be more rigorous, thorough and effective in fault detection. Code coverage measures can be useful as indicators of the thoroughness of unit test suites, while mutation testing turned out to be effective at finding faults. Objective This paper presents an experiment in which Test-First vs. Test-Last programming practices are examined with regard to branch coverage and mutation score indicator of unit tests. Method Student subjects were randomly assigned to Test-First and Test-Last groups. In order to further reduce pre-existing differences among subjects, and to get a more sensitive measure of our experimental effect, multivariate analysis of covariance was performed. Results Multivariate tests results indicate that there is no statistically significant difference between Test-First and Test-Last\u00a0\u2026", "num_citations": "83\n", "authors": ["1151"]}
{"title": "The impact of pair programming and test-driven development on package dependencies in object-oriented design\u2014an experiment\n", "abstract": " Background: Test-driven development (TDD) and pair programming are software development practices popularized by eXtreme Programming methodology. The aim of the practices is to improve software quality.               Objective: Provide an empirical evidence of the impact of both practices on package dependencies playing a role of package level design quality indicators.               Method: An experiment with a hundred and eighty eight MSc students from Wroclaw University of Technology, who developed finance-accounting system in different ways (CS \u2014 classic solo, TS \u2014 TDD solo, CP \u2014 classic pairs, TP \u2014 TDD pairs).               Results: It appeared that package level design quality indicators (namely package dependencies in an object-oriented design) were not significantly affected by development method.               Limitations: Generalization of the results is limited due to the fact that MSc students\u00a0\u2026", "num_citations": "53\n", "authors": ["1151"]}
{"title": "The impact of test-driven development on software development productivity\u2014an empirical study\n", "abstract": " Test-driven development (TDD) is entering the mainstream of software development. We examined the software development process for the purpose of evaluation of the TDD impact, with respect to software development productivity, in the context of a\u00a0web based system development. The design of the study is based on Goal-Question-Metric approach, and may be easily replicated in different industrial contexts where the number of subjects involved in the study is limited. The study reveals that TDD may have positive impact on software development productivity. Moreover, TDD is characterized by the higher ratio of active development time (described as typing and producing code) in total development time than test-last development approach.", "num_citations": "51\n", "authors": ["1151"]}
{"title": "Impact of aspect-oriented programming on software development efficiency and design quality: an empirical study\n", "abstract": " The aspect-oriented programming (AOP) approach is supposed to enhance a system's features such as modularity, readability and simplicity. Owing to a better modularisation of cross- cutting concerns, the developed system implementation would be less complex, and more readable. Thus, software development efficiency would increase, so the system would be created faster than its object-oriented programming (OOP) equivalent. An empirical study of a Web-based system development is carried out to examine AOP against OOP approach with regard to software development efficiency and design quality. The study reveals that the AOP approach appears to be a fullfledged alternative to the pure OOP approach. Nevertheless, the impact of AOP on software development efficiency and design quality was not confirmed. In particular, it appeared that design quality metrics were not significantly associated with using\u00a0\u2026", "num_citations": "46\n", "authors": ["1151"]}
{"title": "On the effects of pair programming on thoroughness and fault-finding effectiveness of unit tests\n", "abstract": " Code coverage and mutation score measure how thoroughly tests exercise programs and how effective they are, respectively. The objective is to provide empirical evidence on the impact of pair programming on both, thoroughness and effectiveness of test suites, as pair programming is considered one of the practices that can make testing more rigorous, thorough and effective. A large experiment with MSc students working solo and in pairs was conducted. The subjects were asked to write unit tests using JUnit, and to follow test-driven development approach, as suggested by eXtreme Programming methodology. It appeared that branch coverage, as well as mutation score indicator (the lower bound on mutation score), was not significantly affected by using pair programming, instead of solo programming. However, slight but insignificant positive impact of pair programming on mutations score indicator was\u00a0\u2026", "num_citations": "42\n", "authors": ["1151"]}
{"title": "Problems of mutation testing and higher order mutation testing\n", "abstract": " Since Mutation Testing was proposed in the 1970s, it has been considered as an effective technique of software testing process for evaluating the quality of the test data. In other words, Mutation Testing is used to evaluate the fault detection capability of the test data by inserting errors into the original program to generate mutations, and after then check whether tests are good enough to detect them. However, the problems of mutation testing such as a large number of generated mutants or the existence of equivalent mutants, are really big barriers for applying mutation testing. A lot of solutions have been proposed to solve that problems. A new form of Mutation Testing, Higher Order Mutation Testing, was first proposed by Harman and Jia in 2009 and is one of the most promising solutions. In this paper, we consider the main limitations of Mutation Testing and previous proposed solutions to solve that\u00a0\u2026", "num_citations": "32\n", "authors": ["1151"]}
{"title": "Impact of pair programming on thoroughness and fault detection effectiveness of unit test suites\n", "abstract": " Pair programming (PP) is regarded as one of the practices that can make testing more rigorous, thorough and effective. Therefore, we examined PP versus solo programming (SP) with respect to both thoroughness and fault detection effectiveness of test suites. Branch coverage (BC) and mutation score indicator (MSI) were used as measures of how thoroughly tests exercise programs, and how effective they are, respectively. It turned out that the PP practice did not significantly affect BC (U = 471.5, nonsignificant, r = \u2212 0.03) and MSI (U = 422.0, nonsignificant, r = \u2212 0.12). These results are consistent with the results of selective analysis in which projects with a limited number of assertions are excluded. Analysis of covariance (ANCOVA) was performed to get a more sensitive measure of our experiment effect as well as to reduce preexisting differences among subjects. The obtained results do not support anecdotal\u00a0\u2026", "num_citations": "32\n", "authors": ["1151"]}
{"title": "Would wider adoption of reproducible research be beneficial for empirical software engineering research?\n", "abstract": " Researchers have identified problems with the validity of software engineering research findings. In particular, it is often impossible to reproduce data analyses, due to lack of raw data, or sufficient summary statistics, or undefined analysis procedures. The aim of this paper is to raise awareness of the problems caused by unreproducible research in software engineering and to discuss the concept of reproducible research (RR) as a mechanism to address these problems. RR is the idea that the outcome of research is both a paper and its computational environment. We report some recent studies that have cast doubts on the reliability of research outcomes in software engineering. Then we discuss the use of RR as a means of addressing these problems. We discuss the use of RR in software engineering research and present the methodology we have used to adopt RR principles. We report a small working example\u00a0\u2026", "num_citations": "26\n", "authors": ["1151"]}
{"title": "Searching for strongly subsuming higher order mutants by applying multi-objective optimization algorithm\n", "abstract": " Higher order mutation testing is considered a promising solution for overcoming the main limitations of first order mutation testing. Strongly subsuming higher order mutants (SSHOMs) are the most valuable among all kinds of higher order mutants (HOMs) generated by combining first order mutants (FOMs). They can be used to replace all of its constituent FOMs without scarifying test effectiveness. Some researchers indicated that searching for SSHOMs is a promising approach. In this paper, we not only introduce a new classification of HOMs but also new objectives and fitness function which we apply in multi-objective optimization algorithm for finding valuable SSHOMs.", "num_citations": "25\n", "authors": ["1151"]}
{"title": "Is external code quality correlated with programming experience or feelgood factor?\n", "abstract": " This paper is inspired by an article by M\u00fcller and Padberg who study the feelgood factor and programming experience, as candidate drivers for the pair programming performance. We not only reveal a possible threat to validity of empirical results presented by M\u00fcller and Padberg but also perform an independent research. Our objective is to provide empirical evidence whether external code quality is correlated with the feelgood factor, or with programming experience. Our empirical study is based on a controlled experiment with MSc students. It appeared that the external code quality is correlated with the feelgood factor, and programming experience, in the case of pairs using a classic (test-last) testing approach. The generalization of the results is limited due to the fact that MSc students participated in the study. The research revealed that both the feelgood factor and programming experience may be the\u00a0\u2026", "num_citations": "25\n", "authors": ["1151"]}
{"title": "Empirical evaluation of multiobjective optimization algorithms searching for higher order mutants\n", "abstract": " First order mutation testing is used to evaluate the quality of a given set of test cases by inserting single changes into the program under test to produce first order mutants (FOMs) of the original program, and then checking whether tests are good enough to detect the artificially injected defects. However, mutation testing is not yet widely used due to the problems of a large number of generated mutants and limited realism of introduced changes that do not necessarily reflect real software defects. Furthermore, many of the generated mutants are equivalent, i.e., they keep the program semantics unchanged and, thus, cannot be detected by any test suite. Higher order mutation testing has been coined as a promising solution for overcoming these limitations of FOM testing. In particular, finding strongly subsuming higher order mutants (SSHOMs), which are able to replace all of their constituent FOMs without scarifying test\u00a0\u2026", "num_citations": "20\n", "authors": ["1151"]}
{"title": "A review of process metrics in defect prediction studies\n", "abstract": " Process metrics appear to be an effective addition to software defect prediction models usually built upon product metrics. We present a review of research studies that investigate process metrics in defect prediction. The following process metrics are discussed: Number of Revisions, Number of Distinct Committers, Number of Modified Lines, Is New and Number of Defects in Previous Revision. We not only introduce the definitions of the aforementioned process metrics but also present the most important results, recent advances and the summary regarding the use of these metrics in software defect prediction models, as well as the taxonomy of the analysed process metrics.", "num_citations": "20\n", "authors": ["1151"]}
{"title": "Problems with statistical practice in human-centric software engineering experiments\n", "abstract": " Background Examples of questionable statistical practice, when published in high quality software engineering (SE) journals, may lead to novice researchers adopting incorrect statistical practices.Objective Our goal is to highlight issues contributing to poor statistical practice in human-centric SE experiments.Method We reviewed the statistical analysis practices used in the 13 papers that reported families of human-centric SE experiments and were published in high quality journals.Results Reviewed papers related to 45 experiments and involved a total of 1303 human participants. We searched for issues that were related to questionable statistical practice that were found in more than one paper. We observed three types of bad practice: incorrect use of terminology, incorrect analysis of repeated measures designs, and post-hoc power testing. We also found two analysis practices (ie, multiple testing and pre-testing\u00a0\u2026", "num_citations": "17\n", "authors": ["1151"]}
{"title": "Addressing mutation testing problems by applying multi-objective optimization algorithms and higher order mutation\n", "abstract": " Traditional mutation testing is a powerful technique to evaluate the quality of test suites. Unfortunately, it is not yet widely used due to the problems of a large number of generated mutants, limited realism (mutants not necessarily reflect real software defects), and equivalent mutants problem. Higher order mutation (HOM) testing has been proposed to overcome these limitations of first order mutation testing. We present an empirical evaluation of our approach to higher order mutation testing. We apply different multi-objective optimization algorithms (including one modified by us), as well as our classification of HOMs, proposed objectives and fitness functions. We search for \u201cHigh Quality and Reasonable HOMs\u201d able to replace all of its constituent FOMs without scarifying test effectiveness and to reflect complex defects requiring more than one change to correct them. Our approach leads to: 1) reduced cost of mutation\u00a0\u2026", "num_citations": "16\n", "authors": ["1151"]}
{"title": "Higher order mutation testing to drive development of new test cases: An empirical comparison of three strategies\n", "abstract": " Mutation testing, which includes first order mutation (FOM) testing and higher order mutation (HOM) testing, appeared as a powerful and effective technique to evaluate the quality of test suites. The live mutants, which cannot be killed by the given test suite, make up a significant part of generated mutants and may drive the development of new test cases. Generating live higher order mutants (HOMs) able to drive development of new test cases is considered in this paper. We apply multi-objective optimization algorithms based on our proposed objectives and fitness functions to generate higher order mutants using three strategies: HOMT1 (HOMs generated from all first order mutants), HOMT2 (HOMs generated from killed first order mutants) and HOMT3 (HOMs generated from not-easy-to-kill first order mutants). We then use mutation score indicator to evaluate, which of the three approaches is better suited\u00a0\u2026", "num_citations": "16\n", "authors": ["1151"]}
{"title": "Architectural design of modern web applications\n", "abstract": " Architectural design is about decisions which influence characteristics of arising system eg maintainability or scalability. Existing architectural frameworks, like MVC or PCMEF, allow building well-structured applications as a result of minimizing dependences between the system modules. Authors of this paper analysed these frameworks in the web application context. MVC and PCMEF appeared to be inspirations for the new XWA (eXtensible Web Architecture) architectural framework combining strengths of both frameworks and incorporating the idea of continuations into a separated controller. Additionally the detailed description of practical implementation of XWA on e-Informatyka portal example and guidelines for building web applications especially based on Apache Cocoon similar technologies are presented.", "num_citations": "16\n", "authors": ["1151"]}
{"title": "Software engineering needs agile experimentation: a new practice and supporting tool\n", "abstract": " This article proposes a novel software engineering practice called Agile Experimentation. It aims mostly small experiments in a business driven software engineering environment where a developer is a scarce resource and the impact of the experimentation on the return-of-investment driven software project needs to be minimal. In such environment the tools used for the sake of research need to have virtually no negative impact on the developers, but simultaneously those tools need to collect high quality data to perform sound enough quantitative analyses. In order to fulfill those requirements, and to support the Agile Experimentation practice, we co-developed a tool called NActivitySensor that gathers the data about the developers activities in a widely used Integrated Development Environment\u2014Visual Studio. The proposed Agile Experimentation practice and the developed tool complement each other\u00a0\u2026", "num_citations": "15\n", "authors": ["1151"]}
{"title": "Assessment of the software defect prediction cost effectiveness in an industrial project\n", "abstract": " Software defect prediction is a promising, new approach to increase both, software quality and development pace. Unfortunately, the cost effectiveness of software defect prediction in industrial settings is not eagerly shared by the pioneering companies. In particular, the cost effectiveness of using the DePress open source software measurement framework, developed by Wroclaw University of Science and Technology, and Capgemini software development company, for defect prediction in commercial software development projects have not been previously investigated. Thus, in this paper, we explore whether defect prediction can positively impact an industrial software development project by generating profits. To meet this goal, we conducted a defect prediction and simulated potential quality assurance costs based on the best prediction result, as well as the proposed Quality Assurance (QA) strategy\u00a0\u2026", "num_citations": "15\n", "authors": ["1151"]}
{"title": "Software measurement and defect prediction with DePress extensible framework\n", "abstract": " Context. Software data collection precedes analysis which, in turn, requires data science related skills. Software defect prediction is hardly used in industrial projects as a quality assurance and cost reduction mean. Objectives. There are many studies and several tools which help in various data analysis tasks but there is still neither an open source tool nor standardized approach. Results. We developed Defect Prediction for software systems (DePress), which is an extensible software measurement, and data integration framework which can be used for prediction purposes (eg defect prediction, effort prediction) and software changes analysis (eg release notes, bug statistics, commits quality). DePress is based on the KNIME project and allows building workflows in a graphic, end-user friendly manner. Conclusions. We present main concepts, as well as the development state of the De-Press framework. The results show that DePress can be used in Open Source, as well as in industrial project analysis.", "num_citations": "15\n", "authors": ["1151"]}
{"title": "Continuous Test-Driven Development\u2014A Novel Agile Software Development Practice and Supporting Tool\n", "abstract": " Continuous testing is a technique in modern software development in which the source code is constantly unit tested in the background and there is no need for the developer to perform the tests manually. We propose an extension to this technique that combines it with well-established software engineering practice called Test-Driven Development (TDD). In our practice, that we called Continuous Test-Driven Development (CTDD), software developer writes the tests first and is not forced to perform them manually. We hope to reduce the time waste resulting from manual test execution in highly test driven development scenario. In this article we describe the CTDD practice and the tool that we intend to use to support and evaluate the CTDD practice in a real world software development project.", "num_citations": "15\n", "authors": ["1151"]}
{"title": "Effect sizes and their variance for AB/BA crossover design studies\n", "abstract": " Vegas et al. IEEE Trans Softw Eng 42(2):120:135 (2016) raised concerns about the use of AB/BA crossover designs in empirical software engineering studies. This paper addresses issues related to calculating standardized effect sizes and their variances that were not addressed by the Vegas et al.\u2019s paper. In a repeated measures design such as an AB/BA crossover design each participant uses each method. There are two major implication of this that have not been discussed in the software engineering literature. Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the\u00a0\u2026", "num_citations": "14\n", "authors": ["1151"]}
{"title": "Continuous defect prediction: the idea and a related dataset\n", "abstract": " We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if\u00a0\u2026", "num_citations": "14\n", "authors": ["1151"]}
{"title": "On the relationship between the order of mutation testing and the properties of generated higher order mutants\n", "abstract": " The goal of higher order mutation testing is to improve mutation testing effectiveness in particular and test effectiveness in general. There are different approaches which have been proposed in the area of second order mutation testing and higher order mutation testing with mutants order ranging from 2 to 70. Unfortunately, the empirical evidence on the relationship between the order of mutation testing and the desired properties of generated mutants is scarce except the conviction that the number of generated mutants could grow exponentially with the order of mutation testing. In this paper, we present the study of finding the relationships between the order of mutation testing and the properties of mutants in terms of number of generated high quality and reasonable mutants as well as generated live mutants. Our approach includes higher order mutants classification, objective functions and fitness\u00a0\u2026", "num_citations": "13\n", "authors": ["1151"]}
{"title": "E-learning\u2013analiza cel\u00f3w i mo\u017cliwo\u015bci ich realizacji na podstawie istniej\u0105cych specyfikacji i standard\u00f3w\n", "abstract": " W latach siedemdziesi\u0105tych powsta\u0142a koncepcja nauczania na odleg\u0142o\u015b\u0107 z wykorzystaniem sieci Internet (ang. e-learning). Wraz z narodzinami tej idei rozpocz\u0119\u0142a si\u0119 ewolucja rozwi\u0105za\u0144 informatycznych maj\u0105cych j\u0105 wspiera\u0107. Artyku\u0142 zawiera analiz\u0119 obecnej sytuacji na rynku specyfikacji i standard\u00f3w. Dokonana analiza jest punktem wyj\u015bcia do rozwa\u017ca\u0144 na temat mo\u017cliwo\u015bci rozwijania narz\u0119dzi i system\u00f3w spe\u0142niaj\u0105cych g\u0142\u00f3wne, zdaniem autor\u00f3w, cele elektronicznego nauczania.", "num_citations": "12\n", "authors": ["1151"]}
{"title": "Bottlenecks in software defect prediction implementation in industrial projects\n", "abstract": " Case studies focused on software defect prediction in real, industrial software development projects are extremely rare. We report on dedicated R&D project established in cooperation between Wroclaw University of Technology and one of the leading automotive software development companies to research possibilities of introduction of software defect prediction using an open source, extensible software measurement and defect prediction framework called DePress (Defect Prediction in Software Systems) the authors are involved in. In the first stage of the R&D project, we verified what kind of problems can be encountered. This work summarizes results of that phase.", "num_citations": "11\n", "authors": ["1151"]}
{"title": "Reproducible research\u2013what, why and how\n", "abstract": " The minimum criterion for valid research and data analyses in software engineering and other disciplines is reproducibility. The aim of this paper is to explain the concept of Reproducible Research (RR), embracing reporting modern data analyses in a reproducible manner, its importance, and how researchers and data analysts can use current technology to adopt RR and increase reproducibility of their data analysis tasks.", "num_citations": "11\n", "authors": ["1151"]}
{"title": "Meta-analysis for families of experiments in software engineering: a systematic review and reproducibility and validity assessment\n", "abstract": " Context                 Previous studies have raised concerns about the analysis and meta-analysis of crossover experiments and we were aware of several families of experiments that used crossover designs and meta-analysis.                                               Objective                 To identify families of experiments that used meta-analysis, to investigate their methods for effect size construction and aggregation, and to assess the reproducibility and validity of their results.                                               Method                 We performed a systematic review (SR) of papers reporting families of experiments in high quality software engineering journals, that attempted to apply meta-analysis. We attempted to reproduce the reported meta-analysis results using the descriptive statistics and also investigated the validity of the meta-analysis process.                                               Results                 Out of 13 identified primary studies\u00a0\u2026", "num_citations": "10\n", "authors": ["1151"]}
{"title": "Cross\u2013project defect prediction with respect to code ownership model: An empirical study\n", "abstract": " The paper presents an analysis of 83 versions of industrial, open-source and academic projects. We have empirically evaluated whether those project types constitute separate classes of projects with regard to defect prediction. Statistical tests proved that there exist significant differences between the models trained on the aforementioned project classes. This work makes the next step towards cross-project reusability of defect prediction models and facilitates their adoption, which has been very limited so far.", "num_citations": "10\n", "authors": ["1151"]}
{"title": "Capable Leader and Skilled and Motivated Team Practices to Introduce eXtreme Programming\n", "abstract": " Applying changes to software engineering processes in organisations usually raises many problems of varying nature. Basing on a real-world 2-year project and a simultaneous process change initiative in Poland the authors studied those problems, their context, and the outcome. The reflection was a need for a set of principles and practices to help introduce eXtreme Programming (XP). In the paper the authors extend their preliminary set, consisting of the Empirical Evidence principle, exemplified using DICE\u00ae,\u00a0and the practice of the Joint Engagement of management and the developers. This preliminary collection is being supplemented with the Capable Leader, as well as the Skilled and Motivated Team practices based on the DICE\u00ae\u00a0framework as well.", "num_citations": "10\n", "authors": ["1151"]}
{"title": "Software defect prediction using bad code smells: A systematic literature review\n", "abstract": " The challenge of effective refactoring in the software development cycle brought forward the need to develop automated defect prediction models. Among many existing indicators of bad code, code smells have attracted particular interest of both the research community and practitioners in recent years. In this paper, we describe the current state-of-the-art in the field of bug prediction with the use of code smells and attempt to identify areas requiring further research. To achieve this goal, we conducted a systematic literature review of 27 research papers published between 2006 and 2019. For each paper, we (i) analysed the reported relationship between smelliness and bugginess, as well as (ii) evaluated the performance of code smell data used as a defect predictor in models developed using machine learning techniques. Our investigation confirms that code smells are both positively correlated with software\u00a0\u2026", "num_citations": "8\n", "authors": ["1151"]}
{"title": "Corrections to effect size variances for continuous outcomes of cross-over clinical trials\n", "abstract": " We would like to make some corrections to the formulas presented in the 2002 Statistics in Medicine article by Curtin et al.[1]. That article presented formulas for the variances of standardized weighted mean difference of an AB/BA cross-over trial that would be comparable both with parallel designs and cross-over designs. There are three main issues in the Curtin et al.\u2019s paper [1] that we address in this communication. Firstly, the paper proposes a standardized effect size for cross-over trials that is inconsistent with the standardized effect sizes used for other repeated measures designs such as the pretest-posttest studies used in educational studies, see [2] and [3]. Secondly, the change to the standardized effect size for cross-over studies necessitates a change to variance of the standardized effect size. Thirdly, the variance of the standardized effect size comparable with parallel trials was not based on the distribution of a valid t-variable, so includes some errors. We follow Curtin et al.\u2019s approach and base our revised variance equations on the moments of the non-central t-distribution, replacing the t-variable with a variable based on the effect size.", "num_citations": "8\n", "authors": ["1151"]}
{"title": "MLCQ: Industry-relevant code smell data set\n", "abstract": " Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code\u00a0\u2026", "num_citations": "7\n", "authors": ["1151"]}
{"title": "Nowy algorytm do szybkiego obliczania niezawodno\u015bci sieci.\n", "abstract": " Network reliability analysis of complex system (including computer, telecomunication, transportation networks or electric networks has been a subject of intensive research for a long time. The problem of network reliability analysis using connectivity based reliability measures is considered in this paper. Such measures are justified since most networks employ dynamic routing so that trafic can be rerouted around failed links as long as a network remains connected. A propbabilistic undirected graph G=(V, E) with a distinguished subset of vertices K is used to a model K. The problem of computing K-terminal reliability of a network with a general structure is known to be NP-hard. It means that it is unlikely that there exists an algorithm which will find exactly optimal solution to all instances of the problem in time which is polynomial in the size of the imput. A new algorithm utilising the cache technique which is a modified technique of dynamic programming and suitable heuristic strategies of preserving labels order and problem decomposition have been presented in this paper. The algorythm has been implemented and has achived excellent performances.", "num_citations": "6\n", "authors": ["1151"]}
{"title": "Code smell prediction employing machine learning meets emerging Java language constructs\n", "abstract": " Background: Defining code smell is not a trivial task. Their recognition tends to be highly subjective. Nevertheless some code smells detection tools have been proposed. Other recent approaches incline towards machine learning (ML) techniques to overcome disadvantages of using automatic detection tools. Objectives: We aim to develop a\u00a0research infrastructure and\u00a0reproduce the process of\u00a0code smell prediction proposed by Arcelli Fontana et al. We investigate ML algorithms performance for\u00a0samples including major modern Java language features. Those such as\u00a0lambdas can shorten the\u00a0code causing code smell presence not as\u00a0obvious to\u00a0detect and\u00a0thus pose a\u00a0challenge to\u00a0both existing code smell detection tools and\u00a0ML algorithms. Method: We extend the study with dataset consisting of 281 Java projects. For driving samples selection we define metrics considering lambdas and method reference, derived\u00a0\u2026", "num_citations": "5\n", "authors": ["1151"]}
{"title": "Creating evolving project data sets in software engineering\n", "abstract": " While the amount of research in the area of software engineering is ever increasing, it is still a challenge to select a research data set. Quite a number of data sets have been proposed, but we still lack a systematic approach to\u00a0creating ones that would evolve together with the industry. We aim to present a systematic method of selecting data sets of industry-relevant software projects for the purposes of software engineering research. We present a set of guidelines for filtering GitHub projects and implement those guidelines in a form of an R script. In particular, we select mostly projects from the biggest industrial open source contributors and remove projects in the first quartile in any of several categories from the data set. We use the latest GitHub\u00a0GraphQL API to select the desired set of repositories. We evaluate the technique on Java projects. Presented technique systematizes methods for creating software\u00a0\u2026", "num_citations": "4\n", "authors": ["1151"]}
{"title": "Continuous Test-Driven Development: A Preliminary Empirical Evaluation using Agile Experimentation in Industrial Settings\n", "abstract": " Test-Driven Development (TDD) is an agile software development and design practice popularized by the eXtreme Programming methodology. Continuous Test-Driven Development (CTDD), proposed by the authors, is the recent enhancement of the TDD practice and combines TDD with the continuous testing (CT) practice that recommends background testing. Thus CTDD eliminates the need to manually execute the tests by a developer. This paper uses CTDD research to test out the idea of Agile Experimentation. It is a refined approach performing disciplined scientific research in an industrial setting. The objective of this paper is to evaluate the new CTDD practice versus the well-established TDD practice via a Single Case empirical study involving a professional developer in a real, industrial software development project employing Microsoft .NET. We found that there was a slight (4\u00a0min) drop in the mean\u00a0\u2026", "num_citations": "4\n", "authors": ["1151"]}
{"title": "Empirical Evidence Principle and Joint Engagement Practice to Introduce XP\n", "abstract": " Bringing software process change to an organisation is a real challenge. The authors have shown a sample attempt to carry out a process change and then reflected on its results and context. The present reflection points to a need for a set of principles and practices that would support the fragile process of introducing agility. For a start, the authors propose the Empirical Evidence principle exemplified using DICE\u00ae and the practice of Joint Engagement of the management and the developers. Both are results of a real-world process change case study in Poland.", "num_citations": "4\n", "authors": ["1151"]}
{"title": "Factors influencing user story estimations: an industrial interview and a conceptual model\n", "abstract": " A proper estimation of time in user stories is a crucial task for both the IT team as well as for the customer, especially in agile projects. Although agile practices offer a lot of flexibility and promote a culture of continuous change, there are always clearly de need timeboxed periods where an IT company has to commit to delivering working soft-ware. Estimating time of user story implementation provides clarity and the opportunity to control the project by the management, yet at the same time, it can increase pressure on software developers. Thus, incorrectly estimated user stories may lead to quality problems including system malfunction, technical debt, and general user experience issues. The paper describes user story characteristics, reasons of user story estimation inaccuracy as well as a model of their potential impact on post-release defects in large IT software ventures, all derived from the conducted interview with practitioners in Capgemini software development company.", "num_citations": "3\n", "authors": ["1151"]}
{"title": "Why reproducible research is beneficial for security research\n", "abstract": " Researchers in software engineering and other disciplines have identified problems with the validity of published research findings. Similar problems occur in security and privacy research. In any science, reproducibility (understood as ability to access the existing data set, to use existing tools with the study-\u2010prescribed defaults and settings, and to apply the study-\u2010prescribed processes to double-\u2010check the results and the study-\u2010prescribed processes and settings) is a minimum criterion for valid research. This paper explains the concept of Reproducible Research (RR), its importance, and how researchers can use current technology to adopt RR to security and privacy. It concludes that RR does not cure all the ills of current security and privacy research practice. But it offers a start toward good scientific practice and more effective methods and tools.", "num_citations": "3\n", "authors": ["1151"]}
{"title": "Appendix to the paper \u201cOvercoming the equivalent mutant problem: a systematic literature review and a comparative experiment of second order mutation\u201d\n", "abstract": " This appendix contains statistical analyses. Thanks to a personal discussion with Kitchenham the following important issues related to statistical analyses can be highlighted. First, it is worth mentioning that variables used in statistical tests ought to be independent (even for nonparametric tests), but we can not be sure that this is the case for the statistical tests used for mutant reduction analysis. The question is how to deal with the problem. In this particular situation, we could use a more stringent alpha level, because Monte Carlo simulations have suggested that when the observations in a group are correlated with one another, the nominal alpha level is no longer the Type I error rate (when a positive correlation is introduced within a group the Type I error rate increases). Moreover, in this particular case (ie mutant reduction) it would be perfectly fair to say that the JudyDiffOp algorithm performed better in terms of total number mutants than the other analysed algorithms without any statistical analyses, which are here only a kind of double-check whether real implementations behave as stated in the description of algorithms. One issue that suggests the problem might not be addressed by a higher alpha level is that the value of a random variable should not be known in advance of measuring it. But for the total number of second order mutants it appears that if one of the algorithms is used, the value for some of the other algorithms may be known. Nonetheless, presented test would be an appropriate method to test any new algorithms that use a different method to construct second order mutants (like JudyDiffOp). Last but not least, some of the findings\u00a0\u2026", "num_citations": "3\n", "authors": ["1151"]}
{"title": "XML w bazach danych\n", "abstract": " Java i XML to dwa bardzo modne i cz\u0119sto u\u017cywane ostatnio s\u0142owa\u2013klucze. Trudno sobie jednak wyobrazi\u0107 powa\u017cne aplikacje korporacyjne bez u\u017cycia baz danych. S\u0105 one integraln\u0105 cz\u0119\u015bci\u0105 zar\u00f3wno aplikacji zastanych (legacy applications), jak i nowych\u2013tworzonych w architekturze wielowarstwowej\u2013aplikacji internetowych biznesu elektronicznego. Podobnie jak Java zapewnia przeno\u015bno\u015b\u0107 tworzonych aplikacji, czyli ich niezale\u017cno\u015b\u0107 od platformy sprz\u0119towo-systemowej, tak XML zapewnia przeno\u015bno\u015b\u0107 danych. Nic wi\u0119c dziwnego, \u017ce Java\u2013aw szczeg\u00f3lno\u015bci XML\u2013wywiera du\u017cy wp\u0142yw r\u00f3wnie\u017c na stosunkowo hermetyczn\u0105 do tej pory dziedzin\u0119 baz danych. Mo\u017cna \u015bmia\u0142o stwierdzi\u0107, \u017ce wraz z popularyzacj\u0105 XML tak\u017ce w bazach danych nast\u0105pi\u0142a swego rodzaju rewolucja. Pojawi\u0142o si\u0119 ca\u0142e spektrum nowych rozwi\u0105za\u0144, kt\u00f3re zostan\u0105 przedstawione w niniejszym artykule. U\u017cyta terminologia i klasyfikacja bazuje na\u00a0\u2026", "num_citations": "3\n", "authors": ["1151"]}
{"title": "Agile Requirements Specification\n", "abstract": " The chapter concerns agile requirements specification based on extreme Programming methodology. Approaches to requirements specification based on use cases as well as the new one based on user stories, screen drafts, acceptance tests and open XML requirements specifiaction format are presented. One of the results of the new approach is executable form of requirements specification. Acceptance tests appeared to be almost equivalent of the executable use cases. This could significantly improve the software development process.", "num_citations": "3\n", "authors": ["1151"]}
{"title": "Nowe koncepcje tworzenia aplikacji internetowych na przyk\u0142adzie portalu e-informatyka. pl\n", "abstract": " W artykule zaprezentowano najwa\u017cniejsze koncepcje i rozwi\u0105zania stosowane przez autora przy tworzeniu zaawansowanych internetowych aplikacji biznesu elektronicznego, aw szczeg\u00f3lno\u015bci przy projekcie portalu e-informatyka. pl. Szczeg\u00f3ln\u0105 uwag\u0119 po\u015bwi\u0119cono personalizacji, warstwie prezentacji, kt\u00f3ra zapewnia\u0142aby obs\u0142ug\u0119 zar\u00f3wno istniej\u0105cych, jak i mog\u0105cych si\u0119 pojawi\u0107 w przysz\u0142o\u015bci urz\u0105dze\u0144 (komunikator\u00f3w osobistych), aplikacji czy format\u00f3w prezentacji, a tak\u017ce zagadnieniom integracji system\u00f3w. Przedstawiono koncepcje integracji technologii platformy J2EE, XML i serwis\u00f3w sieciowych, najnowsze metodyki tworzenia oprogramowania, ich mocne i s\u0142abe strony, aw rezultacie spos\u00f3b odpowiedniego doboru oraz przyk\u0142adowy podzia\u0142 kompetencji cz\u0142onk\u00f3w zespo\u0142u.", "num_citations": "3\n", "authors": ["1151"]}
{"title": "Mutants as Patches: Towards a formal approach to Mutation Testing\n", "abstract": " Background: Mutation testing is a widely explored technique used to evaluate the quality of software tests, but little attention has been given to its mathematical foundations.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Defect prediction with bad smells in code\n", "abstract": " Background: Defect prediction in software can be highly beneficial for development projects, when prediction is highly effective and defect-prone areas are predicted correctly. One of the key elements to gain effective software defect prediction is proper selection of metrics used for dataset preparation. Objective: The purpose of this research is to verify, whether code smells metrics, collected using Microsoft CodeAnalysis tool, added to basic metric set, can improve defect prediction in industrial software development project. Results: We verified, if dataset extension by the code smells sourced metrics, change the effectiveness of the defect prediction by comparing prediction results for datasets with and without code smells-oriented metrics. In a result, we observed only small improvement of effectiveness of defect prediction when dataset extended with bad smells metrics was used: average accuracy value increased by 0.0091 and stayed within the margin of error. However, when only use of code smells based metrics were used for prediction (without basic set of metrics), such process resulted with surprisingly high accuracy (0.8249) and F-measure (0.8286) results. We also elaborated data anomalies and problems we observed when two different metric sources were used to prepare one, consistent set of data. Conclusion: Extending the dataset by the code smells sourced metric does not significantly improve the prediction effectiveness. Achieved result did not compensate effort needed to collect additional metrics. However, we observed that defect prediction based on the code smells only is still highly effective and can be used especially where\u00a0\u2026", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Software product metrics used to build defect prediction models\n", "abstract": " This document presents software product metrics we usually use to build software defect prediction models.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Predykcja defekt\u00f3w na podstawie metryk oprogramowania\u2013identyfikacja klas projekt\u00f3w\n", "abstract": " Istnieje szereg prac naukowych w kt\u00f3rych wykorzystuj\u0105c r\u00f3\u017cne rodzaje regresji b\u0105d\u017a te\u017c metody sztucznej inteligencji konstruuje si\u0119 modele predykcji defekt\u00f3w bazuj\u0105ce na metrykach oprogramowania. S\u0105 to takie modele, kt\u00f3re na podstawie metryk charakteryzuj\u0105cych wytwarzany w projekcie programistycznym artefakt (zazwyczaj klas\u0119 lub plik), dokonuj\u0105 oszacowania prawdopodobie\u0144stwa wyst\u0105pienia w tym artefakcie defektu albo liczby istniej\u0105cych w tym artefakcie defekt\u00f3w. Modele takie s\u0105 bardzo przydatne w procesie testowania, kiedy w wyniku ogranicze\u0144 bud\u017cetowych nie jeste\u015bmy w stanie przetestowa\u0107 wszystkich wytworzonych artefakt\u00f3w. Wtedy testuj\u0105c tylko te artefakty, kt\u00f3re wed\u0142ug modelu predykcji cechuj\u0105 si\u0119 najwi\u0119ksz\u0105 liczb\u0105 defekt\u00f3w, mo\u017cemy zaoszcz\u0119dzi\u0107 sporo czasu, a mimo to zidentyfikowa\u0107 wi\u0119kszo\u015b\u0107 znajduj\u0105cych si\u0119 w systemie defekt\u00f3w. Model predykcji defekt\u00f3w usprawnia r\u00f3wnie\u017c refaktoryzacj\u0119, przez wskazanie tych klas, w przypadku kt\u00f3rych refaktoryzuj\u0105c uzyskamy najwi\u0119ksze korzy\u015bci. Klasy, kt\u00f3re maj\u0105 warto\u015bci metryk b\u0119d\u0105ce wed\u0142ug modelu zazwyczaj powi\u0105zane ze spor\u0105 liczb\u0105 defekt\u00f3w nale\u017cy uzna\u0107 za klasy posiadaj\u0105ce niepoprawn\u0105 struktur\u0119. Przekszta\u0142caj\u0105c je do postaci, dla kt\u00f3rej model nie b\u0119dzie sygnalizowa\u0142 du\u017cej liczby defekt\u00f3w, dokonujemy zazwyczaj dobrej refaktoryzacji. Korzy\u015bci wynikaj\u0105ce ze stosowania modeli predykcji defekt\u00f3w nie budz\u0105 wi\u0119kszej w\u0105tpliwo\u015bci, a mimo to, jak wynika z obserwacji poczynionych przez autor\u00f3w na lokalnym rynku informatycznym, poziom ich stosowania w przemy\u015ble jest znikomy. Podstawow\u0105 przyczyn\u0105 tego stanu rzeczy s\u0105 problemy ze stosowaniem modelu\u00a0\u2026", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Empirical studies on the impact of test-first programming\n", "abstract": " This paper presents productivity and quality effects of test-first programming technique in a tabular form and, as a result, provides a summary of many empirical studies conducted so far. The table has been moved here from one of my research papers according to the anonymous reviewer\u2019s suggestion.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Komunikacja w czasie rzeczywistym w sieci Internet\n", "abstract": " Rozdzia\u0142 prezentuje problemy i rozwi\u0105zania zastosowane podczas realizacji z\u0142o\u017conego systemu komunikacyjnego, kt\u00f3ry mo\u017ce by\u0107 wykorzystany nie tylko w sieci wewn\u0119trznej ale r\u00f3wnie\u017c w sieci Internet. System zosta\u0142 zrealizowany na Politechnice Wroc\u0142awskiej w ramach projektu e-Informatyka. G\u0142\u00f3wne problemy i zaproponowane rozwi\u0105zania dotycz\u0105 synchronizacji strumieni danych i g\u0142osu, transmisji slajd\u00f3w wchodz\u0105cych w sk\u0142ad prezentacji, ogranicze\u0144 nak\u0142adanych przez model bezpiecze\u0144stwa na platformie Java oraz miksowania cyfrowych strumieni audio. System pozwala na r\u00f3wnoleg\u0142\u0105 komunikacj\u0119 g\u0142osow\u0105, tekstow\u0105 (chat) jak i pokaz slajd\u00f3w wielu u\u017cytkownikom jednocze\u015bnie. Wszystko to jest mo\u017cliwe przy minimalnych wymaganiach sprz\u0119towych. Dzi\u0119ki zastosowanym rozwi\u0105zaniom powy\u017csza funkcjonalno\u015b\u0107 jest dost\u0119pna nawet dla u\u017cytkownik\u00f3w korzystaj\u0105cych z wolnych po\u0142\u0105cze\u0144 modemowych.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Dzia\u0142ania projako\u015bciowe w procesie wytwarzania oprogramowania\n", "abstract": " Artyku\u0142 przedstawia propozycj\u0119 wybranych dzia\u0142a\u0144 projako\u015bciowych w procesie wytwarzania oprogramowania. Prezentowane szkieletowe dzia\u0142ania projako\u015bciowe wykorzystuj\u0105 podej\u015bcie GQM i bezpo\u015brednio wywodz\u0105 si\u0119 z wymaga\u0144 normy PN-EN ISO 9001-2001. Ich uszczeg\u00f3\u0142owienie jest wykonane na podstawie standard\u00f3w dziedzinowych i do\u015bwiadczenia autor\u00f3w. Przedstawiona propozycja zostanie praktycznie zweryfikowana w ramach projektu \u201eDependable Distributed Systems\u201d 6 Programu Ramowego Unii Europejskiej, w kt\u00f3rym reprezentowana przez autor\u00f3w jednostka uczelniana jest odpowiedzialna mi\u0119dzy innymi za zarz\u0105dzanie jako\u015bci\u0105.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Rozszerzalny obiektowy model projektu informatycznego\n", "abstract": " Artyku\u0142 dotyczy problem\u00f3w zwi\u0105zanych z narz\u0119dziami wspomagaj\u0105cymi realizacj\u0119 ma\u0142ych i \u015brednich projekt\u00f3w informatycznych. Zaprezentowano pomys\u0142 na rozszerzalny obiektowy model projektu informatycznego (ang. eXtensible Project Object Model) oraz zaproponowano otwart\u0105 i rozszerzaln\u0105 architektur\u0119 realizacji systemu bazuj\u0105c\u0105 na XML. Specyfikacja wymaga\u0144 wobec systemu by\u0142a wynikiem realnych potrzeb firmy realizuj\u0105cej ma\u0142e i \u015brednie projekty informatyczne. W rezultacie powsta\u0142o narz\u0119dzie o ciekawej, rozszerzalnej (za pomoc\u0105 wtyczek) architekturze wspomagaj\u0105ce realizacje projekt\u00f3w informatycznych.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Architecture of Modern Web Application\n", "abstract": " Architectural design is about decisions which influence characteristics of arising system eg maintainability or scalability. Existing architectural frameworks, like MVC or PCMEF, allow building wellstructured applications as a result of minimizing dependences between the system modules. Authors of this paper analysed these frameworks in the web application context. MVC and PCMEF appeared to be inspirations for the new XWA (eXtensible Web Architecture) architectural framework combining strengths of both frameworks and incorporating the idea of continuations into a separated controller. Additionally the detailed description of practical implementation of XWA on eInformatyka portal example and guidelines for building web applications especially based on Apache Cocoon similar technologies are presented.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Zmodyfikowana technika programowania dynamicznego\n", "abstract": " W artykule przedstawiono metod\u0119 (technik\u0119) cz\u0119\u015bciowego spami\u0119tywania partial memoization na tle innych technik programowania dynamicznego. Wykorzystuje ona ide\u0119 ograniczania ilo\u015bci przechowywanych rozwi\u0105za\u0144. Jest to szczeg\u00f3lnie przydatne w przypadkach, w kt\u00f3rych liczba (rozmiar) podproblem\u00f3w uniemo\u017cliwia zastosowanie standardowych technik programowania dynamicznego. Modyfikacja w stosunku do techniki spami\u0119tywania memoization polega na tym, \u017ce (podobnie jak w przypadku koncepcji wykorzystania pami\u0119ci podr\u0119cznej cache) pami\u0119tany jest tylko pewien podzbi\u00f3r rozwi\u0105za\u0144. Technik\u0119 cz\u0119\u015bciowego spami\u0119tywania z powodzeniem wykorzystano do stworzenia nowego algorytmu na bazie standardowego algorytmu faktoryzacji do obliczania niezawodno\u015bci K-terminali. Wydaje si\u0119, \u017ce zaprezentowana technika mo\u017ce znale\u017a\u0107 praktyczne zastosowanie przy projektowaniu r\u00f3wnie\u017c innych\u00a0\u2026", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Protocol for a systematic literature review of methods dealing with equivalent mutant problem\n", "abstract": " Testing is the key method to ensure quality of software. But how does one find out if the test suite is sufficiently covering all quality aspects? There are some established solutions for evaluating if the number of test cases is adequate, but there are still fewer ways to evaluate the quality of tests; mutation testing can be seen as one. Mutation testing seeds artificial faults into an application (mutants) and checks whether a test suite can detect these faults. If these faults are not found, the test suite is considered as \u2018not good enough\u2019[1]. There are also mutations, which keep the program semantics unchanged and thus cannot be detected by any test suite. Finding a way to select, or not select, these mutations is also known as the equivalent mutant problem. The equivalent mutant problem has been increasingly studied since mutation testing was first proposed in the 1971 by Richard Lipton a student paper [2]. The growth of this field can also be dated in the late 1970s, when articles by DeMillo et al.[1], Hamlet [3] and Budd et al.[4] were published.1.1. Objectives of the SLR The overall aim of the study is to develop a new, more effective method for overcoming equivalent mutant problem or to enhance existing methods. To do that, a systematic literature review in the field of equivalent mutants problem is needed, to get to know the current state of knowledge and to have a good starting point. The following objectives are defined to meet the aim:", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Wydajno\u015bciowo-niezawodno\u015bciowa analiza warstwy szkieletowej sieci komputerowych (prolegomena)\n", "abstract": " W artykule zaprezentowano wst\u0119pn\u0105 koncepcj\u0119 i przegl\u0105d literatury dotycz\u0105cej analizy wydajno\u015bciowo-niezawodno\u015bciowej sieci z uwzgl\u0119dnieniem specyfiki warstwy szkieletowej sieci komputerowych. Zaproponowano spos\u00f3b generowania miar wydajno\u015bciowo-niezawodno\u015bciowych (performability measures) sieci umo\u017cliwiaj\u0105c \u0142\u0105czn\u0105 analiz\u0119 wydajno\u015bciowo-niezawodno\u015bciow\u0105 sieci uwzgl\u0119dniaj\u0105c\u0105 przep\u0142ywow\u0105 natur\u0119 sieci. Ze wzgl\u0119du na z\u0142o\u017cono\u015b\u0107 rozpatrywanych problem\u00f3w obliczeniowych przedstawiaj\u0105c metodologi\u0119 analizy wydajno\u015bciowo-niezawodno\u015bciowej sieci zaproponowano wykorzystanie znanej z literatury metody najbardziej prawdopodobnych stan\u00f3w do okre\u015blenia g\u00f3rnych i dolnych ogranicze\u0144 na oczekiwan\u0105 wydajno\u015b\u0107 sieci.", "num_citations": "2\n", "authors": ["1151"]}
{"title": "Code Smells Detection Using Artificial Intelligence Techniques: A Business-Driven Systematic Review\n", "abstract": " Context Code smells in the software systems are indications that usually correspond to deeper problems that can negatively influence software quality characteristics. This review is a part of a R&D project aiming to improve the existing codebeat platform that help developers to avoid code smells and deliver quality code. Objective This study aims to identify and investigate the current state of the art with respect to: (1) predictors used in prediction models to detect code smells, (2) machine learning/artificial intelligence (ML/AI) methods used in prediction models to detect code smells, (3) code smells analyzed in scientific literature. Our secondary objectives were to identify (4) data sets and projects used in research papers to predict code smells, (5) performance measures used to assess prediction models and (6) improvement ideas with regard to code smell detection using ML/AI. Method We conducted a\u00a0\u2026", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Cloud Solutions for Private Permissionless Blockchain Deployment\n", "abstract": " This paper aims to survey the security and scalability problems occurring in private permissionless blockchain systems and solutions to them. The emphasis is put on the blockchain systems hosted by cloud vendors in the form of Blockchain-as-a-Service (BaaS). The currently available solutions offered by the most appreciated cloud providers are reviewed. The most promising services are tested for the real deployment of the consent management system (CMS). Implementing the CMS atop BaaS leads to creating Consent-as-a-Service (CaaS). Through experiments, the proposed system's replication ability and its scalability are examined, along with assessing the feasibility of the consent management system development in the provided cloud environment.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project\n", "abstract": " We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "OECD Recommendation\u2019s draft concerning access to research data from public funding: A review\n", "abstract": " Sharing research data from public funding is an important topic, especially now, during times of global emergencies like the COVID-19 pandemic, when we need policies that enable rapid sharing of research data. Our aim is to discuss and review the revised Draft of the OECD Recommendation Concerning Access to Research Data from Public Funding. The Recommendation is based on ethical scientific practice, but in order to be able to apply it in real settings, we suggest several enhancements to make it more actionable. In particular, constant maintenance of provided software stipulated by the Recommendation is virtually impossible even for commercial software. Other major concerns are insufficient clarity regarding how to finance data repositories in joint private-public investments, inconsistencies between data security and user-friendliness of access, little focus on the reproducibility of submitted data, risks related to the mining of large data sets, and sensitive (particularly personal) data protection. In addition, we identify several risks and threats that need to be considered when designing and developing data platforms to implement the Recommendation (eg, not only the descriptions of the data formats but also the data collection methods should be available). Furthermore, the non-even level of readiness of some countries for the practical implementation of the proposed Recommendation poses a risk of its delayed or incomplete implementation.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Technical Debt Aware Estimations in Software Engineering: A Systematic Mapping Study\n", "abstract": " Context: The Technical Debt metaphor has grown in popularity. More software is being created and has to be maintained. Agile methodologies, in particular Scrum, are widely used by development teams around the world. Estimation is an often practised step in sprint planning. The subject matter of this paper is the impact technical debt has on estimations. Objective: The goal of this research is to identify estimation problems and their solutions due to previously introduced technical debt in software projects. Method: The Systematic mapping study (SMS) method was applied in the research. Papers were selected from the popular digital databases (IEEE, ACM, Scopus, etc.) using defined search criteria. Afterwards, a snowballing procedure was performed and the final publication set was filtered using inclusion/exclusion criteria.Results: 42 studies were selected and evaluated. Five categories of problems and seven proposed solutions to the problems have been extracted from the papers. Problems include items related to business perspective (delivery pressure or lack of technical debt understanding by business decision-makers) and technical perspective (difficulties in forecasting architectural technical debt impact or limits of source code analysis). Solutions were categorized in: more sophisticated decision-making tools for business managers, better tools for estimation support and technical debt management tools on an architectural-level, portfolio approach to technical debt, code audit and technical debt reduction routine conducted every sprint. Conclusion: The results of this mapping study can help taking the appropriate approach in technical\u00a0\u2026", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Supplementary materials for the paper \u201cMeta-analysis for families of experiments in software engineering: a systematic review and reproducibility and validity assessment\u201d\n", "abstract": " The first author (BAK) applied the inclusion and exclusion criteria to the identified candidate primary studies. The third author (PB) checked the application of the inclusion/exclusion criteria to each candidate primary study. The single disagreement during the search and selection process was resolved by discussion.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Empirical evaluation of continuous test-driven development in industrial settings\n", "abstract": " BACKGROUND:Continuous Test-Driven Development (CTDD) is, proposed by the authors, enhancement of the well-established Test-Driven Development (TDD) agile software development and design practice. CTDD combines TDD with continuous testing (CT) that essentially perform background testing. The idea is to eliminate the need to execute tests manually by a TDD-inspired developer.OBJECTIVE:The objective is to compare the efficiency of CTDD vs TDD measured by the red-to-green time (RTG time), ie, time from the moment when the project is rendered not compiling or any of the tests is failing, up until the moment when the project compiles and all the tests are passing. We consider the RTG time to be a possible measurement of efficiency because the shorter the RTG time, the quicker the developer is advancing to the next phase of the TDD cycle.METHOD:We perform single case and small-n\u00a0\u2026", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Cost effectiveness of software defect prediction in an industrial project\n", "abstract": " Software defect prediction is a promising approach aiming to increase software quality and, as a result, development pace. Unfortunately, the cost effectiveness of software defect prediction in industrial settings is not eagerly shared by the pioneering companies. In particular, this is the first attempt to investigate the cost effectiveness of using the DePress open source software measurement framework (jointly developed by Wroclaw University of Science and Technology, and Capgemini software development company) for defect prediction in commercial software projects. We explore whether defect prediction can positively impact an industrial software development project by generating profits. To meet this goal, we conducted a defect prediction and simulated potential quality assurance costs based on the best possible prediction results when using a default, non-tweaked DePress configuration, as well as the proposed Quality Assurance (QA) strategy. Results of our investigation are optimistic: we estimated that quality assurance costs can be reduced by almost 30% when the proposed approach will be used, while estimated DePress usage Return on Investment (ROI) is fully 73 (7300%), and Benefits Cost Ratio (BCR) is 74. Such promising results, being the outcome of the presented research, have caused the acceptance of continued usage of the DePress-based software defect prediction for actual industrial projects run by Volvo Group.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Nowoczesne aplikacje internetowe w praktyce\n", "abstract": " Utworzenie serwisu internetowego to, wed\u0142ug wielu, zadanie ambitne i skomplikowane. To prawda. Ale wybieraj\u0105c odpowiednie \u015brodowisko i technologie mo\u017cna to zadanie znacznie upro\u015bci\u0107. S\u0142u\u017c\u0105 temu najnowocze\u015bniejsze \u015brodowiska i rozwi\u0105zania szkieletowe (ang. web application frameworks), takie jak Apache Cocoon czy Apache Struts. Sta\u0142y si\u0119 one bardzo cennymi narz\u0119dziami w arsenale zespo\u0142\u00f3w tworz\u0105cych nowoczesne, zaawansowane technologicznie aplikacje internetowe. Niniejszy artyku\u0142 jest wynikiem do\u015bwiadcze\u0144 w zakresie u\u017cycia najnowszych technologii zebranych podczas pracy nad projektem serwisu e-informatyka. pl. Stanowi jednocze\u015bnie praktyczne uzupe\u0142nienie artyku\u0142u Tomasza Knyziaka \u201eApache Cocoon\u201d, kt\u00f3ry ukaza\u0142 si\u0119 na \u0142amach Telenetforum we wrze\u015bniu 2002 roku.", "num_citations": "1\n", "authors": ["1151"]}
{"title": "Appendix to \u201ccontinuous build outcome prediction: a small-N experiment in settings of a real software project\u201d\n", "abstract": " This is an online appendix to paper entitled \u201cContinuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project\u201d", "num_citations": "1\n", "authors": ["1151"]}
{"title": "External code quality model and cross-validation of the model\n", "abstract": " One goal of this paper is to empirically explore the relationships between existing object-oriented (OO) structural measures (both, class-level and package-level) and external code quality in the context of different development methods (solo/pair programming and test-first/testlast programming) and based on 122 projects. It appeared that external code quality can be better predicted based on OO measures (class-level CBO measure, package-level NOT measure), and size related LOCC measure than development methods. About 37 per cent of the external code quality variance can be explained by the model based on only aforementioned three measures. The second goal is to answer the question how accurate can these models be considering the unavoidable differences that may exist across projects and systems. This paper attempts to answer this question by means of cross-validation of the model. Twothirds of the projects (81 projects) were randomly picked to build our prediction model and the remaining one-third (41 projects) were used to verify the efficacy of the built model. Generalizability of the model, as currently captured by existing measures, seems to be limited as about 19 per cent of the variance can be explained by the model. Therefore, possible explanations and improvements are suggested.", "num_citations": "1\n", "authors": ["1151"]}