{"title": "Feature diagrams and logics: There and back again\n", "abstract": " Feature modeling is a notation and an approach for modeling commonality and variability in product families. In their basic form, feature models contain mandatory/optional features, feature groups, and implies and excludes relationships. It is known that such feature models can be translated into propositional formulas, which enables the analysis and configuration using existing logic- based tools. In this paper, we consider the opposite translation problem, that is, the extraction of feature models from propositional formulas. We give an automatic and efficient procedure for computing a feature model from a formula. As a side effect we characterize a class of logical formulas equivalent to feature models and identify logical structures corresponding to their syntactic elements. While many different feature models can be extracted from a single formula, the computed model strives to expose graphically the maximum of\u00a0\u2026", "num_citations": "452\n", "authors": ["576"]}
{"title": "Modal I/O automata for interface and product line theories\n", "abstract": " Alfaro and Henzinger use alternating simulation in a two player game as a refinement for interface automata [1]. We show that interface automata correspond to a subset of modal transition systems of Larsen and Thomsen [2], on which alternating simulation coincides with modal refinement. As a consequence a more expressive interface theory may be built, by a simple generalization from interface automata to modal automata. We define modal I/O automata, an extension of interface automata with modality. Our interface theory that follows can express liveness properties, disallowing trivial implementations of interfaces, a problem that exists for theories build around simulation preorders. In order to further exemplify the usefulness of modal I/O automata, we construct a behavioral variability theory for product line development.", "num_citations": "313\n", "authors": ["576"]}
{"title": "42 variability bugs in the linux kernel: a qualitative analysis\n", "abstract": " Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.", "num_citations": "117\n", "authors": ["576"]}
{"title": "Clafer: unifying class and feature modeling\n", "abstract": " We present Clafer (class, feature, reference), a class modeling language with first-class support for feature modeling. We designed Clafer as a concise notation for meta-models, feature models, mixtures of meta- and feature models (such as components with options), and models that couple feature models and meta-models via constraints (such as mapping feature configurations to component configurations or model templates). Clafer allows arranging models into multiple specialization and extension layers via constraints and inheritance. We identify several key mechanisms allowing a meta-modeling language to express feature models concisely. Clafer unifies basic modeling constructs, such as class, association, and property, into a single construct, called clafer. We provide the language with a formal semantics built in a structurally explicit way. The resulting semantics explains the meaning of\u00a0\u2026", "num_citations": "114\n", "authors": ["576"]}
{"title": "On modal refinement and consistency\n", "abstract": " Almost 20 years after the original conception, we revisit several fundamental question about modal transition systems. First, we demonstrate the incompleteness of the standard modal refinement using a counterexample due to H\u00fcttel. Deciding any refinement, complete with respect to the standard notions of implementation, is shown to be computationally hard (co-NP hard). Second, we consider four forms of consistency (existence of implementations) for modal specifications. We characterize each operationally, giving algorithms for deciding, and for synthesizing implementations, together with their complexities.", "num_citations": "84\n", "authors": ["576"]}
{"title": "How does the degree of variability affect bug finding?\n", "abstract": " Software projects embrace variability to increase adaptability and to lower cost; however, others blame variability for increasing complexity and making reasoning about programs more difficult. We carry out a controlled experiment to quantify the impact of variability on debugging of preprocessor-based programs. We measure speed and precision for bug finding tasks defined at three different degrees of variability on several subject programs derived from real systems.", "num_citations": "64\n", "authors": ["576"]}
{"title": "Interface input/output automata\n", "abstract": " Building on the theory of interface automata by de\u00a0Alfaro and Henzinger we design an interface language for Lynch\u2019s I/O, a popular formalism used in the development of distributed asynchronous systems, not addressed by previous interface research. We introduce an explicit separation of assumptions from guarantees not yet seen in other behavioral interface theories. Moreover we derive the composition operator systematically and formally, guaranteeing that the resulting compositions are always the weakest in the sense of assumptions, and the strongest in the sense of guarantees. We also present a method for solving systems of relativized behavioral inequalities as used in our setup and draw a formal correspondence between our work and interface automata.", "num_citations": "62\n", "authors": ["576"]}
{"title": "20 years of modal and mixed specifications\n", "abstract": " Twenty years ago, modal and mixed specifications were proposed as abstract models of system behavior. In this paper, we explain the nature and utility of such specifications, relate them to other formalisms, showcase some of their established applications, and mention some existing tool support. We also present some recent complexity results for decision problems underlying such applications and list some remaining open problems.", "num_citations": "61\n", "authors": ["576"]}
{"title": "Flattening statecharts without explosions\n", "abstract": " We present a polynomial upper bound for flattening of UML statecharts. An efficient flattening technique is derived and implemented in SCOPE---a code generator targeting constrained embedded systems. Programs generated with this new technique are both faster and smaller than those produced by non-flattening code generators. Our approach scales well for big models and exhibits good properties with respect to memory usage, automatic analysis of worst-case reaction time and automatic validation of memory safety.", "num_citations": "56\n", "authors": ["576"]}
{"title": "Constraint markov chains\n", "abstract": " Notions of specification, implementation, satisfaction, and refinement, together with operators supporting stepwise design, constitute a specification theory. We construct such a theory for Markov Chains (MCs) employing a new abstraction of a Constraint MC. Constraint MCs permit rich constraints on probability distributions and thus generalize prior abstractions such as Interval MCs. Linear (polynomial) constraints suffice for closure under conjunction (respectively parallel composition). This is the first specification theory for MCs with such closure properties. We discuss its relation to simpler operators for known languages such as probabilistic process algebra. Despite the generality, all operators and relations are computable.", "num_citations": "51\n", "authors": ["576"]}
{"title": "On efficient program synthesis from statecharts\n", "abstract": " Program synthesis from hierarchical state diagrams has for long been discussed in various communities. My aim is to provide an efficient, lightweight code generation scheme suitable for resource constrained microcontrollers. I describe an initial implementation of SCOPE---a hierarchical code generator for a variant of the statechart language. I shall discuss several techniques implemented in the tool, namely imposing and exploiting a regular hierarchy structure, labeling schemes for fast ancestor queries, improvements in exiting states, compile-time scope resolution for transitions, and various details of compact runtime representation. The resulting algorithm avoids the exponential code growth exhibited by tools based on flattening of hierarchical state machine. At the same time it demonstrates that it is possible to maintain reasonable speed and size results even for small models, while preserving the hierarchy at\u00a0\u2026", "num_citations": "49\n", "authors": ["576"]}
{"title": "Compositional design methodology with constraint Markov chains\n", "abstract": " Notions of specification, implementation, satisfaction, and refinement, together with operators supporting stepwise design, constitute a specification theory. We construct such a theory for Markov Chains (MCs) employing a new abstraction of a Constraint MC. Constraint MCs permit rich constraints on probability distributions and thus generalize prior abstractions such as Interval MCs. Linear (polynomial) constraints suffice for closure under conjunction (respectively parallel composition). This is the first specification theory for MCs with such closure properties. We discuss its relation to simpler operators for known languages such as probabilistic process algebra. Despite the generality, all operators and relations are computable.", "num_citations": "47\n", "authors": ["576"]}
{"title": "Quantifying information leakage of randomized protocols\n", "abstract": " The quantification of information leakage provides a quantitative evaluation of the security of a system. We propose the usage of Markovian processes to model deterministic and probabilistic systems. By using a methodology generalizing the lattice of information approach we model refined attackers capable to observe the internal behavior of the system, and quantify the information leakage of such systems. We also use our method to obtain an algorithm for the computation of channel capacity from our Markovian models. Finally, we show how to use the method to analyze timed and non-timed attacks on the Onion Routing protocol.", "num_citations": "42\n", "authors": ["576"]}
{"title": "Complexity of decision problems for mixed and modal specifications\n", "abstract": " We consider decision problems for modal and mixed transition systems used as specifications: the common implementation problem (whether a set of specifications has a common implementation), the consistency problem (whether a single specification has an implementation), and the thorough refinement problem (whether all implementations of one specification are also implementations of another one). Common implementation and thorough refinement are shown to be PSPACE-hard for modal, and so also for mixed, specifications. Consistency is PSPACE-hard for mixed, while trivial for modal specifications. We also supply upper bounds suggesting strong links between these problems.", "num_citations": "42\n", "authors": ["576"]}
{"title": "Systematic derivation of correct variability-aware program analyses\n", "abstract": " A recent line of work lifts particular verification and analysis methods to Software Product Lines\u2009(SPL). In an effort to generalize such case-by-case approaches, we develop a systematic methodology for lifting single-program analyses to SPLs using abstract interpretation. Abstract interpretation is a classical framework for deriving static analyses in a compositional, step-by-step manner. We show how to take an analysis expressed as an abstract interpretation and lift each of the abstract interpretation steps to a family of programs (SPL). This includes schemes for lifting domain types, and combinators for lifting analyses and Galois connections. We prove that for analyses developed using our method, the soundness of lifting follows by construction. The resulting variational abstract interpretation is a conceptual framework for understanding, deriving, and validating static analyses for SPLs. Then we show how to derive\u00a0\u2026", "num_citations": "40\n", "authors": ["576"]}
{"title": "QUAIL: A quantitative security analyzer for imperative code\n", "abstract": " Quantitative security analysis evaluates and compares how effectively a system protects its secret data. We introduce QUAIL, the first tool able to perform an arbitrary-precision quantitative analysis of the security of a system depending on private information. QUAIL builds a Markov Chain model of the system\u2019s behavior as observed by an attacker, and computes the correlation between the system\u2019s observable output and the behavior depending on the private information, obtaining the expected amount of bits of the secret that the attacker will infer by observing the system. QUAIL is able to evaluate the safety of randomized protocols depending on secret data, allowing to verify a security protocol\u2019s effectiveness. We experiment with a few examples and show that QUAIL\u2019s security analysis is more accurate and revealing than results of other tools.", "num_citations": "40\n", "authors": ["576"]}
{"title": "Model construction with external constraints: An interactive journey from semantics to syntax\n", "abstract": " Mainstream development environments have recently assimilated guidance technologies based on constraint satisfaction. We investigate one class of such technologies, namely, interactive guided derivation of models, where the editing system assists a designer by providing hints about valid editing operations that maintain global correctness. We provide a semantics-based classification of such guidance systems and investigate concrete guidance algorithms for two kinds of modeling languages: a simple subset of class-diagram-like language and for feature models. Both algorithms are efficient and provide exhaustive guidance.", "num_citations": "40\n", "authors": ["576"]}
{"title": "Family-based model checking without a family-based model checker\n", "abstract": " Many software systems are variational: they can be configured to meet diverse sets of requirements. Variability is found in both communication protocols and discrete controllers of embedded systems. In these areas, model checking is an important verification technique. For variational models (systems with variability), specialized family-based model checking algorithms allow efficient verification of multiple variants, simultaneously. These algorithms scale much better than \u201cbrute force\u201d verification of individual systems, one-by-one. Nevertheless, they can deal with only very small variational models.                 We address two key problems of family-based model checking. First, we improve scalability by introducing abstractions that simplify variability. Second, we reduce the burden of maintaining specialized family-based model checkers, by showing how the presented variability abstractions can be used to model\u00a0\u2026", "num_citations": "36\n", "authors": ["576"]}
{"title": "Automatic generation of program families by model restrictions\n", "abstract": " We study the generative development of control programs for families of embedded devices. A software family is described by a single common model and restriction specifications for each of the family members. The model and the specifications are used in the automatic generation of restricted programs. We describe an application of the process of modeling reactive systems with statecharts. A trace inclusion refinement relation is established for automatically generated family members, inducing a behavioral inheritance hierarchy over the generated programs.", "num_citations": "35\n", "authors": ["576"]}
{"title": "Texmo: A multi-language development environment\n", "abstract": " Contemporary software systems contain a large number of artifacts expressed in multiple languages, ranging from domain-specific languages to general purpose languages. These artifacts are interrelated to form software systems. Existing development environments insufficiently support handling relations between artifacts in multiple languages.             This paper presents a taxonomy for multi-language development environments, organized according to language representation, representation of relations between languages, and types of these relations. Additionally, we present TexMo, a prototype of a multi-language development environment, which uses an explicit relation model and implements visualization, static checking, navigation, and refactoring of cross-language relations. We evaluate TexMo by applying it to development of a web-application, JTrac, and provide preliminary evidence of its\u00a0\u2026", "num_citations": "31\n", "authors": ["576"]}
{"title": "Decision problems for interval Markov chains\n", "abstract": " Interval Markov Chains (IMCs) are the base of a classic probabilistic specification theory by Larsen and Jonsson in 1991. They are also a popular abstraction for probabilistic systems.               In this paper we study complexity of several problems for this abstraction, that stem from compositional modeling methodologies. In particular we close the complexity gap for thorough refinement of two IMCs and for deciding the existence of a common implementation for an unbounded number of IMCs, showing that these problems are EXPTIME-complete. We also prove that deciding consistency of an IMC is polynomial and discuss suitable notions of determinism for such specifications.", "num_citations": "31\n", "authors": ["576"]}
{"title": "Abstract probabilistic automata\n", "abstract": " Probabilistic Automata (PAs) are a widely-recognized mathematical framework for the specification and analysis of systems with non-deterministic and stochastic behaviors. This paper proposes Abstract Probabilistic Automata (APAs), that is a novel abstraction model for PAs. In APAs uncertainty of the non-deterministic choices is modeled by may/must modalities on transitions while uncertainty of the stochastic behavior is expressed by (underspecified) stochastic constraints. We have developed a complete abstraction theory for PAs, and also propose the first specification theory for them. Our theory supports both satisfaction and refinement operators, together with classical stepwise design operators. In addition, we study the link between specification theories and abstraction in avoiding the state-space explosion problem.", "num_citations": "28\n", "authors": ["576"]}
{"title": "Vision paper: Make a difference!(semantically)\n", "abstract": " Syntactic difference between models is a wide research area with applications in tools for model evolution, model synchronization and version control. On the other hand, semantic difference between models is rarely discussed. We point out to main use cases of semantic difference between models, and then propose a framework for defining well-formed difference operators on model semantics as adjoints of model combinators such as conjunction, disjunction and structural composition. The framework is defined by properties other then constructively. We instantiate the framework for two rather different modeling languages: feature models and automata specifications. We believe that the algebraic theory of semantic difference will allow to define practical model differencing tools in the future.", "num_citations": "28\n", "authors": ["576"]}
{"title": "Efficient family-based model checking via variability abstractions\n", "abstract": " Many software systems are variational: they can be configured to meet diverse sets of requirements. They can produce a (potentially huge) number of related systems, known as products or variants, by systematically reusing common parts. For variational models (variational systems or families of related systems), specialized family-based model checking algorithms allow efficient verification of multiple variants, simultaneously, in a single run. These algorithms, implemented in a tool , scale much better than \u201cthe brute force\u201d approach, where all individual systems are verified using a single-system model checker, one-by-one. Nevertheless, their computational cost still greatly depends on the number of features and variants. For variational models with a large number of features and variants, the family-based model checking may be too costly or even infeasible. In this work, we address two key problems\u00a0\u2026", "num_citations": "27\n", "authors": ["576"]}
{"title": "Variability-specific abstraction refinement for family-based model checking\n", "abstract": " Variational systems are ubiquitous in many application areas today. They use features to control presence and absence of system functionality. One challenge in the development of variational systems is their formal analysis and verification. Researchers have addressed this problem by designing aggregate so-called family-based verification algorithms. Family-based model checking allows simultaneous verification of all variants of a system family (variational system) in a single run by exploiting the commonalities between the variants. Yet, the computational cost of family-based model checking still greatly depends on the number of variants. In order to make it computationally cheaper, we can use variability abstractions for deriving abstract family-based model checking, where the variational model of a system family is replaced with an abstract (smaller) version of it which preserves the satisfaction of LTL\u00a0\u2026", "num_citations": "26\n", "authors": ["576"]}
{"title": "Variability abstractions: Trading precision for speed in family-based analyses\n", "abstract": " Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is capable of analyzing all valid products (variants) without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a SPL, and produces analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants (which is exponential in the number of features, in the worst case). For a large number of features, the lifted analysis may be too costly or even infeasible. In this paper, we introduce variability abstractions defined as Galois connections and use abstract interpretation as a formal method for the calculational-based derivation of approximate (abstracted) lifted analyses of SPL programs, which are sound by construction. Moreover, given an abstraction we define a syntactic transformation that translates any SPL program into an abstracted version of it, such that the analysis of the abstracted SPL coincides with the corresponding abstracted analysis of the original SPL. We implement the transformation in a tool, that works on Object-Oriented Java program families, and evaluate the practicality of this approach on three Java SPL benchmarks.", "num_citations": "26\n", "authors": ["576"]}
{"title": "Maximizing entropy over Markov processes\n", "abstract": " The channel capacity of a deterministic system with confidential data is an upper bound on the amount of bits of data an attacker can learn from the system. We encode all possible attacks to a system using a probabilistic specification, an Interval Markov Chain. Then the channel capacity computation reduces to finding a model of a specification with highest entropy.Entropy maximization for probabilistic process specifications has not been studied before, even though it is well known in Bayesian inference for discrete distributions. We give a characterization of global entropy of a process as a reward function, a polynomial algorithm to verify the existence of a system maximizing entropy among those respecting a specification, a procedure for the maximization of reward functions over Interval Markov Chains and its application to synthesize an implementation maximizing entropy.We show how to use Interval Markov\u00a0\u2026", "num_citations": "23\n", "authors": ["576"]}
{"title": "A quantitative analysis of variability warnings in linux\n", "abstract": " In order to get insight into challenges with quality in highly-configurable software, we analyze one of the largest open source projects, the Linux kernel, and quantify basic properties of configuration-related warnings. We automatically analyze more than 20 thousand valid and distinct random configurations, in a computation that lasted more than a month. We count and classify a total of 400,000 warnings to get an insight in the distribution of warning types, and the location of the warnings. We run both on a stable and unstable version of the Linux kernel. The results show that Linux contains a significant amount of configuration-dependent warnings, including many that appear harmful. In fact, it appears that there are no configuration-independent warnings in the kernel at all, adding to our knowledge about relevance of family-based analyses.", "num_citations": "22\n", "authors": ["576"]}
{"title": "A modal specification theory for components with data\n", "abstract": " Modal specification is a well-known formalism used as an abstraction theory for transition systems. Modal specifications are transition systems equipped with two types of transitions: must-transitions that are mandatory to any implementation, and may-transitions that are optional. The duality of transitions allows for developing a unique approach for both logical and structural compositions, and eases the step-wise refinement process for building implementations. We propose Modal Specifications with Data (MSDs), the first modal specification theory with explicit representation of data. Our new theory includes the most commonly seen ingredients of a specification theory; that is parallel composition, conjunction and quotient. As MSDs are by nature potentially infinite-state systems, we propose symbolic representations based on effective predicates. Our theory serves as a new abstraction-based formalism for transition\u00a0\u2026", "num_citations": "22\n", "authors": ["576"]}
{"title": "New results on abstract probabilistic automata\n", "abstract": " Probabilistic Automata (PAs) are a recognized framework for modeling and analysis of nondeterministic systems with stochastic behavior. Recently, we proposed Abstract Probabilistic Automata (APAs) -- an abstraction framework for PAs. In this paper, we discuss APAs over dissimilar alphabets, a determinisation operator, conjunction of non-deterministic APAs, and an APA-embedding of Interface Automata. We conclude introducing a tool for automatic manipulation of APAs.", "num_citations": "22\n", "authors": ["576"]}
{"title": "Variability through the eyes of the programmer\n", "abstract": " Preprocessor directives (#ifdefs) are often used to implement compile-time variability, despite the critique that they increase complexity, hamper maintainability, and impair code comprehensibility. Previous studies have shown that the time of bug finding increases linearly with variability. However, little is known about the cognitive process of debugging programs with variability. We carry out an experiment to understand how developers debug programs with variability. We ask developers to debug programs with and without variability, while recording their eye movements using an eye tracker. The results indicate that debugging time increases for code fragments containing variability. Interestingly, debugging time also seems to increase for code fragments without variability in the proximity of fragments that do contain variability. The presence of variability correlates with increase in the number of gaze transitions\u00a0\u2026", "num_citations": "21\n", "authors": ["576"]}
{"title": "Systematic derivation of static analyses for software product lines\n", "abstract": " A recent line of work lifts particular verification and analysis methods to Software Product Lines (SPL). In an effort to generalize such case-by-case approaches, we develop a systematic methodology for lifting program analyses to SPLs using abstract interpretation. Abstract interpretation is a classical framework for deriving static analyses in a compositional, step-by-step manner. We show how to take an analysis expressed as an abstract interpretation and lift each of the abstract interpretation steps to a family of programs. This includes schemes for lifting domain types, and combinators for lifting analyses and Galois connections. We prove that for analyses developed using our method, the soundness of lifting follows by construction. Finally, we discuss approximating variability in an analysis and we derive variational data-flow equations for an example analysis, a constant propagation analysis for a simple imperative\u00a0\u2026", "num_citations": "20\n", "authors": ["576"]}
{"title": "Taming the confusion of languages\n", "abstract": " Large software systems are composed of diverse artifacts. The relations between these artifacts are usually not formalized, if the artifacts use different modeling or programming languages. This hinders component-oriented development, as interfaces of exchangeable components do not capture hidden artifact dependencies. We present GenDeMoG, a tool that allows for mining inter-component dependencies beyond those explicitly specified. GenDeMoG is a generic generator-generator parameterized with a high-level system model containing dependency specifications. So, unlike the language interface mechanisms, GenDeMoG is not restricted to any given kind of links. We apply GenDeMoG to a realistic case study\u2014an open source enterprise system, OFBiz. The experiment confirms that the stereotypical opinion about unknown dependencies across artifact types is indeed correct. Just 22 specifications\u00a0\u2026", "num_citations": "20\n", "authors": ["576"]}
{"title": "Modeling software product lines using color-blind transition systems\n", "abstract": " Families of embedded discrete finite state programs are modeled using input-enabled alternating transition systems. One model describes all functionality, while each variant is defined by an environment, describing its possible uses. The environments show both the inputs that a system can receive and indicate which of the system\u2019s responses are relevant for the environment. The latter trait, called color-blindness, creates new possibilities for system transformations in the specialization process. We demonstrate the use of the framework by applying it to two classes of realistic design languages. An example of a product line of alarm clocks is used throughout the article.", "num_citations": "20\n", "authors": ["576"]}
{"title": "Effective analysis of c programs by rewriting variability\n", "abstract": " Context. Variability-intensive programs (program families) appear in many application areas and for many reasons today. Different family members, called variants, are derived by switching statically configurable options (features) on and off, while reuse of the common code is maximized. Inquiry. Verification of program families is challenging since the number of variants is exponential in the number of features. Existing single-program analysis and verification tools cannot be applied directly to program families, and designing and implementing the corresponding variability-aware versions is tedious and laborious. Approach. In this work, we propose a range of variability-related transformations for translating program families into single programs by replacing compile-time variability with run-time variability (non-determinism). The obtained transformed programs can be subsequently analyzed using the conventional off- the-shelf single-program analysis tools such as type checkers, symbolic executors, model checkers, and static analyzers. Knowledge. Our variability-related transformations are outcome-preserving, which means that the relation between the outcomes in the transformed single program and the union of outcomes of all variants derived from the original program family is equality. Grounding. We show our transformation rules and their correctness with respect to a minimal core imperative language IMP. Then, we discuss our experience of implementing and using the transformations for efficient and effective analysis and verification of real-world C program families. Importance. We report some interesting variability-related bugs that we\u00a0\u2026", "num_citations": "19\n", "authors": ["576"]}
{"title": "Cross-language support mechanisms significantly aid software development\n", "abstract": " Contemporary software systems combine many artifacts specified in various modeling and programming languages, domainspecific and general purpose as well. Since multi-language systems are so widespread, working on them calls for tools with cross-language support mechanisms such as (1) visualization, (2) static checking, (3) navigation, and (4) refactoring of cross-language relations. We investigate whether these four mechanisms indeed improve efficiency and quality of development of multi-language systems. We run a controlled experiment in which 22 participants perform typical software evolution tasks on the JTrac web application using a prototype tool implementing these mechanisms. The results speak clearly for integration of cross-language support mechanisms into software development tools, and justify research on automatic inference, manipulation and handling of cross-language\u00a0\u2026", "num_citations": "19\n", "authors": ["576"]}
{"title": "Robust specification of real time components\n", "abstract": " Specification theories for real-time systems allow to reason about interfaces and their implementation models, using a set of operators that includes satisfaction, refinement, logical and parallel composition. To make such theories applicable throughout the entire design process from an abstract specification to an implementation, we need to be able to reason about possibility to effectively implement the theoretical specifications on physical systems. In the literature, this implementation problem has already been linked to the robustness problem for Timed Automata, where small perturbations in the timings of the models are introduced. We address the problem of robust implementations in timed specification theories. Our contributions include the analysis of robust timed games and the study of robustness with respect to the operators of the theory.", "num_citations": "18\n", "authors": ["576"]}
{"title": "Partial instances via subclassing\n", "abstract": " The traditional notion of instantiation in Object-Oriented Modeling (OOM) requires objects to be complete, i.e., be fully certain about their existence and attributes. This paper explores the notion of partial instantiation of class diagrams, which allows the modeler to omit some details of objects depending on modeler\u2019s intention. Partial instantiation allows modelers to express optional existence of some objects and slots (links) as well as uncertainty of values in some slots. We show that partial instantiation is useful and natural in domain modeling and requirements engineering. It is equally useful in architecture modeling with uncertainty (for design exploration) and with variability (for modeling software product lines).               Partial object diagrams can be (partially) completed by resolving (some of) optional objects and replacing (some of) unknown values with actual ones. Under the Closed World Assumption\u00a0\u2026", "num_citations": "17\n", "authors": ["576"]}
{"title": "APAC: A tool for reasoning about abstract probabilistic automata\n", "abstract": " We recently introduced Abstract Probabilistic Automata (APA), a new powerful abstraction formalism for probabilistic automata. Our theory is equipped with a series of aggressive abstraction techniques for state-space reduction as well as a specification theory for both logical and structural comparisons. This paper reports on the implementation of the approach in the Abstract Probabilistic Automata Checker toolset.", "num_citations": "17\n", "authors": ["576"]}
{"title": "Identifying redundancies in fork-based development\n", "abstract": " Fork-based development is popular and easy to use, but makes it difficult to maintain an overview of the whole community when the number of forks increases. This may lead to redundant development where multiple developers are solving the same problem in parallel without being aware of each other. Redundant development wastes effort for both maintainers and developers. In this paper, we designed an approach to identify redundant code changes in forks as early as possible by extracting clues indicating similarities between code changes, and building a machine learning model to predict redundancies. We evaluated the effectiveness from both the maintainer's and the developer's perspectives. The result shows that we achieve 57-83% precision for detecting duplicate code changes from maintainer's perspective, and we could save developers' effort of 1.9-3.0 commits on average. Also, we show that our\u00a0\u2026", "num_citations": "16\n", "authors": ["576"]}
{"title": "Consistency and refinement for interval markov chains\n", "abstract": " Interval Markov Chains (IMC), or Markov Chains with probability intervals in the transition matrix, are the base of a classic specification theory for probabilistic systems [18]. The standard semantics of IMCs assigns to a specification the set of all Markov Chains that satisfy its interval constraints. The theory then provides operators for deciding emptiness of conjunction and refinement (entailment) for such specifications.In this paper, we study complexity of several problems for IMCs, that stem from compositional modeling methodologies. In particular, we close the complexity gap for thorough refinement of two IMCs and for deciding the existence of a common implementation for an unbounded number of IMCs, showing that these problems are EXPTIME-complete.We discuss suitable notions of determinism for specifications, and show that for deterministic IMCs the syntactic refinement operators are complete with respect\u00a0\u2026", "num_citations": "16\n", "authors": ["576"]}
{"title": "From transition systems to variability models and from lifted model checking back to UPPAAL\n", "abstract": " Variational systems (system families) allow effective building of many custom system variants for various configurations. Lifted (family-based) verification is capable of verifying all variants of the family simultaneously, in a single run, by exploiting the similarities between the variants. These algorithms scale much better than the simple enumerative \u201cbrute-force\u201d way. Still, the design of family-based verification algorithms greatly depends on the existence of compact variability models (state representations). Moreover, developing the corresponding family-based tools for each particular analysis is often tedious and labor intensive.                 In this work, we make two contributions. First, we survey the history of development of variability models of computation that compactly represent behavior of variational systems. Second, we introduce variability abstractions that simplify variability away to achieve efficient\u00a0\u2026", "num_citations": "15\n", "authors": ["576"]}
{"title": "Refinement for transition systems with responses\n", "abstract": " Motivated by the response pattern for property specifications and applications within flexible workflow management systems, we report upon an initial study of modal and mixed transition systems in which the must transitions are interpreted as must eventually, and in which implementations can contain may behaviors that are resolved at run-time. We propose Transition Systems with Responses (TSRs) as a suitable model for this study. We prove that TSRs correspond to a restricted class of mixed transition systems, which we refer to as the action-deterministic mixed transition systems. We show that TSRs allow for a natural definition of deadlocked and accepting states. We then transfer the standard definition of refinement for mixed transition systems to TSRs and prove that refinement does not preserve deadlock freedom. This leads to the proposal of safe refinements, which are those that preserve deadlock freedom. We exemplify the use of TSRs and (safe) refinements on a small medication workflow.", "num_citations": "15\n", "authors": ["576"]}
{"title": "Modal and mixed specifications: key decision problems and their complexities\n", "abstract": " Modal and mixed transition systems are specification formalisms that allow the mixing of over- and under-approximation. We discuss three fundamental decision problems for such specifications:       \u2014whether a set of specifications has a common implementation;\u2014whether an individual specification has an implementation; and\u2014whether all implementations of an individual specification are implementations of another one.       For each of these decision problems we investigate the worst-case computational complexity for the modal and mixed cases. We show that the first decision problem is EXPTIME-complete for both modal and mixed specifications. We prove that the second decision problem is EXPTIME-complete for mixed specifications (it is known to be trivial for modal ones). The third decision problem is also shown to be EXPTIME-complete for mixed specifications.", "num_citations": "15\n", "authors": ["576"]}
{"title": "Techniques for efficient interactive configuration of distribution networks\n", "abstract": " Recovering from power outages is an essential task in distribution of electricity. Our industrial partner postulates that the recovery should be interactive rather than automatic: supporting the operator by preventing choices that destabilize the network. Interactive configurators, successfully used in specifying products and services, support users in selecting logically constrained parameters in a sound, complete and backtrack-free manner. Interactive restoration algorithms based on reduced ordered binary decision diagrams (BDDs) had been developed also for power distribution networks, however they did not scale to the large instances, as BDDs representing these could not be compiled. We discuss the theoretical hardness of the interactive configuration and then provide techniques used to compile two classes of networks. We handle the largest industrial instances available. Our techniques rely on symbolic reachability computation, early variable quantification, domain specific ordering heuristics and conjunctive decomposition.", "num_citations": "15\n", "authors": ["576"]}
{"title": "Why does code review work for open source software communities?\n", "abstract": " Open source software communities have demonstrated that they can produce high quality results. The overall success of peer code review, commonly used in open source projects, has likely contributed strongly to this success. Code review is an emotionally loaded practice, with public exposure of reputation and ample opportunities for conflict. We set off to ask why code review works for open source communities, despite this inherent challenge. We interviewed 21 open source contributors from four communities and participated in meetings of ROS community devoted to implementation of the code review process. It appears that the hacker ethic is a key reason behind the success of code review in FOSS communities. It is built around the ethic of passion and the ethic of caring. Furthermore, we observed that tasks of code review are performed with strong intrinsic motivation, supported by many non-material\u00a0\u2026", "num_citations": "14\n", "authors": ["576"]}
{"title": "Symbolic execution of high-level transformations\n", "abstract": " Transformations form an important part of developing domain specific languages, where they are used to provide semantics for typing and evaluation. Yet, few solutions exist for verifying transformations written in expressive high-level transformation languages. We take a step towards that goal, by developing a general symbolic execution technique that handles programs written in these high-level transformation languages. We use logical constraints to describe structured symbolic values, including containment, acyclicity, simple unordered collections (sets) and to handle deep type-based querying of syntax hierarchies. We evaluate this symbolic execution technique on a collection of refactoring and model transformation programs, showing that the white-box test generation tool based on symbolic execution obtains better code coverage than a black box test generator for such programs in almost all tested cases.", "num_citations": "14\n", "authors": ["576"]}
{"title": "Efficient interactive configuration of unbounded modular systems\n", "abstract": " Interactive configuration guides a user searching through a large combinatorial space of solutions to a system of constraints. We investigate a class of very expressive underlying constraint satisfaction problems: modular recursive constraint systems of unbounded size. A precomputation step is used to obtain a configuration algorithm for such systems that supports the user efficiently with bounded response time. This precomputation step determines all solutions for each module, which are computed and stored in compact data structures such as Binary Decision Diagrams (BDDs), in order to eliminate run-time search. The precomputation step also detects ill-behaved module collections that have no finite solutions. The runtime interaction algorithm scales well as its response time only depends on the amount of the information passed locally between the modules, and not on the size of the entire configured structure\u00a0\u2026", "num_citations": "14\n", "authors": ["576"]}
{"title": "On the formal semantics of visualSTATE statecharts\n", "abstract": " This paper presents a formal semantics of statecharts\u2013a visual language successfully employed in design of control algorithms. Our formalization is implementation oriented, with efficiency in the focus. It has been used as a specification in development of scope, an experimental code generator for embedded systems.The version of statecharts we describe is that implemented in commercial development tool IAR visualstate. IAR visualstate statecharts are similar to Harel\u2019s original statecharts, with several additions and some restrictions. They mostly agree with UML state diagrams on syntax and semantics. A small survey is appended comparing visualstate statecharts terminology and concepts with those of D. Harel\u2019s original statecharts and UML statechart diagrams. The paper may be perceived as a formal equivalent to the official Concept Guide delivered with the visualstate software package from IAR.", "num_citations": "14\n", "authors": ["576"]}
{"title": "Code generation and model driven development for constrained embedded software\n", "abstract": " We consider statechart models of discrete control embedded programs operating under severe memory constraints. There have been very few results in code generation for such systems. We analyze code generation methods for embededded processors utilizing C as an intermediate language and runtime interpreters. We choose a suitable subset of hierarchical statecharts and engineer an efficient interpreter for programs in it. An algorithm is provided that simplifies general models to our sublanguage removing dynamic scoping and transition conflicts. The resulting code generator improves over an industrial implementation provided by IAR A/S. The interpreter for hierarchical statecharts is complex. We define flattening as a process of transforming hierarchical models into their hierarchy-less counterparts. We prove that even with a simulation-based correctness criterion any flattening algorithm would cause a super polynomial growth of models, if it does not exploit message passing. Then we devise a polynomial flattening algorithm based on internal asynchronous communication in the model. The implementation of this algorithm beats our earlier hierarchical code generator by 20\u201330% on realistic examples. In the second part of the thesis we develop a unified theory for specifying correctness of model transformations and modeling software product lines. Our framework is based on a novel notion of color-blindness: a dynamically changing inability of the environment to observe differences in system outputs. Being safe approximations of all possible usage scenarios such environments can be used to specify specialized versions of the product\u00a0\u2026", "num_citations": "13\n", "authors": ["576"]}
{"title": "Finding suitable variability abstractions for lifted analysis.\n", "abstract": " Many software systems are today variational: they are built as program families or Software Product Lines. They can produce a potentially huge number of related programs, known as products or variants, by selecting suitable configuration options (features) at compile time. Many such program families are safety critical, yet the appropriate tools only rarely are able to analyze them effeciently. Researchers have addressed this problem by designing specialized variability-aware static (dataflow) analyses, which allow analyzing all variants of the family, simultaneously, in a single run without generating any of the variants explicitly. They are also known as lifted or family-based analyses. They take as input the common code base, which encodes all variants of a program family, and produce precise analysis results corresponding to all variants. These analyses scale much better than \u201cbrute force\u201d approach, where all individual variants are analyzed in isolation, one-by-one, using off-the-shelf single-program analyzers. Nevertheless, the computational cost of lifted analyses still greatly depends on the number of features and variants (which is often huge). For families with a large number of features and variants, the lifted analyses may be too costly or even infeasible. In order to speed up lifted analyses and make them computationally cheaper, variability abstractions which simplify variability away from program families and lifted analyses have been introduced. However, the space of possible variability abstractions is still intractably large to search naively, with most abstractions being either too imprecise or too costly.We introduce here a method to\u00a0\u2026", "num_citations": "12\n", "authors": ["576"]}
{"title": "Influencers of quality assurance in an open source community\n", "abstract": " ROS (Robot Operating System) is an open source community in robotics that is developing standard robotics operating system facilities such as hardware abstraction, low-level device control, communication middleware, and a wide range of software components for robotics functionality. This paper studies the quality assurance practices of the ROS community. We use qualitative methods to understand how ideology, priorities of the community, culture, sustainability, complexity, and adaptability of the community affect the implementation of quality assurance practices. Our analysis suggests that software engineering practices require social and cultural alignment and adaptation to the community particularities to achieve seamless implementation in open source environments. This alignment should be incorporated into the design and implementation of quality assurance practices in open source communities.", "num_citations": "12\n", "authors": ["576"]}
{"title": "Finding suitable variability abstractions for family-based analysis\n", "abstract": " For program families (Software Product Lines), specially designed variability-aware static (dataflow) analyses allow analyzing all variants (products) of the family, simultaneously, in a single run without generating any of the variants explicitly. They are also known as lifted or family-based analyses. The variability-aware analyses may be too costly or even infeasible for families with a large number of variants. In order to make them computationally cheaper, we can apply variability abstractions which aim to tame the combinatorial explosion of the number of variants (configurations) and reduce it to something more tractable. However, the number of possible abstractions is still intractably large to search naively, with most abstractions being too imprecise or too costly.                 In this work, we propose a technique to efficiently find suitable variability abstractions from a large family of abstractions for a variability\u00a0\u2026", "num_citations": "11\n", "authors": ["576"]}
{"title": "Robust synthesis for real-time systems\n", "abstract": " Specification theories for real-time systems allow reasoning about interfaces and their implementation models, using a set of operators that includes satisfaction, refinement, logical and parallel composition. To make such theories applicable throughout the entire design process from an abstract specification to an implementation, we need to reason about the possibility to effectively implement the theoretical specifications on physical systems, despite their limited precision. In the literature, this implementation problem has been linked to the robustness problem that analyzes the consequences of introducing small perturbations into formal models.We address this problem of robust implementations in timed specification theories. We first consider a fixed perturbation and study the robustness of timed specifications with respect to the operators of the theory. To this end we synthesize robust strategies in timed games\u00a0\u2026", "num_citations": "11\n", "authors": ["576"]}
{"title": "Variability abstraction and refinement for game-based lifted model checking of full CTL\n", "abstract": " Variability models allow effective building of many custom model variants for various configurations. Lifted model checking for a variability model is capable of verifying all its variants simultaneously in a single run by exploiting the similarities between the variants. The computational cost of lifted model checking still greatly depends on the number of variants (the size of configuration space), which is often huge. One of the most promising approaches to fighting the configuration space explosion problem in lifted model checking are variability abstractions. In this work, we define a novel game-based approach for variability-specific abstraction and refinement for lifted model checking of the full CTL, interpreted over 3-valued semantics. We propose a direct algorithm for solving a 3-valued (abstract) lifted model checking game. In case the result of model checking an abstract variability model is indefinite, we\u00a0\u2026", "num_citations": "10\n", "authors": ["576"]}
{"title": "Interfaces and metainterfaces for models and metamodels\n", "abstract": " Evolution and customization of component-based systems require an explicit understanding of component inter-dependencies. Implicit assumptions, poor documentation and hidden dependencies turn even simple changes into challenges. The problem is exacerbated in XML-intensive projects due to the use of soft references and the lack of information hiding. We address this with dependency tracking interface types for models and metamodels. We provide automatic compatibility checks and a heuristic inference procedure for our interfaces, which allows easy and incremental adoption of our technique even in mature projects. We have implemented a prototype and applied it to two large cases: an enterprise resource planning system and a healthcare information system.", "num_citations": "10\n", "authors": ["576"]}
{"title": "The design space of multi-language development environments\n", "abstract": " Non-trivial software systems integrate many artifacts expressed in multiple modeling and programming languages. However, even though these artifacts heavily depend on each other, existing development environments do not sufficiently support handling relations between artifacts in different languages. By means of a literature survey, tool prototyping, and experiments, we study the design space of multi-language development environments (MLDEs)\u2014tools that consider cross-language relations as first artifacts. We ask: What is the state of the art in the MLDE space? What are the design choices and challenges faced by tool builders? To what extent are MLDEs desired by users, and what aspects of MLDEs are particularly helpful? Our main conclusions are that (a) cross-language relations are ubiquitous and troublesome in multi-language systems, (b) users highly appreciate cross-language support\u00a0\u2026", "num_citations": "9\n", "authors": ["576"]}
{"title": "Color-blind specifications for transformations of reactive synchronous programs\n", "abstract": " Execution environments are used as specifications for specialization of input-output programs in the derivation of product lines. These environments, formalized as color-blind I/O-alternating transition systems, are tolerant to mutations in a given program\u2019s outputs. Execution environments enable new compiler optimizations, vastly exceeding usual reductions. We propose a notion of context-dependent refinement for I/O-alternating transition systems, which supports composition and hierarchical reuse. The framework is demonstrated by discussing adaptations to realistic design languages and by presenting an exampleof a product line.", "num_citations": "9\n", "authors": ["576"]}
{"title": "Compositional design methodology with constraint markov chains\n", "abstract": " A specification theory combines notions of specification and implementation with a satisfaction relation, a refinement relation and a set of operators that together support stepwise design. We propose a new abstraction, Constraint Markov Chains, and use it to construct a specification theory for Markov Chains. Constraint Markov Chains generalize previously known abstractions by allowing arbitrary constraints on probability distributions. Our theory is the first specification theory for Markov Chains closed under conjunction, parallel composition and synchronization. Moreover, all the operators and relations introduced are computable.", "num_citations": "8\n", "authors": ["576"]}
{"title": "Exptime-complete decision problems for modal and mixed specifications\n", "abstract": " Modal and mixed transition systems are formalisms that allow mixing of over- and under-approximation in a single specification. We show EXPTIME-completeness of three fundamental decision problems for such specifications: whether a set of modal or mixed specifications has a common implementation, whether a sole mixed specification has an implementation, and whether all implementations of one mixed specification are implementations of another mixed or modal one. These results are obtained by a chain of reductions starting with the acceptance problem for linearly bounded alternating Turing machines.", "num_citations": "7\n", "authors": ["576"]}
{"title": "Compile-time scope resolution for statecharts transitions\n", "abstract": " Despite the success of statecharts, surprisingly little research effort has been devoted to improving code synthesis techniques for them. The work presented below is an outcome of growing interest in the area. We discuss one possible improvement for code generators retaining explicit information about the model hierarchy. The problem of dynamic scope for multitarget transitions is described and an algorithm for compile-time detection and resolution is presented. Finally we examine the possibilities of employing this technology in various compilation optimization for classical UML state diagrams which do not allow multitarget transitions.", "num_citations": "7\n", "authors": ["576"]}
{"title": "Bayesian network mining system\n", "abstract": " The paper provides an exhaustive description of a new system serving learning, viewing and reasoning with Bayesian networks.", "num_citations": "7\n", "authors": ["576"]}
{"title": "Variability abstractions for lifted analyses\n", "abstract": " Family-based (lifted) static analysis for \u201chighly configurable programs\u201d(program families) is capable of analyzing all variants at once without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a program family, and produces precise analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants, which is in the worst case exponential in the number of statically configurable options (features). For a large number of features, the lifted analysis may be too costly or even infeasible. In this work, we introduce variability abstractions defined as Galois connections, which simplify variability away from program families based on# ifdef-s. Then, we use abstract interpretation as a formal method for the calculational-based derivation of abstracted lifted analyses, which are sound by\u00a0\u2026", "num_citations": "6\n", "authors": ["576"]}
{"title": "Variability abstractions: Trading precision for speed in family-based analyses (extended version)\n", "abstract": " Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is capable of analyzing all valid products (variants) without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a SPL, and produces analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants (which is exponential in the number of features, in the worst case). For a large number of features, the lifted analysis may be too costly or even infeasible. In this paper, we introduce variability abstractions defined as Galois connections and use abstract interpretation as a formal method for the calculational-based derivation of approximate (abstracted) lifted analyses of SPL programs, which are sound by construction. Moreover, given an abstraction we define a syntactic transformation that translates any SPL program into an abstracted version of it, such that the analysis of the abstracted SPL coincides with the corresponding abstracted analysis of the original SPL. We implement the transformation in a tool, reconfigurator that works on Object-Oriented Java program families, and evaluate the practicality of this approach on three Java SPL benchmarks.", "num_citations": "6\n", "authors": ["576"]}
{"title": "Information leakage of non-terminating processes\n", "abstract": " In recent years, quantitative security techniques have been providing effective measures of the security of a system against an attacker. Such techniques usually assume that the system produces a finite amount of observations based on a finite amount of secret bits and terminates, and the attack is based on these observations. By modeling systems with Markov chains, we are able to measure the effectiveness of attacks on non-terminating systems. Such systems do not necessarily produce a finite amount of output and are not necessarily based on a finite amount of secret bits. We provide characterizations and algorithms to define meaningful measures of security for non-terminating systems, and to compute them when possible. We also study the bounded versions of the problems, and show examples of non-terminating programs and how their effectiveness in protecting their secret can be measured.", "num_citations": "6\n", "authors": ["576"]}
{"title": "Affiliated participation in open source communities\n", "abstract": " Background: The adoption of Free/Libre and Open Source Software (FOSS) by institutions is significantly increasing, and so is the affiliated participation (the participation of industry engineers in open source communities as part of their jobs). Aims: This study is an investigation into affiliated participation in FOSS communities. So far, little is known about the affiliated participation and the forces that influence it, even though the FOSS innovation model is increasingly becoming a serious contender for the private investment model in many sectors. Method: We present a qualitative inquiry into affiliated participation in the Robot Operating System (ROS) and Linux Kernel communities, using twenty-one in-depth interviews and participatory observation data from twenty-nine community events. Results: Our results show that affiliated participation in these communities is constrained by several barriers: objections of senior\u00a0\u2026", "num_citations": "5\n", "authors": ["576"]}
{"title": "Synthesis of a reconfiguration service for mixed-criticality multi-core systems: An experience report\n", "abstract": " Task-level reconfiguration techniques in automotive applications aim to reallocate tasks to computation cores during failures to guarantee that the desired functionality is still delivered. We consider a class of mixed-criticality asymmetric multi-core systems inspired by our collaboration with a leading automotive manufacturing company, for which we automatically synthesize task-level reconfiguration services to reduce the number of processing cores and decrease the cost without weakening fault-tolerance. We admit the following types of faults: safety violations by tasks, permanent core failures, and temporary core failures. We use timed games to synthesize the controllers. The services suspend and reinstate the periodic executions of the non-critical tasks to ensure enough processing capacity for the critical tasks by maintaining lookup tables, which keep track of processing capacity. We present a\u00a0\u2026", "num_citations": "5\n", "authors": ["576"]}
{"title": "Language-Independent Traceability with L\u00e4ssig\n", "abstract": " Typical programming languages, including model transformation languages, do not support traceability. Applications requiring inter-object traceability implement traceability support repeatedly for different domains. In this paper we introduce a solution for generic traceability which enables the generation of trace models for all programming languages compiling to Virtual Machine (VM) bytecode by leveraging automatically generated observer aspects.               We implement our solution in a tool called L\u00e4ssig adding traceability support to all programming languages compiling to the Java Virtual Machine (JVM). We evaluate and discuss general feasibility, correctness, and the performance overhead of our solution by applying it to three model-to-model transformations.               Our generic traceability solution is capable of automatically establishing complete sets of trace links for transformation programs in\u00a0\u2026", "num_citations": "5\n", "authors": ["576"]}
{"title": "New results for constraint markov chains\n", "abstract": " This paper studies compositional reasoning theories for stochastic systems. A specification theory combines notions of specification and implementation with satisfaction and refinement relations, and a set of operators that together support stepwise design. One of the first behavioral specification theories introduced for stochastic systems is the one of Interval Markov Chains (IMCs), which are Markov Chains whose probability distributions are replaced by a conjunction of intervals. In this paper, we show that IMCs are not closed under conjunction, which gives a formal proof of a conjecture made in several recent works.In order to leverage this problem, we suggested to work with Constraint Markov Chains (CMCs) that is another specification theory where intervals are replaced with general constraints. Contrary to IMCs, one can show that CMCs enjoy the closure properties of a specification theory. In addition, we\u00a0\u2026", "num_citations": "5\n", "authors": ["576"]}
{"title": "On Succinctness of Hierarchical State Diagrams in Absence of Message Passing\n", "abstract": " We show a subexponential but superpolynomial lower bound for flattening problem for statecharts. The result explains why common flattening algorithms explode, if the signal communication is excluded from the target language. This specifically affects flattening-based strategies for automatic model-based program synthesis.", "num_citations": "5\n", "authors": ["576"]}
{"title": "The forgotten case of the dependency bugs: On the example of the robot operating system\n", "abstract": " A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.The paper presents a definition and a taxonomy of dependency bugs extracted from data. It describes multiple facets of these bugs and estimates that as many as 15% (!) of all reported bugs are dependency bugs. We show that lightweight tools can find dependency bugs efficiently, although it is challenging to decide which tools to build and difficult to build general tools. We present the research problem to the\u00a0\u2026", "num_citations": "4\n", "authors": ["576"]}
{"title": "Verification of high-level transformations with inductive refinement types\n", "abstract": " High-level transformation languages like Rascal include expressive features for manipulating large abstract syntax trees: first-class traversals, expressive pattern matching, backtracking and generalized iterators. We present the design and implementation of an abstract interpretation tool, Rabit, for verifying inductive type and shape properties for transformations written in such languages. We describe how to perform abstract interpretation based on operational semantics, specifically focusing on the challenges arising when analyzing the expressive traversals and pattern matching. Finally, we evaluate Rabit on a series of transformations (normalization, desugaring, refactoring, code generators, type inference, etc.) showing that we can effectively verify stated properties.", "num_citations": "4\n", "authors": ["576"]}
{"title": "Effective bug finding in c programs with shape and effect abstractions\n", "abstract": " Software tends to suffer from simple resource mis-manipulation bugs, such as double-locks. Code scanners are used extensively to remove these bugs from projects like the Linux kernel. Yet, these tools are not effective when the manipulation of resources spans multiple functions. We present a shape-and-effect analysis for C, that enables efficient and scalable inter-procedural reasoning about resource manipulation. This analysis builds a program abstraction based on the observable side-effects of functions. Bugs are found by model checking this abstraction, matching undesirable sequences of operations. We implement this approach in the Eba tool, and evaluate it on a collection of historical double-lock bugs from the Linux kernel. Our results show that our tool is more effective at finding bugs than similar code-scanning tools. Eba analyzes nine thousand Linux files in less than half an hour, and uncovers\u00a0\u2026", "num_citations": "4\n", "authors": ["576"]}
{"title": "Tengi interfaces for tracing between heterogeneous components\n", "abstract": " Contemporary software systems comprise many heterogeneous artifacts; some expressed in general programming languages, some in visual and textual domain-specific languages and some in ad hoc textual formats. During construction of a system diverse artifacts are interrelated. Only few formats, typically general programming languages, provide an interface description mechanism able to specify software component boundaries. Unfortunately, these interface mechanisms can not express relations for components containing heterogeneous artifacts.               We introduce Tengi, a tool that allows for the definition of software components containing heterogeneous artifacts. Tengi interfaces link components containing different textual and visual software development artifacts ranging from high-level specification documents to low-level implementation documents. We formally define and implement Tengi\u00a0\u2026", "num_citations": "4\n", "authors": ["576"]}
{"title": "On greatest lower bound of modal transition systems\n", "abstract": " Modal Transition Systems (MTSs) are finite-state automata whose transitions are typed with may and must modalities. MTSs can be used to represent a possibly infinite set of transition systems (TSs) that are its implementations. Informally, a must transition is available in every TS that implements the MTS, while a may transition needs not be. Given two MTSs, we consider the problem of computing their greatest lower bound (GLB), ie, a new MTSs whose set of implementations is the intersection of those of the original MTSs. We show that for non-deterministic MTSs, such an intersection may not be computable. We then consider Acceptance Set Automata (ASAs) that is an extension of MTSs where may and must modalities are replaced by sets of actions. We show that the GLB of two ASs can always be computed. We conclude by showing that, contrary to the deterministic case, the class of non-deterministic MTSs is not a proper subclass of the one of non-deterministic ASAs.", "num_citations": "4\n", "authors": ["576"]}
{"title": "MROS: Runtime Adaptation For Robot Control Architectures\n", "abstract": " Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). These architectures need to be dynamically adaptable in order to cope with changing environment conditions, new mission requirements or component failures. The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. We present a structured model-based framework for the adaptation of robot control architectures at run-time to satisfy set quality requirements. We use a formal meta-model to represent the configuration space of control architectures and the corresponding mission requirements. The meta-model is implemented as an OWL ontology with SWRL rules, enabling the use of an off-the-shelf reasoner for diagnostics and adaptation. The method is discussed and evaluated using two case studies of real, ROS-based systems: (i) for an autonomous dual arm mobile manipulator building a pyramid and (ii) a mobile robot navigating in a factory environment.", "num_citations": "3\n", "authors": ["576"]}
{"title": "A model for industrial real-time systems\n", "abstract": " Introducing automated formal methods for large industrial real-time systems is an important research challenge. We propose timed process automata (TPA) for modeling and analysis of time-critical systems which can be open, hierarchical, and dynamic. The model offers two essential features for large industrial systems: (i) compositional modeling with reusable designs for different contexts, and (ii) an automated state-space reduction technique. Timed process automata model dynamic networks of continuous-time communicating control processes which can activate other processes. We show how to automatically establish safety and reachability properties of TPA by reduction to solving timed games. To mitigate the state-space explosion problem, an automated state-space reduction technique using compositional reasoning and aggressive abstractions is also proposed.", "num_citations": "3\n", "authors": ["576"]}
{"title": "Interface input/output automata: Splitting assumptions from guarantees\n", "abstract": " We propose a new look at one of the most fundamental types of behavioral interfaces: discrete time specifications of communication---directly related to the work of de Alfaro and Henzinger [3]. Our framework is concerned with distributed non-blocking asynchronous systems in the style of Lynch's\\IOAs [11], relying on a context dependent notion of refinement based on relativized language inclusion. There are two main contributions of the work. First, we explicitly separate assumptions from guarantees, increasing the modeling power of the specification language and demonstrating an interesting relation between blocking and non-blocking interface frameworks. Second, our composition operator is systematically and formally derived from the requirements stated as a system of inequalities. The derived composed interfaces are maximal in the sense of behavior, or equivalently are the weakest in the sense of assumptions. Finally, we present a method for solving systems of inequalities as used in our setup.", "num_citations": "3\n", "authors": ["576"]}
{"title": "Generalized abstraction-refinement for game-based CTL lifted model checking\n", "abstract": " System families (Software Product Lines) are becoming omnipresent in application areas ranging from embedded system domains to system-level software and communication protocols. Software Product Line methods and architectures allow effective building many custom variants of a software system in these domains. In many of the applications, their rigorous verification and quality assurance are of paramount importance. Lifted model checking for system families is capable of verifying all their variants simultaneously in a single run by exploiting the similarities between the variants. The computational cost of lifted model checking still greatly depends on the number of variants (the size of configuration space), which is often huge. Variability abstractions have successfully addressed this configuration space explosion problem, giving rise to smaller abstract variability models with fewer abstract configurations\u00a0\u2026", "num_citations": "2\n", "authors": ["576"]}
{"title": "Clafer: Lightweight Modeling of Structure, Behaviour, and Variability Research output: Journal Article or Conference Article in Journal\u203a Journal article\u203a Research\u203a peer-review\n", "abstract": " Embedded software is growing fast in size and complexity, leading to intimate mixture of complex architectures and complex control. Consequently, software speci cation requires modeling both structures and behaviour of systems. Unfortunately, existing languages do not integrate these aspects well, usually prioritizing one of them. It is common to develop a separate language for each of these facetsIn this paper, we contribute Clafer: a small language that attempts to tackle this challenge. It combines rich structural modeling with state of the art behavioural formalisms. We are not aware of any other modeling language that seamlessly combines these facets common to system and software modeling.", "num_citations": "2\n", "authors": ["576"]}
{"title": "Programming storage controllers with ox\n", "abstract": " Offloading processing to storage is a means to minimize data movement and efficiently scale processing to match the increasing volume of stored data. In recent years, the rate at which data is transferred from storage has increased exponentially, while the rate at which data is transferred from memory to a host processor (CPU) has only increased linearly. This trend is expected to continue in the coming years. Soon, CPUs will not be able to keep up with the rate at which stored data is transferred. The increasing volumes of store data compound this problem. The solution is to offload processing from host CPU to storage controllers [1]. Twenty years ago, Jim Gray wrote: Put Everything in Future Disk Controllers (it\u2019s not \u201cif\u201d, it\u2019s \u201cwhen\u201d)[7]. His argument was that running application code on disk controllers would be (a) possible because disks would be equipped with powerful processors and connected to networks via high-level protocols, and (b) necessary to minimize data movement. He concluded that there would be a need for a programming environment for disk controllers. In this paper, we follow-up on Jim Gray\u2019s prediction and reflect on initial lessons learnt programming storage controllers with OX. In the 90s, pioneering efforts to develop Active Disks were based on magnetic drives. Today, renewed efforts fall in two groups. The first group combines Open-Channel SSDs [2] with a programmable storage controller integrated with a fabrics front-end. The second group integrates a programmable storage controller directly onto a SSD (eg, ScaleFlux, NGD). For both groups, the programmable storage is either a Linux-based ARM processor or\u00a0\u2026", "num_citations": "2\n", "authors": ["576"]}
{"title": "Going Beyond Obscurity: Organizational Approaches to Data Anonymization\n", "abstract": " Anonymization is viewed as a solution to over-exposure of personal information in a data-driven society. Yet how organizations apply anonymization techniques to data for regulatory, ethical or commercial reasons remains underexplored. We investigate how such measures are applied in organizations, asking whether anonymization practices are used, what approaches are considered practical and adequate, and how decisions are made to protect the privacy of data subjects while preserving analytical value. Our findings demonstrate that anonymization is applied to data far less pervasively than expected. Organizations that do employ anonymization often view their practices as sensitive and resort to anonymity by obscurity alongside technical means. Rather than being a purely technical question of applying the right algorithms, anonymization in practice is a complex socio-technical process that relies on multi\u00a0\u2026", "num_citations": "2\n", "authors": ["576"]}
{"title": "Synthesis of a reconfiguration service for mixed-criticality multi-core systems\n", "abstract": " Task-level reconfiguration techniques in automotive applications aim to reallocate tasks to computation cores during failures to guarantee that the desired functionality is still delivered. We consider a class of mixed-criticality asymmetric multi-core systems inspired by our collaboration with General Motors, for which we automatically synthesize task-level reconfiguration services to reduce the number of processing cores and decrease cost without weakening fault-tolerance. We admit the following types of faults: safety violations by tasks, permanent core failures, and temporary core failures. We use timed games to synthesize the controllers. The services suspend and reinstate the periodic executions of the non-critical tasks to ensure enough processing capacity for the critical tasks by maintaining lookup tables, which keep track of processing capacity. We present a methodology to synthesize the services and use a case study to show that suitable abstractions can dramatically improve the scalability of timed games-based tools for solving industrial problems.", "num_citations": "2\n", "authors": ["576"]}
{"title": "An aspect-based traceability mechanism for domain specific languages\n", "abstract": " Development environments for domain specific modeling usually represent elements of visual models as objects when in memory and as XML elements when persisted. Visual models are editable using different kinds of editors, and both the in-memory representations and the serialization syntax can be manipulated by automatic tools. We present Tengja, a toolkit, that automatically collects the traces between model elements in abstract, visual, and serialization syntax. Once the trace model is established by Tengja it can be used by other applications to synchronize representations involved, or to navigate across models. We demonstrate the toolkit by implementing a simple navigation support on top of it.", "num_citations": "2\n", "authors": ["576"]}
{"title": "Code Generation for Embedded Systems\n", "abstract": " BackgroundOver the last decade IT technology has entered the market of industrial and home appliances. The number of specialized micro-processors embedded in devices already far exceeds the number of CPUs in personal computers. Their overwhelming success motivates the development of methodologies and tools supporting definition and implementation of control algorithms for embedded systems.Development of embedded software faces somewhat different problems than standard PC programming, where memory is practically unlimited (for most applications) and computation speed is fast and ever increasing. Standard workstation compilers optimize for speed and are implemented to get short compilation times. Despite the success of multi-threaded operating systems, most of the program logic is still inherently sequential.", "num_citations": "2\n", "authors": ["576"]}
{"title": "A Modeling Tool for Reconfigurable Skills in ROS\n", "abstract": " Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. Reusable solutions that support complex, runtime reasoning for robot adaptation have been seen in the adoption of ontologies. While the usage of ontologies significantly increases system reuse and maintainability, it requires additional effort from the application developers to translate requirements into formal rules that can be used by an ontological reasoner. In this paper, we present a design tool that facilitates the specification of reconfigurable robot skills. Based on the specified skills, we generate corresponding runtime models for self-adaptation that can be directly deployed to a running robot that uses a reasoning approach based on ontologies. We\u00a0\u2026", "num_citations": "1\n", "authors": ["576"]}
{"title": "Verification of program transformations with inductive refinement types\n", "abstract": " High-level transformation languages like Rascal include expressive features for manipulating large abstract syntax trees: first-class traversals, expressive pattern matching, backtracking, and generalized iterators. We present the design and implementation of an abstract interpretation tool, Rabit, for verifying inductive type and shape properties for transformations written in such languages. We describe how to perform abstract interpretation based on operational semantics, specifically focusing on the challenges arising when analyzing the expressive traversals and pattern matching. Finally, we evaluate Rabit on a series of transformations (normalization, desugaring, refactoring, code generators, type inference, etc.) showing that we can effectively verify stated properties.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Privug: Quantifying Leakage using Probabilistic Programming for Privacy Risk Analysis\n", "abstract": " Disclosure of data analytics has important scientific and commercial justifications. However, no data shall be disclosed without a diligent investigation of risks posed for privacy of subjects. Do data analysts have the right tools to perform such investigations? Privug is a tool-supported method to explore information leakage properties of programs producing the analytics to be disclosed. It uses classical off-the-shelf tools for Bayesian programming, reinterpreting a regular program probabilistically. This in turn allows information-theoretic analysis of program behavior. For privacy researchers, the method provides a fast and lightweight way to experiment with privacy protection measures and mechanisms. We demonstrate that Privug is accurate, scalable, and applicable. We show how to use it to explore parameters of differential privacy, and how to benefit from a range of leakage estimators.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Galois Connections for Recursive Types\n", "abstract": " Building a static analyser for a real language involves modeling of large domains capturing the many available data types. To scale domain design and support efficient development of project-specific analyzers, it is desirable to be able to build, extend, and change abstractions in a systematic and modular fashion. We present a framework for modular design of abstract domains for recursive types and higher-order functions, based on the theory of solving recursive domain equations. We show how to relate computable abstract domains to our framework, and illustrate the potential of the construction by modularizing a monolithic domain for regular tree grammars. A prototype implementation in the dependently typed functional language Agda shows how the theoretical solution can be used in practice to construct static analysers.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Variability Abstraction and Refinement for Game-based Lifted Model Checking of full CTL (Extended Version)\n", "abstract": " Variability models allow effective building of many custom model variants for various configurations. Lifted model checking for a variability model is capable of verifying all its variants simultaneously in a single run by exploiting the similarities between the variants. The computational cost of lifted model checking still greatly depends on the number of variants (the size of configuration space), which is often huge. One of the most promising approaches to fighting the configuration space explosion problem in lifted model checking are variability abstractions. In this work, we define a novel game-based approach for variability-specific abstraction and refinement for lifted model checking of the full CTL, interpreted over 3-valued semantics. We propose a direct algorithm for solving a 3-valued (abstract) lifted model checking game. In case the result of model checking an abstract variability model is indefinite, we suggest a new notion of refinement, which eliminates indefinite results. This provides an iterative incremental variability-specific abstraction and refinement framework, where refinement is applied only where indefinite results exist and definite results from previous iterations are reused.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Controller synthesis for dynamic hierarchical real-time plants using timed automata\n", "abstract": " We use timed I/O automata based timed games to synthesize task-level reconfiguration services for cost-effective fault tolerance in a case study. The case study shows that state-space explosion is a severe problem for timed games. By applying suitable abstractions, we dramatically improve the scalability. However, timed I/O automata do not facilitate algorithmic abstraction generation techniques. The case study motivates the development of timed process automata to improve modeling and analysis for controller synthesis of time-critical plants which can be hierarchical and dynamic. The model offers two essential features for industrial systems: (i) compositional modeling with reusable designs for different contexts, and (ii) state-space reduction technique. Timed process automata model dynamic networks of continuous-time communicating plant processes which can activate other plant processes. We\u00a0\u2026", "num_citations": "1\n", "authors": ["576"]}
{"title": "Modelling Foundations and Applications: 12th European Conference, ECMFA 2016, Held as Part of STAF 2016, Vienna, Austria, July 6-7, 2016, Proceedings\n", "abstract": " This book constitutes the proceedings of the 12th European Conference on Modelling Foundations and Applications, ECMFA 2016, held as part of STAF 2016, in Vienna, Austria, in July 2016. The 16 papers presented in this volume were carefully reviewed and selected from 47 submissions. The committee decided to accept 16 papers, 12 papers for the Foundations Track and 4 papers for the Applications Track. Papers on a wide range of MBE aspects were accepted, including topics such as multi-and many models, language engineering, UML and meta-modeling, experience reports and case studies, and variability and uncertainty.", "num_citations": "1\n", "authors": ["576"]}
{"title": "40 Variability Bugs in the Linux Kernel\n", "abstract": " Feature-sensitive verification is a recent field that pursues the effective analysis of the exponential number of variants of a program family. Today researchers lack examples of concrete bugs induced by variability, and occurring in real large-scale software. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 40 variability bugs collected from bug-fixing commits to the Linux kernel repository. We investigate each of the 40 bugs, recording the outcome of our analysis into a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights about the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Danfoss EKC trial project deliverables\n", "abstract": " This report documents the results of the Danfoss EKC trial project on model based development using IAR visualState. We present a formal state-model of an refrigeration controller based on a specification given by Danfoss. We report results on modeling, verification, simulation, and code-generation. It is found that the IAR visualState is a promising tool for this application domain, but that improvements must be done to code-generation and automatic test generation.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Zdalna generacja sieci bayesowskich z baz danych\n", "abstract": " Celem projektu jest zaimplementowanie systemu ucz\u0105cego sieci Bayesa. Wykonany system umo\u017cliwia por\u00f3wnanie r\u00f3\u017cnych metod uczenia dla tych samych danych wej\u015bciowych. Wykorzystanie nowoczesnych technologii internetowych gwarantuje dost\u0119pno\u015b\u0107 oprogramowania dla szerokiej grupy u\u017cytkownik\u00f3w korzystaj\u0105cych z r\u00f3\u017cnych platform sprz\u0119towych.Automatyczne i p\u00f3\u0142automatyczne wnioskowanie z wykorzystaniem baz wiedzy jest dzi\u015b jednym z najistotniejszych zastosowa\u0144 sztucznej inteligencji. Jego znaczenie ro\u015bnie wraz z rozwojem kr\u0119gu u\u017cytkownik\u00f3w system\u00f3w eksperckich, czyli r\u00f3\u017cnego rodzaju narz\u0119dzi wspomagaj\u0105cych podejmowanie decyzji. Analiza danych mikro i makroekonomicznych, testowanie wiarygodno\u015bci kredytowej, diagnostyka chor\u00f3b, czy para-inteligentny system pomocy z usuwaniem problem\u00f3w przy drukowaniu w systemie Microsoft Windows [8] to tylko niekt\u00f3re z licznych zastosowa\u0144 [14, 9, 7] takich system\u00f3w, niejednokrotnie opartych o zasady analizy bayesowskiej. Sieci przyczynowo-skutkowe, zwane te\u017c sieciami Bayesa, nale\u017c\u0105 do cz\u0119sto stosowanych sposob\u00f3w reprezentacji w bazach wiedzy system\u00f3w eksperckich. Sie\u0107 taka reprezentuje uk\u0142ad zale\u017cno\u015bci mi\u0119dzy r\u00f3\u017cnymi parametrami w postaci grafu skierowanego. W niniejszej pracy rozwa\u017ca si\u0119 problem zdalnego generowania bazy wiedzy o takiej strukturze.", "num_citations": "1\n", "authors": ["576"]}
{"title": "Why CART works for variability-aware performance prediction? an empirical study on performance distributions\n", "abstract": " This report presents follow-up work for our previous technical report \u201cVariability-Aware Performance Modeling: A Statistical Learning Approach\"(GSDLAB-TR-2012-08-18). We try to give evidence why our approach, based on a statisticallearning technique called Classification And Regression Trees (CART), works for variability-aware performance prediction. To this end, we conduct a comparative analysis of performance distributions on the evaluated case studies and empirically explore why our approach works with small random samples.", "num_citations": "1\n", "authors": ["576"]}