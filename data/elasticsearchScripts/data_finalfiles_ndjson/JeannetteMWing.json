{"title": "Computational thinking\n", "abstract": " It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.", "num_citations": "8633\n", "authors": ["1577"]}
{"title": "Linearizability: A correctness condition for concurrent objects\n", "abstract": " A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.", "num_citations": "3756\n", "authors": ["1577"]}
{"title": "Computational thinking and thinking about computing\n", "abstract": " Computational thinking will influence everyone in every field of endeavour. This vision poses a new educational challenge for our society, especially for our children. In thinking about computing, we need to be attuned to the three drivers of our field: science, technology and society. Accelerating technological advances and monumental societal demands force us to revisit the most basic scientific questions of computing.", "num_citations": "2009\n", "authors": ["1577"]}
{"title": "A behavioral notion of subtyping\n", "abstract": " The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub- and supertypes; the paper presents a way of specifying types that makes it convenient to define  the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.", "num_citations": "1656\n", "authors": ["1577"]}
{"title": "A specifier's introduction to formal methods\n", "abstract": " Formal methods used in developing computer systems (i.e. mathematically based techniques for describing system properties) are defined, and their role is delineated. Formal specification languages, which provide the formal method's mathematical basis, are examined. Certain pragmatic concerns about formal methods and their users, uses, and characteristics are discussed. Six well-known or commonly used formal methods are illustrated by simple examples. They are Z, VDM, Larch, temporal logic, CSP, and transition axioms.< >", "num_citations": "1210\n", "authors": ["1577"]}
{"title": "Specification matching of software components\n", "abstract": " Specification matching is a way to compare two software components, based on descriptions of the component's behaviors. In the context of software reuse and library retrieval, it can help determine whether one component can be substituted for another or how one can be modified to fit the requirements of the other. In the context of object-oriented programming, it can help determine when one type is a behavioral subtype of another. We use formal specifications to describe the behavior of software components and, hence, to determine whether two components match. We give precise definitions of not just exact match, but, more relevantly, various flavors of relaxed match. These definitions capture the notions of generalization, specialization, and substitutability of software  components. Since our formal specifications are pre- and postconditions written as predicates in first-order logic, we rely on theorem proving to\u00a0\u2026", "num_citations": "819\n", "authors": ["1577"]}
{"title": "An attack surface metric\n", "abstract": " Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method\u00a0\u2026", "num_citations": "757\n", "authors": ["1577"]}
{"title": "Game strategies in network security\n", "abstract": " This paper presents a game-theoretic method for analyzing the security of computer networks. We view the interactions between an attacker and the administrator as a two-player stochastic game and construct a model for the game. Using a nonlinear program, we compute Nash equilibria or best-response strategies for the players (attacker and administrator). We then explain why the strategies are realistic and how administrators can use these results to enhance the security of their network.", "num_citations": "606\n", "authors": ["1577"]}
{"title": "Signature matching: a tool for using software libraries\n", "abstract": " Signature matching is a method for organizing, navigating through, and retrieving from software libraries. We consider two kinds of software library components\u2014functions and modules\u2014and hence two kinds of matching\u2014function matching and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match but also various flavors of relaxed match. We describe various applications of signature matching as a tool for using software libraries, inspired by the use of our implementation of a function signature matcher written in Standard ML.", "num_citations": "494\n", "authors": ["1577"]}
{"title": "Research notebook: Computational thinking\u2014What and why\n", "abstract": " In a March 2006 article for the Communications of the ACM, I used the term\" computational thinking\" to articulate a vision that everyone, not just those who major in computer science, can benefit from thinking like a computer scientist [Wing06]. So, what is computational thinking? Here's a definition that Jan Cuny of the National Science Foundation, Larry Snyder of the University of Washington, and I use; it was inspired by an email exchange I had with Al Aho of Columbia University:", "num_citations": "461\n", "authors": ["1577"]}
{"title": "The Larch Family of Specification Languages.\n", "abstract": " The use of suitable formalisms in the specification of computer programs and parts of computer programs offers significant advantages. 1 Although there is considerable theoretical interest in this area, practical experience is rather limited. The Larch Project, a research project intended to have practical applications in the next few years, is developing tools and techniques to aid in the productive application of formal specifications. A major part of the project is a family of specification languages. Each specification has components written in two languages. The Larch interface languages are particular to specific programming languages, while the Larch Shared Language is common to all languages.", "num_citations": "412\n", "authors": ["1577"]}
{"title": "Tools for generating and analyzing attack graphs\n", "abstract": " Attack graphs depict ways in which an adversary exploits system vulnerabilities to achieve a desired state. System administrators use attack graphs to determine how vulnerable their systems are and to determine what security measures to deploy to defend their systems. In this paper, we present details of an example to illustrate how we specify and analyze network attack models. We take these models as input to our attack graph tools to generate attack graphs automatically and to analyze system vulnerabilities. While we have published our generation and analysis algorithms in earlier work, the presentation of our example and toolkit is novel to this paper.", "num_citations": "315\n", "authors": ["1577"]}
{"title": "Measuring relative attack surfaces\n", "abstract": " We propose a metric for determining whether one version of a system is more secure than another with respcct to a fixed set of dimensions. Rather than count bugs at the code level or count vulnerability reports at the system level, we count a system's attack opportunities. We use this count as an indication of the system's \u201cattackability,\u201d likelihood that it will be successfully attacked. We describe a system's attack surface along three abstract dimensions: targets and enablers, channels and protocols, and access rights. Intuitively, the more exposed the system's surface, the more attack opportunities, and hence the more likely it will be a target of attack. Thus, one way to improve system security is to reduce its attack surface.             To validate our ideas, we recast Microsoft Security Bulletin MS02-005 using our terminology, and we show how Howard's Relative Attack Surface Quotient for Windows is an instance of\u00a0\u2026", "num_citations": "297\n", "authors": ["1577"]}
{"title": "Specification matching of software components\n", "abstract": " Specification matching is a way to compare two software components. In the cent ext of software reuse and library retrieval, it can help determine whether one component can be substituted for another or how one can be modified to fit the requirements of the other. In the context of object-oriented programming, it can help determine when one type is a behavioral subtype of another. In the context of system interoperability, it can help determine whether the interfaces of two components mismatch. We use formal specifications to describe the behavior of software components, and hence, to determine whether two components mat ch. We give precise definitions of not just exact match, but more relevantly, various flavors of relaxed match. These definitions capture the notions of generalization, specialization, substitutability, subtyping, and interoperability of software components.", "num_citations": "276\n", "authors": ["1577"]}
{"title": "Computational thinking benefits society\n", "abstract": " Computer science has produced, at an astonishing and breathtaking pace, amazing technology that has transformed our lives with profound economic and societal impact. Computer science\u2019s effect on society was foreseen forty years ago by Gotlieb and Borodin in their book Social Issues in Computing. Moreover, in the past few years, we have come to realize that computer science offers not just useful software and hardware artifacts, but also an intellectual framework for thinking, what I call \u201ccomputational thinking\u201d[Wing06]. Everyone can benefit from thinking computationally. My grand vision is that computational thinking will be a fundamental skill\u2014just like reading, writing, and arithmetic\u2014used by everyone by the middle of the 21st Century. This article describes how pervasive computational thinking has become in research and education. Researchers and professionals in an increasing number of fields beyond\u00a0\u2026", "num_citations": "237\n", "authors": ["1577"]}
{"title": "Axioms for concurrent objects\n", "abstract": " Specification and verification techniques for abstract data types that have been successful for sequential programs can be extended in a natural way to provide the same benefits for concurrent programs. We propose an approach to specifying and verifying concurrent objects based on a novel correctness condition, which we call \u201clinearizability.\u201d Linearizability provides the illusion that each operation takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can still be given by pre-and post-conditions. In this paper, we will define and discuss linearizability, and then give examples of how to reason about concurrent objects and verify their implementations based on their (sequential) axiomatic specifications.", "num_citations": "236\n", "authors": ["1577"]}
{"title": "Lightweight formal methods\n", "abstract": " Formal methods have offered great benefits, but often at a heavy price. For everyday software development, in which the pressures of the market don\u2019t allow full-scale formal methods to be applied, a more lightweight approach is called for. I\u2019ll outline an approach that is designed to provide immediate benefit at relatively low cost. Its elements are a small and succinct modelling language, and a fully automatic analysis scheme that can perform simulations and find errors. I\u2019ll describe some recent case studies using this approach, involving naming schemes, architectural styles, and protocols for networks with changing topologies. I\u2019ll make some controversial claims about this approach and its relationship to UML and traditional formal specification approaches, and I\u2019ll barbeque some sacred cows, such as the belief that executability compromises abstraction.", "num_citations": "226\n", "authors": ["1577"]}
{"title": "Writing Larch interface language specifications\n", "abstract": " Current research in specifications is emphasizing the practical use of formal specifications in program design. One way to encourage their use in practice is to provide specification languages that are accessible to both designers and programmers. With this goal in mind, the Larch family of formal specification languages has evolved to support a two-tiered approach to writing specifications. This approach separates the specification of state transformations and programming language dependencies from the specification of underlying abstractions. Thus, each member of the Larch family has a subset derived from a programming language and another subset independent of any programming languages. We call the former interface languages, and the latter the Larch Shared Language. This paper focuses on Larch interface language specifications. Through examples, we illustrate some salient features of Larch/CLU\u00a0\u2026", "num_citations": "191\n", "authors": ["1577"]}
{"title": "Inheritance of synchronization and recovery properties in Avalon/C++\n", "abstract": " The authors describe their experience adapting inheritance mechanisms to a new application domain, reliable distributed systems. They give an overview of Avalon/C++, a programming language under development that allows programmers to 'customize' the synchronization and fault-tolerance properties of data types by letting them inherit properties such as serializability and crash recovery from a library of basic types. The authors first describe the transaction model used to organize distributed computations and some relevant features of C++, and give an overview of the Avalon/C++ base hierarchy. They then describe in more detail each of the hierarchy's classes and some restrictions on their use that must be obeyed to preserve their semantic intent. An extended example illustrates a directory-type implementation that uses all three of the base classes. Related work is discussed.< >", "num_citations": "184\n", "authors": ["1577"]}
{"title": "Signature matching: A key to reuse\n", "abstract": " Software reuse is only effective if it is easier to locate (and appropriately modify) a reusable component than to write it from scratch. We present signature matching as a method for achieving this goal by using signature information easily derived from the component. We consider two kinds of software components, functions and modules, and hence two kinds of matching, function mathcing and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match, but also various flavors of relaxed match. We briefly describe an experimental facility written in Standard ML for performing signature matching over a library of ML functions.", "num_citations": "171\n", "authors": ["1577"]}
{"title": "A new definition of the subtype relation\n", "abstract": " The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper presents a new definition of the subtype relation that ensures that any property proved about supertype objects also holds for subtype objects. It also discusses the ramifications of the definition on the design of type families.", "num_citations": "169\n", "authors": ["1577"]}
{"title": "Verifiable secret redistribution for archive systems\n", "abstract": " We present a new verifiable secret redistribution protocol for threshold sharing schemes that forms a key component of a proposed archival storage system. Our protocol supports redistribution from (m,n) to (m',n') threshold sharing schemes without requiring reconstruction of the original data. The design is motivated by archive systems for which the added security of threshold sharing of data must be accompanied by the flexibility of dynamic shareholder changes. Our protocol enables the dynamic addition or removal of shareholders, and also guards against mobile adversaries. We observe that existing protocols either cannot be extended readily to allow redistribution between different access structures, or have vulnerabilities that allow faulty old shareholders to distribute invalid shares to new shareholders. Our primary contribution is that in our protocol, new shareholders can verify the validity of their shares after\u00a0\u2026", "num_citations": "143\n", "authors": ["1577"]}
{"title": "Computational thinking\u2019s influence on research and education for all\n", "abstract": " Computer science has produced, at an astonishing and breathtaking pace, amazing technology that has transformed our lives with profound economic and societal impact. In the course of the past ten years, we have come to realize that computer science offers not just useful software and hardware artifacts, but also an intellectual framework for thinking, what I call \u201ccomputational thinking\\u201d. Everyone can benefit from thinking computationally. My grand vision is that computational thinking will be a fundamental skill\\u2014just like reading, writing, and arithmetic\\u2014used by everyone by the middle of the 21st Century.", "num_citations": "139\n", "authors": ["1577"]}
{"title": "Specifications as Search Keys for Software Libraries.\n", "abstract": " 3 expression (fn int int) represents the ML type int! int. type list ml-type-> ml-type. type array ml-type-> ml-type. type fn ml-type-> ml-type-> ml-type.% function type prod ml-type-> ml-type-> ml-type.% cartesian product In order to build a signature library, we use the the constant hasType to form propositions that relate", "num_citations": "134\n", "authors": ["1577"]}
{"title": "Specifications and their use in defining subtypes\n", "abstract": " Specifications are useful because they allow reasoning about objects without concern for their implementations. Type hierarchies are useful because they allow types that share common properties to be designed as a family. This paper is concerned with the interaction between specifications and type hierarchies. We present a way of specifying types, and show how some extra information, in addition to specifications of the objects\u2019 methods, is needed to support reasoning. We also provide a new way of showing that one type is a subtype of another. Our technique makes use of information in the types\u2019 specifications and works even in a very general computational environment in which possibly concurrent users share mutable objects.", "num_citations": "132\n", "authors": ["1577"]}
{"title": "Model checking electronic commerce protocols\n", "abstract": " The paper develops model checking techniques to examine NetBill and Digicash. We show how model checking can verify atomicity properties by analyzing simpli ed versions of these protocols that retain crucial security constraints. For our analysis we used the FDR model checker.", "num_citations": "130\n", "authors": ["1577"]}
{"title": "Computational thinking\n", "abstract": " Computational Thinking [1] is a universal metaphor of reasoning used by both mankind and machines. From this perspective it has the potential to be a comprehensive umbrella for capturing the intrinsic nature of computing and conveying this in an understandable way to students and the general public. It represents a broad spectrum of reasoning across time and disciplines. Learning to count is a beginning of human computational thinking, followed naturally by arithmetic computation and abstract levels of symbol based thinking, often starting with algebra. Counting, arithmetic, symbols and abstract thinking are fundamental to the study of computing.Computational reasoning is the core of all modern Science, Technology, Engineering and Mathematics (STEM) disciplines and is intrinsic to all other disciplines from A to Z. It is used in our everyday lives from baking a cake, changing a tire or brushing our teeth. The\u00a0\u2026", "num_citations": "128\n", "authors": ["1577"]}
{"title": "Measuring the attack surfaces of two FTP daemons\n", "abstract": " Software consumers often need to choose between different software that provide the same functionality. Today, security is a quality that many consumers, especially system administrators, care about and will use in choosing one soft-ware system over another. An attack surface metric is a security metric for comparing the relative security of similar software systems [7]. The measure of a system's attack surface is an indicator of the system's security: given two systems, we compare their attack surface measurements to decide whether one is more secure than another along each of the following three dimensions: methods, channels, and data. In this paper, we use the attack surface metric to measure the attack surfaces of two open source FTP daemons: ProFTPD 1.2. 10 and Wu-FTPD 2.6. 2. Our measurements show that ProFTPD is more secure along the method dimension, ProFTPD is as secure as Wu-FTPD along\u00a0\u2026", "num_citations": "114\n", "authors": ["1577"]}
{"title": "A Two-Tiered Approach to Specifying Programs.\n", "abstract": " Current research in specifications is beginning to emphasize the practical use of formal specifications in program design. This thesis presents a specification approach, a specification language that supports that approach, some ways to evaluate specifications written in that language. The two-tiered approach separates the specification of underlying abstractions from the specification of state transformations. In this approach, state transformations and target programming language dependencies are isolated into an interface language component. All interface specifications are built upon shared language specifications that describe the underlying abstractions. This thesis presents an interface specification language for the CLU programming language and presumes the use of the Larch shared language. This thesis also suggests a number of kinds of analyses that one might want to perform on two-tiered specifications. These are related to the consistency, completeness, and strength of specifications, and are all presented in terms of the theories associated with specifications.Descriptors:", "num_citations": "110\n", "authors": ["1577"]}
{"title": "A study of 12 specifications of the library problem\n", "abstract": " The author studies twelve specifications for a seemingly simple database problem and demonstrates many approaches for classifying informally stated problem requirements. She compares the specifications according to how they address problems of the library example to illustrate the imprecision of natural-language specifications and how twelve different approaches to the same set of informal requirements reveal many of the same problems. The comparison suggests which issues should be addressed in refining an informal set of requirements and shows how these issues are resolved in different specification approaches.< >", "num_citations": "94\n", "authors": ["1577"]}
{"title": "An approach to measuring a system's attack surface\n", "abstract": " Practical software security measurements and metrics are critical to the improvement of software security. We propose a metric to determine whether one software system is more secure than another similar system with respect to their attack surface. We use a systems attack surface measurement as an indicator of the systems security the larger the attack surface, the more insecure the system. We measure a systems attack surface in terms of three kinds of resources used in attacks on the system methods, channels, and data. We demonstrate the use of our attack surface metric by measuring the attack surfaces of two open source IMAP servers and two FTP daemons. We validated the attack surface metric by conducting an expert user survey and by performing statistical analysis of Microsoft Security Bulletins. Our metric can be used as a tool by software developers in the software development process and by software consumers in their decision making process.Descriptors:", "num_citations": "92\n", "authors": ["1577"]}
{"title": "A formal model for a system\u2019s attack surface\n", "abstract": " Practical software security metrics and measurements are essential for secure software development. In this chapter, we introduce the measure of a software system\u2019s attack surface as an indicator of the system\u2019s security. The larger the attack surface, the more insecure the system. We formalize the notion of a system\u2019s attack surface using an I/O automata model of the system and introduce an attack surface metric to measure the attack surface in a systematic manner. Our metric is agnostic to a software system\u2019s implementation language and is applicable to systems of all sizes. Software developers can use the metric in multiple phases of the software development process to improve software security. Similarly, software consumers can use the metric in their decision making process to compare alternative software.", "num_citations": "91\n", "authors": ["1577"]}
{"title": "Measuring a system's attack surface\n", "abstract": " We propose a metric to determine whether one version of a system is relatively more secure than another with respect to the systems attack surface. Intuitively, the more exposed the attack surface, the more likely the system could be successfully attacked, and hence the more insecure it is. We define an attack surface in terms of the systems actions that are externally visible to its users and the systems resources that each action accesses or modifies. To apply our metric in practice, rather than consider all possible system resources, we narrow our focus on a relevant subset of resource types, which we call attack classes these reflect the types of system resources that are more likely to be targets of attack. We assign payoffs to attack classes to represent likelihoods of attack resources in an attack class with a high payoff value are more likely to be targets or enablers of an attack than resources in an attack class with a low payoff value. We outline a method to identify attack classes and to measure a systems attack surface. We demonstrate and validate our method by measuring the relative attack surface of four different versions of the Linux operating system.Descriptors:", "num_citations": "91\n", "authors": ["1577"]}
{"title": "Fast, automatic checking of security protocols\n", "abstract": " Protocols in electronic commerce and other security-sensitive applications require careful reasoning to demonstrate their robustness against attacks. Several logics have been developed for doing this reasoning formally, but protocol designers usually do the proofs by hand, a process which is time-consuming and error-prone.", "num_citations": "91\n", "authors": ["1577"]}
{"title": "A symbiotic relationship between formal methods and security\n", "abstract": " Security played a significant role in the development of formal methods in the 70s and early 80s. Have the tables turned? Are formal methods now ready to play a significant role in the development of more secure systems? While not a panacea, the answer is yes, formal methods can and should play such a role. In this paper we first review the limits of formal methods. Then after a brief historical excursion, we summarize some recent results on how model checking and theorem proving tools revealed new and known flaws in authentication protocols. Looking to the future we discuss the challenges and opportunities for formal methods in analyzing the security of systems, above and beyond the protocol level.", "num_citations": "90\n", "authors": ["1577"]}
{"title": "Five deep questions in computing\n", "abstract": " Even if they seem unanswerable, just trying to answer them will advance the field's scientific foundations and help engineer the systems we can only imagine.", "num_citations": "85\n", "authors": ["1577"]}
{"title": "Behavioral subtyping using invariants and constraints\n", "abstract": " We present a way of defining the subtype relation that ensures that subtype objects preserve behavioral properties of their supertypes. The subtype relation is based on the specifications of the sub and supertypes. Our approach handles mutable types and allows subtypes to have more methods than their supertypes. Dealing with mutable types and subtypes that extend their supertypes has surprising consequences on how to specify and reason about objects. In our approach, we discard the standard data type induction rule, we prohibit the use of an analogous history rule, and we make up for both losses by adding explicit predicates invariants and constraints to our type specifications. We also discuss the ramifications of our approach of subtyping the design of type families.Descriptors:", "num_citations": "85\n", "authors": ["1577"]}
{"title": "Composing first-class transactions\n", "abstract": " \\Ve describe the design of a transaction facilit y for a language that supports higher-order functions. tVe factor transactions into four separable features: persistence, undoability, locking, and threads. Then, relying on function composition, we show how we can put them together again. Our modular approach toward building transactions enables us to construct a model of concurrent, nested, multi threaded transactions, as well as other nontradi tional models where not all features of traditional transactions are present. Key to our approach is the use of higher-order functions to make transactions first-class. Not only do we get clean composability of transactional features, but also we avoid the need to introduce special control and block-structured constructs as done in more traditional transactional systems. We implemented our design in Standard ML of New Jersey.", "num_citations": "81\n", "authors": ["1577"]}
{"title": "Model checking software systems: A case study\n", "abstract": " Model checking is a proven successful technology for verifying hardware. It works, however, on only finite state machines, and most software systems have infinitely many states. Our approach to applying model checking to software hinges on identifying appropriate abstractions that exploit the nature of both the system, S, and the property, 4, to be verified. We check@ on an abstracted, but finite, model of S.Following this approach we verified three cache coherence protocols used in distributed file systems. These protocols have to satisfy this property:\u201cIf a client believes that a cached file is valid then the authorized server believes that the client\u2019s copy is valid.\u201d In our finite model of the system, we need only represent the \u201cbeliefs\u201d that a client and a server have about a cached file; we can abstract from the caches, the files\u2019 contents, and even the files themselves. Moreover, by successive application of the\u00a0\u2026", "num_citations": "75\n", "authors": ["1577"]}
{"title": "Avalon: Language support for reliable distributed systems\n", "abstract": " Avalon is a set of linguistic constructs designed to give programmers explicit control over transactionbased processing of atomic objects for fault-tolerant applications. These constructs are to be implemented as extensions to familiar programming languages such as C++, Common Lisp, and Ada; they are tailored for each base language so the syntax and spirit of each language are maintained.This paper presents an overview of the novel aspects of Avalon/C++:(1) support for testing transaction serialization orders at run-time, and (2) user-defined, but system-invoked, transaction commit and abort operations for atomic data objects. These capabilities provide programmers with the flexibility to exploit the semantics of applications to enhance efficiency, concurrency, and fault-tolerance.", "num_citations": "75\n", "authors": ["1577"]}
{"title": "Specifying graceful degradation\n", "abstract": " Complex programs are often required to display graceful degradation, reacting adaptively to changes in the environment. Under\u25a0 deal circumstances, the program's behavior satisfies a set of applicationdependent constraints. In the presence of events such as failures, timing anomalies, synchronization conflicts, or security breaches, certain constraints may become difficult or impossible to satisfy, and the application designer may choose to relax them as long as the resulting behavior is sufficiently\" close5'to the preferred behavior. This paper describes the relaxation lattice method, a new approach to specifying graceful degradation for a large class of programs. \u041b relaxation lattice is a lattice of specifications parameterized by a set of constraints, where the stronger the set of constraints, the more restrictive the specification. While a program is able to satisfy its strongest set of constraints, it satisfies its preferred\u00a0\u2026", "num_citations": "73\n", "authors": ["1577"]}
{"title": "Mir\u00f3: Visual specification of security\n", "abstract": " Miro is a set of languages and tools that support the visual specification of file system security. Two visual languages are presented: the instance language, which allows specification of file system access, and the constraint language, which allows specification of security policies. Miro visual languages and tools are used to specify security configurations. A visual language is one whose entities are graphical, such as boxes and arrows, specifying means stating independently of any implementation the desired properties of a system. Security means file system protection: ensuring that files are protected from unauthorized access and granting privileges to some users, but not others. Tools implemented and examples of how these languages can be applied to real security specification problems are described.< >", "num_citations": "73\n", "authors": ["1577"]}
{"title": "Towards a theory of trust in networks of humans and computers\n", "abstract": " We argue that a general theory of trust in networks of humans and computers must be build on both a theory of behavioral               trust and a theory of computational               trust. This argument is motivated by increased participation of people in social networking, crowdsourcing, human computation, and socio-economic protocols, e.g., protocols modeled by trust and gift-exchange games [3,10,11], norms-establishing contracts [1], and scams [6,35,33]. User participation in these protocols relies primarily on trust, since on-line verification of protocol compliance is often impractical; e.g., verification can lead to undecidable problems, co-NP complete test procedures, and user inconvenience. Trust is captured by participant preferences (i.e., risk and betrayal aversion) and beliefs in the trustworthiness of other protocol participants [11,10]. Both preferences and beliefs can be enhanced whenever protocol\u00a0\u2026", "num_citations": "71\n", "authors": ["1577"]}
{"title": "A case study in model checking software systems\n", "abstract": " Model checking is a proven successful technology for verifying hardware. It works, however, on only finite state machines, and most software systems have infinitely many states. Our approach to applying model checking to software hinges on identifying appropriate abstractions that exploit the nature of both the system, S, and the property, \u03b8, to be verified. We check \u03b8 on an abstracted, but finite, model of S.Following this approach we verified three cache coherence protocols used in distributed file systems. These protocols have to satisfy this property: \u201cIf a client believes that a cached file is valid then the authorized server believes that the client's copy is valid.\u201d In our finite model of the system, we need only represent the \u201cbeliefs\u201d that a client and a server have about a cached file; we can abstract from the caches, the files' contents, and even the files themselves. Moreover, by successive application of the\u00a0\u2026", "num_citations": "69\n", "authors": ["1577"]}
{"title": "Testing and verifying concurrent objects\n", "abstract": " A concurrent object is a data structure shared by concurrent processes. We present a two-pronged approach for establishing the correctness of implementations of concurrent objects. Our approach is based on a notion of correctness called linearizability: each concurrent object has a sequential specification, which describes how it behaves in sequential executions. Each concurrent execution is required to be equivalent to some sequential execution. We advocate using both testing and verification as complementary approaches for showing a concurrent object is linearizable. We first describe a simulation environment for simulating, testing, and analyzing implementations of concurrent objects. We can use the simulator to gain assurance that an implementation is correct or to detect errors in an incorrect one. The simulator provides a systematic way to testing implementations of any data type. Whereas testing is\u00a0\u2026", "num_citations": "67\n", "authors": ["1577"]}
{"title": "Programming at the processor-memory-switch level\n", "abstract": " Users of networks of heterogeneous processors are concerned with allocating specialized resources to tasks of medium to large size. They need to create processes, which are instances of tasks, allocate these processes to processors, and specify the communication patterns between processes. These activities constitute processor-memory-switch (PMS)-level programming. The authors describe the use of PMS-level programming in computation-intensive, real-time applications, eg vision, robotics, and vehicular control, that require efficient concurrent execution of multiple tasks, eg sensor data collection, obstacle recognition, and global path planning, devoted to specific pieces of the application. They discuss the programming of heterogeneous machines and present the Durra language and tools, which they are developing to support PMS-level programming.<>", "num_citations": "63\n", "authors": ["1577"]}
{"title": "Some notes on putting formal specifications to productive use\n", "abstract": " These notes are personal reflections, stemming from attempts to understand the sources of problems and successes in the application of work on formal specifications. Our intent is to provoke thought about the nature and value of work in the area; not to provide a set of well-tested results. Rather than focusing on yet another specification language, we have tried to take a broad view of the role of formal specifications in the program development process.", "num_citations": "60\n", "authors": ["1577"]}
{"title": "Formalizing and enforcing purpose restrictions in privacy policies\n", "abstract": " Privacy policies often place restrictions on the purposes for which a governed entity may use personal information. For example, regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), require that hospital employees use medical information for only certain purposes, such as treatment, but not for others, such as gossip. Thus, using formal or automated methods for enforcing privacy policies requires a semantics of purpose restrictions to determine whether an action is for a purpose or not. We provide such a semantics using a formalism based on planning. We model planning using a modified version of Markov Decision Processes (MDPs), which exclude redundant actions for a formal definition of redundant. We argue that an action is for a purpose if and only if the action is part of a plan for optimizing the satisfaction of that purpose under the MDP model. We use this formalization to\u00a0\u2026", "num_citations": "55\n", "authors": ["1577"]}
{"title": "Verifiable secret redistribution for threshold sharing schemes\n", "abstract": " We present a new protocol for the verifiable redistribution of secrets from m, n to m, n access structures for threshold sharing schemes. Our protocol enables the addition or removal of shareholders and also guards against mobile adversaries that cause permanent damage. We observe that existing protocols either cannot be readily extended to allow redistribution between different access structures, or have vulnerabilities that allow faulty old shareholders to corrupt the shares of new shareholders. Our primary contribution is that, in our protocol, new shareholders can verify the validity of their shares after redistribution between different access structures.Descriptors:", "num_citations": "55\n", "authors": ["1577"]}
{"title": "Formal methods for privacy\n", "abstract": " Privacy means something different to everyone. Against a vast and rich canvas of diverse types of privacy rights and violations, we argue technology\u2019s dual role in privacy: new technologies raise new threats to privacy rights and new technologies can help preserve privacy. Formal methods, as just one class of technology, can be applied to privacy, but privacy raises new challenges, and thus new research opportunities, for the formal methods community.", "num_citations": "52\n", "authors": ["1577"]}
{"title": "Progress in computational thinking, and expanding the HPC community\n", "abstract": " The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmJeannette Wing considers the proliferation of computational thinking, while Dan Stanzione hopes to bring more HPC practitioners to SC16.", "num_citations": "51\n", "authors": ["1577"]}
{"title": "Persistence+ undoability= transactions\n", "abstract": " Persistence means objects live potentially forever. Undoability means that any change to a program's store can potentially be undone. In our design and implementation of support for single-threaded nested transactions in Standard ML of New Jersey (SML/NJ), we provide persistence and undoability as orthogonal features and combine them in a simple and elegant manner.We provide support for persistence through an SML interface that lets users manipulate a set of persistent roots and provides a save function that causes all data reachable from the persistent roots to be moved into the persistent heap. We implement the interface through simple extensions to SML's generational garbage collector and maintain the persistent heap using CMU's Recoverable Virtual Memory system.", "num_citations": "47\n", "authors": ["1577"]}
{"title": "Invited talk: Weaving formal methods into the undergraduate computer science curriculum\n", "abstract": " We can integrate formal methods into an existing undergraduate curriculum by focusing on teaching their common conceptual elements and by using state of the art formal methods tools. Common elements include state machines, invariants, abstraction mappings, composition, induction, specification, and verification. Tools include model checkers and specification checkers. By introducing and regularly revisiting the concepts throughout the entire curriculum and by using the tools for homework assignments and class projects, we may be able to attain the ideal goal of having computer scientists use formal methods without their even realizing it.", "num_citations": "43\n", "authors": ["1577"]}
{"title": "Family values: A behavioral notion of subtyping\n", "abstract": " The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub-and supertypes, the paper presents a way of specifying types that makes it convenient to define the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.Descriptors:", "num_citations": "43\n", "authors": ["1577"]}
{"title": "Concurrent, atomic garbage collection\n", "abstract": " We describe a concurrent, atomic garbage collection algorithm for transaction-based languages, a class of languages intended to support reliable distributed systems. A garbage collection algorithm for reliable distributed systems must be atomic: a crash during a garbage collection should result in no loss of data. A concurrent collector allows programs to continue operating as their heaps are collected, minimizing delays imposed by collection. A concurrent collector can be used in reliable distributed systems that must service interactive requests. We introduce concurrency into an atomic collector by breaking collection into a series of garbage collection segments, which are similar to transactions, but satisfy weaker properties than are required by transaction semantics. These weaker properties allow better performance. Thus, our collection algorithm enhances the reliability of programs written in transaction-based languages, without necessarily imposing severe performance penalties.We also describe a concurrent \u2018\u2018mostly-copying\u2019\u2019collection algorithm for C++, and present measurements of the performance of an implementation of this algorithm. We survey garbage collection techniques applicable to \u2018\u2018systems\u2019\u2019languages such as C++, and show how these techniques may be combined with concurrent collection. Performance measurements of the resulting collector (on both a uniprocessor and a multiprocessor) indicate that interruptions caused by concurrent collection are quite short, and that adequate mutator performance is sustained during collection.", "num_citations": "42\n", "authors": ["1577"]}
{"title": "A methodology for information flow experiments\n", "abstract": " Information flow analysis has largely focused on methods that require access to the program in question or total control over an analyzed system. We consider the case where the analyst has neither control over nor a white-box model of the analyzed system. We formalize such limited information flow analyses and study an instance of it: detecting the usage of data by websites. We reduce these problems to ones of causal inference by proving a connection between non-interference and causation. Leveraging this connection, we provide a systematic black-box methodology based on experimental science and statistical analysis. Our systematic study leads to practical advice for detecting web data usage, a previously normalized area. We illustrate these concepts with a series of experiments collecting data on the use of information by websites.", "num_citations": "41\n", "authors": ["1577"]}
{"title": "Specifying functional and timing behavior for real-time applications\n", "abstract": " We present a notation and a methodology for specifying the functional and timing behavior of real-time applications for a heterogeneous machine. In our methodology we build upon well-defined, though isolated, pieces of previous work: Larch and Real Time Logic. In our notation, we strive to keep separate the functional specification from the timing specification so that a task's functionality can be understood independently of its timing behavior. We show that while there is a clean separation of concerns between these two specifications, the semantics of both pieces as well as their combination are simple.", "num_citations": "39\n", "authors": ["1577"]}
{"title": "The data life cycle\n", "abstract": " To put data science in context, we present phases of the data life cycle, from data generation to data interpretation. These phases transform raw bits into value for the end user. Data science is thus much more than data analysis, e.g., using techniques from machine learning and statistics; extracting this value takes a lot of work, before and after data analysis. Moreover, data privacy and data ethics need to be considered at each phase of the life cycle.Keywordsanalysis, collection, data life cycle, ethics, generation, interpretation, management, privacy, storage, story-telling, visualization", "num_citations": "38\n", "authors": ["1577"]}
{"title": "A call to action look beyond the horizon\n", "abstract": " Today's most prevalent and widely discussed attacks exploit code-level flaws such as buffer overruns and type-invalid input. Now we should turn to tomorrow's attacks, and think beyond buffer overruns, beyond code-level bugs, and beyond the horizon. This article is a call to arms to the research community to look toward the future. The author outlines a few suggestions for important research directions: software design, usability, and privacy. He argues that if we can make any progress on the first two, we could make a strong impact. He highlights the third topic because he thinks it deserves more attention from the scientific and technical communities, to complement the attention it already receives from the policy and legal communities. Because of the author's background in software engineering, he elaborates more on the first research direction than the other two, but believes all three deserve equal attention.", "num_citations": "35\n", "authors": ["1577"]}
{"title": "Thoughts on a Larch/ML and a new application for LP\n", "abstract": " We describe a preliminary design for a Larch interface language for the programming language ML.1 ML\u2019s support for higher-order functions suggests a need to go beyond the first-order logical foundations of Larch languages. We also propose a new application, specification matching, for the Larch Prover, which could benefit from extending LP to handle full first-order logic. This paper describes on-going work and suggests a number of open problems related to Larch/ML and to LP as used for specification matching. We assume rudimentary knowledge of Larch, its languages and two-tiered approach.", "num_citations": "35\n", "authors": ["1577"]}
{"title": "Unintrusive ways to integrate formal specifications in practice\n", "abstract": " Formal methods can be neatly woven in with less formal, but more widely-used, industrial-strength methods. We show how to integrate the Larch two-tiered specification method [GHW85a] with two used in the waterfall model of software development: Structured Analysis [Ros77] and Structure Charts [YC79]. We use Larch traits to define data elements in a data dictionary and the functionality of basic activities in Structured Analysis data-flow diagrams; Larch interfaces and traits to define the behavior of modules in Structure Charts. We also show how to integrate loosely formal specification in a prototyping model by discussing ways of refining Larch specifications as code evolves. To provide some realism to our ideas, we draw our examples from a non-trivial Larch specification of the graphical editor for the Mir\u00f3 visual languages [HMT+90].", "num_citations": "33\n", "authors": ["1577"]}
{"title": "Computational thinking\n", "abstract": " My vision for the 21st Century: Computational thinking will be a fundamental skill used by everyone in the world. Just as reading, writing, and arithmetic are fundamental skills every child learns, computational thinking is a skill needed for every citizen to function in today's global society. Computational thinking is an approach to solving problems, building systems, and understanding human behavior that draws on the power and limits of computing. Computational thinking is the use of abstraction to tackle complexity and the use of automation to tackle scale. The combination of the automation of abstraction underlies the enormous capability and reach of computing. In this talk I will argue that computational thinking has already begun to influence many disciplines, from the sciences to the humanities, but that the best is yet to come. Looking to the future, we can anticipate even more profound impact of computational\u00a0\u2026", "num_citations": "29\n", "authors": ["1577"]}
{"title": "Using Larch to specify Avalon/C++ objects\n", "abstract": " A formal specification of three base Avalon/C++ classes - recoverable, atomic, and subatomic - is given. Programmers derive from class recoverable to define persistent objects, and from either class atomic or class subatomic to define atomic objects. The specifications, written in Larch, provide the means for showing that classes derived from the base classes implement objects that are persistent or atomic and thus exemplify the applicability of an existing specification method to specifying nonfunctional properties. Writing these formal specifications for Avalon/C++'s built-in classes has helped to clarify places in the programming language where features interact, to make unstated assumptions explicit, and to characterize complex properties of objects.< >", "num_citations": "29\n", "authors": ["1577"]}
{"title": "Scenario graphs applied to network security\n", "abstract": " Traditional model checking produces one counterexample to illustrate a violation of a property by a model of the system. Some applications benefit from having all counterexamples, not just one. We call this set of counterexamples a scenario graph. In this chapter we present two different algorithms for producing scenario graphs and explain how scenario graphs are a natural representation for attack graphs used in the security community. Through a detailed concrete example, we show how we can model a computer network and generate and analyze attack graphs automatically. The attack graph we produce for a network model shows all ways in which an intruder can violate a given desired security property.", "num_citations": "27\n", "authors": ["1577"]}
{"title": "Reliable distributed computing with avalon/common lisp\n", "abstract": " We present an overview of the novel aspects of Avalon/Common Lisp:(1) support for remote evaluation through a new evaluator data type;(2) a generalization of the traditional client/server model of computation, allowing clients to extend server interfaces and server writers to hide aspects of distribution, such as caching, from clients;(3) support for automatic commit and abort processing of transactions and automatic crash recovery of atomic data. These capabilities provide programmers with the flexibility to exploit the semantics of an application to enhance its reliability and efficiency.Avalon/Common Lisp has been implemented and runs on IBM RT's on the Mach operating system. Though the design of Avalon/Common Lisp exploits some of the features of Common Lisp, eg, its packaging mechanism, all of the constructs are applicable to any Lisp-like language.", "num_citations": "27\n", "authors": ["1577"]}
{"title": "Inverse privacy\n", "abstract": " Seeking a market-based solution to the problem of a person's unjustified inaccessibility to their private information.", "num_citations": "25\n", "authors": ["1577"]}
{"title": "Cyber-physical systems\n", "abstract": " Autonomous cyber-physical systems are systems that are capable of making decisions and operating independently. However, at this point in time, cyber-physical system development is mostly in semi-autonomous systems.The term cyber-physical system was coined in 2006 by the United States National Science Foundation\u2019s then Program Manager Dr Helen Gill 1. However, these systems have a much longer history that dates back to the beginning of cybernetics, which was defined by mathematician Norbert Wiener as the science of control and communication in machines and humans 2.", "num_citations": "25\n", "authors": ["1577"]}
{"title": "A library of concurrent objects and their proofs of correctness\n", "abstract": " Answering the first question is one of definition; the second, of method. While there is no general agreement on an answer to the first, we choose the correctness condition called linearizability, which has recently captured the attention of the research community. Informally, we say an implementation of a concurrent object O is correct if and only if each concurrent history H accepted by O is \u201cequivalent\u201d in some sense to some legal sequential history, where (1) legality is defined in terms of the (sequential) type semantics of the object and (2) the \u201cequivalent\u201d sequential history preserves the real-time ordering of operations in H.Linearizability, first coined in Herlihy and Wing\u2019s 1987 POPL paper [3], generalizes correctness notions that had previously been defined for specific data structures like atomic registers and FIFO queues. It is an intuitively appealing notion of correctness, and also enjoys other properties like locality, which simplifies the proof method, that other notions of correctness do not.", "num_citations": "25\n", "authors": ["1577"]}
{"title": "Specifying graceful degradation in distributed systems\n", "abstract": " Distributed programs must often display graceful degradation, reacting adaptively to changes in the environment. Under ideal circumstances, the program\u2019s behavior satisfies a set of application-dependent constraints. In the presence of failures, timing anomalies, or synchronization conflicts, however, certain constraints may become difficult or impossible to Satisfy, and the application designer may choose to relax them as long as the resulting behavior is sufficiently \u201cclose\u201d to the preferred behavior. This paper describes the relaxation lattice method, a new approach to specifying graceful degradation for a large class of highly-concurrent fault-tolerant distributed programs. A relaxation lattice is a lattice of specifications parameterized by a set of constraints, where the stronger the set of constraints, the more restrictive the specification. While a program is able to satisfy its strongest set of constraints, it satisfies its\u00a0\u2026", "num_citations": "25\n", "authors": ["1577"]}
{"title": "Respectful type converters\n", "abstract": " In converting an object of one type to another, we expect some of the original object's behavior to remain the same and some to change. How can we state the relationship between the original object and converted object to characterize what information is preserved and what is lost after the conversion takes place? We answer this question by introducing the new relation, respects, and say that a type converter function C:A/spl rarr/B respects a type T. We formally define respects in terms of the Liskov and Wing behavioral notion of subtyping; types A and B are subtypes of T. We explain in detail the applicability of respectful type converters in the context of the Typed Object Model (TOM) Conversion Service, built at Carnegie Mellon and used on a daily basis throughout the world. We also briefly discuss how our respects relation addresses a similar question in two other contexts: type evolution and interoperability.", "num_citations": "24\n", "authors": ["1577"]}
{"title": "Visual specification of security constraints\n", "abstract": " We argue and demonstrate that the security domain naturally lends itself to pictorial representations of security constraints. Our formal model of security is based on an access matrix that traditionally has been used to indicate which users have access to which files, eg, in operating systems. Our formal visual notation borrows from and extends Harel's statechart ideas, which are based on graphs and Venn diagrams. We present a tour of our visual language's salient features and give examples from the security domain to illustrate the expressiveness of our notation.\"", "num_citations": "24\n", "authors": ["1577"]}
{"title": "A nitpick analysis of mobile IPv6\n", "abstract": " A lightweight formal method enables partial specification and automatic analysis by sacrificing breadth of coverage and expressive power. NP is a specification language that is a subset of Z, and Nitpick is a tool that quickly and automatically checks properties of finite models of systems specified in NP. We used NP to state two critical acyclicity properties of Mobile IPv6, a new internetworking protocol that allows mobile hosts to communicate with each other. In our Nitpick analysis of Mobile IPv6 we discovered a design flaw: one of the acyclicity properties does not hold. It takes only two hosts to exhibit this flaw. This paper gives self-contained overviews of Mobile IPv6 and of NP and Nitpick sufficient to understand the details of our specification and analysis.", "num_citations": "23\n", "authors": ["1577"]}
{"title": "A Language for Distributed Applications.\n", "abstract": " Durra is a language designed to support the development of distributed applications consisting of multiple, concurrent, large-grained tasks executing in a heterogeneous network.An application-level program is written in Durra as a set of task descriptions that prescribes a way to manage the resources of a heterogeneous machine network. The application describes the tasks to be instantiated and executed as concurrent processes, the intermediate queues required to store the messages as they move from producer to consumer processes, and the possible dynamic reconfigurations of the application.", "num_citations": "23\n", "authors": ["1577"]}
{"title": "Developing applications for heterogeneous machine networks: The Durra environment\n", "abstract": " In this paper we describe Durra, a language designed to support PMS-level programming, and its runtime environment.", "num_citations": "23\n", "authors": ["1577"]}
{"title": "Durra: A Task-Level Description Language Reference Manual. Version 2\n", "abstract": " Durra is a programming language designed to support the development of large-grained parallel programming applications. These applications are often computation-intensive, or have real-time requirements that require efficient concurrent execution of multiple tasks, devoted to specific pieces of the application. During execution time the application tasks run on possibly separate processors, and communicate with each other by sending messages of different types across various communication links. The application developer is responsible for prescribing a way to manage all of these resources. We call this prescription a task-level application description. It describes the tasks to be executed, the possible assignments of processes to processors, the data paths between the processors, and the intermediate queues required to store the data as they move from source to destination processes. Durra is a task-level description language, a notation in which to write these application descriptions. This document describes the syntax and semantics of the language and incorporates all the language changes introduced as a result of our experiences writing task and application descriptions in Durra.Descriptors:", "num_citations": "22\n", "authors": ["1577"]}
{"title": "Specifying and prototyping: Some thoughts on why they are successful\n", "abstract": " Two methods that have been successful in producing good software are 1) specifying and then implementing and 2) prototyping and then implementing. This paper identifies what the two methods have in common, namely that the implementation is the second time through carefully thinking about the problem. It proposes that perhaps this common aspect is more important to the successes of the methods than other aspects of the methods.", "num_citations": "21\n", "authors": ["1577"]}
{"title": "What is a Formal Method?\n", "abstract": " A formal method is a mathematically-based technique used in Computer Science to describe properties of hardware and/or software systems. It provides a framework within which large, complex systems may be specified, developed, and verified in a systematic rather than ad hoc manner. A method is formal if it has a sound mathematical basis, typically given by a formal specification language. A formal method is only a method, rather than an isolated mathematical entity in itself, because of a number of pragmatic considerations: who uses it, what it is used for, when it is used, and how it is used. This paper elaborates on what makes up a formal method and compares six different well-known formal methods, three used to specify abstract data types and three used to specify properties of concurrent and distributed systems.", "num_citations": "20\n", "authors": ["1577"]}
{"title": "Mir\u00f3 semantics for security\n", "abstract": " The Mir\u00f3 project at Carnegie Mellon University is designing and implementing a visual language for specifying properties of large software systems. We are designing the language in tandem with giving it a formal semantics. We present the semantics of our language as applied to the security domain.", "num_citations": "20\n", "authors": ["1577"]}
{"title": "Report: Measuring the attack surfaces of enterprise software\n", "abstract": " Software vendors are increasingly concerned about mitigating the security risk of their software. Code quality improvement is a traditional approach to mitigate security risk; measuring and reducing the attack surface of software is a complementary approach. In this paper, we apply a method for measuring attack surfaces to enterprise software written in Java. We implement a tool as an Eclipse plugin to measure an SAP software system\u2019s attack surface in an automated manner. We demonstrate the feasibility of our approach by measuring the attack surfaces of three versions of an SAP software system. We envision our measurement method and tool to be useful to software developers for improving software security and quality.", "num_citations": "19\n", "authors": ["1577"]}
{"title": "FAQ on \u03c0-Calculus\n", "abstract": " Let P and Q denote processes. Then\u2022 P| Q denotes a process composed of P and Q running in parallel.\u2022 a (x). P denotes a process that waits to read a value x from the channel a and then, having received it, behaves like P.\u2022 \u0101\u2329 x\u232a. P denotes a process that first waits to send the value x along the channel a and then, after x has been accepted by some input process, behaves like P.\u2022(\u03bda) P ensures that a is a fresh channel in P.(Read the Greek letter \u201cnu\u201d as \u201cnew.\u201d)\u2022! P denotes an infinite number of copies of P, all running in parallel.\u2022 P+ Q denotes a process that behaves like either P or Q.\u2022 0 denotes the inert process that does nothing.", "num_citations": "18\n", "authors": ["1577"]}
{"title": "Family Values; A Semantic Notion of Subtyping\n", "abstract": " The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The paper also discusses the ramifications of this notion on the design of types families and on the contents of type specifications and presents a notation for specifying types formally.Descriptors:", "num_citations": "17\n", "authors": ["1577"]}
{"title": "Extending Ina Jo with temporal logic\n", "abstract": " The authors give both informal and formal descriptions of both the current Ina Jo specification language and Ina Jo enhanced with temporal logic. They include details of a simple example to demonstrate the use of the proof system and details of an extended example to demonstrate the expressiveness of the enhanced language. The authors discuss their language design goals, decisions, and their implications.< >", "num_citations": "17\n", "authors": ["1577"]}
{"title": "Formal semantics for visual specification of security\n", "abstract": " Visual languages, like all languages, need a formal semantics. This chapter presents an outline of a visual language and gives a formal definition of its meaning.Pictures from a language that has ambiguous (informal) interpretations for graphical constructs only serve to frustrate the user of the visual language, and confuse the reader (\" But what does it mean?\"). Some languages at least come equipped with rules that determine when a\" picture\"* is well formed. A formal semantics, however, would describe not only the syntactically valid pictures, but more importantly, their mathematical interpretation. That is, it does not suffice to give only a BNF for pictures; one must additionally map each well-formed picture onto some underlying mathematical entity.", "num_citations": "17\n", "authors": ["1577"]}
{"title": "Purpose restrictions on information use\n", "abstract": " Privacy policies in sectors as diverse as Web services, finance and healthcare often place restrictions on the purposes for which a governed entity may use personal information. Thus, automated methods for enforcing privacy policies require a semantics of purpose restrictions to determine whether a governed agent used information for a purpose. We provide such a semantics using a formalism based on planning. We model planning using Partially Observable Markov Decision Processes (POMDPs), which supports an explicit model of information. We argue that information use is for a purpose if and only if the information is used while planning to optimize the satisfaction of that purpose under the POMDP model. We determine information use by simulating ignorance of the information prohibited by the purpose restriction, which we relate to noninterference. We use this semantics to develop a sound audit\u00a0\u2026", "num_citations": "16\n", "authors": ["1577"]}
{"title": "Attack graph generation and analysis\n", "abstract": " Attack graphs represent the ways in which an adversary can exploit vulnerabilities to break into a system. System administrators analyze these attack graphs to understand where their system's weaknesses lie and to help decide which security measures will be effective to deploy. In practice, attack graphs are produced manually by Red Teams. Construction by hand, however, is tedious, error-prone, and impractical for attack graphs larger than a hundred nodes. In this talk I present a technique, based on model checking, for generating attack graphs automatically. I also describe different analyses that system administrators can perform in trading off one security measure for another or in using attack graphs in intrusion detection. Work on generating attack graphs is joint with Somesh Jha and Oleg Sheyner; on analyzing them, joint with Oleg Sheyner and Oren Dobzinski.", "num_citations": "16\n", "authors": ["1577"]}
{"title": "Constraining pictures with pictures\n", "abstract": " This paper presents a visual language called Mir\u00f3 for specifying and restricting operating system security configurations. A Mir\u00f3 picture specifies exactly what rights users have on files. A Mir\u00f3 constraint, also stated visually, restricts the set of Mir\u00f3 pictures which are considered legal. Such constraints on pictures give an exact specification of security policies and a practical method for alerting users to potential security holes. The language is easy to use and succinct.This research was sponsored by IBM and the Maryland Procurement Office under Contract No. MDA904-88-C-6005. Additional support for J. Wing was provided in part by the National Science Foundation under grant CCR-8620027 and for JD Tygar under a Presidential Young Investigator Award, Contract No. CCR-8858087. M. Maimone (under contract N00014-88-K-0641) and A. Moormann are also supported by fellowships from the Office of Naval\u00a0\u2026", "num_citations": "16\n", "authors": ["1577"]}
{"title": "Scenario graphs applied to security\n", "abstract": " Traditional model checking produces one counterexample to illustrate a violation of a property by a model of the system. Some applications benefit from having all counterexamples, not just one. We call this set of counterexamples a scenario graph. In this paper we present two different algorithms for producing scenario graphs and explain how scenario graphs are a natural representation for attack graphs used in the security community.", "num_citations": "15\n", "authors": ["1577"]}
{"title": "Specifications and their use in defining subtypes\n", "abstract": " Specifications are useful because they allow reasoning about objects without concern for their implementations. Type hierarchies are useful because they allow types that share common properties to be designed as a family. This paper is concerned with the interaction between specifications and type hierarchies. We present a way of specifying types, and show how some extra information, in addition to specifications of the objects' methods, is needed to support reasoning. We also provide a new way of showing that one type is a subtype of another. Our technique makes use of information in the types' specifications and works even in a very general computational environment in which possibly concurrent users share mutable objects.", "num_citations": "15\n", "authors": ["1577"]}
{"title": "Using belief to reason about cache coherence\n", "abstract": " The notion of belief has been useful in reasoning about authentication protocols. In this paper, we show how the notion of belief can be applied to reasoning about cache coherence in a distributed file system. To the best of our knowledge, this is the first formal analysis of this problem. We used an extended subset of a logic of authentication[4, 5] to help us analyze three cache coherence protocols: a validate-on-use protocol, an invalidation-based protocol, and a new large granulari~ protocol for use in weakly connected environments. In this paper, we present two runs from the large granularity protocol. Using our variant of the logic of authentication, we were able to find flaws in the design of the large granularity protocol. We found the notion of belief not only intuitively appealing for reasoning about our protocols, but also practical given the optimistic nature of our system model.", "num_citations": "14\n", "authors": ["1577"]}
{"title": "Measuring the attack surfaces of sap business applications\n", "abstract": " Software vendors such as SAP are increasingly concerned about mitigating the security risk of their software. Code quality improvement is a traditional approach to mitigate security risk; measuring and reducing the attack surface of software is a complementary approach. In this paper, we introduce a method for measuring the attack surfaces of SAP business applications implemented in Java. We implement a tool as an Eclipse plugin to measure an SAP software system\u2019s attack surface in an automated manner. We demonstrate the feasibility of our approach by measuring the attack surfaces of three versions of an SAP software system. SAP\u2019s software developers can use the tool as part of the software development process to improve software quality and security. SAP\u2019s customers can also use the tool to mitigate their security risk.", "num_citations": "12\n", "authors": ["1577"]}
{"title": "LCL: A Larch interface language for C\n", "abstract": " LCL is a Larch interface language for Standard C. LCL is not a C dialect. Programs specified and developed with LCL are C programs, accepted by ordinary C compilers. Use of LCL will tend to encourage some styles of development, but it does not change the programming language.", "num_citations": "12\n", "authors": ["1577"]}
{"title": "Data Science Leadership Summit: Summary Report\n", "abstract": " Data science is a burgeoning field. As a result of recent technological advances, widespread and accelerated uptake of these technologies by many sectors, and increasing workforce demands, many data science initiatives across universities and colleges in the US and beyond are sprouting up at a rapid pace. The Data Science Leadership Summit, hosted on March 26, 2018 by the Data Science Institute at Columbia University, was the first convening of leaders of these initiatives. The Summit was co-funded by the National Science Foundation, the Gordon and Betty Moore Foundation, and the Alfred P. Sloan Foundation.", "num_citations": "11\n", "authors": ["1577"]}
{"title": "Hints to specifiers\n", "abstract": " Over the years I have been accumulating hints that I give students in response to common problems and recurrent questions that arise as they try their hand at writing specifications. I often remind myself of these hints when I write specifications too. I've broadly categorized them along the following dimensions:", "num_citations": "11\n", "authors": ["1577"]}
{"title": "Specifications as search keys for software libraries: A case study using lambda prolog\n", "abstract": " Searching through a large repository of objects can be a tedious activity if a user cannot easily identify the object of interest. In the context of software development, we describe a method of searching through program libraries using specification matching. We use signature information along with pre-and postcondition specifications as search keys to increase the recall and precision of a query. This paper details a case study of specification matching where we use Lambda Prolog as our specification and query language and higher-order unification to retrieve from a library of ML functions. We discuss the significance of specification matching in general and point out some open issues.", "num_citations": "11\n", "authors": ["1577"]}
{"title": "Specifying Avalon Objects in Larch\n", "abstract": " This paper gives a formal specification of three base Avalon/C++ classes: recoverable, atomic, and subatomic. Programmers derive from class recoverable to define persistent objects, and from either class atomic or class subatomic to define atomic objects. The specifications, written in Larch, provide the means for showing that classes derived from the base classes implement objects that are persistent or atomic, and thus exemplify the applicability of an existing specification method to specifying \u201cnonfunctional\u201d properties. Writing these formal specifications for Avalon/C++'s built-in classes has helped clarify places in the programming language where features interact, make explicit unstated assumptions, and make precise complex properties of objects.", "num_citations": "11\n", "authors": ["1577"]}
{"title": "Extracting conditional confidentiality policies\n", "abstract": " Programs should keep sensitive information, such as medical records, confidential. We present a static analysis that extracts from a program's source code a sound approximation of the most restrictive conditional confidentiality policy that the program obeys. To formalize conditional confidentiality policies, we present a modified definition of noninterference that accommodates runtime information. We implement our analysis and experiment with the resulting tool on C programs. While we focus on using our analysis for policy extraction, the process can more generally be used for information flow analysis. Unlike traditional information flow analysis that simply states what flows are possible in a program, our tool also states what conditions must be satisfied by an execution for each flow to be enabled. Furthermore, our analysis is the first to handle interactive I/O while being compositional and flow sensitive.", "num_citations": "10\n", "authors": ["1577"]}
{"title": "Teaching mathematics to software engineers\n", "abstract": " Based on my experience in teaching formal methods to practicing and aspiring software engineers, I present some of the common stumbling blocks faced when writing formal specifications. The most conspicuous problem is learning to abstract. I address all these problems indirectly by giving a list of hints to specifiers. Thus this paper should be of interest not only to teachers of formal methods but also to their students.", "num_citations": "10\n", "authors": ["1577"]}
{"title": "Verifying atomic data types\n", "abstract": " Atomic transactions are a widely-accepted technique for organizing computation in fault-tolerant distributed systems. In most languages and systems based on transactions, atomicity is implemented through atomic objects, typed data objects that provide their own synchronization and recovery. Hence, atomicity is the key correctness condition required of a data type implementation. This paper presents a technique for verifying the correctness of implementations of atomic data types. The significant aspect of this technique is the extension of Hoare's abstraction function to map to a set of sequences of abstract operations, not just to a single abstract value. We give an example of a proof for an atomic queue implemented in the programming language Avalon/C++.", "num_citations": "10\n", "authors": ["1577"]}
{"title": "Mir\u00f3 tools\n", "abstract": " Mir\u00f3 provides a visual way to specify security configurations. It consists of two visual specification languages, the instance language and the constraint language. This paper describes the current Mir\u00f3 tool support. What makes some of these tools particularly novel are the non-trivial algorithms implemented to check for properties such as ambiguity. What makes the overall design of our Mir\u00f3 environment particularly interesting and useful for prototyping is the loosely-coupled way in which the individual tools interact.", "num_citations": "10\n", "authors": ["1577"]}
{"title": "Ten research challenge areas in data science\n", "abstract": " Although data science builds on knowledge from computer science, mathematics, statistics, and other disciplines, data science is a unique field with many mysteries to unlock: challenging scientific questions and pressing questions of societal importance. This article starts with meta-questions about data science as a discipline and then elaborates on ten ideas for the basis of a research agenda for data science.", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Cyber-physical systems research challenges\n", "abstract": " \u201cAs smart as in-car navigation devices are, they could be smarter. They could talk to each other via the Internet and share information on how fast traffic is moving on the roads they have just traveled. And they could also use the Internet to let you search for places of interest, get map updates, or even receive new destinations wirelessly.\u201d", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Combining theory generation and model checking for security protocol analysis\n", "abstract": " This paper reviews two relatively new tools for automated formal analysis of security protocols. One applies the formal methods technique of model checking to the task of protocol analysis, while the other utilizes the method of theory generation. which borrows from both model checking and automated theorem proving. For purposes of comparison. the tools are both applied to a suite of sampIe protocols with known laws. including the protocol used in an earlier study to provide a baseline. We then suggest a heuristic for combining the two approaches to provide a more complete analysis than either approach can provide alone.Descriptors:", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Decomposing and recomposing transactional concepts\n", "abstract": " Distributed systems are different from concurrent (and parallel) systems because they need to deal with failures, not just concurrency. Transactions are a way of masking the distributed nature of a computation at the programming language level by transforming all failures into aborted transactions. If a communication link goes down or a node crashes, the transaction simply aborts. Users may try again later to rerun their computation, but they are at least guaranteed that the system is left in some consistent state.Transactions are a well-known and fundamental control abstraction that arose out of the database community. They have three properties that distinguish them from normal sequential processes:(1) A transaction is a sequence of operations that is performed atomically (\" all-or-nothing\"). If it completes successfully, it commits; otherwise, it aborts;(2) concurrent transactions are serializable (appear to occur one-at\u00a0\u2026", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Reasoning about atomic objects\n", "abstract": " Atomic transactions are a widely-accepted technique for organizing activities in reliable distributed systems. In most languages and systems based on transactions, atomicity is implemented through atomic objects, which are typed data objects that provide their own synchronization and recovery. This paper describes new linguistic mechanisms for constructing atomic objects from non-atomic components, and it formulates proof techniques that allow programmers to verify the correctness of such implementations.", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Geometric reasoning: a new paradigm for processing geometric information\n", "abstract": " CMU-CS-85-144 Existing approaches to geometric modeling use rigid, static data structures, often tuned for one specific application. This inflexibilityrestricts users to inadequate means of manipulating geometric models. Our alternative approach, Geometric Reasoning, applies deductive reasoning to manipulate geometric information at an abstract level. Geometric reasoning is finding attributes of geometric objects using their intrinsic properties, their relationships with other objects, and the inference rules that bind such properties together in a geometric space. Instead of using data structures tailored for numerical computation, we use an inference mechanism that understands the semantics of geometric objects and allows dynamic definition of abstractions, eg, shapes and relationships.", "num_citations": "9\n", "authors": ["1577"]}
{"title": "Trustworthy ai\n", "abstract": " The promise of AI is huge. AI systems have already achieved good enough performance to be in our streets and in our homes. However, they can be brittle and unfair. For society to reap the benefits of AI systems, society needs to be able to trust them. Inspired by decades of progress in trustworthy computing, we suggest what trustworthy properties would be desired of AI systems. By enumerating a set of new research questions, we explore one approach--formal verification--for ensuring trust in AI. Trustworthy AI ups the ante on both trustworthy computing and formal methods.", "num_citations": "8\n", "authors": ["1577"]}
{"title": "Information flow investigations\n", "abstract": " Nontraditional Information Flow Problems. Concerns about privacy have led to much interest in determining how third-party associates of first-party websites use the information they collect about the visitors to the first-party website. Some researchers have attempted to determine what these third-parties do with the information they collect [3, 6]. These researchers propose and use various analyses to determine what information is tracked and how it is used. They primarily design their analyses by intuition and do not formally present or study their analyses. Thus, questions remain:(1) Are the analyses used sound and/or complete?(2) Are they related to more formal prior work? Furthermore, much work has been done on the detection of illicit flows of copyrighted files, such as work on watermarking [5, 4] and traitor tracing [1]. Similarly, companies handling sensitive data have adopted a variety of methods to discourage the misuse of such data by their employees. In particular, they employ watermarking-like counterintelligence operations to detect such leaks and determine the identity of the employee leaking the information [7].In essence, each of the approaches used to solve these problems is an information flow analysis (IFA). In particular, the analyst would like to determine whether a system (a person or computer) is enabling a concerning flow of information. For example, an public advocacy group (an analyst) might want to determine whether Google (a system) uses a person\u2019s health-related web searches (a sensitive information source) to select advertisements (a low-level information sink).", "num_citations": "8\n", "authors": ["1577"]}
{"title": "Specifying security constraints with relaxation lattices\n", "abstract": " A description is given of the relaxation lattice approach to specifying graceful degradation for a large class of systems. The method is applied to the security domain by identifying degraded systems behaviors with those that can result from security violations such as a user of one security class obtaining access rights associated with those of a higher class. The method can be used in two ways: (1) as a descriptive technique for specifying the behavior of existing systems in which breaches of security may inadvertently or unavoidably occur; and (2) as a formal design technique for specifying a range of behaviors, from ideal to undesired, of systems to be implemented.< >", "num_citations": "8\n", "authors": ["1577"]}
{"title": "Computational Thinking \u8a08\u7b97\u8ad6\u7684\u601d\u8003\n", "abstract": " \u8ad6\u6587\u6284\u9332\u3053\u306e\u30a8\u30c3\u30bb\u30a4\u306f\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u79d1\u5b66\u8005\u3060\u3051\u3067\u306f\u306a\u304f, \u3059\u3079\u3066\u306e\u4eba\u304c\u5b66\u3073, \u305d\u3057\u3066\u4f7f\u3044\u305f\u3044\u3068\u8003\u3048\u308b\u306b\u9055\u3044\u306a\u3044\u4e00\u822c\u7684\u306a\u614b\u5ea6\u3068\u30b9\u30ad\u30eb\u306b\u95a2\u3059\u308b\u3082\u306e\u3067\u3042\u308b.", "num_citations": "7\n", "authors": ["1577"]}
{"title": "Towards an algebra for security policies\n", "abstract": " Clashing security policies leads to vulnerabilities. Violating security policies leads to vulnerabilities. A system today operates in the context of a multitude of security policies, often one per application, one per process, one per user. The more security policies that have to be simultaneously satisfied, the more likely the possibility of a clash or violation, and hence the more vulnerable our system is to attack. Moreover, over time a system\u2019s security policies will change. These changes occur at small-scale time steps, e.g., using setuid to temporarily grant a process additional access rights; and at large-scale time steps, e.g., when a user changes his browser\u2019s security settings. We address the challenge of determining when a system is in a consistent state in the presence of diverse, numerous, and dynamic interacting security policies.", "num_citations": "7\n", "authors": ["1577"]}
{"title": "Specifying weak sets\n", "abstract": " We present formal specifications of a new abstraction, weak sets, which can be used to alleviate high latencies when retrieving data from a wide-area information system like the World Wide Web. In the presence of failures, concurrency, and distribution, clients performing queries may observe behavior that is inconsistent with the stringent semantic requirements of mathematical sets. For example, an element retrieved and returned to the client may be subsequently deleted before the query terminates. We chose to specify formally the behavior of weak sets because we wanted to understand the varying degrees of inconsistency clients might be willing to tolerate and to understand the trade off between providing strong consistency guarantees and implementing weak sets efficiently. Our specification assertion language uses a novel construct that lets us model reachability explicitly; with it, we can distinguish between\u00a0\u2026", "num_citations": "7\n", "authors": ["1577"]}
{"title": "Experience with the Larch prover\n", "abstract": " Many people have argued the importance of mechanical theorem-proving for reasoning about programs. Proving the correctness of programs by hand is usually hard and errorprone. People often miss boundary cases or forget to state hidden assumptions. On the other hand, can current mechanical theorem provers deal with a wide scope of non-trivial problems? Here, the question of scale is in diversity of problems as well as in complexity of each problem. Some provers are more suitable for one class of problems than others and all provers have space and time bounds that set practical limits on the size of an individual problem that can be handled.This position paper summarizes our experience 1181 using the Larch Prover (LP)(61 as a mechanical aid for proving properties of Avalon/C++ programs [5]. Avalon/C++ is a programming language that deals with concurrency and faults. Its semantics are based on a\u00a0\u2026", "num_citations": "7\n", "authors": ["1577"]}
{"title": "A Larch specification of the library problem\n", "abstract": " A claim made by many in the formal specification community is that forcing precision in the early stages of program development can greatly clarify the understanding of a client's problem requirements. We help justify this claim via an example by firstwalking through a Larch specification of Kemmerer's library problem and then discussing the questions that arose in our process of formalization. Following this process helped reveal mistakes, premature design decisions, ambiguities, and incompletenesses in the informal requirements. We also discusshow Larch's two. tiered specification method influenced our modificationsto and extrapolations from the requirements.", "num_citations": "7\n", "authors": ["1577"]}
{"title": "Cyber-physical systems research charge\n", "abstract": " Cyber-Physical Systems Research Charge Page 1 Cyber-Physical Systems Research Charge Jeannette M. Wing Assistant Director Computer and Information Science and Engineering Directorate National Science Foundation and President\u2019s Professor of Computer Science Carnegie Mellon University Cyber-Physical Systems Summit St. Louis, MO 24 April 2008 Page 2 2 CPS Summit Jeannette M. Wing Smart Cars A BMW is \u201cnow actually a network of computers\u201d [R. Achatz, Seimens, Economist Oct 11, 2007] Lampson\u2019s Grand Challenge: Reduce highway traffic deaths to zero. [Butler Lampson, Getting Computers to Understand, Microsoft, J. ACM 50, 1 (Jan. 2003), pp 70-72.] Cars drive themselves Credit: PaulStamatiou.com Dash Express: Cars are nodes in a network Credit: Dash Navigation, Inc. Page 3 3 CPS Summit Jeannette M. Wing Embedded Medical Devices pacemaker infusion pump scanner Page 4 4 \u2026", "num_citations": "6\n", "authors": ["1577"]}
{"title": "Computer science meets science and engineering\n", "abstract": " Theory Generation for Security Protocols Page 1 Computer Science Meets Science and Engineering Jeannette M. Wing Assistant Director for Computer and Information Science and Engineering, NSF and President\u2019s Professor of Computer Science, CMU HEC FSIO R&D Workshop, NSF August 6, 2007 Page 2 2 Looking Forward Jeannette M. Wing Outline \u2022 Two Comments on NSF \u2022 Super Data Cluster \u2022 Questions for You Page 3 3 Looking Forward Jeannette M. Wing Back to Basics \u2022 NSF is about basic science and engineering. Preserve CISE core. \u2022 It\u2019s all about good ideas and good people. \u2022 It\u2019s about \u201chigh risk\u201d long term impact. Impact may be far in the future. Impact is long-lasting (that is real science). Impact can create new economies and change societal behavior. Say \u201cNo\u201d to incrementalism! Promote new, emerging areas of computing. : Transformative Research Page 4 4 Looking Forward Jeannette M. Wing \u2026", "num_citations": "6\n", "authors": ["1577"]}
{"title": "Closing the idealization gap with theory generation\n", "abstract": " Cryptographic protocol design demands careful verification during all phases of development. Belief logics, in the tradition of the Burrows, Abadi, and Needham (BAN) logic of authentication [BAN90], provide a simple, intuitive model, and allow natural expressions of a protocol and its goals. Since manual deduction is error-prone, protocol designers need automated tools to make effective use of these logics. Such tools often require excessive human intervention or supply inadequate feedback during the verification process.We take a new approach,\u201ctheory generation,\u201d which allows highly automated reasoning with these logics, and which supports new forms of protocol analysis. In this approach, given a logic, L, we generate a finite representation, T, of the full theory, corresponding to a protocol, P. Given this representation, determining whether the protocol satisfies some property,, requires only a simple membership test, 2 T?(Figure 1). Furthermore, since the theory is represented by a finite set of formulas, we can analyze differences between protocols by comparing the generated theories, and we can easily answer questions such as,\u201cWhat beliefs does this principal hold after receiving message 2?\u201d In earlier work described in our USENIX paper, we applied theory generation to three different belief logics (BAN, AUTLOG [KW94], and Kailar\u2019s accountability logic [Kai96]), and seven protocols for authentication and electronic commerce [KW96]. BAN-style belief logics enable the designer to think about a protocol at a convenient level of abstraction; however, the gap between the \u201cidealized\u201d protocol", "num_citations": "6\n", "authors": ["1577"]}
{"title": "A formal specification of a visual language editor\n", "abstract": " This paper presents a non-trivial case study on the use of the Larch [GHW85, GHM90] specification languages to describe the Mir\u00f3 visual languages and graphical editor [HMT+ \u00c7O]. In addition to excerpts from the specification, we discuss properties of Mir\u00f3 provable from the specification, limitations of Larch, and general lessons learned from this exercise. The companion technical report, CMU-CS-91-111, contains the entire specification.", "num_citations": "6\n", "authors": ["1577"]}
{"title": "The Avalon/C++ programming language (version 0)\n", "abstract": " Avalon/C++ is a language for implementing reliable distributed programs. People who wish to read or write Avalon/C++ programs should read this document, though not necessarily all of it. It contains a quick overview of the terminology of our intended application domain, a tutorial-by-example introduction to the language, a reference manual for the Avalon extensions to Ct, a library of built-in classes, and a list of practical programming guidelines. The appendices include the language's grammar and the UNIX man pages for acc, the Avalon/C+--preprocessor.", "num_citations": "6\n", "authors": ["1577"]}
{"title": "Linearizable concurrent objects\n", "abstract": " Technological advances are making multiprocessors more readily available, but despite impressive progress at the hardware level, it is still difficult to realize these machines' potential for parallelism. In the sequential domain,\u201cobject-oriented\u201d programming methodologies based on data abstraction are widely recognized as an effective means of enhancing modularity, expressibility, and correctness. Our paper describes the foundations for a new approach to extending object-oriented methodologies to highly-concurrent shared-memory multiprocessors.", "num_citations": "6\n", "authors": ["1577"]}
{"title": "Durra: Language support for large-grained parallelism\n", "abstract": " We are interested in a class of real-time, embedded applications in which a number of concurrent, large-grained tasks cooperate to process data obtained from physical sensors, to make decisions based on these data, and to send commands to control motors and other physical devices. Since the speed of, and the resources required by each task may vary, these applications can best exploit a computing environment consisting of multiple special-and general-purpose, loosely connected processors. We call this environment a heterogeneous machine.During execution time, processes, which are instances of tasks, run on possibly separate processors and communicate with each other by sending messages. Since the patterns of communication can vary over time, and, since the speed of the individual processors can vary over a wide range, additional hardware resources in the form of switching networks and data buffers are also required in the heterogeneous machine. The application developer is responsible for prescribing a way to manage all of these resources. We call this prescription a task-level application description. It describes the tasks to be executed and the intermediate queues required to store the data as it moves from producer to consumer processes. A task-level description language is a notation for writing these application descriptions.", "num_citations": "6\n", "authors": ["1577"]}
{"title": "First International Workshop on Larch: Proceedings of the First International Workshop on Larch, Dedham, Massachusetts, USA, 13\u201315 July 1992\n", "abstract": " The papers in this volume were presented at the First International Workshop on Larch, held at MIT Endicott House near Boston on 13-15 July 1992. Larch is a family of formal specification languages and tools, and this workshop was a forum for those who have designed the Larch languages, built tool support for them, particularly the Larch Prover, and used them to specify and reason about software and hardware systems. The Larch Project started in 1980, led by John Guttag at MIT and James Horning, then at Xerox/Palo Alto Research Center and now at Digital Equipment Corporation/Systems Research Center (DEC/SRC). Major applications have included VLSI circuit synthesis, medical device communications, compiler development and concurrent systems based on Lamport's TLA, as well as several applications to classical theorem proving and algebraic specification. Larch supports a two-tiered approach to specifying software and hardware modules. One tier of a specification is wrillen in the Larch Shared Language (LSL). An LSL specification describes mathematical abstractions such as sets, relations, and algebras; its semantics is defined in terms of first-order theories. The second tier is written in a Larch interface language, one designed for a specific programming language. An interface specification describes the effects of individual modules, eg state changes, resource allocation, and exceptions; its semantics is defined in terms of first-order predicates over two states, where state is defined in terms of the programming language's notion of state. Thus, LSL is programming language independent; a Larch interface language is\u00a0\u2026", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Toward compositional analysis of security protocols using theorem proving\n", "abstract": " Complex security protocols require a formal approach to ensure their correctness. The protocols are frequently composed of several smaller, simpler components. We would like to take advantage of the compositional nature of such protocols to split the large verification task into separate and more manageable pieces. Various formalisms have been used successfully for reasoning about large protocol compositions by hand. However hand proofs are prone to error. Automated proof systems can help make the proofs more rigorous. The goal of our work is to develop an automated proof environment for compositional reasoning about systems. This environment would combine the power of compositional reasoning with the rigor of mechanically-checked proofs. The hope is that the resulting system would be useful in verification of security protocols of real-life size and complexity. Toward this goal we present results of a case study in compositional verification of a private communication protocol with the aid of automated proof tool IsabelleIOA.Descriptors:", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Respectful type converters for mutable types\n", "abstract": " In converting an object of one type to another, we expect some of the original object's behavior to remain the same, and some to change. How can we state the relationship between the original object and converted object to characterize what information is preserved and what is lost after the conversion takes place? We answer this question by introducing the new relation, respects, and say that a type converter function K: A+ B respects a type T. We formally define respects in terms of the Liskov and Wing behavioral notion of subtyping; types A and B are subtypes of T.In previous work we defined respects for immutable types A, B, and T; in this chapter we extend our notion to handle conversions between mutable types. This extension is nontrivial since we need to consider an object's behavior as it varies over time. We present in detail two examples to illustrate our ideas: one for converting between PNG images and GIF images and another for converting between different kinds of bounded event queues. This work was inspired in building at Carnegie Mellon the Typed Object Model (TOM) conversion service, in daily use worldwide.", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Tools and partial analysis\n", "abstract": " As computing systems continue to grow in scale and functionality, so will their complexity. As systems become more complex there is an increased likelihood of errors, some of which may cause catastrophic loss in money, time, or even human life. The formal methods research community has long advocated verifying a model of a system against a set of specified properties so as to gain greater assurance that the system behaves as desired. The process of formalization does not give us any absolute guarantees about correctness, but it greatly increases our understanding of the system, often by revealing inconsistencies, ambiguities, and incompletenesses in its design.In the past, the use of formal methods in practice seemed hopeless. The notations were too obscure, the techniques did not scale, and the tool support was inadequate or too hard to use. There were only a few non-trivial case studies and together\u00a0\u2026", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Tinkertoy transactions\n", "abstract": " We describe the design of a transaction facility for a language that supports higher-order functions. We factor transactions into four separable features: persistence, undoability, locking, and threads. Then, relying on function composition, we show how we can put them together again. Our\\Tinkertoy\" approach towards building transactions enables us to construct a model of concurrent, nested, multi-threaded transactions, as well as other non-traditional models where not all features of transactions are present. Key to our approach is the use of higher-order functions to make transactions rst-class. Not only do we get clean composability of transactional features, but also we avoid the need to introduce special control and block-structured constructs as done in more traditional transactional systems. We implemented our design in Standard ML of New Jersey.This research is sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330; and in part by National Science Foundation Fellowships for D. Kindred and JG Morrisett. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the o cial policies or endorsements, either expressed or implied, of Wright Laboratory or the US Government. The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. This manuscript is submitted for publication with the understanding that the US Government is\u00a0\u2026", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Larch: languages and tools for formal specification\n", "abstract": " Building software often seems harder than it ought to be. It takes longer than expected, the software\u2019s functionality and performance are not as wonderful as hoped, and the software is not particularly malleable or easy to maintain. It does not have to be that way. This book is about programming, and the role that formal specifications can play in making programming easier and programs better. The intended audience is practicing programmers and students in undergraduate or basic graduate courses in software engineering or formal methods. To make the book accessible to such an audience, we have not presumed that the reader has formal training in mathematics or computer science. We have, however, presumed some programming experience.", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Machine-assisted proofs of properties of Avalon programs\n", "abstract": " Proving the correctness of programs by hand is hard and error-prone. How can mechanical theorem proving aids such as the Larch Prover (LP) help in the proofs of complex programs? We address this question by applying LP, a proof checker based on rewrite-rule theory, to the proof of an Avalon/C++ program. Avalon/C++ is a programming language that supports concurrency and fault-tolerance through transaction-based computations. Since reasoning about an Avalon/C++ program requires reasoning about histories, ie, sequences of operations, and not just initial and final states, proofs of correctness are non-trivial. For the Avalon/C++ queue example, we present a formal Larch Shared Language specification, which we also used as input to LP. We discuss the LP-assisted proofs we performed of the representation invariant and the queue's key correctness condition, give detailed statistics of our proofs, and\u00a0\u2026", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Specification firms: A vision for the future\n", "abstract": " The analogies that specifiers are like lawyers and that specifiers are like architects are often drawn. Pursuing these analogies further leads to the conclusion that in the future we may expect to see specification firms for which specifiers work and are hired out independently much like lawyers are in law firms and architects are in architectural firms. In this position paper, I justify these analogies, and explore their ramifications to the software engineering process.", "num_citations": "5\n", "authors": ["1577"]}
{"title": "Enabling Computer and Information Science and Engineering Research and Education in the Cloud\n", "abstract": " Cloud computing has the potential to transform both research and education in the CISE (computer and information science and engineering) community. The CISE directorate of the National Science Foundation convened a workshop on January 8-9, 2018, to bring together representatives from academia, industry, and government to discuss ways to enable CISE research and education to most effectively use the cloud. The workshop agenda and list of attendees appear as appendices at the end of the report.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Jeannette M. Wing@ PCAST; Barbara Liskov Keynote\n", "abstract": " The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmJeannette M. Wing discusses her PCAST presentation about the importance of computer science and its impact. Valerie Barr shares highlights from Barbara Liskov's keynote at Grace Hopper.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "On the semantics of purpose requirements in privacy policies\n", "abstract": " Privacy policies often place requirements on the purposes for which a governed entity may use personal information. For example, regulations, such as HIPAA, require that hospital employees use medical information for only certain purposes, such as treatment. Thus, using formal or automated methods for enforcing privacy policies requires a semantics of purpose requirements to determine whether an action is for a purpose or not. We provide such a semantics using a formalism based on planning. We model planning using a modified version of Markov Decision Processes, which exclude redundant actions for a formal definition of redundant. We use the model to formalize when a sequence of actions is only for or not for a purpose. This semantics enables us to provide an algorithm for automating auditing, and to describe formally and compare rigorously previous enforcement methods.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "CS woes: deadline-driven research, academic inequality\n", "abstract": " The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish excerpts from selected posts.Follow us on Twitter at http://twitter.com/blogCACMJeannette M. Wing writes about the negative effects of deadline-driven research and Mark Guzdial discusses the role of computer science faculty in fostering inequality.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Beyond the horizon: A call to arms\n", "abstract": " This article is a call to arms to the research community. Today's attacks exploit codelevel flaws such as buffer overruns and type invalid input. I would like the attention of the research community to turn to tomorrow's attacks-to think beyond buffer overruns, beyond the level of code, beyond the horizon. There are three reasons I make this call. First, while we will continue in the future to see today's kinds of code-level attacks, in principle we have the technology for fending them, either by applying static and dynamic analysis tools or by coding in type-safe programming languages. Thus, we have the technical solutions in hand to detect or prevent these attacks; it is a\" mere\" matter of deploying them-in an effective, scalable, and practical way. 1 Second, the trends monitored by security watchdog organizations such as SEI/CERT, MITRE/CVE, and Symantec suggest that attacks are getting more sophisticated. As we get better at protecting our systems the enemy gets better at attacking them. This trend will likely escalate since industry and government both have highlighted the growing importance of security (eg, Microsoft's Trustworthy Computing Initiative and the creation of the Department of Homeland Defense). Thus, we should be anticipating today what the buffer overrun of tomorrow will be.Finally, prevention is the most efficient defense. It eliminates classes of attack from the get-go. We need to raise the bar in our own efforts to deploy systems that are more secure by design and more reliably implemented than those we know how to design and implement today. We need to continue to push against the limitations, be they technical or not, of the state\u00a0\u2026", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Program specification\n", "abstract": " Specifications can be used in all phases of program development. In the requirement analysis phase, a specification helps crystallize the customer\u2019s possibly vague ideas and reveals contradictions, ambiguities, and incompleteness in the requirements. In program design, a specification captures precisely the interfaces between the modules of a program. Each interface specification provides the module\u2019s client the information needed to use the module without knowledge of its implementation, and simultaneously provides the module\u2019s implementer the information needed to create the module without knowledge of its clients. In program verification, a specification is the statement against which a program is proved correct. Verification is the process of showing the consistency between a program and its specification. In program validation, a specification can be used to generate test cases for black-box testing\u00a0\u2026", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Special issues for fm'99: the first world congress on formal methods in the development of computing systems\n", "abstract": " FORMAL methods are coming of age: mathematical techniques and tools are now regarded as an important part of the development process in a wide range of industrial and governmental organizations. A transfer of technology into the mainstream of systems development is slowly but surely taking place. FM'99, the First World Congress on Formal Methods in the Development of Computing Systems, was a measure of this new-found maturity. It brought together an impressive array of industrial and applications-oriented papers that show how formal methods have been used to tackle real problems. The proceedings are published as volumes 1,708 and 1,709 in Springer-Verlag's Lecture Notes in Computer Science.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "A comparison and combination of theory generation and model checking for security protocol analysis\n", "abstract": " This paper reviews two relatively new tools for automated formal analysis of security protocols. One applies the formal methods technique of model checking to the task of protocol analysis, while the other utilizes the method of theory generation, which borrows from both model checking and automated theorem proving. For purposes of comparison, the tools are both applied to a suite of sample protocols with known flaws, including the protocol used in an earlier study [4] to provide a baseline. We then suggest a heuristic for combining the two approaches to provide a more complete analysis than either approach can provide alone.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Proving correctness of a controller algorithm for the RAID level 5 system\n", "abstract": " Most RAID controllers implemented in industry are complicated and difficult to reason about. This complexity has led to software and hardware systems that are difficult to debug and hard to modify. To overcome this problem Courtright and Gibson (1994) have developed a rapid prototyping framework for RAID architectures which relies on a generic controller algorithm. The designer of a new architecture needs to specify parts of the generic controller algorithm and must justify the validity of the controller algorithm obtained. However the latter task may be difficult due to the concurrency of operations on the disks. This is the reason why it would be useful to provide designers with an automated verification tool tailored specifically for the RAID prototyping system. As a first step towards building such a tool, our approach consists of studying several controller algorithms manually, to determine the key properties that need\u00a0\u2026", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Hints for writing specifications\n", "abstract": " * This research is sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. Views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of Wright Laboratory or the United States Government.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Helping specifiers evaluate their specifications\n", "abstract": " Current research in specifications aims to put formal specifications to productive use throughout the entire programming process, and especially in program design. In the incremental development of a specification, one problem faced by a specifier is the lack of useful feedback informing him whether the specification is any good. One way of providing such feedback is to supply the specifier with ways to evaluate a specification, eg, check that some desired property of the specification holds. In the context of a two-tiered approach to specifying programs, this paper defines and discusses four properties a specifier may wish to check of his specification, and bow he can use the feedback gained from having checked for them. These properties are related to the consistency and completeness of a specification. All definitions are in terms of the theories associated with specifications.", "num_citations": "4\n", "authors": ["1577"]}
{"title": "Geometric computation: foundations for design\n", "abstract": " Geometric Computation: Foundations for Design describes the mathematical and computational concepts that are central to the practical application of design computation in a manner tailored to the visual designer. Uniquely pairing key topics in code and geometry, this book develops the two key faculties required by designers that seek to integrate computation into their creative practice: an understanding of the structure of code in object-oriented programming, and a proficiency in the fundamental geometric constructs that underlie much of the computational media in visual design.", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Software Security\n", "abstract": " Security vulnerabilities are increasingly due to software. While we focus much of our attention today on code-level vulnerabilities, such as buffer overflows, we should be paying more attention to design-level vulnerabilities. Independently designed and implemented components may individually behave properly, but when put together, unanticipated interactions may occur. An unanticipated interaction between two software components is an opportunity for an attacker to exploit.", "num_citations": "3\n", "authors": ["1577"]}
{"title": "La pens\u00e9e informatique\n", "abstract": " Cet article fait suite aux divers interviews que nous avons faits et qui nous invitaient \u00e0 une r\u00e9flexion sur les fondements de notre discipline et ses aspects philosophiques et \u00e9pist\u00e9mologiques. Aujourd'hui l'article de Jeannette Wing nous conduit \u00e0 r\u00e9fl\u00e9chir sur l'utilit\u00e9 et l'ubiquit\u00e9 de la pens\u00e9e informatique et ses implications, mais aussi sur l'essence m\u00eame de cette pens\u00e9e.Jeannette Wing est professeure d'informatique, sur une chaire du pr\u00e9sident, au d\u00e9partement d'informatique de l'Universit\u00e9 Carnegie Mellon. Elle est actuellement directrice adjointe \u00e0 la direction des sciences et technologies des ordinateurs et de l'information \u00e0 la fondation nationale des sciences des \u00c9tats-Unis, son titre exact en", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Verifiable Secret Redistribution\n", "abstract": " The authors present a new protocol to perform non-interactive verifiable secret redistribution VSR for secrets distributed with Shamirs secret sharing scheme. They base their VSR protocol on Desmedt and Jajodias redistribution protocol for linear secret-sharing schemes, which they specialize for Shamirs scheme. They extend their redistribution protocol with Feldmans non-interactive verifiable secret sharing scheme to ensure that a SUBSHARES-VALID condition is true after redistribution. They show that the SUBSHARES-VALID condition is necessary but not sufficient to guarantee that the new shareholders have valid shares, so they present an additional SHARES-VALID condition.Descriptors:", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Formal Methods: Past, Present, and Future\n", "abstract": " Part 1, 32.1. 1... Formal Proofs shall be used unless the Design Authority, the Independent Safety Auditor, the MOD Safety Assurance Authority and the MOD (PE) PM are satisfied that, taking into account the safety risk from failure of the SCS, Rigourous Arguments offer a sufficient degree of assurance.", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Subtyping for distributed object stores\n", "abstract": " The programming language community has come up with many definitions of the subtype relation. The goal is to determine when this assignment   x:T:=E  ]]$$x:T: = E$$ is legal in the presence of subtyping. Once the assignment has occurred, x will be used according to its \u201capparent\u201d type T, with the expectation that if the program performs correctly when the actual type of x\u2019s object is T, it will also work correctly if the actual type of the object denoted by x is a subtype of T.", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Avalon/C++: C++ extensions for transaction-based programming\n", "abstract": " Avalen extends C++ to support transaction-based programming. Avalon allows programmers to implement classes of atomic objects, objects that provide failure atomicity, permanence, and ensure the serializabUity of the transactions that perform operations on them. Classes gain these properties by inheritance from a set of built-in classes. Atomic objects are encapsulated in processes called servers; Avalon provides syntactic constructs for defining and executing servers. Constructs are also provided for beginning, ending, and aborting transactions, and for executing transactions concurrently.Avalon is being implemented as a preprocessor which translates Avalon code into C++. This preprocessor is being built by modifying die C++ preprocessor. The code we generate makes extensive use of the Camelot transaction processing system [9], which, in turn, relies on the Mach operating system [1].", "num_citations": "3\n", "authors": ["1577"]}
{"title": "Highlights of the Inaugural Data Science Leadership Summit\n", "abstract": " On March 26, 2018, the Data Science Institute at Columbia University   hosted the inaugural Data Science Leadership Summit. It drew together a cross-section of academic researchers and administrators to discuss how university practices could best adapt to the emerging field of data science. The summit was co-funded by the National Science Foundation, the Gordon and Betty Moore Foundation, and the Alfred P. Sloan Foundation. This interview with Jeannette Wing, the Avanessians Director of the Data Science Institute at Columbia University and the author of The Data Life Cycle  , discusses the motivation of the summit and summarizes its key findings. The interview was conducted by another experienced leader in data science, David Banks, the Director of the Statistical and Applied Mathematical Sciences Institute (SAMSI).  Keywords: Data Science Institute; Education Units; Inter-disciplinary, Multi-disciplinary; University Administration", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Embracing Uncertainty\n", "abstract": " Sources of uncertainty abound. Noisy sensor data. Machine learning methods. Hardware and software failures. The physical world. Human behavior. In the past, computer science handled uncertainty by abstracting it away or avoiding it. In the future, instead, computer science needs to embrace uncertainty as a first-class entity. How do we represent uncertainty in our computational models? Probabilities. Thus, we need to make sure that every computer science student learns probability and statistics. Data science, where data drives discovery and decision-making in all fields of study, underscores the importance of having a command of probability and statistics. At the heart of data science is data analytics whose methods such as machine learning rely on probabilistic and statistical reasoning. And since data serve as the currency of any data analytics workflow, explicit representation of probability distributions can\u00a0\u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Computational Thinking for All\n", "abstract": " Technology Areas\u2022 Machine learning (deep learning, reinforcement learning)\u2022 Artificial intelligence (related to ML)\u2022 speech, vision, NLP, personalized agents (Siri, Cortana, Google Now, chatbots)\u2022 Search to Q&A,\u201cknowledge\u201d to decision-making, multi-media (text, video, maps)\u2022 at scale, fine-grained resolution, near real-time fidelity\u2022 Cybersecurity: sophistication and number of threats and attacks\u2022 Crypto made practical: homomorphic encryption, secure multi-party computation", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Inverse privacy (revised)\n", "abstract": " An item of your personal information is inversely private if some party has access to it but you do not. We analyze the provenance of inversely private information and its rise to dominance over other kinds of personal information. In a nutshell, the inverse privacy problem is unjustified inaccessibility to you of your inversely private information. We argue that the inverse privacy problem has a market-based solution.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Understanding network complexity\n", "abstract": " Understanding Network Complexity Page 1 1 Understanding Network Complexity Jeannette M. Wing Assistant Director Computer and Information Science and Engineering Directorate National Science Foundation 11 March 2010 Our networks have evolved to be extremely complex and we do not understand them. The Internet, for example, is complex. In 1970 we could draw the four-node DARPAnet on a napkin. Forty years later, due to its scale and dynamic nature, the Internet is too complex to draw, let alone understand, model, or predict its behavior. The Internet is computer science\u2019s gift to society, but ironically we cannot even describe it. In December 2007 NSF challenged the computer and information science and engineering community to address this fundamental question: Is there a science for understanding the complexity of our networks such that we can engineer them to have predictable, or at least \u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Measuring relative attack surfaces\n", "abstract": " Given that security is not an either-or property, how can we determine that a new release of a system is \u201cmore secure\u201d than an earlier version? What metrics should we use and what things should we count? Our work argues that rather than attempt to measure the security of a system in absolute terms with respect to a yardstick, a more useful approach is to measure its \u201crelative\u201d security. We use \u201crelative\u201d in the following sense: Given System A, we compare its security relative to System B, and we do this comparison with respect to a given number of yardsticks, which we call dimensions. So rather than say \u201cSystem A is secure\u201d or \u201cSystem A has a measured security number N\u201d we say \u201cSystem A is more secure than System B with respect to a \ufb01xed set of dimensions.\u201d In what follows, we assume that System A and System B have the same operating environment. That is, the set of assumptions about the environment in which System A and System B is deployed is the same; in particular, the threat models for System A and System B are the same. Thus, it helps to think of System A and System B as different versions of the same system.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Scenario Graphs Applied to Security (Summary Paper)\n", "abstract": " Traditional model checking produces one counterexample to illustrate a violation of a property by a model of the system. Some applications benefit from having all counterexamples, not just one. We call this set of counterexamples a scenario graph. In this paper we present two different algorithms for producing scenario graphs and explain how scenario graphs are a natural representation for attack graphs used in the security community.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Vulnerability analysis of networked systems\n", "abstract": " Vulnerability Analysis of Networked Systems | Proceedings of the 11th IEEE International Workshops on Enabling Technologies: nfrastructure for Collaborative Enterprises ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsWETICE '02Vulnerability Analysis of Networked Systems ARTICLE Vulnerability Analysis of Networked Systems Share on Author: Jeannette M Wing profile image Jeannette M. Wing View Profile Authors Info & Affiliations Publication: WETICE '02: Proceedings of the 11th IEEE International Workshops on Enabling Technologies: nfrastructure for Collaborative EnterprisesJune 2002 0citation 0 Downloads \u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Minimization and Reliability Analyses of Attack Graphs\n", "abstract": " An attack graph is a succinct representation of all paths through a system that end in a state where an intruder has successfully achieved his goal. Today Red Teams determine the vulnerability of networked systems by drawing gigantic attack graphs by hand. Constructing attack graphs by hand is t\u0117dious, error-prone, and impractical for large systems. By viewing an attack as a violation of a safety property, we can use model checking to produce attack graphs automatically: a successful path from the intruder's viewpoint is a counterexample produced by the model checker. In this paper we present an algorithm for generating attack graphs using model checking.Security analysts use attack graphs for detection, defense, and forensics. In this paper we present a minimization technique that allows analysts to decide which minimal set of security measures would guarantee the safety of the system. We provide a formal\u00a0\u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Mathematics in Computer Science Curricula\n", "abstract": " Mathematics in Computer Science Curricula Page 1 Mathematics in Computer Science Curricula School of Computer Science Carnegie Mellon University Pittsburgh, PA Jeannette M. Wing Sixth International Conference on Mathematics of Program Construction July 2002, Dagstuhl, Germany Page 2 2 Math in CS Curricula Jeannette M. Wing Prelude: Three Observations \u2022 Linear Algebra and Probability & Statistics are increasingly important to Computer Scientists. \u2022 As Computer Science matures, more mathematics enters CS curricula in different guises. \u2022 As Computer Science matures, more course material covering mathematically-based concepts moves from the graduate to the undergraduate level. Page 3 3 Math in CS Curricula Jeannette M. Wing Computing at Carnegie Mellon CMU Supercomputing Neural Cognition Pitt School of Computer Science Learning and Discovery Languages Technology Human \u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Survivability Analysis of Networked Systems.\n", "abstract": " Survivability is the ability of a system to continue operating despite the presence of abnormal events such as failures and intrusions. Ensuring system survivability has increased in importance as critical infrastructures have become heavily dependent on computers. In this paper we present a systematic method for performing survivability analysis of networked systems. An architect injects failure and intrusion events into a system model and then visualizes the ef-fects of the injected events in the form of scenario graphs. Our method enables further global analyses, such as reliability, latency, and cost-bene\ufb01t analyses, where mathemati-cal techniques used in di\ufb02erent domains are combined in a systematic manner. We illustrate our ideas on an abstract model of the United States Payment System.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Specifying Weak Sets\n", "abstract": " We present formal speci\ufb01cations of a new abstraction, weak sets, which can be used to alleviate high latencies when retrieving data from a wide-area information system like the World Wide Web. In the presence of failures, concurrency, and distribution, clients performing queries may observe behavior that is inconsistent with the stringent semantic requirements of mathematical sets. For example, an element retrieved and returned to the client may be subsequently deleted before the query terminates.\\Ve chose to specify formally the behavior of weak sets because we wanted to understand the varying degrees of inconsistency clients might be willing to tolerate and to understand the tradeoff between providing strong consistency guarantees and implementing weak sets efficiently. Our speci\ufb01cation assertion language uses a novel construct that lets us model reachability explicitly; with it. we can distinguish between\u00a0\u2026", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Formal Specification of AEC Product Models\n", "abstract": " This paper illustrates the use of equational specifications in developing product data models. This approach enables a precise and abstract description of products, where both syntactic and semantic checks are used for validation. Because they are formal objects, these specifications can be validated with respect to formal requirements and combined using ordinary mathematics. In addition, the availability of mature tools from the software engineering community further supports this approach to specifying and validating product models.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Adding temporal logic to Ina Jo\n", "abstract": " ABSTRACT\" Toward the overall goal of putting formal spccifications to practical use in the dcsign of largc systems, wc cxplorc thc combination of two spccification mcthods: using temporal logic to spccify concurrency propertics and using an existing specification language, Ina Jo, to spccify functional bchavior of nondcterministic systems. In this paper, we givc both informal and formal dcscriptions of both current Ina Jo and Ina Jo enhanced with temporal logic. We include dctails of a simple example to dcmonstrate the use of the proof systcm and details of an extendcd example to demonstrate the expressiveness of the enhanced language. We discuss at length our language design goals,. decisions, and thcir implications. The appcndices contain complete proofs of derived rules and theorem schemata for the enhanced f\u00f6rmal system.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "The Venari project: Goals and plans\n", "abstract": " The project is meant to be open-ended and ambitious. We intend to provide a loose framework to support interesting language and systems research at CMU for interested faculty, research staff, and graduate students. We touch upon areas in programming and specification language design, semantics, and implementation; concurrent and distributed systems; databases and persistent objects; and software development libraries and environments. We base our new ideas from our experience gained from the Avalon, Camelot, and Ergo Projects; we continue to interact with Ergo faculty, and to a lesser extent, people on the Coda and Mach Projects. This note refers to a paper on Avalon/Common Lisp and three other Venari notes, so the interested reader may wish to read these other documents for further background, motivation, and information.", "num_citations": "2\n", "authors": ["1577"]}
{"title": "Tips on the interview process\n", "abstract": " Easy\u2022 Why did you do what you did for your thesis research?\u2022 Why did you use your approach and not something else or someone else\u2019s? Know the assumptions and limitations of your approach and solution.\u2022 What\u2019s so interesting about your thesis research? What\u2019s novel about your contribution? Why should I be interested in the problem or solution?\u2022 What\u2019s the key insight to your solution? Your secret weapon?\u2022 What are the one or two most significant contributions you feel you have made to the field, to Computer Science?", "num_citations": "2\n", "authors": ["1577"]}
{"title": "The Emergence and Future of Public Health Data Science\n", "abstract": " Data science is a newly\u2010formed and, as yet, loosely\u2010defined discipline that has nonetheless emerged as a critical component of successful scientific research. We seek to provide an understanding of the term \u201cdata science,\u201d particularly as it relates to public health; to identify ways that data science methods can strengthen public health research; to propose ways to strengthen education for public health data science; and to discuss issues in data science that may benefit from a public health perspective.", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Data for Good: Ensuring the Responsible Use of Data to Benefit Society\n", "abstract": " Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Every field has data. We use data to discover new knowledge, to interpret the world, to make decisions, and even to predict the future. The recent convergence of big data, cloud computing, and novel machine learning algorithms and statistical methods is causing an explosive interest in data science and its applicability to all fields. This convergence has already enabled the automation of some tasks that better human performance. The novel capabilities we derive from data science will drive our cars, treat disease, and keep us safe. At the same time, such capabilities risk leading to biased, inappropriate, or unintended action. The design of data science solutions requires both excellence in the fundamentals of the field and expertise to develop applications which meet\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Transforming Probabilistic Programs for Model Checking\n", "abstract": " Probabilistic programming is perfectly suited to reliable and transparent data science, as it allows the user to specify their models in a high-level language without worrying about the complexities of how to fit the models. Static analysis of probabilistic programs presents even further opportunities for enabling a high-level style of programming, by automating time-consuming and error-prone tasks. We apply static analysis to probabilistic programs to automate large parts of two crucial model checking methods: Prior Predictive Checks and Simulation-Based Calibration. Our method transforms a probabilistic program specifying a density function into an efficient forward-sampling form. To achieve this transformation, we extract a factor graph from a probabilistic program using static analysis, generate a set of proposal directed acyclic graphs using a SAT solver, select a graph which will produce provably correct sampling\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Data for Good\n", "abstract": " I use the tagline\" Data for Good\" to state paronomastically how we as a community should be promoting data science, especially in training future generations of data scientists. First, we should use data science for the good of humanity and society. Data science should be used to better people's lives. Data science should be used to improve relationships among people, organizations, and institutions. Data science, in collaboration with other disciplines, should be used to help tackle societal grand challenges such as climate change, education, energy, environment, healthcare, inequality, and social justice. Second, we should use data in a good manner. The acronym FATES suggests what\" good\" means. Fairness means that the models we build are used to make unbiased decisions or predictions. Accountability means to determine and assign responsibility-to someone or to something-for a judgment made by a\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Information flow experiments\n", "abstract": " Web Data Usage Detection. Concerns about privacy have led to much interest in determining how thirdparty associates of first-party websites use information they collect about the visitors to the first-party website. Mayer and Mitchell provide a recent presentation of research that tries to determine what information these third-parties collect [6]. Others have attempted to determine what these third-parties do with the information they collect [1],[5],[16],[21]. We call this problem web data usage detection (WDUD).The researchers involved in WDUD each propose and use various analyses to determine what information is tracked and how it is used. They primarily design their analyses by intuition and do not formally present or study their analyses. Thus, questions remain:(1) Are the analyses used correct?(2) Are they related to more formal prior work?", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Formalizing and enforcing purpose restrictions in privacy policies (full version)\n", "abstract": " This technical report is the full version of a conference paper presented at the 33rd IEEE Symposium on Security and Privacy (San Francisco, May 2012) and published in the conference proceedings as \u201cFormalizing and Enforcing Purpose Restrictions in Privacy Policies\u201d[71]. The authors reported some of the material in this technical report in an earlier technical report [70].", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Towards a theory of trust in networks of humans and computers (CMU-CyLab-11-016)\n", "abstract": " We argue that a general theory of trust in networks of humans and computers must be build on both a theory of behavioral trust and a theory of computational trust. This argument is motivated by increased participation of people in social networking, crowdsourcing, human computation, and socio-economic protocols, eg, protocols modeled by trust and gift-exchange games [3, 10, 11], norms-establishing contracts [1], and scams [6, 35, 33]. User participation in these protocols relies primarily on trust, since on-line verification of protocol compliance is often impractical; eg, verification can lead to undecidable problems, co-NP complete test procedures, and user inconvenience. Trust is captured by participant preferences (ie, risk and betrayal aversion) and beliefs in the trustworthiness of other protocol participants [11, 10]. Both preferences and beliefs can be enhanced whenever protocol non-compliance leads to punishment of untrustworthy participants [11, 23]; ie, it seems natural that betrayal aversion can be decreased and belief in trustworthiness increased by properly defined punishment [1]. We argue that a general theory of trust should focus on the establishment of new trust relations where none were possible before. This focus would help create new economic opportunities by increasing the pool of usable services, removing cooperation barriers among users, and at the very least, taking advantage of \u201cnetwork effects.\u201d Hence a new theory of trust would also help focus security research in areas that promote trust-enhancement infrastructures in human and computer networks. Finally, we argue that a general theory of trust should mirror, to the\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Nsf funding advice: 21st century innovation\n", "abstract": " The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM Jeannette M. Wing shares useful suggestions for department heads. Daniel Reed discusses the importance of synergy among computing specialists and generalists.", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Preface| Journal of Computational Science-Volume 1, Issue 1\n", "abstract": " We are pleased to help inaugurate this important new Journal of Computational Science. The emergence of Computational Science as a discipline in its own right reflects the fact that computation is profoundly transforming the very conduct of science today. In addition to computing\u2019s \u201cmetal\u201d tools, such as computers, networks, storage, and software systems that are indispensable for science, it is also computing\u2019s \u201cmental\u201d tools, computational abstractions and methods, that are responsible for this transformation. Every area of inquiry, extending well beyond science and engineering into art and humanities, is deeply touched by computational thinking. Let us define what we mean by Computational Science. In the last few decades, computer simulation of numerical models has become dominant in many science and engineering disciplines. Without it, one cannot imagine designing an aircraft, forecasting a hurricane\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Confidentiality policies and their extraction from programs\n", "abstract": " We examine a well known confidentiality requirement called noninterference and argue that many systems do not meet this requirement despite maintaining the privacy of its users. We discuss a weaker requirement called incident-insensitive noninterference that captures why these systems maintain the privacy of its users while possibly not satisfying noninterference. We extend this requirement to depend on dynamic information in a novel way. Lastly, we present a method based on model checking to extract from program source code the dynamic incident-insensitive noninterference policy that the given program obeys.Descriptors:", "num_citations": "1\n", "authors": ["1577"]}
{"title": "September 1996\n", "abstract": " We survey recent progress in the development of mathematical techniques for specifying and verifying complex hardware and software systems. Many of these techniques are capable of handling industrial-sized examples; in fact, in some cases these techniques are already being used on a regular basis in industry. Success in formal specification can be attributed to notations that are accessible to system designers and to new methodologies for applying these notations effectively. Success in verification can be attributed to the development of new tools such as more powerful theorem provers and model checkers than were previously available. Finally, we suggest some general research directions that we believe are likely to lead to technological advances. Although it is difficult to predict where the future advances will come, optimism about the next generation of formal methods is justified in view of the progress\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Dynamic sets for search\n", "abstract": " The success of the proposed information highway hinges partly on solving the problem of locating useful objects out of the terabytes of data available. Therefore, a wide area data storage system (WADSS) must provide supporl for search. We view search as a directed iterative process; at each stage a query is run on a set of objects to reduce the focus to amore interesting subset. Search through a WADSS differs from search through a (distributed) database in that users are willing to trade consistency for performance. For instance, auser would be satisfied if aquery were to miss some relevant objects, include irrelevant ones, or obtain out-of-date objects aslong asthe objects are returned promptly. Moreover, queries in a WADSS maybe long-running, execute over geographically distant repositories, and may be prematurely aborted by the user. A key question is \u201cWhat is the meaning of a set in this context?\u201d This\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "A Little Bit of Logic\n", "abstract": " This chapter contains all the logic one needs to know to understand Larch.", "num_citations": "1\n", "authors": ["1577"]}
{"title": "A Status Report on Durra: A Tool for PMS-level Programming,\"\n", "abstract": " Durra is a language designed to support PMS-level programming. An application or PMS-level program is written in Durra as a set of task descriptions and type declarations that prescribes a way to manage the resources of a heterogeneous machine network. The application describes the tasks to be instantiated and executed as concurrent processes, the types of data to be exchanged by the processes, and the intermediate queues required to store the data as they move from producer to consumer processes.The application is executed under control of the runtime environment. The environment consists of a number or servers, tailored to each machine/operating system used in the network, and a scheduler that directs the initiation and termination of tasks, the transmission of data between the tasks, and the dynamic reconfiguration of the application. The scheduler performs these operations by interpreting a set of\u00a0\u2026", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Specifying recoverable objects\n", "abstract": " This paper describes the results of an exercise in writing formal specifications. The specifications capture the system-critical recoverability property of data objects that are accessed by fault-tolerant distributed programs. Recoverability is a\" non-functional\" property requiring that an object's state survives hardware failures.This exercise supports the claim that applying a rigorous specification method can greatly enhance one's understanding of software's complex behavior. The specifications enabled us to articulate precisely questions about an unstated assumption in the underlying operating system, incompleteness in the implementation of recoverable objects, implementation bias in the language design, and even incompleteness in the specifications themselves.", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Abstracts in software engineering\n", "abstract": " The Larch Project is developing tools and techniques intended to aid in the productive use of formal specifications. A major part of the project is a family of specification languages. Each Larch specification has one component written in a language derived from a programming language and another component written in a language independent of any programming language. We call the former Larch interface languages and the latter the Larch 8hated language. We have gathered together five documents about the Larch family of languages: an overview, an informal description of the Shared Language, a reference manual for the Shared Language, a handbook of specifications written in the Shared Language, and a report on using Larch/CLU, which is one of the interface languages.", "num_citations": "1\n", "authors": ["1577"]}
{"title": "Larch in Five Easy Pieces\n", "abstract": " For well over a decade, researchers have suggested that the use of formal specification techniques could play a valuable role in the development of software. Although there has been considerable progress in developing a theoretical basis for such specifications, practical experience is rather limited. This report describes the current state of a research project intended to have practical applications in the next few years.", "num_citations": "1\n", "authors": ["1577"]}