{"title": "Information contagion: An empirical study of the spread of news on digg and twitter social networks\n", "abstract": " Social networks have emerged as a critical factor in information dissemination, search, marketing, expertise and influence discovery, and potentially an important tool for mobilizing people. Social media has made social networks ubiquitous, and also given researchers access to massive quantities of data for empirical analysis. These data sets offer a rich source of evidence for studying dynamics of individual and group behavior, the structure of networks and global patterns of the flow of information on them. However, in most previous studies, the structure of the underlying networks was not directly visible but had to be inferred from the flow of information from one individual to another. As a result, we do not yet understand dynamics of information spread on networks or how the structure of the network affects it. We address this gap by analyzing data from two popular social news sites. Specifically, we extract social networks of active users on Digg and Twitter, and track how interest in news stories spreads among them. We show that social networks play a crucial role in the spread of information on these sites, and that network structure affects dynamics of information flow.", "num_citations": "1107\n", "authors": ["1193"]}
{"title": "A survey on bias and fairness in machine learning\n", "abstract": " With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI\u00a0\u2026", "num_citations": "748\n", "authors": ["1193"]}
{"title": "The DARPA Twitter bot challenge\n", "abstract": " From politicians and nation states to terrorist groups, numerous organizations reportedly conduct explicit campaigns to influence opinions on social media, posing a risk to freedom of expression. Thus, there is a need to identify and eliminate \"influence bots\" - realistic, automated identities that illicitly shape discussions on sites like Twitter and Facebook - before they get too influential.", "num_citations": "392\n", "authors": ["1193"]}
{"title": "Using a model of social dynamics to predict popularity of news\n", "abstract": " Popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both companies that host social media sites and their users. Accurate and timely prediction would enable the companies to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the ever-growing amount of content. Predicting popularity of content in social media, however, is challenging due to the complex interactions among content quality, how the social media site chooses to highlight content, and influence among users. While these factors make it difficult to predict popularity a priori, we show that stochastic models of user behavior on these sites allows predicting popularity based on early user\u00a0\u2026", "num_citations": "383\n", "authors": ["1193"]}
{"title": "Distributed online localization in sensor networks using a moving target\n", "abstract": " We describe a novel method for node localization in a sensor network where there are a fraction of reference nodes with known locations. For application-specific sensor networks, we argue that it makes sense to treat localization through online distributed learning and integrate it with an application task such as target tracking. We propose distributed online algorithm in which sensor nodes use geometric constraints induced by both radio connectivity and sensing to decrease the uncertainty of their position. The sensing constraints, which are caused by a commonly sensed moving target, are usually tighter than connectivity based constraints and lead to a decrease in average localization error over time. Different sensing models, such as radial binary detection and distance-bound estimation, are considered. First, we demonstrate our approach by studying a simple scenario in which a moving beacon broadcasts its\u00a0\u2026", "num_citations": "364\n", "authors": ["1193"]}
{"title": "Social information processing in news aggregation\n", "abstract": " Social media sites underscore the Web's transformation to a participatory medium in which users collaboratively create, evaluate, and distribute information. Innovations in social media have led to social information processing, a new paradigm for interacting with data. The social news aggregator Digg exploits social information processing for document recommendation and rating. Additionally, via mathematical modeling, it's possible to describe how collaborative document rating emerges from the independent decisions users make. Using such a model, the author reproduces observed ratings that actual stories on Digg have received.", "num_citations": "357\n", "authors": ["1193"]}
{"title": "Tracking social media discourse about the covid-19 pandemic: Development of a public coronavirus twitter data set\n", "abstract": " Background: At the time of this writing, the coronavirus disease (COVID-19) pandemic outbreak has already put tremendous strain on many countries' citizens, resources, and economies around the world. Social distancing measures, travel bans, self-quarantines, and business closures are changing the very fabric of societies worldwide. With people forced out of public spaces, much of the conversation about these phenomena now occurs online on social media platforms like Twitter.Objective: In this paper, we describe a multilingual COVID-19 Twitter data set that we are making available to the research community via our COVID-19-TweetIDs GitHub repository.Methods: We started this ongoing data collection on January 28, 2020, leveraging Twitter\u2019s streaming application programming interface (API) and Tweepy to follow certain keywords and accounts that were trending at the time data collection began. We used Twitter\u2019s search API to query for past tweets, resulting in the earliest tweets in our collection dating back to January 21, 2020.Results: Since the inception of our collection, we have actively maintained and updated our GitHub repository on a weekly basis. We have published over 123 million tweets, with over 60% of the tweets in English. This paper also presents basic statistics that show that Twitter activity responds and reacts to COVID-19-related events.Conclusions: It is our hope that our contribution will enable the study of online conversation dynamics in the context of a planetary-scale epidemic outbreak of unprecedented proportions and implications. This data set could also help track COVID-19-related misinformation and\u00a0\u2026", "num_citations": "322\n", "authors": ["1193"]}
{"title": "Analysis of dynamic task allocation in multi-robot systems\n", "abstract": " Dynamic task allocation is an essential requirement for multi-robot systems operating                 in unknown dynamic environments. It allows robots to change their behavior in                 response to environmental changes or actions of other robots in order to improve                 overall system performance. Emergent coordination algorithms for task allocation                 that use only local sensing and no direct communication between robots are                 attractive because they are robust and scalable. However, a lack of formal analysis                 tools makes emergent coordination algorithms difficult to design. In this paper we                 present a mathematical model of a general dynamic task allocation mechanism. Robots                 using this mechanism have to choose between two types of tasks, and the goal is to                 achieve a desired task division in the absence of explicit communication and global\u00a0\u2026", "num_citations": "317\n", "authors": ["1193"]}
{"title": "Mathematical model of foraging in a group of robots: Effect of interference\n", "abstract": " In multi-robot applications, such as foraging or collection tasks, interference, which results from competition for space between spatially extended robots, can significantly affect the performance of the group. We present a mathematical model of foraging in a homogeneous multi-robot system, with the goal of understanding quantitatively the effects of interference. We examine two foraging scenarios: a simplified collection task where the robots only collect objects, and a foraging task, where they find objects and deliver them to some pre-specified \u201chome\u201d location. In the first case we find that the overall group performance improves as the system size grows; however, interference causes this improvement to be sublinear, and as a result, each robot's individual performance decreases as the group size increases. We also examine the full foraging task where robots collect objects and deliver them home. We find\u00a0\u2026", "num_citations": "270\n", "authors": ["1193"]}
{"title": "Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing\n", "abstract": " Existing popular methods for semi-supervised learning with Graph Neural Networks (such as the Graph Convolutional Network) provably cannot learn a general class of neighborhood mixing relationships. To address this weakness, we propose a new model, MixHop, that can learn these relationships, including difference operators, by repeatedly mixing feature representations of neighbors at various distances. MixHop requires no additional memory or computational complexity, and outperforms on challenging baselines. In addition, we propose sparsity regularization that allows us to visualize how the network prioritizes neighborhood information across different graph datasets. Our analysis of the learned architectures reveals that neighborhood mixing varies per datasets.", "num_citations": "268\n", "authors": ["1193"]}
{"title": "Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign\n", "abstract": " Until recently, social media was seen to promote democratic discourse on social and political issues. However, this powerful communication platform has come under scrutiny for allowing hostile actors to exploit online discussions in an attempt to manipulate public opinion. A case in point is the ongoing U.S. Congress investigation of Russian interference in the 2016 U.S. election campaign, with Russia accused of, among other things, using trolls (malicious accounts created for the purpose of manipulation) and bots (automated accounts) to spread misinformation and politically biased information. In this study, we explore the effects of this manipulation campaign, taking a closer look at users who re-shared the posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation. We collected a dataset with over 43 million elections-related posts shared on Twitter between\u00a0\u2026", "num_citations": "258\n", "authors": ["1193"]}
{"title": "Social browsing on flickr\n", "abstract": " The new social media sites - blogs, wikis, del.icio.us and Flickr, among others - underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. The photo-sharing site Flickr, for example, allows users to upload photographs, view photos created by others, comment on those photos, etc. As is common to other social media sites, Flickr allows users to designate others as ``contacts'' and to track their activities in real time. The contacts (or friends) lists form the social network backbone of social media sites. We claim that these social networks facilitate new ways of interacting with information, e.g., through what we call social browsing. The contacts interface on Flickr enables users to see latest images submitted by their friends. Through an extensive analysis of Flickr data, we show that social browsing through the contacts' photo streams is one of the primary methods by which users find new images on Flickr. This finding has implications for creating personalized recommendation systems based on the user's declared contacts lists.", "num_citations": "253\n", "authors": ["1193"]}
{"title": "Wrapper maintenance: A machine learning approach\n", "abstract": " The proliferation of online information sources has led to an increased use of wrappers for extracting data from Web sources. While most of the previous research has focused on quick and efficient generation of wrappers, the development of tools for wrapper maintenance has received less attention. This is an important research problem because Web sources often change in ways that prevent the wrappers from extracting data correctly. We present an efficient algorithm that learns structural information about data from positive examples alone. We describe how this information can be used for two wrapper maintenance applications: wrapper verification and reinduction. The wrapper verification system detects when a wrapper is not extracting correct data, usually because the Web source has changed its format. The reinduction algorithm automatically recovers from changes in the Web source by identifying data on Web pages so that a new wrapper may be generated for this source. To validate our approach, we monitored 27 wrappers over a period of a year. The verification algorithm correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes, resulting in precision of 0.73 and recall of 0.95. We validated the reinduction algorithm on ten Web sources. We were able to successfully reinduce the wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data extraction task.", "num_citations": "249\n", "authors": ["1193"]}
{"title": "Semi-automatically mapping structured sources into the semantic web\n", "abstract": " Linked data continues to grow at a rapid rate, but a limitation of a lot of the data that is being published is the lack of a semantic description. There are tools, such as D2R, that allow a user to quickly convert a database into RDF, but these tools do not provide a way to easily map the data into an existing ontology. This paper presents a semi-automatic approach to map structured sources to ontologies in order to build semantic descriptions (source models). Since the precise mapping is sometimes ambiguous, we also provide a graphical user interface that allows a user to interactively refine the models. The resulting source models can then be used to convert data into RDF with respect to a given ontology or to define a SPARQL end point that can be queried with respect to an ontology. We evaluated the overall approach on a variety of sources and show that it can be used to quickly build source models with\u00a0\u2026", "num_citations": "248\n", "authors": ["1193"]}
{"title": "Accurately and reliably extracting data from the web: A machine learning approach\n", "abstract": " A critical problem in developing information agents for the Web is accessing data that is formatted for human use. We have developed a set of tools for extracting data from web sites and transforming it into a structured data format, such as XML. The resulting data can then be used to build new applications without having to deal with unstructured data. The advantages of our wrapping technology over previous work are the the ability to learn highly accurate extraction rules, to verify the wrapper to ensure that the correct data continues to be extracted, and to automatically adapt to changes in the sites from which the data is being extracted.", "num_citations": "243\n", "authors": ["1193"]}
{"title": "The simple rules of social contagion\n", "abstract": " It is commonly believed that information spreads between individuals like a pathogen, with each exposure by an informed friend potentially resulting in a naive individual becoming infected. However, empirical studies of social media suggest that individual response to repeated exposure to information is far more complex. As a proxy for intervention experiments, we compare user responses to multiple exposures on two different social media sites, Twitter and Digg. We show that the position of exposing messages on the user-interface strongly affects social contagion. Accounting for this visibility significantly simplifies the dynamics of social contagion. The likelihood an individual will spread information increases monotonically with exposure, while explicit feedback about how many friends have previously spread it increases the likelihood of a response. We provide a framework for unifying information visibility\u00a0\u2026", "num_citations": "229\n", "authors": ["1193"]}
{"title": "A review of probabilistic macroscopic models for swarm robotic systems\n", "abstract": " In this paper, we review methods used for macroscopic modeling and analyzing collective behavior of swarm robotic systems. Although the behavior of an individual robot in a swarm is often characterized by an important stochastic component, the collective behavior of swarms is statistically predictable and has often a simple probabilistic description. Indeed, we show that a class of mathematical models that describe the dynamics of collective behavior can be generated using the individual robot controller as modeling blueprint. We illustrate the macroscopic modelling methods with the help of a few sample results gathered in distributed manipulation experiments (collaborative stick pulling, foraging, aggregation). We compare the models\u2019 predictions to results of probabilistic numeric and sensor-based simulations as well as experiments with real robots. Depending on the assumptions, the metric used, and\u00a0\u2026", "num_citations": "217\n", "authors": ["1193"]}
{"title": "Using the structure of web sites for automatic segmentation of tables\n", "abstract": " Many Web sites, especially those that dynamically generate HTML pages to display the results of a user's query, present information in the form of list or tables. Current tools that allow applications to programmatically extract this information rely heavily on user input, often in the form of labeled extracted records. The sheer size and rate of growth of the Web make any solution that relies primarily on user input is infeasible in the long term. Fortunately, many Web sites contain much explicit and implicit structure, both in layout and content, that we can exploit for the purpose of information extraction. This paper describes an approach to automatic extraction and segmentation of records from Web tables. Automatic methods do not require any user input, but rely solely on the layout and content of the Web source. Our approach relies on the common structure of many Web sites, which present information as a list or a table\u00a0\u2026", "num_citations": "214\n", "authors": ["1193"]}
{"title": "Coalition formation for large-scale electronic markets\n", "abstract": " Coalition formation is a desirable behavior in a multiagent system, when a group of agents can perform a task more efficiently than any single agent can. Computational and communications complexity of traditional approaches to coalition formation, e.g., through negotiation, make them impractical for large systems. We propose an alternative, physics-motivated mechanism for coalition formation that treats agents as randomly moving, locally interacting entities. A new coalition may form when two agents encounter one another and it may grow when a single agent encounters it. Such agent-level behavior leads to a macroscopic model that describes how the number and distribution of coalitions change with time. We increase the generality and complexity of the model by letting the agents leave coalitions with some probability. The model is expressed mathematically as a series of differential equations. These\u00a0\u2026", "num_citations": "197\n", "authors": ["1193"]}
{"title": "The\" majority illusion\" in social networks\n", "abstract": " Individual\u2019s decisions, from what product to buy to whether to engage in risky behavior, often depend on the choices, behaviors, or states of other people. People, however, rarely have global knowledge of the states of others, but must estimate them from the local observations of their social contacts. Network structure can significantly distort individual\u2019s local observations. Under some conditions, a state that is globally rare in a network may be dramatically over-represented in the local neighborhoods of many individuals. This effect, which we call the \u201cmajority illusion,\u201d leads individuals to systematically overestimate the prevalence of that state, which may accelerate the spread of social contagions. We develop a statistical model that quantifies this effect and validate it with measurements in synthetic and real-world networks. We show that the illusion is exacerbated in networks with a heterogeneous degree distribution and disassortative structure.", "num_citations": "183\n", "authors": ["1193"]}
{"title": "A macroscopic analytical model of collaboration in distributed robotic systems\n", "abstract": " In this article, we present a macroscopic analytical model of collaboration in a group of reactive robots. The model consists of a series of coupled differential equations that describe the dynamics of group behavior. After presenting the general model, we analyze in detail a case study of collaboration, the stick-pulling experiment, studied experimentally and in simulation by Ijspeert et al. [Autonomous Robots, 11, 149\u2013171]. The robots' task is to pull sticks out of their holes, and it can be successfully achieved only through the collaboration of two robots. There is no explicit communication or coordination between the robots. Unlike microscopic simulations (sensor-based or using a probabilistic numerical model), in which computational time scales with the robot group size, the macroscopic model is computationally efficient, because its solutions are independent of robot group size. Analysis reproduces several\u00a0\u2026", "num_citations": "179\n", "authors": ["1193"]}
{"title": "Social networks and social information filtering on digg\n", "abstract": " The new social media sites -- blogs, wikis, Flickr and Digg, among others -- underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. Digg is a social news aggregator which allows users to submit links to, vote on and discuss news stories. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page. Digg users can designate other users as ``friends'' and easily track friends' activities: what new stories they submitted, commented on or read. The friends interface acts as a \\emph{social filtering} system, recommending to user stories his or her friends liked or found interesting. By tracking the votes received by newly submitted stories over time, we showed that social filtering is an effective information filtering approach. Specifically, we showed that (a) users tend to like stories submitted by friends and (b) users tend to like stories their friends read and liked. As a byproduct of social filtering, social networks also play a role in promoting stories to Digg's front page, potentially leading to ``tyranny of the minority'' situation where a disproportionate number of front page stories comes from the same small group of interconnected users. Despite this, social filtering is a promising new technology that can be used to personalize and tailor information to individual users: for example, through personal front pages.", "num_citations": "174\n", "authors": ["1193"]}
{"title": "How visibility and divided attention constrain social contagion\n", "abstract": " How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that dynamically decaying visibility, which is the increasing cognitive effort required for discovering and acting upon a tweet, combined with limited attention play dominant roles in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their\u00a0\u2026", "num_citations": "169\n", "authors": ["1193"]}
{"title": "Electric Elves: Applying Agent Technology to Support Human Organizations.\n", "abstract": " The operation of a human organization requires dozens of everyday tasks to ensure coherence in organizational activities, to monitor the status of such activities, to gather information relevant to the organization, to keep everyone in the organization informed, etc. Teams of software agents can aid humans in accomplishing these tasks, facilitating the organization\u2019s coherent functioning and rapid response to crises, while reducing the burden on humans. Based on this vision, this paper reports on Electric Elves, a system that has been operational, 24/7, at our research institute since June 1, 2000. Tied to individual user workstations, fax machines, voice, mobile devices such as cell phones and palm pilots, Electric Elves has assisted us in routine tasks, such as rescheduling meetings, selecting presenters for research meetings, tracking people\u2019s locations, organizing lunch meetings, etc. We discuss the underlying AI technologies that led to the success of Electric Elves, including technologies devoted to agenthuman interactions, agent coordination, accessing multiple heterogeneous information sources, dynamic assignment of organizational tasks, and deriving information about organization members. We also report the results of deploying Electric Elves in our own research organization.", "num_citations": "169\n", "authors": ["1193"]}
{"title": "Predicting influential users in online social networks\n", "abstract": " Who are the influential people in an online social network? The answer to this question depends not only on the structure of the network, but also on details of the dynamic processes occurring on it. We classify these processes as conservative and non-conservative. A random walk on a network is an example of a conservative dynamic process, while information spread is non-conservative. The influence models used to rank network nodes can be similarly classified, depending on the dynamic process they implicitly emulate. We claim that in order to correctly rank network nodes, the influence model has to match the details of the dynamic process. We study a real-world network on the social news aggregator Digg, which allows users to post and vote for news stories. We empirically define influence as the number of in-network votes a user's post generates. This influence measure, and the resulting ranking, arises entirely from the dynamics of voting on Digg, which represents non-conservative information flow. We then compare predictions of different influence models with this empirical estimate of influence. The results show that non-conservative models are better able to predict influential users on Digg. We find that normalized alpha-centrality metric turns out to be one of the best predictors of influence. We also present a simple algorithm for computing this metric and the associated mathematical formulation and analytical proofs.", "num_citations": "163\n", "authors": ["1193"]}
{"title": "Top-down vs bottom-up methodologies in multi-agent system design\n", "abstract": " Traditionally, two alternative design approaches have been available to engineers: top-down and bottom-up. In the top-down approach, the design process starts with specifying the global system state and assuming that each component has global knowledge of the system, as in a centralized approach. The solution is then decentralized by replacing global knowledge with communication. In the bottom-up approach, on the other hand, the design starts with specifying requirements and capabilities of individual components, and the global behavior is said to emerge out of interactions among constituent components and between components and the environment. In this paper we present a comparative study of both approaches with particular emphasis on applications to multi-agent system engineering and robotics. We outline the generic characteristics of both approaches from the MAS perspective, and\u00a0\u2026", "num_citations": "153\n", "authors": ["1193"]}
{"title": "Covid-19: The first public coronavirus twitter dataset\n", "abstract": " At the time of this writing, the novel coronavirus (COVID-19) pandemic outbreak has already put tremendous strain on many countries' citizens, resources and economies around the world. Social distancing measures, travel bans, self-quarantines, and business closures are changing the very fabric of societies worldwide. With people forced out of public spaces, much conversation about these phenomena now occurs online, eg, on social media platforms like Twitter. In this paper, we describe a multilingual coronavirus (COVID-19) Twitter dataset that we have been continuously collecting since January 22, 2020. We are making our dataset available to the research community (https://github. com/echen102/COVID-19-TweetIDs). It is our hope that our contribution will enable the study of online conversation dynamics in the context of a planetary-scale epidemic outbreak of unprecedented proportions and implications. This dataset could also help track scientific coronavirus misinformation and unverified rumors, or enable the understanding of fear and panic---and undoubtedly more. Ultimately, this dataset may contribute towards enabling informed solutions and prescribing targeted policy interventions to fight this global crisis.", "num_citations": "152\n", "authors": ["1193"]}
{"title": "Analysis of social voting patterns on digg\n", "abstract": " The social Web is transforming the way information is created and distributed. Blog authoring tools enable users to publish content, while sites such as Digg and Del. icio. us are used to distribute content to a wider audience. With content fast becoming a commodity, interest in using social networks to promote and find content has grown, both on the side of content producers (viral marketing) and consumers (recommendation). Here we study the role of social networks in promoting content on Digg, a social news aggregator that allows users to submit links to and vote on news stories. Digg's goal is to feature the most interesting stories on its front page, and it aggregates opinions of its many users to identify them. Like other social networking sites, Digg allows users to designate other users as\" friends\" and see what stories they found interesting. We studied the spread of interest in news stories submitted to Digg in\u00a0\u2026", "num_citations": "149\n", "authors": ["1193"]}
{"title": "Resource allocation in the grid using reinforcement learning\n", "abstract": " In this paper we study a minimalist decentralized algorithm for resource allocation in a simplified Grid-like environment. We consider a system consisting of large number of heterogenous reinforcement learning agents that share common resources for their computational needs. There is no communication between the agents: the only information that agents receive is the (expected) completion time of a job it submitted to a particular resource and which serves as a reinforcement signal for the agent. The results of our experiments suggest that reinforcement learning can be used to improve the quality of resource allocation in large scale heterogenous system.", "num_citations": "146\n", "authors": ["1193"]}
{"title": "Social dynamics of digg\n", "abstract": " Online social media provide multiple ways to find interesting content. One important method is highlighting content recommended by user\u2019s friends. We examine this process on one such site, the news aggregator Digg. With a stochastic model of user behavior, we distinguish the effects of the content visibility and interestingness to users. We find a wide range of interest and distinguish stories primarily of interest to a users\u2019 friends from those of interest to the entire user community. We show how this model predicts a story\u2019s eventual popularity from users\u2019 early reactions to it, and estimate the prediction reliability. This modeling framework can help evaluate alternative design choices for displaying content on the site.", "num_citations": "140\n", "authors": ["1193"]}
{"title": "Automatic data extraction from lists and tables in web sources\n", "abstract": " We describe a technique for extracting data from lists and tables and grouping it by rows and columns. This is done completely automatically, using only some very general assumptions about the structure of the list. We have developed a suite of unsupervised learning algorithms that induce the structure of lists by exploiting the regularities both in the format of the pages and the data contained in them. Among the tools used are AutoClass for automatic classification of data and grammar induction of regular languages. The approach was tested on 14 Web sources providing diverse data types, and we found that for 10 of these sources we were able to correctly find lists and partition the data into columns and rows.", "num_citations": "128\n", "authors": ["1193"]}
{"title": "Centrality metric for dynamic networks\n", "abstract": " Centrality is an important notion in network analysis and is used to measure the degree to which network structure contributes to the importance of a node in a network. While many different centrality measures exist, most of them apply to static networks. Most networks, on the other hand, are dynamic in nature, evolving over time through the addition or deletion of nodes and edges. A popular approach to analyzing such networks represents them by a static network that aggregates all edges observed over some time period. This approach, however, under or overestimates centrality of some nodes. We address this problem by introducing a novel centrality metric for dynamic network analysis. This metric exploits an intuition that in order for one node in a dynamic network to influence another over some period of time, there must exist a path that connects the source and destination nodes through intermediaries at\u00a0\u2026", "num_citations": "109\n", "authors": ["1193"]}
{"title": "A general methodology for mathematical analysis of multi-agent systems\n", "abstract": " We propose a general mathematical methodology for studying the dynamics of multiagent systems in which complex collective behavior arises out of local interactions between many simple agents. The mathematical model is composed of a system of coupled differential equations describing the macroscopic, or collective, dynamics of an agent-based system. We illustrate our approach by applying it to analyze several agent-based systems, including coalition formation in an electronic marketplace, and foraging and collaboration in a group of robots.", "num_citations": "109\n", "authors": ["1193"]}
{"title": "Friendship paradox redux: Your friends are more interesting than you\n", "abstract": " Feld's friendship paradox states that\" your friends have more friends than you, on average.\" This paradox arises because extremely popular people, despite being rare, are overrepresented when averaging over friends. Using a sample of the Twitter firehose, we confirm that the friendship paradox holds for> 98% of Twitter users. Because of the directed nature of the follower graph on Twitter, we are further able to confirm more detailed forms of the friendship paradox: everyone you follow or who follows you has more friends and followers than you. This is likely caused by a correlation we demonstrate between Twitter activity, number of friends, and number of followers. In addition, we discover two new paradoxes: the virality paradox that states your friends receive more viral content than you, on average, and the activity paradox, which states your friends are more active than you, on average.\" The latter paradox is important in regulating online communication. It may result in users having difficulty maintaining optimal incoming information rates, because following additional users causes the volume of incoming tweets to increase super-linearly. While users may compensate for increased information flow by increasing their own activity, users become information overloaded when they receive more information than they are able or willing to process. We compare the average size of cascades that are sent and received by overloaded and underloaded users. And we show that overloaded users post and receive larger cascades and they are poor detector of small cascades.", "num_citations": "108\n", "authors": ["1193"]}
{"title": "Leveraging position bias to improve peer recommendation\n", "abstract": " With the advent of social media and peer production, the amount of new online content has grown dramatically. To identify interesting items in the vast stream of new content, providers must rely on peer recommendation to aggregate opinions of their many users. Due to human cognitive biases, the presentation order strongly affects how people allocate attention to the available content. Moreover, we can manipulate attention through the presentation order of items to change the way peer recommendation works. We experimentally evaluate this effect using Amazon Mechanical Turk. We find that different policies for ordering content can steer user attention so as to improve the outcomes of peer recommendation.", "num_citations": "107\n", "authors": ["1193"]}
{"title": "Entropy-based classification of'retweeting'activity on twitter\n", "abstract": " Twitter is used for a variety of reasons, including information dissemination, marketing, political organizing and to spread propaganda, spamming, promotion, conversations, and so on. Characterizing these activities and categorizing associated user generated content is a challenging task. We present a information-theoretic approach to classification of user activity on Twitter. We focus on tweets that contain embedded URLs and study their collective `retweeting' dynamics. We identify two features, time-interval and user entropy, which we use to classify retweeting activity. We achieve good separation of different activities using just these two features and are able to categorize content based on the collective user response it generates. We have identified five distinct categories of retweeting activity on Twitter: automatic/robotic activity, newsworthy information dissemination, advertising and promotion, campaigns, and parasitic advertisement. In the course of our investigations, we have shown how Twitter can be exploited for promotional and spam-like activities. The content-independent, entropy-based activity classification method is computationally efficient, scalable and robust to sampling and missing data. It has many applications, including automatic spam-detection, trend identification, trust management, user-modeling, social search and content classification on online social media.", "num_citations": "105\n", "authors": ["1193"]}
{"title": "Electric Elves: Agent technology for supporting human organizations\n", "abstract": " The operation of a human organization requires dozens of everyday tasks to ensure coherence in organizational activities, monitor the status of such activities, gather information relevant to the organization, keep everyone in the organization informed, and so on. Teams of software agents can aid humans in accomplishing these tasks, facilitating the organization's coherent functioning and rapid response to crises and reducing the burden on humans. Based on this vision, this article reports on ELECTRIC ELVES, a system that has been operational 24 hours a day, 7 days a week at our research institute since 1 June 2000. Tied to individual user workstations, fax machines, voice, and mobile devices such as cell phones and palm pilots, ELECTRIC ELVES has assisted us in routine tasks, such as rescheduling meetings, selecting presenters for research meetings, tracking people's locations, organizing lunch meetings, and so on. We discuss the underlying AI technologies that led to the success of ELECTRIC ELVES, including technologies devoted to agent-human interactions, agent coordination, the accessing of multiple heterogeneous information sources, dynamic assignment of organizational tasks, and the deriving of information about organization members. We also report the results of deploying ELECTRIC ELVES in our own research organization.", "num_citations": "105\n", "authors": ["1193"]}
{"title": "Social contagion: An empirical study of information spread on Digg and Twitter follower graphs\n", "abstract": " Social networks have emerged as a critical factor in information dissemination, search, marketing, expertise and influence discovery, and potentially an important tool for mobilizing people. Social media has made social networks ubiquitous, and also given researchers access to massive quantities of data for empirical analysis. These data sets offer a rich source of evidence for studying dynamics of individual and group behavior, the structure of networks and global patterns of the flow of information on them. However, in most previous studies, the structure of the underlying networks was not directly visible but had to be inferred from the flow of information from one individual to another. As a result, we do not yet understand dynamics of information spread on networks or how the structure of the network affects it. We address this gap by analyzing data from two popular social news sites. Specifically, we extract follower graphs of active Digg and Twitter users and track how interest in news stories cascades through the graph. We compare and contrast properties of information cascades on both sites and elucidate what they tell us about dynamics of information flow on networks.", "num_citations": "102\n", "authors": ["1193"]}
{"title": "User participation in social media: Digg study\n", "abstract": " The social news aggregator Digg allows users to submit and moderate stories by voting on (digging) them. As is true of most social sites, user participation on Digg is non-uniformly distributed, with few users contributing a disproportionate fraction of content. We studied user participation on Digg, to see whether it is motivated by competition, fueled by user ranking, or social factors, such as community acceptance. For our study we collected activity data of the top users weekly over the course of a year. We computed the number of stories users submitted, dugg or commented on weekly. We report a spike in user activity in September 2006, followed by a gradual decline, which seems unaffected by the elimination of user ranking. The spike can be explained by a controversy that broke out at the beginning of September 2006. We believe that the lasting acrimony that this incident has created led to a decline of top user\u00a0\u2026", "num_citations": "97\n", "authors": ["1193"]}
{"title": "Constructing folksonomies from user-specified relations on flickr\n", "abstract": " Automatic folksonomy construction from tags has attracted much attention recently. However, inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts. Instead of tags we propose to use user-specified relations for learning folksonomy. We explore two statistical frameworks for aggregating many shallow individual hierarchies, expressed through the collection/set relations on the social photosharing site Flickr, into a common deeper folksonomy that reflects how a community organizes knowledge. Our approach addresses a number of challenges that arise while aggregating information from diverse users, namely noisy vocabulary, and variations in the granularity level of the concepts expressed. Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference\u00a0\u2026", "num_citations": "81\n", "authors": ["1193"]}
{"title": "Learning data prototypes for information extraction\n", "abstract": " A method for determining statistically significant token sequences lends itself for use in the recognition of broken wrappers as well as the construction of new wrapper rules. When new wrapper rules are needed as the underlying wrapped data has changed, training examples are used to recognized data rule candidates that are culled with a bias for rule candidates that would be probably more successful. The resulting rule candidate set is clustered according to feature characteristics, then compared to the training examples. Those rule candidates most similar to the training examples are used to create new wrapper rules.", "num_citations": "81\n", "authors": ["1193"]}
{"title": "Experiments on three systems with non-variational aspects\n", "abstract": " We present recent experimental results for three pattern-forming systems in which non-variational effects play an important role.The first is thermal convection in a shallow horizontal layer of fluid with temperature-dependent properties. In this system, a hexagonal lattice of convection cells forms at onset. This lattice becomes unstable to rolls when the temperature difference is increased sufficiently. In the \u201croll\u201d state, the roll are curved and the system forms stable rotating spirals. The rotating spiral states are associated with non-variational effects.Secondly, we discuss the formation of localized pulses in binary-mixture convection near onset. These pulses would not exist in a potential system. In narrow channels, they have been observed as stable states. In systems which are spatially extended in two dimensions they can form spontaneously, and can be long-lived.The third topic which we discuss is the K\u00fcppers-Lortz\u00a0\u2026", "num_citations": "81\n", "authors": ["1193"]}
{"title": "Analyzing Uber's ride-sharing economy\n", "abstract": " Uber is a popular ride-sharing application that matches people who need a ride (or riders) with drivers who are willing to provide it using their personal vehicles. Despite its growing popularity, there exist few studies that examine large-scale Uber data, or in general the factors affecting user participation in the sharing economy. We address this gap through a study of the Uber market that analyzes large-scale data covering 59 million rides which spans a period of 7 months. The data were extracted from email receipts sent by Uber collected on Yahoo servers, allowing us to examine the role of demographics (eg, age and gender) on participation in the ride-sharing economy. In addition, we evaluate the impact of dynamic pricing (ie, surge pricing) and income on both rider and driver behavior. We find that the surge pricing does not bias Uber use towards higher income riders. Moreover, we show that more homophilous\u00a0\u2026", "num_citations": "79\n", "authors": ["1193"]}
{"title": "Personalizing image search results on flickr\n", "abstract": " The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, eg,\u201cbeetle\u201d could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user\u2019s contacts or a larger social network that includes those contact\u2019s contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users\u2019 interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "num_citations": "79\n", "authors": ["1193"]}
{"title": "Political polarization drives online conversations about COVID\u201019 in the United States\n", "abstract": " Since the outbreak in China in late 2019, the novel coronavirus (COVID\u201019) has spread around the world and has come to dominate online conversations. By linking 2.3 million Twitter users to locations within the United States, we study in aggregate how political characteristics of the locations affect the evolution of online discussions about COVID\u201019. We show that COVID\u201019 chatter in the United States is largely shaped by political polarization. Partisanship correlates with sentiment toward government measures and the tendency to share health and prevention messaging. Cross\u2010ideological interactions are modulated by user segregation and polarized network structure. We also observe a correlation between user engagement with topics related to public health and the varying impact of the disease outbreak in different U.S. states. These findings may help inform policies both online and offline. Decision\u2010makers\u00a0\u2026", "num_citations": "76\n", "authors": ["1193"]}
{"title": "Tripartite graph clustering for dynamic sentiment analysis on social media\n", "abstract": " The growing popularity of social media (eg, Twitter) allows users to easily share information with each other and influence others by expressing their own sentiments on various subjects. In this work, we propose an unsupervised tri-clustering framework, which analyzes both user-level and tweet-level sentiments through co-clustering of a tripartite graph. A compelling feature of the proposed framework is that the quality of sentiment clustering of tweets, users, and features can be mutually improved by joint clustering. We further investigate the evolution of user-level sentiments and latent feature vectors in an online framework and devise an efficient online algorithm to sequentially update the clustering of tweets, users and features with newly arrived data. The online framework not only provides better quality of both dynamic user-level and tweet-level sentiment analysis, but also improves the computational and storage\u00a0\u2026", "num_citations": "76\n", "authors": ["1193"]}
{"title": "Resource allocation games with changing resource capacities\n", "abstract": " In this paper we study a class of resource allocation games which are inspired by the El Farol Bar problem. We consider a system of competitive agents that have to choose between several resources characterized by their time dependent capacities. The agents using a particular resource are rewarded if their number does not exceed the resource capacity, and punished otherwise. Agents use a set of strategies to decide what resource to choose, and use a simple reinforcement learning scheme to update the accuracy of strategies. A strategy in our model is simply a lookup table that suggests to an agent what resource to choose based on the actions of its neighbors at the previous time step. In other words, the agents form a social network whose connectivity controls the average number of neighbors with whom each agent interacts. This statement of the adaptive resource allocation problem allows us to fully\u00a0\u2026", "num_citations": "75\n", "authors": ["1193"]}
{"title": "Portrait of an online shopper: Understanding and predicting consumer behavior\n", "abstract": " Consumer spending accounts for a large fraction of economic footprint of modern countries. Increasingly, consumer activity is moving to the web, where digital receipts of online purchases provide valuable data sources detailing consumer behavior. We consider such data extracted from emails and combined with with consumers' demographic information, which we use to characterize, model, and predict purchasing behavior. We analyze such behavior of consumers in different age and gender groups, and find interesting, actionable patterns that can be used to improve ad targeting systems. For example, we found that the amount of money spent on online purchases grows sharply with age, peaking in the late 30s, while shoppers from wealthy areas tend to purchase more expensive items and buy them more frequently. Furthermore, we look at the influence of social connections on purchasing habits, as well as at\u00a0\u2026", "num_citations": "74\n", "authors": ["1193"]}
{"title": "Resource allocation in the grid with learning agents\n", "abstract": " One of the main challenges in Grid computing is efficient allocation of resources (CPU \u2013 hours, network bandwidth, etc.) to the tasks submitted by users. Due to the lack of centralized control and the dynamic/stochastic nature of resource availability, any successful allocation mechanism should be highly distributed and robust to the changes in the Grid environment. Moreover, it is desirable to have an allocation mechanism that does not rely on the availability of coherent global information. In this paper we examine a simple algorithm for distributed resource allocation in a simplified Grid-like environment that meets the above requirements. Our system consists of a large number of heterogenous reinforcement learning agents that share common resources for their computational needs. There is no explicit communication or interaction between the agents: the only information that agents receive is the expected\u00a0\u2026", "num_citations": "74\n", "authors": ["1193"]}
{"title": "The role of social media in the discussion of controversial topics\n", "abstract": " In recent years, social media has revolutionized how people communicate and share information. Twitter and other blogging sites have seen an increase in political and social activism. Previous studies on the behaviors of users in politics have focused on electoral candidates and election results. Our paper investigates the role of social media in discussing and debating controversial topics. We apply sentiment analysis techniques to classify the position (for, against, neutral) expressed in a tweet about a controversial topic and use the results in our study of user behavior. Our findings suggest that Twitter is primarily used for spreading information to like-minded people rather than debating issues. Users are quicker to rebroadcast information than to address a communication by another user. Individuals typically take a position on an issue prior to posting about it and are not likely to change their tweeting opinion.", "num_citations": "72\n", "authors": ["1193"]}
{"title": "Modeling and mathematical analysis of swarms of microscopic robots\n", "abstract": " The biologically-inspired swarm paradigm is being used to design self-organizing systems of locally interacting artificial agents. A major difficulty in designing swarms with desired characteristics is understanding the causal relation between individual agent and collective behaviors. Mathematical analysis of swarm dynamics can address this difficulty to gain insight into system design. This paper proposes a framework for mathematical modeling of swarms of microscopic robots that may one day be useful in medical applications. While such devices do not yet exist, the modeling approach can be helpful in identifying various design trade-offs for the robots and be a useful guide for their eventual fabrication. Specifically, we examine microscopic robots that reside in a fluid, for example, a bloodstream, and are able to detect and respond to different chemicals. We present the general mathematical model of a scenario in\u00a0\u2026", "num_citations": "70\n", "authors": ["1193"]}
{"title": "Community detection using a measure of global influence\n", "abstract": " The growing popularity of online social networks gave researchers access to large amount of network data and renewed interest in methods for automatic community detection. Existing algorithms, including the popular modularity-optimization methods, look for regions of the network that are better connected internally, e.g., have higher than expected number of edges within them. We believe, however, that edges do not give the true measure of network connectivity. Instead, we argue that influence, which we define as the number of paths, of any length, that exist between two nodes, gives a better measure of network connectivity. We use the influence metric to partition a network into groups or communities by looking for regions of the network where nodes have more influence over each other than over nodes outside the community. We evaluate our approach on several networks and show that it often\u00a0\u2026", "num_citations": "69\n", "authors": ["1193"]}
{"title": "Evolution of conversations in the age of email overload\n", "abstract": " Email is a ubiquitous communications tool in the workplace and plays an important role in social interactions. Previous studies of email were largely based on surveys and limited to relatively small populations of email users within organizations. In this paper, we report results of a large-scale study of more than 2 million users exchanging 16 billion emails over several months. We quantitatively characterize the replying behavior in conversations within pairs of users. In particular, we study the time it takes the user to reply to a received message and the length of the reply sent. We consider a variety of factors that affect the reply time and length, such as the stage of the conversation, user demographics, and use of portable devices. In addition, we study how increasing load affects emailing behavior. We find that as users receive more email messages in a day, they reply to a smaller fraction of them, using shorter replies\u00a0\u2026", "num_citations": "68\n", "authors": ["1193"]}
{"title": "Transient localized states in 2d binary liquid convection\n", "abstract": " We report an experimental study of the onset of convection in ethanol/water mixtures confined in a circular cell of radial aspect ratio of 11.6. The initial bifurcation was to radial traveling waves; however, the linear state typically gave way to a nonlinear one in which convection alternately focused along one cell diameter and then another roughly perpendicular to the first. After a number of cycles, this state frequently contracted radially to a localized pulse of traveling-wave convection very similar to those observed in 1D geometries. The pulses we observed either decayed back to pure conduction or grew to fill or nearly fill the cell.", "num_citations": "68\n", "authors": ["1193"]}
{"title": "A framework for quantitative analysis of cascades on networks\n", "abstract": " How does information flow in online social networks? How does the structure and size of the information cascade evolve in time? How can we efficiently mine the information contained in cascade dynamics? We approach these questions empirically and present an efficient and scalable mathematical framework for quantitative analysis of cascades on networks. We define a cascade generating function that captures the details of the microscopic dynamics of the cascades. We show that this function can also be used to compute the macroscopic properties of cascades, such as their size, spread, diameter, number of paths, and average path length. We present an algorithm to efficiently compute cascade generating function and demonstrate that while significantly compressing information within a cascade, it nevertheless allows us to accurately reconstruct its structure. We use this framework to study information\u00a0\u2026", "num_citations": "67\n", "authors": ["1193"]}
{"title": "Early warnings of cyber threats in online discussions\n", "abstract": " We introduce a system for automatically generating warnings of imminent or current cyber-threats. Our system leverages the communication of malicious actors on the darkweb, as well as activity of cyber security experts on social media platforms like Twitter. In a time period between September, 2016 and January, 2017, our method generated 661 alerts of which about 84% were relevant to current or imminent cyber-threats. In the paper, we first illustrate the rationale and workflow of our system, then we measure its performance. Our analysis is enriched by two case studies: the first shows how the method could predict DDoS attacks, and how it would have allowed organizations to prepare for the Mirai attacks that caused widespread disruption in October 2016. Second, we discuss the method's timely identification of various instances of data breaches.", "num_citations": "66\n", "authors": ["1193"]}
{"title": "Stochastic models of user-contributory web sites\n", "abstract": " We describe a general stochastic processes-based approach to modeling user-contributory web sites, where users create, rate and share content. These models describe aggregate measures of activity and how they arise from simple models of individual users. This approach provides a tractable method to understand user activity on the web site and how this activity depends on web site design choices, especially the choice of what information about other users' behaviors is shown to each user. We illustrate this modeling approach in the context of user-created content on the news rating site Digg.", "num_citations": "64\n", "authors": ["1193"]}
{"title": "Information is not a virus, and other consequences of human cognitive limits\n", "abstract": " The many decisions that people make about what to pay attention to online shape the spread of information in online social networks. Due to the constraints of available time and cognitive resources, the ease of discovery strongly impacts how people allocate their attention to social media content. As a consequence, the position of information in an individual\u2019s social feed, as well as explicit social signals about its popularity, determine whether it will be seen, and the likelihood that it will be shared with followers. Accounting for these cognitive limits simplifies mechanics of information diffusion in online social networks and explains puzzling empirical observations:(i) information generally fails to spread in social media and (ii) highly connected people are less likely to re-share information. Studies of information diffusion on different social media platforms reviewed here suggest that the interplay between human cognitive limits and network structure differentiates the spread of information from other social contagions, such as the spread of a virus through a population. View Full-Text", "num_citations": "63\n", "authors": ["1193"]}
{"title": "Parameterized centrality metric for network analysis\n", "abstract": " A variety of metrics have been proposed to measure the relative importance of nodes in a network. One of these, alpha-centrality [P. Bonacich, Am. J. Sociol. 92, 1170 (1987)], measures the number of attenuated paths that exist between nodes. We introduce a normalized version of this metric and use it to study network structure, for example, to rank nodes and find community structure of the network. Specifically, we extend the modularity-maximization method for community detection to use this metric as the measure of node connectivity. Normalized alpha-centrality is a powerful tool for network analysis, since it contains a tunable parameter that sets the length scale of interactions. Studying how rankings and discovered communities change when this parameter is varied allows us to identify locally and globally important nodes and structures. We apply the proposed metric to several benchmark networks and show\u00a0\u2026", "num_citations": "61\n", "authors": ["1193"]}
{"title": "Learning the common structure of data\n", "abstract": " The proliferation of online information sources has accentuated the need for tools that automatically validate and recognize data. We present an efficient algorithm that learns structural information about data from positive examples alone. We describe two Web wrapper maintenance applications that employ this algorithm. The first application detects when a wrapper is not extracting correct data. The second application automatically identifies data on Web pages so that the wrapper may be reinduced when the source format changes.", "num_citations": "61\n", "authors": ["1193"]}
{"title": "Who falls for online political manipulation?\n", "abstract": " Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes\u00a0\u2026", "num_citations": "59\n", "authors": ["1193"]}
{"title": "Growing a tree in the forest: Constructing folksonomies by integrating structured metadata\n", "abstract": " Many social Web sites allow users to annotate the content with descriptive metadata, such as tags, and more recently to organize content hierarchically. These types of structured metadata provide valuable evidence for learning how a community organizes knowledge. For instance, we can aggregate many personal hierarchies into a common taxonomy, also known as a folksonomy, that will aid users in visualizing and browsing social content, and also to help them in organizing their own content. However, learning from social metadata presents several challenges, since it is sparse, shallow, ambiguous, noisy, and inconsistent. We describe an approach to folksonomy learning based on relational clustering, which exploits structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree\u00a0\u2026", "num_citations": "59\n", "authors": ["1193"]}
{"title": "Characterizing the 2016 Russian IRA influence campaign\n", "abstract": " Until recently, social media were seen to promote democratic discourse on social and political issues. However, this powerful communication ecosystem has come under scrutiny for allowing hostile actors to exploit online discussions in an attempt to manipulate public opinion. A case in point is the ongoing U.S. Congress investigation of Russian interference in the 2016 U.S. election campaign, with Russia accused of, among other things, using trolls (malicious accounts created for the purpose of manipulation) and bots (automated accounts) to spread propaganda and politically biased information. In this study, we explore the effects of this manipulation campaign, taking a closer look at users who re-shared the posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation. We collected a dataset of 13 million election-related posts shared on Twitter in the year of\u00a0\u2026", "num_citations": "58\n", "authors": ["1193"]}
{"title": "Pragmatic evaluation of folksonomies\n", "abstract": " Recently, a number of algorithms have been proposed to obtain hierarchical structures-so-called folksonomies-from social tagging data. Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as:(a) Navigating social tagging systems and (b) Acquiring semantic relationships between tags. While the promises and pitfalls of the latter have been studied to some extent, we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems. This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies. We model exploratory navigation of a tagging system as decentralized search on a network of tags. Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used. The key idea of our\u00a0\u2026", "num_citations": "58\n", "authors": ["1193"]}
{"title": "DarkEmbed: Exploit Prediction With Neural Language Models.\n", "abstract": " Software vulnerabilities can expose computer systems to attacks by malicious actors. With the number of vulnerabilities discovered in the recent years surging, creating timely patches for every vulnerability is not always feasible. At the same time, not every vulnerability will be exploited by attackers; hence, prioritizing vulnerabilities by assessing the likelihood they will be exploited has become an important research problem. Recent works used machine learning techniques to predict exploited vulnerabilities by analyzing discussions about vulnerabilities on social media. These methods relied on traditional text processing techniques, which represent statistical features of words, but fail to capture their context. To address this challenge, we propose DarkEmbed, a neural language modeling approach that learns low dimensional distributed representations, ie, embeddings, of darkweb/deepweb discussions to predict whether vulnerabilities will be exploited. By capturing linguistic regularities of human language, such as syntactic, semantic similarity and logic analogy, the learned embeddings are better able to classify discussions about exploited vulnerabilities than traditional text analysis methods. Evaluations demonstrate the efficacy of learned embeddings on both structured text (such as security blog posts) and unstructured text (darkweb/deepweb posts). DarkEmbed outperforms state-of-the-art approaches on the exploit prediction task with an F1-score of 0.74.", "num_citations": "54\n", "authors": ["1193"]}
{"title": "Rethinking centrality: the role of dynamical processes in social network analysis\n", "abstract": " Many popular measures used in social network analysis, including centrality, are based on the random walk. The random walk is a model of a stochastic process where a node interacts with one other node at a time. However, the random walk may not be appropriate for modeling social phenomena, including epidemics and information diffusion, in which one node may interact with many others at the same time, for example, by broadcasting the virus or information to its neighbors. To produce meaningful results, social network analysis algorithms have to take into account the nature of interactions between the nodes. In this paper we classify dynamical processes as conservative and non-conservative and relate them to well-known measures of centrality used in network analysis: PageRank and Alpha-Centrality. We demonstrate, by ranking users in online social networks used for broadcasting information, that non-conservative Alpha-Centrality generally leads to a better agreement with an empirical ranking scheme than the conservative PageRank.", "num_citations": "54\n", "authors": ["1193"]}
{"title": "Automatically labeling the inputs and outputs of web services\n", "abstract": " Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem: namely, automatically recognizing semantic types of the data used by Web services. We describe a metadatabased classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier\u2019s predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.", "num_citations": "54\n", "authors": ["1193"]}
{"title": "Using lists to measure homophily on twitter\n", "abstract": " Homophily is the tendency of individuals in a social system to link to others who are similar to them and understanding homophily can help us build better user models for personalization and recommender systems. Many studies have verified homophily along demographic dimensions, such as age, location, occupation, etc., not only in real-world social networks but also online. However, there is limited research showing that homophily also exists when similarity is judged by topics of expertise or interests. We demonstrate the existence of topical homophily on Twitter using a novel source of evidence provided by Twitter lists. In this paper, we use LDA to extract topics from Twitter lists (a collection of user accounts created by some user that others can follow) and measure similarity between listed users based on the learned topics. We show that topically similar users are more likely to be linked via a follow relationship than less similar users.", "num_citations": "51\n", "authors": ["1193"]}
{"title": "Exploiting social annotation for automatic resource discovery\n", "abstract": " Information integration applications, such as mediators or mashups, that require access to information resources currently rely on users manually discovering and integrating them in the application. Manual resource discovery is a slow process, requiring the user to sift through results obtained via keyword-based search. Although search methods have advanced to include evidence from document contents, its metadata and the contents and link structure of the referring pages, they still do not adequately cover information sources\u2014often called \u201cthe hidden Web\u201d\u2014that dynamically generate documents in response to a query. The recently popular social bookmarking sites, which allow users to annotate and share metadata about various information sources, provide rich evidence for resource discovery. In this paper, we describe a probabilistic model of the user annotation process in a social bookmarking system del. icio. us. We then use the model to automatically find resources relevant to a particular information domain. Our experimental results on data obtained from del. icio. us show this approach as a promising method for helping automate the resource discovery task.", "num_citations": "50\n", "authors": ["1193"]}
{"title": "Adaptive Boolean networks and minority games with time-dependent capacities\n", "abstract": " In this paper we consider a network of Boolean agents that compete for a limited resource. The agents play the so called generalized minority game where the capacity level is allowed to vary externally. We study the properties of such a system for different values of the mean connectivity K of the network, and show that the system with K= 2 shows a high degree of coordination for relatively large variations of the capacity level.", "num_citations": "46\n", "authors": ["1193"]}
{"title": "Discover: Mining online chatter for emerging cyber threats\n", "abstract": " Widespread adoption of networking technologies has brought about tremendous economic and social growth, but also exposed individuals and organization to new threats from malicious cyber actors. Recent attacks by WannaCry and NotPetya ransomware crypto-worms, infected hundreds of thousands of computer systems world wide, compromising data and critical infrastructure. In order to limit their impact, it is, therefore, critical to detect---and even predict---cyber attacks before they spread. Here, we introduce DISCOVER, an early cyber threat warning system, that mines online chatter from cyber actors on social media, security blogs, and darkweb forums, to identify words that signal potential cyber attacks. We evaluate DISCOVER and find that it can identify terms related to emerging cyber threats with precision above $80% $. DISCOVER also generates a time line of related online discussions on different Web\u00a0\u2026", "num_citations": "45\n", "authors": ["1193"]}
{"title": "Different convection dynamics in mixtures with the same separation ratio\n", "abstract": " We report on convection near threshold in circular samples of radial aspect ratio \u0393= 11.5 using ethanol-water mixtures with two concentrations x, but with the same separation ratio \u03a8\u2243\u2212 0.08. For x= 0.250, convection begins with transients that lead to a time-independent cell-filling state, typically in a day. For x= 0.011, there is a range of temperature differences where repeated transients and aperiodically fluctuating localized regions of disordered convection persist for many days. In rare cases these lead to an apparently stable\" wall\" state of traveling waves surrounding pure conduction.", "num_citations": "42\n", "authors": ["1193"]}
{"title": "Linguistic cues to deception: Identifying political trolls on social media\n", "abstract": " The ease with which information can be shared on social media has opened it up to abuse and manipulation. One example of a manipulation campaign that has garnered much attention recently was the alleged Russian interference in the 2016 US elections, with Russia accused of, among other things, using trolls and malicious accounts to spread misinformation and politically biased information. To take an in-depth look at this manipulation campaign, we collected a dataset of 13 million election-related posts shared on Twitter in 2016 by over a million distinct users. This dataset includes accounts associated with the identified Russian trolls as well as users sharing posts in the same time period on a variety of topics around the 2016 elections. To study how these trolls attempted to manipulate public opinion, we identified 49 theoretically grounded linguistic markers of deception and measured their use by troll and non-troll accounts. We show that deceptive language cues can help to accurately identify trolls, with average F1 score of 82% and recall 88%.", "num_citations": "41\n", "authors": ["1193"]}
{"title": "Time-aware ranking in dynamic citation networks\n", "abstract": " Many algorithms have been developed to identify important nodes in a complex network, including various centrality metrics and Page Rank, but most fail to consider the dynamic nature of the network. They therefore suffer from recency bias and fail to recognize important new nodes that have not had as much time to accumulate links as their older counterparts. This paper describes the Effective Contagion Matrix (ECM), a solution to address recency bias in the analysis of dynamic complex networks. The idea of ECM is to explicitly consider the temporal order of links and chains of links connecting to a node with some temporal decay factors. We tested ECM with three large real world citation networks on the task of predicting papers' future importance. We compared ECM's performance with two static metrics, degree-centrality and Page Rank, and two time-aware metrics, age-based Page Rank and Cite Rank. We\u00a0\u2026", "num_citations": "41\n", "authors": ["1193"]}
{"title": "Using proximity to predict activity in social networks\n", "abstract": " The structure of a social network contains information useful for predicting its evolution. We show that structural information also helps predict activity. People who are\" close\" in some sense in a social network are more likely to perform similar actions than more distant people. We use network proximity to capture the degree to which people are\" close\" to each other. In addition to standard proximity metrics used in the link prediction task, such as neighborhood overlap, we introduce new metrics that model different types of interactions that take place between people. We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter. We show that structural proximity of two users in the follower graph is related to similarity of their activity, ie, how many URLs they both forward. We also show that given friends' activity, knowing their proximity to the user can help better\u00a0\u2026", "num_citations": "40\n", "authors": ["1193"]}
{"title": "Structure of heterogeneous networks\n", "abstract": " Heterogeneous networks play a key role in the evolution of communities and the decisions individuals make. These networks link different types of entities, for example, people and the events they attend. Network analysis algorithms usually project such networks unto simple graphs composed of entities of a single type. In the process, they conflate relations between entities of different types and loose important structural information.We develop a mathematical framework that can be used to compactly represent and analyze heterogeneous networks that combine multiple entity and link types. We generalize Bonacich centrality, which measures connectivity between nodes by the number of paths between them, to heterogeneous networks and use this measure to study network structure. Specifically, we extend the popular modularity maximization method for community detection to use this centrality metric. We also\u00a0\u2026", "num_citations": "40\n", "authors": ["1193"]}
{"title": "The interplay between dynamics and networks: centrality, communities, and cheeger inequality\n", "abstract": " We study the interplay between a dynamic process and the structure of the network on which it is defined. Specifically, we examine the impact of this interaction on the quality-measure of network clusters and node centrality. This enables us to effectively identify network communities and important nodes participating in the dynamics. As the first step towards this objective, we introduce an umbrella framework for defining and characterizing an ensemble of dynamic processes on a network. This framework generalizes the traditional Laplacian framework to continuous-time biased random walks and also allows us to model some epidemic processes over a network. For each dynamic process in our framework, we can define a function that measures the quality of every subset of nodes as a potential cluster (or community) with respect to this process on a given network. This subset-quality function generalizes the\u00a0\u2026", "num_citations": "39\n", "authors": ["1193"]}
{"title": "Agent memory and adaptation in multi-agent systems\n", "abstract": " We describe a general mechanism for adaptation in multi-agent systems in which agents modify their behavior based on their memory of past events. These behavior changes can be elicited by environmental dynamics or arise as response to the actions of other agents. The agents use memory to estimate the global state of the system from individual observations and adjust their actions accordingly. We also present a mathematical model of the dynamics of collective behavior in such systems and apply it to study adaptive coalition formation in electronic marketplaces. In adaptive coalition formation, the agents are more likely to join smaller coalitions than larger ones while there are many small coalitions. The rationale behind this is that smaller coalitions are necessary to nucleate larger ones. The agents remember the sizes of coalition they encountered and use them to estimate the mean coalition size. They decide\u00a0\u2026", "num_citations": "39\n", "authors": ["1193"]}
{"title": "Design and mathematical analysis of agent-based systems\n", "abstract": " Agent-based systems that are composed of simple locally interacting agents but which demonstrate complex group behavior offer several advantages over traditional multi-agent systems. A well-designed complex agent-based systems is an efficient, robust, adaptive and stable system. It has very low communication and computational requirements, meaning that there are virtually no constraints on the system size. The simplicity of agent interactions also makes it amenable to quantitative mathematical analysis. In addition to offering predictive power, mathematical analysis enables the system designer to optimize system performance.               To date, there have been relatively few implementations of complex agent-based systems, mainly because of the difficulty of determining what simple agent strategies will lead to desirable collective behavior in a large system. We claim that there exists a set of primitive\u00a0\u2026", "num_citations": "39\n", "authors": ["1193"]}
{"title": "Travel analytics: understanding how destination choice and business clusters are connected based on social media data\n", "abstract": " Understanding how destination choice and business clusters are connected is of great importance for designing sustainable cities, fostering flourishing business clusters, and building livable communities. As sharing locations and activities on social media platforms becomes increasingly popular, such data can reveal destination choice and activity space which can shed light on human-environment relationships. To this end, this research models the relationship between characteristics of business clusters and check-in activities from Los Angeles County, California. Business clusters are analyzed via two lenses: the supply side (employment data by industry) and the demand side (on-line check-in data). Spatial and statistical analyses are performed to understand how land use and transportation network features affect the popularity of the identified clusters and their relationships. Our results suggest that a cluster\u00a0\u2026", "num_citations": "38\n", "authors": ["1193"]}
{"title": "How the structure of Wikipedia articles influences user navigation\n", "abstract": " In this work we study how people navigate the information network of Wikipedia and investigate (i) free-form navigation by studying all clicks within the English Wikipedia over an entire month and (ii) goal-directed Wikipedia navigation by analyzing wikigames, where users are challenged to retrieve articles by following links. To study how the organization of Wikipedia articles in terms of layout and links affects navigation behavior, we first investigate the characteristics of the structural organization and of hyperlinks in Wikipedia and then evaluate link selection models based on article structure and other potential influences in navigation, such as the generality of an article's topic. In free-form Wikipedia navigation, covering all Wikipedia usage scenarios, we find that click choices can be best modeled by a bias towards article structure, such as a tendency to click links located in the lead section. For the goal-directed\u00a0\u2026", "num_citations": "38\n", "authors": ["1193"]}
{"title": "Using stochastic models to describe and predict social dynamics of web users\n", "abstract": " The popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both the hosts of social media content and its consumers. Accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the content. Predicting the popularity of content in social media is challenging due to the complex interactions between content quality and how the social media site highlights its content. Moreover, most social media sites selectively present content that has been highly rated by similar users, whose similarity is indicated implicitly by their behavior or explicitly by links in a social network. While these factors make it difficult to predict\u00a0\u2026", "num_citations": "37\n", "authors": ["1193"]}
{"title": "Exploiting structure within data for accurate labeling using conditional random fields\n", "abstract": " Automatically assigning semantic class labels such as WindSpeed, Flight Number and Address to data obtained from structured sources including databases or web pages is an important problem in data integration since it enables the researchers to identify the contents of these sources. Automatic semantic annotation is difficult because of the variety of formats used for each semantic type (eg, Date) as well as the similarity between different semantic types (eg, Humidity and Chance of Precipitation). In this paper, we show that by exploiting different kinds of latent structure within data we can perform this task accurately. We show that this improvement happens in spite of higher complexity in terms of both the inference procedure and the increased number of labels. We study how increasing the amount of structure taken into account by the model improves accuracy of semantic labeling. Finally, we show that when exploiting all the relationships, we obtain a significant improvement in field labeling accuracy over the regular-expression-based approach, while still keeping the complexity low.", "num_citations": "37\n", "authors": ["1193"]}
{"title": "Automatically constructing semantic web services from online sources\n", "abstract": " The work on integrating sources and services in the Semantic Web assumes that the data is either already represented in RDF or OWL or is available through a Semantic Web Service. In practice, there is a tremendous amount of data on the Web that is not available through the Semantic Web. In this paper we present an approach to automatically discover and create new Semantic Web Services. The idea behind this approach is to start with a set of known sources and the corresponding semantic descriptions and then discover similar sources, extract the source data, build semantic descriptions of the sources, and then turn them into Semantic Web Services. We implemented an end-to-end solution to this problem in a system called Deimos and evaluated the system across five different domains. The results demonstrate that the system can automatically discover, learn semantic descriptions, and build\u00a0\u2026", "num_citations": "36\n", "authors": ["1193"]}
{"title": "Macroscopic analysis of adaptive task allocation in robots\n", "abstract": " We describe a general mechanism for adaptation in multi-agent systems in which agents modify their behavior in response to changes in the environment or actions of other agents. The agent use memory to estimate the global state of the system from individual observations and adjust their actions accordingly. We present a mathematical model of the dynamics of collective behavior in such systems and apply it to study adaptive task allocation in mobile robots. In this application, the robots task is to forage for red or green pucks. As it travels around the arena, a robot records observations of puck and other robots, and uses these observations to compute the estimated density of each. If it finds there are not enough robots of a specific type, it may switch its foraging state to fill a gap. After a transient, we expect the number of robots in each foraging state to reflect the prevalence of each puck type in the environment. We\u00a0\u2026", "num_citations": "36\n", "authors": ["1193"]}
{"title": "Analyzing microblogs with affinity propagation\n", "abstract": " Recently, there has been a great deal of interest in analyzing inherent structures in posts on microblogs such as Twitter. While many works utilize a well-known topic modeling technique, we instead propose to apply Affinity Propagation [4](AP) to analyze such a corpus, and we hypothesize that AP may provide different perspective to the traditional approach. Our preliminary analysis raises some interesting facts and issues, which suggest future research directions.", "num_citations": "35\n", "authors": ["1193"]}
{"title": "Evidence of online performance deterioration in user sessions on Reddit\n", "abstract": " This article presents evidence of performance deterioration in online user sessions quantified by studying a massive dataset containing over 55 million comments posted on Reddit in April 2015. After segmenting the sessions (i.e., periods of activity without a prolonged break) depending on their intensity (i.e., how many posts users produced during sessions), we observe a general decrease in the quality of comments produced by users over the course of sessions. We propose mixed-effects models that capture the impact of session intensity on comments, including their length, quality, and the responses they generate from the community. Our findings suggest performance deterioration: Sessions of increasing intensity are associated with the production of shorter, progressively less complex comments, which receive declining quality scores (as rated by other users), and are less and less engaging (i.e., they attract fewer responses). Our contribution evokes a connection between cognitive and attention dynamics and the usage of online social peer production platforms, specifically the effects of deterioration of user performance.", "num_citations": "33\n", "authors": ["1193"]}
{"title": "Electric elves: Immersing an agent organization in a human organization\n", "abstract": " Future large-scale human organizations will be highly agentized, with software agents supporting the traditional tasks of information gathering, planning, and execution monitoring, as well as having increased control of resources and devices (communication and otherwise). As these heterogeneous software agents take on more of these activities, they will face the additional tasks of interfacing with people and sometimes acting as their proxies. Dynamic teaming of such heterogeneous agents will enable organizations to act coherently, to robustly attain their mission goals, to react swiftly to crises, and to dynamically adapt to events. Advances in this agentization could potentially assist all organizations, including the military, civilian disaster response organizations, corporations, and universities and research institutions.Within an organization, we envision that agent-based technology will facilitate (and sometimes supervise) all collaborative activities. For a research institution, agentization may facilitate such activities as meeting organization, paper composition, software development, and deployment of people and equipment for out-of-town demonstrations. For a military organization, agentization may enable the teaming of military units and equipment for rapid deployment, the monitoring of the progress of such deployments, and the rapid response to any crises that may arise. To accomplish such goals, we envision the presence of agent proxies for each person within an organization. Thus, for instance, if an organizational crisis requires an urgent deployment of a team of people and equipment, then agent proxies could dynamically volunteer for\u00a0\u2026", "num_citations": "33\n", "authors": ["1193"]}
{"title": "Semantic labeling of online information sources\n", "abstract": " In order to combine data from various heterogeneous sources, software agents must first understand the semantics of the sources, expressed in the source model. Currently, source modeling is manual, but as large numbers of sources come online, it is impractical to expect users to continue modeling them by hand. We describe two machine learning techniques for automatically modeling information sources: one that uses source\u2019s metadata, contained in a Web Service Definition file, and one that uses the source\u2019s content, to classify the semantics of the data it uses. We go beyond previous works and verify predictions by invoking the source with sample data of the predicted type. We provide performance results of both methods and validate our approach on several live Web sources. In addition, we describe the application of semantic modeling within the CALO project.", "num_citations": "32\n", "authors": ["1193"]}
{"title": "Resource allocation and emergent coordination in wireless sensor networks\n", "abstract": " Coordination in wireless sensor networks (WSN) is required for many tasks that are best achieved collectively, such as coverage and medium access. One of the major challenges in the design of WSN are the strong limitations imposed by finite onboard power capacity. Because communication requires considerable energy, it is imperative to have a coordination mechanism that requires little or no communication. Moreover, since WSN are likely to operate in unstructured and dynamic environments, the coordination mechanism has to be adaptive and robust with respect to environmental changes. Lack of centralized control in WSN requires alternative means for coordinating actions and resources of individual nodes to achieve long network lifetime, while not severely compromising network task performance. In this paper we explore the paradigm of emergent coordination as a mechanism for adaptive, distributed coordination in WSN. Specifically, we study a WSN composed of self\u2013interested nodes that utilize a simple reinforcement learning scheme and achieve coordination by playing repeated resource allocation (load balancing) games with changing resource (load) capacities. Our results indicate that for a certain range of parameters the network is very adaptive to these changes. Although we formulate the problem in rather abstract settings of repeated games, the methods can be applied to a range of specific sensor coordination problems such as network coverage and medium access.", "num_citations": "32\n", "authors": ["1193"]}
{"title": "Individual performance in team-based online games\n", "abstract": " Complex real-world challenges are often solved through teamwork. Of special interest are ad hoc teams assembled to complete some task. Many popular multiplayer online battle arena (MOBA) video-games adopt this team formation strategy and thus provide a natural environment to study ad hoc teams. Our work examines data from a popular MOBA game, League of Legends, to understand the evolution of individual performance within ad hoc teams. Our analysis of player performance in successive matches of a gaming session demonstrates that a player\u2019s success deteriorates over the course of the session, but this effect is mitigated by the player\u2019s experience. We also find no significant long-term improvement in the individual performance of most players. Modelling the short-term performance dynamics allows us to accurately predict when players choose to continue to play or end the session. Our findings\u00a0\u2026", "num_citations": "31\n", "authors": ["1193"]}
{"title": "Using stochastic models to predict user response in social media\n", "abstract": " User response to contributed content in online social media depends on many factors. These include how the site lays out new content, how frequently the user visits the site, how many friends the user follows, how active these friends are, as well as how interesting or useful the content is to the user. We present a stochastic modeling framework that relates a user's behavior to details of the site's user interface and user activity and describe a procedure for estimating model parameters from available data. We apply the model to study discussions of controversial topics on Twitter, specifically, to predict how followers of an advocate for a topic respond to the advocate's posts. We show that a model of user behavior that explicitly accounts for a user discovering the advocate's post by scanning through a list of newer posts, better predicts response than models that do not.", "num_citations": "31\n", "authors": ["1193"]}
{"title": "Document clustering in reduced dimension vector space\n", "abstract": " Document clustering is a popular tool for automatically organizing a large collection of texts. Clustering algorithms are usually applied to documents represented as vectors in a high dimensional term space. We investigate the use of Latent Semantic Analysis to create a new vector space, that is the optimal representation of the document collection. Documents are projected onto a small subspace of this vector space and clustered. We compare the performance of clustering algorithms when applied to documents represented in the full term space and in reduced dimension subspace of the LSA-generated vector space. We report significant improvements in cluster quality for LSA subspaces with optimal dimensionality. We discuss the procedure for determining the right number of dimensions for the subspace. Moreover, when this number is small, the total running time of the clustering algorithm is comparable to the one that uses the full term space.", "num_citations": "31\n", "authors": ["1193"]}
{"title": "Predicting cyber-events by leveraging hacker sentiment\n", "abstract": " Recent high-profile cyber-attacks exemplify why organizations need better cyber-defenses. Cyber-threats are hard to accurately predict because attackers usually try to mask their traces. However, they often discuss exploits and techniques on hacking forums. The community behavior of the hackers may provide insights into the groups\u2019 collective malicious activity. We propose a novel approach to predict cyber-events using sentiment analysis. We test our approach using cyber-attack data from two major business organizations. We consider three types of events: malicious software installation, malicious-destination visits, and malicious emails that surmounted the target organizations\u2019 defenses. We construct predictive signals by applying sentiment analysis to hacker forum posts to better understand hacker behavior. We analyze over 400 K posts written between January 2016 and January 2018 on over 100 hacking forums both on the surface and dark web. We find that some forums have significantly more predictive power than others. Sentiment-based models that leverage specific forums can complement state-of-the-art time-series models on forecasting cyber-attacks weeks ahead of the events. View Full-Text", "num_citations": "30\n", "authors": ["1193"]}
{"title": "Geography of emotion: Where in a city are people happier?\n", "abstract": " During the last years, researchers explored the geographic and environmental factors that affect happiness. More recently, location-sharing services provided by the social media has given an unprecedented access to geo-located data for studying the interplay between these factors on a much bigger scale. Do location-sharing services help in turn at distinguishing emotions in places within a city? Which aspects contribute better at understanding happier places? To answer these questions, we use data from Foursquare location-sharing service to identify areas within a major US metropolitan area with many check-ins, ie, areas that people like to use. We then use data from the Twitter microblogging platform to analyze the properties of these areas. Specifically, we have extracted a large corpus of geo-tagged messages, called tweets, from a major metropolitan area and linked them US Census data through their\u00a0\u2026", "num_citations": "29\n", "authors": ["1193"]}
{"title": "LA-LDA: a limited attention topic model for social recommendation\n", "abstract": " Social media users have finite attention which limits the number of incoming messages from friends they can process. Moreover, they pay more attention to opinions and recommendations of some friends more than others. In this paper, we propose -LDA, a latent topic model which incorporates limited, non-uniformly divided attention in the diffusion process by which opinions and information spread on the social network. We show that our proposed model is able to learn more accurate user models from users\u2019 social network and item adoption behavior than models which do not take limited attention into account. We analyze voting on news items on the social news aggregator Digg and show that our proposed model is better able to predict held out votes than alternative models. Our study demonstrates that psycho-socially motivated models have better ability to describe and predict observed behavior\u00a0\u2026", "num_citations": "29\n", "authors": ["1193"]}
{"title": "Model-guided performance tuning of parameter values: A case study with molecular dynamics visualization\n", "abstract": " In this paper, we consider the interaction between application programmers and tools that automatically search a space of application-level parameters that are believed to impact the performance of an application significantly. We study performance tuning of a large scientific application, the visualization component of a molecular dynamics simulation. The key contribution of the approach is the use of high-level programmer-specified models of the expected performance behavior of individual parameters. We use these models to reduce the search space associated with the range of parameter values and achieve results that perform close to that of a more exhaustive search of the parameter space. With this case study, we show the importance of appropriate parameter selection, with the difference between best-case and worst-case performance with a particular input data set and processor configuration of up to a\u00a0\u2026", "num_citations": "29\n", "authors": ["1193"]}
{"title": "LA-CTR: A limited attention collaborative topic regression for social media\n", "abstract": " Probabilistic models can learn users' preferences from the history of their item adoptions on a social media site, and in turn, recommend new items to users based on learned preferences. However, current models ignore psychological factors that play an important role in shaping online social behavior. One such factor is attention, the mechanism that integrates perceptual and cognitive features to select the items the user will consciously process and may eventually adopt. Recent research has shown that people have finite attention, which constrains their online interactions, and that they divide their limited attention non-uniformly over other people. We propose a collaborative topic regression model that incorporates limited, non-uniformly divided attention. We show that the proposed model is able to learn more accurate user preferences than state-of-art models, which do not take human cognitive factors into account. Specifically we analyze voting on news items on the social news aggregator and show that our model is better able to predict held out votes than alternate models. Our study demonstrates that psycho-socially motivated models are better able to describe and predict observed behavior than models which only consider latent social structure and content.", "num_citations": "28\n", "authors": ["1193"]}
{"title": "A model of adaptation in collaborative multi-agent systems\n", "abstract": " Adaptation is an essential requirement for autonomous agent systems functioning in                 uncertain dynamic environments. Adaptation allows agents to change their behavior in                 order to improve the overall sys tem performance. We describe a general mechanism                 for adaptation in multi-agent systems in which agents modify their behavior in                 response to changes in the environment or actions of other agents. The agents                 estimate the global state of the system from local observations and adjust their                 actions accordingly. We derive a mathematical model that describes the collective                 behavior of such adaptive systems. The model, consisting of coupled rate equations,                 governs how the collective behavior changes in time. We apply the model to study                 collaboration in a group of mobile robots. The system we study is an adaptive                 version of\u00a0\u2026", "num_citations": "28\n", "authors": ["1193"]}
{"title": "Gender disparity in the authorship of biomedical research publications during the COVID-19 pandemic: Retrospective observational study\n", "abstract": " Background           Gender imbalances in academia have been evident historically and persist today. For the past 60 years, we have witnessed the increase of participation of women in biomedical disciplines, showing that the gender gap is shrinking. However, preliminary evidence suggests that women, including female researchers, are disproportionately affected by the COVID-19 pandemic in terms of unequal distribution of childcare, elderly care, and other kinds of domestic and emotional labor. Sudden lockdowns and abrupt shifts in daily routines have had disproportionate consequences on their productivity, which is reflected by a sudden drop in research output in biomedical research, consequently affecting the number of female authors of scientific publications.                             Objective           The objective of this study is to test the hypothesis that the COVID-19 pandemic has had a disproportionate adverse effect on the productivity of female researchers in the biomedical field in terms of authorship of scientific publications.                             Methods           This is a retrospective observational bibliometric study. We investigated the proportion of male and female researchers who published scientific papers during the COVID-19 pandemic, using bibliometric data from biomedical preprint servers and selected Springer-Nature journals. We used the ordinary least squares regression model to estimate the expected proportions over time by correcting for temporal trends. We also used a set of statistical methods, such as the Kolmogorov-Smirnov test and regression discontinuity design, to test the validity of the results\u00a0\u2026", "num_citations": "27\n", "authors": ["1193"]}
{"title": "Identifying sentiment of hookah-related posts on Twitter\n", "abstract": " Background: The increasing popularity of hookah (or waterpipe) use in the United States and elsewhere has consequences for public health because it has similar health risks to that of combustible cigarettes. While hookah use rapidly increases in popularity, social media data (Twitter, Instagram) can be used to capture and describe the social and environmental contexts in which individuals use, perceive, discuss, and are marketed this tobacco product. These data may allow people to organically report on their sentiment toward tobacco products like hookah unprimed by a researcher, without instrument bias, and at low costs.Objective: This study describes the sentiment of hookah-related posts on Twitter and describes the importance of debiasing Twitter data when attempting to understand attitudes.Methods: Hookah-related posts on Twitter (N= 986,320) were collected from March 24, 2015, to December 2, 2016. Machine learning models were used to describe sentiment on 20 different emotions and to debias the data so that Twitter posts reflected sentiment of legitimate human users and not of social bots or marketing-oriented accounts that would possibly provide overly positive or overly negative sentiment of hookah.Results: From the analytical sample, 352,116 tweets (59.50%) were classified as positive while 177,537 (30.00%) were classified as negative, and 62,139 (10.50%) neutral. Among all positive tweets, 218,312 (62.00%) were classified as highly positive emotions (eg, active, alert, excited, elated, happy, and pleasant), while 133,804 (38.00%) positive tweets were classified as passive positive emotions (eg, contented, serene, calm\u00a0\u2026", "num_citations": "27\n", "authors": ["1193"]}
{"title": "Partitioning networks with node attributes by compressing information flow\n", "abstract": " Real-world networks are often organized as modules or communities of similar nodes that serve as functional units. These networks are also rich in content, with nodes having distinguished features or attributes. In order to discover a network\u2019s modular structure, it is necessary to take into account not only its links but also node attributes. We describe an information-theoretic method that identifies modules by compressing descriptions of information flow on a network. Our formulation introduces node content into the description of information flow, which we then minimize to discover groups of nodes with similar attributes that also tend to trap the flow of information. The method is conceptually simple and does not require ad-hoc parameters to specify the number of modules or to control the relative contribution of links and node attributes to network structure. We apply the proposed method to partition real-world\u00a0\u2026", "num_citations": "27\n", "authors": ["1193"]}
{"title": "Disentangling the effects of social signals\n", "abstract": " Peer recommendation is a crowdsourcing task that leverages the opinions of many to identify interesting content online, such as news, images, or videos. Peer recommendation applications often use social signals, e.g., the number of prior recommendations, to guide people to the more interesting content. How people react to social signals, in combination with content quality and its presentation order, determines the outcomes of peer recommendation, i.e., item popularity. Using Amazon Mechanical Turk, we experimentally measure the effects of social signals in peer recommendation. Specifically, after controlling for variation due to item content and its position, we find that social signals affect item popularity about half as much as position and content do. These effects are somewhat correlated, so social signals exacerbate the \"rich get richer\" phenomenon, which results in a wider variance of popularity. Further, social signals change individual preferences, creating a \"herding\" effect that biases people's judgments about the content. Despite this, we find that social signals improve the efficiency of peer recommendation by reducing the effort devoted to evaluating content while maintaining recommendation quality.", "num_citations": "27\n", "authors": ["1193"]}
{"title": "Non-conservative diffusion and its application to social network analysis\n", "abstract": " The random walk is fundamental to modeling dynamic processes on networks. Metrics based on the random walk have been used in many applications from image processing to Web page ranking. However, how appropriate are random walks to modeling and analyzing social networks? We argue that unlike a random walk, which conserves the quantity diffusing on a network, many interesting social phenomena, such as the spread of information or disease on a social network, are fundamentally non-conservative. When an individual infects her neighbor with a virus, the total amount of infection increases. We classify diffusion processes as conservative and non-conservative and show how these differences impact the choice of metrics used for network analysis, as well as our understanding of network structure and behavior. We show that Alpha-Centrality, which mathematically describes non-conservative diffusion, leads to new insights into the behavior of spreading processes on networks. We give a scalable approximate algorithm for computing the Alpha-Centrality in a massive graph. We validate our approach on real-world online social networks of Digg. We show that a non-conservative metric, such as Alpha-Centrality, produces better agreement with empirical measure of influence than conservative metrics, such as PageRank. We hope that our investigation will inspire further exploration into the realms of conservative and non-conservative metrics in social network analysis.", "num_citations": "27\n", "authors": ["1193"]}
{"title": "Social browsing & information filtering in social media\n", "abstract": " Social networks are a prominent feature of many social media sites, a new generation of Web sites that allow users to create and share content. Sites such as Digg, Flickr, and Del.icio.us allow users to designate others as \"friends\" or \"contacts\" and provide a single-click interface to track friends' activity. How are these social networks used? Unlike pure social networking sites (e.g., LinkedIn and Facebook), which allow users to articulate their online professional and personal relationships, social media sites are not, for the most part, aimed at helping users create or foster online relationships. Instead, we claim that social media users create social networks to express their tastes and interests, and use them to filter the vast stream of new submissions to find interesting content. Social networks, in fact, facilitate new ways of interacting with information: what we call social browsing. Through an extensive analysis of data from Digg and Flickr, we show that social browsing is one of the primary usage modalities on these social media sites. This finding has implications for how social media sites rate and personalize content.", "num_citations": "27\n", "authors": ["1193"]}
{"title": "The myopia of crowds: Cognitive load and collective evaluation of answers on Stack Exchange\n", "abstract": " Crowds can often make better decisions than individuals or small groups of experts by leveraging their ability to aggregate diverse information. Question answering sites, such as Stack Exchange, rely on the \u201cwisdom of crowds\u201d effect to identify the best answers to questions asked by users. We analyze data from 250 communities on the Stack Exchange network to pinpoint factors affecting which answers are chosen as the best answers. Our results suggest that, rather than evaluate all available answers to a question, users rely on simple cognitive heuristics to choose an answer to vote for or accept. These cognitive heuristics are linked to an answer\u2019s salience, such as the order in which it is listed and how much screen space it occupies. While askers appear to depend on heuristics to a greater extent than voters when choosing an answer to accept as the most helpful one, voters use acceptance itself as a heuristic, and they are more likely to choose the answer after it has been accepted than before that answer was accepted. These heuristics become more important in explaining and predicting behavior as the number of available answers to a question increases. Our findings suggest that crowd judgments may become less reliable as the number of answers grows.", "num_citations": "26\n", "authors": ["1193"]}
{"title": "Populating the semantic web\n", "abstract": " The vision of the Semantic Web is that a vast store of online information \u201cmeaningful to computers will unleash a revolution of new possibilities\u201d. Unfortunately, the vast majority of information on the Web is formatted to be easily read by human users, not computer applications. In order to make the vision of the Semantic Web a reality, tools for automatically annotating Web content with semantic labels will be required. We describe the ADEL system that automatically extracts records from Web sites and semantically labels the fields. The system exploits similarities in the layout of Web pages in order to learn the grammar that generated these pages. It them uses this grammar to extract structured records from these Web pages. ADEL system also exploits the fact that sites in the same domain will provide the same, or similar data. By collecting labeled examples of data during the training stage, we are able to learn structural descriptions of data fields and later use these descriptions to semantically label new data fields. We show that on a Used Car shopping domain, ADEL achieves precision of 64% and recall of 89% on extracting and labeling data columns.", "num_citations": "26\n", "authors": ["1193"]}
{"title": "Information Integration for the Masses.\n", "abstract": " Information integration applications combine data from heterogeneous sources to assist the user in solving repetitive data-intensive tasks. Currently, such applications require a high level of expertise in information integration since users need to know how to extract data from an on-line source, describe its semantics, and build integration plans to answer specific queries. We have integrated three task learning technologies within a single desktop application to assist users in creating information integration applications. It includes a tool for programmatic access to data in on-line information sources, a tool to semantically model them by aligning their input and output parameters with a common ontology, and a tool that enables the user to create complex integration plans using simple text instructions. Our system was integrated within the Calo Desktop Assistant and evaluated independently on a range of problems. It\u00a0\u2026", "num_citations": "25\n", "authors": ["1193"]}
{"title": "Emotions, demographics and sociability in Twitter interactions\n", "abstract": " The social connections people form online affect the quality of information they receive and their online experience. Although a host of socioeconomic and cognitive factors were implicated in the formation of offline social ties, few of them have been empirically validated, particularly in an online setting. In this study, we analyze a large corpus of geo-referenced messages, or tweets, posted by social media users from a major US metropolitan area. We linked these tweets to US Census data through their locations. This allowed us to measure emotions expressed in the tweets posted from an area, the structure of social connections, and also use that area's socioeconomic characteristics in analysis.% We extracted the structure of online social interactions from the people mentioned in tweets from that area. We find that at an aggregate level, places where social media users engage more deeply with less diverse social contacts are those where they express more negative emotions, like sadness and anger. Demographics also has an impact: these places have residents with lower household income and education levels. Conversely, places where people engage less frequently but with diverse contacts have happier, more positive messages posted from them and also have better educated, younger, more affluent residents. Results suggest that cognitive factors and offline characteristics affect the quality of online interactions. Our work highlights the value of linking social media data to traditional data sources, such as US Census, to drive novel analysis of online behavior.", "num_citations": "23\n", "authors": ["1193"]}
{"title": "Network weirdness: Exploring the origins of network paradoxes\n", "abstract": " Social networks have many counter-intuitive properties, including the\" friendship paradox\" that states, on average, your friends have more friends than you do. Recently, a variety of other paradoxes were demonstrated in online social networks. This paper explores the origins of these network paradoxes. Specifically, we ask whether they arise from mathematical properties of the networks or whether they have a behavioral origin. We show that sampling from heavy-tailed distributions always gives rise to a paradox in the mean, but not the median. We propose a strong form of network paradoxes, based on utilizing the median, and validate it empirically using data from two online social networks. Specifically, we show that for any user the majority of user\u2019s friends and followers have more friends, followers, etc. than the user, and that this cannot be explained by statistical properties of sampling. Next, we explore the behavioral origins of the paradoxes by using the shuffle test to remove correlations between node degrees and attributes. We find that paradoxes for the mean persist in the shuffled network, but not for the median. We demonstrate that strong paradoxes arise due to the assortativity of user attributes, including degree, and correlation between degree and attribute.", "num_citations": "23\n", "authors": ["1193"]}
{"title": "Network structure, topology, and dynamics in generalized models of synchronization\n", "abstract": " Network structure is a product of both its topology and interactions between its nodes. We explore this claim using the paradigm of distributed synchronization in a network of coupled oscillators. As the network evolves to a global steady state, nodes synchronize in stages, revealing the network's underlying community structure. Traditional models of synchronization assume that interactions between nodes are mediated by a conservative process similar to diffusion. However, social and biological processes are often nonconservative. We propose a model of synchronization in a network of oscillators coupled via nonconservative processes. We study the dynamics of synchronization of a synthetic and real-world networks and show that the traditional and nonconservative models of synchronization reveal different structures within the same network.", "num_citations": "23\n", "authors": ["1193"]}
{"title": "Dynamics of collaborative document rating systems\n", "abstract": " The rise of social media sites, such as blogs, wikis, Digg and Flickr among others, underscores a transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. The social news aggregator Digg allows users to submit links to and vote on news stories. Like other social media sites, Digg also allows users to designate others as\" friends\" and easily track friends' activities: what new stories they submitted, commented on or liked. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page. We construct a mathematical model to study how collaborative rating and promotion of news stories emerges from independent decisions made by many users. The model takes into account user behavior: eg\u00a0\u2026", "num_citations": "23\n", "authors": ["1193"]}
{"title": "Computational social scientist beware: Simpson\u2019s paradox in behavioral data\n", "abstract": " Observational data about human behavior are often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior. Heterogeneity predisposes analysis to Simpson\u2019s paradox, whereby the trends observed in data that have been aggregated over the entire population may be substantially different from those of the underlying subgroups. I illustrate Simpson\u2019s paradox with several examples coming from studies of online behavior and show that aggregate response leads to wrong conclusions about the underlying individual behavior. I then present a simple method to test whether Simpson\u2019s paradox is affecting results of analysis. The presence of Simpson\u2019s paradox in social data suggests that important behavioral differences exist within the population, and failure to take these differences into account can distort the studies\u2019 findings.", "num_citations": "22\n", "authors": ["1193"]}
{"title": "Modeling social annotation: a bayesian approach\n", "abstract": " Collaborative tagging systems, such as Delicious, CiteULike, and others, allow users to annotate resources, for example, Web pages or scientific papers, with descriptive labels called tags. The social annotations contributed by thousands of users can potentially be used to infer categorical knowledge, classify documents, or recommend new relevant information. Traditional text inference methods do not make the best use of social annotation, since they do not take into account variations in individual users\u2019 perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes the interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which\u00a0\u2026", "num_citations": "22\n", "authors": ["1193"]}
{"title": "Automatically modeling group behavior of simple agents\n", "abstract": " Mathematical modeling and analysis of collective behavior of multi-agent systems is an important tool that will enable researchers to study even very large systems, validate agent models and get insight into multi-agent system design. Biologists, for example, can compare the model\u2019s predictions to the observed collective behavior of simple organisms, such as social insects, to understand individual organism behavior. We describe the process of automatic construction of models of collective behavior. This process consists of 1) observing the sequence of agent behaviors, 2) inducing a model of the agent\u2019s behavior from these observations, 3) translating the model into equations describing group behavior, 4) analyzing the equations to learn more about the system. The focus of this paper is the last two steps, including a \u201crecipe\u201d for creating equations of collective behavior from the details of the individual agent controller.", "num_citations": "21\n", "authors": ["1193"]}
{"title": "Two paradigms for the design of artificial collectives\n", "abstract": " Artificial collectives are systems composed of multiple autonomous information or software agents, mobile robots, or nodes in a sensor or communication network. In the future, such systems will be responsible for many important tasks, such as highway traffic control, disaster response, toxic spill monitoring and cleanup, and exploration of other planets. Because such systems will have to function in environments with unreliable communication channels, where agents are likely to fail, they will have to be reliable, scalable, robust, adaptable, and amenable to quantitative mathematical analysis. The last property is important because analysis is crucial to understanding the issues of the design, control, adaptability, and dynamics of collective behavior. We describe two approaches to distributed control of artificial collectives and study them quantitatively. The first, biologically based control, relies on local\u00a0\u2026", "num_citations": "21\n", "authors": ["1193"]}
{"title": "University of Southern California\n", "abstract": " \u2022 Abby wants a pretty image for her blog. She found one on Flickr. Can she use it on her blog?\u2022 Ben notices many students wearing USC apparel. He decides to take USC logos off the web, stick them on cheap t-shirts and make some extra $$. Can he legally do this?", "num_citations": "21\n", "authors": ["1193"]}
{"title": "Friendship paradox biases perceptions in directed networks\n", "abstract": " Social networks shape perceptions by exposing people to the actions and opinions of their peers. However, the perceived popularity of a trait or an opinion may be very different from its actual popularity. We attribute this perception bias to friendship paradox and identify conditions under which it appears. We validate the findings empirically using Twitter data. Within posts made by users in our sample, we identify topics that appear more often within users\u2019 social feeds than they do globally among all posts. We also present a polling algorithm that leverages the friendship paradox to obtain a statistically efficient estimate of a topic\u2019s global prevalence from biased individual perceptions. We characterize the polling estimate and validate it through synthetic polling experiments on Twitter data. Our paper elucidates the non-intuitive ways in which the structure of directed networks can distort perceptions and presents\u00a0\u2026", "num_citations": "20\n", "authors": ["1193"]}
{"title": "Spectral clustering with epidemic diffusion\n", "abstract": " Spectral clustering is widely used to partition graphs into distinct modules or communities. Existing methods for spectral clustering use the eigenvalues and eigenvectors of the graph Laplacian, an operator that is closely associated with random walks on graphs. We propose a spectral partitioning method that exploits the properties of epidemic diffusion. An epidemic is a dynamic process that, unlike the random walk, simultaneously transitions to all the neighbors of a given node. We show that the replicator, an operator describing epidemic diffusion, is equivalent to the symmetric normalized Laplacian of a reweighted graph with edges reweighted by the eigenvector centralities of their incident nodes. Thus, more weight is given to edges connecting more central nodes. We describe a method that partitions the nodes based on the componentwise ratio of the replicator's second eigenvector to the first and compare its\u00a0\u2026", "num_citations": "20\n", "authors": ["1193"]}
{"title": "Characterising emergent semantics in twitter lists\n", "abstract": " Twitter lists organise Twitter users into multiple, often overlapping, sets. We believe that these lists capture some form of emergent semantics, which may be useful to characterise. In this paper we describe an approach for such characterisation, which consists of deriving semantic relations between lists and users by analyzing the co-occurrence of keywords in list names. We use the vector space model and Latent Dirichlet Allocation to obtain similar keywords according to co-occurrence patterns. These results are then compared to similarity measures relying on WordNet and to existing Linked Data sets. Results show that co-occurrence of keywords based on members of the lists produce more synonyms and more correlated results to that of WordNet similarity measures.", "num_citations": "20\n", "authors": ["1193"]}
{"title": "Learning boundaries of vague places from noisy annotations\n", "abstract": " What ordinary people mean by places may differ dramatically from what experts consider them to be. This is especially evident in how people talk about places in social media, where'Los Angeles', for instance, could include areas well outside of the city or even in another county. In order to make best use of the information in social media, we need to understand what people mean when they refer to a place. Social annotations provide valuable evidence for harvesting knowledge about places, eg, learning their boundaries and relations to other places. However, social annotations are noisy, and this can dramatically distort the learned boundaries. In this paper we propose a method that exploits the distinctive property of social annotations---that it is created by many people---to filter out noise. Using a large data set extracted from Flickr we show that our crowd-based noise filtering method can learn accurate\u00a0\u2026", "num_citations": "20\n", "authors": ["1193"]}
{"title": "Limited attention and centrality in social networks\n", "abstract": " How does one find important or influential people in an online social network? Researchers have proposed a variety of centrality measures to identify individuals that are, for example, often visited by a random walk, infected in an epidemic, or receive many messages from friends. Recent research suggests that a social media users' capacity to respond to an incoming message is constrained by their finite attention, which they divide over all incoming information, i.e., information sent by users they follow. We propose a new measure of centrality - limited-attention version of Bonacich's Alpha-centrality - that models the effect of limited attention on epidemic diffusion. The new measure describes a process in which nodes broadcast messages to their out-neighbors, but the neighbors' ability to receive the message depends on the number of in-neighbors they have. We evaluate the proposed measure on real-world online\u00a0\u2026", "num_citations": "19\n", "authors": ["1193"]}
{"title": "TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers\n", "abstract": " We present a novel longitudinal multimodal corpus of physiological and behavioral data collected from direct clinical providers in a hospital workplace. We designed the study to investigate the use of off-the-shelf wearable and environmental sensors to understand individual-specific constructs such as job performance, interpersonal interaction, and well-being of hospital workers over time in their natural day-to-day job settings. We collected behavioral and physiological data from n= 212 participants through Internet-of-Things Bluetooth data hubs, wearable sensors (including a wristband, a biometrics-tracking garment, a smartphone, and an audio-feature recorder), together with a battery of surveys to assess personality traits, behavioral states, job performance, and well-being over time. Besides the default use of the data set, we envision several novel research opportunities and potential applications, including multi\u00a0\u2026", "num_citations": "18\n", "authors": ["1193"]}
{"title": "Taming the unpredictability of cultural markets with social influence\n", "abstract": " Unpredictability is often portrayed as an undesirable outcome of social influence in cultural markets. Unpredictability stems from the\" rich get richer\" effect, whereby small fluctuations in the market share or popularity of products are amplified over time by social influence. In this paper, we report results of an experimental study that shows that unpredictability is not an inherent property of social influence. We investigate strategies for creating markets in which the popularity of products is better-and more predictably-aligned with their underlying quality. For our study, we created a cultural market of science stories and conducted randomized experiments on different policies for presenting the stories to study participants. Specifically, we varied how the stories were ranked, and whether or not participants were shown the ratings these stories received from others. We present a policy that leverages social influence and\u00a0\u2026", "num_citations": "18\n", "authors": ["1193"]}
{"title": "Attention and visibility in an information-rich world\n", "abstract": " As the rate of content production grows, we must make a staggering number of daily decisions about what information is worth acting on. For any flourishing online social media system, users can barely keep up with the new content shared by friends. How does the user-interface design help or hinder users' ability to find interesting content? We analyze the choices people make about which information to propagate on the social media sites Twitter and Digg. We observe regularities in behavior which can be attributed directly to cognitive limitations of humans, resulting from the different visibility policies of each site. We quantify how people divide their limited attention among competing sources of information, and we show how the user-interface design can mediate information spread.", "num_citations": "18\n", "authors": ["1193"]}
{"title": "A probabilistic approach for learning folksonomies from structured data\n", "abstract": " Learning structured representations has emerged as an important problem in many domains, including document and Web data mining, bioinformatics, and image analysis. One approach to learning complex structures is to integrate many smaller, incomplete and noisy structure fragments. In this work, we present an unsupervised probabilistic approach that extends affinity propagation [7] to combine the small ontological fragments into a collection of integrated, consistent, and larger folksonomies. This is a challenging task because the method must aggregate similar structures while avoiding structural inconsistencies and handling noise. We validate the approach on a real-world social media dataset, comprised of shallow personal hierarchies specified by many individual users, collected from the photosharing website Flickr. Our empirical results show that our proposed approach is able to construct deeper and\u00a0\u2026", "num_citations": "18\n", "authors": ["1193"]}
{"title": "Lessons learned: Recommendations for implementing a longitudinal study using wearable and environmental sensors in a health care organization\n", "abstract": " Although traditional methods of data collection in naturalistic settings can shed light on constructs of interest to researchers, advances in sensor-based technology allow researchers to capture continuous physiological and behavioral data to provide a more comprehensive understanding of the constructs that are examined in a dynamic health care setting. This study gives examples for implementing technology-facilitated approaches and provides the following recommendations for conducting such longitudinal, sensor-based research, with both environmental and wearable sensors in a health care setting: pilot test sensors and software early and often; build trust with key stakeholders and with potential participants who may be wary of sensor-based data collection and concerned about privacy; generate excitement for novel, new technology during recruitment; monitor incoming sensor data to troubleshoot sensor issues; and consider the logistical constraints of sensor-based research. The study describes how these recommendations were successfully implemented by providing examples from a large-scale, longitudinal, sensor-based study of hospital employees at a large hospital in California. The knowledge gained from this study may be helpful to researchers interested in obtaining dynamic, longitudinal sensor data from both wearable and environmental sensors in a health care setting (eg, a hospital) to obtain a more comprehensive understanding of constructs of interest in an ecologically valid, secure, and efficient way.", "num_citations": "17\n", "authors": ["1193"]}
{"title": "Discovering signals from web sources to predict cyber attacks\n", "abstract": " Cyber attacks are growing in frequency and severity. Over the past year alone we have witnessed massive data breaches that stole personal information of millions of people and wide-scale ransomware attacks that paralyzed critical infrastructure of several countries. Combating the rising cyber threat calls for a multi-pronged strategy, which includes predicting when these attacks will occur. The intuition driving our approach is this: during the planning and preparation stages, hackers leave digital traces of their activities on both the surface web and dark web in the form of discussions on platforms like hacker forums, social media, blogs and the like. These data provide predictive signals that allow anticipating cyber attacks. In this paper, we describe machine learning techniques based on deep neural networks and autoregressive time series models that leverage external signals from publicly available Web sources to forecast cyber attacks. Performance of our framework across ground truth data over real-world forecasting tasks shows that our methods yield a significant lift or increase of F1 for the top signals on predicted cyber attacks. Our results suggest that, when deployed, our system will be able to provide an effective line of defense against various types of targeted cyber attacks.", "num_citations": "17\n", "authors": ["1193"]}
{"title": "Comparative analysis of top-down and bottom-up methodologies for multi-agent system design\n", "abstract": " Traditionally, top-down and bottom-up design approaches have competed with each other in Algorithmics and Software Engineering. In the top-down approach, design process starts with specifying the global system state and assuming that each component has global knowledge of the system, as in a centralized approach. The solution is then decentralized by replacing global knowledge with communication. In the bottom-up approach, on the other hand, the design starts with specifying requirements and capabilities of individual components, and the global behavior is said to emerge out of interactions among constituent components and between components and the environment. In this paper we present a comparative study of both approaches with particular emphasis on applications to multi-agent system engineering.", "num_citations": "17\n", "authors": ["1193"]}
{"title": "Dynamics of content quality in collaborative knowledge production\n", "abstract": " We explore the dynamics of user performance in collaborative knowledge production by studying the quality of answers to questions posted on Stack Exchange. We propose four indicators of answer quality: answer length, the number of code lines and hyperlinks to external web content it contains, and whether it is accepted by the asker as the most helpful answer to the question. Analyzing millions of answers posted over the period from 2008 to 2014, we uncover regular short-term and long-term changes in quality. In the short-term, quality deteriorates over the course of a single session, with each successive answer becoming shorter, with fewer code lines and links, and less likely to be accepted. In contrast, performance improves over the long-term, with more experienced users producing higher quality answers. These trends are not a consequence of data heterogeneity, but rather have a behavioral origin. Our findings highlight the complex interplay between short-term deterioration in performance, potentially due to mental fatigue or attention depletion, and long-term performance improvement due to learning and skill acquisition, and its impact on the quality of user-generated content.", "num_citations": "16\n", "authors": ["1193"]}
{"title": "Vip: Incorporating human cognitive biases in a probabilistic model of retweeting\n", "abstract": " Information spread in social media depends on a number of factors, including how the site displays information, how users navigate it to find items of interest, users\u2019 tastes, and the \u2018virality\u2019 of information, i.e., its propensity to be adopted, or retweeted, upon exposure. Probabilistic models can learn users\u2019 tastes from the history of their item adoptions and recommend new items to users. However, current models ignore cognitive biases that are known to affect behavior. Specifically, people pay more attention to items at the top of a list than those in lower positions. As a consequence, items near the top of a user\u2019s social media stream have higher visibility, and are more likely to be seen and adopted, than those appearing below. Another bias is due to the item\u2019s fitness: some items have a high propensity to spread upon exposure regardless of the interests of adopting users. We propose a probabilistic model that\u00a0\u2026", "num_citations": "16\n", "authors": ["1193"]}
{"title": "Leveraging user diversity to harvest knowledge on the social web\n", "abstract": " Social web users are a very diverse group with varying interests, levels of expertise, enthusiasm, and expressiveness. As a result, the quality of content and annotations they create to organize content is highly variable. While several approaches have been proposed to mine social annotations, for example, to learn folksonomies that reflect how people relate narrower concepts to broader ones, these methods treat all users and the annotations they create uniformly. We propose a framework to automatically identify experts, i.e., knowledgeable users who create high quality annotations, and use their knowledge to guide folksonomy learning. We evaluate the approach on a large body of social annotations extracted from the photo sharing site Flickr. We show that using expert knowledge leads to more detailed and accurate folksonomies. Moreover, we show that including annotations from non-expert, or novice, users\u00a0\u2026", "num_citations": "16\n", "authors": ["1193"]}
{"title": "Harvesting geospatial knowledge from social metadata.\n", "abstract": " Up-to-date geospatial information can help crisis management community to coordinate its response. In addition to data that is created and curated by experts, there is an abundance of user-generated, user-curated data on Social Web sites such as Flickr, Delicious, and Google Earth, that can be used to harvest knowledge to solve real-world problems. User-generated, or social, metadata can be used to learn concepts and relations between them that can improve information discovery, and data integration and management. We describe a method that aggregates social metadata created by thousands of users of the social photo-sharing site Flickr to learn geospatial concepts and relations. Our method leverages geotagged data to represent and reason about places. We evaluate learned geospatial relations by comparing them to a reference ontology provided by GeoNames. org. We show that our approach achieves good performance and also learns useful information that does not appear in the reference ontology.", "num_citations": "16\n", "authors": ["1193"]}
{"title": "Analysis of transients for binary mixture convection in cylindrical geometry\n", "abstract": " We present experimental results for early transients near the onset of convection of an ethanol-water mixture in cylindrical containers heated from below. The separation ratio of the mixture was \u03c8\u2248\u2212 0. 0 8, and the aspect ratios \u0393\u2261 r/d (r is the radius and d the height of the sample cell) of two different containers were 10.91 and 11.53. For this system the onset of convection occurs via a subcritical Hopf bifurcation to traveling waves. Beyond the bifurcation we found transient radially traveling waves whose amplitude grew in time. We decomposed the transient patterns into azimuthal modes of the form cos m \u03b8. The azimuthal symmetry of the pattern depended strongly on \u0393. For \u0393= 10.91 odd azimuthal modes were preferred, while for \u0393= 11.53 even modes dominated. We measured the spatial and temporal growth rates at various \u03b5\u2261 \u0394 T/\u0394 T c\u2212 1 for different azimuthal modes and compared the results for the two aspect\u00a0\u2026", "num_citations": "16\n", "authors": ["1193"]}
{"title": "Can you Trust the Trend? Discovering Simpson's Paradoxes in Social Data\n", "abstract": " We investigate how Simpson\u00bb s paradox affects analysis of trends in social data. According to the paradox, the trends observed in data that has been aggregated over an entire population may be different from, and even opposite to, those of the underlying subgroups. Failure to take this effect into account can lead analysis to wrong conclusions. We present a statistical method to automatically identify Simpson\u00bb s paradox in data by comparing statistical trends in the aggregate data to those in the disaggregated subgroups. We apply the approach to data from Stack Exchange, a popular question-answering platform, to analyze factors affecting answerer performance, specifically, the likelihood that an answer written by a user will be accepted by the asker as the best answer to his or her question. Our analysis confirms a known Simpson\u00bb s paradox and identifies several new instances. These paradoxes provide novel\u00a0\u2026", "num_citations": "15\n", "authors": ["1193"]}
{"title": "iPhone's Digital Marketplace: Characterizing the Big Spenders\n", "abstract": " With mobile shopping surging in popularity, people are spending ever more money on digital purchases through their mobile devices and phones. However, few large-scale studies of mobile shopping exist. In this paper we analyze a large data set consisting of more than 776M digital purchases made on Apple mobile devices that include songs, apps, and in-app purchases. We find that 61% of all the spending is on in-app purchases and that the top 1% of users are responsible for 59% of all the spending. These big spenders are more likely to be male and older, and less likely to be from the US. We study how they adopt and abandon individual app, and find that, after an initial phase of increased daily spending, users gradually lose interest: the delay between their purchases increases and the spending decreases with a sharp drop toward the end. Finally, we model the in-app purchasing behavior in multiple steps\u00a0\u2026", "num_citations": "15\n", "authors": ["1193"]}
{"title": "Attention inequality in social media\n", "abstract": " Social media can be viewed as a social system where the currency is attention. People post content and interact with others to attract attention and gain new followers. In this paper, we examine the distribution of attention across a large sample of users of a popular social media site Twitter. Through empirical analysis of these data we conclude that attention is very unequally distributed: the top 20\\% of Twitter users own more than 96\\% of all followers, 93\\% of the retweets, and 93\\% of the mentions. We investigate the mechanisms that lead to attention inequality and find that it results from the \"rich-get-richer\" and \"poor-get-poorer\" dynamics of attention diffusion. Namely, users who are \"rich\" in attention, because they are often mentioned and retweeted, are more likely to gain new followers, while those who are \"poor\" in attention are likely to lose followers. We develop a phenomenological model that quantifies attention diffusion and network dynamics, and solve it to study how attention inequality grows over time in a dynamic environment of social media.", "num_citations": "15\n", "authors": ["1193"]}
{"title": "A visibility-based model for link prediction in social media\n", "abstract": " A core task of social network analysis is to predict the formation of new social links. In the context of social media, link prediction serves as the foundation for forecasting the evolution of the follower graph and predicting interactions and the flow of information between users. Previous link prediction methods have generally represented the social network as a graph and leveraged topological and semantic measures of similarity between two nodes to estimate the probability of a new link between them. In this work, we suggest another link creation mechanism for social media that is based on the ease of discovering the new node. Specifically, a user v creates a link to another user u after seeing u\u2019s name on his or her screen; in other words, visibility of a user (name) is a necessary condition for new link formation. We propose a model for link prediction, which estimates the probability a user will see another user\u2019s name, and use this model to predict new links. We estimate a set of parameters in the proposed model using Maximum-Likelihood and Minorize-Maximize methods. Empirical results show that the proposed model can more accurately predict both follow and co-mention links than alternative state-of-the-art methods. Our work suggests that the effort required to discover a new social contact is negatively correlated with link formation, and the easier it is to discover a user, the higher the likelihood a link will be created.", "num_citations": "15\n", "authors": ["1193"]}
{"title": "Integrating structured metadata with relational affinity propagation\n", "abstract": " Structured and semi-structured data describing entities, taxonomies and ontologies appears in many domains. There is a huge interest in integrating structured information from multiple sources; however integrating structured data to infer complex common structures is a difficult task because the integration must aggregate similar structures while avoiding structural inconsistencies that may appear when the data is combined. In this work, we study the integration of structured social metadata: shallow personal hierarchies specified by many individual users on the Social Web, and focus on inferring a collection of integrated, consistent taxonomies. We frame this task as an optimization problem with structural constraints. We propose a new inference algorithm, which we refer to as Relational Affinity Propagation (RAP) that extends affinity propagation (Frey and Dueck, 2007) by introducing structural constraints. We validate the approach on a real-world social media dataset, collected from the photosharing website Flickr. Our empirical results show that our proposed approach is able to construct deeper and denser structures compared to an approach using only the standard affinity propagation algorithm.", "num_citations": "15\n", "authors": ["1193"]}
{"title": "Analysis of a stochastic model of adaptive task allocation in robots\n", "abstract": " Adaptation is an essential requirement for self\u2013organizing multi\u2013agent systems functioning in unknown dynamic environments. Adaptation allows agents to change their actions in response to environmental changes or actions of other agents in order to improve overall system performance, and remain robust even while a sizeable fraction of agents fails. In this paper we present and study a simple model of adaptation for task allocation problem in a multi\u2013robot system. In our model robots have to choose between two types of task, and the goal is to achieve desired task division without any explicit communication between robots. Robots estimate the state of the environment from repeated local observations and decide what task to choose based on these observations. We model robots and observations as stochastic processes and study the dynamics of individual robots and the collective behavior. We\u00a0\u2026", "num_citations": "15\n", "authors": ["1193"]}
{"title": "COVID-19 misinformation and the 2020 US presidential election\n", "abstract": " Voting is the defining act for a democracy. However, voting is only meaningful if public deliberation is grounded in veritable and equitable information. This essay investigates the politicization of public health practices during the Democratic primaries in the context of the 2020 U.S. presidential election, using a dataset of more than 67 million tweets. We find the public sphere on Twitter is politically heterogeneous and the majority\u2014liberal and conservative alike\u2014advocates for wearing masks and vote-by-mail. However, a small, but dense group of conservative users push anti-mask and voter fraud narratives.", "num_citations": "14\n", "authors": ["1193"]}
{"title": "Discovering hidden structure in high dimensional human behavioral data via tensor factorization\n", "abstract": " In recent years, the rapid growth in technology has increased the opportunity for longitudinal human behavioral studies. Rich multimodal data, from wearables like Fitbit, online social networks, mobile phones etc. can be collected in natural environments. Uncovering the underlying low-dimensional structure of noisy multi-way data in an unsupervised setting is a challenging problem. Tensor factorization has been successful in extracting the interconnected low-dimensional descriptions of multi-way data. In this paper, we apply non-negative tensor factorization on a real-word wearable sensor data, StudentLife, to find latent temporal factors and group of similar individuals. Meta data is available for the semester schedule, as well as the individuals' performance and personality. We demonstrate that non-negative tensor factorization can successfully discover clusters of individuals who exhibit higher academic performance, as well as those who frequently engage in leisure activities. The recovered latent temporal patterns associated with these groups are validated against ground truth data to demonstrate the accuracy of our framework.", "num_citations": "14\n", "authors": ["1193"]}
{"title": "Modified Cheeger and ratio cut methods using the Ginzburg\u2013Landau functional for classification of high-dimensional data\n", "abstract": " Recent advances in clustering have included continuous relaxations of the Cheeger cut problem and those which address its linear approximation using the graph Laplacian. In this paper, we show how to use the graph Laplacian to solve the fully nonlinear Cheeger cut problem, as well as the ratio cut optimization task. Both problems are connected to total variation minimization, and the related Ginzburg\u2013Landau functional is used in the derivation of the methods. The graph framework discussed in this paper is undirected. The resulting algorithms are efficient ways to cluster the data into two classes, and they can be easily extended to the case of multiple classes, or used on a multiclass data set via recursive bipartitioning. In addition to showing results on benchmark data sets, we also show an application of the algorithm to hyperspectral video data.", "num_citations": "14\n", "authors": ["1193"]}
{"title": "Structural properties of ego networks\n", "abstract": " The structure of real-world social networks in large part determines the evolution of social phenomena, including opinion formation, diffusion of information and influence, and the spread of disease. Globally, network structure is characterized by features such as degree distribution, degree assortativity, and clustering coefficient. However, information about global structure is usually not available to each vertex. Instead, each vertex\u2019s knowledge is generally limited to the locally observable portion of the network consisting of the subgraph over its immediate neighbors. Such subgraphs, known as ego networks, have properties that can differ substantially from those of the global network. In this paper, we study the structural properties of ego networks and show how they relate to the global properties of networks from which they are derived. Through empirical comparisons and mathematical derivations, we\u00a0\u2026", "num_citations": "14\n", "authors": ["1193"]}
{"title": "Interactively Mapping Data Sources into the Semantic Web.\n", "abstract": " The Linked Open Data continues to grow rapidly, but a limitation of much of the data that is being published is the lack of a semantic description. While there are tools that help users to quickly convert a database into RDF, they do not provide a way to easily map the data into an existing ontology. This paper presents an approach that allows users to interactively map their structured sources into an existing ontology and then use that mapping to generate RDF triples. This approach automatically generates a mapping from the data source into the ontology, but since the precise mapping is sometimes ambiguous, we allow the user to interactively refine the mappings. We implemented this approach in a system called Karma, and demonstrate that the system can map sources into an ontology with minimal user interaction and efficiently generate the corresponding RDF.", "num_citations": "14\n", "authors": ["1193"]}
{"title": "Political Partisanship and Anti-Science Attitudes in Online Discussions about Covid-19\n", "abstract": " The novel coronavirus pandemic continues to ravage communities across the US. Opinion surveys identified importance of political ideology in shaping perceptions of the pandemic and compliance with preventive measures. Here, we use social media data to study complexity of polarization. We analyze a large dataset of tweets related to the pandemic collected between January and May of 2020, and develop methods to classify the ideological alignment of users along the moderacy (hardline vs moderate), political (liberal vs conservative) and science (anti-science vs pro-science) dimensions. While polarization along the science and political dimensions are correlated, politically moderate users are more likely to be aligned with the pro-science views, and politically hardline users with anti-science views. Contrary to expectations, we do not find that polarization grows over time; instead, we see increasing activity by moderate pro-science users. We also show that anti-science conservatives tend to tweet from the Southern US, while anti-science moderates from the Western states. Our findings shed light on the multi-dimensional nature of polarization, and the feasibility of tracking polarized opinions about the pandemic across time and space through social media data.", "num_citations": "13\n", "authors": ["1193"]}
{"title": "Characterizing activity on the deep and dark web\n", "abstract": " The deep and darkweb (d2web) refers to limited access web sites that require registration, authentication, or more complex encryption protocols to access them. These web sites serve as hubs for a variety of illicit activities: to trade drugs, stolen user credentials, hacking tools, and to coordinate attacks and manipulation campaigns. Despite its importance to cyber crime, the d2web has not been systematically investigated. In this paper, we study a large corpus of messages posted to 80 d2web forums over a period of more than a year. We identify topics of discussion using LDA and use a non-parametric HMM to model the evolution of topics across forums. Then, we examine the dynamic patterns of discussion and identify forums with similar patterns. We show that our approach surfaces hidden similarities across different forums and can help identify anomalous events in this rich, heterogeneous data.", "num_citations": "13\n", "authors": ["1193"]}
{"title": "RAPTOR: ransomware attack predictor\n", "abstract": " Ransomware, a type of malicious software that encrypts a victim's files and only releases the cryptographic key once a ransom is paid, has emerged as a potentially devastating class of cybercrimes in the past few years. In this paper, we present RAPTOR, a promising line of defense against ransomware attacks. RAPTOR fingerprints attackers' operations to forecast ransomware activity. More specifically, our method learns features of malicious domains by looking at examples of domains involved in known ransomware attacks, and then monitors newly registered domains to identify potentially malicious ones. In addition, RAPTOR uses time series forecasting techniques to learn models of historical ransomware activity and then leverages malicious domain registrations as an external signal to forecast future ransomware activity. We illustrate RAPTOR's effectiveness by forecasting all activity stages of Cerber, a popular ransomware family. By monitoring zone files of the top-level domain .top starting from August 30, 2016 through May 31, 2017, RAPTOR predicted 2,126 newly registered domains to be potential Cerber domains. Of these, 378 later actually appeared in blacklists. Our empirical evaluation results show that using predicted domain registrations helped improve forecasts of future Cerber activity. Most importantly, our approach demonstrates the value of fusing different signals in forecasting applications in the cyber domain.", "num_citations": "13\n", "authors": ["1193"]}
{"title": "Democratizing data science through data science training\n", "abstract": " The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific\u00a0\u2026", "num_citations": "13\n", "authors": ["1193"]}
{"title": "Using conditional random fields to exploit token structure and labels for accurate semantic annotation\n", "abstract": " Automatic semantic annotation of structured data enables unsupervised integration of data from heterogeneous sources but is difficult to perform accurately due to the presence of many numeric fields and proper-noun fields that do not allow reference-based approaches and the absence of natural language text that prevents the use of language-based approaches. In addition, several of these semantic types have multiple heterogeneous representations, while sharing syntactic structure with other types. In this work, we propose a new approach to use conditional random fields (CRFs) to perform semantic annotation of structured data that takes advantage of the structure and labels of the tokens for higher accuracy of field labeling, while still allowing the use of exact inference techniques. We compare our approach with a linear-CRF based model that only labels fields and also with a regular-expression based approach.", "num_citations": "13\n", "authors": ["1193"]}
{"title": "Modeling evolution of topics in large-scale temporal text corpora\n", "abstract": " Large text temporal collections provide insights into social and cultural change over time. To quantify changes in topics in these corpora, embedding methods have been used as a diachronic tool. However, they have limited utility for modeling changes in topics due to the stochastic nature of training. We propose a new computational approach for tracking and detecting temporal evolution of topics in a large collection of texts. This approach for identifying dynamic topics and modeling their evolution combines the advantages of two methods:(1) word embeddings to learn contextual semantic representation of words from temporal snapshots of the data and (2) dynamic network analysis to identify dynamic topics by using dynamic semantic similarity networks developed using embedding models. Experimenting with two large temporal data sets from the legal and real estate domains, we show that this approach performs faster (due to parallelizing different snapshots), uncovers more coherent topics (compared to available dynamic topic modeling approaches), and effectively enables modeling evolution leveraging the network structure.", "num_citations": "12\n", "authors": ["1193"]}
{"title": "Graph Filters and the Z-Laplacian\n", "abstract": " In network science, the interplay between dynamical processes and the underlying topologies of complex systems has led to a diverse family of models with different interpretations. In graph signal processing, this is manifested in the form of different graph shifts and their induced algebraic systems. In this paper, we propose the unifying Z-Laplacian framework, whose instances can act as graph shift operators. As a generalization of the traditional graph Laplacian, the Z-Laplacian spans the space of all possible Z -matrices, i.e., real square matrices with nonpositive off-diagonal entries. We show that the Z -Laplacian can model general continuous-time dynamical processes, including information flows and epidemic spreading on a given graph. It is also closely related to general nonnegative graph filters in the discrete time domain. We showcase its flexibility by considering two applications. First, we consider a\u00a0\u2026", "num_citations": "12\n", "authors": ["1193"]}
{"title": "Neighbor-neighbor correlations explain measurement bias in networks\n", "abstract": " In numerous physical models on networks, dynamics are based on interactions that exclusively involve properties of a node\u2019s nearest neighbors. However, a node\u2019s local view of its neighbors may systematically bias perceptions of network connectivity or the prevalence of certain traits. We investigate the strong friendship paradox, which occurs when the majority of a node\u2019s neighbors have more neighbors than does the node itself. We develop a model to predict the magnitude of the paradox, showing that it is enhanced by negative correlations between degrees of neighboring nodes. We then show that by including neighbor-neighbor correlations, which are degree correlations one step beyond those of neighboring nodes, we accurately predict the impact of the strong friendship paradox in real-world networks. Understanding how the paradox biases local observations can inform better measurements of network\u00a0\u2026", "num_citations": "12\n", "authors": ["1193"]}
{"title": "Leaders and negotiators: An influence-based metric for rank\n", "abstract": " We propose influence as a measure of the centrality of nodes in a network. Influence takes into account not only direct links but also all paths between nodes. We parametrize the influence metric by a variable alpha that measures the strength of links. Variations in rankings as alpha changes provides a mechanism to identify the central nodes within communities (leaders), as well as nodes that act as bridges between communities (negotiators).", "num_citations": "12\n", "authors": ["1193"]}
{"title": "Wrapper Maintenance.\n", "abstract": " A Web wrapper is a software application that extracts information from a semi-structured source and converts it to a structured format. While semi-structured sources, such as Web pages, contain no explicitly specified schema, they do have an implicit grammar that can be used to identify relevant information in the document. A wrapper learning system analyzes page layout to generate either grammar-based or \u201clandmark\u201d-based extraction rules that wrappers use to extract data. As a consequence, even slight changes in the page layout can break the wrapper and prevent it from extracting data correctly. Wrapper maintenance is a composite task that (1) verifies that the wrapper continues to extract data correctly from a source, and (2) repairs the wrapper so that it works on the changed pages.", "num_citations": "12\n", "authors": ["1193"]}
{"title": "A stochastic model of platoon formation in traffic flow\n", "abstract": " In this paper we study formation of platoons and their velocity\u2013size distribution in freeway traffic using the stochastic Master equation approach. The solution to the Master equation and the moments of the distribution are obtained by numeric integration. We also discuss a possible generalization of this approach to the case of a multi\u2013lane traffic flow and propose a microscopic, particle\u2013hopping model for simulations.", "num_citations": "12\n", "authors": ["1193"]}
{"title": "A geometric solution to fair representations\n", "abstract": " To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and% the methodology cannot easily extend other algorithms they are not easily transferable across models%(eg, methods to reduce bias in random forests cannot be extended to neural networks). To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction\u00a0\u2026", "num_citations": "11\n", "authors": ["1193"]}
{"title": "Collaboration drives individual productivity\n", "abstract": " How does the number of collaborators affect individual productivity? Results of prior research have been conflicting, with some studies reporting an increase in individual productivity as the number of collaborators grows, while other studies showing that the free-rider effect skews the effort invested by individuals, making larger groups less productive. The difference between these schools of thought is substantial: if a super-scaling effect exists, as suggested by former studies, then as groups grow, their productivity will increase even faster than their size, super-linearly improving their efficiency. We address this question by studying two planetary-scale collaborative systems: GitHub and Wikipedia. By analyzing the activity of over 2 million users on these platforms, we discover that the interplay between group size and productivity exhibits complex, previously-unobserved dynamics: the productivity of smaller groups\u00a0\u2026", "num_citations": "11\n", "authors": ["1193"]}
{"title": "Finding prerequisite relations using the wikipedia clickstream\n", "abstract": " The increased availability of online learning resources in the form of courses, videos, and tutorials has created new opportunities for independent learners, but it has also increased the difficulty of planning a course of study. Where should the learner start? What should the learner know before tackling a new course? Manually identifying these prerequisite relations between learning resources or concepts is expensive in terms of time and expertise, and it is particularly difficult to do so for new or rapidly changing areas of knowledge. To address this challenge, we present a new method for identifying prerequisite relations based on naturally occurring data, namely the navigation patterns of users on the Wikipedia online encyclopedia. Our supervised learning approach shows that the navigation network structure can be used to identify dependencies among concepts in several domains.", "num_citations": "11\n", "authors": ["1193"]}
{"title": "Using Simpson's Paradox to Discover Interesting Patterns in Behavioral Data\n", "abstract": " We describe a data-driven discovery method that leverages Simpson's paradox to uncover interesting patterns in behavioral data. Our method systematically disaggregates data to identify subgroups within a population whose behavior deviates significantly from the rest of the population. Given an outcome of interest and a set of covariates, the method follows three steps. First, it disaggregates data into subgroups, by conditioning on a particular covariate, so as minimize the variation of the outcome within the subgroups. Next, it models the outcome as a linear function of another covariate, both in the subgroups and in the aggregate data. Finally, it compares trendsto identify disaggregations that produce subgroups with different behaviors from the aggregate. We illustrate the method by applying it to three real-world behavioral datasets, including Q\\&A site Stack Exchange and online learning platforms Khan Academy and Duolingo.", "num_citations": "11\n", "authors": ["1193"]}
{"title": "Network flows and the link prediction problem\n", "abstract": " Link prediction is used by many applications to recommend new products or social connections to people. Link prediction leverages information in network structure to identify missing links or predict which new one will form in the future. Recent research has provided a theoretical justification for the success of some popular link prediction heuristics, such as the number of common neighbors and the Adamic-Adar score, by showing that they estimate the distance between nodes in some latent feature space. In this paper we examine the link prediction task from the novel perspective of network flows. We show that how easily two nodes can interact with or influence each other depends not only on their position in the network, but also on the nature of the flow that mediates interactions between them. We show that different types of flows lead to different notions of network proximity, some of which are mathematically\u00a0\u2026", "num_citations": "11\n", "authors": ["1193"]}
{"title": "Massive multi-agent data-driven simulations of the github ecosystem\n", "abstract": " Simulating and predicting planetary-scale techno-social systems poses heavy computational and modeling challenges. The DARPA SocialSim program set the challenge to model the evolution of GitHub, a large collaborative software-development ecosystem, using massive multi-agent simulations. We describe our best performing models and our agent-based simulation framework, which we are currently extending to allow simulating other planetary-scale techno-social systems. The challenge problem measured participant\u2019s ability, given 30\u00a0months of meta-data on user activity on GitHub, to predict the next months\u2019 activity as measured by a broad range of metrics applied to ground truth, using agent-based simulation. The challenge required scaling to a simulation of roughly 3 million agents producing a combined 30 million actions, acting on 6 million repositories with commodity hardware. It was also\u00a0\u2026", "num_citations": "10\n", "authors": ["1193"]}
{"title": "Degree correlations amplify the growth of cascades in networks\n", "abstract": " Networks facilitate the spread of cascades, allowing a local perturbation to percolate via interactions between nodes and their neighbors. We investigate how network structure affects the dynamics of a spreading cascade. By accounting for the joint degree distribution of a network within a generating function framework, we can quantify how degree correlations affect both the onset of global cascades and the propensity of nodes of specific degree class to trigger large cascades. However, not all degree correlations are equally important in a spreading process. We introduce a new measure of degree assortativity that accounts for correlations among nodes relevant to a spreading cascade. We show that the critical point defining the onset of global cascades has a monotone relationship to this new assortativity measure. In addition, we show that the choice of nodes to seed the largest cascades is strongly affected by\u00a0\u2026", "num_citations": "10\n", "authors": ["1193"]}
{"title": "Mathematical analysis of multi-agent systems\n", "abstract": " We review existing approaches to mathematical modeling and analysis of multi-agent systems in which complex collective behavior arises out of local interactions between many simple agents. Though the behavior of an individual agent can be considered to be stochastic and unpredictable, the collective behavior of such systems can have a simple probabilistic description. We show that a class of mathematical models that describe the dynamics of collective behavior of multi-agent systems can be written down from the details of the individual agent controller. The models are valid for Markov or memoryless agents, in which each agents future state depends only on its present state and not any of the past states. We illustrate the approach by analyzing in detail applications from the robotics domain: collaboration and foraging in groups of robots.", "num_citations": "10\n", "authors": ["1193"]}
{"title": "Moral framing and ideological bias of news\n", "abstract": " News outlets are a primary source for many people to learn what is going on in the world. However, outlets with different political slants, when talking about the same news story, usually emphasize various aspects and choose their language framing differently. This framing implicitly shows their biases and also affects the reader\u2019s opinion and understanding. Therefore, understanding the framing in the news stories is fundamental for realizing what kind of view the writer is conveying with each news story. In this paper, we describe methods for characterizing moral frames in the news. We capture the frames based on the Moral Foundation Theory. This theory is a psychological concept which explains how every kind of morality and opinion can be summarized and presented with five main dimensions. We propose an unsupervised method that extracts the framing Bias and the framing Intensity without any\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Language, demographics, emotions, and the structure of online social networks\n", "abstract": " Social networks affect individuals\u2019 economic opportunities and well-being. However, few of the factors thought to shape networks\u2014culture, language, education, and income\u2014were empirically validated at scale. To fill this gap, we collected a large number of social media posts from a major US metropolitan area. By associating these posts with US Census tracts through their locations, we linked socioeconomic indicators to group-level signals extracted from social media, including emotions, language, and online social ties. Our analysis shows that tracts with higher education levels have weaker social ties, but this effect is attenuated for tracts with high ratio of Hispanic residents. Negative emotions are associated with more frequent online interactions, or stronger social ties, while positive emotions are associated with weaker ties. These results hold for both Spanish and English tweets, evidencing that\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Twitter session analytics: Profiling users\u2019 short-term behavioral changes\n", "abstract": " Human behavior shows strong daily, weekly, and monthly patterns. In this work, we demonstrate online behavioral changes that occur on a much smaller time scale: minutes, rather than days or weeks. Specifically, we study how people distribute their effort over different tasks during periods of activity on the Twitter social platform. We demonstrate that later in a session on Twitter, people prefer to perform simpler tasks, such as replying and retweeting others\u2019 posts, rather than composing original messages, and they also tend to post shorter messages. We measure the strength of this effect empirically and statistically using mixed-effects models, and find that the first post of a session is up\u00a0to 25\u00a0% more likely to be a composed message, and 10\u201320\u00a0% less likely to be a reply or retweet. Qualitatively, our results hold for different populations of Twitter users segmented by how active and well-connected they are\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Information-theoretic clustering of neuroimaging metrics related to cognitive decline in the elderly\n", "abstract": " As Alzheimer\u2019s disease progresses, there are changes in metrics of brain atrophy and network breakdown derived from anatomical or diffusion MRI. Neuroimaging biomarkers of cognitive decline are crucial to identify, but few studies have investigated how sets of biomarkers cluster in terms of the information they provide. Here, we evaluated more than 700 frequently studied diffusion and anatomical measures in 247 elderly participants from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI). We used a novel unsupervised machine learning technique - CorEx - to identify groups of measures with high multivariate mutual information; we computed latent factors to explain correlations among them. We visualized groups of measures discovered by CorEx in a hierarchical structure and determined how well they predict cognitive decline. Clusters of variables significantly predicted cognitive decline\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Identifying transformative scientific research\n", "abstract": " Transformative research refers to research that shifts or disrupts established scientific paradigms. Notable examples include the discovery of high-temperature superconductivity that disrupted the theory established 30 years ago. Identifying potential transformative research early and accurately is important for funding agencies to maximize the impact of their investments. It also helps scientists identify and focus their attention on promising emerging works. This paper presents a data driven approach where citation patterns of scientific papers are analyzed to quantify how much a potential challenger idea shifts an established paradigm. The key idea is that transformative research creates an observable disruption in the structure of \"information cascades,\" chains of references that can be traced back to the papers establishing some scientific paradigm. Such a disruption is visible soon after the challenger's introduction\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Dynamics of a collaborative rating system\n", "abstract": " The rise of social media sites, such as blogs, wikis, Digg and Flickr among others, underscores a transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. The social news aggregator Digg allows users to submit links to and vote on news stories. Like other social media sites, Digg also allows users to designate others as \u201cfriends\u201d and easily track friends\u2019 activities: what new stories they submitted, commented on or liked. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page. We construct two mathematical models of collaborative decision-making on Digg. First, we study how collective rating of news stories emerges from the decisions made by many users. The model takes\u00a0\u2026", "num_citations": "9\n", "authors": ["1193"]}
{"title": "A systematic approach to model-guided empirical search for memory hierarchy optimization\n", "abstract": " The goal of this work is a systematic approach to compiler optimization for simultaneously optimizing across multiple levels of the memory hierarchy. Our approach combines compiler models and heuristics with guided empirical search to take advantage of their complementary strengths. The models and heuristics limit the search to a small number of candidate implementations, and the empirical results provide accurate feedback information to the compiler. In previous work, we propose a compiler algorithm for deriving a set of parameterized solutions, followed by a model-guided empirical search to determine the best integer parameter values and select the best overall solution. This paper focuses on formalizing the process of deriving parameter values, which is a multi-variable optimization problem, and considers the role of AI search techniques in deriving a systematic framework for the search.", "num_citations": "9\n", "authors": ["1193"]}
{"title": "A machine learning approach to accurately and reliably extracting data from the web\n", "abstract": " A critical problem in developing information agents for the Web is accessing data that is formatted for human use. We have developed a set of tools for extracting data from web sites and transforming it into a structured data format, such as XML. The resulting data can then be used to build new applications without having to deal with unstructured data. The advantages of our wrapping technology over previous work are the the ability to learn highly accurate extraction rules, to verify the wrapper to ensure that the correct data continues to be extracted, and to automatically adapt to changes in the sites from which the data is being extracted.", "num_citations": "9\n", "authors": ["1193"]}
{"title": "Systematizing Confidence in Open Research and Evidence (SCORE)\n", "abstract": " Assessing the credibility of research claims is a central, continuous, and laborious part of the scientific process. Credibility assessment strategies range from expert judgment to aggregating existing evidence to systematic replication efforts. Such assessments can require substantial time and effort. Research progress could be accelerated if there were rapid, scalable, accurate credibility indicators to guide attention and resource allocation for further assessment. The SCORE program is creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools, teams are creating: a database of claims from papers in the social and behavioral sciences; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates. Beyond the primary research objective, the data and artifacts generated from this program will be openly shared and provide an unprecedented opportunity to examine research credibility and evidence.", "num_citations": "8\n", "authors": ["1193"]}
{"title": "Diffusion in social networks: Effects of monophilic contagion, friendship paradox, and reactive networks\n", "abstract": " We consider SIS diffusion processes over networks, where a classical assumption is that individuals\u2019 decisions to adopt a contagion are based on their immediate neighbors. However, recent literature shows that some attributes are more correlated between two-hop neighbors, a concept referred to as  monophily . This motivates us to explore monophilic contagion, the case where a contagion (e.g., a product, disease) is adopted by considering two-hop neighbors instead of immediate neighbors (e.g., you ask your friend about the new iPhone and she recommends you the opinion of one of her friends). We show that the phenomenon named  friendship paradox  makes it easier for the monophilic contagion to spread widely. We also consider the case where the underlying network stochastically evolves in response to the state of the contagion (e.g., depending on the severity of a flu virus, people restrict their\u00a0\u2026", "num_citations": "8\n", "authors": ["1193"]}
{"title": "BD2K ERuDIte: The educational resource discovery index for data science\n", "abstract": " The field of data science has developed over the years to enable the efficient integration and analysis of the increasingly large amounts of data being generated across many domains, ranging from social media, to sensor networks, to scientific experiments. Numerous subfields of biology and medicine, such as genetics, neuroimaging, and mobile health, are witnessing a data explosion that promises to revolutionize biomedical science by yielding novel insights and discoveries. To address the challenges posed by biomedical big data, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative (datascience. nih. gov). An important component of this effort is the training of biomedical researchers. To this end, the NIH has funded the BD2K Training Coordinating Center (TCC). A core activity of the BD2K TCC is to develop a web portal (bigdatau. org) to provide personalized training in\u00a0\u2026", "num_citations": "8\n", "authors": ["1193"]}
{"title": "Structural and cognitive bottlenecks to information access in social networks\n", "abstract": " Information in networks is non-uniformly distributed, enabling individuals in certain network positions to get preferential access to information. Social scientists have developed influential theories about the role of network structure in information access. These theories were validated through numerous studies, which examined how individuals leverage their social networks for competitive advantage, such as a new job or higher compensation. It is not clear how these theories generalize to online networks, which differ from real-world social networks in important respects, including asymmetry of social links. We address this problem by analyzing how users of the social news aggregator Digg adopt stories recommended by friends, ie, users they follow. We measure the impact different factors, such as network position and activity rate; have on access to novel information, which in Digg's case means set of distinct news\u00a0\u2026", "num_citations": "8\n", "authors": ["1193"]}
{"title": "How visibility and divided attention constrain social contagion\n", "abstract": " How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, ie, a tweet (message) from a friend, and reveals that% retweeting behavior is constrained by a few simple principles. the\" principle of least effort\" combined with limited attention plays a dominant role in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter stream. Moreover, our\u00a0\u2026", "num_citations": "8\n", "authors": ["1193"]}
{"title": "Estimating individualized daily self-reported affect with wearable sensors\n", "abstract": " The following topics are dealt with: health care; medical information systems; learning (artificial intelligence); diseases; data mining; medical computing; electronic health records; text analysis; data analysis; natural language processing.", "num_citations": "7\n", "authors": ["1193"]}
{"title": "Network vector: distributed representations of networks with global context\n", "abstract": " We propose a neural embedding algorithm called Network Vector, which learns distributed representations of nodes and the entire networks simultaneously. By embedding networks in a low-dimensional space, the algorithm allows us to compare networks in terms of structural similarity and to solve outstanding predictive problems. Unlike alternative approaches that focus on node level features, we learn a continuous global vector that captures each node's global context by maximizing the predictive likelihood of random walk paths in the network. Our algorithm is scalable to real world graphs with many nodes. We evaluate our algorithm on datasets from diverse domains, and compare it with state-of-the-art techniques in node classification, role discovery and concept analogy tasks. The empirical results show the effectiveness and the efficiency of our algorithm.", "num_citations": "7\n", "authors": ["1193"]}
{"title": "Capturing the interplay of dynamics and networks through parameterizations of Laplacian operators\n", "abstract": " We study the interplay between a dynamical process and the structure of the network on which it unfolds using the parameterized Laplacian framework. This framework allows for defining and characterizing an ensemble of dynamical processes on a network beyond what the traditional Laplacian is capable of modeling. This, in turn, allows for studying the impact of the interaction between dynamics and network topology on the quality-measure of network clusters and centrality, in order to effectively identify important vertices and communities in the network. Specifically, for each dynamical process in this framework, we define a centrality measure that captures a vertex\u2019s participation in the dynamical process on a given network and also define a function that measures the quality of every subset of vertices as a potential cluster (or community) with respect to this process. We show that the subset-quality function generalizes the traditional conductance measure for graph partitioning. We partially justify our choice of the quality function by showing that the classic Cheeger\u2019s inequality, which relates the conductance of the best cluster in a network with a spectral quantity of its Laplacian matrix, can be extended to the parameterized Laplacian. The parameterized Laplacian framework brings under the same umbrella a surprising variety of dynamical processes and allows us to systematically compare the different perspectives they create on network structure.", "num_citations": "7\n", "authors": ["1193"]}
{"title": "The myopia of crowds: A study of collective evaluation on stack exchange\n", "abstract": " Crowds can often make better decisions than individuals or small groups of experts by leveraging their ability to aggregate diverse information. Question answering sites, such as Stack Exchange, rely on the \u201cwisdom of crowds\u201d effect to identify the best answers to questions asked by users. We analyze data from 250 communities on the Stack Exchange network to pinpoint factors affecting which answers are chosen as the best answers. Our results suggest that, rather than evaluate all available answers to a question, users rely on simple cognitive heuristics to choose an answer to vote for or accept. These cognitive heuristics are linked to an answer\u2019s salience, such as the order in which it is listed and how much screen space it occupies. While askers appear to depend more on heuristics, compared to voting users, when choosing an answer to accept as the most helpful one, voters use acceptance itself as a heuristic: they are more likely to choose the answer after it is accepted than before that very same answer was accepted. These heuristics become more important in explaining and predicting behavior as the number of available answers increases. Our findings suggest that crowd judgments may become less reliable as the number of answers grow.", "num_citations": "7\n", "authors": ["1193"]}
{"title": "User effort and network structure mediate access to information in networks\n", "abstract": " Individuals' access to information in a social network depends on its distributed and where in the network individuals position themselves. However, individuals have limited capacity to manage their social connections and process information. In this work, we study how this limited capacity and network structure interact to affect the diversity of information social media users receive. Previous studies of the role of networks in information access were limited in their ability to measure the diversity of information. We address this problem by learning the topics of interest to social media users by observing messages they share online with their followers. We present a probabilistic model that incorporates human cognitive constraints in a generative model of information sharing. We then use the topics learned by the model to measure the diversity of information users receive from their social media contacts. We confirm that users in structurally diverse network positions, which bridge otherwise disconnected regions of the follower graph, are exposed to more diverse information. In addition, we identify user effort as an important variable that mediates access to diverse information in social media. Users who invest more effort into their activity on the site not only place themselves in more structurally diverse positions within the network than the less engaged users, but they also receive more diverse information when located in similar network positions. These findings indicate that the relationship between network structure and access to information in networks is more nuanced than previously thought.", "num_citations": "7\n", "authors": ["1193"]}
{"title": "Social informatics: Using Big Data to understand social behavior\n", "abstract": " Online social media has emerged as a critical factor in information dissemination, search, marketing, expertise and influence discovery, and potentially an important tool for mobilizing people. It has also given researchers access to massive quantities of social data for empirical analysis. These data sets offer a rich source of evidence for studying dynamics of individual and group behavior, the structure of networks and global patterns of the flow of information on them. However, in most previous studies, the structure of the underlying networks was not directly visible but had to be inferred from the flow of information from one individual to another. As a result, we do not yet understand dynamics of information spread on networks or how the structure of the network affects it. We analyze data from two popular social news sites, Digg and Twitter, to understand the mechanisms of information diffusion in social\u00a0\u2026", "num_citations": "7\n", "authors": ["1193"]}
{"title": "Follow the leader: Documents on the leading edge of semantic change get more citations\n", "abstract": " Diachronic word embeddings\u2014vector representations of words over time\u2014offer remarkable insights into the evolution of language and provide a tool for quantifying sociocultural change from text documents. Prior work has used such embeddings to identify shifts in the meaning of individual words. However, simply knowing that a word has changed in meaning is insufficient to identify the instances of word usage that convey the historical meaning or the newer meaning. In this study, we link diachronic word embeddings to documents, by situating those documents as leaders or laggards with respect to ongoing semantic changes. Specifically, we propose a novel method to quantify the degree of semantic progressiveness in each word usage, and then show how these usages can be aggregated to obtain scores for each document. We analyze two large collections of documents, representing legal opinions and\u00a0\u2026", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Predictability limit of partially observed systems\n", "abstract": " Applications from finance to epidemiology and cyber-security require accurate forecasts of dynamic phenomena, which are often only partially observed. We demonstrate that a system\u2019s predictability degrades as a function of temporal sampling, regardless of the adopted forecasting model. We quantify the loss of predictability due to sampling, and show that it cannot be recovered by using external signals. We validate the generality of our theoretical findings in real-world partially observed systems representing infectious disease outbreaks, online discussions, and software development projects. On a variety of prediction tasks\u2014forecasting new infections, the popularity of topics in online discussions, or interest in cryptocurrency projects\u2014predictability irrecoverably decays as a function of sampling, unveiling predictability limits in partially observed systems.", "num_citations": "6\n", "authors": ["1193"]}
{"title": "White Paper: DEEP FAKERY\u2014An Action Plan\n", "abstract": " This white paper came out of an exploratory workshop held on November 15-16, 2019 at the Institute for Pure and Applied Mathematics at UCLA. Represented at the workshop were members of the mathematics, machine learning, cryptography, philosophy, social science, legal, and policy communities. Discussion at the workshop focused on the impact of deep fakery and how to respond to it. The opinions expressed in this white paper represent those of the individuals involved, and not of their organizations or of the Institute for Pure and Applied Mathematics.\u201cDeep fake\u201d technology represents a substantial advance on earlier technologies of image, audio, and video manipulation like photoshopping. It emerged from the recent deep learning revolution, especially the development of generative adversarial networks. It enables the efficient, computer-assisted production of highly believable audio and video in which real people appear to be saying things they never said and doing things they never did.", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Predicting and explaining behavioral data with structured feature space decomposition\n", "abstract": " Modeling human behavioral data is challenging due to its scale, sparseness (few observations per individual), heterogeneity (differently behaving individuals), and class imbalance (few observations of the outcome of interest). An additional challenge is learning an interpretable model that not only accurately predicts outcomes, but also identifies important factors associated with a given behavior. To address these challenges, we describe a statistical approach to modeling behavioral data called the structured sum-of-squares decomposition (S3D). The algorithm, which is inspired by decision trees, selects important features that collectively explain the variation of the outcome, quantifies correlations between the features, and bins the subspace of important features into smaller, more homogeneous blocks that correspond to similarly-behaving subgroups within the population. This partitioned subspace allows us to\u00a0\u2026", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Tensor embedding: a supervised framework for human behavioral data mining and prediction\n", "abstract": " Today's densely instrumented world offers tremendous opportunities for continuous acquisition and analysis of multimodal sensor data providing temporal characterization of an individual's behaviors. Is it possible to efficiently couple such rich sensor data with predictive modeling techniques to provide contextual, and insightful assessments of individual performance and wellbeing? Prediction of different aspects of human behavior from these noisy, incomplete, and heterogeneous bio-behavioral temporal data is a challenging problem, beyond unsupervised discovery of latent structures. We propose a Supervised Tensor Embedding (STE) algorithm for high dimension multimodal data with join decomposition of input and target variable. Furthermore, we show that features selection will help to reduce the contamination in the prediction and increase the performance. The efficiently of the methods was tested via two different real world datasets.", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Effort mediates access to information in online social networks\n", "abstract": " Individuals\u2019 access to information in a social network depends on how it is distributed and where in the network individuals position themselves. In addition, individuals vary in how much effort they invest in managing their social connections. Using data from a social media site, we study how the interplay between effort and network position affects social media users\u2019 access to diverse and novel information. Previous studies of the role of networks in information access were limited in their ability to measure the diversity of information. We address this problem by learning the topics of interest to social media users from the messages they share online with followers. We use the learned topics to measure the diversity of information users receive from the people they follow online. We confirm that users in structurally diverse network positions, which bridge otherwise disconnected regions of the follower network, tend to\u00a0\u2026", "num_citations": "6\n", "authors": ["1193"]}
{"title": "A probabilistic approach to mining geospatial knowledge from social annotations\n", "abstract": " Knowledge produced online often comes in the form of free-text labels, known as tags, with which users annotate the content they create, such as photos and videos. Increasingly, such content is also georeferenced, i.e., it is associated with geographic coordinates. The implicit relationships between tags and their locations can tell us much about how people conceptualize places and relations between them. However, extracting such knowledge from social annotations presents many challenges, since annotations are often ambiguous, noisy, uncertain and spatially inhomogeneous. We introduce a probabilistic framework for modeling georeferenced annotations and a method for learning model parameters from data. The framework is flexible and general, and can be used in a variety of applications that mine geospatial knowledge from user generated content. Specifically, we study two problems --- extracting place\u00a0\u2026", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Minority games and distributed coordination in non-stationary environments\n", "abstract": " We examine emergent coordination in a network of competing Boolean agents. The agents play a so called generalized minority game where the capacity level is allowed to vary externally. We study the properties of such a system for different values of the mean connectivity K of the network, and show that the system with K = 2 shows a high degree of coordination for relatively large variations of the capacity level. We also show that for K > 2 coordination can be achieved by tuning the homogeneity parameter of the agents' Boolean strategies.", "num_citations": "6\n", "authors": ["1193"]}
{"title": "Learning Behavioral Representations from Wearable Sensors\n", "abstract": " Continuous collection of physiological data from wearable sensors enables temporal characterization of individual behaviors. Understanding the relation between an individual\u2019s behavioral patterns and psychological states can help identify strategies to improve quality of life. One challenge in analyzing physiological data is extracting the underlying behavioral states from the temporal sensor signals and interpreting them. Here, we use a non-parametric Bayesian approach to model sensor data from multiple people and discover the dynamic behaviors they share. We apply this method to data collected from sensors worn by a population of hospital workers and show that the learned states can cluster participants into meaningful groups and better predict their cognitive and psychological states. This method offers a way to learn interpretable compact behavioral representations from multivariate sensor signals.", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Discovering latent psychological structures from self-report assessments of hospital workers\n", "abstract": " Hospitals are high-stress environments where workers face a high risk of occupational burnout due to a mix of imbalanced schedules, understaffing, and emotional stress. In this paper, we propose a computational framework to infer the latent psychological makeup and traits of hospital workers. We apply machine learning models to psychometric data obtained from a suite of psychological survey instruments, collected as a part of TILES, a ten-week research study carried out in a large Los Angeles hospital. The study population represents over 200 hospital employees, including nurses and those in administrative positions. A computational framework that combines clustering and non-negative matrix factorization was used to extract the latent interplay between psychological constructs along dimensions of health, affect, personality, cognitive ability, and job performance. We illustrate how the proposed framework\u00a0\u2026", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Quantifying the Impact of Cognitive Biases in Question-Answering Systems\n", "abstract": " Crowdsourcing can identify high-quality solutions to problems; however, individual decisions are constrained by cognitive biases. We investigate some of these biases in an experimental model of a question-answering system. We observe a strong position bias in favor of answers appearing earlier in a list of choices. This effect is enhanced by three cognitive factors: the attention an answer receives, its perceived popularity, and cognitive load, measured by the number of choices a user has to process. While separately weak, these effects synergistically amplify position bias and decouple user choices of best answers from their intrinsic quality. We end our paper by discussing the novel ways we can apply these findings to substantially improve how high-quality answers are found in question-answering systems.", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Emotions, demographics and sociability in Twitter\n", "abstract": " The social connections people form online affect the quality of information they receive and their online experience. Although a host of socioeconomic and cognitive factors were implicated in the formation of offline social ties, few of them have been empirically validated, particularly in an online setting. In this study, we analyze a large corpus of georeferenced messages, or tweets, posted by social media users from a major US metropolitan area. We linked these tweets to US Census data through their locations. This allowed us to measure emotions expressed in the tweets posted from an area, the structure of social connections, and also use that area\u2019s socioeconomic characteristics in analysis. We find that at an aggregate level, places where social media users engage more deeply with less diverse social contacts are those where they express more negative emotions, like sadness and anger. Demographics also has an impact: these places have residents with lower household income and education levels. Conversely, places where people engage less frequently but with diverse contacts have happier, more positive messages posted from them and also have better educated, younger, more affluent residents. Results suggest that cognitive factors and offline characteristics affect the quality of online interactions. Our work highlights the value of linking social media data to traditional data sources, such as US Census, to drive novel analysis of online behavior.", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Classifying message content based on rebroadcast diversity\n", "abstract": " A computer system running a program of instructions may classify content of a message. The message may be re-broadcasted in whole or in part by one or more re-broadcasters. An amount of time interval diversity may be determined in the time intervals between each successive pair of re-broadcasted messages. An amount of re-broadcaster diversity may be determined in the number of times the message has been re-broadcasted by each of the re-broadcasters. The content of the message may be classified based on the amount of time interval diversity and the amount of re-broadcaster diversity.", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Mining geospatial knowledge on the social web\n", "abstract": " Up-to-date geospatial information can help crisis management community to coordinate its response. In addition to data that is created and curated by experts, there is an abundance of user-generated, user-curated data on Social Web sites such as Flickr, Twitter, and Google Earth. User-generated data and metadata can be used to harvest knowledge, including geospatial knowledge that will help solve real-world problems including information discovery, geospatial information integration and data management. This paper proposes a method for acquiring geospatial knowledge in the form of places and relations between them from the user-generated data and metadata on the Social Web. The key to acquiring geospatial knowledge from social metadata is the ability to accurately represent places. The authors describe a simple, efficient algorithm for finding a non-convex boundary of a region from a sample of\u00a0\u2026", "num_citations": "5\n", "authors": ["1193"]}
{"title": "Mapping Moral Valence of Tweets Following the Killing of George Floyd\n", "abstract": " The viral video documenting the killing of George Floyd by Minneapolis police officer Derek Chauvin inspired nation-wide protests that brought national attention to widespread racial injustice and biased policing practices towards black communities in the United States. The use of social media by the Black Lives Matter movement was a primary route for activists to promote the cause and organize over 1,400 protests across the country. Recent research argues that moral discussions on social media are a catalyst for social change. This study sought to shed light on the moral dynamics shaping Black Lives Matter Twitter discussions by analyzing over 40,000 Tweets geo-located to Los Angeles. The goal of this study is to (1) develop computational techniques for mapping the structure of moral discourse on Twitter and (2) understand the connections between social media activism and protest.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Can badges foster a more welcoming culture on Q&A boards?\n", "abstract": " Thriving online communities rely on a steady stream of newcomers to contribute new content. However, retaining newcomers has proven challenging. In this paper, we measure the success of an intervention used by Stack Exchange question-answering communities to create a more welcoming environment for newcomers. That intervention consisted in highlighting contributions by new users with a special indicator. We hypothesize that Stack Exchange's new policy would reduce negative reactions to new users and, ultimately, increase new user retention. We leverage causal modeling to assess the introduction of the so-called \u201cnew contributor indicator\u201d, and we find it did not counter user retention decline in the short-and long-terms. However, our results indicate it did reduce unwelcoming reactions towards newcomers in the short-term. Our work has practical implications for online community managers aiming to improve their onboarding processes.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Towards quantifying sampling bias in network inference\n", "abstract": " Relational inference leverages relationships between entities and links in a network to infer information about the network from a small sample. This method is often used when global information about the network is not available or difficult to obtain. However, how reliable is inference from a small labeled sample How should the network be sampled, and what effect does it have on inference error How does the structure of the network impact the sampling strategy We address these questions by systematically examining how network sampling strategy and sample size affect accuracy of relational inference in networks. To this end, we generate a family of synthetic networks where nodes have a binary attribute and a tunable level of homophily. As expected, we find that in heterophilic networks, we can obtain good accuracy when only small samples of the network are initially labeled, regardless of the sampling\u00a0\u2026", "num_citations": "4\n", "authors": ["1193"]}
{"title": "On quitting: Performance and practice in online game play\n", "abstract": " We study the relationship between performance and practice by analyzing the activity of many players of a casual online game. We find significant heterogeneity in the improvement of player performance, given by score, and address this by dividing players into similar skill levels and segmenting each player's activity into sessions, that is, sequence of game rounds without an extended break. After disaggregating data, we find that performance improves with practice across all skill levels. More interestingly, players are more likely to end their session after an especially large improvement, leading to a peak score in their very last game of a session. In addition, success is strongly correlated with a lower quitting rate when the score drops, and only weakly correlated with skill, in line with psychological findings about the value of persistence and \u201cgrit:\u201d successful players are those who persist in their practice despite lower scores. Finally, we train an \u03b5-machine, a type of hidden Markov model, and find a plausible mechanism of game play that can predict player performance and quitting the game. Our work raises the possibility of real-time assessment and behavior prediction that can be used to optimize human performance.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Mining social semantics on the social web\n", "abstract": " In recent years the amount of data available on the social web has grown massively. Consequently, researchers have developed approaches that leverage this social web data to tackle interesting challenges of the semantic web. Among these are methods for learning ontologies from social media or crowdsourcing, extracting semantics from data collected by citizen science and participatory sensing initiatives, or for better understanding and describing user activities. The rich data provided by the social web can be used to build the semantic web. This task includes learning basic semantic relationships, eg, between entities, or by employing more sophisticated methods to construct a complete knowledge graph or ontology. There are additional synergies between the social web and the semantic web. For example, content from the social web could be enriched and linked to the semantic web using named entity recognition and linking, as well as sentiment analysis. These topics were covered previously in the Special Issue on The Personal and Social Semantic Web. 1This special issue attracted six submissions. Each submission was peer-reviewed by three reviewers. Three submissions were judged to be appropriate and, after revisions with subsequent reviews, were accepted for publication in this special issue. These three submissions share a common theme: the extraction of meaning from user-generated texts and the challenges associated therein. The submissions also show the importance of Twitter, the most popular microblogging service to date, since the methods described in the sub-", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Assessing the navigational effects of click biases and link insertion on the web\n", "abstract": " Websites have an inherent interest in steering user navigation in order to, for example, increase sales of specific products or categories, or to guide users towards specific information. In general, website administrators can use the following two strategies to influence their visitors' navigation behavior. First, they can introduce click biases to reinforce specific links on their website by changing their visual appearance, for example, by locating them on the top of the page. Second, they can utilize link insertion to generate new paths for users to navigate over. In this paper, we present a novel approach for measuring the potential effects of these two strategies on user navigation. Our results suggest that, depending on the pages for which we want to increase user visits, optimal link modification strategies vary. Moreover, simple topological measures can be used as proxies for assessing the impact of the intended changes\u00a0\u2026", "num_citations": "4\n", "authors": ["1193"]}
{"title": "The impact of network flows on community formation in models of opinion dynamics\n", "abstract": " We study dynamics of opinion formation in a network of coupled agents. As the network evolves to a steady state, opinions of agents within the same community converge faster than those of other agents. This framework allows us to study how network topology and network flow, which mediates the transfer of opinions between agents, both affect the formation of communities. In traditional models of opinion dynamics, agents are coupled via conservative flows, which result in one-to-one opinion transfer. However, social interactions are often nonconservative, resulting in one-to-many transfer of opinions. We study opinion formation in networks using one-to-one and one-to-many interactions and show that they lead to different community structure within the same network.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Effects of social influence in peer online recommendation\n", "abstract": " Web site providers often use peer recommendation to help people find interesting content. A common method for leveraging opinions of others on these web sites is to display the number of prior recommendations as a social signal. How people react to these social influence signals, in combination with other effects, such as the quality of content and its presentation order, determines how many recommendations content receives. Using Amazon Mechanical Turk, we experimentally measure the effects of social influence on user decisions to recommend content. Specifically, after controlling for variation in content quality and position, we find that social influence affects outcomes of peer recommendation about half as much as position and quality do. These effects are somewhat correlated, increasing the inequality of popularity in the presence of social influence. Further, we find that social influence changes people\u2019s preferences, creating a \u201cherding\u201d effect that biases their judgements about the content. While similar adverse outcomes have been noted in previous studies, we demonstrate a benefit of social influence: namely, it reduces the effort devoted to evaluating content without significantly diminishing the performance of peer recommendation.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "The role of dynamic interactions in multi-scal e analysis of network structure\n", "abstract": " To find interesting structure in networks, community detection algorithms have to consider not only the network topology, but also the dynamics of interactions between nodes. We investigate this claim using the paradigm of synchronization in a network of coupled oscillators. As the network evolves to a global equilibrium, nodes belonging to the same community synchronize faster than nodes belonging to different communities. We classify interactions as conservative (eg, random walk) and non-conservative (eg, viral contagion, information diffusion) and formulate a new model of non-conservative interactions. To find multi-scale community structure, we define a similarity function that measures the degree to which nodes are synchronized and use it to hierarchically cluster nodes. We study three data sets, that include a benchmark network, a synthetic graph with a known hierarchical community structure, and a large network of a social media provider. We find that conservative and nonconservative interaction models lead to dramatically different communities, with the non-conservative model revealing communities closer to the ground truth. Our method uncovers a significantly more complex multi-scale organization of networks than previously thought. The discovered structure of a real-world network resembles an onion: in each layer of the hierarchy, we find a large core and a number of small components with a long-tailed size distribution. Our work offers a novel, process-dependent perspective on community detection in real-world social networks.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Social mechanics: An empirically grounded science of social media\n", "abstract": " What will social media sites of tomorrow look like? What behaviors will their interfaces enable? A major challenge for designing new sites that allow a broader range of user actions is the difficulty of extrapolating from experience with current sites without first distinguishing correlations from underlying causal mechanisms. The growing availability of data on user activities provides new opportunities to uncover correlations among user activity, contributed content and the structure of links among users. However, such correlations do not necessarily translate into predictive models. Instead, empirically grounded mechanistic models provide a stronger basis for establishing causal mechanisms and discovering the underlying statistical laws governing social behavior. We describe a statistical physics-based framework for modeling and analyzing social media and illustrate its application to the problems of prediction and inference. We hope these examples will inspire the research community to explore these methods to look for empirically valid causal mechanisms for the observed correlations.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Constructing folksonomies by integrating structured metadata with relational clustering\n", "abstract": " Many social Web sites allow users to annotate the content with descriptive metadata, such as tags, and more recently also to organize content hierarchically. These types of structured metadata provide valuable evidence for learning how a community organizes knowledge. For instance, we can aggregate many personal hierarchies into a common taxonomy, also known as a folksonomy, that will aid users in visualizing and browsing social content, and also to help them in organizing their own content. However, learning from social metadata presents several challenges: sparseness, ambiguity, noise, and inconsistency. We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr. We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy. Our empirical results suggest that the proposed framework, which addresses the challenges listed above, improves on existing folksonomy learning methods.", "num_citations": "4\n", "authors": ["1193"]}
{"title": "Intelligent optimization of parallel and distributed applications\n", "abstract": " This paper describes a new project that systematically addresses the enormous complexity of mapping applications to current and future parallel platforms. By integrating the system layers - domain-specific environment, application program, compiler, run-time environment, performance models and simulation, and workflow manager - and through a systematic strategy for application mapping, our approach exploit the vast machine resources available in such parallel platforms to dramatically increase the productivity of application programmers. This project brings together computer scientists in the areas represented by the system layers (i.e., language extensions, compilers, run-time systems, workflows) together with expertise in knowledge representation and machine learning. With expert domain scientists in molecular dynamics (MD) simulation, we are developing our approach in the context of a specific\u00a0\u2026", "num_citations": "4\n", "authors": ["1193"]}
{"title": "A formal design methodology for coordinated multi-robot systems\n", "abstract": " To enable the successful deployment of task-achieving multi-robot systems (MRS), the interactions must be coordinated among the robots within the MRS and between the robots and the task environment. There have been a number of experimentally demonstrated coordinated MRS; however, most have been designed through ad hoc procedures, typically providing task-specific, empirical insights with few contributions toward general-purpose, principled design methods.This proposal presents a formal MRS design methodology applicable to homogeneous, distributed MRS performing sequential tasks in a Markovian world. We introduce a suite of systematic methods for synthesizing satisficing controllers for robots constituting a MRS. Each of these methods synthesizes a MRS that achieves system-level, task-directed coordination through the use of a variety of local control features: inter-robot communication, the maintenance of internal state, and both deterministic and probabilistic action selection. Complimentary to the synthesis methods, we present two MRS modeling approaches that share a common formal foundation with the MRS synthesis methods. The first modeling approach is a Bayesian macroscopic model and the second is a probabilistic microscopic model. Both models are capable of quantitatively predicting the task performance of a given MRS. Together, the unified synthesis and analysis methods provide more than just pragmatic design tools. Based on their common formal foundations and integrated nature, they provide a platform from which to formally characterize some relationships and dependencies among MRS task\u00a0\u2026", "num_citations": "4\n", "authors": ["1193"]}
{"title": "The transsortative structure of networks\n", "abstract": " Network topologies can be highly non-trivial, due to the complex underlying behaviours that form them. While past research has shown that some processes on networks may be characterized by local statistics describing nodes and their neighbours, such as degree assortativity, these quantities fail to capture important sources of variation in network structure. We define a property called transsortativity that describes correlations among a node\u2019s neighbours. Transsortativity can be systematically varied, independently of the network\u2019s degree distribution and assortativity. Moreover, it can significantly impact the spread of contagions as well as the perceptions of neighbours, known as the majority illusion. Our work improves our ability to create and analyse more realistic models of complex networks.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Challenges in forecasting malicious events from incomplete data\n", "abstract": " The ability to accurately predict cyber-attacks would enable organizations to mitigate their growing threat and avert the financial losses and disruptions they cause. But how predictable are cyber-attacks? Researchers have attempted to combine external data\u2013ranging from vulnerability disclosures to discussions on Twitter and the darkweb\u2013with machine learning algorithms to learn indicators of impending cyber-attacks. However, successful cyber-attacks represent a tiny fraction of all attempted attacks: the vast majority are stopped, or filtered by the security appliances deployed at the target. As we show in this paper, the process of filtering reduces the predictability of cyber-attacks. The small number of attacks that do penetrate the target\u2019s defenses follow a different generative process compared to the whole data which is much harder to learn for predictive models. This could be caused by the fact that the resulting\u00a0\u2026", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Affect Estimation with Wearable Sensors\n", "abstract": " Affective states are associated with people\u2019s mental health status and have profound impact on daily life, thus unobtrusively understanding and estimating affects have been brought to the public attention. The pervasiveness of wearable sensors makes it possible to build automatic systems for affect tracking. However, constructing such systems is a challenging task due to the complexity of human behaviors. In this work, we focus on the problem of estimating daily self-reported affects from sensor-generated data. We first analyze the intra-and inter-subject differences of self-reported affect labels. Second, we explore different machine learning models as well as label transformation techniques to overcome the individual differences in self-reported responses estimation. We conceptualize three experimental settings including long-term and short-term estimation scenarios. Our experimental results show that the mixed effects model and label transformation can yield better estimation of individual daily affect. This work poses the basis for future sensor-based individualized and real-time affective digital and/or clinical interventions.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "The darpa socialsim challenge: massive multi-agent simulations of the github ecosystem\n", "abstract": " The DARPA SocialSim challenge problem measured participant\u2019s ability, given 30 months of meta-data on user activity on GitHub, to predict the next months\u2019 activity as measured by a broad range of metrics applied to ground truth, using agent-based simulation. The challenge involved making predictions about roughly 3 million individuals performing a combined 30 million actions on 6 million repositories. We describe the agent framework and the models we employed. Our team used a variety of learning methods contributing to six different types of agents that were tested against a wide range of metrics. The broadly most successful method of those tried sampled from a stationary probability distribution of actions and target repositories for each agent. First, we describe the agent-based simulator we developed to carry out massive-scale simulations of techno-social systems. Second, we present the inference methods that we employed to implement different agent-based models, based on statistical modeling of historical activity, graph embedding to infer future interactions, Bayesian models to capture activity processes, and methods to predict the emergence of new users and repositories that did not exist in the historical data. These are novel applications of existing analytical tools to derive agent models from available data. Third, we provide a rigorous evaluation of the performance of six different models, as measured by a wide range of metrics. We also describe", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Model of cognitive dynamics predicts performance on standardized tests\n", "abstract": " In the modern knowledge economy, success demands sustained focus and high cognitive performance. Research suggests that human cognition is linked to a finite resource, and upon its depletion, cognitive functions such as self-control and decision-making may decline. While fatigue, among other factors, affects human activity, how cognitive performance evolves during extended periods of focus remains poorly understood. By analyzing performance of a large cohort answering practice standardized test questions online, we show that accuracy and learning decline as the test session progresses and recover following prolonged breaks. To explain these findings, we hypothesize that answering questions consumes some finite cognitive resources on which performance depends, but these resources recover during breaks between test questions. We propose a dynamic mechanism of the consumption and\u00a0\u2026", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Bounded rationality in scholarly knowledge discovery\n", "abstract": " In an information-rich world, people's time and attention must be divided among rapidly changing information sources and the diverse tasks demanded of them. How people decide which of the many sources, such as scientific articles or patents, to read and use in their own work affects dissemination of scholarly knowledge and adoption of innovation. We analyze the choices people make about what information to propagate on the citation networks of Physical Review journals, US patents and legal opinions. We observe regularities in behavior consistent with human bounded rationality: rather than evaluate all available choices, people rely on simply cognitive heuristics to decide what information to attend to. We demonstrate that these heuristics bias choices, so that people preferentially propagate information that is easier to discover, often because it is newer or more popular. However, we do not find evidence that popular sources help to amplify the spread of information beyond making it more salient. Our paper provides novel evidence of the critical role that bounded rationality plays in the decisions to allocate attention in social communication.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "The success of question answering communities: How diversity influences ad hoc groups\n", "abstract": " Question and answer (Q&A) web sites have become increasing popular in recent years. These communities are social media platforms that enable users around the world to easily share their knowledge with each other. Q&A sites are based in part on the wisdom of crowds [Surowiecki 2005], ie, everyone involved in the community can contribute something, and through collaborative production they create a comprehensive knowledge repository [Fichman 2011]. Harper et al.[Harper et al. 2008] describe how these sites presented high-quality answers compared with the professionally-staffed library reference services. Understanding the dynamics in the Q&A websites can play an important role in developing better Q&A platforms for general use, and for use within organizations. Moreover, since the groups of individuals who participate in these forums are brought together on ad hoc basis, they also provide insight into how ad hoc teams perform in general, and can provide insights into the role of different factors in the performance of these teams.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Placing user-generated content on the map with confidence\n", "abstract": " We describe a method that predicts the location of user-generated content using textual features alone. Unlike previous methods for geotagging text documents, our proposed method is not sensitive to how we discretize space. We also discover that spatial resolution has an impact on the prediction accuracy, which allows us to trade-off the spatial resolution of the predicted location against our confidence about its accuracy. Our method can be used to estimate the error in document's predicted location, enabling us to filter out poor quality predictions. We evaluate the proposed method extensively on user-generated content collected from two different social media sites, Flickr and Twitter. Our evaluation examines its performance on the geotagging task and with respect to different parameters. We achieve state-of-the-art results for all three tasks: location prediction, error estimation and result ranking and also provide\u00a0\u2026", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Stochastic models of social media dynamics\n", "abstract": " A major challenge for designing future social media sites allowing a broader range of user actions is the difficulty of extrapolating from experience with current sites without first distinguishing correlations from underlying causal mechanisms leading to successful communities. The growing availability of data on user activities provides new opportunities to uncover correlations among user activity, contributed content and links among users. However, such correlations do not necessarily translate into methods for predicting outcomes or improving the productivity of the user communities that arise around social media. Instead, mechanistic models and intervention experiments provide a stronger basis for establishing causal mechanisms underlying the development of social media. In particular, stochastic models of large communities are well-suited to account for the large variation in user behavior, quality of contributed content, and effect of current events. Such models readily incorporate the structure of the web site, especially how content is presented to users, and thereby indicate the likely effects of design choices for new sites. We describe the ingredients of this approach, illustrate its use on Digg, a crowdsourced web site rating stories on current events [Note: mention any other examples], and its application to developing future social media.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Stochastic Models of Large-Scale Human Behavior on the Web.\n", "abstract": " We describe stochastic models of user-contributory web sites, where users create, rate and share the content. These models describe how aggregate measures of activity arise from simple models of individual users. This approach provides a tractable, approximate method to understand user activity on the web site and how this activity depends on web site design choices, such as what information on other users\u2019 behaviors is shown to each user. We illustrate this approach in the context of user-created content on the news rating site, Digg.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Exploiting data semantics to discover, extract, and model web sources\n", "abstract": " We describe Deimos, a system that automatically discovers and models new sources of information.The system exploits four core technologies developed by our group that makes an end-to-end solution to this problem possible. First, given an example source, Deimos finds other similar sources online. Second, it invokes and extracts data from these sources. Third, given the syntactic structure of a source, Deimos maps its inputs and outputs to semantic types. Finally, it infers the source's semantic definition, i.e., the function that maps the inputs to the outputs. Deimos is able to successfully automate these steps by exploiting a combination of background knowledge and data semantics. We describe the challenges in integrating separate components into a unified approach to discovering, extracting and modeling new online sources. We provide an end-to-end validation of the system in two information domains to\u00a0\u2026", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Mathematical modeling of large multi-agent systems\n", "abstract": " The biologically-inspired swarm paradigm is being used to design self-organizing systems of locally interacting agents. A major difficulty in designing swarms with desired characteristics is understanding the causal relation between individual agent and collective behaviors. Mathematical analysis of swarm dynamics can address this difficulty to gain insight into system design. This project developed a formal framework for mathematical modeling and analysis of multi-agent swarms. Though the behavior of an individual agent can be considered to be stochastic and unpredictable, collective behavior of a swarm can have a simple probabilistic description. We showed that a class of mathematical models that describe the dynamics of collective behavior of multi-agent systems can be written down from the details of the individual agent controller. We have successfully applied this formalism to study collective behavior of distributed robot systems for which a body of experimental and simulations data exists.Descriptors:", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Threshold Behavior in a Boolean Network Model for SAT.\n", "abstract": " Boolean satisfiability (SAT) is the canonical NP-complete problem that plays an important role in AI and has many practical applications in Computer Science in general. Boolean networks (BN) are dynamical systems that have recently been proposed as an algorithm for solving SAT problems [7]. We have carried out a detailed investigation of the dynamical properties of BN corresponding to random SAT problems of different size. We varied the problem size by changing the number of variables and the number of clauses in the Boolean formula. We show that dynamics of BN corresponding to 3-SAT problems display a threshold-like behavior, although this transition occurs far below the well known phase transition in the computational complexity of random 3-SAT. This threshold behavior does not appear to be connected to the transition between frozen and chaotic dynamics regimes of random BN.", "num_citations": "3\n", "authors": ["1193"]}
{"title": "Having a Bad Day? Detecting the Impact of Atypical Life Events Using Wearable Sensors\n", "abstract": " Life events can dramatically affect our psychological state and work performance. Stress, for example, has been linked to professional dissatisfaction, increased anxiety, and workplace burnout. We explore the impact of positive and negative life events on a number of psychological constructs through a multi-month longitudinal study of hospital and aerospace workers. Through causal inference, we demonstrate that positive life events increase positive affect, while negative events increase stress, anxiety and negative affect. While most events have a transient effect on psychological states, major negative events, like illness or attending a funeral, can reduce positive affect for multiple days. Next, we assess whether these events can be detected through wearable sensors, which can cheaply and unobtrusively monitor health-related factors. We show that these sensors paired with embedding-based learning models can be used ``in the wild'' to capture atypical life events in hundreds of workers across both datasets. Overall our results suggest that automated interventions based on physiological sensing may be feasible to help workers regulate the negative effects of life events.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Unequal impact and spatial aggregation distort covid-19 growth rates\n", "abstract": " The COVID-19 pandemic has emerged as a global public health crisis. To make decisions about mitigation strategies and to understand the disease dynamics, policy makers and epidemiologists must know how the disease is spreading in their communities. We analyze confirmed infections and deaths over multiple geographic scales to show that COVID-19's impact is highly unequal: many subregions have nearly zero infections, and others are hot spots. We attribute the effect to a Reed-Hughes-like mechanism in which disease arrives at different times and grows exponentially. Hot spots, however, appear to grow faster than neighboring subregions and dominate spatially aggregated statistics, thereby amplifying growth rates. The staggered spread of COVID-19 can also make aggregated growth rates appear higher even when subregions grow at the same rate. Public policy, economic analysis and epidemic modeling need to account for potential distortions introduced by spatial aggregation.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Graph embedding with personalized context distribution\n", "abstract": " Graph representation learning embeds graph nodes in a low-dimensional latent space, which allows for mathematical operations on nodes using low-dimensional vectors for downstream tasks, such as link prediction, node classification, and recommendation. Traditional graph embedding methods rely on hyper-parameters to capture the rich variation hidden in the structure of real-world graphs. In many applications, it may not be computationally feasible to search for optimal hyper-parameters. In this work, built on WatchYourStep which a graph embedding method leveraging graph attention, we propose a method that utilizes node-personalized context attention to capture the local variation in a graph structure. Specifically, we replace the shared context distribution among nodes with learnable personalized context distribution for each node. We evaluate our model on seven real-world graphs and show that our\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Learning Fair and Interpretable Representations via Linear Orthogonalization\n", "abstract": " To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and they are not easily transferable across models. To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "BD2K Training Coordinating Center's ERuDIte: the Educational Resource Discovery Index for Data Science\n", "abstract": " Data science is a field that has developed to enable efficient integration and analysis of increasingly large data sets in many domains. In particular, big data in genetics, neuroimaging, mobile health, and other subfields of biomedical science, promises new insights, but also poses challenges. To address these challenges, the National Institutes of Health launched the Big Data to Knowledge (BD2K) initiative, including a Training Coordinating Center (TCC) tasked with developing a resource for personalized data science training for biomedical researchers. The BD2K TCC web portal is powered by ERuDIte, the Educational Resource Discovery Index, which collects training resources for data science, including online courses, videos of tutorials and research talks, textbooks, and other web-based materials. While the availability of so many potential learning resources is exciting, they are highly heterogeneous in\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Deep context: a neural language model for large-scale networked documents\n", "abstract": " We propose a scalable neural language model that leverages the links between documents to learn the deep context of documents. Our model, Deep Context Vector, takes advantage of distributed representations to exploit the word order in document sentences, as well as the semantic connections among linked documents in a document network. We evaluate our model on large-scale data collections that include Wikipedia pages, and scientific and legal citations networks. We demonstrate its effectiveness and efficiency on document classification and link prediction tasks.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Techniques to evaluate and enhance cognitive performance\n", "abstract": " In one embodiment of the present application, an operator provides entries into the computer workstation indicative of an evaluation. Assessment of the entries takes place relative to data representative of a corresponding multiple resource neuroenergetic model for the cognitively demanding activ ity. Generation of an operator instruction results from the assessment that is provided to the operator through the computer workstation. The operator instruction directs the operator to take a break from the cognitively demanding activity based on the assessment.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Network composition from multi-layer data\n", "abstract": " It is common for people to access multiple social networks, for example, using phone, email, and social media. Together, the multi-layer social interactions form a \"integrated social network.\" How can we extend well developed knowledge about single-layer networks, including vertex centrality and community structure, to such heterogeneous structures? In this paper, we approach these challenges by proposing a principled framework of network composition based on a unified dynamical process. Mathematically, we consider the following abstract problem: Given multi-layer network data and additional parameters for intra and inter-layer dynamics, construct a (single) weighted network that best integrates the joint process. We use transformations of dynamics to unify heterogeneous layers under a common dynamics. For inter-layer compositions, we will consider several cases as the inter-layer dynamics plays different roles in various social or technological networks. Empirically, we provide examples to highlight the usefulness of this framework for network analysis and network design.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Analysis: An Introduction\n", "abstract": " In many systems considered in this book, computation is an emergent property of a large population of interacting individuals. The role of analysis is to uncover and validate the microscopic mechanisms that govern an individual\u2019s behavior. The products of analysis are descriptive models and theories of individual behavior, and a framework that explains the collective behavior that arises from interactions among many individuals. In addition to being descriptive, the models are often used to predict emergent collective behavior and motivate the design of future human computational algorithms and user interfaces that support them.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "A world wider than the web: End user programming across multiple domains\n", "abstract": " Publisher SummaryThis chapter presents Integrated Task Learning (ITL), an approach for learning procedures across domains using end-user programming (EUP). ITL provides a suite of complementary learning and reasoning capabilities for acquiring procedures. This includes inducing generalized procedures from observed demonstrations in instrumented applications, providing template-based procedure visualizations that are easily understandable to end users, and supporting procedure editing. To incorporate other domains, including Web domains, ITL also includes facilities for semantically mapping actions across domains. This discussion begins by describing each of these capabilities in turn and then describing the central engineering concept that ITL uses to facilitate cross-domain learning: pluggable domain models, which are independently generated type and action models over different application\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Leveraging User-Specified Metadata to Personalize Image Search\n", "abstract": " The social media sites, such as Flickr and del. icio. us, allow users to upload content and annotate it with descriptive labels known as tags, join special-interest groups, and so forth. We believe user-generated metadata expresses user\u2019s tastes and interests and can be used to personalize information to an individual user. Specifically, we describe a machine learning method that analyzes a corpus of tagged content to find hidden topics. We then these learned topics to select content that matches user\u2019s interests. We empirically validated this approach on the social photo-sharing site Flickr, which allows users to annotate images with freely chosen tags and to search for images labeled with a certain tag. We use metadata associated with images tagged with an ambiguous query term to identify topics corresponding to different senses of the term, and then personalize results of image search by displaying to the user\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Discovering and learning semantic models of online sources for information integration\n", "abstract": " Much work in Information Integration and the Semantic Web assumes that rich semantic models of sources exist. In practice, there is a tremendous amount of data on the Web, but it is typically hard to find, has little or no explicit structure, and there is rarely any semantic description of the data. We describe an integrated end-to-end system that can automatically discover web sources, invoke and extract the data from them, and build their semantic models. We describe the challenges in integrating the component technologies into a unified approach to discovering, extracting and modeling new online sources. We evaluate the integrated system in three different domains and demonstrate that it can automatically discover and model new data sources.", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Special issue on wrapping web data islands\n", "abstract": " Information integration applications combine data from heterogeneous sources to assist the user in solving repetitive data-intensive tasks. Currently, such applications require a high level of expertise in information integration since users need to know how to extract data from an on-line source, describe its semantics, and build integration plans to answer specific queries. We have integrated three task learning technologies within a single desktop application to assist users in creating information integration applications. It includes a tool for programmatic access to data in on-line information sources, a tool to semantically model them by aligning their input and output parameters with a common ontology, and a tool that enables the user to create complex integration plans using simple text instructions. Our system was integrated within the Calo Desktop Assistant and evaluated independently on a range of problems. It\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Learning information-gathering procedures by combined demonstration and instruction\n", "abstract": " Existing systems are able to learn information agents through demonstration that provide programmatic access to web-based information. However it is still difficult for end users to combine these information agents in procedures that are customized to their particular needs. We combine learning by demonstration with learning by instruction to build a system to learn such procedures with a small amount of human input. The instruction system relies on knowing the input-output types of the information agents in order to combine them. We make use a system that learns to predict the types from examples to simplify this part of the task. Our instruction system performs a search that has interesting similarities with proof search in explanationbased learning.Intelligent software agents aim to assist users in the office environment by carrying out complex everyday tasks, for example planning travel, or managing the purchasing of equipment. These tasks are often on-going processes, where the assistant should initially combine information from a variety of heterogeneous sources and process the information as the user wishes, perhaps monitoring the progress of the task and continuing to give help where appropriate. For example, in planning a trip, the assistant may initially gather information about flights and hotels based on the user\u2019s preferences and budget, make some recommendations and assist with booking the user\u2019s choice, send reminders to the travelers, monitor for changes in prices, and check in for flights at the appropriate time. The software assistant must be able to learn new tasks and procedures, due to the wide range of potential tasks the\u00a0\u2026", "num_citations": "2\n", "authors": ["1193"]}
{"title": "Socioeconomic Correlates of Anti-Science Attitudes in the US\n", "abstract": " Successful responses to societal challenges require sustained behavioral change. However, as responses to the COVID-19 pandemic in the US showed, political partisanship and mistrust of science can reduce public willingness to adopt recommended behaviors such as wearing a mask or receiving a vaccination. To better understand this phenomenon, we explored attitudes toward science using social media posts (tweets) that were linked to counties in the US through their locations. The data allowed us to study how attitudes towards science relate to the socioeconomic characteristics of communities in places from which people tweet. Our analysis revealed three types of communities with distinct behaviors: those in large metro centers, smaller urban places, and rural areas. While partisanship and race are strongly associated with the share of anti-science users across all communities, income was negatively and positively associated with anti-science attitudes in suburban and rural areas, respectively. We observed that emotions in tweets, specifically negative high arousal emotions, are expressed among suburban and rural communities by many anti-science users, but not in communities in large urban places. These trends were not apparent when pooled across all counties. In addition, we found that anti-science attitudes expressed five years earlier were significantly associated with lower COVID-19 vaccination rates. Our analysis demonstrates the feasibility of using spatially resolved social media data to monitor public attitudes on issues of social importance.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "The Wide, the Deep, and the Maverick: Types of Players in Team-based Online Games\n", "abstract": " Although player performance in online games has been widely studied, few studies have considered the behavioral preferences of players and how they impact performance. In a competitive setting where players must cooperate with temporary teammates, it is even more crucial to understand how differences in playing style contribute to teamwork. Drawing on theories of individual behavior in teams, we describe a methodology to empirically profile players based on the diversity and conformity of their gameplay styles. Applying this approach to a League of Legends dataset, we find three distinct types of players that align with our theoretical framework: generalists, specialists, and mavericks. Importantly, the behavior of each player type remains stable despite players becoming more experienced. Additionally, we extensively investigate the benefits and drawbacks of each type of player by evaluating their individual\u00a0\u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "A Model of Densifying Collaboration Networks\n", "abstract": " Research collaborations provide the foundation for scientific advances, but we have only recently begun to understand how they form and grow on a global scale. Here we analyze a model of the growth of research collaboration networks to explain the empirical observations that the number of collaborations scales superlinearly with institution size, though at different rates (heterogeneous densification), the number of institutions grows as a power of the number of researchers (Heaps' law) and institution sizes approximate Zipf's law. This model has three mechanisms: (i) researchers are preferentially hired by large institutions, (ii) new institutions trigger more potential institutions, and (iii) researchers collaborate with friends-of-friends. We show agreement between these assumptions and empirical data, through analysis of co-authorship networks spanning two centuries. We then develop a theoretical understanding of this model, which reveals emergent heterogeneous scaling such that the number of collaborations between institutions scale with an institution's size.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "User-Based Collaborative Filtering Mobile Health System\n", "abstract": " Mobile health systems predict health conditions based on multimodal signals. Users are often reluctant to provide their health status over privacy concerns. It is challenging to make health predictions without sufficient historical data from the users. In this paper, we propose a user-based collaborative filtering mobile health system. The system requests users to provide a few health labels. These labels are used to determine cohort similarity and discarded afterward to ensure privacy protection. The cohorts are designed to maximize user similarity across health labels, variable relationships, and sensor data. Our system predicts users based on the health information from their cohort. We empirically evaluate the system by conducting a ten-week longitudinal study to assess the health conditions of 212 hospital workers using mobile devices, wearables, and sensors. The results show successful cohort assignments with\u00a0\u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "The Leaky Pipeline in Physics Publishing\n", "abstract": " Women make up a shrinking portion of physics faculty in senior positions, a phenomenon known as a \"leaky pipeline.\" While fixing this problem has been a priority in academic institutions, efforts have been stymied by the diverse sources of leaks. In this paper we identify a bias potentially contributing to the leaky pipeline. We analyze bibliographic data provided by the American Physical Society (APS), a leading publisher of physics research. By inferring the gender of authors from names, we are able to measure the fraction of women authors over past decades. We show that the more selective, higher impact APS journals have lower fractions of women authors compared to other APS journals. Correcting this bias may help more women publish in prestigious APS journals, and in turn help improve their academic promotion cases.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Origins of Algorithmic Instabilities in Crowdsourced Ranking\n", "abstract": " Crowdsourcing systems aggregate decisions of many people to help users quickly identify high-quality options, such as the best answers to questions or interesting news stories. A long-standing issue in crowdsourcing is how option quality and human judgement heuristics interact to affect collective outcomes, such as the perceived popularity of options. We address this limitation by conducting a controlled experiment where subjects choose between two ranked options whose quality can be independently varied. We use this data to construct a model that quantifies how judgement heuristics and option quality combine when deciding between two options. The model reveals popularity-ranking can be unstable: unless the quality difference between the two options is sufficiently high, the higher quality option is not guaranteed to be eventually ranked on top. To rectify this instability, we create an algorithm that\u00a0\u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Contagions in social networks: Effects of monophilic contagion, friendship paradox and reactive networks\n", "abstract": " We consider SIS contagion processes over networks where, a classical assumption is that individuals' decisions to adopt a contagion are based on their immediate neighbors. However, recent literature shows that some attributes are more correlated between two-hop neighbors, a concept referred to as monophily. This motivates us to explore monophilic contagion, the case where a contagion (e.g. a product, disease) is adopted by considering two-hop neighbors instead of immediate neighbors (e.g. you ask your friend about the new iPhone and she recommends you the opinion of one of her friends). We show that the phenomenon called friendship paradox makes it easier for the monophilic contagion to spread widely. We also consider the case where the underlying network stochastically evolves in response to the state of the contagion (e.g. depending on the severity of a flu virus, people restrict their interactions with others to avoid getting infected) and show that the dynamics of such a process can be approximated by a differential equation whose trajectory satisfies an algebraic constraint restricting it to a manifold. Our results shed light on how graph theoretic consequences affect contagions and, provide simple deterministic models to approximate the collective dynamics of contagions over stochastic graph processes.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Analyzing the Digital Traces of Political Manipulation\n", "abstract": " Analyzing the Digital Traces of Political Manipulation: (The 2016 Russian Interference Twitter Campaign) Page 1 ANALYZING THE DIGITAL TRACES OF POLITICAL MANIPULATION: (THE 2016 RUSSIAN INTERFERENCE TWITTER CAMPAIGN) Adam Badawy, Emilio Ferrara, Kristina Lerman University of Southern California Page 2 BACKGROUND \u2022 Studying large scale online political manipulation campaign \u2022 US Presidential Elections \u2022 Trolls Page 3 RESEARCH QUESTIONS \u2022 What was the role of the users\u2019 political ideology? \u2022 What was the role of social bots? \u2022 Did trolls especially succeed in specific areas of the US? \u2022 Can we predict which users will become susceptible to Russian trolls? \u2022 What features distinguish users who spread trolls\u2019 messages? Page 4 DATA COLLECTION \u2022 Twitter dataset: 43.7 M tweets posted by 5.7 M users from 15th of September to 9th of November 2016. \u2022 Data collected using \u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Multi-layer network composition under a unified dynamical process\n", "abstract": " In this paper, we take a step towards a principled method of network composition from multi-layer data. We argue that inter-layer dynamics is a essential component of understanding the structure as a whole. Mathematically, we consider the following abstract problem: given multiple layers of network data over a shared vertex set, and additional parameters for inter-layer transitions, construct a (single) weighted network that best integrates the multi-layer dynamics. In this context, we will also study an empirical use case of the composition framework.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Will Break for Productivity: Generalized Symptoms of Cognitive Depletion\n", "abstract": " In this work, we address the symptoms of cognitive depletion as they relate to generalized knowledge workers. We unify previous findings within a single analytical model of cognitive depletion. Our purpose is to develop a model that will help us predict when a person has reached a sufficient state of cognitive depletion such that taking a break or some other restorative action will benefit both his or her own wellbeing and the quality of his or her performance. We provide a definition of each symptom in our model as well as the effect it would have on a knowledge worker's ability to work productively. We discuss methods to detect each symptom that do not require self assessment. Understanding symptoms of cognitive depletion provides the ability to support human knowledge workers by reducing the stress involved with cognitive and work overload while maintaining or improving the quality of their performance.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Cognitive Heuristics and Collective Opinions in Peer Recommendation\n", "abstract": " The world presents far more information than people have the capacity to examine. As a result, humans have evolved to use cognitive heuristics to decide quickly what information to pay attention to. For example, people pay more attention to items near the top of a list than those below [Payne 1951]. A consequence of this cognitive heuristic, called position bias, is the strong effect the presentation order\u2014item ranking\u2014has on individual choices. It affects which items in a list of search results users click on [Buscher et al. 2009], and the answer they select in response to multiple choice questions [Blunch 1984]. Another common cognitive heuristic is social influence: people pay attention to the choices of others. Social influence affects most daily decisions, such as what to buy and who to vote for. Studies showed that social influence biases individual judgements [Salganik et al. 2006; Muchnik et al. 2013], creating an \u201cirrational herding\u201d effect that can obscure the underlying quality of choices. Cognitive heuristics also play an important role online, where rapid proliferation of user-generated content makes it difficult to identify high-quality items. Since people often do not have time or energy to evaluate all available choices, they may rely on the opinions of others. Crowdsourcing, peer recommendation, and markets are some of the mechanisms for aggregating individual decisions into a collective opinion, which can help individuals identify high-quality items. The choices content providers make about how and what information to display to users has a profound effect on collective behavior. For instance, the choice of how to rank items shown to users can\u00a0\u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Scalable mining of social data using stochastic gradient fisher scoring\n", "abstract": " The rapid growth of social data in the form of videos, microblog posts and other items shared on social media presents new opportunities for learning user behavior and preferences. Bayesian models have been used widely for modeling social data, since they capture uncertainty and prior knowledge, avoid overfitting, and can be easily extended to incorporate new types of data. Researchers have used a variety of inference procedures to learn model parameters from data. Specifically, Stochastic Gradient Fisher Scoring (SGFS) method was recently proposed for efficient inference. This method samples from a Bayesian posterior using small number of data samples in each iteration, instead of the entire data, to speed up the inference process. In this paper we explore the feasibility of SGFS for social data mining. We find that SGFS often outperforms other inference methods in dense data, but it fails in the sparse\"\u00a0\u2026", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Constructing folksonomies by integrating structured metadata\n", "abstract": " Aggregating many personal hierarchies into a common taxonomy, also known as a folksonomy, presents several challenges due to its sparseness, ambiguity, noise, and inconsistency. We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr. We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project. Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Harvesting geospatial knowledge from online social networks\n", "abstract": " Social Web has moved knowledge production from the hands of the experts and professionals to the masses. Today online social networking sites, such as Twitter, Facebook, YouTube, and Flickr, allow ordinary people not only to create massive quantities of new data, but also organize it, use it, and share it with others. Unlike earlier information technologies, the Social Web exposes social activity, allowing each person to observe and be influenced by the actions of others in real time. How will such real-time, many-to-many communication change how we discover, use, and manage information? And how will it transform society and how we solve problems? My research addresses these questions by developing methods to harvest social knowledge.Consider a gazetteer, for example, Geonames. org, which compiles geospatial knowledge within a directory of places and place names, often organizing it hierarchically within taxonomy of geospatial concepts. Such gazetteers have been useful for creating geo-aware applications and integrating geospatial knowledge. However, since gazetteers are manually and painstakingly created by an expert or a small group of experts, they are rarely complete or comprehensive, do not reflect the variety of views, and fail to keep up with our changing ideas about places.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Parameterized Centrality for Network Analysis\n", "abstract": " Bonacich centrality measures the number of attenuated paths between nodes in a network. We use this metric to study network structure, specifically, to rank nodes and find community structure of the network. To this end we extend the modularity-maximization method for community detection to use this centrality metric as a measure of node connectivity. Bonacich centrality contains a tunable parameter that sets the length scale of interactions. By studying how rankings and discovered communities change when this parameter is varied allows us to identify globally important nodes and structures. We apply the proposed method to several benchmark networks and show that it leads to better insight into network structure than earlier methods.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Finding Structure in Heterogeneous Networks\n", "abstract": " Complex networks play a key role in the evolution of communities and individual decisions community members make. These networks are becoming increasingly heterogeneous, linking many different types of entities. Network analysis and community detection algorithms, however, usually reduce complex networks to homogeneous networks composed of entities of a single type. In the process, they conflate relations between different entity types and loose important structural information. In this paper we describe a generalization of the modularity-based community detection algorithm and apply it to complex, heterogeneous networks. First, we redefine network connectivity in terms of influence, measured by the number of paths of any length that exist between two nodes. We define influence-based modularity and use it to partition a network into communities. We also use influence to measure the relative importance of nodes within the network. Our second contribution is mathematical formalism that allows us to represent complex networks by combining multiple heterogeneous types of evidence within a single model. We apply our approach to standard datasets used in literature and show that exploiting additional sources of evidence corresponding to links between, as well as among, different entity types leads to a better understanding of network structure. Besides identifying network structure, our approach can also identify the most influential members of communities, as well as the \u201cweak ties,\u201d who bridge different communities.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "On Constructing Shallow Taxonomies from Social Annotations.\n", "abstract": " Tagging in social media system has demonstrated to be a convenient way for users to annotate objects of interest. One reason behind its success obviously because tags can be chosen by users arbitrarily without any topic and specificity constraints. Although tags are free-from keywords, there are some evidences 1 suggesting that, for a particular object type, users tend to use \u201csimilar\u201d tag sets. In addition, such tags are in different levels of specificity. This might suggest that there are some hierarchical concepts behind users\u2019 tagging processes. In this paper, we outline a problem in extracting hierarchical concepts from social annotation data and propose a possible solution\u2013a probabilistic generative model that describes tagging processes.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "Spatial behavior of individuals and groups: preliminary findings from a museum scenario\n", "abstract": " In order to better understand human navigation, way-finding, and general spatial behavior, we are conducting research into the effects of social interactions among individuals within a shared space. This paper describes work in the integration of instrumentation of active public areas, macroscopic modeling, microscopic simulation, and experimentation with human subjects. The aim is to produce empirically grounded models of individual and collective spatial behavior. We describe the challenges and lessons learned from our experience with data collection, construction of tractable models, and pilot experiments.", "num_citations": "1\n", "authors": ["1193"]}
{"title": "WebCompass: an agent-based metasearch and metadata discovery tool for the Web\n", "abstract": " WebCompass | Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ir Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesIRProceedingsSIGIR '96WebCompass: an agent-based metasearch and metadata discovery tool for the Web ARTICLE WebCompass: an agent-based metasearch and metadata discovery tool for the Web Share on Authors: Bradley P. Allen profile image Brad Allen View Profile , John Elliot Jensen profile image John Jensen View Profile , Jay Nelson profile image Jay Nelson View Profile , Brian Ulicny profile image \u2026", "num_citations": "1\n", "authors": ["1193"]}