{"title": "CrowdBuild: a methodology for enterprise software development using crowdsourcing\n", "abstract": " We present and evaluate a software development methodology that addresses key challenges for the application of Crowd sourcing to an enterprise application development. Our methodology presents a mechanism to systematically break the overall business application into small tasks such that the tasks can be completed independently and in parallel by the crowd. Our methodology supports automated testing and automatic integration. We evaluate our methodology by developing a web application through Crowd sourcing. The methodology was tested through two Crowd sourcing models: one through contests and the other through hiring freelancers. We present various metrics of the Crowd sourcing experiment and compare against the estimate for the traditional software development methodology.", "num_citations": "34\n", "authors": ["1983"]}
{"title": "Trustworthiness in enterprise crowdsourcing: a taxonomy & evidence from data\n", "abstract": " In this paper we study the trustworthiness of the crowd for crowdsourced software development. Through the study of literature from various domains, we present the risks that impact the trustworthiness in an enterprise context. We survey known techniques to mitigate these risks. We also analyze key metrics from multiple years of empirical data of actual crowdsourced software development tasks from two leading vendors. We present the metrics around untrustworthy behavior and the performance of certain mitigation techniques. Our study and results can serve as guidelines for crowdsourced enterprise software development.", "num_citations": "25\n", "authors": ["1983"]}
{"title": "Assessing practitioner beliefs about software defect prediction\n", "abstract": " Just because software developers say they believe in \u201cX\u201d, that does not necessarily mean that \u201cX\u201d is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?. To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically, A commit that involves more added and removed lines is more bug-prone\u201d and \u201cFiles with fewer lines contributed by their owners (who contribute most changes) are bug-prone\u201d. Most of the widely-held beliefs studied are only sporadically supported in the data; i.e. large effects can appear in project data and then disappear in subsequent releases. Such sporadic\u00a0\u2026", "num_citations": "11\n", "authors": ["1983"]}
{"title": "Generating a test script execution order\n", "abstract": " A device may determine probabilities for test scripts associated with a test to be executed on a software element, where a respective probability is associated with a respective test script, indicates a likelihood that the respective test script will be unsuccessful in a test cycle, and is determined based on historical test results, associated with the software element, for the respective test script. The device may generate, based on the probabilities, a test script execution order, of the test scripts, for the test cycle, and may execute, based on the test script execution order, the test on the software element in the test cycle. The device may dynamically generate, based on results for the test in the test cycle, an updated test script execution order, and may execute, based on the updated test script execution order, the test on the software element in the test cycle.", "num_citations": "3\n", "authors": ["1983"]}
{"title": "Assessing Practitioner Beliefs about Software Engineering\n", "abstract": " Software engineering is a highly dynamic discipline. Hence, as times change, so too might our beliefs about core processes in this field. This paper checks some five beliefs that originated in the past decades that comment on the relationships between (i) developer productivity;(ii) software quality and (iii) years of developer experience. Using data collected from 1,356 developers in the period 1995 to 2006, we found support for only one of the five beliefs titled \u201cQuality entails productivity.\u201d We found no clear support for four other beliefs based on programming languages and software developers. However, from the sporadic evidence of the four other beliefs, we learned that a narrow scope could delude practitioners in misinterpreting certain effects to hold in their day-to-day work. Lastly, through an aggregated view of assessing the five beliefs, we find programming languages act as a confounding factor for developer\u00a0\u2026", "num_citations": "2\n", "authors": ["1983"]}
{"title": "Assessing practitioner beliefs\n", "abstract": " Just because software developers say they believe in \u201cX\u201d that does not necessarily mean that \u201cX\u201d is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?.To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically,\u201cA commit that involves more added and removed lines is more bugprone\u201d and \u201cFiles with fewer lines contributed by their owners (who contribute most changes) are bug-prone\u201d. Most of the widely-held beliefs studied are only sporadically supported in the data; ie large effects can appear in project data and then disappear in subsequent releases. Such sporadic support explains why developers believe things that were relevant to their prior work, but not necessarily their current work. Our conclusion will be that we need to change the nature of the debate with SE. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time.", "num_citations": "2\n", "authors": ["1983"]}
{"title": "Simulation of Consensus Based Approaches to Mitigate the Challenges in Crowdsourcing.\n", "abstract": " Crowdsourcing is an emerging area and has evolved as a powerful practice to leverage the collective intelligence of the crowd. It has been applied in various domains ranging from creative resolution of a problem to improving the business process using several platforms such as CrowdFlower, Freelancer and Amazon Mechanical Turk. Crowd is a creative workforce that has niche abilities to solve complex business challenges across various domains. It can be seen as an alternate workforce by participating in all phases of software development life cycle. However the common problem seen in crowdsourcing is the quality of the work performed by the crowd mostly due to the anonymity of the crowd member. In this work, we evaluated consensus based approach to assess the quality of the work done by the crowd through a simulation of crowd behavior. We also investigated the performance of these techniques for evaluating crowd members.", "num_citations": "2\n", "authors": ["1983"]}
{"title": "Early Life Cycle Software Defect Prediction. Why? How?\n", "abstract": " Many researchers assume that, for software analytics, \u201cmore data is better.\u201d We write to show that, at least for learning defect predictors, this may not be true. To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models. We hope these results inspire other researchers to adopt a \u201csimplicity-first\u201d approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for \u201cshort cuts\u201d that can simplify the analysis.", "num_citations": "1\n", "authors": ["1983"]}
{"title": "Incident prediction and prevention\n", "abstract": " In some examples, incident prediction and prevention may include ascertaining a plurality of past incidents, clustering the plurality of past incidents to generate a plurality of incidents clusters, and identifying, for each past incident of the plurality of past incidents that is in a respective incidents cluster of the plurality of incidents clusters, a time of occurrence. Incident prediction and prevention may include ascertaining a new incident, assigning the new incident to an incidents cluster of the plurality of incidents clusters, and determining, for the assigned incidents cluster, at least one further predicted incident associated with at least one further corresponding incidents cluster. Further, incident prediction and prevention may include determining a resolution to the at least one further predicted incident, and preventing occurrence of the at least one further predicted incident by executing the determined resolution to the at\u00a0\u2026", "num_citations": "1\n", "authors": ["1983"]}
{"title": "Assessing Developer Beliefs: A Reply to\" Perceptions, Expectations, and Challenges in Defect Prediction\"\n", "abstract": " It can be insightful to extend qualitative studies with a secondary quantitative analysis (where the former suggests insightful questions that the latter can answer). Documenting developer beliefs should be the start, not the end, of Software Engineering research. Once prevalent beliefs are found, they should be checked against real-world data. For example, this paper finds several notable discrepancies between empirical evidence and the developer beliefs documented in Wan et al.'s recent TSE paper\" Perceptions, expectations, and challenges in defect prediction\". By reporting these discrepancies we can stop developers (a) wasting time on inconsequential matters or (b) ignoring important effects. For the future, we would encourage more\" extension studies\" of prior qualitative results with quantitative empirical evidence.", "num_citations": "1\n", "authors": ["1983"]}