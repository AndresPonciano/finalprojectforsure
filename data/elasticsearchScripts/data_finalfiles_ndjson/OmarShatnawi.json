{"title": "Unified framework for developing testing effort dependent software reliability growth models\n", "abstract": " Several software reliability growth models (SRGMs) have been presented in the literature in the last three decades. These SRGMs take into account different testing environment depending on size and efficiency of testing team, type of components and faults, design of test cases, software architecture etc. The plethora of models makes the model selection an uphill task. Recently, some authors have tried to develop a unifying approach so as to capture different growth curves, thus easing the model selection process. The work in this area done so far relates the fault removal process to the testing/execution time and does not consider the consumption pattern of testing resources such as CPU time, manpower and number of executed test cases. More realistic modeling techniques can result if the reliability growth process is studied with respect to the amount of expended testing efforts. In this paper, we propose a unified framework for testing effort dependent software reliability growth models incorporating imperfect debugging and error generation. The proposed framework represents the realistic case of time delays between the different stages of fault removal process ie Failure Observation/Fault Detection and Fault Removal/Correction processes. The Convolution of probability distribution functions have been used to characterize time differentiation between these two processes. Several existing and new effort dependent models have been derived by using different types of distribution functions. We have also provided data analysis based on the actual software failure data sets for some of the models discussed and proposed in the paper.", "num_citations": "36\n", "authors": ["436"]}
{"title": "Why software reliability growth modelling should define errors of different severity\n", "abstract": " Why software reliability growth modelling should define errors of different severity - Dialnet Ayuda \u00bfEn qu\u00e9 podemos ayudarle? \u00d7 Buscar en la ayuda Buscar Consultar la ayuda Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda Why software reliability growth modelling should define errors of different severity Autores: PK Kapur, AK Bardhan, Omar Shatnawi Localizaci\u00f3n: Quality control and applied statistics, ISSN 0033-5207, Vol. , N\u00ba. , 2004, p\u00e1gs. 699-702 Idioma: ingl\u00e9s Texto completo no disponible (Saber m\u00e1s ...) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook Twitter Opciones de entorno Sugerencia / Errata \u00a9 2001-2021 Fundaci\u00f3n Dialnet \u00b7 Todos los derechos reservados Dialnet Plus Accesibilidad Aviso \u2026", "num_citations": "26\n", "authors": ["436"]}
{"title": "Discrete time NHPP models for software reliability growth phenomenon.\n", "abstract": " Nonhomogeneous poisson process based software reliability growth models are generally classified into two groups. The first group contains models, which use the machine execution time or calendar time as a unit of fault detection/removal period. Such models are called continuous time models. The second group contains models, which use the number of test occasions/cases as a unit of fault detection period. Such models are called discrete time models, since the unit of software fault detection period is countable. A large number of models have been developed in the first group while there are fewer in the second group. Discrete time models in software reliability are important and a little effort has been made in this direction. In this paper, we develop two discrete time SRGMs using probability generating function for the software failure occurrence/fault detection phenomenon based on a NHPP namely, basic and extended models. The basic model exploits the fault detection/removal rate during the initial and final test cases. Whereas, the extended model incorporates fault generation and imperfect debugging with learning. Actual software reliability data have been used to demonstrate the proposed models. The results are fairly encouraging in terms of goodness-of-fit and predictive validity criteria due to applicability and flexibility of the proposed models as they can capture a wide class of reliability growth curves ranging from purely exponential to highly S-shaped.", "num_citations": "23\n", "authors": ["436"]}
{"title": "A software fault classification model\n", "abstract": " This paper presents a software reliability growth model (SRGM) for classification of software faults during testing phase based on a non-homogeneous Poisson process (NHPP). The model assumes       that the testing phase consists of three processes namely, failure observation, fault isolation and fault removal. The software faults are classified into three types namely, simple, hard and       complex according to the amount of testing-effort needed to remove them. The removal complexity is proportional to the amount of testing-effort required to remove the fault. The testing-effort       expenditures are represented by the number of stages required to remove the fault after the failure observation or fault isolation (with delay between the stages). The time delay between the       failure observation and the subsequent fault removal is assumed to represent the severity of the fault. The more severe the fault, the more the\u00a0\u2026", "num_citations": "17\n", "authors": ["436"]}
{"title": "A generalized software fault classification model\n", "abstract": " Most non-homogenous Poisson process (NHPP) based software reliability growth models (SRGMs) presented in the literature assume that the faults in the software are of the same type. However, this assumption is not truly representative of reality. It has been observed that the software contains different types of faults and each fault requires different strategies and different amount of testing-effort to remove it. If this assumption is not taken into account, the SRGM may give misleading results. This paper proposes a generalized model based on classification the faults in the software system according to their removal complexity. The removal complexity is proportional to the amount of testing-effort required to remove the fault. The testing-effort expenditures are represented by the number of stages required to remove the fault after the failure observation/fault isolation (with time delay between the stages). Therefore, it explicitly takes into account the faults of different severity and can capture variability in the growth curves depending on the environment it is being used and at the same time it has the capability to reduce either to exponential or S-shaped growth curves. Such modelling approach is very much suited for object-oriented programming and distributed development environments. Actual software reliability data have been used to demonstrate the proposed generalized model.", "num_citations": "15\n", "authors": ["436"]}
{"title": "Measuring commercial software operational reliability: an interdisciplinary modelling approach\n", "abstract": " In the software reliability engineering (SRE) literature, few attempts have been made to model the failure phenomenon of commercial software during its operational use. One of the reasons can be attributed to the inability of software engineers to measure the growth in usage of commercial software while it is in the market. It is unlike the testing phase where resources follow a definite pattern. In this paper, an attempt has been made to model the software reliability growth linking it to the number of users. Since the number of instructions executed depends on the number of users. The number of users is estimated through an innovation diffusion model of marketing. Once the estimated value is known, the rate at which instructions are executed can be found. The intensity with which failures would be reported depends upon this value. To model the failure observation or defect removal phenomena, a non-homogenous Poisson process (NHPP) based software reliability models developed in the literature have been employed. Software reliability models are most often used for reliability projection when development work is complete and before the software is shipped to customers. They can also be used to model the failure pattern or the defect arrival pattern in the field and thereby provide valuable input to maintenance planning. Numerical example with real software field reliability data is presented to illustrate descriptive and predictive performance as well as to show practical applications of the proposed models.", "num_citations": "13\n", "authors": ["436"]}
{"title": "Release time determination depending on number of test runs using multi attribute utility theory\n", "abstract": " To achieve competitive success in software industry, technological innovation is very important. Due to stiff competition, the software developers are trying very hard to survive in the market by adding some new features to the existing software. A most effective way of handling software development method is to go version-by-version. This stepwise release is termed as multi-release of a software product. Due to demand of new features and highly reliable software system, the software industries are speeding their up-gradations/add-ons in the software. An important decision problem that the management encounters is to determine when to stop testing and release the software system to the user. Such a problem is known as \u201cSoftware Release Time Problem\u201d. We propose an optimization problem of determining the optimal time of software release based on goals set by the management in terms of cost\u00a0\u2026", "num_citations": "12\n", "authors": ["436"]}
{"title": "An integrated framework for developing discrete\u2010time modelling in software reliability engineering\n", "abstract": " In the software reliability engineering literature, few attempts have been made to study the fault debugging environment using discrete\u2010time modelling. Most endeavours assume that a detected fault to have been either immediately removed or is perfectly debugged. Such discrete\u2010time models may be used for any debugging environment and may be termed black\u2010box, which are used without having prior knowledge about the nature of the fault being debugged. However, if one has to develop a white\u2010box model, one needs to be cognizant of the debugging environment. During debugging, there are numerous factors that affect the debugging process. These factors may include the internal, for example, fault density, and fault debugging complexity and the external that originates in the debugging environment itself, such as the skills of the debugging team and the debugging effort expenditures. Hence, the debugging\u00a0\u2026", "num_citations": "11\n", "authors": ["436"]}
{"title": "Testing-effort dependent software reliability model for distributed systems\n", "abstract": " Distributed systems are being developed in the context of the client-server architecture. Client-server architectures dominate the landscape of computer-based systems. Client-server systems are developed using the classical software engineering activities. Developing distributed systems is an activity that consumes time and resources. Even if the degree of automation of software development activities increased, resources are an important limitation. Reusability is widely believed to be a key direction to improving software development productivity and quality. Software metrics are needed to identify the place where resources are needed; they are an extremely important source of information for decision making. In this paper, an attempt has been made to describe the relationship between the calendar time, the fault removal process and the testing-effort consumption in a distributed development environment\u00a0\u2026", "num_citations": "9\n", "authors": ["436"]}
{"title": "Discrere time modelling in software reliability engineering\u2014A unified approach\n", "abstract": " In the software reliability engineering literature, few attempts have been made to measure software reliability using discrete time modeling. One of the reasons can be attributed to the mathematical complexity involved in constructing such models. The proposed unified modelling approach provides a broad framework for developing NHPP type of discrete SRGMs. The framework adopts the number of test occasions/cases as a unit of fault detection/removal period, which is countable and more appropriate measure than CPU time or calendar time used in continuous SRGMs. And classifies the faults that encountered during software testing phase into three types of faults namely, simple, hard and complex according to their removal complexity. Accordingly, their fault removal processes is modelled separately and the total fault removal phenomena is the superposition of the underlying processes. Such type of modelling approach is very much suited for object-oriented and distributed systems development environments. Actual software reliability data have been used to demonstrate the proposed framework.", "num_citations": "8\n", "authors": ["436"]}
{"title": "The Pham Nordmann Zhang (PNZ) software reliability model revisited\n", "abstract": " The Pham Nordmann Zhang (PNZ) software reliability model [12] is revisited and some research directions are further discussed. The PNZ model assumed that on a failure, the fault causing the failure is removed with certainty. In reality this may always not be true. In this paper, a newly developed continuous SRGM with two types of imperfect debugging and learning process of the testing team as testing progresses is proposed. The first type, known as fault generation, describes the situation when each fault removal attempt increases the fault content of the software. The second type, less damaging, is the case of imperfect debugging where all detected faults are not removed completely. Here the numbers of removal attempts are more than actual fault content but imperfect debugging does not change the content of faults in the software. The concept of learning has been incorporated in the fault removal rate to show the gain in experience and improvement in the testing efficiency of the team as the testing grows. To model learning, fault removal rate has been taken as logistic function. The proposed model has been validated and compared with well-established existing NHPP models by applying them on two real fault removal datasets. The results are encouraging in terms of goodness of fit criteria, predictive validity criterion, and software reliability evaluation measures for software reliability data due to its applicability and flexibility. A discrete version of the proposed model has also been presented.", "num_citations": "6\n", "authors": ["436"]}
{"title": "On the development of unified scheme for discrete software reliability growth modeling\n", "abstract": " The importance of Software Reliability Growth Models to control the testing process and for quantitative assessment of software reliability is a well established fact. However, difficulties created by their underlying assumptions, their relevance and validity to real testing environment have made the selection of appropriate model an uphill task. Recently, new dimensions have been added to software reliability engineering with the development of unified modeling schemes. These schemes have proved seminal in the development of the general theory, partially because of their simplicity and mathematical tractability. In this paper, we propose a unified scheme for discrete software reliability growth modeling using the fault detection/correction rate function. Standard probability distributions have been used to model the fault correction and detection times. Initially, we have formulated the unified scheme when the fault\u00a0\u2026", "num_citations": "5\n", "authors": ["436"]}
{"title": "Modelling Software Fault Dependency Using Lag Function\n", "abstract": " This paper proposes a new software reliability growth model (SRGM) based on the non-homogenous Poisson process (NHPP) that assumes the presence of two types of faults in the software namely, leading and dependent faults. Leading faults are those that can be removed upon a failure. But dependent faults are masked by the leading faults and can be removed only after the corresponding leading fault has been removed. A time dependent lag function has been introduced which can account for delay in removal of dependent faults. The proposed model has the ability to fit a variety of reliability growth curves and has been validated on actual software test data sets and its performance has been compared with well-documented SRGMs in the literature. A discrete version of the proposed model has also been presented.", "num_citations": "3\n", "authors": ["436"]}
{"title": "Mathematical modelling in software reliability\n", "abstract": " A mathematical model based on stochastic and statistics theories is useful to describe the software fault-removal phenomena or the software failure-occurrence phenomena and estimate the software reliability quantitatively. A mathematical tool which describes software reliability aspect is a software reliability growth model (SRGM). Discrete time models in software reliability are important and a little effort has been made in this direction. Their importance cannot be underestimated since the number of test cases is more appropriate measure of the fault removal/detection period than the CPU/calendar time used by continuous time model. These models generally provide a better fit than their continuous time counterparts. It is important to note that due to the complexity of software design, it is not expected that any single model can incorporate all factors which are thought to influence software reliability. In this paper, we show how beginning with very simple assumptions, non-homogenous Poisson process (NHPP) type of discrete time SRGMs, are gradually made more realistic with the incorporation of imperfect debugging, involvement of a learning-process in debugging and introduction of new faults. The applicability of the resultant generalized model is demonstrated through several actual software reliability data sets obtained from different software development projects. The proposed generalized model is also checked against different components of the model, including existing one, thus highlighting its applicability.", "num_citations": "2\n", "authors": ["436"]}
{"title": "Release Policy, Change-Point Concept, and Effort Control through Discrete-Time Imperfect Software Reliability Modelling\n", "abstract": " Nonhomogeneous Poisson process based software reliability models play an important role in developing software systems and enhancing the performance of computer software. As software reliability grows on the basis of the execution of computer test runs. Nonhomogeneous Poisson process type of discrete-time software reliability models, or difference equations, is more realistic and often provides better fit than their continuous-time counterparts. Since discrete-time model conserves the properties of the continuous-time model, the estimation of its parameter would be simpler and more accurate. In this paper, we explore the importance of testing resource and imperfect debugging phenomenon consideration in software reliability growth modeling. The resultant model is very useful for the reliability analysis as the measure of reliability is computed considering the distribution of testingeffort, influence of the testing efficiency and the changes of the testing process. Using the resultant model, testing-effort control, change-point concept and optimal release policy have also been investigated. Therefore, this paper thus provides a new insight into development of discrete-time modelling in software reliability engineering, that could be of immense help to the software project manager in monitoring and controlling the testing process closely and effectively allocating the resources in order to reduce the testing cost and to meet the given reliability requirements.", "num_citations": "1\n", "authors": ["436"]}
{"title": "Modelling software fault debugging complexity under imperfect debugging environment\n", "abstract": " The fault debugging progress is influenced by various factors all of which may not be deterministic in nature such as the debugging effort, debugging efficiency and debuggers skill, and debugging methods and strategies. In order to address these realistic factors that influencing the debugging process we propose an integrated no homogeneous Poisson process (NHPP) based software reliability model. The integrated modelling approach incorporates the effect of imperfect fault debugging environment, fault debugging complexity and learning debuggers\u2019 phenomenon. The debugging phase is assumed to be composed of three processes namely, fault detection, fault isolation and fault removal. The software faults are categorized into three types namely, simple, hard and complex according to their debugging complexity. As the debugging progresses, the fault removal rate changes to capture learning process of the debuggers. In order to relax the ideal debugging environment, two types of imperfect debugging phenomena are incorporated. Incorporating the imperfect fault debugging phenomena in software reliability modelling is very important to the reliability measurement as it is related to the efficiency of the debugging team. Accordingly, the total debugging process is the superposition of the three debugging activities processes. Such modeling approach can capture the variability in the software reliability growth curve due to debugging complexity of the faults depending on the debugging environment which enables the management to plan and control their debugging activities to tackle each type of fault. Actual test datasets cited from real\u00a0\u2026", "num_citations": "1\n", "authors": ["436"]}