{"title": "Representing and using nonfunctional requirements: A process-oriented approach\n", "abstract": " The paper proposes a comprehensive framework for representing and using nonfunctional requirements during the development process. The framework consists of five basic components which provide for the representation of nonfunctional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of nonfunctional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.", "num_citations": "1515\n", "authors": ["1443"]}
{"title": "On conceptual modelling: Perspectives from artificial intelligence, databases, and programming languages\n", "abstract": " The growing demand for systems of ever-increasing complexity and precision has stimulated the need for higher level concepts, tools, and techniques in every area of Computer Science. Some of these areas, in particular Artificial Intelligence, Databases, and Programming Lan guages, are attempting to meet this demand by defining a new, more abstract level of system description. We call this new level conceptual in recognition of its basic conceptual nature. In Artificial Intelligence, the problem of designing an expert system is seen primarily as a problem of building a knowledge base that repre sents knowledge about an enterprise. Consequently, Knowledge Repre sentation is viewed as a central issue in Artificial Intelligence research. Database design methodologies developed during the last five years are almost unanimous in offering semantic data models in terms of which the designer directly and naturally models an enterprise before proceed ing to a detailed logical and physical database design. In Programming Languages, different forms of abstraction which allow implementation independent specifications of data, functions, and control have been a major research theme for a decade. To emphasize the common goals of these three research efforts, we call this new activity conceptual modelling.", "num_citations": "1064\n", "authors": ["1443"]}
{"title": "A language facility for designing database-intensive applications\n", "abstract": " TAXIS, a language for the design of interactive information systems (e.g., credit card verification, student-course registration, and airline reservations) is described. TAXIS offers (relational) database management facilities, a means of specifying semantic integrity constraints, and an exception-handling mechanism, integrated into a single language through the concepts of class, property, and the IS-A (generalization) relationship. A description of the main constructs of TAXIS is included and their usefulness illustrated with examples.", "num_citations": "638\n", "authors": ["1443"]}
{"title": "Data management for peer-to-peer computing: A vision\n", "abstract": " We motivate special database problems introduced by peer-to-peer computing and propose the Local Relational Model (LRM) to solve some of them. As well, we summarize a formalization of LRM, present an architecture for a prototype implementation, and discuss open research questions.", "num_citations": "531\n", "authors": ["1443"]}
{"title": "Conceptual modelling and Telos\n", "abstract": " We review basic premises underlying the application of conceptual modelling to the development of information systems and point out a fundamental problem arising from the broad range of concepts that need to be modelled. We then argue that conventional conceptual models are weak for such broad domains of discourse because they come with built-in collections of primitive notions in terms of which conceptual modelling is to be done. Telos is then introduced as a conceptual modelling language designed for capturing knowledge about information systems and it is argued that, unlike its peers, it offers facilities not only for modelling an application but also the notions used to model an application. The presentation of features of the language is eclectic and generally non-technical. Details about Telos can be found in [Mylopoulos90] and [Koubarakis89].", "num_citations": "506\n", "authors": ["1443"]}
{"title": "Knowledge representation as the basis for requirements specifications\n", "abstract": " Specification of many kinds of knowledge about the world is essential to requirements engineering. Research on knowledge representation in artificial intelligence provides a wealth of relevant techniques that can be incorporated into specification languages.", "num_citations": "404\n", "authors": ["1443"]}
{"title": "Information Modeling in the Time of the Revolution\n", "abstract": " Information modeling is concerned with the construction of computer-based symbol structures which capture the meaning of information and organize it in ways that make it understandable and useful to people. Given that information is becoming an ubiquitous, abundant and precious resource, its modeling is serving as a core technology for information systems engineering.We present a brief history of information modeling techniques in Computer Science and briefly survey such techniques developed within Knowledge Representation (Artificial Intelligence), Data Modeling (Databases), and Requirements Analysis (Software Engineering and Information Systems). We then offer a characterization of information modeling techniques which classifies them according to their ontologies, i.e., the type of application for which they are intended, the set of abstraction mechanisms (or, structuring principles) they support, as\u00a0\u2026", "num_citations": "357\n", "authors": ["1443"]}
{"title": "On knowledge base management systems: integrating artificial intelligence and database technologies\n", "abstract": " Current experimental systems in industry, government, and the military take advantage of knowledge-based processing. For example, the Defense Advanced Research Projects Agency (DARPA), and the United States Geological Survey (USGS) are supporting the develop ment of information systems that contain diverse, vast, and growing repositories of data (eg, vast databases storing geographic informa tion). These systems require powerful reasoning capabilities and pro cessing such as data processing, communications, and multidisciplinary of such systems will scientific analysis. The number and importance grow significantly in the near future. Many of these systems are severely limited by current knowledge base and database systems technology. Currently, knowledge-based system technology lacks the means to provide efficient and robust knowledge bases, while database system technology lacks knowledge representation and reasoning capabilities. The time has come to face the complex research problems that must be solved before we can design and implement real, large scale software systems that depend on knowledge-based processing. To date there has been little research directed at integrating knowledge base and database technologies. It is now imperative that such coordinated research be initiated and that it respond to the urgent need for a tech nology that will enable operational large-scale knowledge-based system applications.", "num_citations": "331\n", "authors": ["1443"]}
{"title": "An overview of knowledge representation\n", "abstract": " Knowledge Representation is a central problem in Artificial Intelligence (AI) today. Its importance stems from the fact that the current design paradigm for \u201cintelligent\u201d systems stresses the need for the availability of expert knowledge in the system along with associated knowledge handling facilities. This paradigm is in sharp contrast to earlier ones which might be termed \u201cpower-oriented\u201d [Goldstein and Papert 77] in that they placed an emphasis on genera purpose heuristic search techniques [Nilsson 71].", "num_citations": "315\n", "authors": ["1443"]}
{"title": "On formal requirements modeling languages: RML revisited\n", "abstract": " Research issues related to requirements modeling are introduced and discussed through a review of the requirements modeling language RML, its peers and its successors from the time it was first proposed at the Sixth International Conference on Software Engineering (ICSE-6) to the present - ten ICSEs later. We note that the central theme of \"Capturing More World Knowledge\" in the original RML proposal is becoming increasingly important in requirements engineering. The paper highlights key ideas and research issues that have driven RML and its peers, evaluates them retrospectively in the context of experience and more recent developments, and points out significant remaining problems and directions for requirements modeling research.< >", "num_citations": "260\n", "authors": ["1443"]}
{"title": "Generalization/specialization as a basis for software specification\n", "abstract": " This paper describes a software specification methodology based on the notion of concept specialization. The methodology, which is particularly useful for Information Systems, applies uniformly to the various components of such systems, such as data classes, transactions, exceptions, and user interfaces (scripts), and its goal is the systematic and structured description of highly detailed world models, where concepts occur in many variations. An example from the domain of university information systems is used to illustrate and motivate the approach.", "num_citations": "258\n", "authors": ["1443"]}
{"title": "The hyperion project: from data integration to data coordination\n", "abstract": " We present an architecture and a set of challenges for peer database management systems. These systems team up to build a network of nodes (peers) that coordinate at run time most of the typical DBMS tasks such as the querying, updating, and sharing of data. Such a network works in a way similar to conventional multidatabases. Conventional multidatabase systems are founded on key concepts such as those of a global schema, central administrative authority, data integration, global access to multiple databases, permanent participation of databases, etc. Instead, our proposal assumes total absence of any central authority or control, no global schema, transient participation of peer databases, and constantly evolving coordination rules among databases. In this work, we describe the status of the Hyperion project, present our current solutions, and outline remaining research issues.", "num_citations": "252\n", "authors": ["1443"]}
{"title": "On the frame problem in procedure specifications\n", "abstract": " The paper provides examples of situations where formal specifications of procedures in the standard pre/postcondition style become lengthy, cumbersome and difficult to change, a problem which is particularly acute in the case of object oriented specifications with inheritance. We identify the problem as the inability to express that a procedure changes only those things it has to, leaving everything else unmodified, and review some attempts at dealing with this \"frame problem\" in the software specification community. The second part of the paper adapts a recent proposal for a solution to the frame problem in artificial intelligence-the notion of explanation closure axioms-to provide an approach whereby one can state such conditions succinctly and modularly, with the added advantage of having the specifier be reminded of things that she may have omitted saying in procedure specifications. Since this approach is\u00a0\u2026", "num_citations": "246\n", "authors": ["1443"]}
{"title": "A framework for visual motion understanding\n", "abstract": " A framework for the abstraction of motion concepts from sequences of images by computer is presented. The framework includes: 1) representation of knowledge for motion concepts that is based on semantic networks; and 2) associated algorithms for recognizing these motion concepts. These algorithms implement a form of feedback by allowing competition and cooperation among local hypotheses. They also allow a change of attention mechanism that is based on similarity links between knowledge units, and a hypothesis ranking scheme based on updating of certainty factors that reflect the hypothesis set inertia. The framework is being realized with a system called ALVEN. The purpose behind this system is to provide an evolving research prototype for experimenting with the analysis of certain classes of biomedical imagery, and for refining and quantifying the body of relevant medical knowledge.", "num_citations": "228\n", "authors": ["1443"]}
{"title": "Revisiting the core ontology and problem in requirements engineering\n", "abstract": " In their seminal paper in the ACM Transactions on Software Engineering and Methodology, Zave and Jackson established a core ontology for requirements engineering (RE) and used it to formulate the \"requirements problem\", thereby defining what it means to successfully complete RE. Given that stakeholders of the system-to-be communicate the information needed to perform RE, we show that Zave and Jackson's ontology is incomplete. It does not cover all types of basic concerns that the stakeholders communicate. These include beliefs, desires, intentions, and attitudes. In response, we propose a core ontology that covers these concerns and is grounded in sound conceptual foundations resting on a foundational ontology. The new core ontology for RE leads to a new formulation of the requirements problem that extends Zave and Jackson's formulation. We thereby establish new standards for what minimum\u00a0\u2026", "num_citations": "198\n", "authors": ["1443"]}
{"title": "Awareness requirements for adaptive systems\n", "abstract": " Recently, there has been a growing interest in self-adaptive systems. Roadmap papers in this area point to feedback loops as a promising way of operationalizing adaptivity in such systems. In this paper, we define a new type of requirement-called Awareness Requirement-that can refer to other requirements and their success/failures. We propose a way to elicit and formalize such requirements and offer a requirements monitoring framework to support them.", "num_citations": "186\n", "authors": ["1443"]}
{"title": "Capturing more world knowledge in the requirements specification\n", "abstract": " The view is adopted that software requirements involve the representation(modeling) of considerable real-world knowledge, not just functional specifications. A framework (RMF) for requirements models is presented and its main features are illustrated. RMF allows information about three types of conceptual entities (objects, activities, and assertions) to be recorded uniformly using the notion of properties. By grouping all entities into classes or metaclasses, and by organizing classes into generalization (specialization) hierarchies, RMF supports three abstraction principles(classification, aggregation, and generalization) which appear to be of universal importance in the development and organization of complex descriptions. Finally, by providing a mathematical model underlying our terminology, we achieve both unambiguity and the potential to verify consistency of the model.", "num_citations": "175\n", "authors": ["1443"]}
{"title": "Mapping adaptation under evolving schemas\n", "abstract": " Publisher SummaryThis chapter identifies the problem of mapping adaptation in dynamic environments with evolving schemas. To achieve interoperability, modem information systems and e-commerce applications use mappings to translate data from one representation to another. In dynamic environments like the Web, data sources may change not only their data but also their schemas, their semantics, and their query capabilities. Such changes must be reflected in the mappings. The chapter motivates the need for an automated system to adapt mappings and describes several areas in which the solutions can be applied. This chapter presents a novel framework and a tool, Toronto Mapping Adaptation System (ToMAS), that automatically maintains the consistency of the mappings as schemas evolve. The approach is unique in many ways. It considers and manages a very general class of mappings including\u00a0\u2026", "num_citations": "174\n", "authors": ["1443"]}
{"title": "Using semantic networks for data base management\n", "abstract": " This paper presents a semantic model of data bases. The model assumes the availability of a semantic network storing knowledge about a data base and a set of attributes for the data base. The use of the semantic net in generating a relational schema for the data base, in defining a set of semantic operators and in maintaining the data base consistent is then demonstrated.", "num_citations": "169\n", "authors": ["1443"]}
{"title": "Adaptive socio-technical systems: a requirements-based approach\n", "abstract": " A socio-technical system (STS) consists of an interplay of humans, organizations, and technical systems. STSs are heterogeneous, dynamic, unpredictable, and weakly controllable. Their operational environment changes unexpectedly, actors join and leave the system at will, actors fail to meet their objectives and under-perform, and dependencies on other actors are violated. To deal with such situations, we propose an architecture for STSs that makes an STS self-reconfigurable, i.e., capable of switching autonomously from one configuration to a better one. Our architecture performs a Monitor-Diagnose-Reconcile-Compensate cycle: it monitors actor behaviors and context changes, diagnoses failures and under-performance by checking whether monitored behavior is compliant with actors goals, finds a possible way to address the problem, and enacts compensation actions to reconcile actual and desired\u00a0\u2026", "num_citations": "168\n", "authors": ["1443"]}
{"title": "Techne: Towards a new generation of requirements modeling languages with goals, preferences, and inconsistency handling\n", "abstract": " Techne is an abstract requirements modeling language that lays formal foundations for new modeling languages applicable during early phases of the requirements engineering process. During these phases, the requirements problem for the system-to-be is being structured, its candidate solutions described and compared in terms of how desirable they are to stakeholders. We motivate the need for Techne, introduce it through examples, and sketch its formalization.", "num_citations": "165\n", "authors": ["1443"]}
{"title": "A requirements modeling language and its logic\n", "abstract": " This paper describes some aspects of a Requirements Modeling Language (RML) which can be used in the initial phases of software development. RML is based on the idea that a requirements specification should embody a conceptual world model and that the language for expressing it should provide facilities for organizing and abstracting details, yet at the same time have qualities such as precision, consistency and clarity.RML has a number of novel features including assertion classes, the treatment of time and various abbreviation techniques, all integrated into one uniform object-oriented framework. The precise semantics of these and other features are provided in this paper by relating RML to a logic involving time. This demonstrates that a language can offer highly structured and convenient mechanisms for requirements specifications while having solid mathematical underpinnings.", "num_citations": "149\n", "authors": ["1443"]}
{"title": "Requirements analysis for customizable software: A goals-skills-preferences framework\n", "abstract": " Software customization has been argued to benefit both the productivity of software engineers and end users. However, most customization methods rely on specialists to manually tweak individual applications for a specific user group. Existing software development methods also fail to acknowledge the importance of different kinds of user skills and preferences and how these might be incorporated into a customizable software design. We propose a framework for performing requirements analysis on user goals, skills, and preferences in order to generate a customizable software design. We illustrate our methodology with an email system and review an on-going case study involving users with traumatic brain injury.", "num_citations": "146\n", "authors": ["1443"]}
{"title": "Inferring complex semantic mappings between relational tables and ontologies from simple correspondences\n", "abstract": " There are many problems requiring a semantic account of a database schema. At its best, such an account consists of mapping formulas between the schema and a formal conceptual model or ontology (CM) of the domain. This paper describes the underlying principles, algorithms, and a prototype of a tool which infers such semantic mappings when given simple correspondences from table columns in a relational schema and datatype properties of classes in an ontology. Although the algorithm presented is necessarily heuristic, we offer formal results stating that the answers returned are \u201ccorrect\u201d for relational schemas designed according to standard Entity-Relationship techniques. We also report on experience in using the tool with public domain schemas and ontologies.", "num_citations": "140\n", "authors": ["1443"]}
{"title": "Case-based reasoning in IVF: Prediction and knowledge mining\n", "abstract": " In vitro fertilization (IVF) is a medically-assisted reproduction technique, enabling infertile couples to achieve successful pregnancy. Given the unpredictability of the task, we propose to use a case-based reasoning system that exploits past experiences to suggest possible modifications to an IVF treatment plan in order to improve overall success rates. Once the system's knowledge base is populated with a sufficient number of past cases, it can be used to explore and discover interesting relationships among data, thereby achieving a form of knowledge mining. The article describes the TA3IVF system\u2014a case-based reasoning system which relies on context-based relevance assessment to assist in knowledge visualization, interactive data exploration and discovery in this domain. The system can be used as an advisor to the physician during clinical work and during research to help determine what knowledge\u00a0\u2026", "num_citations": "136\n", "authors": ["1443"]}
{"title": "A procedural semantics for semantic networks\n", "abstract": " In this chapter, we describe an approach to the representation of knowledge that formalizes traditional semantic network concepts within a procedural framework. The basic entities of the representation are classes and (binary) relations, and their semantics are provided by a small set of programs. Two organizational principles are offered by the representation: the IS-A and PART-OF hierarchies, which allow the specification of generalization and whole-part relationships, respectively. The concept of a metaclass is also introduced (i.e., a class of classes), and it is shown how it can be used to explain certain features of the representation within itself. Properties of classes are then classified as structural or assertional and it is demonstrated that each type must have different inheritance rules, which can be expressed quite simply in the representation. Finally, a representation for programs is proposed in terms of the IS-A\u00a0\u2026", "num_citations": "136\n", "authors": ["1443"]}
{"title": "Representing and reasoning about preferences in requirements engineering\n", "abstract": " The priorities that stakeholders associate with requirements may vary from stakeholder to stakeholder and from one situation to the next. Differing priorities, in turn, imply different design decisions for the system to be. While elicitation of requirement priorities is a well-studied activity, modeling and reasoning with prioritization has not enjoyed equal attention. In this paper, we address this problem by extending a state-of-the-art goal modeling notation to support the representation of preference (\u201cnice-to-have\u201d) requirements. In our extension, preference goals are distinguished from mandatory ones. Then, quantitative prioritizations of the former are constructed and used as criteria for evaluating alternative ways to achieve the latter. To generate solutions, an existing preference-based planner is utilized to efficiently search for alternatives that best satisfy a given set of mandatory and preferred requirements. With\u00a0\u2026", "num_citations": "134\n", "authors": ["1443"]}
{"title": "Domain-specific conceptual modeling\n", "abstract": " This book represents the result of a community effort and cooperation to create and develop modeling methods and languages, based on the OMiLAB 1 Collaborative Environment.It aims to increase the visibility of domain-specific conceptual modeling by presenting work of thought leaders who designed and deployed a specific modeling method. Furthermore it provides a hands-on guidance on how to build models in a particular domain, such as requirements in engineering, business process modeling or enterprise architecture. Not only the results are presented, but also the ideas for future developments are communicated.", "num_citations": "123\n", "authors": ["1443"]}
{"title": "An empirical evaluation of the i* framework in a model-based software generation environment\n", "abstract": " Organizational modelling has been found to be very effective in facilitating the elicitation of requirements for organizational information systems. In this context, the i* modelling framework has been used widely in research and \u2013 some \u2013 industrial projects. However, no empirical evaluation exists to-date to identify areas of strength as well as weaknesses of the framework. This paper presents the results of an empirical evaluation of i* using industrial case studies. These were conducted in collaboration with an industrial partner who employs an object-oriented and model-driven approach for software development. The evaluation of i* uses a feature-based framework. The paper reports on lessons learned from this experience, both in terms of strengths and detected weaknesses. The results of this evaluation can play an important role in guiding extensions of the i* framework.", "num_citations": "111\n", "authors": ["1443"]}
{"title": "Data sharing in the hyperion peer database system\n", "abstract": " This demo presents Hyperion, a prototype system that supports data sharing for a network of independent Peer Relational Database Management Systems (PDBMSs). The nodes of such a network are assumed to be autonomous PDBMSs that form acquaintances at run-time, and manage mapping tables to define value correspondences among different databases. They also use distributed Event-Condition-Action (ECA) rules to enable and coordinate data sharing. Peers perform local querying and update processing, and also propagate queries and updates to their acquainted peers. The demo illustrates the following key functionalities of Hyperion:(1) the use of (data level) mapping tables to infer new metadata as peers dynamically join the network,(2) the ability to answer queries using data in acquaintances, and (3) the ability to coordinate peers through update propagation.", "num_citations": "108\n", "authors": ["1443"]}
{"title": "On the topological properties of quantized spaces, I. the notion of dimension\n", "abstract": " An attempt is made to define meaningful counterparts of topological notions in quantized spaces. Finitely presented Abelian groups are used as a model for such spaces. Then the notion of dimension is introduced through a recursive definition and it is proven that for free Abelian groups it equals the number of generators.", "num_citations": "105\n", "authors": ["1443"]}
{"title": "Integrating preferences into goal models for requirements engineering\n", "abstract": " Requirements can differ in their importance. As such the priorities that stakeholders associate with requirements may vary from stakeholder to stakeholder and from one situation to the next. Differing priorities, in turn, imply different design decisions for the end system. While elicitation of requirements priorities is a well studied activity, though, the modeling and reasoning side of prioritization has not enjoyed equal attention. In this paper, we address this by extending a traditional goal modeling notation to support the representation of optional and preference requirements. In our extension, optional goals are distinguished from mandatory ones. Then, quantitative prioritizations of the former are constructed and used as criteria for evaluating alternative ways to achieve the latter. A state-of-the-art preference-based planner is utilized to efficiently search for alternatives that best satisfy the given preferences. This way\u00a0\u2026", "num_citations": "96\n", "authors": ["1443"]}
{"title": "A semantic approach to XML-based data integration\n", "abstract": " The paper describes a prototype tool, named DIXSE, which supports the integration of XML Document Type Definitions (DTDs) into a common conceptual schema. The mapping from each individual DTD into the common schema is used to automatically generate wrappers for XML documents, which conform to a given DTD. These wrappers are used to populate the common conceptual schema thereby achieving data integration for XML documents.", "num_citations": "95\n", "authors": ["1443"]}
{"title": "Some features of the Taxis data model\n", "abstract": " This paper's principal goal is to provide a discussion on issues raised by the coexistence in a semantic data model of (i) An object-oriented framework including the notions of token, class and property as well as the IS-A and INSTANCE-OF relations;(ii) Transactions that can cause state changes;(iii) Special (null) values such as\" unknown\",\" nothing\" and\" inconsistent\".", "num_citations": "90\n", "authors": ["1443"]}
{"title": "Constructing complex semantic mappings between XML data and ontologies\n", "abstract": " Much data is published on the Web in XML format satisfying schemas, and to make the Semantic Web a reality, such data needs to be interpreted with respect to ontologies. Interpretation is achieved through a semantic mapping between the XML schema and the ontology. We present work on the heuristic construction of complex such semantic mappings, when given an initial set of simple correspondences from XML schema attributes to datatype properties in the ontology. To accomplish this, we first offer a mapping formalism to capture the semantics of XML schemas. Second, we present our heuristic mapping construction algorithm. Finally, we show through an empirical study that considerable effort can be saved when constructing complex mappings by using our prototype tool.", "num_citations": "89\n", "authors": ["1443"]}
{"title": "Intelligent agents and financial risk monitoring systems\n", "abstract": " A society of intelligent agents can work together to monitor financial transactions and yield important information regarding potential financial calamities.", "num_citations": "89\n", "authors": ["1443"]}
{"title": "Two views of data semantics: A survey of data models in artificial intelligence and database management\n", "abstract": " The goal of this paper is to establish that there exists a strong relationship between current issues of data models in Database Management and representations of knowledge in Artificial Intelligence. The different data modelling techniques that have been used in the two research areas are surveyed and classified into Predicate Calculus-based, Network, and Procedural ones. The similarities and differences between them are presented.", "num_citations": "85\n", "authors": ["1443"]}
{"title": "Self-repair through reconfiguration: A requirements engineering approach\n", "abstract": " High variability software systems can deliver their functionalities in multiple ways by reconfiguring their components. High variability has become important because of current trends towards software systems that come in product families, offer high levels of personalization, and fit well within a service-oriented architecture. The purpose of our research is to propose a framework that exploits such variability to allow a software system to self-repair in cases of failure. We propose an autonomic architecture that consists of monitoring, diagnosis, reconfiguration and execution components. This architecture uses requirements models as a basis for monitoring, diagnosis, and reconfiguration. We illustrate our proposal with a medium-sized publicly available case study (an automated teller machine (ATM) simulation), and evaluate its performance through a series of experiments. Our experimental results demonstrate that it is\u00a0\u2026", "num_citations": "84\n", "authors": ["1443"]}
{"title": "Local relational model: A logical formalization of database coordination\n", "abstract": " We propose a new data model intended for peer-to-peer (P2P) databases. The model assumes that each peer has a (relational) database and exchanges data with other peers (its acquaintances). In this context, one needs a data model that views the space of available data within the P2P network as an open collection of possibly overlapping and inconsistent databases. Accordingly, the paper proposes the Local Relational Model, develops a semantics for coordination formulas. The main result of the paper generalizes Reiter\u2019s characterization of a relational database in terms of a first order theory [1], by providing a syntactic characterization of a relational space in terms of a multi-context system. This work extends earlier work by Giunchiglia and Ghidini on Local Model Semantics [2].", "num_citations": "84\n", "authors": ["1443"]}
{"title": "GaiusT: supporting the extraction of rights and obligations for regulatory compliance\n", "abstract": " Ensuring compliance of software systems with government regulations, policies, and laws is a complex problem. Generally speaking, solutions to the problem first identify rights and obligations defined in the law and then treat these as requirements for the system under design. This work examines the challenge of developing tool support for extracting such requirements from legal documents. To address this challenge, we have developed a tool called GaiusT. The tool is founded on a framework for textual semantic annotation. It semiautomatically generates elements of requirements models, including actors, rights, and obligations. We present the complexities of annotating prescriptive text, the architecture of GaiusT, and the process by which annotation is accomplished. We also present experimental results from two case studies to illustrate the application of the tool and its effectiveness relative to manual\u00a0\u2026", "num_citations": "83\n", "authors": ["1443"]}
{"title": "Modeling domain variability in requirements engineering with contexts\n", "abstract": " Various characteristics of the problem domain define the context in which the system is to operate and thus impact heavily on its requirements. However, most requirements specifications do not consider contextual properties and few modeling notations explicitly specify how domain variability affects the requirements. In this paper, we propose an approach for using contexts to model domain variability in goal models. We discuss the modeling of contexts, the specification of their effects on system goals, and the analysis of goal models with contextual variability. The approach is illustrated with a case study.", "num_citations": "83\n", "authors": ["1443"]}
{"title": "A semantic approach to discovering schema mapping expressions\n", "abstract": " In many applications it is important to find a meaningful relationship between the schemas of a source and target database. This relationship is expressed in terms of declarative logical expressions called schema mappings. The more successful previous solutions have relied on inputs such as simple element correspondences between schemas in addition to local schema constraints such as keys and referential integrity. In this paper, we investigate the use of an alternate source of information about schemas, namely the presumed presence of semantics for each table, expressed in terms of a conceptual model (CM) associated with it. Our approach first compiles each CM into a graph and represents each table's semantics as a subtree in it. We then develop algorithms for discovering subgraphs that are plausible connections between those concepts/nodes in the CM graph that have attributes participating in\u00a0\u2026", "num_citations": "81\n", "authors": ["1443"]}
{"title": "Telos: Features and formalization\n", "abstract": " It has been argued that the first step in the development of an information system should be the description of the environment within which the system is intended to function together with the role the information system will eventually play within that environment. This is called the requirements modelling phase in information system development. In this report, we present the features and the formalization of Telos, a knowledge representation language for requirements modelling. Telos supports a structurally object-oriented representational framework which encourages the organization of knowledge. Moreover, the framework treats attributes as objects in their own right and provides representational facilities for time and assertions. As with other representation systems, assertions can either serve as constraints on the knowledge base or rules from which new facts can be derived. Interaction with the knowledge\u00a0\u2026", "num_citations": "81\n", "authors": ["1443"]}
{"title": "Guest editor's introduction: Cooperative information systems\n", "abstract": " Traditionally, information systems consisted of databases and files storing large amounts of data, applications programs performing useful update or reporting tasks, and user interfaces for data entry or retrieval. Construction of such systems required analysis of an organizational setting, design of databases and applications programs, and implementation that depended on database and programming technologies. Moreover, such constructions were generally tailored to the features and needs of the customer organization, resulting in development projects that have been notorious for their underestimated costs, late delivery dates, and reported failures.This picture is changing rapidly, partly because the software industry is maturing, making greater use of off-the-shelf components and generic solutions, and partly because of the onslaught of the information revolution. These changes have resulted in a new set of\u00a0\u2026", "num_citations": "80\n", "authors": ["1443"]}
{"title": "A core ontology for requirements\n", "abstract": " In their seminal paper (ACM T. Softw. Eng. Methodol., 6 (1)(1997), 1\u201330), Zave and Jackson established a core ontology for Requirements Engineering (RE) and used it to formulate the \u201crequirements problem\u201d, thereby defining what it means to successfully complete RE. Starting from the premise that the stakeholders of the system-to-be communicate to the software engineer the information needed to perform RE, Zave and Jackson's ontology is shown to be incomplete, in that it does not cover all classes of basic concerns\u2013namely, the beliefs, desires, intentions, and evaluations\u2013that the stakeholders communicate. In response, we provide a new core ontology for requirements that covers these classes of basic stakeholder concerns. The proposed new core ontology leads to a new formulation of the requirements problem. We thereby establish a new framework for the information that needs to be elicited over the\u00a0\u2026", "num_citations": "79\n", "authors": ["1443"]}
{"title": "Building semantic mappings from databases to ontologies\n", "abstract": " A recent special issue of AI Magazine (AAAI 2005) was dedicated to the topic of semantic integration\u2014the problem of sharing data across disparate sources. At the core of the solution lies the discovery the \u201csemantics\u201d of different data sources. Ideally, the semantics of data are captured by a formal ontology of the domain together with a semantic mapping connecting the schema describing the data to the ontology. However, establishing the semantic mapping from a database schema to a formal ontology in terms of formal logic expressions is inherently difficult to automate, so the task was left to humans. In this paper, we report on our study (An, Borgida, & Mylopoulos 2005a; 2005b) of a semi-automatic tool, called MAPONTO, that assists users to discover plausible semantic relationships between a database schema (relational or XML) and an ontology, expressing them as logical formulas/rules.", "num_citations": "79\n", "authors": ["1443"]}
{"title": "Requirements-driven software evolution\n", "abstract": " It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one\u00a0R\u2212. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature.", "num_citations": "76\n", "authors": ["1443"]}
{"title": "Readings in artificial intelligence and databases\n", "abstract": " The interaction of database and AI technologies is crucial to such applications as data mining, active databases, and knowledge-based expert systems. This volume collects the primary readings on the interactions, actual and potential, between these two fields. The editors have chosen articles to balance significant early research and the best and most comprehensive articles from the 1980s. An in-depth introduction discusses basic research motivations, giving a survey of the history, concepts, and terminology of the interaction. Major themes, approaches and results, open issues and future directions are all discussed, including the results of a major survey conducted by the editors of current work in industry and research labs. Thirteen sections follow, each with a short introduction. Topics examined include semantic data models with emphasis on conceptual modeling techniques for databases and information systems and the integration of data model concepts in high-level data languages, definition and maintenance of integrity constraints in databases and knowledge bases, natural language front ends, object-oriented database management systems, implementation issues such as concurrency control and error recovery, and representation of time and knowledge incompleteness from the viewpoints of databases, logic programming, and AI.", "num_citations": "75\n", "authors": ["1443"]}
{"title": "Evidence in the palm of your hand: Development of an outcomes\u2010focused knowledge translation intervention\n", "abstract": " Aim: The aim of the project was to develop an electronic information gathering and dissemination system to support both nursing\u2010sensitive outcomes data collection and evidence\u2010based decision\u2010making at the point\u2010of\u2010patient care.  Background: With the current explosion of health\u2010related knowledge, it is a challenge for nurses to regularly access information that is most current. The Internet provides timely access to health information, however, nurses do not readily use the Internet to access practice information because of being task\u2010driven and coping with heavy workloads. Mobile computing technology addresses this reality by providing the opportunity for nurses to access relevant information at the time of nurse\u2013patient contact.  Method: A cross\u2010sectional, mixed\u2010method design was used to describe nurses' requirements for point\u2010of\u2010care information collection and utilization. The sample consisted of 51 nurses\u00a0\u2026", "num_citations": "75\n", "authors": ["1443"]}
{"title": "Designing web services with tropos\n", "abstract": " We propose a methodology for designing Web services. The methodology is founded on Tropos (Perini et al., 2001 and Castro, et al., 2002), an agent-oriented software development technique, and supports early and late requirements analysis, as well as architectural and detailed design. An online retailer example is used for illustration of the proposed methodology. We also compare the generated design with a sample design presented in [BPT01].", "num_citations": "74\n", "authors": ["1443"]}
{"title": "Cerno: Light-weight tool support for semantic annotation of textual documents\n", "abstract": " Enrichment of text documents with semantic metadata reflecting their meaning facilitates document organization, indexing and retrieval. However, most web data remain unstructured because of the difficulty and the cost of manually annotating text. In this work, we present Cerno, a framework for semi-automatic semantic annotation of textual documents according to a domain-specific semantic model. The proposed framework is founded on light-weight techniques and tools intended for legacy code analysis and markup. To illustrate the feasibility of our proposal, we report experimental results of its application to two different domains. These results suggest that light-weight semi-automatic techniques for semantic annotation are feasible, require limited human effort for adaptation to a new domain, and demonstrate markup quality comparable with state-of-the-art methods.", "num_citations": "72\n", "authors": ["1443"]}
{"title": "Implementation of a compiler for a semantic data model: Experiences with taxis\n", "abstract": " The features of a compiler for the Taxis design language are described and discussed. Taxis offers an entity-based framework for designing interactive information systems and supports generalisation, classification and aggregation as abstraction mechanisms. Its features include multiple inheritance of attributes, isA hierarchies of transactions, metaclasses, typed attributes, a procedural exception-handling mechanism and an iteration construct based on the abstraction mechanisms supported Developing a compiler for the language involved dealing with the problems of efficiently representing and accessing a large collection of entities, performing (static) type checking and representing isA hierarchies of transactions.", "num_citations": "72\n", "authors": ["1443"]}
{"title": "From task-oriented to goal-oriented Web requirements analysis\n", "abstract": " Task analysis has been used traditionally in HCI and CSCW to define requirements for user interfaces, Web-based or otherwise. This paper argues that a shift of paradigm is needed in Web engineering from task-oriented to goal-oriented approaches for designing applications delivering a quality user experience and achieving the objectives of the stakeholders. Task models focus on fine-grained and precisely defined user needs, thereby risking a commitment to premature design decisions. Moreover, since task analysis focuses on users doing things with the system, tasks do not capture the goals of other stakeholders who are not users. Goal-oriented methods, on the other hand, provide specific support for coping with high-level users' and stakeholders' goals, facilitates the exploration of design alternatives and the definition of requirements at a suitable level of abstraction. As such, goal-based techniques are more\u00a0\u2026", "num_citations": "69\n", "authors": ["1443"]}
{"title": "Discovering the semantics of relational tables through mappings\n", "abstract": " Many problems in Information and Data Management require a semantic account of a database schema. At its best, such an account consists of formulas expressing the relationship (\u201cmapping\u201d) between the schema and a formal conceptual model or ontology (CM) of the domain. In this paper we describe the underlying principles, algorithms, and a prototype tool that finds such semantic mappings from relational tables to ontologies, when given as input simple correspondences from columns of the tables to datatype properties of classes in an ontology. Although the algorithm presented is necessarily heuristic, we offer formal results showing that the answers returned by the tool are \u201ccorrect\u201d for relational schemas designed according to standard Entity-Relationship techniques. To evaluate its usefulness and effectiveness, we have applied the tool to a number of public domain schemas and ontologies. Our\u00a0\u2026", "num_citations": "68\n", "authors": ["1443"]}
{"title": "Business process-based regulation compliance: The case of the sarbanes-oxley act\n", "abstract": " Balance Sheets and Annual Financial Reports play a major role in determining the public worth of any company. In the wake of corporate scandals such as Enron and WorldCom, the US and other countries passed legislation governing reporting processes. The Sarbanes Oxley Act of 2002 (hereafter SOX) requires US national securities exchange and US national security associations not to list any securities of any issuer that is not in compliance with the act. In this paper, we present a business process-based solution to the SOX compliance problem and offer evidence that such a solution is feasible through an industrial case study. The proposed solution aims to support SOX reporting requirements based on core business processes and a continuous improvement of the company's adopted business processes. This means that the solution integrates SOX-related tasks into the \"daily work\" of a company, rather than\u00a0\u2026", "num_citations": "67\n", "authors": ["1443"]}
{"title": "Classes and instances\n", "abstract": " The power of any information system technology is delimited by the expressiveness of the notation used to represent information. Classification constitutes a fundamental notational structuring mechanism and, not surprisingly, is supported in one form or another by many formal notations intended for data or knowledge modelling. This paper presents an overview of various manifestations of classification mechanisms. Further, characteristic features of classification, such as the form of inheritance allowed from classes to instances, having single or multiple classifications and the structure of the classification hierarchy are identified, discussed, and contrasted. The paper also describes the classification mechanism offered by the knowledge representation language Telos. In Telos, classification is stratified and applicable not only to objects but also to (binary) relationships. The paper argues that these features lead to a\u00a0\u2026", "num_citations": "67\n", "authors": ["1443"]}
{"title": "Runtime goal models: Keynote\n", "abstract": " Goal models capture stakeholder requirements for a system-to-be, but also circumscribe a space of alternative specifications for fulfilling these requirements. Recent proposals for self-adaptive software systems rely on variants of goal models to support monitoring and adaptation functions. In such cases, goal models serve as mechanisms in terms of which systems reflect upon their requirements during their operation. We argue that existing proposals for using goal models at runtime are using design artifacts for purposes they were not intended, i.e., for reasoning about runtime system behavior. In this paper, we propose a conceptual distinction between Design-time Goal Models (DGMs)-used to design a system-and Runtime Goal Models (RGMs)-used to analyze a system's runtime behavior with respect to its requirements. RGMs extend DGMs with additional state, behavioral and historical information about the\u00a0\u2026", "num_citations": "64\n", "authors": ["1443"]}
{"title": "'... and nothing else changes': the frame problem in procedure specifications\n", "abstract": " The first aim of this analysis is to outline a certain general problem which arises in all formal specifications using the pre/postcondition notation, and which is related to a longstanding problem in the field of AI, called the frame problem (J. McCarthy and P. Hayes, 1969). The authors then present examples illustrating this problem, which becomes particularly acute for large object-oriented specifications where inheritance plays a central role. The examples are intended to demonstrate that failure to deal with the frame problem compromises a formal specification language with respect to its notational suitability and its capacity to support a methodology for formally proving properties of specifications. How existing specification languages have endeavored to cope with the problem are reviewed. A novel approach is presented based on recent work intended to solve the frame problem in planning applications within AI.< >", "num_citations": "63\n", "authors": ["1443"]}
{"title": "Representing and querying data transformations\n", "abstract": " Modern information systems often store data that has been transformed and integrated from a variety of sources. This integration may obscure the original source semantics of data items. For many tasks, it is important to be able to determine not only where data items originated, but also why they appear in the integration as they do and through what transformation they were derived. This problem is known as data provenance. In this work, we consider data provenance at the schema and mapping level. In particular, we consider how to answer questions such as \"what schema elements in the source(s) contributed to this value\", or \"through what transformations or mappings was this value derived?\" Towards this end, we elevate schemas and mappings to first-class citizens that are stored in a repository and are associated with the actual data values. An extended query language, called MXQL, is also developed that\u00a0\u2026", "num_citations": "58\n", "authors": ["1443"]}
{"title": "System identification for adaptive software systems: A requirements engineering perspective\n", "abstract": " Control Theory and feedback control in particular have been steadily gaining momentum in software engineering for adaptive systems. Feedback controllers work by continuously measuring system outputs, comparing them with reference targets and adjusting control inputs if there is a mismatch. In Control Theory, quantifying the effects of control input on measured output is a process known as system identification. This process usually relies either on detailed and complex system models or on system observation. In this paper, we adopt a Requirements Engineering perspective and ideas from Qualitative Reasoning to propose a language and a systematic system identification method for adaptive software systems that can be applied at the requirements level, with the system not yet developed and its behavior not completely known.", "num_citations": "56\n", "authors": ["1443"]}
{"title": "Text mining through semi automatic semantic annotation\n", "abstract": " The Web is the greatest information source in human history. Unfortunately, mining knowledge out of this source is a laborious and error-prone task. Many researchers believe that a solution to the problem can be founded on semantic annotations that need to be inserted in web-based documents and guide information extraction and knowledge mining. In this paper, we further elaborate a tool-supported process for semantic annotation of documents based on techniques and technologies traditionally used in software analysis and reverse engineering for large-scale legacy code bases. The outcomes of the paper include an experimental evaluation framework and empirical results based on two case studies adopted from the Tourism sector. The conclusions suggest that our approach can facilitate the semi-automatic annotation of large document bases.", "num_citations": "56\n", "authors": ["1443"]}
{"title": "Theatermanagement: Eine Einf\u00fchrung\n", "abstract": " Das Lehrbuch Theatermanagement gibt eine Einf\u00fchrung in den Theaterbetrieb, seine Strukturen und Prozesse und die wesentlichen Grundlagen des Managements in den Bereichen Finanzen, Personal, Marketing und Vertrieb, Planung, Organisation und Kommunikation. Anhand von Fallbeispielen werden aktuelle Entwicklungen in der deutschen Theaterlandschaft und ihre Reformpotentiale vor dem Hintergrund der sich ver\u00e4ndernden Rahmenbedingungen analysiert. Der Autor geht davon aus, dass der Managementbegriff grunds\u00e4tzlich auf den Theaterbetrieb anwendbar ist, wenn eine Symbiose zwischen k\u00fcnstlerischen und wirtschaftlich-organisatorischen Aspekten hergestellt wird. Die wesentliche Zielstellung dabei ist es, die Zukunftsf\u00e4higkeit des einzelnen Theaterbetriebs wie des deutschen Theatersystems in seiner Gesamtheit durch Transformationsprozesse wieder herzustellen.", "num_citations": "54\n", "authors": ["1443"]}
{"title": "The common ontology of value and risk\n", "abstract": " Risk analysis is traditionally accepted as a complex and critical activity in various contexts, such as strategic planning and software development. Given its complexity, several modeling approaches have been proposed to help analysts in representing and analyzing risks. Naturally, having a clear understanding of the nature of risk is fundamental for such an activity. Yet, risk is still a heavily overloaded and conceptually unclear notion, despite the wide number of efforts to properly characterize it, including a series of international standards. In this paper, we address this issue by means of an in-depth ontological analysis of the notion of risk. In particular, this analysis shows a surprising and important result, namely, that the notion of risk is irreducibly intertwined with the notion of value and, more specifically, that risk assessment is a particular case of value ascription. As a result, we propose a concrete artifact\u00a0\u2026", "num_citations": "53\n", "authors": ["1443"]}
{"title": "Coordinating peer databases using ECA rules\n", "abstract": " Peer databases are stand-alone, independently developed databases that are linked to each other through acquaintances. They each contain local data, a set of mapping tables and expressions, and a set of ECA rules that are used to exchange data among them. The set of acquaintances and peers constitutes a dynamic peer-to-peer network in which acquaintances are continuously established and abolished. We present techniques for specifying data exchange policies on-the-fly based on constraints imposed on the way in which peers exchange and share data. We realize the on-the-fly specification of data exchange policies by building coordination ECA rules at acquaintance time. Finally, we describe mechanisms related to establishing and abolishing acquaintances by means of examples. Specifically, we consider syntactical constructs and executional semantics of establishing and abolishing\u00a0\u2026", "num_citations": "53\n", "authors": ["1443"]}
{"title": "From information system requirements to designs: a mapping framework\n", "abstract": " Comprehensive methodologies for information system development need to provide a framework for the adequate representation of system requirements and also for their usage in generating system designs. Requirements specifications are assumed to include a functional description of what the information system is intended to do, how it will interact with its environment, what information it will manage and how that information relates to the system's environment. p]The generation of a design is achieved by mapping elements of the requirements model into one or more corresponding design objects. This mapping process is guided by two considerations. Locally, the process is directed by dependency types among requirements and design objects which determine allowable mappings for a particular requirements object. Globally, the process is guided by non-functional requirements, such as accuracy and\u00a0\u2026", "num_citations": "53\n", "authors": ["1443"]}
{"title": "Requirements-based software system adaptation\n", "abstract": " Nowadays, there are more and more software systems operating in highly open, dynamic and unpredictable environments. Moreover, as technology advances, requirements for these systems become ever more ambitious. We have reached a point where system complexity and environmental uncertainty are major challenges for the Information Technology in-dustry. A solution proposed to deal with this challenge is to make systems (self-) adaptive, meaning they would evaluate their own behavior and performance, in order to re-plan and reconfigure their operations when needed. In order to develop an adaptive system, one needs to account for some kind of feedback loop. A feedback loop constitutes an architectural prosthetic to a system proper, introducing monitoring and adaptation functionalities to the overall system. Even if implicit or hidden in the system\u2019s architecture, adaptive systems must have a feedback loop among their components in order to evaluate their behavior and act accordingly. In this thesis, we take a Requirements Engineering perspective to the design of adaptive software systems and, given that feedback loops constitute an (architectural) solution for adaptation, we ask the", "num_citations": "52\n", "authors": ["1443"]}
{"title": "An extensible tool for source code representation using XML\n", "abstract": " One of the problems facing software re-engineering projects is the fact that program source code is invariably stored in ASCII plain text format. This format doesn't reflect the underlying structure of the program. Consequently, software re-engineering or code migration tools need to unearth this structure. This paper explores the possibility of adopting XML format to represent program structure for software systems, and describes a tool, the XMLizer, which has been implemented to support the transformation of software programs from ASCII plain text format to XML. In addition, the XMLizer allows variable-depth marking up of program structure by using a multi-weight parsing technique. The XMLizer currently supports three languages, Java, PL/IX and Pascal, and can be extended to support others. The performance of the XMLizer in converting PL/IX programs into XML was tested. XMLized output is accessible through\u00a0\u2026", "num_citations": "52\n", "authors": ["1443"]}
{"title": "TORUS: a natural language understanding system for data management\n", "abstract": " This paper describes TORUS, a natural language understanding system which serves as a front end to a data base management system in order to facilitate communication with a casual user. The system uses a semantic network for\" understanding\" each input statement and for deciding what information to output in response. The semantic network stores general knowledge about the problem domain, in this case\" student files\" and the educational process at the University of Toronto, along with specific information obtained during the dialogue with the user. A number of associated functions make it possible to integrate the meaning of an input statement to the semantic network, and to select a portion of the semantic network which stores information that must be output. A first version of TORUS has been implemented and is currently being tested.", "num_citations": "51\n", "authors": ["1443"]}
{"title": "Some results in computational topology\n", "abstract": " It is the object of this paper to study the topological properties of finite graphs that can be embedded in the n-dimensional integral lattice (denoted Nn). The basic notion of deletability of a node is first introduced. A node is deletable with respect to a graph if certain computable conditions are satisfied on its neighborhood. An equivalence relation on graphs called reducibility and denoted by \u201c\u223c\u201d is then defined in terms of deletability, and it is shown that (a) most important topological properties of the graph (homotogy, homology, and cohomology groups) are \u223c-invariants; (b) for graphs embedded in N3, different knot types belong to different \u223c-equivalence classes; (c) it is decidable whether two graphs are reducible to each other in N2 but this problem is undecidable in Nn for n \u2265 4. Finally, it is shown that two different methods of approximating an n-dimensional closed manifold with boundary by a graph of the type\u00a0\u2026", "num_citations": "50\n", "authors": ["1443"]}
{"title": "Conceptualizing and specifying key performance indicators in business strategy models\n", "abstract": " Key Performance Indicators (KPI) measure the performance of an organization relative to its objectives. To monitor organizational performance relative to KPIs, such KPIs need to be manually implemented in the form of data warehouse queries, to be used in dashboards or scorecards. Moreover, dashboards include little if any information about business strategy and offer a scattered view of KPIs and what do they mean relative to business concerns. In this paper, we propose an integrated view of strategic business models and conceptual data warehouse models. The main benefit of our proposal is that it links strategic business models to the data through which objectives can be monitored and assessed. In our proposal, KPIs are defined in Structured English and are implemented in a semi-automatic way, allowing for quick modifications. This enables real-time monitoring and what-if analysis, thereby\u00a0\u2026", "num_citations": "49\n", "authors": ["1443"]}
{"title": "Visualizing non-functional requirements\n", "abstract": " Information systems can be visualized with many tools. Typically these tools present functional artifacts from various phases of the development life-cycle; these include requirements models, architecture and design diagrams, and implementation code. The syntactic structures of these artifacts are often presented in a textual language using symbols, or a graphical one using nodes and edges. In this paper, we propose a quality-based visualization scheme. Such a scheme is layered on top of these functional artifacts for presenting non-functional aspects of the system. To do this, we use quantified quality attributes. As an example, we visualize the quality attributes of trust and performance among various nonfunctional requirements of information systems.", "num_citations": "49\n", "authors": ["1443"]}
{"title": "Extracting rights and obligations from regulations: toward a tool-supported process\n", "abstract": " Security, privacy and governance are increasingly the focus of government regulations in the US, Europe and elsewhere. This trendhas created a\" regulation compliance problem\", whereby companiesand developers are required to ensure that their software complies with relevant regulations, either through design or reengineering. We previously proposed a methodology for extracting stakeholder requirements, called rights and obligations, from regulations. In this paper, we examine the challenges of developing tool support for this process. We apply the Cerno framework for textual semantic annotation to propose a tool for semi-automatic semantic annotation of concepts that constitute sources of requirements", "num_citations": "48\n", "authors": ["1443"]}
{"title": "Building knowledge base management systems\n", "abstract": " Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using  existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose\u00a0\u2026", "num_citations": "48\n", "authors": ["1443"]}
{"title": "On the perception of software quality requirements during the project lifecycle\n", "abstract": " [Context and motivation] A key requirements consideration in software development is the system\u2019s quality requirements. Quality is usually defined in terms of global properties for a software system, such as \u201creliability\u201d, \u201cusability\u201d and \u201cmaintainability\u201d. In the context of software maintenance they are particularly relevant: maintenance activities are performed to ensure software quality. [Question/problem] Recently an expanded view of RE has been emerging, wherein requirements artifacts play a role throughout a system\u2019s lifecycle. How important are quality requirements as the lifecycle progresses? We examine two questions: whether requirements are discussed more as the software matures; secondly, whether different software projects have similar levels of interest about quality requirements. [Principal ideas/results] We use a software repository mining technique we call signifier extraction, and\u00a0\u2026", "num_citations": "47\n", "authors": ["1443"]}
{"title": "An ontological analysis of value propositions\n", "abstract": " In competitive markets, companies need well-designed business strategies if they seek to grow and obtain sustainable competitive advantage. At the core of a successful business strategy there is a carefully crafted value proposition, which ultimately defines what a company delivers to its customers. Despite their widely recognized importance, there is however little agreement on what exactly value propositions are. This lack of conceptual clarity harms the communication among stakeholders and the harmonization of current business strategy theories and strategy support frameworks. Furthermore, it hinders the development of systematic methodologies for crafting value propositions, as well as adequate support for representing and analyzing them. In this paper, we present an ontological analysis of value propositions based on a review of most relevant business and marketing theories and on previous work on\u00a0\u2026", "num_citations": "46\n", "authors": ["1443"]}
{"title": "Specification and derivation of key performance indicators for business analytics: A semantic approach\n", "abstract": " Key Performance Indicators (KPI) measure the performance of an enterprise relative to its objectives thereby enabling corrective action where there are deviations. In current practice, KPIs are manually integrated within dashboards and scorecards used by decision makers. This practice entails various shortcomings. First, KPIs are not related to their business objectives and strategy. Consequently, decision makers often obtain a scattered view of the business status and business concerns. Second, while KPIs are defined by decision makers, their implementation is performed by IT specialists. This often results in discrepancies that are difficult to identify. In this paper, we propose an approach that provides decision makers with an integrated view of strategic business objectives and conceptual data warehouse KPIs. The main benefit of our proposal is that it links strategic business models to the data for monitoring and\u00a0\u2026", "num_citations": "45\n", "authors": ["1443"]}
{"title": "From stakeholder requirements to formal specifications through refinement\n", "abstract": " [Context and motivation] Stakeholder requirements are notoriously informal, vague, ambiguous and often unattainable. The requirements engineering problem is to formalize these requirements and then transform them through a systematic process into a formal specification that can be handed over to designers for downstream development. [Question/problem] This paper proposes a framework for transforming informal requirements to formal ones, and then to a specification. [Principal ideas/results] The framework consists of an ontology of requirements, a formal requirements modeling language for representing both functional and non-functional requirements, as well as a rich set of refinement operators whereby requirements are incrementally transformed into a formal, practically satisfiable and measurable specification. [Contributions] Our proposal includes a systematic, tool-supported\u00a0\u2026", "num_citations": "45\n", "authors": ["1443"]}
{"title": "Knowledge representation: features of knowledge\n", "abstract": " It is by now a clich\u00e9 to claim that knowledge representation is a fundamental research issue in Artificial Intelligence (AI) underlying much of the research, and the progress, of the last fifteen years. And yet. it is difficult to pinpoint exactly what knowledge representation is, does, or promises to do. A thorough survey of the field by Ron Brachman and Brian Smith [Brachman & Smith 80] points out quite clearly the tremendous range in viewpoints and methodologies of researchers in knowledge representation. This paper is a further attempt to look at the field in order to examine the state of the art and provide some insights into the nature of the research methods and results. The distinctive mark of this overview is its viewpoint: that propositions encoded in knowledge bases have a number of important features, and these features serve, or ought to serve, as a basis for guiding current interest and activity in AI\u00a0\u2026", "num_citations": "45\n", "authors": ["1443"]}
{"title": "Agile requirements engineering via paraconsistent reasoning\n", "abstract": " Innovative companies need an agile approach towards product and service requirements, to rapidly respond to and exploit changing conditions. The agile approach to requirements must nonetheless be systematic, especially with respect to accommodating legal and non-functional requirements. This paper examines how to support lightweight, agile requirements processes which can still be systematically modeled, analyzed and changed. We propose a framework, RE-KOMBINE, which is based on a propositional language for requirements modeling called Techne. We define operations on Techne models which tolerate the presence of inconsistencies. This paraconsistent reasoning is vital for supporting delayed commitment to particular design solutions. We evaluate these operations with an industry case study using two well-known formal analysis tools. Our evaluations show that the proposed framework\u00a0\u2026", "num_citations": "44\n", "authors": ["1443"]}
{"title": "An Ontological Interpretation of Non-Functional Requirements.\n", "abstract": " Non-functional requirements (NFRs) have been the focus of research in Requirements Engineering (RE) for more than 20 years. Despite this attention, their ontological nature is still an open question, thereby hampering efforts to develop concepts, tools and techniques for eliciting, modeling, and analyzing them, in order to produce a specification for a system-to-be. In this paper, we propose to treat NFRs as qualities, based on definitions of the UFO foundational ontology. Furthermore, based on these ontological definitions, we provide guidelines for distinguishing between non-functional and functional requirements, and sketch a syntax of a specification language that can be used for capturing NFRs.", "num_citations": "44\n", "authors": ["1443"]}
{"title": "Requirements Engineering: an educational dilemma\n", "abstract": " The emergence of Requirements Engineering (hereafter, RE) as an identifiable, coherent research area---evidenced by a bi-annual symposium (RE93, RE95), an IEEE International Conference initiated in 1994, a recently founded IFIP working group, an international journal and numerous international workshops--suggests that we should be re-examining the role and place of requirements engineering in computer science and engineering university programmes. After all, university education remains a primary vehicle for the transfer of knowledge from the research community to industry and government. Requirements are variously described by practitioners as' intangible','moving targets','inherently inconsistent','ever-changing'and a host of other adjectives which fill the average university lecturer with horror. In a similar vein, Requirements Engineering is described by experienced researchers as a'wicked problem'\u00a0\u2026", "num_citations": "44\n", "authors": ["1443"]}
{"title": "Agile requirements evolution via paraconsistent reasoning\n", "abstract": " Innovative companies need an agile approach for the engineering of their product requirements, to rapidly respond to and exploit changing conditions. The agile approach to requirements must nonetheless be systematic, especially with respect to accommodating legal and nonfunctional requirements. This paper examines how to support a combination of lightweight, agile requirements which can still be systematically modeled, analyzed and changed. We propose a framework, RE-KOMBINE, which is based on a propositional language for requirements modeling called Techne. We define operations on Techne models which tolerate the presence of inconsistencies in the requirements. This paraconsistent reasoning is vital for supporting delayed commitment to particular design solutions. We evaluate these operations with an industry case study using two well-known formal analysis tools. Our evaluations\u00a0\u2026", "num_citations": "43\n", "authors": ["1443"]}
{"title": "What nurses want: diffusion of an innovation\n", "abstract": " We investigated the usability of personal digital assistants (PDAs) to improve research utilization and timely access to electronic practice information to assist in clinical decisions. Nurses used a decision support tool on a PDA to collect point-of-care outcomes data. Follow-up interviews documented usability. Nurses liked the portability and size of the PDA, as well as ease of use of the PDA software. Electronic decision support tools at point of care have the potential to improve nurses' research utilization and quality of care.", "num_citations": "43\n", "authors": ["1443"]}
{"title": "Non-functional requirements as qualities, with a spice of ontology\n", "abstract": " We propose a modeling language for non-functional requirements (NFRs) that views NFRs as requirements over qualities, mapping a software-related domain to a quality space. The language is compositional in that it allows (recursively) complex NFRs to be constructed in several ways. Importantly, the language allows the definition of requirements about the quality of fulfillment of other requirements, thus capturing, among others, the essence of probabilistic and fuzzy goals as proposed in the literature. We also offer a methodology for systematically refining informal NFRs elicited from stakeholders, resulting in unambiguous, de-idealized, and measurable requirements. The proposal is evaluated with a requirements dataset that includes 370 NFRs crossing 15 projects. The results suggest that our framework can adequately handle and clarify NFRs generated in practice.", "num_citations": "42\n", "authors": ["1443"]}
{"title": "The local relational model: Model and proof theory\n", "abstract": " In this paper we identify desirable data management mechanisms for peer-to-peer (P2P) computing. P2P networks have to remain open and dynamic, while peers remain autonomous and need only be aware of their immediate acquaintances. In such a setting, we argue that one cannot assume the existence of a global schema for all the peer databases. Instead, one needs a data model which views the space of data being managed within the P2P network as an open collection of possibly overlapping and inconsistent databases. Accordingly, the paper proposes the Local Relational Model and others a formal semantics for coordination between peer databases. Our result generalizes Reiter's characterization of a relational database in terms of a first order theory, by providing a syntactic characterization of a relational space in terms of a multicontext system.", "num_citations": "42\n", "authors": ["1443"]}
{"title": "TORUS: a step towards bridging the gap between data bases and the casual user\n", "abstract": " This paper describes TORUS, a natural language understanding system that serves as a front end to a data base management system in order to facilitate communication with casual users. The system employs a semantic network to store knowledge about a data base of student files. This knowledge is used to find the meaning of each input statement, to decide what action to take with respect to the data base, and to select information that must be output in response to the input statement. A prototype version of TORUS has been implemented.", "num_citations": "42\n", "authors": ["1443"]}
{"title": "Model predictive control for software systems with CobRA\n", "abstract": " Self-adaptive software systems monitor their operation and adapt when their requirements fail due to unexpected phenomena in their environment. This paper examines the case where the environment changes dynamically over time and the chosen adaptation has to take into account such changes. In control theory, this type of adaptation is known as Model Predictive Control and comes with a well-developed theory and myriads of successful applications. The paper focuses on modelling the dynamic relationship between requirements and possible adaptations. It then proposes a controller that exploits this relationship to optimize the satisfaction of requirements relative to a cost-function. This is accomplished through a model-based framework for designing self- adaptive software systems that can guarantee a certain level of requirements satisfaction over time, by dynamically composing adaptation strategies when\u00a0\u2026", "num_citations": "41\n", "authors": ["1443"]}
{"title": "Awareness requirements\n", "abstract": " The functional specification of any software system operationalizes stakeholder requirements. In this paper we focus on a class of requirements that lead to feedback loop operationalizations. These Awareness Requirements talk about the runtime success/failure of other requirements and domain assumptions. Our proposal includes a language for expressing awareness requirements, as well as techniques for elicitation and implementation based on the EEAT requirements monitoring framework.", "num_citations": "41\n", "authors": ["1443"]}
{"title": "Design Requirements Engineering: A Ten-Year Perspective: Design Requirements Workshop, Cleveland, OH, USA, June 3-6, 2007, Revised and Invited Papers\n", "abstract": " Since its inception in 1968, software engineering has undergone numerous changes. In the early years, software development was organized using the waterfall model, where the focus of requirements engineering was on a frozen requirements document, which formed the basis of the subsequent design and implementation process. Since then, a lot has changed: software has to be developed faster, in larger and distributed teams, for pervasive as well as large-scale applications, with more flexibility, and with ongoing maintenance and quick release cycles. What do these ongoing developments and changes imply for the future of requirements engineering and software design? Now is the time to rethink the role of requirements and design for software intensive systems in transportation, life sciences, banking, e-government and other areas. Past assumptions need to be questioned, research and education need to be rethought. This book is based on the Design Requirements Workshop, held June 3-6, 2007, in Cleveland, OH, USA, where leading researchers met to assess the current state of affairs and define new directions. The papers included were carefully reviewed and selected to give an overview of the current state of the art as well as an outlook on probable future challenges and priorities. After a general introduction to the workshop and the related NSF-funded project, the contributions are organized in topical sections on fundamental concepts of design; evolution and the fluidity of design; quality and value-based requirements; requirements intertwining; and adapting requirements practices in different domains.", "num_citations": "41\n", "authors": ["1443"]}
{"title": "On the topological properties of quantized spaces, II. connectivity and order of connectivity\n", "abstract": " A notion of equivalence (c-equivalence) is defined as the counterpart of homeoinorphism for quantized spaces. It is shown that sets with the same number of components and holes are c-equivalent in two-dimensional spaces. Then it is shown that for each arbitrary set there is a set c-equivalent to it with certain\" regular\" features (rectangular perimeter, holes with diameter one, etc.).", "num_citations": "41\n", "authors": ["1443"]}
{"title": "The requirements problem for adaptive systems\n", "abstract": " Requirements Engineering (RE) focuses on eliciting, modeling, and analyzing the requirements and environment of a system-to-be in order to design its specification. The design of the specification, known as the Requirements Problem (RP), is a complex problem-solving task because it involves, for each new system, the discovery and exploration of, and decision making in a new problem space. A system is adaptive if it can detect deviations between its runtime behavior and its requirements, specifically situations where its behavior violates one or more of its requirements. Given such a deviation, an Adaptive System uses feedback mechanisms to analyze these changes and decide, with or without human intervention, how to adjust its behavior as a result. We are interested in defining the Requirements Problem for Adaptive Systems (RPAS). In our case, we are looking for a configurable specification such that\u00a0\u2026", "num_citations": "40\n", "authors": ["1443"]}
{"title": "A multi-level relational system\n", "abstract": " A relation can be conceptually viewed as a table of data. The table's heading defines the relation's name, the column headings are the attribute names, and each row corresponds to an n-tuple of data values describing a single entity. The set of values which can be used in a column is called a domain. A relational data base is composed of a set of time varying relations inter-related through common domains.", "num_citations": "39\n", "authors": ["1443"]}
{"title": "Semi-Automatic Semantic Annotations for Web Documents.\n", "abstract": " Semantic annotation of the web documents is the only way to make the Semantic Web vision a reality. Considering the scale and dynamics of worldwide web, the largest knowledge base ever built, it becomes clear that we cannot afford to annotate web documents manually. In this work we propose a generic domain-independent architecture for semi-automatic semantic annotation, basing on the lightweight and robust techniques, proven effective in source code processing for software analysis field. We demonstrate feasibility of our method applying it for annotation of the documents for Tourism domain. The results of this experiment are validated using a three-stage evaluation scheme.", "num_citations": "38\n", "authors": ["1443"]}
{"title": "Concurrency Control for Knowledge Bases.\n", "abstract": " As the demand for ever-larger knowledge bases grows, knowledge base management techniques assume paramount importance. In this paper we show that large, multi-user knowledge bases need concurrency control. We discuss known techniques from database concurrency control and explain their inadequacies in the context of knowledge bases. We o er a concurrency control algorithm, called the Dynamic Directed Graph (DDG) policy that addresses the speci c needs of knowledge bases. The DDG policy exploits the rich structure of a knowledge base to support the interleaved, concurrent execution of several user requests, thereby improving overall system performance. We give a proof of correctness of the proposed concurrency control algorithm and an analysis of its properties. We demonstrate that these results from concurrency control interact in interesting ways with knowledge base features and highlight the importance of performanceoriented tradeo s in the design of knowledgebased systems.", "num_citations": "38\n", "authors": ["1443"]}
{"title": "Support for Data-Intensive Applications: Conceptual Design and Software Development.\n", "abstract": " In the process of developing an Information System, one passes through stages that include requirements gathering, design specification, and software implementation. The purpose of the TDL language is to express the conceptual design of an information system; it is the intermediate language in a triad that includes the language Telos, which captures an evolutionary view of the application domain and requirements, and DBPL, a procedural programming language that has persistent values and transactions supporting the development of databases. We consider TDL\u2019s features for specifying the data and eventual procedural components of the system, and discuss how these are related to its companions. We also survey several tools for manipulating TDL descriptions which are currently under development, and give a detailed example of the iterative refinement of TDL designs into DBPL programs.", "num_citations": "38\n", "authors": ["1443"]}
{"title": "Learning to rank for question-oriented software text retrieval (t)\n", "abstract": " Question-oriented text retrieval, aka natural language-based text retrieval, has been widely used in software engineering. Earlier work has concluded that questions with the same keywords but different interrogatives (such as how, what) should result in different answers. But what is the difference? How to identify the right answers to a question? In this paper, we propose to investigate the \"answer style\" of software questions with different interrogatives. Towards this end, we build classifiers in a software text repository and propose a re-ranking approach to refine search results. The classifiers are trained by over 16,000 answers from the StackOverflow forum. Each answer is labeled accurately by its question's explicit or implicit interrogatives. We have evaluated the performance of our classifiers and the refinement of our re-ranking approach in software text retrieval. Our approach results in 13.1% and 12.6\u00a0\u2026", "num_citations": "37\n", "authors": ["1443"]}
{"title": "Business intelligence modeling in action: a hospital case study\n", "abstract": " Business Intelligence (BI) projects are long and painful endeavors that employ a variety of design methodologies, inspired mostly by software engineering and project management lifecycle models. In recent BI research, new design methodologies are emerging founded on conceptual business models that capture business objectives, strategies, and more. Their claim is that they facilitate the description of the problem-at-hand, its analysis towards a solution, and the implementation of that solution. The key question explored in this work is:Are such models actually useful to BI design practitioners? To answer this question, we conducted an in situ empirical evaluation based on an on-going BI project for a Toronto hospital. The lessons learned from the study include: confirmation that the BI implementation is well-supported by models founded on business concepts; evidence that these models enhance\u00a0\u2026", "num_citations": "36\n", "authors": ["1443"]}
{"title": "Establishing regulatory compliance for software requirements\n", "abstract": " A software system complies with a regulation if its operation is consistent with the regulation under all circumstances. The importance of regulatory compliance for software systems has been growing, as regulations are increasingly impacting both the functional and non-functional requirements of legacy and new systems. HIPAA and SOX are recent examples of laws with broad impact on software systems, as attested by the billions of dollars spent in the US alone on compliance. In this paper we propose a framework for establishing regulatory compliance for a given set of software requirements. The framework assumes as inputs models of the requirements (expressed in i*) and the regulations (expressed in N\u00f2mos). In addition, we adopt and integrate with i* and N\u00f2mos a modeling technique for capturing arguments and establishing their acceptability. Given these, the framework proposes a systematic\u00a0\u2026", "num_citations": "35\n", "authors": ["1443"]}
{"title": "Requirements evolution and what (research) to do about it\n", "abstract": " Requirements evolution is a research problem that has received little attention hitherto, but deserves much more. For systems to survive in a volatile world, where business needs, government regulations and computing platforms keep changing, software systems must evolve too in order to survive. We discuss the state-of-the-art for research on the topic, and predict some of the research problems that will need to be addressed in the next decade. We conclude with a concrete proposal for a run-time monitoring framework based on (requirements) goal models.", "num_citations": "35\n", "authors": ["1443"]}
{"title": "Incremental iterative retrieval and browsing for efficient conversational CBR systems\n", "abstract": " A case base is a repository of past experiences that can be used for problem solving. Given a new problem, expressed in the form of a query, the case base is browsed in search of \u201csimilar\u201d or \u201crelevant\u201d cases. Conversational case-based reasoning (CBR) systems generally support user interaction during case retrieval and adaptation. Here we focus on case retrieval where users initiate problem solving by entering a partial problem description. During an interactive CBR session, a user may submit additional queries to provide a \u201cfocus of attention\u201d. These queries may be obtained by relaxing or restricting the constraints specified for a prior query. Thus, case retrieval involves the iterative evaluation of a series of queries against the case base, where each query in the series is obtained by restricting or relaxing the preceding query.               This paper considers alternative approaches for implementing iterative\u00a0\u2026", "num_citations": "35\n", "authors": ["1443"]}
{"title": "Analysis of multi-party agreement in requirements validation\n", "abstract": " A requirements engineering artifact is valid relative to the stakeholders of the system-to-be if they agree on the content of that artifact. Checking relative validity involves a discussion between the stakeholders and the requirements engineer. This paper proposes (I) a language for the representation of information exchanged in a discussion about the relative validity of an artifact; (ii) the acceptability condition, which, when it verifies in a discussion captured in the proposed language, signals that the relative validity holds for the discussed artifact and for the participants in the discussion; and (iii) reasoning procedures to automatically check the acceptability condition in a discussions captured by the proposed language.", "num_citations": "34\n", "authors": ["1443"]}
{"title": "On the philosophical foundations of conceptual models\n", "abstract": " This paper contributes to the philosophical foundations of conceptual modeling by addressing a number of foundational questions such as: What is a conceptual model? Among models used in computer science, which are conceptual, and which are not? How are conceptual models different from other models used in the Sciences and Engineering? The paper takes a stance in answering these questions and, in order to do that, it draws from a broad literature in philosophy, cognitive science, Logics, as well as several areas of Computer Science (including Databases, Software Engineering, Artificial Intelligence, Information Systems Engineering, among others). After a brief history of conceptual modeling, the paper addresses the aforementioned questions by proposing a characterization of conceptual models with respect to conceptual semantics and ontological commitments. Finally, we position our work wrt to a \u201cReference Framework for Conceptual\u201d modeling recently proposed in the literature.", "num_citations": "33\n", "authors": ["1443"]}
{"title": "Goal-oriented requirements engineering\n", "abstract": " The last fifteen years have seen the rise of a new phase in software development which is concerned with the acquisition, modelling and analysis of stakeholder purposes (? goals?) in order to derive functional and non-functional requirements. We review the history of ideas and research results for this new phase and sketch on-going research on the topic. Specifically, we discuss an agent-oriented software development methodology called Tropos that is founded on the concepts of goal, actor as well as inter-actor dependencies. We also show how goal models that characterize a space of possible solutions for meeting stakeholder goals can be used as a basis for designing high variability software.", "num_citations": "33\n", "authors": ["1443"]}
{"title": "From awareness requirements to adaptive systems: A control-theoretic approach\n", "abstract": " Several proposals for the design of adaptive systems rely on some kind of feedback loop that monitors the system output and adapts in case of failure. Roadmap papers in the area advocate the need to make such feedback loops first class entities in adaptive systems design. We go further by adopting a Requirements Engineering perspective that is not only based on feedback loops but also applies other concepts from Control Theory to the design of adaptive systems. Our plans include a framework that reasons over requirements at runtime to provide adaptivity to a system proper. In this position paper, we argue for a control-theoretic view for adaptive systems and outline our long-term research agenda, briefly presenting work that we have already accomplished and discussing our plans for the future.", "num_citations": "32\n", "authors": ["1443"]}
{"title": "Refining semantic mappings from relational tables to ontologies\n", "abstract": " To support the Semantic Web, it will be necessary to construct mappings between legacy database schemas and ontologies. We have developed a prototype tool which starts from a simple set of correspondences from table columns to ontology components, and then helps derive algorithmically candidate logical mappings between complete tables and the ontology. We report here some refinements of this algorithm inspired by an analysis of the ways in which relational schemas are standardly derived from Extended Entity Relationship diagrams, and relate this to the main heuristic used by the Clio system [6], which maps between relational database schemas.", "num_citations": "32\n", "authors": ["1443"]}
{"title": "Reasoning with optional and preferred requirements\n", "abstract": " Of particular concern in requirements engineering is the selection of requirements to implement in the next release of a system. To that end, there has been recent work on multi-objective optimization and user-driven prioritization to support the analysis of requirements trade-offs. Such work has focused on simple, linear models of requirements; in this paper, we work with large models of interacting requirements. We present techniques for selecting sets of solutions to a requirements problem consisting of mandatory and optional\u00a0goals, with preferences among them. To find solutions, we use a modified version of the framework from Sebastiani et al.[1] to label our requirements goal models. For our framework to apply to a problem, no numeric valuations are necessary, as the language is qualitative. We conclude by introducing a local search technique for navigating the exponential solution space. The\u00a0\u2026", "num_citations": "31\n", "authors": ["1443"]}
{"title": "Modeling and analyzing context-aware composition of services\n", "abstract": " Service-oriented modeling and analysis is a promising approach to manage context-aware cooperation among organizations belonging to the same value chain. Following this approach, a value chain is modeled as a composition of services provided by different partners and coordinated in a way that their interactions can be reorganized according to changes in the environment. However, so far, most of the research work in this area has been focused on the design of architectures handling service discovery, compatibility and orchestration. Little attention has been given to the specification and verification of context-aware composition of services during the requirement engineering process. The goal of this paper is to fill this gap through a methodological approach based on the strict coupling between a social and a process model. The methodology is discussed through a simple example.", "num_citations": "31\n", "authors": ["1443"]}
{"title": "Improving the build architecture of legacy C/C++ software systems\n", "abstract": " The build architecture of legacy C/C++ software systems, groups program files in directories to represent logical components. The interfaces of these components are loosely defined by a set of header files that are typically grouped in one common include directory. As legacy systems evolve, these interfaces decay, which contribute to an increase in the build time and the number of conflict in parallel developments. This paper presents an empirical study of the build architecture of large commercial software systems, introduces a restructuring approach, based on Reflexion models and automatic clustering, and reports on a case study using VIM open source editor.", "num_citations": "31\n", "authors": ["1443"]}
{"title": "(Requirement) evolution requirements for adaptive systems\n", "abstract": " It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature.", "num_citations": "30\n", "authors": ["1443"]}
{"title": "Reducing build time through precompilations for evolving large software\n", "abstract": " Large-scale legacy programs take long time to compile, thereby hampering productivity. This paper presents algorithms that reduce compilation time by analyzing syntactic dependencies in fine-grain program units, and by removing redundancies as well as false dependencies. These algorithms are combined with parallel compilation techniques (compiler farms, compiler caches), to further reduce build time. We demonstrate through experiments their effectiveness in achieving significant speedup for both fresh and incremental builds.", "num_citations": "30\n", "authors": ["1443"]}
{"title": "Traceability for the maintenance of secure software\n", "abstract": " Traceability links among different software engineering artifacts make explicit how a software system was implemented to accommodate its requirements. For secure and dependable software system development, one must ensure the linked entities are truly traceable to each other and the links are updated to reflect true traceability among changed entities. However, traditional traceability relationships link recovery techniques are not accurate enough. To address this problem, we propose a traceability technique based on refactoring, which is then continuously integrated with other software maintenance activities. Applying our traceability technique to the proven SSL protocol design, we found a significant vulnerability bug in its open-source implementation. The results also demonstrate the level of accuracy and change resilience of our technique that enable reuse of the traceability-related analysis on different\u00a0\u2026", "num_citations": "29\n", "authors": ["1443"]}
{"title": "Data semantics revisited\n", "abstract": " The problem of data semantics is establishing and maintaining the correspondence between a data source and its intended subject matter. We review the long history of the problem in Databases, and contrast it with recent research on the Semantic Web. We then propose two new directions for research on the problem and sketch some open research questions.", "num_citations": "29\n", "authors": ["1443"]}
{"title": "Cooperation strategies for agent-based p2p systems\n", "abstract": " We are interested in peer-to-peer (P2P) computing, where a P2P application consists of a (wireless) network of nodes (peers), and assumes full peer autonomy, no global control, and intermittent connectivity. P2P computing has many advantages over classical client-server and web-based distributed architectures. However, the P2P computing model also has a number of limitations in the mechanisms it supports for data management and interchange. To overcome some of these, we propose an agent-based P2P model whose nodes are software agents (peer agents). This paper uses the i* modeling framework to analyze and evaluate peer agent cooperation strategies using three possible evaluation criteria.", "num_citations": "29\n", "authors": ["1443"]}
{"title": "Semantics, features, and applications of the viewpoint abstraction\n", "abstract": " Partitioning information bases such that their contents may be viewed from different situations and represented and processed in different contexts, constitutes a fundamental concern in various disciplines of computer science. Not surprisingly, numerous notations and techniques support certain aspects of the viewpoint abstraction. This paper motivates the use of a well defined terminology and framework regarding basic notions accompanying the viewpoint abstraction, such as contexts, perspectives, situations, and relativism. Furthermore, it establishes the cognitive and linguistic evidence on the usefulness of considering multiple views. A previous paper introduced a generic framework for contexts in order to provide a common kernel for the modelling of information base partitions. This paper demonstrates the embedding of the framework into an extensible, structurally object-oriented data/knowledge model\u00a0\u2026", "num_citations": "29\n", "authors": ["1443"]}
{"title": "Semantic Networks and the Generation of Context.\n", "abstract": " This paper outlines a representation of knowledge based on semantic networks and organized in terms of semantic axes and\" scenarios\". A context mechanism for such nets is also discussed, along with an algorithm for integrating new information to a semantic net. Finally, an example is given to illustrate the representation and the context mechanism.", "num_citations": "29\n", "authors": ["1443"]}
{"title": "Towards an ontology of software: a requirements engineering perspective\n", "abstract": " Although software plays an essential role in modern society, its ontological nature is still unclear. For many, software is just code, but this is not illuminating. Several researchers have attempted to understand the core nature of software and programs in terms of concepts such as code, copy, medium and execution. More recently, a proposal was made to consider software as an abstract artifact, distinct from code, just because code may change while the software remains the same. We explore in this paper the implications of such a proposal in the light of software engineering and requirements engineering literature. We make a sharp distinction between different kinds of software artifacts (code, program, software system, and software product), and describe the ways they are inter-connected in the context of a software engineering process.", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Requirements-driven qualitative adaptation\n", "abstract": " Coping with run-time uncertainty pose an ever-present threat to the fulfillment of requirements for most software systems (embedded, robotic, socio-technical, etc.). This is particularly true for large-scale, cooperative information systems. Adaptation mechanisms constitute a general solution to this problem, consisting of a feedback loop that monitors the environment and compensates for deviating system behavior. In our research, we apply a requirements engineering perspective to the problem of designing adaptive systems, focusing on developing a qualitative software-centric, feedback loop mechanism as the architecture that operationalizes adaptivity. In this paper, we propose a framework that provides qualitative adaptation to target systems based on information from their requirements models. The key characteristc of this framework is extensibility, allowing for it to cope with qualitative information about\u00a0\u2026", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Goal-Oriented Requirements Engineering, Part II.\n", "abstract": " \u2192 We briefly review the history and key ideas in Goal-Oriented Requirements Engineering research. We then sketch two applications of these ideas. The first involves establishing an Agent-Oriented Software Development method called Tropos which covers not only requirements but also design phases. The second addresses the design of high-variability software for applications such as home care software and business process design.", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Tomas: A system for adapting mappings while schemas evolve\n", "abstract": " We demonstrate the Toronto Mapping Adaptation System (ToMAS), a tool for automatically detecting and adapting mappings that have become invalid or inconsistent due to changes in either data semantics or schemas. Due to its modular architecture and its stand-alone nature, ToMAS can easily be applied to numerous scenarios and can interoperate with many other tools. To the best of our knowledge, no other tool can correctly maintain the consistency of the mappings under schema changes at the level of complexity supported by ToMAS.", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Supporting distributed autonomous information services using coordination\n", "abstract": " The large quantity and often questionable quality of available information in the information age provides a shaky foundation for decision making by individuals and organizations alike. This has created a tremendous demand for information services which can access, filter, process and present information on an as-needed basis. However, two factors complicate the design of such information services, namely the distributed and the autonomous nature of data sources.         This paper reports on the design and implementation of a generic architecture for supporting information services, which meets the above challenge. The architecture adopts concepts from conceptual modeling to offer a transparent description of the information sources' setting and uses active databases techniques to offer a declarative, event-based language for defining coordination rules for integrating distributed information services\u00a0\u2026", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Process management and assertion enforcement for a semantic data model\n", "abstract": " The Taxis design language offers an entity-based framework for designing interactive information systems and a data model which supports generalisation, classification and aggregation as abstraction mechanisms. With the aim of balancing expressiveness and performance, this paper describes and discusses design, implementation and performance analysis of the closely related issues of management of long-term activities for Taxis and enforcement of semantic integrity constraints.", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Knowledge bases vs databases\n", "abstract": " This position statement argues that an important difference between knowledge bases and databases is that the former require a semantic theory for the interpretation of their contents, while the latter require a computational theory for their efficient implementation on physical machines.3", "num_citations": "28\n", "authors": ["1443"]}
{"title": "Goal-oriented conceptual database design\n", "abstract": " We present details of a goal-oriented process for database requirements analysis. This process consists of a number of steps, spanning the spectrum from high-level stakeholder goal analysis to detailed conceptual schema design. The paper shows how goal modeling contributes to systematic scoping and analysis of the application domain, and subsequent formal specification of database requirements based on this domain analysis. Moreover, a goal-oriented design strategy is proposed to structure the transformation from the domain model to the conceptual schema, according to a set of user defined design issues, also modeled as goals. The proposed process is illustrated step-by-step using a running example from the design of a real-world, industrial biological database. We also report early progress towards building full tool support, by presenting a prototype that captures and stores design sessions in a\u00a0\u2026", "num_citations": "27\n", "authors": ["1443"]}
{"title": "Design of a compiler for a semantic data model\n", "abstract": " The features of a compiler for a Taxis design language are described and discussed. Taxis offers an entity-based framework for designing interactive information systems and a data model which supports generalization, classification and aggregation as abstraction mechanisms. Its features include multiple inheritance of attributes, isA hierarchies of transactions, metaclasses, typed attributes, a procedural exception-handling mechanism, an iteration construct based on the abstraction mechanisms supported, semantic integrity constraints, including time-dependent ones, and communicating Petri net-like processes (called scripts). Designing and implementing a compiler for the language requires tackling the problem of efficiently representing and accessing a large collection of entities, performing (static) type checking and representing is A hierarchies of transactions. The compiler has also been designed to\u00a0\u2026", "num_citations": "27\n", "authors": ["1443"]}
{"title": "A systematic approach for dynamic targeted monitoring of KPIs.\n", "abstract": " There has been growing interest for more than a decade in Business Analytics as a means for improving business performance. One of the most popular Business Analytics technique involves monitoring performance by means of Key Performance Indicators (KPIs). A KPI is a powerful tool that relates enterprise data to business goals, thereby enabling managers to guide the analytic process and identify deviations in their strategic plan. Nevertheless, monitoring KPIs requires that they are evaluated at multiple levels of detail, in order to identify potential problems earlier instead of being noted after the fact. Unfortunately, there are obstacles to the generation and enactment of such monitoring processes. In particular, there is no systematic, tool-supported process for defining what is to be monitored given a strategic plan, nor are there tools for automatically generating monitoring queries. As a result, monitoring consists of a manual process whereby queries are generated for high level indicators across a few scorecards and dashboards. In this paper we present a systematic semi-automatic approach that covers the entire monitoring process. Our approach performs a par-", "num_citations": "26\n", "authors": ["1443"]}
{"title": "Modeling concept evolution: a historical perspective\n", "abstract": " The world is changing, and so must the data that describes its history. Not surprisingly, considerable research effort has been spent in Databases along this direction, covering topics such as temporal models and schema evolution. A topic that has not received much attention, however, is that of concept evolution. For example, Germany (instance-level concept) has evolved several times in the last century as it went through different governance structures, then split into two national entities that eventually joined again. Likewise, a caterpillar is transformed into a butterfly, while a mother becomes two (maternally-related) entities. As well, the concept of Whale (a class-level concept) changed over the past two centuries thanks to scientific discoveries that led to a better understanding of what the concept entails. In this work, we present a formal framework for modeling, querying and managing such evolution. In\u00a0\u2026", "num_citations": "26\n", "authors": ["1443"]}
{"title": "On the recognition of topological invariants by 4-way finite automata\n", "abstract": " It is the purpose of this paper to show that 4-way nondeterministic finite automata (4NFAs) cannot recognize simple topological properties of twodimensional patterns, whereas 4NFAs with a finite number of markers (pebbles) available can recognize many such properties. Multidimensional finite automata with markers have been studied by several authors, among them Blum and Hewitt [1968] who were concerned with 4-way deterministic, halting FAs that were presented with two-dimensional rectangular patterns; Shank [1971] who dealt with graph-walking automata computing properties of arbitrary directed graphs with all edges leaving each node ordered; and Milgram and Rosenfeld [1971] whose work involves FAs that can write on the input pattern. One conclusion that can be drawn from this research is that it is very difficult to show just how powerful FAs are for recognizing simple properties of patterns. The\u00a0\u2026", "num_citations": "26\n", "authors": ["1443"]}
{"title": "Integrating security patterns with security requirements analysis using contextual goal models\n", "abstract": " Security patterns capture proven security knowledge to help analysts tackle security problems. Although advanced research in this field has produced an impressive collection of patterns, they are not widely applied in practice. In parallel, Requirements Engineering has been increasing focusing on security-specific issues, arguing for an upfront treatment of security in system design. However, the vast body of security patterns are not integrated with existing proposals for security requirements analysis, making them difficult to apply as part of early system analysis and design. In this paper, we propose to integrate security patterns with our previously introduced goal-oriented security requirements analysis approach. Specifically, we provide a full concept mapping between textual security patterns and contextual goal models, as well as systematic instructions for constructing contextual goal models from security\u00a0\u2026", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Rationalism with a dose of empiricism: Case-based reasoning for requirements-driven self-adaptation\n", "abstract": " Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative behaviours, environmental parameters and requirements are clearly understood, which is often simply not true. Moreover, they do not consider the influence of the current behaviour of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed behaviours. The approach does not depend on a set of original adaptation cases, but employs goal\u00a0\u2026", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Dealing with multiple failures in zanshin: a control-theoretic approach\n", "abstract": " Adaptive software systems monitor the environment to ensure that their requirements are being fullled. When this is not the case, their adaptation mechanism proposes an adaptation (a change to the behaviour/configuration) that can lead to restored satisfaction of system requirements. Unfortunately, such adaptation mechanisms don't work very well in cases where there are multiple failures (divergence of system behaviour relative to several requirements). This paper proposes an adaptation mechanism that can handle multiple failures. The proposal consists of extending the Qualia adaptation mechanism of Zanshin enriched with features adopted from Control Theory. The proposed framework supports the definition of requirements for the adaptation process prescribing how to deal at runtime with problems such as conflicting requirements and synchronization, enhancing the precision and effectiveness of the\u00a0\u2026", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Incorporating goal analysis in database design: A case study from biological data management\n", "abstract": " We present a case study of a real-world industrial application which produced several versions of conceptual schema design for a biological database during its evolution. We apply the techniques of early requirements analysis to produce a goal model of the problem domain. We then show that by incorporating goal analysis in the design process we can account for the original schemas by tracing them back to stakeholder goals. Moreover, the goal-oriented analysis supports the systematic examination of the space of design alternatives, and better explains what the output of the design process (a conceptual schema) really means. Our results advocate the need for an extended design methodology for databases driven by stakeholder goals", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Representing and reasoning with preference requirements using goals\n", "abstract": " We introduce a goal-based framework for representing and reasoning with stakeholder preferences, aimed at supporting the analysis of behavioral variability at the requirements level. Stakeholder goals and alternative ways for fulfilling them are modeled through a goal graph. Temporal constraints are added to indicate conditions under which human and system tasks can be combined to satisfy the goals. The result implies a large set of alternative behaviors that can satisfy the root goals. Individual preferences are then specified as weighted rankings over formulae in linear temporal logic. The goal graph is translated in situation calculus and well-studied algorithms and tools for planning with preferences are appropriately adapted and used to generate plans that best fit to the specified preferences. This way, priorities over the stakeholders\u2019 high-level quality desires can be used to suggest behaviors that are most\u00a0\u2026", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Removing false code dependencies to speedup software build processes.\n", "abstract": " The development of large software systems involves a continual lengthy build process that may include preprocessing, compilation and linking of tens of thousands of source code files. In many cases, much of this build time is wasted because of false dependencies between implementation files and their respective header files. We present a graph algorithm and a programming tool that discovers and removes false dependencies among files. We show experimentally that the resulting preprocessed code is more compact, thereby contributing to faster build processes.", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Accommodating integrity constraints during database design\n", "abstract": " We address the problem of maintaining the integrity of large knowledge bases using a compile-time transaction modification technique. The novelty of the approach lies in the adaptation of ideas from Artificial Intelligence (AI) planning research. Starting with the observation that solutions to the frame and ramification problems can be used during database transaction design time, we propose an integrity maintenance technique that modifies transaction specifications by incorporating into them conditions necessary of the constraints' satisfaction. Additions to the transactions' postconditions whose effect is to maintain the integrity constraints, are generated from a set of transaction specifications. Thus, the implications of constraints are realized by the transaction specifier and the effort of having to prove transaction safety is saved, since it is guaranteed by the correctness of the generation process.", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Knowledge bases and databases: semantic vs. computational theories of information\n", "abstract": " The idea that Artificial Intelligence (hereafter Al) and databases have something to offer each other is not new. It began during the adolescence of AI and during the infancy of databases with early natural language front ends for file management systems [Thompson et al. 1969],[Kellogg et al. 1971]; limited\" deductive\" databases [Lindsay 1963],[Levien and Maron 1967]; and primitive\" semantic\" data models [Abrial 1974].Since 1975, the interaction between the two areas has broadened and has become more systematic. On the topic of natural language front ends to databases, prototypes such as [Harris 1977] and [Codd et al. 1978] have led to the acceptance of natural language as a useful communication medium between databases and casual users. Such prototypes also have been responsible for the first commercial applications of AI. Following Abrial, a host of semantic data models were proposed, including [Roussopoulos 1976],[Smith and Smith 1979],[Hammer and McLeod 1981],[Mylopoulos and Wong 1980], and [Codd 1979]. Many of these models accepted the idea that elements of semantic network representations have a useful role to play in data modeling [Roussopoulos and Mylopoulos 1975][Smith and Smith 1977b]. The relevance of Mathematical Logic to data modeling also was established during the same period thanks to research reported in [McSkimin and Minker 1977],[Nicolas and Gallaire 1978],", "num_citations": "25\n", "authors": ["1443"]}
{"title": "Building knowledge-based systems: The PSN experience\n", "abstract": " Knowledge representation is recognized today as a central problem in artificial-intelligence research because the current design paradigm for expert systems stresses the need for a knowledge base that stores expert knowledge and provides associated knowledge-handling facilities. This paradigm sharply contrasts with earlier ones that emphasized general-purpose heuristic search tech-niques.Research in knowledge representation may involve a study of howwe can represent particularsemantic no-tions, suchas time, causality, beliefs, and intentions. On the other hand, it may take the form of a languagedesign project, where the language is intended for knowledge representation and the\" programs\" written in the language are knowledge bases storing knowledge about some domain of expertise. A third alternative is that research in knowledgerepresentation may involve developing a programming environment, in the sense of Unix or Interlisp, for building and using knowledge bases. The Procedural Semantic Networks, or PSN, project began at the University of Toronto in 1976as a language design and implementation project. Since last year, however, we have been paying increasing attention to programming environmentalissues. Knowledge-representation languages have been clas-sified traditionally as declarative or procedural, depend-ing on whether their basic features come from mathe-matical logic or data structures, on one hand, or from programming languages. 1 Declarative representation languages, such as Prolog and KL-One, encourage per-spicuity and support the maintainability of a knowledge base. Procedural\u00a0\u2026", "num_citations": "25\n", "authors": ["1443"]}
{"title": "On the application of formal language and automata theory to pattern recognition\n", "abstract": " This paper studies M-way automata as a method of defining patterns in a multidimensional discrete space. It is shown that the membership question is recursively solvable for large classes of automata while the emptiness question is r, unsolvable even for the class of M-way finite automata. Efficient recognition algorithms are given for the pattern sets defined by M-way pushdown automata and certain error operators designed to make a set of patterns insensitive to noise are also presented and studied.", "num_citations": "25\n", "authors": ["1443"]}
{"title": "On the relation of graph grammars and graph automata\n", "abstract": " It is shown that a strong relationship exists between sets of graphs defined by graph (walking) automata with markers available and sets defined by graph grammars. Polynomial recognition algorithms are presented for certain classes of sets and it is argued that the existence of polynomial algorithms for other classes is doubtful. Other properties of the classes of sets defined by graph automata and graph grammars are also studied.", "num_citations": "24\n", "authors": ["1443"]}
{"title": "Stateful requirements monitoring for self-repairing socio-technical systems\n", "abstract": " Socio-technical systems consist of human, hardware and software components that work in tandem to fulfill stakeholder requirements. By their very nature, such systems operate under uncertainty as components fail, humans act in unpredictable ways, and the environment of the system changes. Self-repair refers to the ability of such systems to restore fulfillment of their requirements by relying on monitoring, reasoning, and diagnosing on the current state of individual requirements. Self-repair is complicated by the multi-agent nature of socio-technical systems, which demands that requirements monitoring and self-repair be done in a decentralized fashion. In this paper, we propose a stateful requirements monitoring approach by maintaining an instance of a state machine for each requirement, represented as a goal, with runtime monitoring and compensation capabilities. By managing the interactions between the\u00a0\u2026", "num_citations": "23\n", "authors": ["1443"]}
{"title": "Toward Web-based application management systems\n", "abstract": " As Web technology spreads, the number, variety, and sophistication of Web based information services is literally exploding. While some effort has been put into managing a single, centrally controlled Web site, current Web technologies offer little help for managing Web based applications in-the-large. This is partly due to the distributed, heterogeneous, and open nature of such applications. The paper proposes a generic framework for managing Web based applications which addresses both semantic and managerial issues. Semantic issues are addressed through the inclusion of a domain model component in the framework which describes the kinds of information that are available. Management issues are treated through a framework which includes formally defined notions for an information model, information base consistency, transactions, and concurrency control. Thus, the proposed management system\u00a0\u2026", "num_citations": "23\n", "authors": ["1443"]}
{"title": "The TaxisDL software description language\n", "abstract": " The purpose of the TaxisDL language is to express the conceptual design of an information system. The focus of the design process includes the data classes of the proposed system, the functions and transactions manipulating them, and the larger conceptual groupings of these actions into long-term activities, which we call scripts. The design of the language is based on ideas from semantic data models and formal specification languages.", "num_citations": "23\n", "authors": ["1443"]}
{"title": "Knowledge based and database systems: enhancements, coupling or integration?\n", "abstract": " Position: The technologies in the areas of Knowledge Based or Expert Systems (KBS) and of Database Management Systems (DBMS) will play a major role in future Information Systems. Research issues present in a combined use of the two technologies have attracted researchers, both from Artificial Intelligence and Databases.               This short paper examines possible research strategies. They are basically three: (a) enhancements of existing systems, (b) coupling of independent systems, and (c) technology integration resulting in a new class of systems, which are not constrained by the objectives and design characteristics of Knowledge Based and Database Systems.               The position taken is: system enhancements present a short-term partial solution, coupling presents an easy and practical solution and although integration is elegant and promising, it may never lead to a practically acceptable\u00a0\u2026", "num_citations": "23\n", "authors": ["1443"]}
{"title": "Towards a compositional semantic account of data quality attributes\n", "abstract": " We address the fundamental question: what does it mean for data in a database to be of high quality? We motivate our discussion with examples, where traditional views on data quality are found to be unsatisfactory. Our work is founded on the premise that data values are primarily linguistic signs that convey meaning from their producer to their user through senses and referents. In this setting, data quality issues arise when discrepancies occur during this communication. We sketch a theory of senses for individual values in a relational table based on its semantics expressed using some ontology. We use this to offer a compositional approach, where data quality is expressed in terms of a variety of primitive relationships among values and their senses. We evaluate our approach by accounting for quality attributes in other frameworks proposed in the literature. This exercise allows us to (i) reveal and\u00a0\u2026", "num_citations": "22\n", "authors": ["1443"]}
{"title": "Discovering and maintaining semantic mappings between XML schemas and ontologies\n", "abstract": " There is general agreement that the problem of data semantics has to be addressed for XML data to become machine-processable. This problem can be tackled by defining a semantic mapping between an XML schema and an ontology. Unfortunately, creating such mappings is a tedious, time-consuming, and error-prone task. To alleviate this problem, we present a solution that heuristically discovers semantic mappings between XML schemas and ontologies. The solution takes as input an initial set of simple correspondences between element attributes in an XML schema and class attributes in an ontology, and then generates a set of mapping formulas. Once such a mapping is created, it is important and necessary to maintain the consistency of the mapping when the associated XML schema and ontology evolve. In this paper, we first offer a mapping formalism to represent semantic mappings. Second, we present our heuristic mapping discovery algorithm. Third, we show through an empirical study that considerable effort can be saved when discovering complex mappings by using our prototype tool. Finally, we propose a mapping maintenance plan dealing with schema evolution. Our study provides a set of effective solutions for building sustainable semantic integration systems for XML data.", "num_citations": "22\n", "authors": ["1443"]}
{"title": "Developing a science base for enterprise interoperability\n", "abstract": " Interoperability is an important characteristic of information systems, organisations, their processes and data. Achieving automated collaboration of processes and systems may lead to a dramatic increase in productivity for enterprises of any size. As a result of this projected benefit, interoperability has been prescribed by numerous standardization frameworks, guidelines at enterprise level, data schemas and techniques to tackle the problem of non-communicating systems or organisations. In parallel, most international software, hardware and service vendors created their own strategies for achieving the goal of open, collaborative, loosely coupled systems and components. This paper goes beyond the presentation of the main milestones in this fascinating quest for collaboration between people, systems and information: it attempts to describe how this new interdisciplinary research area can transform into a\u00a0\u2026", "num_citations": "21\n", "authors": ["1443"]}
{"title": "Integrating requirements engineering and cognitive work analysis: A case study\n", "abstract": " Requirements engineering is a fundamental component of systems engineering. This paper describes how, and where, cognitive engineering (specifically the Cognitive Work Analysis (CWA) framework) could be applied in requirements engineering. We introduce an existing RE toolkit, the \u03b9* modeling framework, and compare it with the CWA framework. The paper concludes with an outline of opportunities to integrate the techniques of cognitive engineering with those from requirements engineering in the design of complex sociotechnical systems.", "num_citations": "21\n", "authors": ["1443"]}
{"title": "Applying software analysis technology to lightweight semantic markup of document text\n", "abstract": " Software analysis techniques, and in particular software \u201cdesign recovery\u201d, have been highly successful at both technical and businesslevel semantic markup of large scale software systems written in a wide variety of programming languages, and in particular have proven e.cient and scalable in assisting the resolution of the \u201cyear 2000\u201d problem for billions of lines of legacy source code. In this work we describe a first experiment in applying the same technical solutions and tools that have proven so successful in software markup to the more general problem of semantic markup of text documents. In this early report we describe our adaptation of the software analysis techniques, propose a general domain-independent architecture for semantic markup using them, and demonstrate its feasibility in a limited but realistic domain of application by comparison with both raw and tool-assisted human semantic\u00a0\u2026", "num_citations": "21\n", "authors": ["1443"]}
{"title": "Properties of Information Modeling Techniques for Information Systems Engineering\n", "abstract": " Information modeling constitutes a cornerstone of information systems engineering and management. To build, operate and maintain an information system, one needs to capture and represent the meaning and inherent structure of a variety of rich and multi-faceted information, including the system\u2019s subject matter, its internal structure, its operational environment and its development history. Once captured, the information can be used for communication between people--say, the information system owners, users and developers--but also for building tools which facilitate their management throughout their lifetime.", "num_citations": "21\n", "authors": ["1443"]}
{"title": "A language facility for designing interactive database-intensive applications\n", "abstract": " This paper describes TAXIS, a language for the design of Interactive Information Systems (eg, credit card varification, student-course registration and airline reservations). TAXIS offers (relational) database management facilities, a means of specifying semantic integrity constraints and an exception-handling mechanism, integrated into a single language through the concepts of class, property and the ISA (generalization) relationship. The paper includes a description of the main constructs of TAXIS and illustrates their usefulness with examples.", "num_citations": "21\n", "authors": ["1443"]}
{"title": "Holistic security requirements analysis for socio-technical systems\n", "abstract": " Security has been a growing concern for large organizations, especially financial and governmental institutions, as security breaches in the systems they depend on have repeatedly resulted in billions of dollars in losses per year, and this cost is on the rise. A primary reason for these breaches is that the systems in question are \u201csocio-technical\u201d a mix of people, processes, technology, and infrastructure. However, such systems are designed in a piecemeal rather than a holistic fashion, leaving parts of the system vulnerable. To tackle this problem, we propose a three-layer security analysis framework consisting of a social layer (business processes, social actors), a software layer (software applications that support the social layer), and an infrastructure layer (physical and technological infrastructure). In our proposal, global security requirements lead to local security requirements, cutting across conceptual\u00a0\u2026", "num_citations": "20\n", "authors": ["1443"]}
{"title": "Evaluating modeling languages: an example from the requirements domain\n", "abstract": " Modeling languages have been evaluated through empirical studies, comparisons of language grammars, and ontological analyses. In this paper we take the first approach, evaluating the expressiveness and effectiveness of Techne, a requirements modeling language, by applying it to three requirements problems from the literature. We use our experiences to propose a number of language improvements for Techne, addressing challenges discovered during the studies. This work presents an example evaluation of modeling language expressiveness and effectiveness through realistic case studies.", "num_citations": "20\n", "authors": ["1443"]}
{"title": "A lightweight approach to semantic annotation of research papers\n", "abstract": " This paper presents a novel application of a semantic annotation system, named Cerno, to analyze research publications in electronic format. Specifically, we address the problem of providing automatic support for authors who need to deal with large volumes of research documents. To this end, we have developed Biblio, a user-friendly tool based on Cerno. The tool directs the user\u2019s attention to the most important elements of the papers and provides assistance by generating automatically a list of references and an annotated bibliography given a collection of published research articles. The tool performance has been evaluated on a set of papers and preliminary evaluation results are promising. The backend of Biblio uses a standard relational database to store the results.", "num_citations": "20\n", "authors": ["1443"]}
{"title": "Requirements for telecommunications services: An attack on complexity\n", "abstract": " In engineering the requirements for a telecommunications system, the greatest obstacle to be overcome is the sheer complexity of the required behavior. We present several ways of managing and minimizing this complexity, all of proven effectiveness. Most of the specification techniques result from specific application of general requirements principles to the telecommunications domain.", "num_citations": "20\n", "authors": ["1443"]}
{"title": "Key performance indicator elicitation and selection through conceptual modelling\n", "abstract": " Key Performance Indicators (KPIs) operationalize ambiguous enterprise goals into quantified variables with clear thresholds. Their usefulness has been established in multiple domains yet it remains a difficult and error-prone task to find suitable KPIs for a given strategic goal. A careful analysis of the literature on both strategic modeling, planning and management reveals that this difficulty is due to a number of factors. Firstly, there is a general lack of adequate conceptualizations that capture the subtle yet important differences between performance and result indicators. Secondly, there is a lack of integration between modelling and data analysis techniques that interleaves analysis with the modeling process. In order to tackle these deficiencies, we propose an approach for selecting explicitly KPIs and Key Result Indicators (KRIs). Our approach is comprised of (i) a novel modeling language that exploits the\u00a0\u2026", "num_citations": "19\n", "authors": ["1443"]}
{"title": "Rationalism with a dose of empiricism: combining goal reasoning and case-based reasoning for self-adaptive software systems\n", "abstract": " Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative system configurations, environmental parameters and requirements are clearly understood, which is often not true. Moreover, they do not consider the influence of the current configuration of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed system configurations. The approach does not depend on a set of original\u00a0\u2026", "num_citations": "19\n", "authors": ["1443"]}
{"title": "Capturing Contextual Variability in i* Models.\n", "abstract": " Exploration and analysis of alternatives is one of the main activities in requirements engineering, both in early and in late requirements phases. While L and L-derived modeling notations provide facilities for capturing certain types of variability, domain properties (and other external influences) and their effects on L models cannot be easily modeled. In this paper, we propose to explore how our previous work on context-dependent goal models can be extended to support L. Here, we examine how L modeling can benefit from both monitorable (ie, defined through real-world phenomena) and non-monitorable (eg, viewpoints, versions, etc.) contexts defined using our context framework.", "num_citations": "19\n", "authors": ["1443"]}
{"title": "A lightweight approach to semantic web service synthesis\n", "abstract": " Web service technologies are becoming a new paradigm for distributed computing. With increasing number of web services available on the internet, there is an urgent need for information brokers that can autonomously integrate web services on behalf of a user. To this end, we propose web service specification, which defines what the service is but not how the service is implemented, and service synthesizer, which can dynamically synthesize the implementation of the specification from existing web services over the web.", "num_citations": "19\n", "authors": ["1443"]}
{"title": "Higher order generalization and its application in program verification\n", "abstract": " Generalization is a fundamental operation of inductive inference. While first order syntactic generalization (anti\u2013unification) is well understood, its various extensions are often needed in applications. This paper discusses syntactic higher order generalization in a higher order language \u03bb2 [1]. Based on the application ordering, we prove that least general generalization exists for any two terms and is unique up to renaming. An algorithm to compute the least general generalization is also presented. To illustrate its usefulness, we propose a program verification system based on higher order generalization that can reuse the proofs of similar programs.", "num_citations": "19\n", "authors": ["1443"]}
{"title": "Integration issues in implementing semantic data models\n", "abstract": " Integration issues in implementing semantic data models | Advances in database programming languages ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksAdvances in database programming languagesIntegration issues in implementing semantic data models chapter Integration issues in implementing semantic data models Share on Authors: Brian Andrew Nixon profile image Brian Nixon View Profile , John P Mylopoulos profile image John Mylopoulos View Profile Authors Info & Affiliations Publication: Advances in database programming languagesNovember 1990 Pages 187\u2013217https://doi.org/10.1145/101620.101632 3\u2026", "num_citations": "19\n", "authors": ["1443"]}
{"title": "Engineering self-adaptive software systems: from requirements to model predictive control\n", "abstract": " Self-adaptive software systems monitor their operation and adapt when their requirements fail due to unexpected phenomena in their environment. This article examines the case where the environment changes dynamically over time and the chosen adaptation has to take into account such changes. In control theory, this type of adaptation is known as Model Predictive Control and comes with a well-developed theory and myriad successful applications. The article focuses on modeling the dynamic relationship between requirements and possible adaptations. It then proposes a controller that exploits this relationship to optimize the satisfaction of requirements relative to a cost function. This is accomplished through a model-based framework for designing self-adaptive software systems that can guarantee a certain level of requirements satisfaction over time by dynamically composing adaptation strategies when\u00a0\u2026", "num_citations": "18\n", "authors": ["1443"]}
{"title": "An overview of requirements evolution\n", "abstract": " Changing requirements are widely regarded as one of the most significant risks for software systems development. However, in today\u2019s business landscape, these changing requirements also represent opportunities to exploit new and evolving business conditions. In consonance with other agile methods, we advocate requirements engineering techniques that embrace and accommodate requirements change. This agile approach to requirements must nonetheless be systematic and incorporate some degree of planning, especially with respect to accommodating quality attributes such as safety and security. This chapter examines the nature of requirements evolution, and the two main problems that it entails. The first is to correctly understand what is changing in the requirements, that is, the elicitation problem. The other is to act on that new information using models and other representations of the\u00a0\u2026", "num_citations": "18\n", "authors": ["1443"]}
{"title": "Requirements models for design-and runtime: A position paper\n", "abstract": " In this position paper we review the history of requirements models and conclude that a goal-oriented perspective offers a suitable abstraction for requirements analysis. We stake positions on the nature of modelling languages in general, and requirements modelling languages in particular. We then sketch some of the desirable features (... \u201crequirements\u201d) of design-time and runtime requirements models and draw conclusions about their similarities and differences.", "num_citations": "18\n", "authors": ["1443"]}
{"title": "Strategic management for real-time business intelligence\n", "abstract": " Even though much research has been devoted on real-time data warehousing, most of it ignores business concerns that underlie all uses of such data. The complete Business Intelligence (BI) problem begins with modeling and analysis of business objectives and specifications, followed by a systematic derivation of real-time BI queries on warehouse data. In this position paper, we motivate the need for the development of a complete Real Time BI stack able to continuously evaluate and reason about strategic objectives. We argue that an integrated system, able to receive formal specifications of the organization\u2019s strategic objectives and to transform them into a set of queries that are continuously evaluated against the warehouse, offers significant benefits. In this context, we propose the development of a set of real-time query answering mechanisms able to identify warehouse segments with temporal\u00a0\u2026", "num_citations": "18\n", "authors": ["1443"]}
{"title": "Knowledge bases and databases: Current trends and future directions\n", "abstract": " Bt~ ding computer systems that manage and interpret large amounts of information is an old idea, as old as computer science. After a decade of practice in building expert systems, we are concluding that for any expert system technology to mature, somehow, this old idea will have to be realized. We will try to outline the reasons for requiring from a computer system both access to large amounts of information and the ability to interpret that information. We will also characterize the main approaches to a solution. In the process, we hope to convince the reader that deep solutions to the problem at hand lead to a new perspective on knowledge bases and knowledge representation systems which transcend both expert system and database technologies as we now know them.Knowledge bases are not a novelty any more. They have been studied in research labs since the late'60s and have seen commercial\u00a0\u2026", "num_citations": "18\n", "authors": ["1443"]}
{"title": "Distributed cognition in the management of design requirements\n", "abstract": " Requirements processes in contemporary information systems development efforts have become increasingly distributed across organizational and temporal boundaries. This trend has created a range of challenges and opportunities for IT design professionals. In the present study, we combine the insights from a multiple-case study research effort and the framework provided by the theory of distributed cognition to development a model of distributed requirements practice. The model reveals that systems development projects reflect social, structural, and temporal forms of distributed cognition as they pursue effective design outcomes.", "num_citations": "17\n", "authors": ["1443"]}
{"title": "Computational Properties of Epistemic Logic Programs.\n", "abstract": " Gelfond\u2019s epistemic logic programs are not only an extension of disjunctive extended logic programs for handling difficulties in reasoning with incomplete information, but also an effective formalism to represent agents\u2019 epistemic reasoning under a logic programming setting. Recently there is an increasing research in this direction. However, for many years the complexity of epistemic logic programs remains unclear. This paper provides a precise answer to this problem. We prove that consistency check for epistemic logic programs is in PSPACE and this upper bound is also tight. The approach developed in our proof is of interest on its own: it immediately yields an algorithm to compute world views of an epistemic logic program, and it can also be used to study computational properties of nested epistemic logic programs-a significant generalization of Gelfond\u2019s formalism.", "num_citations": "17\n", "authors": ["1443"]}
{"title": "Experimenting with Linguistic Tools for Conceptual Modelling: Quality of the models and critical features\n", "abstract": " This paper presents the results of three experiments designed to assess the extent to which a Natural-Language Processing (NLP) tool improves the quality of conceptual models, specifically object-oriented ones. Our main experimental hypothesis is that the quality of a domain class model is higher if its development is supported by a NLP system. The tool used for the experiment \u2013 named NL-OOPS \u2013 extracts classes and associations from a knowledge base realized by a deep semantic analysis of a sample text. In our experiments, we had groups working with and without the tool, and then compared and evaluated the final class models they produced. The results of the experiments give insights on the state of the art of NL-based Computer Aided Software Engineering (CASE) tools and allow identifying important guidelines to improve their performance, highlighting which of the linguistic tasks are more\u00a0\u2026", "num_citations": "17\n", "authors": ["1443"]}
{"title": "A distributed rule mechanism for multidatabase systems\n", "abstract": " We describe a mechanism based on distributed Event-Condition-Action (ECA) rules that supports data coordination in a multidatabase setting. The proposed mechanism includes an ECA rule language and a rule execution engine that transforms and partitions rules when they are first posted, and then coordinates their execution. The paper also presents a prototype implementation in a simulated environment as well as preliminary experimental results on its performance. This work is part of an on-going project intended to develop data coordination techniques for peer-to-peer databases.", "num_citations": "17\n", "authors": ["1443"]}
{"title": "Object-Oriented and Knowledge Representation\n", "abstract": " CiNii \u8ad6\u6587 - Object-Oriented and Knowledge Representation CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Object-Oriented and Knowledge Representation MYLOPOULOS J. \u88ab\u5f15\u7528 \u6587\u732e: 1\u4ef6 \u8457\u8005 MYLOPOULOS J. \u53ce\u9332\u520a\u884c\u7269 Object-Oriented Databases: Analysis, Design, & Construction (DS-4) Object-Oriented Databases: Analysis, Design, & Construction (DS-4), 23-37, 1990 North-Holland \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u4ed5\u69d8\u8a18\u8ff0 Hirotaka Sakai \u60c5\u5831\u51e6\u7406 \u5b66\u4f1a\u8ad6\u6587\u8a8c 37(6), 1162-1170, 1996-06-15 \u53c2\u8003\u6587\u732e17\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10000032709 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u306e\u4e00\u90e8 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 \u2026", "num_citations": "17\n", "authors": ["1443"]}
{"title": "Database programming by formal refinement of conceptual designs\n", "abstract": " In the process of developing an Information System, one passes through stages that include requirements gathering, design speci cation, and, last but not least, the implementation and maintenance of large database application programs. With an evolutionary view on the application domain and its requirements, captured by the knowledge representation language Telos, we rst discuss the conceptual design of an information system by the TDL language and then concentrate on the iterative re nement of TDL designs into database programs. Our target language is DBPL, a procedural database programming language with persistent values of a wide variety of types and with database transactions as rst-class language constructs. DBPL was designed with particular emphasis on database and program modularity. The re nement of TDL designs into DBPL implementations is formalized in J.-R. Abrial's framework of Abstract Machines and Generalized Substitutions.", "num_citations": "17\n", "authors": ["1443"]}
{"title": "Capturing variability in adaptation spaces: A three-peaks approach\n", "abstract": " Variability is essential for adaptive software systems, because it captures the space of alternative adaptations a system is capable of when it needs to adapt. In this work, we propose to capture variability for an adaptation space in terms of a three dimensional model. The first dimension captures requirements through goals and reflects all possible ways of achieving these goals. The second dimension captures supported variations of a system\u2019s architectural structure, modeled in terms of connectors and components. The third dimension describes supported system behaviors, by modeling possible sequences for goal fulfillment and task execution. Of course, the three dimensions of a variability model are inter-twined as choices made with respect to one dimension have impact on the other two. Therefore, we propose an incremental design methodology for variability models that keeps the three dimensions\u00a0\u2026", "num_citations": "15\n", "authors": ["1443"]}
{"title": "TBIM: A language for modeling and reasoning about business plans\n", "abstract": " Conceptual models of different aspects of an organization\u2014business objectives, processes, rules, policies and objects\u2014have been used for organizational design, analysis, planning, and knowledge management. Such models have also served as starting points for designing information systems and conducting business intelligence activities. This paper proposes the Tactical Business Intelligence Model (TBIM), a language for modeling strategic business plans. TBIM lies in between the strategic and tactical level, for strategic plans are abstract business tactics. TBIM extends the BIM strategic modeling language with primitives for business model design. In addition to presenting the syntax of TBIM, we illustrate its usage through a medium-sized case study. We also propose a method for evaluating alternative plans through a mapping to business process models and the usage of simulation techniques.", "num_citations": "15\n", "authors": ["1443"]}
{"title": "IRIS\u2014A mapping assistant for generating designs from requirements\n", "abstract": " The problem of generating information system designs from requirements specifications is addressed, with the presentation of a framework for representing requirements and a mapping assistant, IRIS3, that facilitates the design generation process. Requirements are viewed as knowledge bases and the knowledge representation formalism for the prototype, also the language for implementing IRIS, is Telos which provides facilities for describing entities and relationships and for representing and reasoning with temporal knowledge. The generation of a design is achieved with a mapping process from requirements which is: (i) Locally guided by dependency types determining allowable mappings of an element of a requirements model, (ii) globally guided by non-functional requirements, such as accuracy and security requirements on the intended system, represented as goals describing desirable properties\u00a0\u2026", "num_citations": "15\n", "authors": ["1443"]}
{"title": "Methodological and computer aids for interactive information system development\n", "abstract": " The methodology involves concept specialization and is particularly well suited for llSs, which generally involve large amounts of unstructured detail that must somehow be integrated into a coherent system The Taxis programming language offers constructs which enable the designer to define taxonomies of classes representing data structures, transactions, exceptions and their handlers, as well as scripts that specify the patterns of user-system dialogues The design environment provides facilities for displaying parts of the conceptual structure of systems, class oriented editors and verifiers of semantic consistency; an interpreter used for prototyping and a compiler from Taxis to a conventional programming language and database management system are also included", "num_citations": "15\n", "authors": ["1443"]}
{"title": "Goals and Compliance in Nomos 3.\n", "abstract": " The impact of laws in Requirement Engineering has increasingly drawn attention to new methodologies and techniques that address the problem of aligning a set of requirements with applicable norms. Goal models have long provided a lightweight approach for the representation and analysis of requirements. This paper presents ongoing work for evaluating the compliance of a goal model with applicable laws.", "num_citations": "14\n", "authors": ["1443"]}
{"title": "Towards augmenting requirements models with preferences\n", "abstract": " The analysis of stakeholder requirements is a critical aspect of software engineering. A common way of specifying stakeholder requirements is in terms of a hierarchy of goals whose AND/OR decomposition captures a family of software solutions that comply with the goals. In this paper, we extend this goal modeling framework to include the specification of optional user requirements and user preferences, aggregated together into weighted formulae to be optimized. We team this with an automated reasoning tool, adapted from state of the art research in artificial intelligence planning with preferences, in order to synthesize solutions that both comply with the goals and optimize stakeholder preferences and optional requirements.", "num_citations": "14\n", "authors": ["1443"]}
{"title": "Techne: A (nother) requirements modeling language\n", "abstract": " This paper introduces the requirements modeling language called Techne, in response to a new core ontology for requirements [5]. The basic elements of Techne models are propositions that represent domain assumptions, goals, quality constraints, or tasks. Moreover, goals and quality constraints may be mandatory or optional and there can be preference relations defined over them. Given a requirements problem consisting of mandatory/optional goals and domain assumptions, a solution consists of a collection of tasks and quality constraints which together satisfy all mandatory goals and do as well as possible with respect to optional ones. A formal semantics is provided for Techne, along with a syntax, as well as illustrative examples.", "num_citations": "14\n", "authors": ["1443"]}
{"title": "Knowledge representation in the software development process: A case study\n", "abstract": " This paper presents an overview of the premises, research results, prototype implementations and prospectus of the Taxis Project at the University of Toronto. The project addresses several problems of Software Engineering within a unified representational framework. The framework offers a coherent set of basic principles for modelling and abstraction \u2014 drawn from work in knowledge representation in the field of Artificial Intelligence (AI) \u2014 and applies them to several aspects of system design and description.", "num_citations": "14\n", "authors": ["1443"]}
{"title": "On the definition and recognition of patterns in discrete spaces\n", "abstract": " On the definition and recognition of patterns in discrete spaces | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleThesesOn the definition and recognition of patterns in discrete spaces ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books cover image On the definition and recognition of patterns in discrete spaces January 1970 193 pages Author: John P. Mylopoulos Copyright \u00a9 1970 Publisher Princeton University United \u2026", "num_citations": "14\n", "authors": ["1443"]}
{"title": "Stress testing strategic goals with SWOT analysis\n", "abstract": " Business strategies are intended to guide a company across the mine fields of competitive markets through the fulfilment of strategic objectives. The design of a business strategy generally considers a SWOT operating context consisting of inherent Strengths (S) and Weaknesses (W) of a company, as well as external Opportunities (O) and potential Threats (T) that the company may be facing. Given an ever-changing and uncertain environment, it is important to continuously maintain an updated view of the operating context, in order to determine whether the current strategy is adequate. However, traditional SWOT analysis only provides support for the initial design of business strategy, as opposed to on-going analysis as new, unexpected factors appear and disappear. This paper proposes a systematic analysis for business strategy founded on models of strategic goals and stress test scenarios. Our\u00a0\u2026", "num_citations": "13\n", "authors": ["1443"]}
{"title": "Requirements evolution drives software evolution\n", "abstract": " Changes to software should be made with reference to the requirements of that software, as these requirements provide the reasons for a change. Requirements serve to tie the implementation world of the developers to the problem world of the stakeholders. Most empirical studies of requirements have shown that misunderstood and changing requirements cause the majority of failures and costs in software. However, research in software evolution has typically focused on how to evolve software and not why. In our view, evolving software is about solving requirements problems, that is, finding new implementations which will satisfy the requirements while respecting domain assumptions. We argue that by describing this relationship, an implementation choice that best meets stakeholder needs can be made. We describe a tool that models requirements problems. This tool can find incremental solutions to evolving\u00a0\u2026", "num_citations": "13\n", "authors": ["1443"]}
{"title": "The business intelligence model: Strategic modelling\n", "abstract": " Business intelligence (BI) consists of a range of technologies for using information within organizations to ensure compliance to strategic and tactical objectives, as well as to laws and regulations. As a research field, it encompasses data and knowledge management, modeling of processes and policies, data quality, data privacy and security, data cleaning and integration, data exchange, inconsistency management, information retrieval, data mining, analytics, and decision support. This interest in technologies and services that improve organizational governance has caused dramatic growth for the BI market and the industry that serves it. By now, most competitive organizations have a significant investment in BI, much of it technology-related, based on software tools and artifacts. However, as summarized by Gartner [35], one important problem of BI technologies is that information generated by BI systems and other decision inputs are rarely linked to business decisions and outcomes. In addition, business people\u2013be they executives, managers, consultants, or analysts\u2013are in general agreement that what they are looking for is not new gadgets producing a dizzying array of largely useless statistics. Instead, they are interested in having their business data analyzed in their terms: strategic objectives, business models and strategies, business processes, markets, trends and risks. This gap between the worlds of business and data remains today the greatest barrier to the adoption of BI technology, and the greatest factor in the cost of applying BI technology. We propose to bridge this gap by extending the notion of conceptual schema to include\u00a0\u2026", "num_citations": "13\n", "authors": ["1443"]}
{"title": "Annotating accommodation advertisements using cerno\n", "abstract": " There has been great interest in applying Semantic Web technologies to the tourism sector ever since Tim Berners-Lee introduced his vision. Unfortunately, there is a major obstacle in realizing such applications: tourist (or other) information on the Web has to be semantically annotated, and this happens to be a very time-and resource-consuming process. In this work we present the application of a lightweight automated approach for the annotation of accommodation advertisements. The annotation tool, called Cerno, allows for annotation of text according to a predefined conceptual schema. Resulting annotations are stored in a database, allowing users to quickly find the best match to personal requirements. To evaluate our framework, we have conducted a series of experiments that support the efficacy of our proposal with respect to annotation quality and fulfilment of user information needs.", "num_citations": "13\n", "authors": ["1443"]}
{"title": "Design matters for semantic web services\n", "abstract": " The several Semantic Web Services (SWS) initiatives seems to really pave the way for supporting autonomous and goal-oriented behavior during the whole service providing life-cycle. The main results based on service ontology languages, e.g., OWL-S, allow for automatic discover, invocation, and composition of services reducing the human intervention at a minimum.  Nevertheless, the SWS approaches have not focused enough on the analysis and design phases in order to support discover, composition, and the service ontology definition also at design time. On the contrary, our approach aims to illustrate possible advantages when considering the Tropos analysis and design levels over the SWS technologies", "num_citations": "13\n", "authors": ["1443"]}
{"title": "Semantic models for knowledge management\n", "abstract": " We explore the use of a semantic model to support a group of strategic business analysts in their daily work. In particular, we present a set of modeling constructs for representing goals, events and actors that are relevant to the work of analysts. We also describe a qualitative goal analysis procedure which makes it possible to reason about a goal model under different assumptions. The paper also reports on an incremental document classification scheme that can be used to classify relevant documents with respect to the concepts constituting the semantic model.", "num_citations": "13\n", "authors": ["1443"]}
{"title": "Efficient maintenance of temporal integrity in knowledge bases.\n", "abstract": " Degree: Ph. D.DegreeYear: 1996Institute: University of Toronto (Canada)Adviser: John Mylopoulos.The maintenance of semantic integrity has been recognized as a cornerstone issue for the development of databases and knowledge bases alike. Despite the extensive research conducted during the last two decades, semantic integrity maintenance has yet to become a practical technology. Furthermore, the need for modeling evolving domains has given rise to challenging research issues relating to the incorporation of time in knowledge bases. In this thesis, we study the problem of maintaining the integrity of temporal deductive knowledge bases. We argue that existing approaches in either temporal or deductive databases do not address the problem in a satisfactory manner, nor do they deal with all the issues involved in a unified framework. At first, we propose an assertion language that permits us to express\u00a0\u2026", "num_citations": "13\n", "authors": ["1443"]}
{"title": "A perspective for research on conceptual modelling\n", "abstract": " This position paper is intended to provide a perspective for research on conceptual modelling carried out over the past five years at the University of Toronto and to draw some conclusions from the experiences we have accumulated.\u201cConceptual modelling\u201d here refers to the activity of constructing abstract models of knowledge about some world and is synonymous with the terms \u201cknowledge representation\u201d and \u201csemantic data model\u201d as they have been used in AI and Databases respectively. Much of the research on the subject has focused on the development of descriptive tools for the description of such models. Less attention has been paid, so far, on methodologies for building such models.", "num_citations": "13\n", "authors": ["1443"]}
{"title": "A pattern language for value modeling in ArchiMate\n", "abstract": " In recent years, there has been a growing interest in modeling value in the context of Enterprise Architecture, which has been driven by a need to align the vision and strategic goals of an enterprise with its business architecture. Nevertheless, the current literature shows that the concept of value is conceptually complex and still causes a lot of confusion. For example, we can find in the literature the concept of value being taken as equivalent to notions as disparate as goals, events, objects and capabilities. As a result, there is still a lack of proper support for modeling all aspects of value as well as its relations to these aforementioned notions. To address this issue, we propose in this paper a pattern language for value modeling in ArchiMate, which is based on the Common Ontology of Value and Risk, a well-founded reference ontology developed following the principles of the Unified Foundation Ontology\u00a0\u2026", "num_citations": "12\n", "authors": ["1443"]}