{"title": "An overview of the BlueGene/L supercomputer\n", "abstract": " This paper gives an overview of the BlueGene/L Supercomputer. This is a jointly funded research partnership between IBM and the Lawrence Livermore National Laboratory as part of the United States Department of Energy ASCI Advanced Architecture Research Program. Application performance and scaling studies have recently been initiated with partners at a number of academic and government institutions,including the San Diego Supercomputer Center and the California Institute of Technology. This massively parallel system of 65,536 nodes is based on a new architecture that exploits system-on-a-chip technology to deliver target peak processing power of 360 teraFLOPS (trillion floating-point operations per second). The machine is scheduled to be operational in the 2004-2005 time frame, at price/performance and power consumption/performance targets unobtainable with conventional architectures.", "num_citations": "646\n", "authors": ["646"]}
{"title": "Virtual memory mapped network interface for the SHRIMP multicomputer\n", "abstract": " The network interfaces of existing multicomputers require a significant amount of software overhead to provide protection and to implement message passing protocols. This paper describes the design of a low-latency, high-bandwidth, virtual memory-mapped network interface for the SHRIMP multicomputer project at Princeton University. Without sacrificing protection, the network interface achieves low latency by using virtual memory mapping and write-latency hiding techniques, and obtains high bandwidth by providing a user-level block data transfer mechanism. We have implemented several message passing primitives in an experimental environment, demonstrating that our approach can reduce the message passing overhead to a few user-level instructions.", "num_citations": "623\n", "authors": ["646"]}
{"title": "Overview of the Blue Gene/L system architecture\n", "abstract": " The Blue Gene\u00ae/L computer is a massively parallel supercomputer based on IBM system-on-a-chip technology. It is designed to scale to 65,536 dual-processor nodes, with a peak performance of 360 teraflops. This paper describes the project objectives and provides an overview of the system architecture that resulted. We discuss our application-based approach and rationale for a low-power, highly integrated design. The key architectural features of Blue Gene/L are introduced in this paper: the link chip component and five Blue Gene/L networks, the PowerPC\u00ae 440 core and floating-point enhancements, the on-chip and off-chip distributed memory system, the node- and system-level design for high reliability, and the comprehensive approach to fault isolation.", "num_citations": "534\n", "authors": ["646"]}
{"title": "Blue Gene/L torus interconnection network\n", "abstract": " The main interconnect of the massively parallel Blue Gene\u00ae/L is a three-dimensional torus network with dynamic virtual cut-through routing. This paper describes both the architecture and the microarchitecture of the torus and a network performance simulator. Both simulation results and hardware measurements are presented.", "num_citations": "445\n", "authors": ["646"]}
{"title": "Multi-petascale highly efficient parallel supercomputer\n", "abstract": " Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.) 2010-01-08", "num_citations": "420\n", "authors": ["646"]}
{"title": "The ibm blue gene/q compute chip\n", "abstract": " Blue Gene/Q aims to build a massively parallel high-performance computing system out of power-efficient processor chips, resulting in power-efficient, cost-efficient, and floor-space- efficient systems. Focusing on reliability during design helps with scaling to large systems and lowers the total cost of ownership. This article examines the architecture and design of the Compute chip, which combines processors, memory, and communication functions on a single chip.", "num_citations": "361\n", "authors": ["646"]}
{"title": "Massively parallel supercomputer\n", "abstract": " A novel massively parallel supercomputer of hundreds of teraOPS-scale includes node architectures based upon System-On-a-Chip technology, ie, each processing node comprises a single Application Specific Integrated Circuit (ASIC). Within each ASIC node is a plurality of processing elements each of which consists of a central processing unit (CPU) and plurality of floating point processors to enable optimal balance of computational performance, packaging density, low cost, and power and cooling requirements. The plurality of processors within a single node may be used individually or simultaneously to work on any combination of computation or communication as required by the particular algorithm being solved or executed at any point in time. The system-on-a-chip ASIC nodes are interconnected by multiple independent networks that optimally maximizes packet communications throughput and minimizes\u00a0\u2026", "num_citations": "301\n", "authors": ["646"]}
{"title": "Massively parallel supercomputer\n", "abstract": " A novel massively parallel supercomputer of hundreds of teraOPS-scale includes node architectures based upon System-On-a-Chip technology, ie, each processing node comprises a single Application Specific Integrated Circuit (ASIC). Within each ASIC node is a plurality of processing elements each of which consists of a central processing unit (CPU) and plurality of floating point processors to enable optimal balance of computational performance, packaging density, low cost, and power and cooling requirements. The plurality of processors within a single node individually or simultaneously work on any combination of computation or communication as required by the particular algorithm being solved. The system-on-a-chip ASIC nodes are interconnected by multiple independent networks that optimally maximizes packet communications throughput and minimizes latency. The multiple networks include three\u00a0\u2026", "num_citations": "262\n", "authors": ["646"]}
{"title": "Ultrascalable petaflop parallel supercomputer\n", "abstract": " A massively parallel supercomputer of petaOPS-scale includes node architectures based upon System-On-a-Chip technology, where each processing node comprises a single Application Specific Integrated Circuit (ASIC) having up to four processing elements. The ASIC nodes are interconnected by multiple independent networks that optimally maximize the throughput of packet communications between nodes with minimal latency. The multiple networks may include three high-speed networks for parallel algorithm message passing including a Torus, collective network, and a Global Asynchronous network that provides global barrier and notification functions. These multiple independent networks may be collaboratively or independently utilized according to the needs or phases of an algorithm for optimizing algorithm processing performance. The use of a DMA engine is provided to facilitate message passing\u00a0\u2026", "num_citations": "239\n", "authors": ["646"]}
{"title": "Method and system for initiating and loading DMA controller registers by using user-level programs\n", "abstract": " In a computer system the typically high overhead requirement for CPU instructions to operate a conventional direct memory access (DMA) controller are reduced to two user-level memory references via User-level Direct Memory Access (UDMA). The UDMA apparatus is located between the CPU and a DMA Controller, whereby the UDMA is programmed to use existing virtual memory translation hardware of the associated computer system to perform permission checking and address translation without Kernel involvement, and otherwise use minimal Kernel involvement for other operations.", "num_citations": "209\n", "authors": ["646"]}
{"title": "Overview of the IBM Blue Gene/P project\n", "abstract": " On June 26, 2007, IBM announced the Blue Gene/P (TM) system as the leading offering in its massively parallel Blue Geneo supercomputer line, succeeding the Blue Gene/L (TM) system. The Blue Gene/P system is designed to scale to at least 262,144 quad-processor nodes, with a peak performance of 3.56 petaflops. More significantly, the Blue Gene/P system enables this unprecedented scaling via architectural and design choices that maximize performance per watt, performance per square foot, and mean time between failures. This paper describes our vision of this petascale system, that is, a system capable of delivering more than a quadrillion (10(15)) floating-point operations per second. We also provide an overview of the system architecture, packaging, system software, and initial benchmark results.", "num_citations": "202\n", "authors": ["646"]}
{"title": "Virtual-memory-mapped network interfaces\n", "abstract": " In today's multicomputers, software overhead dominates the message-passing latency cost. We designed two multicomputer network interfaces that significantly reduce this overhead. Both support virtual-memory-mapped communication, allowing user processes to communicate without expensive buffer management and without making system calls across the protection boundary separating user processes from the operating system kernel. Here we compare the two interfaces and discuss the performance trade-offs between them.< >", "num_citations": "158\n", "authors": ["646"]}
{"title": "Design and analysis of the BlueGene/L torus interconnection network\n", "abstract": " BlueGene/L (BG/L) is a 64K (65,536) node scientific and engineering supercomputer that IBM is developing with partial funding from the United States Department of Energy. This paper describes one of the primary BG/L interconnection networks, a three dimensional torus. We describe a parallel performance simulator that was used extensively to help architect and design the torus network and present sample simulator performance studies that contributed to design decisions. In addition to such studies, the simulator was also used during the logic verification phase of BG/L for performance verification, and its use there uncovered a bug in the VHDL implementation of one of the arbiters.", "num_citations": "112\n", "authors": ["646"]}
{"title": "Protected, user-level dma for the shrimp network interface\n", "abstract": " Traditional DMA requires the operating system to perform many tasks to initiate a transfer, with overhead on the order of hundreds or thousands of CPU instructions. This paper describes a mechanism, called User-level Direct Memory Access (UDMA), for initiating DMA transfers of input/output data, with full protection, at a cost of only two user-level memory references. The UDMA mechanism uses existing virtual memory translation hardware to perform permission checking and address translation without kernel involvement. The implementation of the UDMA mechanism is simple, requiring a small extension to the traditional DMA controller and minimal operating system kernel support. The mechanism can be used with a wide variety of I/O devices including network interfaces, data storage devices such as disks and tape drives, and memory-mapped devices such as graphics frame-buffers. As an illustration, we\u00a0\u2026", "num_citations": "106\n", "authors": ["646"]}
{"title": "Early experience with message-passing on the shrimp multicomputer\n", "abstract": " The SHRIMP multicomputer provides virtual memory-mapped communication (VMMC), which supports protected, user-level message passing, allows user programs to perform their own buffer management, and separates data transfers from control transfers so that a data transfer can be done without the intervention of the receiving node CPU. An important question is whether such a mechanism can indeed deliver all of the available hardware performance to applications which use conventional message-passing libraries.This paper reports our early experience with message-passing on a small, working SHRIMP multicomputer. We have implemented several user-level communication libraries on top of the VMMC mechanism, including the NX message-passing interface, Sun RPC, stream sockets, and specialized RPC. The first three are fully compatible with existing systems. Our experience shows that the VMMC\u00a0\u2026", "num_citations": "105\n", "authors": ["646"]}
{"title": "Method and system for dynamically partitioning a shared cache\n", "abstract": " A cache memory shared among a plurality of separate, disjoint entities each having a disjoint address space, includes a cache segregator for dynamically segregating a storage space allocated to each entity of the entities such that no interference occurs with respective ones of the entities. A multiprocessor system including the cache memory, a method and a signal bearing medium for storing a program embodying the method also are provided.", "num_citations": "86\n", "authors": ["646"]}
{"title": "Design choices in the SHRIMP system: An empirical study\n", "abstract": " The SHRIMP cluster-computing system has progressed to a point of relative maturity; a variety of applications are running on a 16-node system. We have enough experience to understand what we did right and wrong in designing and building the system. In this paper we discuss some of the lessons we learned about computer architecture, and about the challenges involved in building a significant working system in an academic research environment. We evaluate significant design choices by modifying the network interface firmware and the system software in order to empirically compare our design to other approaches.", "num_citations": "66\n", "authors": ["646"]}
{"title": "Global tree network for computing structures enabling global processing operations\n", "abstract": " A system and method for enabling high-speed, low-latency global tree network communications among processing nodes interconnected according to a tree network structure. The global tree network enables collective reduction operations to be performed during parallel algorithm operations executing in a computer structure having a plurality of the interconnected processing nodes. Router devices are included that interconnect the nodes of the tree via links to facilitate performance of low-latency global processing operations at nodes of the virtual tree and sub-tree structures. The global operations performed include one or more of: broadcast operations downstream from a root node to leaf nodes of a virtual tree, reduction operations upstream from leaf nodes to the root node in the virtual tree, and point-to-point message passing from any node to the root node. The global tree network is configurable to provide\u00a0\u2026", "num_citations": "65\n", "authors": ["646"]}
{"title": "Collective network for computer structures\n", "abstract": " A system and method for enabling high-speed, low-latency global collective communications among interconnected processing nodes. The global collective network optimally enables collective reduction operations to be performed during parallel algorithm operations executing in a computer structure having a plurality of the interconnected processing nodes. Router devices ate included that interconnect the nodes of the network via links to facilitate performance of low-latency global processing operations at nodes of the virtual network and class structures. The global collective network may be configured to provide global barrier and interrupt functionality in asynchronous or synchronized manner. When implemented in a massively-parallel supercomputing structure, the global collective network is physically and logically partitionable according to needs of a processing algorithm.", "num_citations": "64\n", "authors": ["646"]}
{"title": "Design and implementation of the Blue Gene/P snoop filter\n", "abstract": " As multi-core processors evolve, coherence traffic between cores is becoming problematic, both in terms of performance and power. The negative effects of coherence (snoop) traffic can be significantly mitigated through snoop filtering. Shielding each cache with a device that can squash snoop requests for addresses known not to be in cache improves performance significantly for caches that cannot perform normal load and snoop lookups simultaneously. In addition, reducing snoop lookups yields power savings. This paper describes the design of the Blue Gene/P snoop filters, and presents hardware measurements to demonstrate their effectiveness. The Blue Gene/P snoop filters combine stream registers and snoop caches to capture both the locality of snoop addresses and their streaming behavior. Simulations of SPLASH-2 benchmarks illustrate tradeoffs and strengths of these two techniques. Their\u00a0\u2026", "num_citations": "60\n", "authors": ["646"]}
{"title": "Arithmetic functions in torus and tree networks\n", "abstract": " Methods and systems for performing arithmetic functions. In accordance with a first aspect of the invention, methods and apparatus are provided, working in conjunction of software algorithms and hardware implementation of class network routing, to achieve a very significant reduction in the time required for global arithmetic operation on the torus. Therefore, it leads to greater scalability of applications running on large parallel machines. The invention involves three steps in improving the efficiency and accuracy of global operations:(1) Ensuring, when necessary, that all the nodes do the global operation on the data in the same order and so obtain a unique answer, independent of roundoff error;(2) Using the topology of the torus to minimize the number of hops and the bidirectional capabilities of the network to reduce the number of time steps in the data transfer operation to an absolute minimum; and (3) Using\u00a0\u2026", "num_citations": "57\n", "authors": ["646"]}
{"title": "Local rollback for fault-tolerance in parallel computing systems\n", "abstract": " A control logic device performs a local rollback in a parallel super computing system. The super computing system includes at least one cache memory device. The control logic device determines a local rollback interval. The control logic device runs at least one instruction in the local rollback interval. The control logic device evaluates whether an unrecoverable condition occurs while running the at least one instruction during the local rollback interval. The control logic device checks whether an error occurs during the local rollback. The control logic device restarts the local rollback interval if the error occurs and the unrecoverable condition does not occur during the local rollback interval.", "num_citations": "42\n", "authors": ["646"]}
{"title": "Multi-petascale highly efficient parallel supercomputer\n", "abstract": " A Multi-Petascale Highly Efficient Parallel Supercomputer of 100 petaflop-scale includes node architectures based upon System-On-a-Chip technology, where each processing node comprises a single Application Specific Integrated Circuit (ASIC). The ASIC nodes are interconnected by a five dimensional torus network that optimally maximize the throughput of packet communications between nodes and minimize latency. The network implements collective network and a global asynchronous network that provides global barrier and notification functions. Integrated in the node design include a list-based prefetcher. The memory system implements transaction memory, thread level speculation, and multiversioning cache that improves soft error rate at the same time and supports DMA functionality allowing for parallel processing message-passing.", "num_citations": "41\n", "authors": ["646"]}
{"title": "Optimized scalable network switch\n", "abstract": " In a massively parallel computing system having a plurality of nodes configured in m multi-dimensions, each node including a computing device, a method for routing packets towards their destination nodes is provided which includes generating at least one of a 2m plurality of compact bit vectors containing information derived from downstream nodes. A multilevel arbitration process in which downstream information stored in the compact vectors, such as link status information and fullness of downstream buffers, is used to determine a preferred direction and virtual channel for packet transmission. Preferred direction ranges are encoded and virtual channels are selected by examining the plurality of compact bit vectors. This dynamic routing method eliminates the necessity of routing tables, thus enhancing scalability of the switch.", "num_citations": "41\n", "authors": ["646"]}
{"title": "Snoop filter for filtering snoop requests\n", "abstract": " A method and apparatus for supporting cache coherency in a multiprocessor computing environment having multiple processing units, each processing unit having one or more local cache memories associated and operatively connected therewith. The method comprises providing a snoop filter device associated with each processing unit, each snoop filter device having a plurality of dedicated input ports for receiving snoop requests from dedicated memory writing sources in the multiprocessor computing environment. Each snoop filter device includes a plurality of parallel operating port snoop filters in correspondence with the plurality of dedicated input ports, each port snoop filter implementing one or more parallel operating sub-filter elements that are adapted to concurrently filter snoop requests received from respective dedicated memory writing sources and forward a subset of those requests to its associated\u00a0\u2026", "num_citations": "40\n", "authors": ["646"]}
{"title": "Cellular supercomputing with system-on-a-chip\n", "abstract": " System-on-a-chip technology allows a level of integration that can be leveraged to develop inexpensive high-performance, low-power computing nodes. When used in aggregate, this approach promises to challenge conventional supercomputer architectures in the high-performance computing arena. Systems under consideration reach into the hundreds of thousand nodes per machine. Architecture for these systems are described.", "num_citations": "40\n", "authors": ["646"]}
{"title": "Class network routing\n", "abstract": " Class network routing is implemented in a network such as a computer network comprising a plurality of parallel compute processors at nodes thereof. Class network routing allows a compute processor to broadcast a message to a range (one or more) of other compute processors in the computer network, such as processors in a column or a row. Normally this type of operation requires a separate message to be sent to each processor. With class network routing pursuant to the invention, a single message is sufficient, which generally reduces the total number of messages in the network as well as the latency to do a broadcast. Class network routing is also applied to dense matrix inversion algorithms on distributed memory parallel supercomputers with hardware class function (multicast) capability. This is achieved by exploiting the fact that the communication patterns of dense matrix inversion can be served by\u00a0\u2026", "num_citations": "39\n", "authors": ["646"]}
{"title": "Low latency memory access and synchronization\n", "abstract": " A low latency memory system access is provided in association with a weakly-ordered multiprocessor system. Bach processor in the multiprocessor shares resources, and each shared resource has an associated lock within a locking device that provides support for synchronization between the multiple processors in the multiprocessor and the orderly sharing of the resources. A processor only has permission to access a resource when it owns the lock associated with that resource, and an attempt by a processor to own a lock requires only a single load operation, rather than a traditional atomic load followed by store, such that the processor only performs a read operation and the hardware locking device performs a subsequent write operation rather than the processor. A simple prefetching for non-contiguous data structures is also disclosed. A memory line is redefined so that in addition to the normal physical\u00a0\u2026", "num_citations": "37\n", "authors": ["646"]}
{"title": "Power and performance optimization at the system level\n", "abstract": " The BlueGene/L supercomputer has been designed with a focus on power/performance efficiency to achieve high application performance under the thermal constraints of common data centers. To achieve this goal, emphasis was put on system solutions to engineer a power-efficient system. To exploit thread level parallelism, the BlueGene/L system can scale to 64 racks with a total of 65536 computer nodes consisting of a single compute ASIC integrating all system functions with two industry-standard PowerPC microprocessor cores in a chip multiprocessor configuration. Each PowerPC processor exploits data-level parallelism with a high-performance SIMD oating point unitTo support good application scaling on such a massive system, special emphasis was put on efficient communication primitives by including five highly optimized communification networks. After an initial introduction of the Blue-Gene/L system\u00a0\u2026", "num_citations": "36\n", "authors": ["646"]}
{"title": "The bluegene/l supercomputer and quantum chromodynamics\n", "abstract": " We describe our methods for performing quantum chromodynamics (QCD) simulations that sustain up to 20% of the peak performance on BlueGene supercomputers. We present our methods, scaling properties, and first cutting edge results relevant to QCD. We show how this enables unprecedented computational scale that brings lattice QCD to the next generation of calculations. We present our QCD simulation that achieved 12.2 Teraflops sustained performance with perfect speedup to 32K CPU cores. Among other things, these calculations are critical for cosmology, for the heavy ion experiments at RHIC-BNL, and for the upcoming experiments at CERN-Geneva. Furthermore, we demonstrate how QCD dramatically exposes memory and network latencies inherent in any computer system and propose that QCD should be used as a new, powerful HPC benchmark. Our sustained performance demonstrates the\u00a0\u2026", "num_citations": "34\n", "authors": ["646"]}
{"title": "Multiple node remote messaging\n", "abstract": " A method for passing remote messages in a parallel computer system formed as a network of interconnected compute nodes includes that a first compute node (A) sends a single remote message to a remote second compute node (B) in order to control the remote second compute node (B) to send at least one remote message. The method includes various steps including controlling a DMA engine at first compute node (A) to prepare the single remote message to include a first message descriptor and at least one remote message descriptor for controlling the remote second compute node (B) to send at least one remote message, including putting the first message descriptor into an injection FIFO at the first compute node (A) and sending the single remote message and the at least one remote message descriptor to the second compute node (B).", "num_citations": "30\n", "authors": ["646"]}
{"title": "Snoop filtering system in a multiprocessor system\n", "abstract": " A system and method for supporting cache coherency in a computing environment having multiple processing units, each unit having an associated cache memory system operatively coupled therewith. The system includes a plurality of interconnected snoop filter units, each snoop filter unit corresponding to and in communication with a respective processing unit, with each snoop filter unit comprising a plurality of devices for receiving asynchronous snoop requests from respective memory writing sources in the computing environment; and a point-to-point interconnect comprising communication links for directly connecting memory writing sources to corresponding receiving devices; and, a plurality of parallel operating filter devices coupled in one-to-one correspondence with each receiving device for processing snoop requests received thereat and one of forwarding requests or preventing forwarding of requests to\u00a0\u2026", "num_citations": "29\n", "authors": ["646"]}
{"title": "Multidimensional switch network\n", "abstract": " Multidimensional switch data networks are disclosed, such as are used by a distributed-memory parallel computer, as applied for example to computations in the field of life sciences. A distributed memory parallel computing system comprises a number of parallel compute nodes and a message passing data network connecting the compute nodes together. The data network connecting the compute nodes comprises a multidimensional switch data network of compute nodes having N dimensions, and a number/array of compute nodes Ln in each of the N dimensions. Each compute node includes an N port routing element having a port for each of the N dimensions. Each compute node of an array of Ln compute nodes in each of the N dimensions connects through a port of its routing element to an Ln port crossbar switch having Ln ports. Several embodiments are disclosed of a 4 dimensional computing system\u00a0\u2026", "num_citations": "27\n", "authors": ["646"]}
{"title": "Blue Gene/L advanced diagnostics environment\n", "abstract": " This paper describes the Blue Gene\u00ae/L advanced diagnostics environment (ADE) used throughout all aspects of the Blue Gene/L project, including design, logic verification, bring-up, diagnostics, and manufacturing test. The Blue Gene/L ADE consists of a lightweight multithreaded coherence-managed kernel, runtime libraries, device drivers, system programming interfaces, compilers, and host-based development tools. It provides complete and flexible access to all features of the Blue Gene/L hardware. Prior to the existence of hardware, ADE was used on Very high-speed integrated circuit Hardware Description Language (VHDL) models, not only for logic verification, but also for performance measurements, code-path analysis, and evaluation of architectural tradeoffs. During early hardware bring-up, the ability to run in a cycle-reproducible manner on both hardware and VHDL proved invaluable in fault isolation\u00a0\u2026", "num_citations": "27\n", "authors": ["646"]}
{"title": "Two virtual memory mapped network interface designs\n", "abstract": " In existing multicomputers, software overhead dom inates the message-passing latency cost. Our research on the SHRIMP project at Princeton indicates that appropriate network interface support can significantly reduce this software overhead. We have designed two network interfaces for the SHRIMP multicomputer. Both support virtual memory mapped communication allowing user processes to communicate without doing expensive buffer management, and without using sys tem calls to cross the protection boundary separating user processes from the operating system kernel. This paper describes and compares the two network in terfaces, and discusses performance tradeoffs between them.", "num_citations": "27\n", "authors": ["646"]}
{"title": "Method for prefetching non-contiguous data structures\n", "abstract": " A low latency memory system access is provided in association with a weakly-ordered multiprocessor system. Each processor in the multiprocessor shares resources, and each shared resource has an associated lock within a locking device that provides support for synchronization between the multiple processors in the multiprocessor and the orderly sharing of the resources. A processor only has permission to access a resource when it owns the lock associated with that resource, and an attempt by a processor to own a lock requires only a single load operation, rather than a traditional atomic load followed by store, such that the processor only performs a read operation and the hardware locking device performs a subsequent write operation rather than the processor. A simple perfecting for non-contiguous data structures is also disclosed. A memory line is redefined so that in addition to the normal physical\u00a0\u2026", "num_citations": "26\n", "authors": ["646"]}
{"title": "Method and apparatus for filtering snoop requests using stream registers\n", "abstract": " A method and apparatus for supporting cache coherency in a multiprocessor computing environment having multiple processing units, each processing unit having a local cache memory associated therewith. A snoop filter device is associated with each processing unit and includes at least one snoop filter primitive implementing filtering method based on usage of stream registers sets and associated stream register comparison logic. From the plurality of stream registers sets, at least one stream register set is active, and at least one stream register set is labeled historic at any point in time. In addition, the snoop filter block is operatively coupled with cache wrap detection logic whereby the content of the active stream register set is switched into a historic stream register set upon the cache wrap condition detection, and the content of at least one active stream register set is reset. Each filter primitive implements stream\u00a0\u2026", "num_citations": "26\n", "authors": ["646"]}
{"title": "Global interrupt and barrier networks\n", "abstract": " A system and method for generating global asynchronous signals in a computing structure. Particularly, a global interrupt and barrier network is implemented that implements logic for generating global interrupt and barrier signals for controlling global asynchronous operations performed by processing elements at selected processing nodes of a computing structure in accordance with a processing algorithm; and includes the physical interconnecting of the processing nodes for communicating the global interrupt and barrier signals to the elements via low-latency paths. The global asynchronous signals respectively initiate interrupt and barrier operations at the processing nodes at times selected for optimizing performance of the processing algorithms. In one embodiment, the global interrupt and barrier network is implemented in a scalable, massively parallel supercomputing device structure comprising a plurality\u00a0\u2026", "num_citations": "25\n", "authors": ["646"]}
{"title": "Deterministic error recovery protocol\n", "abstract": " Disclosed are an error recovery method and system for use with a communication system having first and second nodes, each of said nodes having a receiver and a sender, the sender of the first node being connected to the receiver of the second node by a first cable, and the sender of the second node being connected to the receiver of the first node by a second cable. The method comprising the step of after one of the nodes detects an error, both of the nodes entering the same defined state. In particular, the receiver of the first node enters an error state, stays in the error state for a defined period of time T, and, after said defined period of time T, enters a wait state. Also, the sender of the first node sends to the receiver of the second node an error message for a defined period of time Te, and after the defined period of time Te, the sender of the first node enters an idle state.", "num_citations": "23\n", "authors": ["646"]}
{"title": "Network interface for protected, user-level communication\n", "abstract": " Scalable parallel architectures are converging on the general framework of a distributed memory parallel processor (DMPP), consisting of a group of interconnected compute nodes, each typically consisting of a CPU, some memory, and a network interface. DMPP architectures range from tightly-coupled multicomputers to loosely-coupled networks of personal computers (PCs), and support a number of parallel programming paradigms including shared-memory and message-passing. Networks of PCs are very attractive DMPPs because of the high performance and low cost of the PC nodes.", "num_citations": "23\n", "authors": ["646"]}
{"title": "System and method for programmable bank selection for banked memory subsystems\n", "abstract": " A programmable memory system and method for enabling one or more processor devices access to shared memory in a computing environment, the shared memory including one or more memory storage structures having addressable locations for storing data. The system comprises: one or more first logic devices associated with a respective one or more processor devices, each first logic device for receiving physical memory address signals and programmable for generating a respective memory storage structure select signal upon receipt of pre-determined address bit values at selected physical memory address bit locations; and, a second logic device responsive to each of the respective select signal for generating an address signal used for selecting a memory storage structure for processor access. The system thus enables each processor device of a computing environment memory storage access\u00a0\u2026", "num_citations": "22\n", "authors": ["646"]}
{"title": "Massively parallel supercomputer\n", "abstract": " A novel massively parallel supercomputer of hundreds of teraOPS-scale includes node architectures based upon System-On-a-Chip technology, ie, each processing node comprises a single Application Specific Integrated Circuit (ASIC). Within each ASIC node is a plurality of processing elements each of which consists of a central processing unit (CPU) and plurality of floating point processors to enable optimal balance of computational performance, packaging density, low cost, and power and cooling requirements. The plurality of processors within a single node individually or simultaneously work on any combination of computation or communication as required by the particular algorithm being solved. The system-on-a-chip ASIC nodes are interconnected by multiple independent networks that optimally maximizes packet communications throughput and minimizes latency. The multiple networks include three\u00a0\u2026", "num_citations": "21\n", "authors": ["646"]}
{"title": "Method and apparatus for filtering snoop requests in a point-to-point interconnect architecture\n", "abstract": " A method and apparatus for supporting cache coherency in a multiprocessor computing environment having multiple processing units, each processing unit having one or more local cache memories associated and operatively connected therewith. The method comprises providing a snoop filter device associated with each processing unit, each snoop filter device having a plurality of dedicated input ports for receiving snoop requests from dedicated memory writing sources in the multiprocessor computing environment. Each of the memory writing sources is directly connected to the dedicated input ports of all other snoop filter devices associated with all other processing units in a point-to-point interconnect fashion. Each snoop filter device includes a plurality of parallel operating port snoop filters in correspondence with the plurality of dedicated input ports that are adapted to concurrently filter snoop requests\u00a0\u2026", "num_citations": "21\n", "authors": ["646"]}
{"title": "Improving the accuracy of snoop filtering using stream registers\n", "abstract": " Multi-core processors have become mainstream; they provide parallelism with relatively low complexity. As true on-chip SMPs evolve, coherence traffic between cores is becoming problematic, both in terms of performance and power. The negative effects of coherence (snoop) traffic can be significantly mitigated through snoop filtering. Shielding each cache with a device that can squash snoop requests for addresses known not to be in cache improves performance significantly for caches that cannot perform normal load and snoop lookups simultaneously. In addition, reducing snoop lookups yields power savings.", "num_citations": "21\n", "authors": ["646"]}
{"title": "Non-volatile memory for checkpoint storage\n", "abstract": " A system, method and computer program product for supporting system initiated checkpoints in high performance parallel computing systems and storing of checkpoint data to a non-volatile memory storage device. The system and method generates selective control signals to perform checkpointing of system related data in presence of messaging activity associated with a user application running at the node. The checkpointing is initiated by the system such that checkpoint data of a plurality of network nodes may be obtained even in the presence of user applications running on highly parallel computers that include ongoing user messaging activity. In one embodiment, the non-volatile memory is a pluggable flash memory card.", "num_citations": "20\n", "authors": ["646"]}
{"title": "Managing coherence via put/get windows\n", "abstract": " A method and apparatus for managing coherence between two processors of a two processor node of a multi-processor computer system. Generally the present invention relates to a software algorithm that simplifies and significantly speeds the management of cache coherence in a message passing parallel computer, and to hardware apparatus that assists this cache coherence algorithm. The software algorithm uses the opening and closing of put/get windows to coordinate the activated required to achieve cache coherence. The hardware apparatus may be an extension to the hardware address decode, that creates, in the physical memory address space of the node, an area of virtual memory that (a) does not actually exist, and (b) is therefore able to respond instantly to read and write requests from the processing elements.", "num_citations": "19\n", "authors": ["646"]}
{"title": "Method and apparatus for single-stepping coherence events in a multiprocessor system under software control\n", "abstract": " An apparatus and method are disclosed for single-stepping coherence events in a multiprocessor system under software control in order to monitor the behavior of a memory coherence mechanism. Single-stepping coherence events in a multiprocessor system is made possible by adding one or more step registers. By accessing these step registers, one or more coherence requests are processed by the multiprocessor system. The step registers determine if the snoop unit will operate by proceeding in a normal execution mode, or operate in a single-step mode.", "num_citations": "19\n", "authors": ["646"]}
{"title": "Shared virtual memory with automatic update support\n", "abstract": " Shared virtual memory systems provide the abstraction of a shared address space on top of a messagepassing communication architecture. The overall performance of an SVM system therefore depends on both the raw performance of the underlying communication mechanism and the efficiency with which the SVM protocol uses that mechanism. The Automatic Update Release Consistency (AURC) protocol was proposed to take advantage of simple memory-mapped communication and automatic update support to accelerate a shared virtual memory protocol. However, there has not yet been a real system on which an implementation of this protocol could be evaluated.This paper reports our evaluation of AURC on the SHRIMP multicomputer, the only hardware platform that supports an automatic update mechanism. Automatic update propagates local memory writes to remote memory locations automatically. We\u00a0\u2026", "num_citations": "19\n", "authors": ["646"]}
{"title": "Cache as point of coherence in multiprocessor system\n", "abstract": " In a multiprocessor system, a conflict checking mechanism is implemented in the L2 cache memory. Different versions of speculative writes are maintained in different ways of the cache. A record of speculative writes is maintained in the cache directory. Conflict checking occurs as part of directory lookup. Speculative versions that do not conflict are aggregated into an aggregated version in a different way of the cache. Speculative memory access requests do not go to main memory.", "num_citations": "17\n", "authors": ["646"]}
{"title": "Conditional load and store in a shared memory\n", "abstract": " A method, system and computer program product for implementing load-reserve and store-conditional instructions in a multi-processor computing system. The computing system includes a multitude of processor units and a shared memory cache, and each of the processor units has access to the memory cache. In one embodiment, the method comprises providing the memory cache with a series of reservation registers, and storing in these registers addresses reserved in the memory cache for the processor units as a result of issuing load-reserve requests. In this embodiment, when one of the processor units makes a request to store data in the memory cache using a store-conditional request, the reservation registers are checked to determine if an address in the memory cache is reserved for that processor unit. If an address in the memory cache is reserved for that processor, the data are stored at this address.", "num_citations": "17\n", "authors": ["646"]}
{"title": "Collective network for computer structures\n", "abstract": " A system and method for enabling high-speed, low-latency global collective communications among interconnected processing nodes. The global collective network optimally enables collective reduction operations to be performed during parallel algorithm operations executing in a computer structure having a plurality of the interconnected processing nodes. Router devices are included that interconnect the nodes of the network via links to facilitate performance of low-latency global processing operations at nodes of the virtual network. The global collective network may be configured to provide global barrier and interrupt functionality in asynchronous or synchronized manner. When implemented in a massively-parallel supercomputing structure, the global collective network is physically and logically partitionable according to needs of a processing algorithm.", "num_citations": "17\n", "authors": ["646"]}
{"title": "Programmable partitioning for high-performance coherence domains in a multiprocessor system\n", "abstract": " A multiprocessor computing system and a method of logically partitioning a multiprocessor computing system are disclosed. The multiprocessor computing system comprises a multitude of processing units, and a multitude of snoop units. Each of the processing units includes a local cache, and the snoop units are provided for supporting cache coherency in the multiprocessor system. Each of the snoop units is connected to a respective one of the processing units and to all of the other snoop units. The multiprocessor computing system further includes a partitioning system for using the snoop units to partition the multitude of processing units into a plurality of independent, memory-consistent, adjustable-size processing groups. Preferably, when the processor units are partitioned into these processing groups, the partitioning system also configures the snoop units to maintain cache coherency within each of said groups.", "num_citations": "15\n", "authors": ["646"]}
{"title": "One-bounce network\n", "abstract": " A one-bounce data network comprises a plurality of nodes interconnected to each other via communication links, the network including a plurality of interconnected switch devices, said switch devices interconnected such that a message is communicated between any two switches passes over a single link from a source switch to a destination switch; and, the source switch concurrently sends a message to an arbitrary bounce switch which then sends the message to the destination switch.", "num_citations": "15\n", "authors": ["646"]}
{"title": "Method and apparatus for filtering snoop requests using multiple snoop caches\n", "abstract": " A method and apparatus for implementing a snoop filter unit associated with a single processor in a multiprocessor system. The snoop filter unit has a plurality of ports, each port receiving snoop requests from exactly one dedicated source. Associated with each port is a snoop cache filter that processes each snoop cache request and records addresses of the most recent snoop requests for exactly one source. The snoop cache filter uses vector encoding to record the occurrence of snoop requests for a sequence of consecutive cache lines. All addresses of snoop requests are added to the snoop cache unless a received snoop cache request matches an entry present in the associated snoop cache, in which case the snoop request is discarded. Otherwise, the associated snoop cache request is enqueued for forwarding to the single processor. Information from all snoop cache filters assigned to all ports in the snoop\u00a0\u2026", "num_citations": "15\n", "authors": ["646"]}
{"title": "Methods and apparatus using commutative error detection values for fault isolation in multiple node computers\n", "abstract": " Methods and apparatus perform fault isolation in multiple node computing systems using commutative error detection values for\u2014example, checksums\u2014to identify and to isolate faulty nodes. When information associated with a reproducible portion of a computer program is injected into a network by a node, a commutative error detection value is calculated. At intervals, node fault detection apparatus associated with the multiple node computer system retrieve commutative error detection values associated with the node and stores them in memory. When the computer program is executed again by the multiple node computer system, new commutative error detection values are created and stored in memory. The node fault detection apparatus identifies faulty nodes by comparing commutative error detection values associated with reproducible portions of the application program generated by a particular node from\u00a0\u2026", "num_citations": "15\n", "authors": ["646"]}
{"title": "Massively parallel quantum chromodynamics\n", "abstract": " Quantum chromodynamics (QCD), the theory of the strong nuclear force, can be numerically simulated on massively parallel supercomputers using the method of lattice gauge theory. We describe the special programming requirements of lattice QCD (LQCD) as well as the optimal supercomputer hardware architectures for which LQCD suggests a need. We demonstrate these methods on the IBM Blue Gene/L\u2122 (BG/L) massively parallel supercomputer and argue that the BG/L architecture is very well suited for LQCD studies. This suitability arises from the fact that LQCD is a regular lattice discretization of space into lattice sites, while the BG/L supercomputer is a discretization of space into compute nodes. Both LQCD and the BG/L architecture are constrained by the requirement of short-distance exchanges. This simple relation is technologically important and theoretically intriguing. We demonstrate a computational\u00a0\u2026", "num_citations": "14\n", "authors": ["646"]}
{"title": "Verification strategy for the Blue Gene/L chip\n", "abstract": " The Blue Gene\u00ae/L compute chip contains two PowerPC\u00ae 440 processor cores, private L2 prefetch caches, a shared L3 cache and double-data-rate synchronous dynamic random access memory (DDR SDRAM) memory controller, a collective network interface, a torus network interface, a physical network interface, an interrupt controller, and a bridge interface to slower devices. System-on-a-chip verification problems require a multilevel verification strategy in which the strengths of each layer offset the weaknesses of another layer. The verification strategy we adopted relies on the combined strengths of random simulation, directed simulation, and code-driven simulation at the unit and system levels. The strengths and weaknesses of the various techniques and our reasons for choosing them are discussed. The verification platform is based on event simulation and cycle simulation running on a farm of Intel\u00a0\u2026", "num_citations": "12\n", "authors": ["646"]}
{"title": "Insertion of coherence requests for debugging a multiprocessor\n", "abstract": " A method and system are disclosed to insert coherence events in a multiprocessor computer system, and to present those coherence events to the processors of the multiprocessor computer system for analysis and debugging purposes. The coherence events are inserted in the computer system by adding one or more special insert registers. By writing into the insert registers, coherence events are inserted in the multiprocessor system as if they were generated by the normal coherence protocol. Once these coherence events are processed, the processing of coherence events can continue in the normal operation mode.", "num_citations": "11\n", "authors": ["646"]}
{"title": "Method and apparatus for filtering snoop requests using a scoreboard\n", "abstract": " An apparatus for implementing snooping cache coherence that locally reduces the number of snoop requests presented to each cache in a multiprocessor system. A snoop filter device associated with a single processor includes one or more \u201cscoreboard\u201d data structures that make snoop determinations, ie, for each snoop request from another processor, to determine if a request is to be forwarded to the processor or, discarded. At least one scoreboard is active, and at least one scoreboard is determined to be historic at any point in time. A snoop determination of the queue indicates that an entry may be in the cache, but does not indicate its actual residence status. In addition, the snoop filter block implementing scoreboard data structures is operatively coupled with a cache wrap detection logic means whereby, upon detection of a cache wrap condition, the content of the active scoreboard is copied into a historic\u00a0\u2026", "num_citations": "10\n", "authors": ["646"]}
{"title": "Exploring the architecture of a stream register-based snoop filter\n", "abstract": " Multi-core processors have become mainstream; they provide parallelism with relatively low complexity. As true on-chip symmetric multiprocessors evolve, coherence traffic between cores is becoming problematic, both in terms of performance and power. The negative effects of coherence (snoop) traffic can be significantly mitigated through the use of snoop filtering. The idea is to shield each cache with a device that can eliminate snoop requests for addresses that are known not to be in the cache. This improves performance significantly for caches that cannot perform normal load and snoop lookups simultaneously. In addition, the reduction of snoop lookups yields power savings. This paper describes Stream Register snoop filtering, which captures the spatial locality of multiple memory reference streams in a small number of registers. We propose a snoop filter that combines Stream Registers with \u201dsnoop\u00a0\u2026", "num_citations": "9\n", "authors": ["646"]}
{"title": "Design of the IBM Blue Gene/Q compute chip\n", "abstract": " The heart of a Blue Gene (R)/Q system is the Blue Gene/Q Compute (BQC) chip, which combines processors, memory, and communication functions on a single chip. The Blue Gene/Q Compute chip has 16+ 1+ 1 processor cores, each with a quad single-instruction, multiple-data (SIMD) floating-point unit, and a multi-versioned Level 2 cache that provides hardware support for transactional memory, speculative execution, and atomic operations. The Blue Gene/Q Compute chip further contains dual on-chip memory controllers for directly attached DDR3 (double data rate type 3) memory and sophisticated networking logic for chip-to-chip communications.", "num_citations": "8\n", "authors": ["646"]}
{"title": "Simplifying and speeding the management of intra-node cache coherence\n", "abstract": " A method and apparatus for managing coherence between two processors of a two processor node of a multi-processor computer system. Generally the present invention relates to a software algorithm that simplifies and significantly speeds the management of cache coherence in a message passing parallel computer, and to hardware apparatus that assists this cache coherence algorithm. The software algorithm uses the opening and closing of put/get windows to coordinate the activated required to achieve cache coherence. The hardware apparatus may be an extension to the hardware address decode, that creates, in the physical memory address space of the node, an area of virtual memory that (a) does not actually exist, and (b) is therefore able to respond instantly to read and write requests from the processing elements.", "num_citations": "5\n", "authors": ["646"]}
{"title": "Atomicity: a multi-pronged approach\n", "abstract": " In a multiprocessor system with speculative execution, atomicity can be approached in several fashions. One approach is to have atomic instructions that achieve multiple functions and are guaranteed to complete. Another approach is to have blocks of code that are grouped to succeed or fail together. A system can incorporate more than one such approach. In implementing more than one approach, the system may prioritize one over another. When conflict detection is done through a directory lookup in cache memory, atomic instructions and atomicity related operations may be implemented in a cache data array access pipeline in that cache memory. This implementation may include feedback to the pipeline for implementing multiple functions within an atomic instruction and also for cascading atomic instructions.", "num_citations": "5\n", "authors": ["646"]}
{"title": "Method and apparatus for granting processors access to a resource\n", "abstract": " An apparatus and method for granting one or more requesting entities access to a resource in a predetermined time interval. The apparatus includes a first circuit receiving one or more request signals, and implementing logic for assigning a priority to the one or more request signals, and, generating a set of first_request signals based on the priorities assigned. One or more priority select circuits for receiving the set of first_request signals and generating corresponding one or more fixed grant signals representing one or more highest priority request signals when asserted during the predetermined time interval. A second circuit device receives the one or more fixed grant signals generates one or more grant signals associated with one or more highest priority request signals assigned, the grant signals for enabling one or more respective requesting entities access to the resource in the predetermined time interval\u00a0\u2026", "num_citations": "5\n", "authors": ["646"]}
{"title": "Blue gene/l, a system-on-a-chip\n", "abstract": " Summary form only given. Large powerful networks coupled to state-of-the-art processors have traditionally dominated supercomputing. As technology advances, this approach is likely to be challenged by a more cost-effective System-On-A-Chip approach, with higher levels of system integration. The scalability of applications to architectures with tens to hundreds of thousands of processors is critical to the success of this approach. Significant progress has been made in mapping numerous compute-intensive applications, many of them grand challenges, to parallel architectures. Applications hoping to efficiently execute on future supercomputers of any architecture must be coded in a manner consistent with an enormous degree of parallelism. The BG/L program is developing a peak nominal 180 TFLOPS (360 TFLOPS for some applications) supercomputer to serve a broad range of science applications. BG/L\u00a0\u2026", "num_citations": "5\n", "authors": ["646"]}
{"title": "Exploiting idle resources in a high-radix switch for supplemental storage\n", "abstract": " A general-purpose switch for a high-performance network is usually designed with symmetric ports providing credit-based flow control and error recovery via link-level re-transmission. Because port buffers must be sized for the longest links and modern asymmetric network topologies have a wide range of link lengths, we observe that there can be a significant amount of unused buffer memory, particularly in edge switches. We also observe that the tiled architecture used in many high-radix switches contains an abundance of internal bandwidth. We combine these observations to create a new switch architecture that allows ports to stash packets in unused buffers on other ports, accessible via excess internal bandwidth in the tiled switch. We explore this architecture through two use cases: end-to-end resilience and congestion mitigation. We find that stashing is highly effective and does not degrade network\u00a0\u2026", "num_citations": "3\n", "authors": ["646"]}
{"title": "A Virtual Memory Mapped Network Interface for SHPIMP Multiprocessor\n", "abstract": " CiNii \u8ad6\u6587 - A Virtual Memory Mapped Network Interface for SHPIMP Multiprocessor CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 A Virtual Memory Mapped Network Interface for SHPIMP Multiprocessor BLUMRICH M. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BLUMRICH M. \u53ce\u9332\u520a\u884c\u7269 Proc. 21th Int. Symp. on Computer Architecture, April 1994 Proc. 21th Int. Symp. on Computer Architecture, April 1994, 1994 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u5171\u6709\u30e1\u30e2\u30eavs. \u30e1\u30c3\u30bb\u30fc\u30b8\u30d1\u30c3\u30b7\u30f3\u30b0 \u677e\u672c \u5c1a , \u5e73\u6728 \u656c \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a. ARC,\u8a08\u7b97\u6a5f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7814\u7a76\u4f1a\u5831\u544a 126, 85-90, 1997-10-28 \u53c2\u8003\u6587\u732e17\u4ef6 \u88ab\u5f15\u7528\u6587\u732e2\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10020639488 \u8cc7\u6599\u2026", "num_citations": "3\n", "authors": ["646"]}
{"title": "An empirical comparison of loop scheduling algorithms on a shared memory multiprocessor\n", "abstract": " This paper studies several methods of instruction level parallelization applied at the statement level on a shared memory multiprocessor, and reports the results of an empirical evaluation to determine which of the methods yields the best results. Using sequential code as a base case, we compared doacross, list scheduling, greedy software pipelining (a variant of perfect pipelining), and top down scheduling. The experiments were performed on loops both with and without loop carried dependencies. We nd that statement level parallelism does yield speedups on the shared memory multiprocessor. In addition, we observed an interesting superlinearity e ect for fully vectorized loops.", "num_citations": "3\n", "authors": ["646"]}
{"title": "Error recovery to enable error-free message transfer between nodes of a computer network\n", "abstract": " An error-recovery method to enable error-free message transfer between nodes of a computer network. A first node of the network sends a packet to a second node of the network over a link between the nodes, and the first node keeps a copy of the packet on a sending end of the link until the first node receives acknowledgment from the second node that the packet was received without error. The second node tests the packet to determine if the packet is error free. If the packet is not error free, the second node sets a flag to mark the packet as corrupt. The second node returns acknowledgement to the first node specifying whether the packet was received with or without error. When the packet is received with error, the link is returned to a known state and the packet is sent again to the second node.", "num_citations": "2\n", "authors": ["646"]}
{"title": "A holistic approach to system reliability in Blue Gene\n", "abstract": " Optimizing supercomputer performance requires a balance between objectives for processor performance, network performance, power delivery and cooling, cost and reliability. In particular, scaling a system to a large number of processors poses challenges for reliability, availability and serviceability. Given the power and thermal constraints of data centers, the BlueGene/L supercomputer has been designed with a focus on maximizing floating point operations per second per Watt (FLOPS/Watt). This results in a drastic reduction in FLOPS/m 2  floor space and FLOPS/dollar, allowing for affordable scale-up. The BlueGene/L system has been scaled to a total of 65,536 compute nodes in 64 racks. A system approach was used to minimize power at all levels, from the processor to the cooling plant. A BlueGene/L compute node consists of a single ASIC and associated memory. The ASIC integrates all system functions\u00a0\u2026", "num_citations": "2\n", "authors": ["646"]}
{"title": "The IBM blue gene project\n", "abstract": " This paper provides a short overview of the IBM Blue Gene (R) project and an introduction to all of the papers in this issue of the IBM Journal of Research and Development.", "num_citations": "1\n", "authors": ["646"]}