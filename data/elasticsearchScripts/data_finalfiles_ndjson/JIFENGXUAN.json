{"title": "Developer Prioritization in Bug Repositories\n", "abstract": " Developers build all the software artifacts in development. Existing work has studied the social behavior in software repositories. In one of the most important software repositories, a bug repository, developers create and update bug reports to support software development and maintenance. However, no prior work has considered the priorities of developers in bug repositories. In this paper, we address the problem of the developer prioritization, which aims to rank the contributions of developers. We mainly explore two aspects, namely modeling the developer prioritization in a bug repository and assisting predictive tasks with our model. First, we model how to assign the priorities of developers based on a social network technique. Three problems are investigated, including the developer rankings in products, the evolution over time, and the tolerance of noisy comments. Second, we consider leveraging the\u00a0\u2026", "num_citations": "203\n", "authors": ["206"]}
{"title": "Learning to combine multiple ranking metrics for fault localization\n", "abstract": " Fault localization is an inevitable step in software debugging. Spectrum-based fault localization consists in computing a ranking metric on execution traces to identify faulty source code. Existing empirical studies on fault localization show that there is no optimal ranking metric for all faults in practice. In this paper, we propose Multric, a learning-based approach to combining multiple ranking metrics for effective fault localization. In Multric, a suspiciousness score of a program entity is a combination of existing ranking metrics. Multric consists two major phases: learning and ranking. Based on training faults, Multric builds a ranking model by learning from pairs of faulty and non-faulty source code elements. When a new fault appears, Multric computes the final ranking with the learned model. Experiments are conducted on 5386 seeded faults in ten open-source Java programs. We empirically compare Multric against four\u00a0\u2026", "num_citations": "159\n", "authors": ["206"]}
{"title": "Test case purification for improving fault localization\n", "abstract": " Finding and fixing bugs are time-consuming activities in software development. Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases. Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis. In this paper, we propose a novel concept of spectrum driven test case purification for improving fault localization. The goal of test case purification is to separate existing test cases into small fractions (called purified test cases) and to enhance the test oracles to further localize faults. Combining with an original fault localization technique (eg, Tarantula), test case purification results in better ranking the program statements. Our experiments on 1800 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques.", "num_citations": "152\n", "authors": ["206"]}
{"title": "Automatic bug triage using semi-supervised text classification\n", "abstract": " In this paper, we propose a semi-supervised text classification approach for bug triage to avoid the deficiency of labeled bug reports in existing supervised approaches. This new approach combines naive Bayes classifier and expectation-maximization to take advantage of both labeled and unlabeled bug reports. This approach trains a classifier with a fraction of labeled bug reports. Then the approach iteratively labels numerous unlabeled bug reports and trains a new classifier with labels of all the bug reports. We also employ a weighted recommendation list to boost the performance by imposing the weights of multiple developers in training the classifier. Experimental results on bug reports of Eclipse show that our new approach outperforms existing supervised approaches in terms of classification accuracy.", "num_citations": "145\n", "authors": ["206"]}
{"title": "Automatic repair of buggy if conditions and missing preconditions with smt\n", "abstract": " We present Nopol, an approach for automatically repairing buggy if conditions and missing preconditions. As input, it takes a program and a test suite which contains passing test cases modeling the expected behavior of the program and at least one failing test case embodying the bug to be repaired. It consists of collecting data from multiple instrumented test suite executions, transforming this data into a Satisfiability Modulo Theory (SMT) problem, and translating the SMT result--if there exists one--into a source code patch. Nopol repairs object oriented code and allows the patches to contain nullness checks as well as specific method calls.", "num_citations": "139\n", "authors": ["206"]}
{"title": "Towards Effective Bug Triage with Software Data Reduction Techniques\n", "abstract": " Software companies spend over 45 percent of cost in dealing with software bugs. An inevitable step of fixing bugs is bug triage, which aims to correctly assign a developer to a new bug. To decrease the time cost in manual work, text classification techniques are applied to conduct automatic bug triage. In this paper, we address the problem of data reduction for bug triage, i.e., how to reduce the scale and improve the quality of bug data. We combine instance selection with feature selection to simultaneously reduce data scale on the bug dimension and the word dimension. To determine the order of applying instance selection and feature selection, we extract attributes from historical bug data sets and build a predictive model for a new bug data set. We empirically investigate the performance of data reduction on totally 600,000 bug reports of two large open source projects, namely Eclipse and Mozilla. The results\u00a0\u2026", "num_citations": "134\n", "authors": ["206"]}
{"title": "Solving the large scale next release problem with a backbone-based multilevel algorithm\n", "abstract": " The Next Release Problem (NRP) aims to optimize customer profits and requirements selection for the software releases. The research on the NRP is restricted by the growing scale of requirements. In this paper, we propose a Backbone-based Multilevel Algorithm (BMA) to address the large scale NRP. In contrast to direct solving approaches, the BMA employs multilevel reductions to downgrade the problem scale and multilevel refinements to construct the final optimal set of customers. In both reductions and refinements, the backbone is built to fix the common part of the optimal customers. Since it is intractable to extract the backbone in practice, the approximate backbone is employed for the instance reduction while the soft backbone is proposed to augment the backbone application. In the experiments, to cope with the lack of open large requirements databases, we propose a method to extract instances from\u00a0\u2026", "num_citations": "99\n", "authors": ["206"]}
{"title": "Send Hardest Problems My Way: Probabilistic Path Prioritization for Hybrid Fuzzing\n", "abstract": " Hybrid fuzzing which combines fuzzing and concolic execution has become an advanced technique for software vulnerability detection. Based on the observation that fuzzing and concolic execution are complementary in nature, the stateof-the-art hybrid fuzzing systems deploy \u201cdemand launch\u201d and \u201coptimal switch\u201d strategies. Although these ideas sound intriguing, we point out several fundamental limitations in them, due to oversimplified assumptions. We then propose a novel \u201cdiscriminative dispatch\u201d strategy to better utilize the capability of concolic execution. We design a novel Monte Carlo based probabilistic path prioritization model to quantify each path\u2019s difficulty and prioritize them for concolic execution. This model treats fuzzing as a random sampling process. It calculates each path\u2019s probability based on the sampling information. Finally, our model prioritizes and assigns the most difficult paths to concolic execution. We implement a prototype system DigFuzz and evaluate our system with two representative datasets. Results show that the concolic execution in DigFuzz outperforms than those in state-of-the-art hybrid fuzzing systems in every major aspect. In particular, the concolic execution in DigFuzz contributes to discovering more vulnerabilities (12 vs. 5) and producing more code coverage (18.9% vs. 3.8%) on the CQE dataset than the concolic execution in Driller.", "num_citations": "66\n", "authors": ["206"]}
{"title": "A hybrid ACO algorithm for the next release problem\n", "abstract": " In this paper, we propose a Hybrid Ant Colony Optimization algorithm (HACO) for Next Release Problem (NRP). NRP, a NP-hard problem in requirement engineering, is to balance customer requests, resource constraints, and requirement dependencies by requirement selection. Inspired by the successes of Ant Colony Optimization algorithms (ACO) for solving NP-hard problems, we design our HACO to approximately solve NRP. Similar to traditional ACO algorithms, multiple artificial ants are employed to construct new solutions. During the solution construction phase, both pheromone trails and neighborhood information will be taken to determine the choices of every ant. In addition, a local search (first found hill climbing) is incorporated into HACO to improve the solution quality. Extensively wide experiments on typical NRP test instances show that HACO outperforms the existing algorithms (GRASP and simulated\u00a0\u2026", "num_citations": "60\n", "authors": ["206"]}
{"title": "Towards Training Set Reduction for Bug Triage\n", "abstract": " Bug triage is an important step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the training sets of bug reports. In practice, these approaches suffer from the large-scale and low-quality training sets. In this paper, we propose the training set reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm X 2 -test, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the training set reduction on the bug data of Eclipse. For the training set, 70% words and 50% bug reports are removed after the training set reduction. The experimental results\u00a0\u2026", "num_citations": "59\n", "authors": ["206"]}
{"title": "MICHAC: Defect Prediction via Feature Selection based on Maximal Information Coefficient with Hierarchical Agglomerative Clustering\n", "abstract": " Defect prediction aims to estimate software reliability via learning from historical defect data. A defect prediction method identifies whether a software module is defect-prone or not according to metrics that are mined from software projects. These metric values, also known as features, may involve irrelevance and redundancy, which will hurt the performance of defect prediction methods. Existing work employs feature selection to preprocess defect data to filter out useless features. In this paper, we propose a novel feature selection framework, MICHAC, short for defect prediction via Maximal Information Coefficient with Hierarchical Agglomerative Clustering. MICHAC consists of two major stages. First, MICHAC employs maximal information coefficient to rank candidate features to filter out irrelevant ones, second, MICHAC groups features with hierarchical agglomerative clustering and selects one feature from each\u00a0\u2026", "num_citations": "52\n", "authors": ["206"]}
{"title": "Crash reproduction via test case mutation: Let existing test cases help\n", "abstract": " Developers reproduce crashes to understand root causes during software debugging. To reduce the manual effort by developers, automatic methods of crash reproduction generate new test cases for triggering crashes. However, due to the complex program structures, it is challenging to generate a test case to cover a specific program path. In this paper, we propose an approach to automatic crash reproduction via test case mutation, which updates existing test cases to trigger crashes rather than creating new test cases from scratch. This approach leverages major structures and objects in existing test cases and increases the chance of executing the specific path. Our preliminary result on 12 crashes in Apache Commons Collections shows that 7 crashes are reproduced by our approach of test case mutation.", "num_citations": "48\n", "authors": ["206"]}
{"title": "What causes my test alarm? Automatic cause analysis for test alarms in system and integration testing\n", "abstract": " Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the\u00a0\u2026", "num_citations": "43\n", "authors": ["206"]}
{"title": "Revisit of Automatic Debugging via Human Focus-tracking Analysis\n", "abstract": " In many fields of software engineering, studies on human behavior have attracted a lot of attention; however, few such studies exist in automated debugging. Parnin and Orso conducted a pioneering study comparing the performance of programmers in debugging with and without a ranking-based fault localization technique, namely Spectrum-Based Fault Localization (SBFL). In this paper, we revisit the actual helpfulness of SBFL, by addressing some major problems that were not resolved in Parnin and Orso's study. Our investigation involved 207 participants and 17 debugging tasks. A user-friendly SBFL tool was adopted. It was found that SBFL tended not to be helpful in improving the efficiency of debugging. By tracking and analyzing programmers' focus of attention, we characterized their source code navigation patterns and provided in-depth explanations to the observations. Results indicated that (1) a short\"\u00a0\u2026", "num_citations": "41\n", "authors": ["206"]}
{"title": "Hyper-heuristics with low level parameter adaptation\n", "abstract": " Recent years have witnessed the great success of hyper-heuristics applying to numerous real-world applications. Hyper-heuristics raise the generality of search methodologies by manipulating a set of low level heuristics (LLHs) to solve problems, and aim to automate the algorithm design process. However, those LLHs are usually parameterized, which may contradict the domain independent motivation of hyper-heuristics. In this paper, we show how to automatically maintain low level parameters (LLPs) using a hyper-heuristic with LLP adaptation (AD-HH), and exemplify the feasibility of AD-HH by adaptively maintaining the LLPs for two hyper-heuristic models. Furthermore, aiming at tackling the search space expansion due to the LLP adaptation, we apply a heuristic space reduction (SAR) mechanism to improve the AD-HH framework. The integration of the LLP adaptation and the SAR mechanism is able to\u00a0\u2026", "num_citations": "28\n", "authors": ["206"]}
{"title": "Extracting elite pairwise constraints for clustering\n", "abstract": " Semi-supervised clustering under pairwise constraints (i.e. must-links and cannot-links) has been a hot topic in the data mining community in recent years. Since pairwise constraints provided by distinct domain experts may conflict with each other, a lot of research work has been conducted to evaluate the effects of noise imposing on semi-supervised clustering. In this paper, we introduce elite pairwise constraints, including elite must-link (EML) and elite cannot-link (ECL) constraints. In contrast to traditional constraints, both EML and ECL constraints are required to be satisfied in every optimal partition (i.e. a partition with the minimum criterion function). Therefore, no conflict will be caused by those new constraints. First, we prove that it is NP-hard to obtain EML or ECL constraints. Then, a heuristic method named Limit Crossing is proposed to achieve a fraction of those new constraints. In practice, this new method\u00a0\u2026", "num_citations": "23\n", "authors": ["206"]}
{"title": "Towards Better Summarizing Bug Reports with Crowdsourcing Elicited Attributes\n", "abstract": " Recent years have witnessed the growing demands for resolving numerous bug reports in software maintenance. Aiming to reduce the time testers/developers take in perusing bug reports, the task of bug report summarization has attracted a lot of research efforts in the literature. However, no systematic analysis has been conducted on attribute construction, which heavily impacts the performance of supervised algorithms for bug report summarization. In this study, we first conduct a survey to reveal the existing methods for attribute construction in mining software repositories. Then, we propose a new method named Crowd-Attribute to infer new effective attributes from the crowd-generated data in crowdsourcing and develop a new tool named Crowdsourcing Software Engineering Platform to facilitate this method. With Crowd-Attribute, we successfully construct 11 new attributes and propose a new supervised\u00a0\u2026", "num_citations": "22\n", "authors": ["206"]}
{"title": "Approximate backbone based multilevel algorithm for next release problem\n", "abstract": " The next release problem (NRP) aims to effectively select software requirements in order to acquire maximum customer profits. As an NP-hard problem in software requirement engineering, NRP lacks efficient approximate algorithms for large scale instances. The backbone is a new tool for tackling large scale NP-hard problems in recent years. In this paper, we employ the backbone to design high performance approximate algorithms for large scale NRP instances. Firstly we show that it is NP-hard to obtain the backbone of NRP. Then, we illustrate by fitness landscape analysis that the backbone can be well approximated by the shared common parts of local optimal solutions. Therefore, we propose an approximate backbone based multilevel algorithm (ABMA) to solve large scale NRP instances. This algorithm iteratively explores the search spaces by multilevel reductions and refinements. Experimental results\u00a0\u2026", "num_citations": "21\n", "authors": ["206"]}
{"title": "How do Multiple Pull Requests Change the Same Code: A Study of Competing Pull Requests in GitHub\n", "abstract": " GitHub is a widely used collaborative platform for global software development. A pull request plays an important role in bridging code changes with version controlling. Developers can freely and parallelly submit pull requests to base branches and wait for the merge of their contributions. However, several developers may submit pull requests to edit the same lines of code; such pull requests result in a latent collaborative conflict. We refer such pull requests that tend to change the same lines and remain open during an overlapping time period to as competing pull requests. In this paper, we conduct a study on 9,476 competing pull requests from 60 Java repositories in GitHub. The data are collected by mining pull requests that are submitted in 2017 from top Java projects with the most forks. We explore how multiple pull requests change the same code via answering four research questions, including the distribution\u00a0\u2026", "num_citations": "19\n", "authors": ["206"]}
{"title": "Automated localization for unreproducible builds\n", "abstract": " Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries.", "num_citations": "18\n", "authors": ["206"]}
{"title": "New insights into diversification of hyper-heuristics\n", "abstract": " There has been a growing research trend of applying hyper-heuristics for problem solving, due to their ability of balancing the intensification and the diversification with low level heuristics. Traditionally, the diversification mechanism is mostly realized by perturbing the incumbent solutions to escape from local optima. In this paper, we report our attempt toward providing a new diversification mechanism, which is based on the concept of instance perturbation. In contrast to existing approaches, the proposed mechanism achieves the diversification by perturbing the instance under solving, rather than the solutions. To tackle the challenge of incorporating instance perturbation into hyper-heuristics, we also design a new hyper-heuristic framework HIP-HOP (recursive acronym of HIP-HOP is an instance perturbation-based hyper-heuristic optimization procedure), which employs a grammar guided high level strategy to\u00a0\u2026", "num_citations": "16\n", "authors": ["206"]}
{"title": "Juta: an automated unit testing framework for java\n", "abstract": " Testing is very important and time consuming in the development of high-quality software systems. This paper proposes an automatic testing tool JUTA for unit testing of Java programs. The approach is based on sharp analysis of the programs. JUTA firstly employs the Java optimization framework Soot to parse a single Java method into byte code and translates it into a control flow graph (CFG). It then performs depth-first or breadth-first search on the CFG to extract paths from it. Some techniques such as path length restriction are used to prevent path number explosion. Finally JUTA analyzes the paths based on the combination of symbolic execution and constraint solving. The goal of path analysis lies in two folds. It can generate a set of test cases satisfying the test criterion such as statement coverage. The test set typically has small number of test cases that are all executable. In addition to test generation for\u00a0\u2026", "num_citations": "16\n", "authors": ["206"]}
{"title": "EH-Recommender: Recommending Exception Handling Strategies Based on Program Context\n", "abstract": " Exception handling is widely used in software development to guarantee code robustness and system reliability. Developers are expected to choose appropriate handling strategies to ensure exceptions are handled properly without causing program crashes or unintended behaviors. However, making such choices is challenging especially for the novices due to lack of experience on exceptional flow design. To assist developers in deciding how to handle exceptions, we propose a method to automatically recommend exception handling strategies based on program context. This method learns practices of exception handling from existing high-quality projects and code by well-skilled developers. We extracted three type of program context (exceptional context, architectural context, and functional context) as features and applied machine learning techniques to recommend an optimized strategy of exception\u00a0\u2026", "num_citations": "13\n", "authors": ["206"]}
{"title": "How does code style inconsistency affect pull request integration? an exploratory study on 117 github projects\n", "abstract": " GitHub is a popular code platform that provides infrastructures to facilitate collaborative development. A Pull Request (PR) is one of the key ideas to support collaboration. Developers are encouraged to submit PRs to ask for the integration of their contributions. In practice, not all submitted PRs can be integrated into the codebase by project maintainers. Existing studies have investigated factors affecting PR integration. Nevertheless, the code style of PRs, which is largely considered by project maintainers, has not been deeply studied yet. In this paper, we performed an exploratory analysis on the effect of code style on PR integration in GitHub. We modeled the code style via the inconsistency between a submitted PR and the existing code in its target codebase. Such modeling makes our study not limited by a specific definition of code style. We conducted our experiments on 50,092 closed PRs in 117 Java projects\u00a0\u2026", "num_citations": "12\n", "authors": ["206"]}
{"title": "Multi-Level Random Walk for Software Test Suite Reduction\n", "abstract": " Software testing is important and time-consuming. A test suite, i.e., a set of test cases, plays a key role in validating the expected program behavior. In modern test-driven development, a test suite pushes the development progress. Software evolves over time; its test suite is executed to detect whether a new code change adds bugs to the existing code. Executing all test cases after each code change is unnecessary and may be impossible due to the limited development cycle. On the one hand, multiple test cases may focus on an identical piece of code; then several test cases cannot detect extra bugs. On the other hand, even executing a test suite once in a large project takes around one hour [1]; frequent code changes require much time for conducting testing. For instance, in Hadoop, a framework of distributed computing, 2,847 version commits are accepted within one year from September 2014 with a peak of 135\u00a0\u2026", "num_citations": "12\n", "authors": ["206"]}
{"title": "An approximate muscle guided global optimization algorithm for the three-index assignment problem\n", "abstract": " The three-index assignment problem (AP3) is a famous NP-hard problem with wide applications. Since itpsilas intractable, many heuristics have been proposed to obtain near optimal solutions in reasonable time. In this paper, a new meta-heuristic was proposed for solving the AP3. Firstly, we introduced the conception of muscle (the union of optimal solutions) and proved that it is intractable to obtain the muscle under the assumption that P ne NP. Moreover, we showed that the whole muscle can be approximated by the union of local optimal solutions. Therefore, the approximate muscle guided global optimization (AMGO) is proposed to solve the AP3. AMGO employs a global optimization strategy to search in a search space reduced by the approximate muscle, which is constructed by a multi-restart scheme. During the global optimization procedure, the running time can be dramatically saved by detecting feasible\u00a0\u2026", "num_citations": "12\n", "authors": ["206"]}
{"title": "A perturbation adaptive pursuit strategy based hyper-heuristic for multi-objective optimization problems\n", "abstract": " For multi-objective optimization problems, obtaining a uniformly distributed approximation set is among the most important issues. During the past decades, various diversity mechanisms have been proposed to address this challenge. However, the existing diversity mechanisms tend to be problem-specific, and may not generalize well over different problem domains. Inspired by the idea of utilizing multiple low-level heuristics to achieve better diversity performance in multi-discipline problem solving, we focus on efficient algorithm design based on the methodology of selection hyper-heuristics. This study proposes a novel selection hyper-heuristic operating over multiple diversity mechanisms. The unique feature of the proposed approach lies in its ability to intelligently learn, select, and combine different diversity mechanisms with the purpose of taking advantages of them to obtain well-distributed approximation sets\u00a0\u2026", "num_citations": "10\n", "authors": ["206"]}
{"title": "Supporting many-objective software requirements decision: an exploratory study on the next release problem\n", "abstract": " The decision of which requirements should be satisfied in the next release is crucial to software company. The next release problem, a family of requirements selection decision, aims to maximize profits by satisfying requirements to balance customer profits and development costs. However, due to diverse practical scenarios, solutions to the next release problem have to face many different objectives. In this paper, we propose an exploratory study on the many-objective next release problem with five evolutionary optimization algorithms. The goal of this study is to use the experimental results to assist project managers to make the requirements decision in the scenario of many decision objectives. This study focuses on four research questions, including the effectiveness of optimization, the significance of results, the distribution of metric values, and the correlation between metrics. We design the study to explore five\u00a0\u2026", "num_citations": "10\n", "authors": ["206"]}
{"title": "Misleading classification\n", "abstract": " In this paper, we investigate a new problem-misleading classification in which each test instance is associated with an original class and a misleading class. Its goal for the data owner is to form the training set out of candidate instances such that the data miner will be misled to classify those test instances to their misleading classes rather than original classes. We discuss two cases of misleading classification. For the case where the classification algorithm is unknown to the data owner, a KNN based Ranking Algorithm (KRA) is proposed to rank all candidate instances based on the similarities between candidate instances and test instances. For the case where the classification algorithm is known, we propose a Greedy Ranking Algorithm (GRA) which evaluates each candidate instance by building up a classifier to predict the test set. In addition, we also show how to accelerate GRA in an incremental way\u00a0\u2026", "num_citations": "10\n", "authors": ["206"]}
{"title": "An Accelerated-Limit-Crossing-Based Multilevel Algorithm for the -Median Problem\n", "abstract": " In this paper, we investigate how to design an efficient heuristic algorithm under the guideline of the backbone and the fat, in the context of the p-median problem. Given a problem instance, the backbone variables are defined as the variables shared by all optimal solutions, and the fat variables are defined as the variables that are absent from every optimal solution. Identification of the backbone (fat) variables is essential for the heuristic algorithms exploiting such structures. Since the existing exact identification method, i.e., limit crossing (LC), is time consuming and sensitive to the upper bounds, it is hard to incorporate LC into heuristic algorithm design. In this paper, we develop the accelerated-LC (ALC)-based multilevel algorithm (ALCMA). In contrast to LC which repeatedly runs the time-consuming Lagrangian relaxation (LR) procedure, ALC is introduced in ALCMA such that LR is performed only once, and every\u00a0\u2026", "num_citations": "10\n", "authors": ["206"]}
{"title": "Ant based hyper heuristics with space reduction: a case study of the p-median problem\n", "abstract": " Recent years have witnessed great success of ant based hyper heuristics applying to real world applications. Ant based hyper heuristics intend to explore the heuristic space by traversing the fully connected graph induced by low level heuristics (LLHs). However, existing ant based models treat LLH in an equivalent way, which may lead to imbalance between the intensification and the diversification of the search procedure. Following the definition of meta heuristics, we propose an Ant based Hyper heuristic with SpAce Reduction (AHSAR) to adapt the search over the heuristic space. AHSAR reduces the heuristic space by replacing the fully connected graph with a bipartite graph, which is induced by the Cartesian product of two LLH subsets. With the space reduction, AHSAR enforces consecutive execution of intensification and diversification LLHs. We apply AHSAR to the p-median problem, and\u00a0\u2026", "num_citations": "10\n", "authors": ["206"]}
{"title": "A random walk based algorithm for structural test case generation\n", "abstract": " Structural testing is a significant and expensive process in software development. By converting test data generation into an optimization problem, search-based software testing is one of the key technologies of automated test case generation. Motivated by the success of random walk in solving the satisfiability problem (SAT), we proposed a random walk based algorithm (WalkTest) to solve structural test case generation problem. WalkTest provides a framework, which iteratively calls random walk operator to search the optimal solutions. In order to improve search efficiency, we sorted the test goals with the costs of solutions completely instead of traditional dependence analysis from control flow graph. Experimental results on the condition-decision coverage demonstrated that WalkTest achieves better performance than existing algorithms (random test and tabu search) in terms of running time and coverage rate.", "num_citations": "7\n", "authors": ["206"]}
{"title": "Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems\n", "abstract": " A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.", "num_citations": "6\n", "authors": ["206"]}
{"title": "Feature based problem hardness understanding for requirements engineering\n", "abstract": " Heuristics and metaheuristics have achieved great accomplishments in various fields, and the investigation of the relationship between these algorithms and the problem hardness has been a hot topic in the research field. Related research work has contributed much to the understanding of the underlying mechanisms of the algorithms for problem solving. However, most existing studies consider traditional combinatorial problems as their case studies. In this study, taking the Next Release Problem (NRP) from the requirements engineering as a case study, we investigate the relationship between software engineering problem instances and heuristics. We employ an evolutionary algorithm to evolve NRP instances, which are uniquely hard or easy for the target heuristic (Greedy Randomized Adaptive Search Procedure and Randomized Hill Climbing in this paper). Then, we use a feature-based method to\u00a0\u2026", "num_citations": "6\n", "authors": ["206"]}
{"title": "Can this fault be detected: A study on fault detection via automated test generation\n", "abstract": " Automated test generation can reduce the manual effort in improving software quality. A test generation method employs code coverage, such as the widely-used branch coverage, to guide the inference of tests. These tests can be used to detect hidden faults. An automatic tool takes a specific type of code coverage as a configurable parameter. Given an automated tool of test generation, a fault may be detected by one type of code coverage, but omitted by another. In frequently released software projects, the time budget of testing is limited. Configuring code coverage for a testing tool can effectively improve the quality of projects. In this paper, we conduct a study on whether a fault can be detected by specific code coverage in automated test generation. We build predictive models with 60 metrics of faulty source code to identify detectable faults under eight types of code coverage. In the experiment, an off-the-shelf\u00a0\u2026", "num_citations": "5\n", "authors": ["206"]}
{"title": "Writing tests for this higher-order function first: Automatically identifying future callings to assist testers\n", "abstract": " In functional programming languages, such as Scala and Haskell, a higher-order function is a function that takes one or more functions as parameters or returns a function. Using higher-order functions in programs can increase the generality and reduce the redundancy of source code. To test a higher-order function, a tester needs to check the requirements and write another function as the test input. However, due to the complexity of higher-order functions, testing higher-order functions is a time-consuming and labor-intensive task. Testers have to spend an amount of manual effort in testing all higher-order functions. Such testing is infeasible if the time budget is limited, such as a period before a project release. In this paper, we propose an automatic approach, namely PHOF, which predicts whether a higher-order function will be called in the future. Higherorder functions that are most likely to be called should be\u00a0\u2026", "num_citations": "5\n", "authors": ["206"]}
{"title": "Frequency distribution based hyper-heuristic for the bin-packing problem\n", "abstract": " In the paper, we investigate the pair frequency of low-level heuristics for the bin packing problem and propose a Frequency Distribution based Hyper-Heuristic (FDHH). FDHH generates the heuristic sequences based on a pair of low-level heuristics rather than an individual low-level heuristic. An existing Simulated Annealing Hyper-Heuristic (SAHH) is employed to form the pair frequencies and is extended to guide the further selection of low-level heuristics. To represent the frequency distribution, a frequency matrix is built to collect the pair frequencies while a reverse-frequency matrix is generated to avoid getting trapped into the local optima. The experimental results on the bin-packing problems show that FDHH can obtain optimal solutions on more instances than the original hyper-heuristic.", "num_citations": "5\n", "authors": ["206"]}
{"title": "A hyper-heuristic using grasp with path-relinking: A case study of the nurse rostering problem\n", "abstract": " The goal of hyper-heuristics is to design and choose heuristics to solve complex problems. The primary motivation behind the hyper-heuristics is to generalize the solving ability of the heuristics. In this paper, the authors propose a Hyper-heuristic using GRASP with Path-Relinking (HyGrasPr). HyGrasPr generates heuristic sequences to produce solutions within an iterative procedure. The procedure of HyGrasPr consists of three phases, namely the construction phase, the local search phase, and the path-relinking phase. To show the performance of the HyGrasPr, the authors use the nurse rostering problem as a case study. The authors use an existing simulated annealing based hyper-heuristic as a baseline. The experimental results indicate that HyGrasPr can achieve better solutions than SAHH within the same running time and the path-relinking phase is effective for the framework of HyGrasPr.", "num_citations": "4\n", "authors": ["206"]}
{"title": "Can This Fault Be Detected by Automated Test Generation: A Preliminary Study\n", "abstract": " Automated test generation can reduce the manual effort to improve software quality. A test generation method employs code coverage, such as the widely-used branch coverage, to guide the inference of test cases. These test cases can be used to detect hidden faults. An automatic tool takes a specific type of code coverage as a configurable parameter. Given an automated tool of test generation, a fault may be detected by one type of code coverage, but omitted by another. In frequently released software projects, the time budget of testing is limited. Configuring code coverage for a testing tool can effectively improve the quality of projects. In this paper, we conduct a preliminary study on whether a fault can be detected by specific code coverage in automated test generation. We build predictive models with 60 metrics of faulty source code to identify detectable faults under eight types of code coverage, such as branch\u00a0\u2026", "num_citations": "3\n", "authors": ["206"]}
{"title": "Multi-perspective visualization to assist code change review\n", "abstract": " Change-based code review plays an important role in open-source project development. Due to the large amount of human involvement and tight time schedule, tools that can facilitate this activity would be of great help. Current tools mainly focus on difference extraction, code style examination, static analysis, comment and discussion, etc. However, there is little support to change impact analysis for code change review. In this paper, we serve this purpose by providing a change review assistance tool, namely, MultiViewer, for the most popular OSS GitHub. We define metrics to characterize code changes from multiple perspectives. Specifically, these metrics mine coupling relations among related files in the changes, as well as estimate the change effort, risk and impact. Such information is visualized by MultiViewer in two formats. We demonstrate the helpfulness of MultiViewer by showing its ability as indicators to\u00a0\u2026", "num_citations": "3\n", "authors": ["206"]}
{"title": "Backbone guided local search for the weighted maximum satisfiability problem\n", "abstract": " The Satisfiability problem (SAT) is a famous NP-Complete problem, which consists of an assignment of Boolean variables (true or false) and some clauses formed of these variables. A clause is a disjunction of some Boolean literals and can be true if and only if any of them is true. A SAT instance is satisfied if and only if all the clauses are simultaneously true. As a generalization of SAT, the Maximum Satisfiability problem (MAX-SAT) aims to maximize the number of satisfied clauses. When every clause is associated with some weight, the MAX-SAT turns to the weighted Maximum Satisfiability problem (weighted MAX-SAT) with numerous applications arising in artificial intelligence, such as scheduling, data mining, pattern recognition, and automatic reasoning. According to the computational complexity theory, there\u2019s no polynomial time algorithm for solving weighted MAX-SAT unless P= NP. Hence, many heuristic algorithms capable of finding near optimal solutions in reasonable time have been proposed for weighted MAX-SAT, including GSAT, GLS, Taboo Scatter Search, ACO, and GRASP/GRASP with path-relinking. As an efficient tool for heuristic design, the backbone has attracted great attention from the society of artificial intelligence in recent years. The backbone is defined as the common parts of all optimal solutions for an instance. Since it\u2019s usually intractable to obtain the exact backbone, many approximate backbone guided heuristics have been developed for NP-Complete problems, including 3-SAT, TSP, QAP, et al. For SAT or MAX-SAT, some character and algorithms of backbone are in research. And a BGWalksat (backbone guided\u00a0\u2026", "num_citations": "3\n", "authors": ["206"]}
{"title": "From Code to Natural Language: Type-Aware Sketch-Based Seq2Seq Learning\n", "abstract": " Code comment generation aims to translate existing source code into natural language explanations. It provides an easy-to-understand description for developers who are unfamiliar with the functionality of source code. Existing approaches to code comment generation focus on summarizing multiple lines of code with a short text, but often cannot effectively explain a single line of code. In this paper, we propose an asynchronous learning model, which learns the code semantics and generates a fine-grained natural language explanation for each line of code. Different from a coarse-grained code comment generation, this fine-grained explanation can help developers better understand the functionality line-by-line. The proposed model adopts a type-aware sketch-based sequence-to-sequence learning method to generate natural language explanations for source code. This method incorporates the type of source\u00a0\u2026", "num_citations": "2\n", "authors": ["206"]}
{"title": "Regression models for performance ranking of configurable systems: A comparative study\n", "abstract": " Finding the best configurations for a highly configurable system is challenging. Existing studies learned regression models to predict the performance of potential configurations. Such learning suffers from the low accuracy and the high effort of examining the actual performance for data labeling. A recent approach uses an iterative strategy to sample a small number of configurations from the training pool to reduce the number of sampled ones. In this paper, we conducted a comparative study on the rank-based approach of configurable systems with four regression methods. These methods are compared on 21 evaluation scenarios of 16 real-world configurable systems. We designed three research questions to check the impacts of different methods on the rank-based approach. We find out that the decision tree method of Classification And Regression Tree (CART) and the ensemble learning method of Gradient\u00a0\u2026", "num_citations": "2\n", "authors": ["206"]}
{"title": "Progress on software crash research\n", "abstract": " Software crashes represent unexpected program interrupts that manifest as software faults and can be dangerous in that their frequent occurrence diminishes user experience, can damage the reputation of a company, and potentially cause significant losses to stakeholders. The increasing scale and complexity of modern software requires methods for preventing/handling software crashes. Here, we briefly review and summarize progress in three areas of the research field associated with handling software crashes: crash analysis, crash reproduction, and crash localization and repair.", "num_citations": "2\n", "authors": ["206"]}
{"title": "Automatic reproducible crash detection\n", "abstract": " Crash reproduction, which spends much time of developers in reading and understanding source code, is a crucial yet time-consuming task in program debugging. To reduce the time and resource cost, automatic techniques of test generation have been proposed. These techniques aim to automatically generate test cases to reproduce the scenario of a crashed project. Unfortunately, due to the lack of a detailed comprehension of the source code, a generated test case may fail in reproducing an expected crash. In this paper, we propose an automatic approach to reproducible bug detection. This approach predicts whether a crash is difficult to reproduce or not via training a classifier based on historical reproducible crash data. If a crash is difficult to reproduce, it is better to assign the crash to a developer, instead of using an automatic technique of test generation. Our work can help to prioritize crashes and to save\u00a0\u2026", "num_citations": "2\n", "authors": ["206"]}
{"title": "Probabilistic Path Prioritization for Hybrid Fuzzing\n", "abstract": " Hybrid fuzzing that combines fuzzing and concolic execution has become an advanced technique for software vulnerability detection. Based on the observation that fuzzing and concolic execution are complementary in nature, state-of-the-art hybrid fuzzing systems deploy \u201coptimal concolic testing\u201d and \u201cdemand launch\u201d strategies. Although these ideas sound intriguing, we point out several fundamental limitations in them, due to oversimplified assumptions. Further, we propose a novel \u201cdiscriminative dispatch\u201d strategy and design a probabilistic hybrid fuzzing system to better utilize the capability of concolic execution. In details, we design a Monte Carlo based probabilistic path prioritization model to quantify each path's difficulty and prioritize them for concolic execution. This model treats fuzzing as a random sampling process and calculates each path's probability based on the sampling information. Finally, our\u00a0\u2026", "num_citations": "1\n", "authors": ["206"]}
{"title": "MetPurity: A Learning-Based Tool of Pure Method Identification for Automatic Test Generation\n", "abstract": " In object-oriented programming, a method is pure if calling the method does not change object states that exist in the pre-states of the method call. Pure methods are widely-used in automatic techniques, including test generation, compiler optimization, and program repair. Due to the source code dependency, it is infeasible to completely and accurately identify all pure methods. Instead, existing techniques such as ReImInfer are designed to identify a subset of accurate results of pure method and mark the other methods as unknown ones. In this paper, we designed and implemented MetPurity, a learning-based tool of pure method identification. Given all methods in a project, MetPurity labels a training set via automatic program analysis and builds a binary classifier (implemented with the random forest classifier) based on the training set. This classifier is used to predict the purity of all the other methods (i.e\u00a0\u2026", "num_citations": "1\n", "authors": ["206"]}
{"title": "Multi-Objective Configuration Sampling for Performance Ranking in Configurable Systems\n", "abstract": " The problem of performance ranking in configurable systems is to find the optimal (near-optimal) configurations with the best performance. This problem is challenging due to the large search space of potential configurations and the cost of manually examining configurations. Existing methods, such as the rank-based method, use a progressive strategy to sample configurations to reduce the cost of examining configurations. This sampling strategy is guided by frequent and random trials and may fail in balancing the number of samples and the ranking difference (i.e., the minimum of actual ranks in the predicted ranking). In this paper, we proposed a sampling method, namely MoConfig, which uses multi-objective optimization to minimize the number of samples and the ranking difference. Each solution in MoConfig is a sampling set of configurations and can be directly used as the input of existing methods of\u00a0\u2026", "num_citations": "1\n", "authors": ["206"]}
{"title": "Changes are Similar: Measuring Similarity of Pull Requests that Change the Same Code in GitHub\n", "abstract": " Pull-based development is widely used in globally collaborative platforms, such as GitHub and BitBucket. A pull request is a set of changes to existing source code in a project. A developer submits a pull request and tends to update the source code. Due to the parallel mechanism, several developers may submit multiple pull requests to change the same lines of code. This fact results in the conflict between changes, which makes the project manager difficult to decide which pull request should be merged. In this paper, we conducted a preliminary study on measuring the similarity of pull requests that aim to change the same code in GitHub. We proposed two methods, i.e., the cosine and the doc2vec, to quantify the structural similarity and the semantic similarity between pull requests and evaluated the similarity on four widely-studied open source Java projects. Our study shows that there indeed exists high\u00a0\u2026", "num_citations": "1\n", "authors": ["206"]}