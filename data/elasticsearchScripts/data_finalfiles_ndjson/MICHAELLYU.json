{"title": "Recommender systems with social regularization\n", "abstract": " Although Recommender Systems have been comprehensively analyzed in the past decade, the study of social-based recommender systems just started. In this paper, aiming at providing a general method for improving recommender systems by incorporating social network information, we propose a matrix factorization framework with social regularization. The contributions of this paper are four-fold:(1) We elaborate how social network information can benefit recommender systems;(2) We interpret the differences between social-based recommender systems and trust-aware recommender systems;(3) We coin the term Social Regularization to represent the social constraints on recommender systems, and we systematically illustrate how to design a matrix factorization objective function with social regularization; and (4) The proposed method is quite general, which can be easily extended to incorporate other\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1649\n", "authors": ["14"]}
{"title": "Sorec: social recommendation using probabilistic matrix factorization\n", "abstract": " Data sparsity, scalability and prediction quality have been recognized as the three most crucial challenges that every collaborative filtering algorithm or recommender system confronts. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings or even none at all. Moreover, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the social interactions or connections among users. In view of the exponential growth of information generated by online social networks, social network analysis is becoming important for many Web applications. Following the intuition that a person's social network will affect personal behaviors on the Web, this paper proposes a factor analysis approach based on probabilistic matrix factorization to solve the data sparsity and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1496\n", "authors": ["14"]}
{"title": "Learning to recommend with social trust ensemble\n", "abstract": " As an indispensable technique in the field of Information Filtering, Recommender System has been well studied and developed both in academia and in industry recently. However, most of current recommender systems suffer the following problems:(1) The large-scale and sparse data of the user-item matrix seriously affect the recommendation quality. As a result, most of the recommender systems cannot easily deal with users who have made very few ratings.(2) The traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real world recommendations. Aiming at modeling recommender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework, which naturally fuses the users' tastes and their trusted friends' favors together. In this framework\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "984\n", "authors": ["14"]}
{"title": "QoS-aware web service recommendation by collaborative filtering\n", "abstract": " With increasing presence and adoption of Web services on the World Wide Web, Quality-of-Service (QoS) is becoming important for describing nonfunctional characteristics of Web services. In this paper, we present a collaborative filtering approach for predicting QoS values of Web services and making Web service recommendation by taking advantages of past usage experiences of service users. We first propose a user-collaborative mechanism for past Web service QoS information collection from different service users. Then, based on the collected QoS data, a collaborative filtering approach is designed to predict Web service QoS values. Finally, a prototype called WSRec is implemented by Java language and deployed to the Internet for conducting real-world experiments. To study the QoS value prediction accuracy of our approach, 1.5 millions Web service invocation results are collected from 150 service\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "838\n", "authors": ["14"]}
{"title": "A hybrid particle swarm optimization\u0393\u00c7\u00f4back-propagation algorithm for feedforward neural network training\n", "abstract": " The particle swarm optimization algorithm was showed to converge rapidly during the initial stages of a global search, but around global optimum, the search process will become very slow. On the contrary, the gradient descending method can achieve faster convergent speed around global optimum, and at the same time, the convergent accuracy can be higher. So in this paper, a hybrid algorithm combining particle swarm optimization (PSO) algorithm with back-propagation (BP) algorithm, also referred to as PSO\u0393\u00c7\u00f4BP algorithm, is proposed to train the weights of feedforward neural network (FNN), the hybrid algorithm can make use of not only strong global searching ability of the PSOA, but also strong local searching ability of the BP algorithm. In this paper, a novel selection strategy of the inertial weight is introduced to the PSO algorithm. In the proposed PSO\u0393\u00c7\u00f4BP algorithm, we adopt a heuristic way to give a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "756\n", "authors": ["14"]}
{"title": "Fused matrix factorization with geographical and social influence in location-based social networks\n", "abstract": " Recently, location-based social networks (LBSNs), such as Gowalla, Foursquare, Facebook, and Brightkite, etc., have attracted millions of users to share their social friendship and their locations via check-ins. The available check-in information makes it possible to mine users\u0393\u00c7\u00d6 preference on locations and to provide favorite recommendations. Personalized Point-of-interest (POI) recommendation is a significant task in LBSNs since it can help targeted users explore their surroundings as well as help third-party developers to provide personalized services. To solve this task, matrix factorization is a promising tool due to its success in recommender systems. However, previously proposed matrix factorization (MF) methods do not explore geographical influence, eg, multi-center check-in property, which yields suboptimal solutions for the recommendation. In this paper, to the best of our knowledge, we are the first to fuse MF with geographical and social influence for POI recommendation in LBSNs. We first capture the geographical influence via modeling the probability of a user\u0393\u00c7\u00d6s check-in on a location as a Multi-center Gaussian Model (MGM). Next, we include social information and fuse the geographical influence into a generalized matrix factorization framework. Our solution to POI recommendation is efficient and scales linearly with the number of observations. Finally, we conduct thorough experiments on a large-scale real-world LBSNs dataset and demonstrate that the fused matrix factorization framework with MGM utilizes the distance information sufficiently and outperforms other state-of-the-art methods significantly.", "num_citations": "651\n", "authors": ["14"]}
{"title": "Effective missing data prediction for collaborative filtering\n", "abstract": " Memory-based collaborative filtering algorithms have been widely adopted in many popular recommender systems, although these approaches all suffer from data sparsity and poor prediction quality problems. Usually, the user-item matrix is quite sparse, which directly leads to inaccurate recommendations. This paper focuses the memory-based collaborative filtering problems on two crucial factors:(1) similarity computation between users or items and (2) missing data prediction algorithms. First, we use the enhanced Pearson Correlation Coefficient (PCC) algorithm by adding one parameter which overcomes the potential decrease of accuracy when computing the similarity of users or items. Second, we propose an effective missing data prediction algorithm, in which information of both users and items is taken into account. In this algorithm, we set the similarity threshold for users and items respectively, and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "600\n", "authors": ["14"]}
{"title": "Wsrec: A collaborative filtering based web service recommender system\n", "abstract": " As the abundance of Web services on the World Wide Web increase, designing effective approaches for Web service selection and recommendation has become more and more important. In this paper, we present WSRec, a Web service recommender system, to attack this crucial problem. WSRec includes a user-contribution mechanism for Web service QoS information collection and an effective and novel hybrid collaborative filtering algorithm for Web service QoS value prediction. WSRec is implemented by Java language and deployed to the real-world environment. To study the prediction performance, a total of 21,197 public Web services are obtained from the Internet and a large-scale real-world experiment is conducted, where more than 1.5 millions test results are collected from 150 service users in different countries on 100 publicly available Web services located all over the world. The comprehensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "572\n", "authors": ["14"]}
{"title": "Where you like to go next: Successive point-of-interest recommendation\n", "abstract": " Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user experience as well as enable third-party services, eg, launching advertisements. To provide a good recommendation, various research has been conducted in the literature. However, pervious efforts mainly consider the \u0393\u00c7\u00a3check-ins\u0393\u00c7\u00a5 in a whole and omit their temporal relation. They can only recommend POI globally and cannot know where a user would like to go tomorrow or in the next few days. In this paper, we consider the task of successive personalized POI recommendation in LBSNs, which is a much harder task than standard personalized POI recommendation or prediction. To solve this task, we observe two prominent properties in the check-in sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely FPMCLR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploits the personalized Markov chain in the check-in sequence, but also takes into account users\u0393\u00c7\u00d6 movement constraint, ie, moving around a localized region. More importantly, utilizing the information of localized regions, we not only reduce the computation cost largely, but also discard the noisy information to boost recommendation. Results on two real-world LBSNs datasets demonstrate the merits of our proposed FPMC-LR.", "num_citations": "519\n", "authors": ["14"]}
{"title": "A comprehensive method for multilingual video text detection, localization, and extraction\n", "abstract": " Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "477\n", "authors": ["14"]}
{"title": "Batch mode active learning and its application to medical image classification\n", "abstract": " The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient since the classification model has to be retrained for every labeled example. In this paper, we present a framework for\" batch mode active learning\" that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an efficient greedy algorithm that is based on the property of submodular functions. Our empirical studies with five UCI datasets and one real-world medical image classification show that the proposed batch mode active learning algorithm is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "475\n", "authors": ["14"]}
{"title": "Software fault tolerance\n", "abstract": " From the Publisher: Software fault tolerance techniques involve error detection, exception handling, monitoring mechanisms and error recovery. This important book also focuses on identification, application, formulation and evaluation of current software tolerance techniques.", "num_citations": "461\n", "authors": ["14"]}
{"title": "Distributed qos evaluation for real-world web services\n", "abstract": " Quality-of-Service (QoS) is widely employed for describing non-functional characteristics of Web services. Although QoS of Web services has been investigated in a lot of previous works, there is a lack of real-world Web service QoS datasets for validating new QoS based techniques and models of Web services. To study the performance of real-world Web services as well as provide reusable research datasets for promoting the research of QoS-driven Web services, we conduct several large-scale evaluations on real-world Web services. Firstly, addresses of 21,358 Web services are obtained from the Internet. Then, invocation failure probability performance of 150 Web services is assessed by 100 distributed service users. After that, response time and throughput performance of 5,825 Web services are evaluated by 339 distributed service users. Detailed experimental results are presented in this paper and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "437\n", "authors": ["14"]}
{"title": "Software Reliability Engineering: A Roadmap\n", "abstract": " Software reliability engineering is focused on engineering techniques for developing and maintaining software systems whose reliability can be quantitatively evaluated. In order to estimate as well as to predict the reliability of software systems, failure data need to be properly measured by various means during software development and operational phases. Moreover, credible software reliability models are required to track underlying software failure processes for accurate reliability analysis and forecasting. Although software reliability has remained an active research subject over the past 35 years, challenges and open questions still exist. In particular, vital future goals include the development of new software reliability engineering paradigms that take software architectures, testing techniques, and software failure manifestation mechanisms into consideration. In this paper, we review the history of software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "424\n", "authors": ["14"]}
{"title": "Collaborative web service qos prediction via neighborhood integrated matrix factorization\n", "abstract": " With the increasing presence and adoption of web services on the World Wide Web, the demand of efficient web service quality evaluation approaches is becoming unprecedentedly strong. To avoid the expensive and time-consuming web service invocations, this paper proposes a collaborative quality-of-service (QoS) prediction approach for web services by taking advantages of the past web service usage experiences of service users. We first apply the concept of user-collaboration for the web service QoS information sharing. Then, based on the collected QoS data, a neighborhood-integrated approach is designed for personalized web service QoS value prediction. To validate our approach, large-scale real-world experiments are conducted, which include 1,974,675 web service invocations from 339 service users on 5,825 real-world web services. The comprehensive experimental studies show that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "421\n", "authors": ["14"]}
{"title": "Learning distance metrics with contextual constraints for image retrieval\n", "abstract": " Relevant Component Analysis (RCA) has been proposed for learning distance metrics with contextual constraints for image retrieval. However, RCA has two important disadvantages. One is the lack of exploiting negative constraints which can also be informative, and the other is its incapability of capturing complex nonlinear relationships between data instances with the contextual information. In this paper, we propose two algorithms to overcome these two disadvantages, i.e., Discriminative Component Analysis (DCA) and Kernel DCA. Compared with other complicated methods for distance metric learning, our algorithms are rather simple to understand and very easy to solve. We evaluate the performance of our algorithms on image retrieval in which experimental results show that our algorithms are effective and promising in learning good quality distance metrics for image retrieval.", "num_citations": "382\n", "authors": ["14"]}
{"title": "Semisupervised svm batch mode active learning with applications to image retrieval\n", "abstract": " Support vector machine (SVM) active learning is one popular and successful technique for relevance feedback in content-based image retrieval (CBIR). Despite the success, conventional SVM active learning has two main drawbacks. First, the performance of SVM is usually limited by the number of labeled examples. It often suffers a poor performance for the small-sized labeled examples, which is the case in relevance feedback. Second, conventional approaches do not take into account the redundancy among examples, and could select multiple examples that are similar (or even identical). In this work, we propose a novel scheme for explicitly addressing the drawbacks. It first learns a kernel function from a mixture of labeled and unlabeled data, and therefore alleviates the problem of small-sized training data. The kernel will then be used for a batch mode active learning method to identify the most informative and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "377\n", "authors": ["14"]}
{"title": "Investigating QoS of real-world web services\n", "abstract": " Quality of service (QoS) is widely employed for describing nonfunctional characteristics of web services. Although QoS of web services has been investigated intensively in the field of service computing, there is a lack of real-world web service QoS data sets for validating various QoS-based techniques and models. To investigate QoS of real-world web services and to provide reusable research data sets for future research, we conduct several large-scale evaluations on real-world web services. First, addresses of 21,358 web services are obtained from the Internet. Then, three large-scale real-world evaluations are conducted. In our evaluations, more than 30 million real-world web service invocations are conducted on web services in more than 80 countries by users from more than 30 counties. Detailed evaluation results are presented in this paper and comprehensive web service QoS data sets are publicly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "367\n", "authors": ["14"]}
{"title": "Discriminative semi-supervised feature selection via manifold regularization\n", "abstract": " Feature selection has attracted a huge amount of interest in both research and application communities of data mining. We consider the problem of semi-supervised feature selection, where we are given a small amount of labeled examples and a large amount of unlabeled examples. Since a small number of labeled samples are usually insufficient for identifying the relevant features, the critical problem arising from semi-supervised feature selection is how to take advantage of the information underneath the unlabeled data. To address this problem, we propose a novel discriminative semi-supervised feature selection method based on the idea of manifold regularization. The proposed approach selects features through maximizing the classification margin between different classes and simultaneously exploiting the geometry of the probability distribution that generates both labeled and unlabeled data. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "362\n", "authors": ["14"]}
{"title": "QoS ranking prediction for cloud services\n", "abstract": " Cloud computing is becoming popular. Building high-quality cloud applications is a critical research problem. QoS rankings provide valuable information for making optimal cloud service selection from a set of functionally equivalent service candidates. To obtain QoS values, real-world invocations on the service candidates are usually required. To avoid the time-consuming and expensive real-world service invocations, this paper proposes a QoS ranking prediction framework for cloud services by taking advantage of the past service usage experiences of other consumers. Our proposed framework requires no additional invocations of cloud services when making QoS ranking prediction. Two personalized QoS ranking prediction approaches are proposed to predict the QoS rankings directly. Comprehensive experiments are conducted employing real-world QoS data, including 300 distributed users and 500 real\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "351\n", "authors": ["14"]}
{"title": "Ratings meet reviews, a combined approach to recommend\n", "abstract": " Most existing recommender systems focus on modeling the ratings while ignoring the abundant information embedded in the review text. In this paper, we propose a unified model that combines content-based filtering with collaborative filtering, harnessing the information of both ratings and reviews. We apply topic modeling techniques on the review text and align the topics with rating dimensions to improve prediction accuracy. With the information embedded in the review text, we can alleviate the cold-start problem. Furthermore, our model is able to learn latent topics that are interpretable. With these interpretable topics, we can explore the prior knowledge on items or users and recommend completely\" cold\"'items. Empirical study on 27 classes of real-life datasets show that our proposed model lead to significant improvement compared with strong baseline methods, especially for datasets which are extremely\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "324\n", "authors": ["14"]}
{"title": "Collaborative reliability prediction of service-oriented systems\n", "abstract": " Service-oriented architecture (SOA) is becoming a major software framework for building complex distributed systems. Reliability of the service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet. Designing effective and accurate reliability prediction approaches for the service-oriented systems has become an important research issue. In this paper, we propose a collaborative reliability prediction approach, which employs the past failure data of other similar users to predict the Web service reliability for the current user, without requiring real-world Web service invocations. We also present a user-collaborative failure data sharing mechanism and a reliability composition model for the service-oriented systems. Large-scale real-world experiments are conducted and the experimental results show that our collaborative reliability prediction approach obtains better\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "322\n", "authors": ["14"]}
{"title": "Simple and efficient multiple kernel learning by group lasso\n", "abstract": " We consider the problem of how to improve the efficiency of Multiple Kernel Learning (MKL). In literature, MKL is often solved by an alternating approach:(1) the minimization of the kernel weights is solved by complicated techniques, such as Semi-infinite Linear Programming, Gradient Descent, or Level method;(2) the maximization of SVM dual variables can be solved by standard SVM solvers. However, the minimization step in these methods is usually dependent on its solving techniques or commercial softwares, which therefore limits the efficiency and applicability. In this paper, we formulate a closed-form solution for optimizing the kernel weights based on the equivalence between group-lasso and MKL. Although this equivalence is not our invention, our derived variant equivalence not only leads to an efficient algorithm for MKL, but also generalizes to the case for Lp-MKL (p\u0393\u00eb\u00d1 1 and denoting the Lp-norm of kernel weights). Therefore, our proposed algorithm provides a unified solution for the entire family of Lp-MKL models. Experiments on multiple data sets show the promising performance of the proposed technique compared with other competitive methods.", "num_citations": "302\n", "authors": ["14"]}
{"title": "Mining social networks using heat diffusion processes for marketing candidates selection\n", "abstract": " Social Network Marketing techniques employ pre-existing social networks to increase brands or products awareness through word-of-mouth promotion. Full understanding of social network marketing and the potential candidates that can thus be marketed to certainly offer lucrative opportunities for prospective sellers. Due to the complexity of social networks, few models exist to interpret social network marketing realistically. We propose to model social network marketing using Heat Diffusion Processes. This paper presents three diffusion models, along with three algorithms for selecting the best individuals to receive marketing samples. These approaches have the following advantages to best illustrate the properties of real-world social networks:(1) We can plan a marketing strategy sequentially in time since we include a time factor in the simulation of product adoptions;(2) The algorithm of selecting marketing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "298\n", "authors": ["14"]}
{"title": "Experience report: System log analysis for anomaly detection\n", "abstract": " Anomaly detection plays an important role in management of modern large-scale distributed systems. Logs, which record system runtime information, are widely used for anomaly detection. Traditionally, developers (or operators) often inspect the logs manually with keyword search and rule matching. The increasing scale and complexity of modern systems, however, make the volume of logs explode, which renders the infeasibility of manual inspection. To reduce manual effort, many anomaly detection methods based on automated log analysis are proposed. However, developers may still have no idea which anomaly detection methods they should adopt, because there is a lack of a review and comparison among these anomaly detection methods. Moreover, even if developers decide to employ an anomaly detection method, re-implementation requires a nontrivial effort. To address these problems, we provide a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "289\n", "authors": ["14"]}
{"title": "On the intruder detection for sinkhole attack in wireless sensor networks\n", "abstract": " In a wireless sensor network, multiple nodes would send sensor readings to a base station for further processing. It is well-known that such a many-to-one communication is highly vulnerable to the sinkhole attack, where an intruder attracts surrounding nodes with unfaithful routing information, and then performs selective forwarding or alters the data passing through it. A sinkhole attack forms a serious threat to sensor networks, particularly considering that such networks are often deployed in open areas and of weak computation and battery power. In this paper, we present a novel algorithm for detecting the intruder in a sinkhole attack. The algorithm first finds a list of suspected nodes, and then effectively identifies the intruder in the list through a network flow graph. The algorithm is also robust to deal with cooperative malicious nodes that attempt to hide the real intruder. We have evaluated the performance of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "288\n", "authors": ["14"]}
{"title": "Large-scale text categorization by batch mode active learning\n", "abstract": " Large-scale text categorization is an important research topic for Web data mining. One of the challenges in large-scale text categorization is how to reduce the human efforts in labeling text documents for building reliable classification models. In the past, there have been many studies on applying active learning methods to automatic text categorization, which try to select the most informative documents for labeling manually. Most of these studies focused on selecting a single unlabeled document in each iteration. As a result, the text categorization model has to be retrained after each labeled document is solicited. In this paper, we present a novel active learning algorithm that selects a batch of text documents for labeling manually in each iteration. The key of the batch mode active learning is how to reduce the redundancy among the selected examples such that each example provides unique information for model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "286\n", "authors": ["14"]}
{"title": "A trust model based routing protocol for secure ad hoc networks\n", "abstract": " Security issues have been emphasized when mobile ad hoc networks (MANETs) are employed into military and aerospace fields. We design a novel secure routing protocol for MANETs. This protocol TAODV (Trusted AODV) extends the widely used AODV (ad hoc on-demand distance vector) routing protocol and employs the idea of a trust model to protect routing behaviors in the network layer of MANETs. In the TAODV, trust among nodes is represented by opinion, which is an item derived from subjective logic. The opinions are dynamic and updated frequently as our protocol specification: if one node performs normal communications, its opinion from other nodes' points of view can be increased; otherwise, if one node performs some malicious behaviors, it is ultimately denied by the whole network. A trust recommendation mechanism is also designed to exchange trust information among nodes. The salient\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "282\n", "authors": ["14"]}
{"title": "Improving recommender systems by incorporating social contextual information\n", "abstract": " Due to their potential commercial value and the associated great research challenges, recommender systems have been extensively studied by both academia and industry recently. However, the data sparsity problem of the involved user-item matrix seriously affects the recommendation quality. Many existing approaches to recommender systems cannot easily deal with users who have made very few ratings. In view of the exponential growth of information generated by online users, social contextual information analysis is becoming important for many Web applications. In this article, we propose a factor analysis approach based on probabilistic matrix factorization to alleviate the data sparsity and poor prediction accuracy problems by incorporating social contextual information, such as social networks and social tags. The complexity analysis indicates that our approach can be applied to very large datasets since\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "274\n", "authors": ["14"]}
{"title": "Component-based software engineering: technologies, development frameworks, and quality assurance schemes\n", "abstract": " Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is very different from the traditional approach, quality assurance (QA) for component-based software development is a new topic in the software engineering community. In this paper, we survey current component-based software technologies, describe their advantages and disadvantages, and discuss the features they inherit. We also address QA issues for component-based software. As a major contribution, we propose a QA model for component-based software which covers component requirement analysis, component development, component certification, component customization, and system architecture design, integration, testing and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "271\n", "authors": ["14"]}
{"title": "Service-generated big data and big data-as-a-service: an overview\n", "abstract": " With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "248\n", "authors": ["14"]}
{"title": "WSPred: A time-aware personalized QoS prediction framework for Web services\n", "abstract": " The exponential growth of Web service makes building high-quality service-oriented applications an urgent and crucial research problem. User-side QoS evaluations of Web services are critical for selecting the optimal Web service from a set of functionally equivalent service candidates. Since QoS performance of Web services is highly related to the service status and network environments which are variable against time, service invocations are required at different instances during a long time interval for making accurate Web service QoS evaluation. However, invoking a huge number of Web services from user-side for quality evaluation purpose is time-consuming, resource-consuming, and sometimes even impractical (e.g., service invocations are charged by service providers). To address this critical challenge, this paper proposes a Web service QoS prediction framework, called WSPred, to provide time-aware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "247\n", "authors": ["14"]}
{"title": "Learning to recommend with trust and distrust relationships\n", "abstract": " With the exponential growth of Web contents, Recommender System has become indispensable for discovering new information that might interest Web users. Despite their success in the industry, traditional recommender systems suffer from several problems. First, the sparseness of the user-item matrix seriously affects the recommendation quality. Second, traditional recommender systems ignore the connections among users, which loses the opportunity to provide more accurate and personalized recommendations. In this paper, aiming at providing more realistic and accurate recommendations, we propose a factor analysis-based optimization framework to incorporate the user trust and distrust relationships into the recommender systems. The contributions of this paper are three-fold:(1) We elaborate how user distrust information can benefit the recommender systems.(2) In terms of the trust relations, distinct from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "219\n", "authors": ["14"]}
{"title": "Achieving software quality with testing coverage measures\n", "abstract": " Coverage testing helps the tester create a thorough set of tests and gives a measure of test completeness. The concepts of coverage testing are well-described in the literature. However, there are few tools that actually implement these concepts for standard programming languages, and their realistic use on large-scale projects is rare. In this article, we describe the uses of a dataflow coverage-testing tool for C programs-called ATAC for Automatic Test Analysis for C/sup 3/-in measuring, controlling,and understanding the testing process. We present case studies of two real-world software projects using ATAC. The first study involves 12 program versions developed by a university/industry fault-tolerant software project for a critical automatic-flight-control system. The second study involves a Bellcore project of 33 program modules. These studies indicate that coverage analysis of programs during testing not only gives\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "214\n", "authors": ["14"]}
{"title": "An assessment of testing-effort dependent software reliability growth models\n", "abstract": " Over the last several decades, many Software Reliability Growth Models (SRGM) have been developed to greatly facilitate engineers and managers in tracking and measuring the growth of reliability as software is being improved. However, some research work indicates that the delayed S-shaped model may not fit the software failure data well when the testing-effort spent on fault detection is not a constant. Thus, in this paper, we first review the logistic testing-effort function that can be used to describe the amount of testing-effort spent on software testing. We describe how to incorporate the logistic testing-effort function into both exponential-type, and S-shaped software reliability models. The proposed models are also discussed under both ideal, and imperfect debugging conditions. Results from applying the proposed models to two real data sets are discussed, and compared with other traditional SRGM to show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "213\n", "authors": ["14"]}
{"title": "Towards continuous and passive authentication via touch biometrics: An experimental study on smartphones\n", "abstract": " Current smartphones generally cannot continuously authenticate users during runtime. This poses severe security and privacy threats: A malicious user can manipulate the phone if bypassing the screen lock. To solve this problem, our work adopts a continuous and passive authentication mechanism based on a user\u0393\u00c7\u00d6s touch operations on the touchscreen. Such a mechanism is suitable for smartphones, as it requires no extra hardware or intrusive user interface. We study how to model multiple types of touch data and perform continuous authentication accordingly. As a first attempt, we also investigate the fundamentals of touch operations as biometrics by justifying their distinctiveness and permanence. A onemonth experiment is conducted involving over 30 users. Our experiment results verify that touch biometrics can serve as a promising method for continuous and passive authentication.", "num_citations": "211\n", "authors": ["14"]}
{"title": "A new approach for video text detection\n", "abstract": " Text detection is fundamental to video information retrieval and indexing. Existing methods cannot handle well those texts with different contrast or embedded in a complex background. To handle these difficulties, this paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution. First, it applies edge detection and uses a low threshold to filter out definitely non-text edges. Then, a local threshold is selected to both keep low-contrast text and simplify complex background of high-contrast text. Next, two text-area enhancement operators are proposed to highlight those areas with either high edge strength or high edge density. Finally, coarse-to-fine detection locates text regions efficiently. Experimental results show that this approach is robust for contrast, font-size, font-color, language, and background complexity.", "num_citations": "211\n", "authors": ["14"]}
{"title": "A unified log-based relevance feedback scheme for image retrieval\n", "abstract": " Relevance feedback has emerged as a powerful tool to boost the retrieval performance in content-based image retrieval (CBIR). In the past, most research efforts in this field have focused on designing effective algorithms for traditional relevance feedback. Given that a CBIR system can collect and store users' relevance feedback information in a history log, an image retrieval system should be able to take advantage of the log data of users' feedback to enhance its retrieval performance. In this paper, we propose a unified framework for log-based relevance feedback that integrates the log of feedback data into the traditional relevance feedback schemes to learn effectively the correlation between low-level image features and high-level concepts. Given the error-prone nature of log data, we present a novel learning technique, named soft label support vector machine, to tackle the noisy data problem. Extensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "208\n", "authors": ["14"]}
{"title": "Formal models for expert finding on dblp bibliography data\n", "abstract": " Finding relevant experts in a specific field is often crucial for consulting, both in industry and in academia. The aim of this paper is to address the expert-finding task in a real world academic field. We present three models for expert finding based on the large-scale DBLP bibliography and Google scholar for data supplementation. The first, a novel weighted language model, models an expert candidate based on the relevance and importance of associated documents by introducing a document prior probability, and achieves much better results than the basic language model. The second, a topic-based model, represents each candidate as a weighted sum of multiple topics, whilst the third, a hybrid model, combines the language model and the topic-based model. We evaluate our system using a benchmark dataset based on human relevance judgments of how well the expertise of proposed experts matches a query\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "205\n", "authors": ["14"]}
{"title": "Software defect prediction via convolutional neural network\n", "abstract": " To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs and allocating their testing efforts. Traditional defect prediction studies mainly focus on designing hand-crafted features, which are input into machine learning classifiers to identify defective code. However, these hand-crafted features often fail to capture the semantic and structural information of programs. Such information is important in modeling program functionality and can lead to more accurate defect prediction. In this paper, we propose a framework called Defect Prediction via Convolutional Neural Network (DP-CNN), which leverages deep learning for effective feature generation. Specifically, based on the programs' Abstract Syntax Trees (ASTs), we first extract token vectors, which are then encoded as numerical vectors via mapping and word embedding. We feed the numerical vectors into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "201\n", "authors": ["14"]}
{"title": "STELLAR: Spatial-temporal latent ranking for successive point-of-interest recommendation\n", "abstract": " Successive point-of-interest (POI) recommendation in location-based social networks (LBSNs) becomes a significant task since it helps users to navigate a number of candidate POIs and provides the best POI recommendations based on users\u0393\u00c7\u00d6 most recent check-in knowledge. However, all existing methods for successive POI recommendation only focus on modeling the correlation between POIs based on users\u0393\u00c7\u00d6 check-in sequences, but ignore an important fact that successive POI recommendation is a time-subtle recommendation task. In fact, even with the same previous check-in information, users would prefer different successive POIs at different time. To capture the impact of time on successive POI recommendation, in this paper, we propose a spatial-temporal latent ranking (STELLAR) method to explicitly model the interactions among user, POI, and time. In particular, the proposed STELLAR model is built upon a ranking-based pairwise tensor factorization framework with a fine-grained modeling of user-POI, POI-time, and POI-POI interactions for successive POI recommendation. Moreover, we propose a new interval-aware weight utility function to differentiate successive check-ins\u0393\u00c7\u00d6 correlations, which breaks the time interval constraint in prior work. Evaluations on two real-world datasets demonstrate that the STELLAR model outperforms state-of-the-art successive POI recommendation model about 20% in Precision@ 5 and Recall@ 5.", "num_citations": "199\n", "authors": ["14"]}
{"title": "Framework for modeling software reliability, using various testing-efforts and fault-detection rates\n", "abstract": " This paper proposes a new scheme for constructing software reliability growth models (SRGM) based on a nonhomogeneous Poisson process (NHPP). The main focus is to provide an efficient parametric decomposition method for software reliability modeling, which considers both testing efforts and fault detection rates (FDR). In general, the software fault detection/removal mechanisms depend on previously detected/removed faults and on how testing efforts are used. From practical field studies, it is likely that we can estimate the testing efforts consumption pattern and predict the trends of FDR. A set of time-variable, testing-effort-based FDR models were developed that have the inherent flexibility of capturing a wide range of possible fault detection trends: increasing, decreasing, and constant. This scheme has a flexible structure and can model a wide spectrum of software development environments, considering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "198\n", "authors": ["14"]}
{"title": "Web service recommendation via exploiting location and QoS information\n", "abstract": " Web services are integrated software components for the support of interoperable machine-to-machine interaction over a network. Web services have been widely employed for building service-oriented applications in both industry and academia in recent years. The number of publicly available Web services is steadily increasing on the Internet. However, this proliferation makes it hard for a user to select a proper Web service among a large amount of service candidates. An inappropriate service selection may cause many problems (e.g., ill-suited performance) to the resulting applications. In this paper, we propose a novel collaborative filtering-based Web service recommender system to help users select services with optimal Quality-of-Service (QoS) performance. Our recommender system employs the location information and QoS values to cluster users and services, and makes personalized service\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "191\n", "authors": ["14"]}
{"title": "Drain: An online log parsing approach with fixed depth tree\n", "abstract": " Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format; and then apply data mining models to extract critical system behavior information, which can assist Web service management. Most of the existing log parsing methods focus on offline, batch processing of logs. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "190\n", "authors": ["14"]}
{"title": "Optimal release time for software systems considering cost, testing-effort, and test efficiency\n", "abstract": " In this paper, we study the impact of software testing effort & efficiency on the modeling of software reliability, including the cost for optimal release time. This paper presents two important issues in software reliability modeling & software reliability economics: testing effort, and efficiency. First, we propose a generalized logistic testing-effort function that enjoys the advantage of relating work profile more directly to the natural flow of software development, and can be used to describe the possible testing-effort patterns. Furthermore, we incorporate the generalized logistic testing-effort function into software reliability modeling, and evaluate its fault-prediction capability through several numerical experiments based on real data. Secondly, we address the effects of new testing techniques or tools for increasing the efficiency of software testing. Based on the proposed software reliability model, we present a software cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "190\n", "authors": ["14"]}
{"title": "Predicting protein interaction sites from residue spatial sequence profile and evolution rate\n", "abstract": " This paper proposes a novel method that can predict protein interaction sites in heterocomplexes using residue spatial sequence profile and evolution rate approaches. The former represents the information of multiple sequence alignments while the latter corresponds to a residue\u0393\u00c7\u00d6s evolutionary conservation score based on a phylogenetic tree. Three predictors using a support vector machines algorithm are constructed to predict whether a surface residue is a part of a protein\u0393\u00c7\u00f4protein interface. The efficiency and the effectiveness of our proposed approach is verified by its better prediction performance compared with other models. The study is based on a non-redundant data set of heterodimers consisting of 69 protein chains.", "num_citations": "187\n", "authors": ["14"]}
{"title": "Learning to recommend with explicit and implicit social relations\n", "abstract": " Recommender systems have been well studied and developed, both in academia and in industry recently. However, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real-world observations where we always turn to our trusted friends for recommendations. Aiming at modeling recommender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework which naturally fuses the users' tastes and their trusted friends' favors together. The proposed framework is quite general, and it can also be applied to pure user-item rating matrix even if we do not have explicit social trust information among users. In this framework, we coin the term social trust ensemble to represent the formulation of the social trust restrictions on the recommender\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "183\n", "authors": ["14"]}
{"title": "Exploring latent features for memory-based QoS prediction in cloud computing\n", "abstract": " With the increasing popularity of cloud computing as a solution for building high-quality applications on distributed components, efficiently evaluating user-side quality of cloud components becomes an urgent and crucial research problem. However, invoking all the available cloud components from user-side for evaluation purpose is expensive and impractical. To address this critical challenge, we propose a neighborhood-based approach, called CloudPred, for collaborative and personalized quality prediction of cloud components. CloudPred is enhanced by feature modeling on both users and components. Our approach CloudPred requires no additional invocation of cloud components on behalf of the cloud application designers. The extensive experimental results show that CloudPred achieves higher QoS prediction accuracy than other competing methods. We also publicly release our large-scale QoS dataset\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "180\n", "authors": ["14"]}
{"title": "Wsexpress: A qos-aware search engine for web services\n", "abstract": " Web services are becoming prevalent nowadays. Finding desired Web services is becoming an emergent and challenging research problem. In this paper, we present WSExpress (Web Service Express), a novel Web service search engine to expressively find expected Web services. WSExpress ranks the publicly available Web services not only by functional similarities to users' queries, but also by nonfunctional QoS characteristics of Web services. WSExpress provides three searching styles, which can adapt to the scenario of finding an appropriate Web service and the scenario of automatically replacing a failed Web service with a suitable one. WSExpress is implemented by Java language and large-scale experiments employing real-world Web services are conducted. Totally 3,738 Web services (15,811 operations) from 69 countries are involved in our experiments. The experimental results show that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "174\n", "authors": ["14"]}
{"title": "A generalized co-hits algorithm and its application to bipartite graphs\n", "abstract": " Recently many data types arising from data mining and Web search applications can be modeled as bipartite graphs. Examples include queries and URLs in query logs, and authors and papers in scientific literature. However, one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph. There is a lack of constraints to make sure the final relevance of the score propagation on the graph, as there are many noisy edges within the bipartite graph. In this paper, we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance. Moreover, we investigate the algorithm based on two frameworks, including the iterative and the regularization frameworks, and illustrate the generalized Co-HITS algorithm from different views. For the iterative framework, it\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "172\n", "authors": ["14"]}
{"title": "Learning to log: Helping developers make informed logging decisions\n", "abstract": " Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a \"learning to log\" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "168\n", "authors": ["14"]}
{"title": "Tools and benchmarks for automated log parsing\n", "abstract": " Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "167\n", "authors": ["14"]}
{"title": "Localized support vector regression for time series prediction\n", "abstract": " Time series prediction, especially financial time series prediction, is a challenging task in machine learning. In this issue, the data are usually non-stationary and volatile in nature. Because of its good generalization power, the support vector regression (SVR) has been widely applied in this application. The standard SVR employs a fixed \u256c\u2561-tube to tolerate noise and adopts the \u0393\u00e4\u00f4 p-norm (p= 1 or 2) to model the functional complexity of the whole data set. One problem of the standard SVR is that it considers data in a global fashion only. Therefore it may lack the flexibility to capture the local trend of data; this is a critical aspect of volatile data, especially financial time series data. Aiming to attack this issue, we propose the localized support vector regression (LSVR) model. This novel model is demonstrated to provide a systematic and automatic scheme to adapt the margin locally and flexibly; while the margin in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "164\n", "authors": ["14"]}
{"title": "A novel method for early software quality prediction based on support vector machine\n", "abstract": " The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "163\n", "authors": ["14"]}
{"title": "An efficient intruder detection algorithm against sinkhole attacks in wireless sensor networks\n", "abstract": " In a wireless sensor network, multiple nodes would send sensor readings to a base station for further processing. It is known that such a many-to-one communication is highly vulnerable to a sinkhole attack, where an intruder attracts surrounding nodes with unfaithful routing information, and then performs selective forwarding or alters the data passing through it. A sinkhole attack forms a serious threat to sensor networks, particularly considering that the sensor nodes are often deployed in open areas and of weak computation and battery power.In this paper, we present a novel algorithm for detecting the intruder in a sinkhole attack. The algorithm first finds a list of suspected nodes through checking data consistency, and then effectively identifies the intruder in the list through analyzing the network flow information. The algorithm is also robust to deal with multiple malicious nodes that cooperatively hide the real\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "160\n", "authors": ["14"]}
{"title": "Bridging the semantic gap between image contents and tags\n", "abstract": " With the exponential growth of Web 2.0 applications, tags have been used extensively to describe the image contents on the Web. Due to the noisy and sparse nature in the human generated tags, how to understand and utilize these tags for image retrieval tasks has become an emerging research direction. As the low-level visual features can provide fruitful information, they are employed to improve the image retrieval results. However, it is challenging to bridge the semantic gap between image contents and tags. To attack this critical problem, we propose a unified framework in this paper which stems from a two-level data fusions between the image contents and tags: 1) A unified graph is built to fuse the visual feature-based image similarity graph with the image-tag bipartite graph; 2) A novel random walk model is then proposed, which utilizes a fusion parameter to balance the influences between the image\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "158\n", "authors": ["14"]}
{"title": "Reliability simulation of component-based software systems\n", "abstract": " Prevalent Markovian and semi Markovian methods to predict the reliability and performance of component based heterogeneous systems suffer from several limitations: they are subject to an intractably large state space for more complex scenarios, and they cannot take into account the influence of various parameters such as reliability growth of individual components, dependencies among components, etc., in a single model. Discrete event simulation offers an alternative to analytical models as it can capture a detailed system structure, and can be used to study the influence of different factors separately as well as in a combined fashion on dependability measures. We demonstrate the flexibility offered by discrete event simulation to analyze such complex systems through two case studies, one of a terminating application, and the other of a real time application with feedback control. We simulate the failure\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "158\n", "authors": ["14"]}
{"title": "Learning nonparametric kernel matrices from pairwise constraints\n", "abstract": " Many kernel learning methods have to assume parametric forms for the target kernel functions, which significantly limits the capability of kernels in fitting diverse patterns. Some kernel learning methods assume the target kernel matrix to be a linear combination of parametric kernel matrices. This assumption again importantly limits the flexibility of the target kernel matrices. The key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernels to the input patterns. In this paper, we resolve this problem by introducing the graph Laplacian of the observed data as a regularizer when optimizing the kernel matrix with respect to the pairwise constraints. We formulate the problem into Semi-Definite Programs (SDP), and propose an efficient algorithm to solve the SDP problem. The extensive evaluation on clustering with pairwise constraints shows that the proposed nonparametric\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "154\n", "authors": ["14"]}
{"title": "An evaluation study on log parsing and its use in log mining\n", "abstract": " Logs, which record runtime information of modern systems, are widely utilized by developers (and operators) in system development and maintenance. Due to the ever-increasing size of logs, data mining models are often adopted to help developers extract system behavior information. However, before feeding logs into data mining models, logs need to be parsed by a log parser because of their unstructured format. Although log parsing has been widely studied in recent years, users are still unaware of the advantages of different log parsers nor the impact of them on subsequent log mining tasks. Thus they often re-implement or even re-design a new log parser, which would be time-consuming yet redundant. To address this issue, in this paper, we study four log parsers and package them into a toolkit to allow their reuse. In addition, we obtain six insightful findings by evaluating the performance of the log parsers\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "153\n", "authors": ["14"]}
{"title": "Geo-teaser: Geo-temporal sequential embedding rank for point-of-interest recommendation\n", "abstract": " Point-of-interest (POI) recommendation is an important application for location-based social networks (LBSNs), which learns the user preference and mobility pattern from check-in sequences to recommend POIs. Previous studies show that modeling the sequential pattern of user check-ins is necessary for POI recommendation. Markov chain model, recurrent neural network, and the word2vec framework are used to model check-in sequences in previous work. However, all previous sequential models ignore the fact that check-in sequences on different days naturally exhibit the various temporal characteristics, for instance,\" work\" on weekday and\" entertainment\" on weekend. In this paper, we take this challenge and propose a Geo-Temporal sequential embedding rank (Geo-Teaser) model for POI recommendation. Inspired by the success of the word2vec framework to model the sequential contexts, we propose a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "147\n", "authors": ["14"]}
{"title": "Cloudrank: A qos-driven component ranking framework for cloud computing\n", "abstract": " The rising popularity of cloud computing makes building high quality cloud applications a critical and urgently required research problem. Component quality ranking approaches are crucial for making optimal component selection from a set of functionally equivalent component candidates. Moreover, quality ranking of cloud components helps the application designers detect the poor performing components in the complex cloud applications, which usually include huge number of distributed components. To provide personalized cloud component ranking for different designers of cloud applications, this paper proposes a QoS-driven component ranking framework for cloud applications by taking advantage of the past component usage experiences of different component users. Our approach requires no additional invocations of the cloud components on behalf of the application designers. The extensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "146\n", "authors": ["14"]}
{"title": "Component ranking for fault-tolerant cloud applications\n", "abstract": " Cloud computing is becoming a mainstream aspect of information technology. More and more enterprises deploy their software systems in the cloud environment. The cloud applications are usually large scale and include a lot of distributed cloud components. Building highly reliable cloud applications is a challenging and critical research problem. To attack this challenge, we propose a component ranking framework, named FTCloud, for building fault-tolerant cloud applications. FTCloud includes two ranking algorithms. The first algorithm employs component invocation structures and invocation frequencies for making significant component ranking. The second ranking algorithm systematically fuses the system structure information as well as the application designers' wisdom to identify the significant components in a cloud application. After the component ranking phase, an algorithm is proposed to automatically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "143\n", "authors": ["14"]}
{"title": "Optimal allocation of test resources for software reliability growth modeling in software development\n", "abstract": " A component-based software development approach has become a trend in integrating modern software systems. To ensure the overall reliability of an integrated software system, its software components have to meet certain reliability requirements, subject to some testing schedule and resource constraints. Efficiency improvement of the system-testing can be formulated as a combinatorial optimization problem with known cost, reliability, effort and other attributes of the system components. This paper considers \"software component testing resource allocation\" for a system with single or multiple applications, each with a pre-specified reliability requirement. The relation between failure rates of components and \"cost to decrease this rate\" is modeled by various types of reliability-growth curves. Closed-form solutions to the problem for systems with one single application are developed, and then \"how to solve the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "141\n", "authors": ["14"]}
{"title": "Effect of code coverage on software reliability measurement\n", "abstract": " Existing software reliability-growth models often over-estimate the reliability of a given program. Empirical studies suggest that the over-estimations exist because the models do not account for the nature of the testing. Every testing technique has a limit to its ability to reveal faults in a given system. Thus, as testing continues in its region of saturation, no more faults are discovered and inaccurate reliability-growth phenomena are predicted from the models. This paper presents a technique intended to solve this problem, using both time and code coverage measures for the prediction of software failures in operation. Coverage information collected during testing is used only to consider the effective portion of the test data. Execution time between test cases, which neither increases code coverage nor causes a failure, is reduced by a parameterized factor. Experiments were conducted to evaluate this technique, on a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "141\n", "authors": ["14"]}
{"title": "A novel log-based relevance feedback technique in content-based image retrieval\n", "abstract": " Relevance feedback has been proposed as an important technique to boost the retrieval performance in content-based image retrieval (CBIR). However, since there exists a semantic gap between low-level features and high-level semantic concepts in CBIR, typical relevance feedback techniques need to perform a lot of rounds of feedback for achieving satisfactory results. These procedures are time-consuming and may make the users bored in the retrieval tasks. For a long-term study purpose in CBIR, we notice that the users' feedback logs can be available and employed for helping the retrieval tasks in CBIR systems. In this paper, we propose a novel scheme to study the log-based relevance feedback (LRF) technique for improving retrieval performance and reducing the semantic gap in CBIR. In order to effectively incorporate the users' feedback logs, we propose a modified support vector machine (SVM\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "138\n", "authors": ["14"]}
{"title": "Learning classifiers from imbalanced data based on biased minimax probability machine\n", "abstract": " We consider the problem of the binary classification on imbalanced data, in which nearly all the instances are labelled as one class, while far fewer instances are labelled as the other class, usually the more important class. Traditional machine learning methods seeking an accurate performance over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into the majority, usually the less important class. Moreover, some current methods have tried to utilize some intermediate factors, e.g., the distribution of the training set, the decision thresholds or the cost matrices, to influence the bias of the classification. However, it remains uncertain whether these methods can improve the performance in a systematic way. In this paper, we propose a novel model named biased minimax probability machine. Different from previous methods, this model directly controls the worst\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["14"]}
{"title": "Analyzing and predicting question quality in community question answering services\n", "abstract": " Users tend to ask and answer questions in community question answering (CQA) services to seek information and share knowledge. A corollary is that myriad of questions and answers appear in CQA service. Accordingly, volumes of studies have been taken to explore the answer quality so as to provide a preliminary screening for better answers. However, to our knowledge, less attention has so far been paid to question quality in CQA. Knowing question quality provides us with finding and recommending good questions together with identifying bad ones which hinder the CQA service. In this paper, we are conducting two studies to investigate the question quality issue. The first study analyzes the factors of question quality and finds that the interaction between askers and topics results in the differences of question quality. Based on this finding, in the second study we propose a Mutual Reinforcement-based Label\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "132\n", "authors": ["14"]}
{"title": "Learning latent semantic relations from clickthrough data for query suggestion\n", "abstract": " For a given query raised by a specific user, the Query Suggestion technique aims to recommend relevant queries which potentially suit the information needs of that user. Due to the complexity of the Web structure and the ambiguity of users' inputs, most of the suggestion algorithms suffer from the problem of poor recommendation accuracy. In this paper, aiming at providing semantically relevant queries for users, we develop a novel, effective and efficient two-level query suggestion model by mining clickthrough data, in the form of two bipartite graphs (user-query and query-URL bipartite graphs) extracted from the clickthrough data. Based on this, we first propose a joint matrix factorization method which utilizes two bipartite graphs to learn the low-rank query latent feature space, and then build a query similarity graph based on the features. After that, we design an online ranking algorithm to propagate similarities on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "131\n", "authors": ["14"]}
{"title": "PORT: a price-oriented reliable transport protocol for wireless sensor networks\n", "abstract": " In wireless sensor networks, to obtain reliability and minimize energy consumption, a dynamic rate-control and congestion-avoidance transport scheme is very important. We notice that reporting packets may contribute to the sink's fidelity of its knowledge on the phenomenon of interest to different extents. Thus, reliability cannot simply be measured by the sink's total incoming packet rate as considered in current schemes. Also, communication costs between sources and the sink may be different and may change dynamically. Based on these considerations, we propose PORT (price-oriented reliable transport protocol) to facilitate the sink to achieve reliability. Under the constraint that the sink must obtain enough fidelity for reliability purpose, PORT minimizes energy consumption with two schemes. One is based on the sink's application-based optimization approach that feeds back the optimal reporting rates. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "130\n", "authors": ["14"]}
{"title": "BFTCloud: A byzantine fault tolerance framework for voluntary-resource cloud computing\n", "abstract": " Cloud computing is becoming a popular and important solution for building highly reliable applications on distributed resources. However, it is a critical challenge to guarantee the system reliability of applications especially in voluntary-resource cloud due to the highly dynamic environment. In this paper, we present BFT Cloud (Byzantine Fault Tolerant Cloud), a Byzantine fault tolerance framework for building robust systems involuntary-resource cloud environments. BFT Cloud guarantees robustness of systems when up to f of totally 3f+1 resource providers are faulty, including crash faults, arbitrary behaviors faults, etc. BFT Cloud is evaluated in a large-scale real-world experiment which consists of 257 voluntary-resource providers located in 26 countries. The experimental results shows that BFT Cloud guarantees high reliability of systems built on the top of voluntary-resource cloud infrastructure and ensures good\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "127\n", "authors": ["14"]}
{"title": "A novel scheme for hybrid digital video watermarking: approach, evaluation and experimentation\n", "abstract": " We have seen an explosion of data exchange in the Internet and the extensive use of digital media. Consequently, digital data owners can quickly and massively transfer multimedia documents across the Internet. This leads to wide interest in multimedia security and multimedia copyright protection. We propose a novel hybrid digital video watermarking scheme based on the scene change analysis and error correction code. Our video watermarking algorithm is robust against the attacks of frame dropping, averaging and statistical analysis, which were not solved effectively in the past. We start with a complete survey of current watermarking technologies, and noticed that none of the existing schemes is capable of resisting all attacks. Accordingly, we propose the idea of embedding different parts of a single watermark into different scenes of a video. We then analyze the strengths of different watermarking schemes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "127\n", "authors": ["14"]}
{"title": "Firewall security: Policies, testing and performance evaluation\n", "abstract": " Explores the firewall security and performance relationships for distributed systems. Experiments are conducted to set firewall security into seven different levels and to quantify their performance impacts. These firewall security levels are formulated, designed, implemented and tested, phase by phase, under an experimental environment in which all performed tests are evaluated and compared. Based on the test results, the impacts of the various firewall security levels on system performance with respect to transaction time and latency are measured and analyzed. It is interesting to note that the intuitive belief about security's relationship to performance, i.e. that more security would result in less performance, does not always hold in firewall testing. The results reveal that a significant impact of enhanced security on performance could only be observed under some particular scenarios, and thus their relationship is not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "125\n", "authors": ["14"]}
{"title": "Near-duplicate keyframe retrieval by nonrigid image matching\n", "abstract": " Near-duplicate image retrieval plays an important role in many real-world multimedia applications. Most previous approaches have some limitations. For example, conventional appearance-based methods may suffer from the illumination variations and occlusion issue, and local feature correspondence-based methods often do not consider local deformations and the spatial coherence between two point sets. In this paper, we propose a novel and effective Nonrigid Image Matching (NIM) approach to tackle the task of near-duplicate keyframe retrieval from real-world video corpora. In contrast to previous approaches, the NIM technique can recover an explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data effectively. To make our technique applicable to large-scale applications, we suggest an effective multi-level ranking\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "122\n", "authors": ["14"]}
{"title": "Cloud service reliability enhancement via virtual machine placement optimization\n", "abstract": " With rapid adoption of the cloud computing model, many enterprises have begun deploying cloud-based services. Failures of virtual machines (VMs) in clouds have caused serious quality assurance issues for those services. VM replication is a commonly used technique for enhancing the reliability of cloud services. However, when determining the VM redundancy strategy for a specific service, many state-of-the-art methods ignore the huge network resource consumption issue that could be experienced when the service is in failure recovery mode. This paper proposes a redundant VM placement optimization approach to enhancing the reliability of cloud services. The approach employs three algorithms. The first algorithm selects an appropriate set of VM-hosting servers from a potentially large set of candidate host servers based upon the network topology. The second algorithm determines an optimal strategy to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["14"]}
{"title": "A classification-based approach to question routing in community question answering\n", "abstract": " Community-based Question and Answering (CQA) services have brought users to a new era of knowledge dissemination by allowing users to ask questions and to answer other users' questions. However, due to the fast increasing of posted questions and the lack of an effective way to find interesting questions, there is a serious gap between posted questions and potential answerers. This gap may degrade a CQA service's performance as well as reduce users' loyalty to the system. To bridge the gap, we present a new approach to Question Routing, which aims at routing questions to participants who are likely to provide answers. We consider the problem of question routing as a classification task, and develop a variety of local and global features which capture different aspects of questions, users, and their relations. Our experimental results obtained from an evaluation over the Yahoo!~ Answers dataset\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["14"]}
{"title": "Trust-and clustering-based authentication services in mobile ad hoc networks\n", "abstract": " A mobile ad hoc network is a kind of wireless communication network that does not rely on a fixed infrastructure and is lack of any centralized control. These characteristics make it vulnerable to security attack, so protecting the security of the network is essential. Like many distributed systems, security in ad hoc networks widely relies on the use of key management mechanisms. However, traditional key management systems are not appropriate for them. We aim at providing a secure and distributed authentication service in ad hoc networks. We propose a secure public key authentication service based on our trust model and network model to prevent nodes from obtaining false public keys of the others when there are malicious nodes in the network. We perform an overall evaluation of our proposed approach by simulations. The experimental results indicate clear advantages of our approach in providing effective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "119\n", "authors": ["14"]}
{"title": "Reputation measurement and malicious feedback rating prevention in web service recommendation systems\n", "abstract": " Web service recommendation systems can help service users to locate the right service from the large number of available web services. Avoiding recommending dishonest or unsatisfactory services is a fundamental research problem in the design of web service recommendation systems. Reputation of web services is a widely-employed metric that determines whether the service should be recommended to a user. The service reputation score is usually calculated using feedback ratings provided by users. Although the reputation measurement of web service has been studied in the recent literature, existing malicious and subjective user feedback ratings often lead to a bias that degrades the performance of the service recommendation system. In this paper, we propose a novel reputation measurement approach for web service recommendations. We first detect malicious feedback ratings by adopting the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["14"]}
{"title": "Estimation and analysis of some generalized multiple change-point software reliability models\n", "abstract": " Software typically undergoes debugging during both a testing phase before product release, and an operational phase after product release. But it is noted that the fault detection and removal processes during software development and operation are different. For example, the fault removal during operation occurs generally at a slower pace than development. In this paper, we derive a powerful, easily deployable technique for software reliability prediction and assessment in the testing and operational phases. We first review how several existing software reliability growth models (SRGM) based on non- homogeneous Poisson processes (NHPP) can be readily derived from a unified theory. With the unified theory, we further incorporate the concept of multiple change-points, i.e. points in time when the software environment changes, into software reliability modeling. Several models are proposed and discussed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["14"]}
{"title": "Clustering web services to facilitate service discovery\n", "abstract": " Clustering Web services would greatly boost the ability of Web service search engine to retrieve relevant services. The performance of traditional Web service description language (WSDL)-based Web service clustering is not satisfied, due to the singleness of data source. Recently, Web service search engines such as Seekda! allow users to manually annotate Web services using tags, which describe functions of Web services or provide additional contextual and semantical information. In this paper, we cluster Web services by utilizing both WSDL documents and tags. To handle the clustering performance limitation caused by uneven tag distribution and noisy tags, we propose a hybrid Web service tag recommendation strategy, named WSTRec, which employs tag co-occurrence, tag mining, and semantic relevance measurement for tag recommendation. Extensive experiments are conducted based on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "116\n", "authors": ["14"]}
{"title": "A Hough transform based line recognition method utilizing both parameter space and image space\n", "abstract": " Hough Transform (HT) is recognized as a powerful tool for graphic element extraction from images due to its global vision and robustness in noisy or degraded environment. However, the application of HT has been limited to small-size images for a long time. Besides the well-known heavy computation in the accumulation, the peak detection and the line verification become much more time-consuming for large-size images. Another limitation is that most existing HT-based line recognition methods are not able to detect line thickness, which is essential to large-size images, usually engineering drawings. We believe these limitations arise from that these methods only work on the HT parameter space. This paper therefore proposes a new HT-based line recognition method, which utilizes both the HT parameter space and the image space. The proposed method devises an image-based gradient prediction to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "112\n", "authors": ["14"]}
{"title": "Toward fine-grained, unsupervised, scalable performance diagnosis for production cloud computing systems\n", "abstract": " Performance diagnosis is labor intensive in production cloud computing systems. Such systems typically face many real-world challenges, which the existing diagnosis techniques for such distributed systems cannot effectively solve. An efficient, unsupervised diagnosis tool for locating fine-grained performance anomalies is still lacking in production cloud computing systems. This paper proposes CloudDiag to bridge this gap. Combining a statistical technique and a fast matrix recovery algorithm, CloudDiag can efficiently pinpoint fine-grained causes of the performance problems, which does not require any domain-specific knowledge to the target system. CloudDiag has been applied in a practical production cloud computing systems to diagnose performance problems. We demonstrate the effectiveness of CloudDiag in three real-world case studies.", "num_citations": "111\n", "authors": ["14"]}
{"title": "Diffusionrank: a possible penicillin for web spamming\n", "abstract": " While the PageRank algorithm has proven to be very effective for ranking Web pages, the rank scores of Web pages can be manipulated. To handle the manipulation problem and to cast a new insight on the Web structure, we propose a ranking algorithm called DiffusionRank. DiffusionRank is motivated by the heat diffusion phenomena, which can be connected to Web ranking because the activities flow on the Web can be imagined as heat flow, the link from a page to another can be treated as the pipe of an air-conditioner, and heat flow can embody the structure of the underlying Web graph. Theoretically we show that DiffusionRank can serve as a generalization of PageRank when the heat diffusion co-efficient \u256c\u2502 tends to infinity. In such a case 1= \u256c\u2502= 0, DiffusionRank (PageRank) has low ability of anti-manipulation. When \u256c\u2502= 0, DiffusionRank obtains the highest ability of anti-manipulation, but in such a case, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "111\n", "authors": ["14"]}
{"title": "A semi-supervised active learning framework for image retrieval\n", "abstract": " Although recent studies have shown that unlabeled data are beneficial to boosting the image retrieval performance, very few approaches for image retrieval can learn with labeled and unlabeled data effectively. This paper proposes a novel semi-supervised active learning framework comprising a fusion of semi-supervised learning and support vector machines. We provide theoretical analysis of the active learning framework and present a simple yet effective active learning algorithm for image retrieval. Experiments are conducted on real-world color images to compare with traditional methods. The promising experimental results show that our proposed scheme significantly outperforms the previous approaches.", "num_citations": "111\n", "authors": ["14"]}
{"title": "The effect of code coverage on fault detection under different testing profiles\n", "abstract": " Software testing is a key procedure to ensure high quality and reliability of software programs. The key issue in software testing is the selection and evaluation of different test cases. Code coverage has been proposed to be an estimator for testing effectiveness, but it remains a controversial topic which lacks of support from empirical data. In this study, we hypothesize that the estimation of code coverage on testing effectiveness varies under different testing profiles. To evaluate the performance of code coverage, we employ coverage testing and mutation testing in our experiment to investigate the relationship between code coverage and fault detection capability under different testing profiles. From our experimental data, code coverage is simply a moderate indicator for the capability of fault detection on the whole test set. However, it is clearly a good estimator for the fault detection of exceptional test cases, but a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "109\n", "authors": ["14"]}
{"title": "Towards automated log parsing for large-scale log data analysis\n", "abstract": " Logs are widely used in system management for dependability assurance because they are often the only data available that record detailed system runtime behaviors in production. Because the size of logs is constantly increasing, developers (and operators) intend to automate their analysis by applying data mining methods, therefore structured input data (e.g., matrices) are required. This triggers a number of studies on log parsing that aims to transform free-text log messages into structured events. However, due to the lack of open-source implementations of these log parsers and benchmarks for performance comparison, developers are unlikely to be aware of the effectiveness of existing log parsers and their limitations when applying them into practice. They must often reimplement or redesign one, which is time-consuming and redundant. In this paper, we first present a characterization study of the current state\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "107\n", "authors": ["14"]}
{"title": "Personalized reliability prediction of web services\n", "abstract": " Service Oriented Architecture (SOA) is a business-centric IT architectural approach for building distributed systems. Reliability of service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet connections. Designing efficient and effective reliability prediction approaches of Web services has become an important research issue. In this article, we propose two personalized reliability prediction approaches of Web services, that is, neighborhood-based approach and model-based approach. The neighborhood-based approach employs past failure data of similar neighbors (either service users or Web services) to predict the Web service reliability. On the other hand, the model-based approach fits a factor model based on the available Web service failure data and use this factor model to make further reliability prediction. Extensive experiments are conducted with our real\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "107\n", "authors": ["14"]}
{"title": "Batch mode active learning with applications to text categorization and image retrieval\n", "abstract": " Most machine learning tasks in data classification and information retrieval require manually labeled data examples in the training stage. The goal of active learning is to select the most informative examples for manual labeling in these learning tasks. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient, since the classification model has to be retrained for every acquired labeled example. It is also inappropriate for the setup of information retrieval tasks where the user's relevance feedback is often provided for the top K retrieved items. In this paper, we present a framework for batch mode active learning, which selects a number of informative examples for manual labeling in each iteration. The key feature of batch mode active learning is to reduce the redundancy among the selected examples such that each example provides\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["14"]}
{"title": "A simulation approach to structure-based software reliability analysis\n", "abstract": " Structure-based techniques enable an analysis of the influence of individual components on the application reliability. In an effort to ensure analytical tractability, prevalent structure-based analysis techniques are based on assumptions which preclude the use of these techniques for reliability analysis during the testing and operational phases. In this paper, we develop simulation procedures to assess the impact of individual components on the reliability of an application in the presence of fault detection and repair strategies that may be employed during testing. We also develop simulation procedures to analyze the application reliability for various operational configurations. We illustrate the potential of simulation procedures using several examples. Based on the results of these examples, we provide novel insights into how testing and repair strategies can be tailored depending on the application structure to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["14"]}
{"title": "In search of effective diversity: a six-language study of fault-tolerant flight control software\n", "abstract": " Multi-version software systems achieve fault tolerance through software redundancy and diversity. In order to investigate this approach, this joint UCLA/Honeywell research project investigated multi-version software systems, employing six different programming languages to create six versions of software for an automatic landing program. The rationale, preparation, execution, and evaluation of this investigation are reported.", "num_citations": "105\n", "authors": ["14"]}
{"title": "A DWT-based digital video watermarking scheme with error correcting code\n", "abstract": " In this paper, a digital video watermarking algorithm is proposed. We present a novel DWT-based blind digital video watermarking scheme with scrambled watermark and error correcting code. Our scheme embeds different parts of a single watermark into different scenes of a video under the wavelet domain. To increase robustness of the scheme, the watermark is refined by the error correcting code, while the correcting code is embedded as watermark in audio channel. Our video watermarking algorithm is robust against the attacks of frame dropping, averaging and statistical analysis, which were not solved effectively in the past. Furthermore, it allows blind retrieval of embedded watermark which does not need the original video; and the watermark is perceptually invisible. The algorithm design, evaluation, and experimentation of the proposed scheme are described in this paper.", "num_citations": "101\n", "authors": ["14"]}
{"title": "Improving the n-version programming process through the evolution of a design paradigm\n", "abstract": " The application of the N-version programming (NVP) technique in a project that reused the revised specification of a real, automatic airplane landing problem. The study involved 40 students, who formed 15 independent programming teams to design, program, test, and evaluate the application. The impact of the paradigm on the software development process, the improvement of the resulting N-version software (NVS) product, the insight, experience, and learning in conducting this project, various testing procedures applied to the program versions, several quantitative measures of the resulting NVS product, and some comparisons with previous projects are discussed. The effectiveness of the revised NVP design paradigm in improving software reliability by providing fault tolerance is demonstrated.< >", "num_citations": "101\n", "authors": ["14"]}
{"title": "Location-based hierarchical matrix factorization for web service recommendation\n", "abstract": " Web service recommendation is of great importance when users face a large number of functionally-equivalent candidate services. To recommend Web services that best fit a user's need, QoS values which characterize the non-functional properties of those candidate services are in demand. But in reality, the QoS information of Web service is not easy to obtain, because only limited historical invocation records exist. To tackle this challenge, in recent literature, a number of QoS prediction methods are proposed, but they still demonstrate disadvantages on prediction accuracy. In this paper, we design a location-based hierarchical matrix factorization (HMF) method to perform personalized QoS prediction, whereby effective service recommendation can be made. We cluster users and services into several user-service groups based on their location information, each of which contains a small set of users and services\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["14"]}
{"title": "A social recommendation framework based on multi-scale continuous conditional random fields\n", "abstract": " This paper addresses the issue of social recommendation based on collaborative filtering (CF) algorithms. Social recommendation emphasizes utilizing various attributes information and relations in social networks to assist recommender systems. Although recommendation techniques have obtained distinct developments over the decades, traditional CF algorithms still have these following two limitations:(1) relational dependency within predictions, an important factor especially when the data is sparse, is not being utilized effectively; and (2) straightforward methods for combining features like linear integration suffer from high computing complexity in learning the weights by enumerating the whole value space, making it difficult to combine various information into an unified approach. In this paper, we propose a novel model, Multi-scale Continuous Conditional Random Fields (MCCRF), as a framework to solve\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["14"]}
{"title": "A qos-aware fault tolerant middleware for dependable service composition\n", "abstract": " Based on the framework of service-oriented architecture (SOA), complex distributed systems can be dynamically and automatically composed by integrating distributed Web services provided by different organizations, making dependability of the distributed SOA systems a big challenge. In this paper, we propose a QoS-aware fault tolerant middleware to attack this critical problem. Our middleware includes a user-collaborated QoS model, various fault tolerance strategies, and a context-aware algorithm in determining optimal fault tolerance strategy for both stateless and stateful Web services. The benefits of the proposed middleware are demonstrated by experiments, and the performance of the optimal fault tolerance strategy selection algorithm is investigated extensively. As illustrated by the experimental results, fault tolerance for the distributed SOA systems can be efficient, effective and optimized by the proposed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["14"]}
{"title": "Efficient sparse generalized multiple kernel learning\n", "abstract": " Kernel methods have been successfully applied in various applications. To succeed in these applications, it is crucial to learn a good kernel representation, whose objective is to reveal the data similarity precisely. In this paper, we address the problem of multiple kernel learning (MKL), searching for the optimal kernel combination weights through maximizing a generalized performance measure. Most MKL methods employ the -norm simplex constraints on the kernel combination weights, which therefore involve a sparse but non-smooth solution for the kernel weights. Despite the success of their efficiency, they tend to discard informative complementary or orthogonal base kernels and yield degenerated generalization performance. Alternatively, imposing the -norm constraint on the kernel weights will keep all the information in the base kernels. This leads to non-sparse solutions and brings the risk of being sensitive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["14"]}
{"title": "An adaptive QoS-aware fault tolerance strategy for web services\n", "abstract": " Service-Oriented Architecture (SOA) is widely adopted for building mission-critical systems, ranging from on-line stores to complex airline management systems. How to build reliable SOA systems becomes a big challenge due to the compositional nature of Web services. This paper proposes an adaptive QoS-aware fault tolerance strategy for Web services. Based on a user-collaborated QoS-aware middleware, SOA systems can dynamically adjust their optimal fault tolerance configurations to achieve optimal service reliability as well as good overall performance. Both the subjective user requirements and the objective system performance of the Web services are considered in our adaptive fault tolerance strategy. Experiments are conducted to illustrate the advantages of the proposed adaptive fault tolerance strategy. Performance and effectiveness comparisons of the proposed adaptive fault tolerance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["14"]}
{"title": "Probabilistic factor models for web site recommendation\n", "abstract": " Due to the prevalence of personalization and information filtering applications, modeling users' interests on the Web has become increasingly important during the past few years. In this paper, aiming at providing accurate personalized Web site recommendations for Web users, we propose a novel probabilistic factor model based on dimensionality reduction techniques. We also extend the proposed method to collective probabilistic factor modeling, which further improves model performance by incorporating heterogeneous data sources. The proposed method is general, and can be applied to not only Web site recommendations, but also a wide range of Web applications, including behavioral targeting, sponsored search, etc. The experimental analysis on Web site recommendation shows that our method outperforms other traditional recommendation approaches. Moreover, the complexity analysis indicates that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["14"]}
{"title": "Ws-dream: A distributed reliability assessment mechanism for web services\n", "abstract": " It is critical to guarantee the reliability of service-oriented applications. This is because they may employ remote Web services as components, which may easily become unavailable in the unpredictable Internet environment. This practical experience report presents a distribute reliability assessment mechanism for Web services (WS-DREAM), allowing users to carry out Web services reliability assessment in a collaborative manner. With WS-DREAM, users in different geography locations help each other to carry out testing, and share test cases under the coordination of a centralized server. Based on this collaborative mechanism, reliability assessment for Web services in real environment from different locations of the world becomes seamless. To illustrate the advantage of this mechanism, a prototype is implemented and a case study is carried out. Users from five locations all over the world perform reliability\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["14"]}
{"title": "System reliability analysis of an N-version programming application\n", "abstract": " This paper presents a quantitative reliability analysis of a system designed to tolerate both hardware and software faults. The system achieves integrated fault tolerance by implementing N-version programming (NVP) on redundant hardware. The system analysis considers unrelated software faults, related software faults, transient hardware faults, permanent hardware faults, and imperfect coverage. The overall model is Markov in which the states of the Markov chain represent the long-term evolution of the system-structure. For each operational configuration, a fault-tree model captures the effects of software faults and transient hardware faults on the task computation. The software fault model is parameterized using experimental data associated with a recent implementation of an NVP system using the current design paradigm. The hardware model is parameterized by considering typical failure rates associated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["14"]}
{"title": "A pseudoinverse learning algorithm for feedforward neural networks with stacked generalization applications to software reliability growth data\n", "abstract": " A supervised learning algorithm, Pseudoinverse Learning Algorithm (PIL), for feedforward neural networks is developed. The algorithm is based on generalized linear algebraic methods, and it adopts matrix inner products and pseudoinverse operations. Incorporating with network architecture of which the number of hidden layer neuron is equal to the number of examples to be learned, the algorithm eliminates learning errors by adding hidden layers and will give an exact solution (perfect learning). Unlike the existing gradient descent algorithm, the PIL is a feedforward only, fully automated algorithm, including no critical user-dependent parameters such as learning rate or momentum constant. The algorithm is tested on case studies with stacked generalization applications to software reliability growth data. The results indicate that the proposed algorithm is very efficient for the investigation on the computation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "92\n", "authors": ["14"]}
{"title": "Multi-head attention with disagreement regularization\n", "abstract": " Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed approach.", "num_citations": "90\n", "authors": ["14"]}
{"title": "A privacy-preserving qos prediction framework for web service recommendation\n", "abstract": " QoS-based Web service recommendation has recently gained much attention for providing a promising way to help users find high-quality services. To facilitate such recommendations, existing studies suggest the use of collaborative filtering techniques for personalized QoS prediction. These approaches, by leveraging partially observed QoS values from users, can achieve high accuracy of QoS predictions on the unobserved ones. However, the requirement to collect users' QoS data likely puts user privacy at risk, thus making them unwilling to contribute their usage data to a Web service recommender system. As a result, privacy becomes a critical challenge in developing practical Web service recommender systems. In this paper, we make the first attempt to cope with the privacy concerns for Web service recommendation. Specifically, we propose a simple yet effective privacy-preserving framework by applying\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["14"]}
{"title": "Maxi\u0393\u00c7\u00f4min margin machine: learning large margin classifiers locally and globally\n", "abstract": " In this paper, we propose a novel large margin classifier, called the maxi-min margin machine (M 4 ). This model learns the decision boundary both locally and globally. In comparison, other large margin classifiers construct separating hyperplanes only either locally or globally. For example, a state-of-the-art large margin classifier, the support vector machine (SVM), considers data only locally, while another significant model, the minimax probability machine (MPM), focuses on building the decision hyperplane exclusively based on the global information. As a major contribution, we show that SVM yields the same solution as M 4  when data satisfy certain conditions, and MPM can be regarded as a relaxation model of M 4 . Moreover, based on our proposed local and global view of data, another popular model, the linear discriminant analysis, can easily be interpreted and extended as well. We describe the M 4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "87\n", "authors": ["14"]}
{"title": "A novel adaptive sequential niche technique for multimodal function optimization\n", "abstract": " This paper proposes a novel adaptive sequential niche particle swarm optimization (ASNPSO) algorithm, which uses multiple sub-swarms to detect optimal solutions sequentially. In this algorithm, the hill valley function is used to determine how to change the fitness of a particle in a sub-swarm run currently. This algorithm has strong and adaptive searching ability. The experimental results show that the proposed ASNPSO algorithm is very effective and efficient in searching for multiple optimal solutions for benchmark test functions without any prior knowledge.", "num_citations": "85\n", "authors": ["14"]}
{"title": "Software reliability modeling with test coverage: Experimentation and measurement with a fault-tolerant software project\n", "abstract": " As the key factor in software quality, software reliability quantifies software failures. Traditional software reliability growth models use the execution time during testing for reliability estimation. Although testing time is an important factor in reliability, it is likely that the prediction accuracy of such models can be further improved by adding other parameters which affect the final software quality. Meanwhile, in software testing, test coverage has been regarded as an indicator for testing completeness and effectiveness in the literature. In this paper, we propose a novel method to integrate time and test coverage measurements together to predict the reliability. The key idea is that failure detection is not only related to the time that the software experiences under testing, but also to what fraction of the code has been executed by the testing. This is the first time that execution time and test coverage are incorporated together into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["14"]}
{"title": "Mining web graphs for recommendations\n", "abstract": " As the exponential explosion of various contents generated on the Web, Recommendation techniques have become increasingly indispensable. Innumerable different kinds of recommendations are made on the Web every day, including movies, music, images, books recommendations, query suggestions, tags recommendations, etc. No matter what types of data sources are used for the recommendations, essentially these data sources can be modeled in the form of various types of graphs. In this paper, aiming at providing a general framework on mining Web graphs for recommendations, (1) we first propose a novel diffusion method which propagates similarities between different nodes and generates recommendations; (2) then we illustrate how to generalize different recommendation problems into our graph diffusion framework. The proposed framework can be utilized in many recommendation tasks on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["14"]}
{"title": "Imbalanced learning with a biased minimax probability machine\n", "abstract": " Imbalanced learning is a challenged task in machine learning. In this context, the data associated with one class are far fewer than those associated with the other class. Traditional machine learning methods seeking classification accuracy over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into a majority class, usually the less important class. In this correspondence, the authors describe a new approach named the biased minimax probability machine (BMPM) to deal with the problem of imbalanced learning. This BMPM model is demonstrated to provide an elegant and systematic way for imbalanced learning. More specifically, by controlling the accuracy of the majority class under all possible choices of class-conditional densities with a given mean and covariance matrix, this model can quantitatively and systematically incorporate a bias for the minority\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["14"]}
{"title": "A real-time communication framework for wireless sensor-actuator networks\n", "abstract": " Wireless sensor-actuator network (WSAN) comprises of a group of distributed sensors and actuators that communicate through wireless links. Sensors are small and static devices with limited power, computation, and communication capabilities responsible for observing the physical world. On the other hand, actuators are equipped with richer resources, able to move and perform appropriate actions. Sensors and actuators cooperate with each other: While sensors perform sensing, actuators make decisions and react to the environment with the right actions. WSAN can be applied in a wide range of applications, like environmental monitoring, battlefield surveillance, chemical attack detection, intrusion detection, space missions, etc. Since actuators perform actions in response to the sensed events, real-time communications and quick reaction are necessary. To provide effective applications by WSAN, two major\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["14"]}
{"title": "Optimal testing resource allocation, and sensitivity analysis in software development\n", "abstract": " We consider two kinds of software testing-resource allocation problems. The first problem is to minimize the number of remaining faults given a fixed amount of testing-effort, and a reliability objective. The second problem is to minimize the amount of testing-effort given the number of remaining faults, and a reliability objective. We have proposed several strategies for module testing to help software project managers solve these problems, and make the best decisions. We provide several systematic solutions based on a nonhomogeneous Poisson process model, allowing systematic allocation of a specified amount of testing-resource expenditures for each software module under some constraints. We describe several numerical examples on the optimal testing-resource allocation problems to show applications & impacts of the proposed strategies during module testing. Experimental results indicate the advantages\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["14"]}
{"title": "Entropy-biased models for query representation on the click graph\n", "abstract": " Query log analysis has received substantial attention in recent years, in which the click graph is an important technique for describing the relationship between queries and URLs. State-of-the-art approaches based on the raw click frequencies for modeling the click graph, however, are not noise-eliminated. Nor do they handle heterogeneous query-URL pairs well. In this paper, we investigate and develop a novel entropy-biased framework for modeling click graphs. The intuition behind this model is that various query-URL pairs should be treated differently, ie, common clicks on less frequent but more specific URLs are of greater value than common clicks on frequent and general URLs. Based on this intuition, we utilize the entropy information of the URLs and introduce a new concept, namely the inverse query frequency (IQF), to weigh the importance (discriminative ability) of a click on a certain URL. The IQF\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["14"]}
{"title": "Analysis of software fault removal policies using a non-homogeneous continuous time Markov chain\n", "abstract": " Software reliability is an important metric that quantifies the quality of a software product and is inversely related to the residual number of faults in the system. Fault removal is a critical process in achieving desired level of quality before software deployment in the field. Conventional software reliability models assume that the time to remove a fault is negligible and that the fault removal process is perfect. In this paper we examine various kinds of fault removal policies, and analyze their effect on the residual number of faults at the end of the testing process, using a non-homogeneous continuous time Markov chain. The fault removal rate is initially assumed to be constant, and it is subsequently extended to cover time and state dependencies. We then extend the non-homogeneous continuous time Markov chain (NHCTMC) framework to include imperfections in the fault removal process. A method to compute\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["14"]}
{"title": "Dependability modeling for fault-tolerant software and systems\n", "abstract": " Three major fault\u0393\u00c7\u00f6tolerant software system architectures, distributed recovery blocks, N\u0393\u00c7\u00f6version programming, and N self\u252c\u2557 checking programming, are modeled by a combination of fault tree techniques and Markov processes. In these three architectures, transient and permanent hardware faults as well as unrelated and related software faults are modeled in the system-level domain. The model parameter values are determined from the analysis of data collected from a fault-tolerant avionic application. Quantitative analyses for reliability and safety factors achieved in these three fault\u0393\u00c7\u00f6tolerant system architectures are presented.", "num_citations": "77\n", "authors": ["14"]}
{"title": "Learning the unified kernel machines for classification\n", "abstract": " Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper, we propose a novel general framework of learning the Unified Kernel Machines (UKM) from both labeled and unlabeled data. Our proposed framework integrates supervised learning, semi-supervised kernel learning, and active learning in a unified solution. In the suggested framework, we particularly focus our attention on designing a new semi-supervised kernel learning method, ie, Spectral Kernel Learning (SKL), which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework, we present a specific paradigm of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["14"]}
{"title": "Time-dependent semantic similarity measure of queries using historical click-through data\n", "abstract": " It has become a promising direction to measure similarity of Web search queries by mining the increasing amount of click-through data logged by Web search engines, which record the interactions between users and the search engines. Most existing approaches employ the click-through data for similarity measure of queries with little consideration of the temporal factor, while the click-through data is often dynamic and contains rich temporal information. In this paper we present a new framework of time-dependent query semantic similarity model on exploiting the temporal characteristics of historical click-through data. The intuition is that more accurate semantic similarity values between queries can be obtained by taking into account the timestamps of the log data. With a set of user-defined calendar schema and calendar patterns, our time-dependent query similarity model is constructed using the marginalized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "75\n", "authors": ["14"]}
{"title": "On cloud service reliability enhancement with optimal resource usage\n", "abstract": " An increasing number of companies are beginning to deploy services/applications in the cloud computing environment. Enhancing the reliability of cloud service has become a critical and challenging research problem. In the cloud computing environment, all resources are commercialized. Therefore, a reliability enhancement approach should not consume too much resource. However, existing approaches cannot achieve the optimal effect because of checkpoint image-sharing neglect, and checkpoint image inaccessibility caused by node crashing. To address this problem, we propose a cloud service reliability enhancement approach for minimizing network and storage resource usage in a cloud data center. In our proposed approach, the identical parts of all virtual machines that provide the same service are checkpointed once as the service checkpoint image, which can be shared by those virtual machines to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["14"]}
{"title": "Incorporating fault debugging activities into software reliability models: A simulation approach\n", "abstract": " A large number of software reliability growth models have been proposed to analyse the reliability of a software application based on the failure data collected during the testing phase of the application. To ensure analytical tractability, most of these models are based on simplifying assumptions of instantaneous & perfect debugging. As a result, the estimates of the residual number of faults, failure rate, reliability, and optimal software release time obtained from these models tend to be optimistic. To obtain realistic estimates, it is desirable that the assumptions of instantaneous & perfect debugging be amended. In this paper we discuss the various policies according to which debugging may be conducted. We then describe a rate-based simulation framework to incorporate explicit debugging activities, which may be conducted according to the different debugging policies, into software reliability growth models. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["14"]}
{"title": "A spatial-temporal QoS prediction approach for time-aware web service recommendation\n", "abstract": " Due to the popularity of service-oriented architectures for various distributed systems, an increasing number of Web services have been deployed all over the world. Recently, Web service recommendation became a hot research topic, one that aims to accurately predict the quality of functional satisfactory services for each end user. Generally, the performance of Web service changes over time due to variations of service status and network conditions. Instead of employing the conventional temporal models, we propose a novel spatial-temporal QoS prediction approach for time-aware Web service recommendation, where a sparse representation is employed to model QoS variations. Specifically, we make a zero-mean Laplace prior distribution assumption on the residuals of the QoS prediction, which corresponds to a Lasso regression problem. To effectively select the nearest neighbor for the sparse representation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["14"]}
{"title": "Making services fault tolerant\n", "abstract": " With ever growing use of Internet, Web services become increasingly popular and their growth rate surpasses even the most optimistic predictions. Services are self-descriptive, self-contained, platform-independent and openly-available components that interact over the network. They are written strictly according to open specifications and/or standards and provide important and often critical functions for many business-to-business systems. Failures causing either service downtime or producing invalid results in such systems may range from a mere inconvenience to significant monetary penalties or even loss of human lives. In applications where sensing and control of machines and other devices take place via services, making the services highly dependable is one of main critical goals. Currently, there is no experimental investigation to evaluate the reliability and availability of Web services systems. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["14"]}
{"title": "A coverage analysis tool for the effectiveness of software testing\n", "abstract": " This paper describes the software testing and analysis tool, \"ATAC (Automatic Test Analysis for C)\", developed as a research instrument to measure the effectiveness of testing data. It is also a tool to facilitate the design and evaluation of test cases during software development. To demonstrate the capability and applicability of ATAC, the authors obtained 12 program versions of a critical industrial application developed in a recent university/industry N-version software project, and used ATAC to analyze and compare coverage of the testing on the program versions. Preliminary results from this investigation show that ATAC is a powerful testing tool to provide testing metrics and quality control guidance for the certification of high quality software components or systems.< >", "num_citations": "73\n", "authors": ["14"]}
{"title": "Non-monotonic feature selection\n", "abstract": " We consider the problem of selecting a subset of m most informative features where m is the number of required features. This feature selection problem is essentially a combinatorial optimization problem, and is usually solved by an approximation. Conventional feature selection methods address the computational challenge in two steps:(a) ranking all the features by certain scores that are usually computed independently from the number of specified features m, and (b) selecting the top m ranked features. One major shortcoming of these approaches is that if a feature f is chosen when the number of specified features is m, it will always be chosen when the number of specified features is larger than m. We refer to this property as the\" monotonic\" property of feature selection. In this work, we argue that it is important to develop efficient algorithms for non-monotonic feature selection. To this end, we develop an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["14"]}
{"title": "Learning large margin classifiers locally and globally\n", "abstract": " A new large margin classifier, named Maxi-Min Margin Machine (M 4) is proposed in this paper. This new classifier is constructed based on both a\" local: and a\" global\" view of data, while the most popular large margin classifier, Support Vector Machine (SVM) and the recently-proposed important model, Minimax Probability Machine (MPM) consider data only either locally or globally. This new model is theoretically important in the sense that SVM and MPM can both be considered as its special case. Furthermore, the optimization of M 4 can be cast as a sequential conic programming problem, which can be solved efficiently. We describe the M 4 model definition, provide a clear geometrical interpretation, present theoretical justifications, propose efficient solving methods, and perform a series of evaluations on both synthetic data sets and real world benchmark data sets. Its comparison with SVM and MPM also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["14"]}
{"title": "An empirical study on testing and fault tolerance for software reliability engineering\n", "abstract": " Software testing and software fault tolerance are two major techniques for developing reliable software systems, yet limited empirical data are available in the literature to evaluate their effectiveness. We conducted a major experiment to engage 34 programming teams to independently develop multiple software versions for an industry-scale critical flight application, and collected faults detected in these program versions. To evaluate the effectiveness of software testing and software fault tolerance, mutants were created by injecting real faults occurred in the development stage. The nature, manifestation, detection, and correlation of these faults were carefully investigated. The results show that coverage testing is generally an effective means to detecting software faults, but the effectiveness of testing coverage is not equivalent to that of mutation coverage, which is a more truthful indicator of testing quality. We also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["14"]}
{"title": "Online QoS prediction for runtime service adaptation via adaptive matrix factorization\n", "abstract": " Cloud applications built on service-oriented architectures generally integrate a number of component services to fulfill certain application logic. The changing cloud environment highlights the need for these applications to keep resilient against QoS variations of their component services so that end-to-end quality-of-service (QoS) can be guaranteed. Runtime service adaptation is a key technique to achieve this goal. To support timely and accurate adaptation decisions, effective and efficient QoS prediction is needed to obtain real-time QoS information of component services. However, current research has focused mostly on QoS prediction of working services that are being used by a cloud application, but little on predicting QoS values of candidate services that are equally important in determining optimal adaptation actions. In this paper, we propose an adaptive matrix factorization (namely AMF) approach to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["14"]}
{"title": "Design and evaluation of a fault-tolerant mobile-agent system\n", "abstract": " The mobile agents create a new paradigm for data exchange and resource sharing in rapidly growing and continually changing computer networks. In a distributed system, failures can occur in any software or hardware component. A mobile agent can get lost when its hosting server crashes during execution, or it can get dropped in a congested network. Therefore, survivability and fault tolerance are vital issues for deploying mobile-agent systems. This fault tolerance approach deploys three kinds of cooperating agents to detect server and agent failures and recover services in mobile-agent systems. An actual agent is a common mobile agent that performs specific computations for its owner. Witness agents monitor the actual agent and detect whether it's lost. A probe recovers the failed actual agent and the witness agents. A peer-to-peer message-passing mechanism stands between each actual agent and its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["14"]}
{"title": "Web service personalized quality of service prediction via reputation-based matrix factorization\n", "abstract": " With the fast development of Web services in service-oriented systems, the requirement of efficient Quality of Service (QoS) evaluation methods becomes strong. However, many QoS values are unknown in reality. Therefore, it is necessary to predict the unknown QoS values of Web services based on the obtainable QoS values. Generally, the QoS values of similar users are employed to make predictions for the current user. However, the QoS values may be contributed from unreliable users, leading to inaccuracy of the prediction results. To address this problem, we present a highly credible approach, called reputation-based Matrix Factorization (RMF), for predicting the unknown Web service QoS values. RMF first calculates the reputation of each user based on their contributed QoS values to quantify the credibility of users, and then takes the users' reputation into consideration for achieving more accurate QoS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["14"]}
{"title": "Reliability analysis for various communication schemes in wireless CORBA\n", "abstract": " For the purpose of designing more reliable networks, we extend the traditional reliability analysis from wired networks to wireless networks with imperfect components. Wireless network systems, such as wireless CORBA, inherit the unique handoff characteristic which leads to different communication structures with various components & links. Therefore, the traditional definition of two-terminal reliability is not applicable any more. We propose a new term, end-to-end expected instantaneous reliability, to integrate those different communication structures into one metric, which includes not only failure parameters but also service parameters. Nevertheless, it is still a monotonously decreasing function of time. The end-to-end expected instantaneous reliability, and its corresponding MTTF, are evaluated quantitatively in different wireless communication schemes. To observe the gain in overall reliability improvement, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["14"]}
{"title": "Gradient boosting factorization machines\n", "abstract": " Recommendation techniques have been well developed in the past decades. Most of them build models only based on user item rating matrix. However, in real world, there is plenty of auxiliary information available in recommendation systems. We can utilize these information as additional features to improve recommendation performance. We refer to recommendation with auxiliary information as context-aware recommendation. Context-aware Factorization Machines (FM) is one of the most successful context-aware recommendation models. FM models pairwise interactions between all features, in such way, a certain feature latent vector is shared to compute the factorized parameters it involved. In practice, there are tens of context features and not all the pairwise feature interactions are useful. Thus, one important challenge for context-aware recommendation is how to effectively select\" good\" interaction features\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["14"]}
{"title": "Introduction to social recommendation\n", "abstract": " As the exponential growth of information generated on the World Wide Web, Social Recommendation has emerged as one of the hot research topics recently. Social Recommendation forms a specific type of information filtering technique that attempts to suggest information (blogs, news, music, travel plans, web pages, images, tags, etc.) that are likely to interest the users. Social Recommendation involves the investigation of collective intelligence by using computational techniques such as machine learning, data mining, natural language processing, etc. on social behavior data collected from blogs, wikis, recommender systems, question & answer communities, query logs, tags, etc. from areas such as social networks, social search, social media, social bookmarks, social news, social knowledge sharing, and social games. In this tutorial, we will introduce Social Recommendation and elaborate on how the various\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["14"]}
{"title": "Reliable reporting of delay-sensitive events in wireless sensor-actuator networks\n", "abstract": " Wireless sensor-actuator networks, or WSANs, greatly enhance the existing wireless sensor network architecture by introducing powerful and even mobile actuators. The actuators work with the sensor nodes, but can perform much richer application-specific actions. To act responsively and accurately, an efficient and reliable reporting scheme is crucial for the sensors to inform the actuators about the environmental events. Unfortunately, the low-power multi-hop communications in a WSAN are inherently unreliable; the frequent sensor failures and the excessive delays due to congestion or in-network data aggregation further aggravate the problem. In this paper, we propose a general reliability-centric framework for event reporting in WSANs. We argue that the reliability in such a real-time system depends not only on the accuracy, but also the importance and freshness of the reported data. Our design follows this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["14"]}
{"title": "Selecting an optimal fault tolerance strategy for reliable service-oriented systems with local and global constraints\n", "abstract": " Functionally equivalent Web services can be composed to form more reliable service-oriented systems. However, the choice of fault tolerance strategy can have a significant effect on the quality-of-service (QoS) of the resulting service-oriented systems. In this paper, we investigate the problem of selecting an optimal fault tolerance strategy for building reliable service-oriented systems. We formulate the user requirements as local and global constraints and model the selection of fault tolerance strategy as an optimization problem. A heuristic algorithm is proposed to efficiently solve the optimization problem. Fault tolerance strategy selection for semantically related tasks is also investigated in this paper. Large-scale real-world experiments are conducted to illustrate the benefits of the proposed approach. The experimental results show that our problem modeling approach and the proposed selection algorithm make it\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["14"]}
{"title": "Reliability assessment and sensitivity analysis of software reliability growth modeling based on software module structure\n", "abstract": " Software reliability is an important characteristic for most systems. A number of reliability models have been developed to evaluate the reliability of a software system. The parameters in these software reliability models are usually directly obtained from the field failure data. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate values of the parameters are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. Sensitivity analysis provides a way to analyzing the impact of the different parameters. In order to assess the reliability of a component-based software, we propose a new approach to analyzing the reliability of the system, based on the reliabilities of the individual components and the architecture of the system. Furthermore, we present the sensitivity analysis on the reliability of a component-based software in order to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["14"]}
{"title": "A multimodal and multilevel ranking scheme for large-scale video retrieval\n", "abstract": " A critical issue of large-scale multimedia retrieval is how to develop an effective framework for ranking the search results. This problem is particularly challenging for content-based video retrieval due to some issues such as short text queries, insufficient sample learning, fusion of multimodal contents, and large-scale learning with huge media data. In this paper, we propose a novel multimodal and multilevel (MMML) ranking framework to attack the challenging ranking problem of content-based video retrieval. We represent the video retrieval task by graphs and suggest a graph based semi-supervised ranking (SSR) scheme, which can learn with small samples effectively and integrate multimodal resources for ranking smoothly. To make the semi-supervised ranking solution practical for large-scale retrieval tasks, we propose a multilevel ranking framework that unifies several different ranking approaches in a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "65\n", "authors": ["14"]}
{"title": "Collaborative image retrieval via regularized metric learning\n", "abstract": " In content-based image retrieval (CBIR), relevant images are identified based on their similarities to query images. Most CBIR algorithms are hindered by the semantic gap between the low-level image features used for computing image similarity and the high-level semantic concepts conveyed in images. One way to reduce the semantic gap is to utilize the log data of users' feedback that has been collected by CBIR systems in history, which is also called \u0393\u00c7\u00a3collaborative image retrieval.\u0393\u00c7\u00a5 In this paper, we present a novel metric learning approach, named \u0393\u00c7\u00a3regularized metric learning,\u0393\u00c7\u00a5 for collaborative image retrieval, which learns a distance metric by exploring the correlation between low-level image features and the log data of users' relevance judgments. Compared to the previous research, a regularization mechanism is used in our algorithm to effectively prevent overfitting. Meanwhile, we formulate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "65\n", "authors": ["14"]}
{"title": "Identifying impactful service system problems via log analysis\n", "abstract": " Logs are often used for troubleshooting in large-scale software systems. For a cloud-based online system that provides 24/7 service, a huge number of logs could be generated every day. However, these logs are highly imbalanced in general, because most logs indicate normal system operations, and only a small percentage of logs reveal impactful problems. Problems that lead to the decline of system KPIs (Key Performance Indicators) are impactful and should be fixed by engineers with a high priority. Furthermore, there are various types of system problems, which are hard to be distinguished manually. In this paper, we propose Log3C, a novel clustering-based approach to promptly and precisely identify impactful system problems, by utilizing both log sequences (a sequence of log events) and system KPIs. More specifically, we design a novel cascading clustering algorithm, which can greatly save the clustering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["14"]}
{"title": "Modeling and exploiting heterogeneous bibliographic networks for expertise ranking\n", "abstract": " Recently expertise retrieval has received increasing interests in both academia and industry. Finding experts with demonstrated expertise for a given query is a nontrivial task especially from a large-scale Web 2.0 systems, such as question answering and bibliography data, where users are actively publishing useful content online, interacting with each other, and forming social networks in various ways, leading to heterogeneous networks in addition to the large amounts of textual content information. Many approaches have been proposed and shown to be useful for expertise ranking. However, most of these methods only consider the textual documents while ignoring heterogeneous network structures or can merely integrate with one additional kind of information. None of them can fully exploit the characteristics of heterogeneous networks. In this paper, we propose a joint regularization framework to enhance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["14"]}
{"title": "Face annotation using transductive kernel fisher discriminant\n", "abstract": " Face annotation in images and videos enjoys many potential applications in multimedia information retrieval. Face annotation usually requires many training data labeled by hand in order to build effective classifiers. This is particularly challenging when annotating faces on large-scale collections of media data, in which huge labeling efforts would be very expensive. As a result, traditional supervised face annotation methods often suffer from insufficient training data. To attack this challenge, in this paper, we propose a novel Transductive Kernel Fisher Discriminant (TKFD) scheme for face annotation, which outperforms traditional supervised annotation methods with few training data. The main idea of our approach is to solve the Fisher's discriminant using deformed kernels incorporating the information of both labeled and unlabeled data. To evaluate the effectiveness of our method, we have conducted extensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["14"]}
{"title": "Low-temperature processed solar cells with formamidinium tin halide perovskite/fullerene heterojunctions\n", "abstract": " A new type of lead-free, formamidinium (FA)-based halide perovskites, FASnI2Br, are investigated as light-harvesting materials for low-temperature processed p\u0393\u00c7\u00f4i\u0393\u00c7\u00f4n heterojunction solar cells with different configurations. The FASnI2Br perovskite, with a band-gap of 1.68 eV, exhibits optimal photovoltaic performance after low-temperature annealing at 75 \u252c\u2591C. By using C60 as electron-transport layer, the device yields a hysteresis-less power conversion efficiency of 1.72%. The possible use of an inorganic MoO                   x                  film as a new type of independent hole-transport layer for the present tin-based perovskite solar cells is also demonstrated.", "num_citations": "62\n", "authors": ["14"]}
{"title": "Online learning for collaborative filtering\n", "abstract": " Collaborative filtering (CF), aiming at predicting users' unknown preferences based on observational preferences from some users, has become one of the most successful methods to building recommender systems. Various approaches to CF have been proposed in this area, but seldom do they consider the dynamic scenarios: 1) new items arriving in the system, 2) new users joining the system; or 3) new rating updating the system are all dynamically obtained with respect to time. To capture these changes, in this paper, we develop an online learning framework for collaborative filtering. Specifically, we construct this framework consisting of two state-of-the-art matrix factorization based CF methods: the probabilistic matrix factorization and the top-one probability based ranking matrix factorization. Moreover, we demonstrate that the proposed online algorithms bring several attractive advantages: 1) they scale\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["14"]}
{"title": "Machine learning: modeling data locally and globally\n", "abstract": " Machine Learning-Modeling Data Locally and Globally presents a novel and unified theory that tries to seamlessly integrate different algorithms. Specifically, the book distinguishes the inner nature of machine learning algorithms as either\" local learning\" or\" global learning.\" This theory not only connects previous machine learning methods, or serves as roadmap in various models, but\u0393\u00c7\u00f4more importantly\u0393\u00c7\u00f4it also motivates a theory that can learn from data both locally and globally. This would help the researchers gain a deeper insight and comprehensive understanding of the techniques in this field. The book reviews current topics, new theories and applications. Kaizhu Huang was a researcher at the Fujitsu Research and Development Center and is currently a research fellow in the Chinese University of Hong Kong. Haiqin Yang leads the image processing group at HiSilicon Technologies. Irwin King and Michael R. Lyu are professors at the Computer Science and Engineering department of the Chinese University of Hong Kong.", "num_citations": "62\n", "authors": ["14"]}
{"title": "Group-based relevance feedback with support vector machine ensembles\n", "abstract": " Support vector machines (SVMs) have become one of the most promising techniques for relevance feedback in content-based image retrieval (CBIR). Typical SVM-based relevance feedback techniques simply apply the strict binary classifications: positive (relevant) class and negative (irrelevant) class. However, in a real-world relevance feedback task, it is more reasonable and practical to assume the data come from multiple positive classes and one negative class. In order to formulate an effective relevance feedback algorithm, we propose a novel group-based relevance feedback scheme constructed with the SVM ensembles technique. Experiments are conducted to evaluate the performance of our proposed scheme and the traditional SVM-based relevance feedback technique in CBIR. The experimental results show that our proposed scheme is more effective than the regular method.", "num_citations": "62\n", "authors": ["14"]}
{"title": "Extracting and selecting distinctive EEG features for efficient epileptic seizure prediction\n", "abstract": " This paper presents compact yet comprehensive feature representations for the electroencephalogram (EEG) signal to achieve efficient epileptic seizure prediction performance. The initial EEG feature vectors are formed by acquiring the dominant amplitude and frequency components on an epoch-by-epoch basis from the EEG signals. These extracted parameters can reveal the intrinsic EEG signal changes as well as the underlying stage transitions. To improve the efficacy of feature extraction, an elimination-based feature selection method has been applied on the initial feature vectors. This diminishes redundant and noisy points, providing each patient with a lower dimensional and independent final feature form. In this context, our study is distinguished from that of others currently prevailing. Usually, these latter approaches adopted feature extraction processes, which employed time-consuming high-dimensional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["14"]}
{"title": "Reputation-aware qos value prediction of web services\n", "abstract": " QoS value prediction of Web services is an important research issue for service recommendation, selection and composition. Collaborative Filtering (CF) is one of the most widely used methods which employs QoS values contributed by similar users to make predictions. Therefore, historical QoS values contributed by different users can have great impacts on prediction results. However, existing Web service QoS value prediction approaches did not take data credibility into consideration, which may impact the prediction accuracy. To address this problem, we propose a reputation-aware QoS value prediction approach, which first calculates the reputation of each user based on their contributed values, and then takes advantage of reputation-based ranking to exclude the values contributed by untrustworthy users. CF QoS prediction approach is finally used to predict the missing QoS values based on the purified\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["14"]}
{"title": "An authentication service against dishonest users in mobile ad hoc networks\n", "abstract": " A mobile ad hoc network is a collection of wireless mobile nodes, dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. It is an emerging technology for civilian and military applications. However, security in mobile ad hoc networks is hard to achieve due to the vulnerability of the links, the limited physical protection of the nodes, and the absence of a certification authority or centralized management point. Similar to other distributed systems, security in mobile ad hoc networks usually relies on the use of different key management mechanisms. We exploit characteristics of an ad hoc network and present our authentication service to protect network security in the presence of dishonest users. Nodes originally trustable in the network may become malicious due to sudden attacks, so an adequate security support for authentication to deal with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["14"]}
{"title": "Software reliability modeling and cost estimation incorporating testing-effort and efficiency\n", "abstract": " Many studies have been performed on the subject of software reliability but few have explicitly considered the impact of software testing on the reliability process. This paper presents two important issues on software reliability modeling and software reliability economics: testing effort and efficiency. First, we discuss on how to extend the logistic testing-effort function into a general form. The generalized logistic testing-effort function has the advantage of relating the work profile more directly to the natural flow of software development. Therefore, it can be used to describe the actual consumption of resources during the software development process and to obtain a conspicuous improvement in modeling testing-effort expenditures. Furthermore, we incorporate the generalized logistic testing-effort function into software reliability modeling and its fault-prediction capability is evaluated through four numerical experiments\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["14"]}
{"title": "Energy landscapes and urban trajectories towards sustainability\n", "abstract": " An urban energy transition is needed to address the two global environmental challenges of urbanisation and increasing carbon emissions. Urban energy landscapes represent the spatial patterns of urban energy systems which are visible in the built environment. Spatial regularities in the way systems of energy provision and use are organised are manifest in urban energy landscapes. Energy uses may vary in relation to the structures of the built environment, and the perceptions that coevolve with technologies.This paper presents evidence from three case studies of urban energy landscapes in Hong Kong (PRC), Bengaluru (India) and Maputo (Mozambique). The cases suggest a variety of patterns (uniform, fragmented, scattered) in terms of how different fuels and electricity are provided and who has access to them. Qualitative research among policy makers reveals different trajectories towards sustainability\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["14"]}
{"title": "A clustering-based QoS prediction approach for Web service recommendation\n", "abstract": " The rising popularity of service-oriented architecture to construct versatile distributed systems makes Web service recommendation and composition a hot research topic. It's a challenge to design accurate personalized QoS prediction approaches for Web service recommendation due to the unpredictable Internet environment and the sparsity of available historical QoS information. In this paper, we propose a novel landmark-based QoS prediction framework and then present two clustering-based prediction algorithms for Web services, named UBC and WSBC, aiming at enhancing the QoS prediction accuracy via clustering techniques. Hierarchical clustering is adopted based on the real-word Web service QoS dataset collected with PlanetLab1, which contains response-time values of 200 distributed service users and 1,597 Web services. The comprehensive experimental comparison and analysis show that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["14"]}
{"title": "Online app review analysis for identifying emerging issues\n", "abstract": " Detecting emerging issues (eg, new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (eg, misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["14"]}
{"title": "Efficient online learning for multitask feature selection\n", "abstract": " Learning explanatory features across multiple related tasks, or MultiTask Feature Selection (MTFS), is an important problem in the applications of data mining, machine learning, and bioinformatics. Previous MTFS methods fulfill this task by batch-mode training. This makes them inefficient when data come sequentially or when the number of training data is so large that they cannot be loaded into the memory simultaneously. In order to tackle these problems, we propose a novel online learning framework to solve the MTFS problem. A main advantage of the online algorithm is its efficiency in both time complexity and memory cost. The weights of the MTFS models at each iteration can be updated by closed-form solutions based on the average of previous subgradients. This yields the worst-case bounds of the time complexity and memory cost at each iteration, both in the order of O(d \u251c\u00f9 Q), where d is the number of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["14"]}
{"title": "Toward optimal deployment of communication-intensive cloud applications\n", "abstract": " Strongly promoted by the leading industrial companies, cloud computing becomes increasingly popular in re-cent years. The growth rate of cloud computing surpasses even the most optimistic predictions. A cloud application is a large-scale distributed system that consist a lot of distributed cloud nodes. How to make optimal deployment of cloud applications is a challenging research problem. When deploying a cloud application to the cloud environment, cloud node ranking is one of the most important approaches for selecting optimal cloud nodes for the cloud application. Traditional ranking methods usually rank the cloud nodes based on their QoS values, without considering the communication performance between cloud nodes. However, such kind of node relationship is very important for the communication-intensive cloud applications (e.g., Message Passing Interface (MPI) programs), which have a lot of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["14"]}
{"title": "Towards operational cost minimization in hybrid clouds for dynamic resource provisioning with delay-aware optimization\n", "abstract": " Recently, hybrid cloud computing paradigm has be widely advocated as a promising solution for Software-as-a-Service (SaaS) providers to effectively handle the dynamic user requests. With such a paradigm, the SaaS providers can extend their local services into the public clouds seamlessly so that the dynamic user request workload to a SaaS can be elegantly processed with both the local servers and the rented computing capacity in the public cloud. However, although it is suggested that a hybrid cloud may save cost compared with building a powerful private cloud, considerable renting cost and communication cost are still introduced in such a paradigm. How to optimize such operational cost becomes one major concern for the SaaS providers to adopt the hybrid cloud computing paradigm. However, this critical problem remains unanswered in the current state of the art. In this paper, we focus on optimizing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["14"]}
{"title": "Reliableweb services: Methodology, experiment and modeling\n", "abstract": " We identify parameters impacting Web services dependability, describe the methods of dependability enhancement by redundancy in space and redundancy in time, and perform a series of experiments to evaluate the availability of Web services. To increase the availability of Web services, we employ several replication schemes and compare them with a single service. The Web services are coordinated by a replication manager. It provides a round robin algorithm for scheduling the workload of the Web services and keeps updating the availability of each Web service. The replication algorithm and the detailed system configuration are described. Experiments are performed to evaluate the resulting service availability. Modeling on the Web services with Petri-net is constructed and verified through experiments with different applications. With the parameters obtained from the experiments, the proposed model can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["14"]}
{"title": "Assuring design diversity in N-version software: a design paradigm for N-version programming\n", "abstract": " The N-Version Programming (NVP) approach achieves fault-tolerant software units, called N-version Software (NVS) units, through the development and use of software diversity. To maximize the effectiveness of the NVP approach, the probability of similar errors that coincide at the NVS decision points should be reduced to the lowest possible value. Design diversity is potentially an effective method to get this result. It has been the major concern of this paper to formulate a set of rigorous guidelines, or a design paradigm for the investigation and implementation of design diversity in building NVS units for practical applications. This effort includes the description of a most recent formulation of the NVP design paradigm, which integrates the knowledge and experience obtained from fault-tolerant system design with software engineering techniques, and the application of this design paradigm to a real-world\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["14"]}
{"title": "MatchSim: a novel similarity measure based on maximum neighborhood matching\n", "abstract": " Measuring object similarity in a graph is a fundamental data- mining problem in various application domains, including Web linkage mining, social network analysis, information retrieval, and recommender systems. In this paper, we focus on the neighbor-based approach that is based on the intuition that \u0393\u00c7\u00a3similar objects have similar neighbors\u0393\u00c7\u00a5 and propose a novel similarity measure called MatchSim. Our method recursively defines the similarity between two objects by the average similarity of the maximum-matched similar neighbor pairs between them. We show that MatchSim conforms to the basic intuition of similarity; therefore, it can overcome the counterintuitive contradiction in SimRank. Moreover, MatchSim can be viewed as an extension of the traditional neighbor-counting scheme by taking the similarities between neighbors into account, leading to higher flexibility. We present the MatchSim score\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["14"]}
{"title": "Dynamic web service composition: A new approach in building reliable web service\n", "abstract": " The use of services, especially Web services, became a common practice. In Web services, standard communication protocols and simple broker-request architectures are needed to facilitate exchange of services, and this standardization simplifies interoperability. In the coming few years, services are expected to dominate software industry. There are increasing amount of Web services being made available in the Internet, and an efficient Web services composition algorithm would help to integrate different algorithm together to provide a variety of services. In this paper, we provide a dynamic Web service composition algorithm with verification of Petri-Nets. Each Web service is described by Web Service Definition Language (WSDL) and their interactions with other services are described by Web Service Choreography Interface (WSCI). Our algorithm compose the Web services with the information provided by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["14"]}
{"title": "An empirical study of the correlation between code coverage and reliability estimation\n", "abstract": " Existing time-domain models for software reliability often result in an overestimation, of such reliability because they do not take the nature of testing techniques into account. Since every testing technique has a limit to its ability to reveal faults in a given system, as a technique approaches its saturation region fewer faults are discovered and reliability growth phenomena are predicted from the models. When the software is turned over to field operation, significant overestimates of reliability are observed. We present a technique to solve this problem by addressing both time and coverage measures for the prediction of software failures. Our technique uses coverage information collected during testing to extract only effective data from a given operational profile. Execution time between test cases which neither increase coverage nor cause a failure as reduced by a parameterized factor. Experiments using this technique\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["14"]}
{"title": "Sliding mode control for a class of nonlinear multi-agent system with time delay and uncertainties\n", "abstract": " In this paper, a type of multi-agent system is adopted in the practical project with time delay, uncertainties, and linear feedback. In addition, sliding mode control is used to ensure the robust stability of the system since it is insensitive to parameter change and interference. For the system in three different conditions, namely fixed structure, every agent being only influenced by single time delay and each agent being affected by multiple time delay, the corresponding sliding surface and the control law are improved. The reaction factors are proven by Lyapunov functions and the linear matrix inequality approach is taken to guarantee the robust stability of the sliding surface. To prove the effectiveness of the conclusion, experiments on each condition are conducted. Besides, in the last part, a simple application is applied and proved to be effective.", "num_citations": "55\n", "authors": ["14"]}
{"title": "Optimal allocation of testing-resource considering cost, reliability, and testing-effort\n", "abstract": " We investigate an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the number of remaining faults and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Besides, sensitivity analysis is also conducted. We analyze the sensitivity of parameters of proposed software reliability growth models and show the results in detail. In addition, we present the impact on the resource allocation problem if some parameters are either overestimated or underestimated. We can evaluate the optimal resource allocation problems for various conditions by examining the behavior of the parameters with the most significant influence. The experimental results greatly help us to identify the contributions of each\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["14"]}
{"title": "Reliability and maintainability related software coupling metrics in C++ programs.\n", "abstract": " This paper describes some difficulties that one encounters in the testing and maintenance of C++ programs, which may result in program unreliability. Inheritance and polymorphism are key concepts in objectoriented programming (OOP), and are essential for achieving reusability and extendibility, but they also make programs more difficult to understand. We have tried to show by arguments and by some empirical evidence that widely used complexity metrics like lines of code, cyclomatic complexity, and Software Science\u0393\u00c7\u00d6s metrics may not be appropriate to measure the complexity of C++ programs and those written in other objectoriented languages, since they do not address concepts like inheritance and encapsulation, apart from having other weaknesses. Some measures using a notion from the world of functional decomposition-coupling, are defined for C++ programs. Two of them-CC and AMC-and equivalent ones for the three widely used complexity metrics (for comparison) are computed for five C++ programs. Our preliminary results show that our coupling measures correlate better with difficulty of testing and maintenance than the three widely used complexity metrics.", "num_citations": "55\n", "authors": ["14"]}
{"title": "A delay-aware reliable event reporting framework for wireless sensor\u0393\u00c7\u00f4actuator networks\n", "abstract": " Wireless sensor\u0393\u00c7\u00f4actuator networks (WSANs) greatly enhance the existing wireless sensor network architecture by introducing powerful and possibly even mobile actuators. The actuators work with the sensor nodes, but can perform much richer application-specific actions. To act responsively and accurately, an efficient and reliable reporting scheme is crucial for the sensors to inform the actuators about the environmental events. Unfortunately, the low-power multi-hop communications in a WSAN are inherently unreliable; frequent sensor failures and excessive delays due to congestion or in-network data aggregation further aggravate the problem.In this paper, we propose a general reliability-centric framework for event reporting in WSANs. We argue that the reliability in such a real-time system depends not only on the accuracy, but also the importance and freshness of the reported data. Our design follows this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["14"]}
{"title": "Tagrec: Leveraging tagging wisdom for recommendation\n", "abstract": " Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent userspsila judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect userspsila opinions, this paper proposes a factor analysis approach called TagRec based on a unified\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["14"]}
{"title": "Software fault tolerance in a clustered architecture: Techniques and reliability modeling\n", "abstract": " System architectures based on a cluster of computers have gained substantial attention recently. In a clustered system, complex software-intensive applications can be built with commercial hardware, operating systems, and application software to achieve high system availability and data integrity, while performance and cost penalties are greatly reduced by the use of separate error detection hardware and dedicated software fault tolerance routines. Within such a system a watchdog provides mechanisms for error detection and switch-over to a spare or backup processor in the presence of processor failures. The application software is responsible for the extent of the error detection, subsequent recovery actions and data backup. The application can be made as reliable as the user requires, being constrained only by the upper bounds on reliability imposed by the clustered architecture under various\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["14"]}
{"title": "Topology-aware deployment of scientific applications in cloud computing\n", "abstract": " Nowadays, more and more scientific applications are moving to cloud computing. The optimal deployment of scientific applications is critical for providing good services to users. Scientific applications are usually topology-aware applications. Therefore, considering the topology of a scientific application during the development will benefit the performance of the application. However, it is challenging to automatically discover and make use of the communication pattern of a scientific application while deploying the application on cloud. To attack this challenge, in this paper, we propose a framework to discover the communication topology of a scientific application by pre-execution and multi-scale graph clustering, based on which the deployment can be optimized. Comprehensive experiments are conducted by employing a well-known MPI benchmark and comparing the performance of our method with those of other\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["14"]}
{"title": "MalPat: Mining patterns of malicious and benign Android apps via permission-related APIs\n", "abstract": " The dramatic rise of Android application (app) marketplaces has significantly gained the success of convenience for mobile users. Consequently, with the advantage of numerous Android apps, Android malware seizes the opportunity to steal privacy-sensitive data by pretending to provide functionalities as benign apps do. To distinguish malware from millions of Android apps, researchers have proposed sophisticated static and dynamic analysis tools to automatically detect and classify malicious apps. Most of these tools, however, rely on manual configuration of lists of features based on permissions, sensitive resources, intents, etc., which are difficult to come by. To address this problem, we study real-world Android apps to mine hidden patterns of malware and are able to extract highly sensitive APIs that are widely used in Android malware. We also implement an automated malware detection system, MalPat, to fight\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["14"]}
{"title": "Reliability-based design optimization for cloud migration\n", "abstract": " The on-demand use, high scalability, and low maintenance cost nature of cloud computing have attracted more and more enterprises to migrate their legacy applications to the cloud environment. Although the cloud platform itself promises high reliability, ensuring high quality of service is still one of the major concerns, since the enterprise applications are usually complicated and consist of a large number of distributed components. Thus, improving the reliability of an application during cloud migration is a challenging and critical research problem. To address this problem, we propose a reliability-based optimization framework, named ROCloud, to improve the application reliability by fault tolerance. ROCloud includes two ranking algorithms. The first algorithm ranks components for the applications that all their components will be migrated to the cloud. The second algorithm ranks components for hybrid applications\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["14"]}
{"title": "Fast algorithms for Dyck-CFL-reachability with applications to alias analysis\n", "abstract": " The context-free language (CFL) reachability problem is a well-known fundamental formulation in program analysis. In practice, many program analyses, especially pointer analyses, adopt a restricted version of CFL-reachability, Dyck-CFL-reachability, and compute on edge-labeled bidirected graphs. Solving the all-pairs Dyck-CFL-reachability on such bidirected graphs is expensive. For a bidirected graph with n nodes and m edges, the traditional dynamic programming style algorithm exhibits a subcubic time complexity for the Dyck language with k kinds of parentheses. When the underlying graphs are restricted to bidirected trees, an algorithm with O (n log n log k) time complexity was proposed recently. This paper studies the Dyck-CFL-reachability problems on bidirected trees and graphs. In particular, it presents two fast algorithms with O (n) and O (n+ m log m) time complexities on trees and graphs respectively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["14"]}
{"title": "An authentication service based on trust and clustering in wireless ad hoc networks: description and security evaluation\n", "abstract": " Security in wireless ad hoc networks is hard to achieve due to the vulnerability of its links, limited physical protection, and the absence of a centralized management point. Consequently, novel approaches are necessary to address the security problem without sacrificing the essential properties of the wireless ad hoc network. Similar to other distributed systems, security in wireless ad hoc networks usually relies on the use of key management mechanisms. In this paper, we present a distributed public key authentication service to protect the network containing malicious and colluding nodes. Our solution was built on a clustering-based network model and a trust model. These models allow mobile hosts to monitor and rate each other with an authentication metric. We also propose a new system of public key certification in conjunction with a trust value update algorithm. Our authentication service is able to discover\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["14"]}
{"title": "Nonnegative independent component analysis based on minimizing mutual information technique\n", "abstract": " A novel neural network technique for nonnegative independent component analysis is proposed in this letter. Compared with other algorithms, this method can work efficiently even when the source signals are not well grounded. Moreover, this method is insensitive to the particular underlying distribution of the source data. Experimental results demonstrate the advantages of our approach in achieving satisfactory results regardless of whether the source data are well grounded or not.", "num_citations": "51\n", "authors": ["14"]}
{"title": "A unified point-of-interest recommendation framework in location-based social networks\n", "abstract": " Location-based social networks (LBSNs), such as Gowalla, Facebook, Foursquare, Brightkite, and so on, have attracted millions of users to share their social friendship and their locations via check-ins in the past few years. Plenty of valuable information is accumulated based on the check-in behaviors, which makes it possible to learn users\u0393\u00c7\u00d6 moving patterns as well as their preferences. In LBSNs, point-of-interest (POI) recommendation is one of the most significant tasks because it can help targeted users explore their surroundings as well as help third-party developers provide personalized services. Matrix factorization is a promising method for this task because it can capture users\u0393\u00c7\u00d6 preferences to locations and is widely adopted in traditional recommender systems such as movie recommendation. However, the sparsity of the check-in data makes it difficult to capture users\u0393\u00c7\u00d6 preferences accurately. Geographical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["14"]}
{"title": "An empirical study of common challenges in developing deep learning applications\n", "abstract": " Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["14"]}
{"title": "A topic-biased user reputation model in rating systems\n", "abstract": " In rating systems like Epinions and Amazon\u0393\u00c7\u00d6s product review systems, users rate items on different topics to yield item scores. Traditionally, item scores are estimated by averaging all the ratings with equal weights. To improve the accuracy of estimated item scores, user reputation [a.k.a., user reputation (UR)] is incorporated. The existing algorithms on UR, however, have underplayed the role of topics in rating systems. In this paper, we first reveal that UR is topic-biased from our empirical investigation. However, existing algorithms cannot capture this characteristic in rating systems. To address this issue, we propose a topic-biased model (TBM) to estimate UR in terms of different topics as well as item scores. With TBM, we develop six topic-biased algorithms, which are subsequently evaluated with experiments using both real-world and synthetic data sets. Results of the experiments demonstrate that the topic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["14"]}
{"title": "Progressive finite newton approach to real-time nonrigid surface detection\n", "abstract": " Detecting nonrigid surfaces is an interesting research problem for computer vision and image analysis. One important challenge of nonrigid surface detection is how to register a nonrigid surface mesh having a large number of free deformation parameters. This is particularly significant for detecting nonrigid surfaces from noisy observations. Nonrigid surface detection is usually regarded as a robust parameter estimation problem, which is typically solved iteratively from a good initialization in order to avoid local minima. In this paper, we propose a novel progressive finite Newton optimization scheme for the non-rigid surface detection problem, which is reduced to only solving a set of linear equations. The key of our approach is to formulate the nonrigid surface detection as an unconstrained quadratic optimization problem which has a closed-form solution for a given set of observations. Moreover, we employ a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["14"]}
{"title": "Sentomist: Unveiling transient sensor network bugs via symptom mining\n", "abstract": " Wireless Sensor Network (WSN) applications are typically event-driven. While the source codes of these applications may look simple, they are executed with a complicated concurrency model, which frequently introduces software bugs, in particular, transient bugs. Such buggy logics may only be triggered by some occasionally interleaved events that bear implicit dependency, but can lead to fatal system failures. Unfortunately, these deeply-hidden bugs or even their symptoms can hardly be identified by state-of-the-art debugging tools, and manual identification from massive running traces can be prohibitively expensive. In this paper, we present Sentomist (Sensor application anatomist), a novel tool for identifying potential transient bugs in WSN applications. The Sentomist design is based on a key observation that transient bugs make the behaviors of a WSN system deviate from the normal, and thus outliers (i.e\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["14"]}
{"title": "Maximizing sensitivity in medical diagnosis using biased minimax probability machine\n", "abstract": " The challenging task of medical diagnosis based on machine learning techniques requires an inherent bias, i.e., the diagnosis should favor the \"ill\" class over the \"healthy\" class, since misdiagnosing a patient as a healthy person may delay the therapy and aggravate the illness. Therefore,the objective in this task is not to improve the overall accuracy of the classification,but to focus on improving the sensitivity (the accuracy of the \"ill\" class) while maintaining an acceptable specificity (the accuracy of the \"healthy\" class). Some current methods adopt roundabout ways to impose a certain bias toward the important class, i.e., they try to utilize some intermediate factors to influence the classification. However, it remains uncertain whether these methods can improve the classification performance systematically. In this paper, by engaging a novel learning tool, the biased minimax probability machine(BMPM), we deal with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["14"]}
{"title": "Robust regularized kernel regression\n", "abstract": " Robust regression techniques are critical to fitting data with noise in real-world applications. Most previous work of robust kernel regression is usually formulated into a dual form, which is then solved by some quadratic program solver consequently. In this correspondence, we propose a new formulation for robust regularized kernel regression under the theoretical framework of regularization networks and then tackle the optimization problem directly in the primal. We show that the primal and dual approaches are equivalent to achieving similar regression performance, but the primal formulation is more efficient and easier to be implemented than the dual one. Different from previous work, our approach also optimizes the bias term. In addition, we show that the proposed solution can be easily extended to other noise-reliable loss function, including the Huber-epsiv insensitive loss function. Finally, we conduct a set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["14"]}
{"title": "Arcade: Augmented reality computing arena for digital entertainment\n", "abstract": " Augmented reality (AR) technology for digital composition of animation with real scenes is to bring new digital entertainment experience to the viewers. Augmented reality is a form of human-machine interaction. The key feature of the augmented reality technology is to present auxiliary information in the field of view for an individual automatically without human intervention. The effect is similar to composing computer-animated images with real scenes. To achieve the new augmented reality experience, two main problems are: How to keep track of the viewing parameters of the individual viewer? How to render a virtual image in the field of view correctly and seamlessly? To tackle the above, we are designing and implementing an enabling framework, called augmented reality computing arena for digital entertainment (ARCADE), to support the creation of augmented reality entertainment applications. Moreover, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["14"]}
{"title": "Capturing geographical influence in POI recommendations\n", "abstract": " Point-of-Interest (POI) recommendation is a significant service for location-based social networks (LBSNs). It recommends new places such as clubs, restaurants, and coffee bars to users. Whether recommended locations meet users\u0393\u00c7\u00d6 interests depends on three factors: user preference, social influence, and geographical influence. Hence extracting the information from users\u0393\u00c7\u00d6 check-in records is the key to POI recommendation in LBSNs. Capturing user preference and social influence is relatively easy since it is analogical to the methods in a movie recommender system. However, it is a new topic to capture geographical influence. Previous studies indicate that check-in locations disperse around several centers and we are able to employ Gaussian distribution based models to approximate users\u0393\u00c7\u00d6 check-in behaviors. Yet centers discovering methods are dissatisfactory. In this paper, we propose two models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["14"]}
{"title": "Effective latent space graph-based re-ranking model with global consistency\n", "abstract": " Recently the re-ranking algorithms have been quite popular for web search and data mining. However, one of the issues is that those algorithms treat the content and link information individually. Inspired by graph-based machine learning algorithms, we propose a novel and general framework to model the re-ranking algorithm, by regularizing the smoothness of ranking scores over the graph, along with a regularizer on the initial ranking scores (which are obtained by the base ranker). The intuition behind the model is the global consistency over the graph: similar entities are likely to have the same ranking scores with respect to a query. Our approach simultaneously incorporates the content with other explicit or implicit link information in a latent space graph. Then an effective unified re-ranking algorithm is performed on the graph with respect to the query. To illustrate our methodology, we apply the framework to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["14"]}
{"title": "Characterizing the natural language descriptions in software logging statements\n", "abstract": " Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["14"]}
{"title": "An automatic framework for detecting and characterizing performance degradation of software systems\n", "abstract": " Software systems that run continuously over a long time have been frequently reported encountering gradual degradation issues. That is, as time progresses, software tends to exhibit degraded performance, deflated service capacity, or deteriorated QoS. Currently, the state-of-the-art approach of Mann-Kendall Test & Seasonal Kendall Test & Sen's Slope Estimator & Seasonal Sen's Slope Estimator (MKSK) detects and characterizes degradation via a combination of techniques in statistical trend analysis. Nevertheless, we pinpoint some drawbacks of MKSK in this paper: 1) MKSK cannot be automated for large scale software degradation analysis, 2) MKSK estimates the degradation trend of software in an oversimplified linear way, 3) MKSK is sensitive to noise, and 4) MKSK suffers from high computational complexity. To overcome all these limitations, we propose a more advanced approach called Modified Cox\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["14"]}
{"title": "Enhanced models for expertise retrieval using community-aware strategies\n", "abstract": " Expertise retrieval, whose task is to suggest people with relevant expertise on the topic of interest, has received increasing interest in recent years. One of the issues is that previous algorithms mainly consider the documents associated with the experts while ignoring the community information that is affiliated with the documents and the experts. Motivated by the observation that communities could provide valuable insight and distinctive information, we investigate and develop two community-aware strategies to enhance expertise retrieval. We first propose a new smoothing method using the community context for statistical language modeling, which is employed to identify the most relevant documents so as to boost the performance of expertise retrieval in the document-based model. Furthermore, we propose a query-sensitive AuthorRank to model the authors' authorities based on the community coauthorship\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["14"]}
{"title": "Sensitivity analysis of software reliability for component-based software applications\n", "abstract": " The parameters in these software reliability models are usually directly obtained from the field failure data. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate values of the parameters are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. Sensitivity analysis provides a way to analyzing the impact of the different parameters. In order to assess the reliability of a component-based software, we propose a new approach to analyzing the reliability of the system, based on the reliabilities of the individual components and the architecture of the system. Furthermore, we present the sensitivity analysis on the reliability of a component-based software in order to determine which of the components affects the reliability of the system most. Finally, three general examples are evaluated to validate and show the effectiveness of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["14"]}
{"title": "An effective approach to 3d deformable surface tracking\n", "abstract": " The key challenge with 3D deformable surface tracking arises from the difficulty in estimating a large number of 3D shape parameters from noisy observations. A recent state-of-the-art approach attacks this problem by formulating it as a Second Order Cone Programming (SOCP) feasibility problem. The main drawback of this solution is the high computational cost. In this paper, we first reformulate the problem into an unconstrained quadratic optimization problem. Instead of handling a large set of complicated SOCP constraints, our new formulation can be solved very efficiently by resolving a set of sparse linear equations. Based on the new framework, a robust iterative method is employed to handle large outliers. We have conducted an extensive set of experiments to evaluate the performance on both synthetic and real-world testbeds, from which the promising results show that the proposed algorithm not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["14"]}
{"title": "Face recognition committee machine\n", "abstract": " Face recognition has been of interest to a growing number of researchers due to its applications on security. Within past years, there are numerous face recognition algorithms proposed by researchers. However, there is no unified framework for the integration. We implement different existing well-known algorithms, eigenface, Fisherface, elastic graph matching (EGM), support vector machine (SVM) and neural network, to give a comprehensive testing under same face databases. Moreover, we present a face recognition committee machine (FRCM), which is a novel approach for assembling the outputs of various face recognition algorithms to obtain a unified decision with improved accuracy. The machine consists of an ensemble of the above algorithms to cope with various face images. We have tested our system with the ORL face database and Yale face database. A comparative experimental result of different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["14"]}
{"title": "Digital video watermarking techniques for secure multimedia creation and delivery\n", "abstract": " Due to the extensive use of digital media applications, multimedia security and the copyright protection has gained tremendous important. Digital watermarking is a technology used for the copyright protection of digital application. In this paper we have compressive approach for digital video watermarking is introduced, were watermark image is embedded in to the video frame each video frame is decomposed in to sub images using 2 level Discrete Wavelet Transform (DWT) and Principal Component Analysis (PCA) Transform is applied for each block in the two bands LL & HH.[1, 2] combining the two transform improved the performance of the watermark algorithm. The scheme is tested by various attacks. Experimental result shows no visible difference between watermark frame and original video frame, it shows the robustness against a wide range of attack such as Gaussion noise, salt & pepper Noise, median filtering, rotation, cropping etc. The Proposed scheme is tested using number of video sequences. its experimental result shows high imperceptibility where there is no noticeable difference between the watermark video frame and original video frame. Without attacking any noise on to the watermark video frame the computed normalized correlation (NC) is 1 and Peak Signal to Noise Ratio (PSNR) having high Score which is 44.097.", "num_citations": "36\n", "authors": ["14"]}
{"title": "Wsp: A network coordinate based web service positioning framework for response time prediction\n", "abstract": " With the rapid growth of Web services in recent years, the optimal service selection from functionally-equivalent service candidates has become more critical for building high quality service-oriented systems. To provide accurate QoSvalues for service selection, user-side QoS prediction thus becomes an important research problem. Although collaborative filtering based prediction approaches have been studied in several previous works, these methods suffer from the limitation of the sparsity of available historical QoS data, which greatly degrades the prediction accuracy. To address this problem, this paper proposes a Web service positioning (WSP)framework for response time prediction, which is one of the most important QoS properties. In our approach, a small set of landmarks are deployed to periodically monitor the response times of the Web service candidates and provide references to the numerous service\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["14"]}
{"title": "Cross-library api recommendation using web search engines\n", "abstract": " Software systems are often built upon third party libraries. Developers may replace an old library with a new library, for the consideration of functionality, performance, security, and so on. It is tedious to learn the often complex APIs in the new library from the scratch. Instead, developers may identify the suitable APIs in the old library, and then find counterparts of these APIs in the new library. However, there is typically no such cross-references for APIs in different libraries. Previous work on automatic API recommendation often recommends related APIs in the same library. In this paper, we propose to mine search results of Web search engines to recommend related APIs of different libraries. In particular, we use Web search engines to collect relevant Web search results of a given API in the old library, and then recommend API candidates in the new library that are frequently appeared in the Web search results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["14"]}
{"title": "Nonrigid shape recovery by gaussian process regression\n", "abstract": " Most state-of-the-art nonrigid shape recovery methods usually use explicit deformable mesh models to regularize surface deformation and constrain the search space. These triangulated mesh models heavily relying on the quadratic regularization term are difficult to accurately capture large deformations, such as severe bending. In this paper, we propose a novel Gaussian process regression approach to the nonrigid shape recovery problem, which does not require to involve a predefined triangulated mesh model. By taking advantage of our novel Gaussian process regression formulation together with a robust coarse-to-fine optimization scheme, the proposed method is fully automatic and is able to handle large deformations and outliers. We conducted a set of extensive experiments for performance evaluation in various environments. Encouraging experimental results show that our proposed approach is both\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["14"]}
{"title": "Real-time non-rigid shape recovery via active appearance models for augmented reality\n", "abstract": " One main challenge in Augmented Reality (AR) applications is to keep track of video objects with their movement, orientation, size, and position accurately. This poses a challenging task to recover non-rigid shape and global pose in real-time AR applications. This paper proposes a novel two-stage scheme for online non-rigid shape recovery toward AR applications using Active Appearance Models (AAMs). First, we construct 3D shape models from AAMs offline, which do not involve processing of the 3D scan data. Based on the computed 3D shape models, we propose an efficient online algorithm to estimate both 3D pose and non-rigid shape parameters via local bundle adjustment for building up point correspondences. Our approach, without manual intervention, can recover the 3D non-rigid shape effectively from either real-time video sequences or single image. The recovered 3D pose parameters can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["14"]}
{"title": "Recognition of merged characters based on forepart prediction, necessity-sufficiency matching, and character-adaptive masking\n", "abstract": " Merged characters are the major cause of recognition errors. We classify the merging relationship between two involved characters into three types: \"linear\", \" nonlinear\", and \"overlapped\". Most segmentation methods handle the first type well, however, their capabilities of handling the other two types are limited. The weakness of handling the nonlinear and overlapped types results from character segmentation by linear, usually vertical, cuts assumed in these methods. This paper proposes a novel merged character segmentation and recognition method based on forepart prediction, necessity-sufficiency matching and character-adaptive masking. This method utilizes the information obtained from the forepart of merged characters to predict candidates for the leftmost character, and then applies character-adaptive masking and character recognition to verifying the prediction. Therefore, the arbitrary-shaped cutting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["14"]}
{"title": "Optimal fault tolerance strategy selection for web services\n", "abstract": " Service-oriented systems are usually composed by heterogeneous Web services, which are distributed across the Internet and provided by organizations. Building highly reliable service-oriented systems is a challenge due to the highly dynamic nature of Web services. In this paper, the authors apply software fault tolerance techniques for Web services, where the component failures are handled by fault tolerance strategies. In this paper, a distributed fault tolerance strategy evaluation and selection framework is proposed based on versatile fault tolerance techniques. The authors provide a systematic comparison of various fault tolerance strategies by theoretical formulas, as well as real-world experiments. This paper also presents the optimal fault tolerance strategy selection algorithm, which employs both the QoS performance of Web services and the requirements of service users for selecting optimal fault\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["14"]}
{"title": "A fast 2d shape recovery approach by fusing features and appearance\n", "abstract": " In this paper, we present a fusion approach to solve the nonrigid shape recovery problem, which takes advantage of both the appearance information and the local features. We have two major contributions. First, we propose a novel progressive finite Newton optimization scheme for the feature-based nonrigid surface detection problem, which is reduced to only solving a set of linear equations. The key is to formulate the nonrigid surface detection as an unconstrained quadratic optimization problem that has a closed-form solution for a given set of observations. Second, we propose a deformable Lucas-Kanade algorithm that triangulates the template image into small patches and constrains the deformation through the second-order derivatives of the mesh vertices. We formulate it into a sparse regularized least squares problem, which is able to reduce the computational cost and the memory requirement. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["14"]}
{"title": "Web page classification with heterogeneous data fusion\n", "abstract": " Web pages are more than text and they contain much contextual and structural information, eg, the title, the meta data, the anchor text, etc., each of which can be seen as a data source or are presentation. Due to the different dimensionality and different representing forms of these heterogeneous data sources, simply putting them together would not greatly enhance the classification performance. We observe that via a kernel function, different dimensions and types of data sources can be represented into acommon format of kernel matrix, which can be seen as a generalized similarity measure between a pair of web pages. In this sense, a kernel learning approach is employed to fuse these heterogeneous data sources. The experimental results on a collection of the ODP database validate the advantages of the proposed method over traditional methods based on any single data source and the uniformly weighted\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["14"]}
{"title": "Comparative studies on feature extraction methods for multispectral remote sensing image classification\n", "abstract": " Feature extraction of multispectral remote sensing image is an important task before classifying the image. When land areas are clustered into groups of similar land cover, one of the most important things is to extract the key features of a given image. Usually multispectral remote sensing images have many bands, and there may have been much redundancy information and it becomes difficult to extract the key features of the image. Therefore, it is necessary to study methods regarding how to extract the main features of the image effectively. In this paper, five methods are comparatively studied to reduce the multi-bands into lower dimensions in order to extract the most available features. These methods include the Euclid distance measurement (EDM), the discrete measurement criteria function (DMCF), the minimum differentiated entropy (MDE), the probability distance criterion (PDC), and the principle component\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["14"]}
{"title": "A robust statistic method for classifying color polarity of video text\n", "abstract": " Video text extraction and recognition are prerequisite tasks for video indexing and retrieval. Color polarity classification of video text is very important to these tasks. Most existing text extraction methods assume that the text color is always light (or dark). Obviously, this assumption restricts the application of these methods to some specific domains. Only a few methods can detect the color polarity on condition that the background is clear. However, many real video texts have various appearances and complex backgrounds that existing methods cannot handle. This paper proposes a statistic color polarity classification method that is robust to various background complexities, font styles, stroke widths, and languages. We discover the intrinsic relationships between text edges and background edges, and then develop an efficient measurement to detect the color polarity. The experimental results show that the proposed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["14"]}
{"title": "Random unit-test generation with MUT-aware sequence recommendation\n", "abstract": " A key component of automated object-oriented unit-test generation is to find method-call sequences that generate desired inputs of a method under test (MUT). Previous work cannot find desired sequences effectively due to the large search space of possible sequences. To address this issue, we present a MUT-aware sequence recommendation approach called RecGen to improve the effectiveness of random object-oriented unit-test generation. Unlike existing random testing approaches that select sequences without considering how a MUT may use inputs generated from sequences, RecGen analyzes object fields accessed by a MUT and recommends a short sequence that mutates these fields. In addition, for MUTs whose test generation keeps failing, RecGen recommends a set of sequences to cover all the methods that mutate object fields accessed by the MUT. This technique further improves the chance of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["14"]}
{"title": "Multi-task learning for one-class classification\n", "abstract": " In this paper, we address the problem of one-class classification. Taking into account the fact that in some applications, the given training samples are rather limited, we attempt to utilize the advantages of Multi-task Learning (MTL), where the data of related tasks may share similar structure and helpful information. We then propose an MTL framework for one-class classification. The framework derives from the one-class v-SVM and makes use of related tasks by constraining them to have similar solutions. This formulation can be cast into a second-order cone program, which achieves a global solution and is solved efficiently. Further, the framework also maintains the favorable property of the v parameter in the v-SVM, which can control the fraction of outliers and support vectors, in one-class classification. This framework also connects with several existing models. Experimental results on both synthetic and real-world\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["14"]}
{"title": "A hierarchical mixture model for software reliability prediction\n", "abstract": " It is important to develop general prediction models in current software reliability research. In this paper, we propose a hierarchical mixture of software reliability models (HMSRM) for software reliability prediction. This is an application of the hierarchical mixtures of experts (HME) architecture. In HMSRM, individual software reliability models are used as experts. During the training of HMSRM, an Expectation\u0393\u00c7\u00f4Maximizing (EM) algorithm is employed to estimate the parameters of the model. Experiments illustrate that our approach performs quite well in the later stages of software development, and better than single classical software reliability models. We show that the method can automatically select the most appropriate lower-level model for the data and performances are well in prediction.", "num_citations": "33\n", "authors": ["14"]}
{"title": "ART: augmented reality table for interactive trading card game\n", "abstract": " Real world games and computer games have their own distinct strengths. Augmented reality allows us to combine both strengths, improve existing game styles, and produce new games. In this paper, we present the Augmented Reality Table (ART), a prototype platform employing augmented reality technology to provide a virtual table for playing trading card games. ART consists of an overhead head camera to perceive card inputs and player commands, and a plasma TV placing horizontally to act as the game table, display 3D models, and generate sound for the game play. The idea is to provide an interactive environment and input method for players, much the same as the original game, but with 3D graphics and sound enhancement to visualize and realize the game while maintaining complicated game rules at the same time. We illustrate ART as a system engaging augmented reality techniques to enable card\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["14"]}
{"title": "Effective multiresolution arc segmentation: Algorithms and performance evaluation\n", "abstract": " Arc segmentation plays an important role in the process of graphics recognition from scanned images. The GREC arc segmentation contest shows that there is a lot of room for improvement in this area. This paper proposes a multiresolution arc segmentation method based on our previous seeded circular tracking algorithm which largely depends on the OOPSV model. The newly-introduced multiresolution paradigm can handle arcs/circles with large radii well. We describe new approaches for arc seed detection, arc localization, and arc verification, making the proposed method self-contained and more efficient. Moreover, this paper also brings major improvement to the dynamic adjustment algorithm of circular tracking to make it more robust. A systematic performance evaluation of the proposed method has been conducted using the third-party evaluation tool and test images obtained from the GREC arc\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["14"]}
{"title": "Performance and effectiveness analysis of checkpointing in mobile environments\n", "abstract": " Many mathematical models have been proposed to evaluate the execution performance of an application with and without checkpointing in the presence of failures. They assume that the total program execution time without failure is known in advance, under which condition the optimal checkpointing interval can be determined. In mobile environments, application components are distributed and tasks are computed by sending and receiving computational and control messages. The total execution time includes communication time and depends on multiple factor, such as heterogeneous processing speeds, link bandwidth, etc., making it unpredictable during different executions. However, the number of total computational messages received is usually unchanged within an application. Another special factor that should be considered for checkpointing purpose is handoff, which often happens in mobile networks\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["14"]}
{"title": "Software diversity metrics and measurements\n", "abstract": " The authors define and formalize the concept of software diversity which characterizes N-Version software (NVS) from four different points of view that are designated as structural diversity, fault diversity, tough-spot diversity, and failure diversity. The goals are to find a way to quantify software diversity and to investigate the measurements which can be applied during the life cycle of NVS to gain confidence that operation will be dependable when NVS is actually used. The versions from a six-language N-Version programming project for fault-tolerant flight control software were used in the software diversity measurement.<>", "num_citations": "33\n", "authors": ["14"]}
{"title": "Multi-version software development\n", "abstract": " Multi-version software systems achieve fault tolerance through software redundancy. Diverse software versions are executed concurrently by a supervisory system that reports consensus results, allowing the results from erroneous versions to be masked by the majority. The Second Generation Experiment is a large scale empirical study of multi-version software systems engaging researchers at six sites. This paper presents UCLA's perspective of this experiment, its role in the preliminary analysis, and related research at the Dependable Computing and Fault Tolerant Systems Laboratory.", "num_citations": "33\n", "authors": ["14"]}
{"title": "Paid: Prioritizing app issues for developers by tracking user reviews over versions\n", "abstract": " User review analysis is critical to the bug-fixing and version-modification process for app developers. Many research efforts have been put to user review mining in discovering app issues, including laggy user interface, high memory overhead, privacy leakage, etc. Existing exploration of app reviews generally depends on static collections. As a result, they largely ignore the fact that user reviews are tightly related to app versions. Furthermore, the previous approaches require a developer to spend much time on filtering out trivial comments and digesting the informative textual data. This would be labor-intensive especially to popular apps with tremendous reviews. In the paper, we target at designing a framework in Prioritizing App Issues for Developers (PAID) with minimal manual power and good accuracy. The PAID design is based on the fact that the issues presented in the level of phrase, i.e., a couple of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["14"]}
{"title": "A novel kernel-based maximum a posteriori classification method\n", "abstract": " Kernel methods have been widely used in pattern recognition. Many kernel classifiers such as Support Vector Machines (SVM) assume that data can be separated by a hyperplane in the kernel-induced feature space. These methods do not consider the data distribution and are difficult to output the probabilities or confidences for classification. This paper proposes a novel Kernel-based Maximum A Posteriori (KMAP) classification method, which makes a Gaussian distribution assumption instead of a linear separable assumption in the feature space. Robust methods are further proposed to estimate the probability densities, and the kernel trick is utilized to calculate our model. The model is theoretically and empirically important in the sense that: (1) it presents a more generalized classification model than other kernel-based algorithms, e.g.,\u252c\u00e1Kernel Fisher Discriminant Analysis (KFDA); (2) it can output probability or\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["14"]}
{"title": "Software reliability growth models incorporating fault dependency with various debugging time lags\n", "abstract": " Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and most SRGMs assume that detected faults are immediately corrected. Actually, this assumption may not be realistic in practice. In this paper we first give a review of fault detection and correction processes in software reliability modeling. Furthermore, we show how several existing SRGMs based on NHPP models can be derived by applying the time-dependent delay function. On the other hand, it is generally observed that mutually independent software faults are on different program paths. Sometimes mutually dependent faults can be removed if and only if the leading faults were removed. Therefore, here we incorporate the ideas of fault dependency and time-dependent delay\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["14"]}
{"title": "Online nonlinear AUC maximization for imbalanced data sets\n", "abstract": " Classifying binary imbalanced streaming data is a significant task in both machine learning and data mining. Previously, online area under the receiver operating characteristic (ROC) curve (AUC) maximization has been proposed to seek a linear classifier. However, it is not well suited for handling nonlinearity and heterogeneity of the data. In this paper, we propose the kernelized online imbalanced learning (KOIL) algorithm, which produces a nonlinear classifier for the data by maximizing the AUC score while minimizing a functional regularizer. We address four major challenges that arise from our approach. First, to control the number of support vectors without sacrificing the model performance, we introduce two buffers with fixed budgets to capture the global information on the decision boundary by storing the corresponding learned support vectors. Second, to restrict the fluctuation of the learned decision function\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "An online performance prediction framework for service-oriented systems\n", "abstract": " The exponential growth of Web service makes building high-quality service-oriented systems an urgent and crucial research problem. Performance of the service-oriented systems highly depends on the remote Web services as well as the unpredictability of the Internet. Performance prediction of service-oriented systems is critical for automatically selecting the optimal Web service composition. Since the performance of Web services is highly related to the service status and network environments which are variable over time, it is an important task to predict the performance of service-oriented systems at run-time. To address this critical challenge, this paper proposes an online performance prediction framework, called OPred, to provide personalized service-oriented system performance prediction efficiently. Based on the past usage experience from different users, OPred builds feature models and employs time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "A qos-aware middleware for fault tolerant web services\n", "abstract": " Reliability is a key issue of the service-oriented architecture (SOA) which is widely employed in critical domains such as e-commerce and e-government. Redundancy-based fault tolerance strategies are usually employed for building reliable SOA on top of unreliable remote Web services. Based on the idea of user-collaboration, this paper proposes a QoS-aware middleware for fault tolerant Web services. Based on this middleware, service-oriented applications can dynamically adjust their optimal fault tolerance strategy to achieve good service reliability as well as good overall performance. A dynamic fault tolerance replication strategy is designed and evaluated. Experiments are conducted to illustrate the advantage of the proposed middleware as well as the dynamic fault tolerance replication strategy. Comparison of the effectiveness of the proposed dynamic fault tolerance strategy and various traditional fault\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "A multilingual, multimodal digital video library system\n", "abstract": " This paper presents the iVIEW system, a multi-lingual, multi-modal digital video content management system for intelligent searching and access of English and Chinese video contents. iVIEW allows full content indexing, searching and retrieval of multi-lingual text, audio and video material. It consists image processing techniques for scenes and scene changes analyses, speech processing techniques for audio signal transcriptions, and multi-lingual natural language processing techniques for word relevance determination. iVIEW can host multi-lingual contents and allow multi-modal search. It facilitate content developers to perform multi-modal information processing of rich video media and to construct XML-based multimedia representation in enhancing multi-modal indexing and searching capabilities, so that the end users can enjoy viewing flexible and seamless delivery of multimedia contents in various\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "Software reliability analysis incorporating fault detection and debugging activities\n", "abstract": " The software reliability measurement problem can be approached by obtaining the estimates of the residual number of faults in the software. Traditional black box based approaches to software reliability modeling assume that the debugging process is instantaneous and perfect. The estimates of the remaining number of faults, and hence reliability, are based on these oversimplified assumptions and they tend to be optimistic. We propose a framework relying on rate based simulation technique for incorporating explicit debugging activities along with the possibility of imperfect debugging into the black box software reliability models. We present various debugging policies and analyze the effect of these policies on the residual number of faults in the software. In addition, we propose a methodology to compute the reliability of the software, taking into account explicit debugging activities. An economic cost model to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "Fault-tolerant software reliability engineering\n", "abstract": " Software development processes and methods have been studied for decades. Despite that, we still do not have reliable tools to guarantee that complicated software systems are fault-free. In fact, it may never happen that we will be able to guarantee error-free software. The reason is that the two basic ways of showing that software is correct, proof of program correctness and exhaustive testing, may never be practical for use with very complex software-based systems, although reuse of reliable software building blocks (objects) may go a long way toward achieving that goal. Techniques for proving software correct tend to work only for relatively small and simple synchronous systems, while testing methods, although increasingly more sophisticated, do not guarantee production of error-free code because exhaustive testing is not practical in most cases. Therefore, it is reasonable to investigate techniques that permit\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["14"]}
{"title": "A generalized technique for simulating software reliability\n", "abstract": " Although several models have been proposed for assessing software reliability, none has emerged as the most effective predictor. The authors offer a general simulation technique that relaxes or removes many of the usual reliability-modeling assumptions and expends the reliability process to encompass the entire software life cycle.", "num_citations": "31\n", "authors": ["14"]}
{"title": "Information aggregation for multi-head attention with routing-by-agreement\n", "abstract": " Multi-head attention is appealing for its ability to jointly extract different types of information from multiple representation subspaces. Concerning the information aggregation, a common practice is to use a concatenation followed by a linear transformation, which may not fully exploit the expressiveness of multi-head attention. In this work, we propose to improve the information aggregation for multi-head attention with a more powerful routing-by-agreement algorithm. Specifically, the routing algorithm iteratively updates the proportion of how much a part (i.e. the distinct information learned from a specific subspace) should be assigned to a whole (i.e. the final output representation), based on the agreement between parts and wholes. Experimental results on linguistic probing tasks and machine translation tasks prove the superiority of the advanced information aggregation over the standard linear transformation.", "num_citations": "30\n", "authors": ["14"]}
{"title": "Experience report: Understanding cross-platform app issues from user reviews\n", "abstract": " App developers publish apps on different platforms, such as Google Play, App Store, and Windows Store, to maximize the user volumes and potential revenues. Due to the different characteristics of the platforms and the different user preference (e.g., Android is more customized than iOS), app testing cases on these three platforms should also be designed differently. Comprehensive app testing can be time-consuming for developers. Therefore, understanding the differences of the app issues on these platforms can facilitate the testing process. In this paper, we propose a novel framework named CrossMiner to analyze the essential app issues and explore whether the app issues exhibit differently on the three platforms. Based on five million user reviews, the framework automatically captures the distributions of seven app issues, i.e., \"battery\", \"crash\", \"memory\", \"network\", \"privacy\", \"spam\", and \"UI\". We discover that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["14"]}
{"title": "Towards online, accurate, and scalable qos prediction for runtime service adaptation\n", "abstract": " Service-based cloud applications are typically built on component services to fulfill certain application logic. To meet quality-of-service (QoS) guarantees, these applications have to become resilient against the QoS variations of their component services. Runtime service adaptation has been recognized as a key solution to achieve this goal. To make timely and accurate adaptation decisions, effective QoS prediction is desired to obtain the QoS values of component services. However, current research has focused mostly on QoS prediction of the working services that are being used by a cloud application, but little on QoS prediction of candidate services that are also important for making adaptation decisions. To bridge this gap, in this paper, we propose a novel QoS prediction approach, namely adaptive matrix factorization (AMF), which is inspired from the collaborative filtering model used in recommender systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["14"]}
{"title": "Highly scalable sequential pattern mining based on MapReduce model on the cloud\n", "abstract": " Sequential pattern mining is an essential data mining technique that has been widely applied to many real world applications. However, traditional algorithms generally suffer from the scalability problem when dealing with big data. In this paper, we aim to significantly upgrade the scale and propose Sequential PAttern Mining algorithm based on MapReduce model on the Cloud (abbreviated as SPAMC). Derived from the prior SPAM algorithm, we design an iterative MapReduce framework to efficiently generate and prune candidate patterns when constructing the lexical sequence tree. This framework not only distributes the sub-tasks of tree construction to independent mappers in parallel, but also enables the parallel processing of support counting. We conduct extensive experiments on the cloud environment of 32 virtual machines with up to 12.8 million transactional sequences. Experimental results show that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["14"]}
{"title": "Semantic video summarization using mutual reinforcement principle and shot arrangement patterns\n", "abstract": " We propose a novel semantic video summarization framework, which generates video skimmings that guarantee both the balanced content coverage and the visual coherence. First, we collect video semantic information with a semi-automatic video annotation tool. Secondly, we analyze the video structure and determine each video scene\u0393\u00c7\u00d6s target skim length. Then, mutual reinforcement principle is used to compute the relative importance value and cluster the video shots according to their semantic descriptions. Finally, we analyze the arrangement pattern of the video shots, and the key shot arrangement patterns are extracted to form the final video skimming, where the video shot importance value is used as guidance. Experiments are conducted to evaluate the effectiveness of our proposed approach.", "num_citations": "30\n", "authors": ["14"]}
{"title": "A novel scheme for video similarity detection\n", "abstract": " In this paper, a new two-phase scheme for video similarity detection is proposed. For each video sequence, we extract two kinds of signatures with different granularities: coarse and fine. Coarse signature is based on the Pyramid Density Histogram (PDH) technique and fine signature is based on the Nearest Feature Trajectory (NFT) technique. In the first phase, most of unrelated video data are filtered out with respect to the similarity measure of the coarse signature. In the second phase, the query video example is compared with the results of the first phase according to the similarity measure of the fine signature. Different from the conventional nearest neighbor comparison, our NFT based similarity measurement method well incorporates the temporal order of video sequences. Experimental results show that our scheme achieves better quality results than the conventional approach.", "num_citations": "30\n", "authors": ["14"]}
{"title": "A new approach for line recognition in large-size images using Hough transform\n", "abstract": " Applications of the Hough Transform (HT) have been limited to small-size images for a long time. For large-size images, peak detection and line verification become much more time-consuming. Many HT-based line detection methods are not able to detect line width. This paper proposes a new approach for detecting line segments using HT, with applicability to large size images, especially for those situations where line width is critical. Our approach applies a boundary recorder to eliminate redundant analyses, and employs an image-analysis-based line-verification method to overcome the difficulty of using a threshold to distinguish short lines from noise. It avoids overlapping lines by removing the pixels of detected line segments, a method which is more robust than only clearing the N/spl times/N neighborhood. This approach could be easily extended to improved HT methods that perform global accumulation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["14"]}
{"title": "Some Coupling Measures for C++ Programs.\n", "abstract": " There is a great deal of\" hype\" about the objectoriented paradigm offering all the solutions to the problems of software engineering. Goals of software engineering like reliability, maintainability, and reusability are said to be more easily achieved using this paradigm, than with traditional ones based on functional decomposition. In order to monitor whether these goals are indeed being achieved, appropriate measures are necessary. Widely used complexity metrics like lines of code, cyclomatic complexity, and Software Science\u0393\u00c7\u00d6s metrics may not be appropriate, since they do not address object-oriented concepts like inheritance and encapsulation, apart from having other weaknesses. We consider one attribute of object-oriented software-coupling-and define some measures based in measurement theory. Though these measures have been defined primarily for C++, they could be extended to other object-oriented languages. We then computed the measures for five large (by university standards) C++ software, and studied their correlation with the difficulty of maintenance as perceived by the developers of the software. Our preliminary results show that our coupling measures correlate better with difficulty of maintenance than the three widely used complexity metrics.", "num_citations": "30\n", "authors": ["14"]}
{"title": "A heuristic approach for software reliability prediction: the equally-weighted linear combination model.\n", "abstract": " This paper proposes a heuristic approach to addressing the software reliability modeling problem. The heuristic approach is based on a linear combination of three popular software reliability models. A simple, predetermined combination is suggested by assigning equal weights to each component model for the final delivery of the software reliability prediction. In a preliminary examination, this Equally-Weighted Linear Combination (ELC) Model is judged to perform well when applied to three published software failure data sets. We further present five other sets of software failure data taken recently from major projects at the Jet Propulsion Laboratory, and apply the ELC model as well as six other popular models for a detailed comparison and evaluation. A number of statistical techniques are used to determine the applicability of these software reliability models. Our evaluation results indicate that the proposed ELC\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["14"]}
{"title": "Scaling service-oriented applications into geo-distributed clouds\n", "abstract": " With the significant prevalence of cloud computing, more and more data centers are built to host and deliver various online services. However, a key challenge faced by service providers is how to scale their applications into geo-distributed data centers to improve application performance as well as minimizing the operational cost. While most existing deployment methods ignore the service dependencies in an application, this paper proposes a general dynamic service deployment framework to bridge this gap, in which a deployment manager and a local scheduler are designed to optimize data center selection and auto-scale the service instances in each data center respectively. More specifically, we formulate the deployment problem across multiple data centers as a compact minimization model, which can be solved efficiently by a genetic algorithm. To evaluate the performance of our approach, extensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["14"]}
{"title": "An empirical study on large-scale content-based image retrieval\n", "abstract": " One key challenge in content-based image retrieval (CBIR) is to develop a fast solution for indexing high-dimensional image contents, which is crucial to building large-scale CBIR systems. In this paper, we propose a scalable content-based image retrieval scheme using locality-sensitive hashing (LSH), and conduct extensive evaluations on a large image testbed of a half million images. To the best of our knowledge, there is less comprehensive study on large-scale CBIR evaluation with a half million images. Our empirical results show that our proposed solution is able to scale for hundreds of thousands of images, which is promising for building Web-scale CBIR systems.", "num_citations": "29\n", "authors": ["14"]}
{"title": "An experimental evaluation on reliability features of N-version programming\n", "abstract": " Although N-version programming has been employed in some mission-critical applications, the reliability and fault correlation issues remain a debatable topic in the research community. In this paper, we perform a comprehensive evaluation on our recent project data on N-version programming and present statistical investigations on coincident failures and correlated faults. Furthermore, we compare our project with NASA 4-University project to identify the \"variants\" and \"invariants\" with respect to failure rate, fault density, coincident failures, related faults, and reliability improvement for N-version programming. Our experimental results support fault tolerance as an effective software reliability engineering technique", "num_citations": "29\n", "authors": ["14"]}
{"title": "A novel video summarization framework for document preparation and archival applications\n", "abstract": " With the rapid growth of network bandwidth and high-capacity storage devices, videos have become an important way of communication in the aerospace industry and many other entities. However, browsing and managing huge video databases are quite tedious. To solve the problem, in this paper, we propose a novel video summarization framework, and discuss its potential usage in the document preparation and archival applications. The proposed framework generates video skimmings that guarantee both the balanced content coverage and the visual coherence. First, we segment the raw video into video shots, analyze the structure of the video, find the boundaries of semantic scenes, then calculate each scene's skimming length by its structure and content entropy. Second, we define a spatial-temporal dissimilarity function between video shots, model each video scene as a graph, and find each scene's\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["14"]}
{"title": "Outliers treatment in support vector regression for financial time series prediction\n", "abstract": " Recently, the Support Vector Regression (SVR) has been applied in the financial time series prediction. The financial data are usually highly noisy and contain outliers. Detecting outliers and deflating their influence are important but hard problems. In this paper, we propose a novel \u0393\u00c7\u00a3two-phase\u0393\u00c7\u00a5 SVR training algorithm to detect outliers and reduce their negative impact. Our experimental results on three indices: Hang Seng Index, NASDAQ, and FSTE 100 index show that the proposed \u0393\u00c7\u00a3two-phase\u0393\u00c7\u00a5 algorithm has improvement on the prediction.", "num_citations": "29\n", "authors": ["14"]}
{"title": "Software reliability theory\n", "abstract": " Software reliability modeling has, surprisingly to many, been around since the early 1970s with the pioneering works of Jelinski and Moranda, Shooman, and Coutinho. The theory behind software reliability is presented, and some of the major models that have appeared in the literature from both historical and applications perspectives are described. Emerging techniques for software reliability research field are also included. The following four key components in software reliability theory and modeling: historical background, theory, modeling, and emerging techniques are addressed. These items are discussed in a general way, rather than attempting to discuss a long list of details.Software reliability modeling has, surprisingly to many, been around since the early 1970s with the pioneering works of Jelinski and Moranda (1972), Shooman (1972, 1973, 1976, 1977), and Coutinho (1973). We present the theory behind software reliability, and describe some of the major models that have appeared in the literature from both historical and applications perspectives. Emerging techniques for software reliability research field are also included. We address the following four key components in software reliability theory and modeling: historical background, theory, modeling, and emerging techniques. We describe these items in a general way, rather than attempting to discuss a long list of details. For a comprehensive treatment of this subject, see Lyu (1996).", "num_citations": "29\n", "authors": ["14"]}
{"title": "Reliability simulation of fault-tolerant software and systems\n", "abstract": " Fault tolerance is a survival attribute of complex computer systems and software in their ability to deliver continuous service to their users in the presence of faults. Formulating an analytic model for dependability and performance evaluation of hardware/software fault tolerant architectures can be quite cumbersome. Also, in practice, isolating the effect of various parameters on a system, while holding the others constant requires exploring a variety of scenarios. It is economically infeasible to build several such systems. Simulation offers an attractive mechanism for dependability evaluation and the study of the influence of various parameters on the failure behavior of the system. In this paper, we develop algorithms to simulate the failure behavior of three commonly used fault tolerant architectures, viz., Distributed Recovery Block (DRB), N-Version Programming (NVP) and N-Self Checking Programming (NSCP). We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["14"]}
{"title": "Optimization of reliability allocation and testing schedule for software systems\n", "abstract": " To ensure an overall reliability of an integrated software system, software components of the system have to meet certain reliability requirements, subject to some testing schedule and resource constraints. The system testing activity can be formulated as a combinatorial optimization problem with known cost, reliability, effort and other attributes of the system components. In this paper, we consider the software component reliability allocation problem for a system with multiple applications. The failure rate of components used to build the applications are related to the testing cost through various types of reliability growth curves. We achieve closed-form solutions to problems where there is one single application in the system. Analytical solutions are not readily available when there are multiple applications; however, numerical solutions can be obtained using a nonlinear programming tool. To ease the specification of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["14"]}
{"title": "A latency-aware co-deployment mechanism for cloud-based services\n", "abstract": " Cloud computing attracts considerable attention from both industry and academic these years. Nowadays, a number of research investigations have been conducted on cloud-based services (e.g., IaaS, PaaS, SaaS, etc.). Deployment of cloud-based services is one of the most important research problems. In cloud computing, multiple services tend to cooperate with each other to accomplish complicated tasks. Deploying these services independently may not lead to good overall performance, since there are a lot of interactions among different services. Making an optimal co-deployment of multiple services is critical for reducing latency of user requests. When deploying highly related services, taking only distribution of users into consideration is not enough, since the deployment of one service would affect others. To attack this challenge, we employ cross service information as well as user locations to build a new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["14"]}
{"title": "Spectrum analysis based onwindows with variable widths for online signature verification\n", "abstract": " In this paper, an online signature verification scheme based on spectrum analysis and Mahalanobis decision is proposed. We firstly divided signatures to a number of frames with variable widths according to the characteristics of the time sequences, and then employed the fast Fourier transformation (FFT) to extract the spectrum of signatures. The distance between the Fourier coefficient within the corresponding frames is computed, and the Mahalanobis decision making is employed. Experimentation demonstrates that spectrum analysis based on windows with variable widths is effective for online signature signals", "num_citations": "28\n", "authors": ["14"]}
{"title": "A coverage analysis tool for the effectiveness of software testing\n", "abstract": " We describe a software testing and analysis tool, called ATAC (Automatic Test Analysis for C), which is developed as a research instrument at Bellcore to measure the effectiveness of testing data. The design, functionality, and usage of ATAC are presented in this paper. Furthermore, to demonstrate the capability and applicability of ATAC, we obtain the 12 program versions of a critical industrial application developed in a recent university/industry N-Version Software project, and use the ATAC tool to analyze and compare coverage of the testing conducted in the program versions. Preliminary results from this investigation show that ATAC could be a powerful testing tool to provide testing metrics and quality control guidance for the certification of high quality software components or systems. It can also assist software reliability researchers and practitioners in searching for the missing link between structure-based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["14"]}
{"title": "A user experience-based cloud service redeployment mechanism\n", "abstract": " Cloud computing has attracted much interest recently from both industry and academic. Nowadays, more and more Internet applications are moving to the cloud environment. Making optimal deployment of cloud applications is critical for providing good performance to attract users. Optimizing user experience is usually required for cloud service deployment. However, it is a challenging task to know the user experience of end users, since there is generally no proactive connection between a user to the machine that will host the service instance. To attack this challenge, in this paper, we first propose a framework to model cloud features and capture user experience. Then based on the collected user connection information, we formulate the redeployment of service instances as k-median and max k-cover problems. We proposed several approximation algorithms to efficiently solve these problems. Comprehensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["14"]}
{"title": "Robust face recognition using minimax probability machine\n", "abstract": " Face recognition has been widely explored. Many techniques have been applied in various applications. Robustness and reliability become more and more important for these applications especially, in security systems. A new face recognition approach is proposed based on a state-of-the-art classification technique called minimax probability machine (MPM). Engaging the binary MPM technique, we present a multi-class MPM classification for robust face recognition. In experiments, we compare our MPM-based face recognition algorithm with traditional techniques, including neural network and support vector machine. The experimental results show that the MPM-based face recognition technique is competitive and promising for robust face recognition", "num_citations": "27\n", "authors": ["14"]}
{"title": "Affinity rank: a new scheme for efficient web search\n", "abstract": " Maximizing only the relevance between queries and documents will not satisfy users if they want the top search results to present a wide coverage of topics by a few representative documents. In this paper, we propose two new metrics to evaluate the performance of information retrieval: diversity, which measures the topic coverage of a group of documents, and information richness, which measures the amount of information contained in a document. Then we present a novel ranking scheme, Affinity Rank, which utilizes these two metrics to improve search results. We demonstrate how Affinity Rank works by a toy data set, and verify our method by experiments on real-world data sets.", "num_citations": "27\n", "authors": ["14"]}
{"title": "Discriminative training of Bayesian Chow-Liu multinet classifiers\n", "abstract": " Discriminative classifiers such as support vector machines directly learn a discriminant function or a posterior probability model to perform classification. On the other hand, generative classifiers often learn a joint probability model and then use Bayes rules to construct a posterior classifier from this model. In general, generative classifiers are not as accurate as discriminant classifier. However generative classifiers provide a principled way to handle the missing information problems, which discriminant classifiers cannot easily deal with. To achieve good performances in various classification tasks, it is better to combine these two strategies. In this paper, we develop a novel method to iteratively train a kind of generative Bayesian classifier: Bayesian Chow-Liu multinet classifier in a discriminative way. Different with the traditional Bayesian multinet classifiers, our discriminative method adds into the optimization\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["14"]}
{"title": "Optimal allocation of testing resources for modular software systems\n", "abstract": " In this paper, based on software reliability growth models with generalized logistic testing-effort function, we study three optimal resource allocation problems in modular software systems during the testing phase: 1) minimization of the remaining faults when a fixed amount of testing-effort and a desired reliability objective are given; 2) minimization of the required amount of testing-effort when a specific number of remaining faults and a desired reliability objective are given; and 3) minimization of the cost when the number of remaining faults and a desired reliability objective are given. Several useful optimization algorithms based on the Lagrange multiplier method are proposed and numerical examples are illustrated. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. In addition, we also introduce the testing-resource control problem and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["14"]}
{"title": "Component-based embedded software engineering: development framework, quality assurance and a generic assessment environment\n", "abstract": " Embedded software is used to control the functions of mechanical and physical devices by dedicated digital signal processor and computers. Nowadays, heterogeneous and collaborative embedded software systems are widely adopted to engage the physical world. To make such software extremely reliable, very efficient and highly flexible, component-based embedded software development can be employed for the complex embedded systems, especially those based on object-oriented (OO) approaches. In this paper, we introduce a component-based embedded software framework and the features it inherits. We propose a quality assurance (QA) model for component-based embedded software development, which covers both the component QA and the system QA as well as their interactions. Furthermore, we propose a generic quality assessment environment for component-based embedded systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["14"]}
{"title": "Software reliability engineering study of a large-scale telecommunications software system\n", "abstract": " Software reliability is a crucial performance factor of telecommunications network elements and operational systems. This paper describes the state-of-practice software reliability engineering (SRE) methods that we selected and organized into an SRE framework for use at Bellcore. This framework comprises several SRE methods: determination of a reliability objective for a product, development and use of operational profiles, reliability modeling and estimation (prediction) to manage system testing, estimation of the product's reliability in the field, and subsequent validation of this estimate using actual field data. Reliability modeling involves assessment of several models according to their predictive accuracy and the use of the most accurate model for reliability estimation. We have successfully tested this framework on several pilot projects. As part of these projects, we tested the usefulness of three different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["14"]}
{"title": "Aggregated temporal tensor factorization model for point-of-interest recommendation\n", "abstract": " Point-of-interest\u252c\u00e1(POI) recommendation is an important application in location-based social networks\u252c\u00e1(LBSNs), which mines user check-in sequences to suggest interesting locations for users. Because user check-in behavior exhibits strong temporal patterns\u0393\u00c7\u00f6for instance, users would like to check-in at restaurants at noon and visit bars at night. Hence, capturing the temporal influence is necessary to ensure the high performance in a POI recommendation system. Previous studies observe that the temporal characteristics of user mobility in LBSNs can be summarized in three aspects: periodicity, consecutiveness, and non-uniformness. However, previous work does not model the three characteristics together. More importantly, we observe that the temporal characteristics exist at different time scales, which cannot be modeled in prior work. In this paper, we propose an Aggregated Temporal Tensor\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["14"]}
{"title": "Online reliability prediction via motifs-based dynamic Bayesian networks for service-oriented systems\n", "abstract": " A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction. We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["14"]}
{"title": "Fast Relative-Error Approximation Algorithm for Ridge Regression.\n", "abstract": " Ridge regression is one of the most popular and effective regularized regression methods, and one case of particular interest is that the number of features p is much larger than the number of samples n, ie p\u0393\u00eb\u00bd n. In this case, the standard optimization algorithm for ridge regression computes the optimal solution x\u0393\u00ea\u00f9 in O (n2p+ n3) time. In this paper, we propose a fast relativeerror approximation algorithm for ridge regression. More specifically, our algorithm outputs a solution x satisfying x\u0393\u00ea\u00c6 x\u0393\u00ea\u00f9 2\u0393\u00eb\u00f1 \u2567\u2561x\u0393\u00ea\u00f9 2 with high probability and runs inO (nnz (A)+ n3/\u2567\u25612) time, where nnz (A) is the number of non-zero entries of matrix A.To the best of our knowledge, this is the first algorithm for ridge regression that runs in o (n2p) time with provable relative-error approximation bound on the output vector. In addition, we analyze the risk inflation bound of our algorithm and apply our techniques to two generalizations of ridge regression, including multiple response ridge regression and a non-linear ridge regression problem. Finally, we show empirical results on both synthetic and real datasets.", "num_citations": "26\n", "authors": ["14"]}
{"title": "Qos-aware web service recommendation via collaborative filtering\n", "abstract": " With the increasing number of Web services on the Internet, selecting appropriate services to build one\u0393\u00c7\u00d6s application becomes a nontrivial issue. When searching Web services, users are often overwhelmed by a bunch of candidates with similar functionalities. Quality-of-Service (QoS), the non-functional characteristics of Web services, has become an important factor to distinguish the functionally equivalent ones. In this paper, we introduce two collaborative filtering based Web service recommendation approaches to help users select Web service with optimal QoS performance. The basic idea is to leverage user experience provided by similar users and generate recommendation for the target user. Experiments with large scale real world Web services show the effectiveness and efficiency of the two approaches.", "num_citations": "25\n", "authors": ["14"]}
{"title": "QoS management of web services\n", "abstract": " Web services are widely employed for building loosely coupled distributed systems, such as e-commerce, e-government, automotive systems, and multimedia services. Quality of service (QoS) is usually engaged for describing the nonfunctional characteristics of Web services. QoS management of Web services refers to the activities in QoS specification, evaluation, prediction, aggregation, and control of resources to meet end-to-end user and application requirements. This book delivers the main contemporary themes in service computing, including service QoS evaluation, service QoS prediction, and QoS-based service fault tolerance. To collect sufficient Web service QoS data, an effective and efficient Web service evaluation mechanism is required. However, in the real world, a comprehensive Web service evaluation may not be possible. Therefore, Web service QoS prediction approaches, which require no\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["14"]}
{"title": "Smooth optimization for effective multiple kernel learning\n", "abstract": " Multiple Kernel Learning (MKL) can be formulated as a convex-concave minmax optimization problem, whose saddle point corresponds to the optimal solution to MKL. Most MKL methods employ the L1-norm simplex constraints on the combination weights of kernels, which therefore involves optimization of a non-smooth function of the kernel weights. These methods usually divide the optimization into two cycles: one cycle deals with the optimization on the kernel combination weights, and the other cycle updates the parameters of SVM. Despite the success of their efficiency, they tend to discard informative complementary kernels. To improve accuracy, we introduce smoothness to the optimization procedure. Furthermore, we transform the optimization into a single smooth convex optimization problem and employ the Nesterov\u0393\u00c7\u00d6s method to efficiently solve the optimization problem. Experiments on benchmark data sets demonstrate that the proposed algorithm clearly improves current MKL methods in a number scenarios.", "num_citations": "25\n", "authors": ["14"]}
{"title": "Towards understanding neural machine translation with word importance\n", "abstract": " Although neural machine translation (NMT) has advanced the state-of-the-art on various language pairs, the interpretability of NMT remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.", "num_citations": "24\n", "authors": ["14"]}
{"title": "Gene selection based on mutual information for the classification of multi-class cancer\n", "abstract": " With the development of mirocarray technology, microarray data are widely used in the diagnoses of cancer subtypes. However, people are still facing the complicated problem of accurate diagnosis of cancer subtypes. Building classifiers based on the selected key genes from microarray data is a promising approach for the development of microarray technology; yet the selection of non-redundant but relevant genes is complicated. The selected genes should be small enough to allow diagnosis even in regular laboratories and ideally identify genes involved in cancer-specific regulatory pathways. Instead of the traditional gene selection methods used for the classification of two categories of cancers, in the present paper, a novel gene selection algorithm based on mutual information is proposed for the classification of multi-class cancer using microarray data, and the selected key genes are fed into the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["14"]}
{"title": "An empirical study on reliability modeling for diverse software systems\n", "abstract": " Reliability and fault correlation are two main concerns for design diversity, yet empirical data are limited in investigating these two. In previous work, we conducted a software project with real-world application for investigation on software testing and fault tolerance for design diversity. Mutants were generated by injecting one single real fault recorded in the software development phase to the final versions. In this paper, we perform more analysis and experiments on these mutants to evaluate and investigate the reliability features in diverse software systems. We apply our project data on two different reliability models and estimate the reliability bounds for evaluation purpose. We also parameterize fault correlations to predict the reliability of various combinations of versions, and compare three different fault-tolerant software architectures.", "num_citations": "24\n", "authors": ["14"]}
{"title": "A directed acyclic graph approach to online log parsing\n", "abstract": " Logs are widely used in modern software system management because they are often the only data accessible that record system events at runtime. In recent years, because of the ever-increasing log size, data mining techniques are often utilized to help developers and operators conduct system reliability management. A typical log-based system reliability management procedure is to first parse log messages because of their unstructured format; and apply data mining techniques on the parsed logs to obtain critical system behavior information. Most of existing research studies focus on offline log parsing, which need to parse logs in batch mode. However, software systems, especially distributed systems, require online monitoring and maintenance. Thus, a log parser that can parse log messages in a streaming manner is highly in demand. To address this problem, we propose an online log parsing method, namely Drain, based on directed acyclic graph, which encodes specially designed rules for parsing. Drain can automatically generate a directed acyclic graph for a new system and update the graph according to the incoming log messages. Besides, Drain frees developers from the burden of parameter tuning by allowing them use Drain with no pre-defined parameters. To evaluate the performance of Drain, we collect 11 log datasets generated by real-world systems, ranging from distributed systems, Web applications, supercomputers, operating systems, to standalone software. The experimental results show that Drain has the highest accuracy on all 11 datasets. Moreover, Drain obtains 37.15\\% 97.14\\% improvement in the running time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["14"]}
{"title": "Modeling and exploiting tag relevance for Web service mining\n", "abstract": " Web service tags, i.e., terms annotated by users to describe the functionality or other aspects of Web services, are being treated as collective user knowledge for Web service mining. Since user tagging is inherently uncontrolled, ambiguous, and overly personalized, a critical and fundamental problem is how to measure the relevance of a user-contributed tag with respect to the functionality of the annotated Web service. In this paper, we propose a hybrid mechanism by using Web Service Description Language documents and service-tag network information to compute the relevance scores of tags by employing semantic computation and Hyperlink-Induced Topic Search model, respectively. Further, we introduce tag relevance measurement mechanism into three applications of Web service mining: (1) Web service clustering; (2) Web service tag recommendation; and (3) tag-based Web service retrieval. To\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["14"]}
{"title": "A multimodal and multilevel ranking framework for content-based video retrieval\n", "abstract": " One critical task in content-based video retrieval is to rank search results with combinations of multimodal resources effectively. This paper proposes a novel multimodal and multilevel ranking framework for content-based video retrieval. The main idea of our approach is to represent videos by graphs and learn harmonic ranking functions through fusing multimodal resources over these graphs smoothly. We further tackle the efficiency issue by a multilevel learning scheme, which makes the semi-supervised ranking method practical for large-scale applications. Our empirical evaluations on TRECVID 2005 dataset show that the proposed multimodal and multilevel ranking framework is effective and promising for content-based video retrieval.", "num_citations": "23\n", "authors": ["14"]}
{"title": "POWER-SPEED: a power-controlled real-time data transport protocol for wireless sensor-actuator networks\n", "abstract": " This paper investigates the data transport problem for reporting delay-sensitive events in wireless sensor-actuator networks (WSANs). We specifically tailor the protocol design according to the features of WSANs and propose POWER-SPEED, a real-time data transport protocol for WSANs to achieve energy-efficient data transport for delay-sensitive event reporting. In POWER-SPEED, sensor nodes select the next-hop neighbor to actuators according to the spatio-temporal historic data of the upstream QoS condition, which completely avoids control packets. With an adaptive transmitter power control scheme, POWER-SPEED conveys packets in an energy-efficient manner while maintaining soft real-time packet transport. It thus reduces the energy consumption of data transport while ensuring the QoS requirement in timeliness domain. We demonstrate the effectiveness of POWER-SPEED through simulations with NS2.", "num_citations": "23\n", "authors": ["14"]}
{"title": "Local learning vs. global learning: An introduction to maxi-min margin machine\n", "abstract": " Abstract                          We present a unifying theory of the Maxi-Min Margin Machine (M4) that subsumes the Support Vector Machine (SVM), the Minimax Probability Machine (MPM), and the Linear Discriminant Analysis (LDA). As a unified approach, M4 combines some merits from these three models. While LDA and MPM focus on building the decision plane using global information and SVM focuses on constructing the decision plane in a local manner, M4 incorporates these two seemingly different yet complementary characteristics in an integrative framework that achieves good classification accuracy. We give some historical perspectives on the three models leading up to the development of M4. We then outline the M4 framework and perform investigations on various aspects including the mathematical definition, the geometrical interpretation, the time complexity, and its relationship with other existing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["14"]}
{"title": "Web image learning for searching semantic concepts in image databases\n", "abstract": " Without textual descriptions or label information of images, searching semantic concepts in image databases is still a very challenging task. While automatic annotation techniques are yet along way off, we can seek other alternative techniques to solve this difficult issue. In this paper, we propose to learn Web images for searching the semantic concepts in large image databases. To formulate effective algorithms, we suggest to engage the support vector machines for attacking the problem. We evaluate our algorithm in a large image database and demonstrate the preliminary yet promising results.", "num_citations": "23\n", "authors": ["14"]}
{"title": "Renegotiable quality of service-a new scheme for fault tolerance in wireless networks\n", "abstract": " In this paper we propose the concept that faults in telecommunications networks often manifest themselves as reductions in service quality, which can be addressed by using the notion of Quality of Service (QoS). In wireless ATM networks, the ability to provide QoS guarantees for high priority traffic in the presence of noise or faults is of utmost importance. Moreover there is a need for renegotiating existing QoS on an established connection, since the characteristics of a wireless link may well change during the lifetime of a connection due to mobile hosts' movements or external interference. In this paper we describe a general QoS strategy as a fault tolerance mechanism, and address the problems associated with providing QoS over a wireless link. We present a QoS scheme with renegotiation capability, define an API (application programming interface) for the access to this scheme and describe our implementation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["14"]}
{"title": "Neural relational topic models for scientific article analysis\n", "abstract": " Topic modelling and citation recommendation of scientific articles are important yet challenging research problems in scientific article analysis. In particular, the inference on coherent topics can be easily affected by irrelevant contents in articles. Meanwhile, the extreme sparsity of citation networks brings difficulty to a valid citation recommendation. Intuitively, articles with similar topics are more likely to cite each other, and cited articles tend to share similar themes. Motivated from this intuition, we aim to boost the performance of both topic modelling and citation recommendation by effectively leverage this underlying correlation between latent topics and citation networks. To this end, we propose a novel Bayesian deep generative model termed as Neural Relational Topic Model (NRTM), which is composed with a Stacked Variational Auto-Encoder (SVAE) and a multilayer perception (MLP). Specifically, the SVAE\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["14"]}
{"title": "On secure and usable program obfuscation: A survey\n", "abstract": " Program obfuscation is a widely employed approach for software intellectual property protection. However, general obfuscation methods (e.g., lexical obfuscation, control obfuscation) implemented in mainstream obfuscation tools are heuristic and have little security guarantee. Recently in 2013, Garg et al. have achieved a breakthrough in secure program obfuscation with a graded encoding mechanism and they have shown that it can fulfill a compelling security property, i.e., indistinguishability. Nevertheless, the mechanism incurs too much overhead for practical usage. Besides, it focuses on obfuscating computation models (e.g., circuits) rather than real codes. In this paper, we aim to explore secure and usable obfuscation approaches from the literature. Our main finding is that currently we still have no such approaches made secure and usable. The main reason is we do not have adequate evaluation metrics concerning both security and performance. On one hand, existing code-oriented obfuscation approaches generally evaluate the increased obscurity rather than security guarantee. On the other hand, the performance requirement for model-oriented obfuscation approaches is too weak to develop practical program obfuscation solutions.", "num_citations": "22\n", "authors": ["14"]}
{"title": "Boosting response aware model-based collaborative filtering\n", "abstract": " Recommender systems are promising for providing personalized favorite services. Collaborative filtering (CF) technologies, making prediction of users' preference based on users' previous behaviors, have become one of the most successful techniques to build modern recommender systems. Several challenging issues occur in previously proposed CF methods: (1) most CF methods ignore users' response patterns and may yield biased parameter estimation and suboptimal performance; (2) some CF methods adopt heuristic weight settings, which lacks a systematical implementation; and (3) the multinomial mixture models may weaken the computational ability of matrix factorization for generating the data matrix, thus increasing the computational cost of training. To resolve these issues, we incorporate users' response models into the probabilistic matrix factorization (PMF), a popular matrix factorization CF model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["14"]}
{"title": "DR2: Dynamic request routing for tolerating latency variability in online cloud applications\n", "abstract": " Application latency is one significant user metric for evaluating the performance of online cloud applications. However, as applications are migrated to the cloud and deployed across a wide-area network, the application latency usually presents high variability over time. Among lots of subtleties that influence the latency, one important factor is relying on the Internet for application connectivity, which introduces a high degree of variability and uncertainty on user-perceived application latency. As a result, a key challenge faced by application designers is how to build consistently low-latency cloud applications with the large number of geo-distributed and latency-varying cloud components. In this paper, we propose a dynamic request routing framework, DR2, by taking full advantage of redundant components in the clouds to tolerate latency variability. In practice, many functionally-equivalent components have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["14"]}
{"title": "Direct zero-norm optimization for feature selection\n", "abstract": " Zero-norm, defined as the number of non-zero elements in a vector, is an ideal quantity for feature selection. However, minimization of zero-norm is generally regarded as a combinatorially difficult optimization problem. In contrast to previous methods that usually optimize a surrogate of zero-norm, we propose a direct optimization method to achieve zero-norm for feature selection in this paper. Based on Expectation Maximization (EM), this method boils down to solving a sequence of Quadratic Programming problems and hence can be practically optimized in polynomial time. We show that the proposed optimization technique has a nice Bayesian interpretation and converges to the true zero norm asymptotically, provided that a good starting point is given. Following the scheme of our proposed zero-norm, we even show that an arbitrary-norm based Support Vector Machine can be achieved in polynomial time. A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["14"]}
{"title": "An experiment in determining software reliability model applicability\n", "abstract": " Most reported experience with software reliability models is from a project's testing phases, during which researchers have little control over the failure data. Since failure data can be noisy and distorted, reported procedures for determining model applicability may be incomplete. To gain additional insight into this problem, we generated forty sets of data by drawing samples from two distributions, which were used as inputs to six different software reliability models. We used several different methods to analyze the applicability of the models. We expected that a model would perform best on the data sets created to comply with the model's assumptions, but initially found that this was not always the case. More detailed examination showed that a model using a data set created to satisfy its assumptions tended to have better prequential likelihood bias, and bias trend measures, although the Kolmogorov-Smirnov test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["14"]}
{"title": "Logzip: extracting hidden structures via iterative clustering for log compression\n", "abstract": " System logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering. As modern software systems are evolving into large scale and complex structures, logs have become one type of fast-growing big data in industry. In particular, such logs often need to be stored for a long time in practice (e.g., a year), in order to analyze recurrent problems or track security issues. However, archiving logs consumes a large amount of storage space and computing resources, which in turn incurs high operational cost. Data compression is essential to reduce the cost of log storage. Traditional compression tools (e.g., gzip) work well for general texts, but are not tailed for system logs. In this paper, we propose a novel and effective log compression method, namely logzip. Logzip is capable of extracting hidden structures from raw logs via fast iterative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "Traffic prediction based power saving in cellular networks: A machine learning method\n", "abstract": " In smart cities, green cellular networks play a crucial role to support wireless access for numerous devices anywhere and anytime with efficiency and sustainability. Because base stations (BSes) consume more than 70% of overall cellular network infrastructure energy, saving the power consumption of BSes is the key task to build a green cellular network. Except for low power design of the BS hardware and software, the traffic-driven BS sleeping operation is an economical way to improve existing cellular networks, which can reduce the BS power consumption at low traffic load. However, prior BS sleeping strategies establish on the static temporal characteristics of traffic load, which ignore the fact that network traffic is influenced by many factors such as time, human mobility, holiday, weather, etc. Hence, prior traffic estimation is coarse, and the BS sleeping strategies cannot apply to the changing network traffic. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "Asymmetric correlation regularized matrix factorization for web service recommendation\n", "abstract": " Web service recommendation has recently drawn much attention with the growing amount of Web services. Previous work usually exploits the collaborative filtering techniques for Web service recommendation, but suffers from the data sparsity problem that leads to inaccurate results. Our analysis on a real-world Quality of Service (QoS) dataset shows that there is a hidden correlation among users and services. We define such hidden correlation with an asymmetric matrix (namely asymmetric correlation), in which each entry presents the hidden correlation between a user pair or between a service pair. The goal of this work is to employ such asymmetric correlation among users and services to alleviate the data sparsity problem and further enhance the prediction accuracy in service recommendation. Specifically, we propose an asymmetric correlation regularized matrix factorization (MF) framework, in which\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "An adaptive delay-minimized route design for wireless sensor\u0393\u00c7\u00f4actuator networks\n", "abstract": " Wireless sensor-actuator networks (WSANs) have recently been suggested as an extension to conventional sensor networks. The powerful and mobile actuators can patrol along different routes and communicate with the static sensor nodes. Obviously, it is crucial to optimize the routes for the actuators to collect the sensor data in a timely fashion. Given the nonuniform and time-varying distribution of sensors and events in large networks, the route design has to be dynamic and scalable as well as balance the loads of the actuators. In this paper, we propose probabilistic route design (PROUD), which is an effective and adaptive algorithm for weight-differentiated route calculation. PROUD constructs an  a priori  route that covers the sensor locations, following which, the actuators probabilistically and cyclically visit the sensor locations according to their weights. We show that this probabilistic approach adapts well to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "Arbitrary norm support vector machines\n", "abstract": " Support vector machines (SVM) are state-of-the-art classifiers. Typically L2-norm or L1-norm is adopted as a regularization term in SVMs, while other norm-based SVMs, for example, the L0-norm SVM or even the L\u0393\u00ea\u20a7-norm SVM, are rarely seen in the literature. The major reason is that L0-norm describes a discontinuous and nonconvex term, leading to a combinatorially NP-hard optimization problem. In this letter, motivated by Bayesian learning, we propose a novel framework that can implement arbitrary norm-based SVMs in polynomial time. One significant feature of this framework is that only a sequence of sequential minimal optimization problems needs to be solved, thus making it practical in many real applications. The proposed framework is important in the sense that Bayesian priors can be efficiently plugged into most learning methods without knowing the explicit form. Hence, this builds a connection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "Semi-supervised learning from general unlabeled data\n", "abstract": " We consider the problem of semi-supervised learning (SSL) from general unlabeled data, which may contain irrelevant samples. Within the binary setting, our model manages to better utilize the information from unlabeled data by formulating them as a three-class (-1,+1, 0) mixture, where class 0 represents the irrelevant data. This distinguishes our work from the traditional SSL problem where unlabeled data are assumed to contain relevant samples only, either +1 or -1, which are forced to be the same as the given labeled samples. This work is also different from another family of popular models, universum learning (universum means \"irrelevant\" data), in that the universum need not to be specified beforehand. One significant contribution of our proposed framework is that such irrelevant samples can be automatically detected from the available unlabeled data, even though they are mixed with relevant data. This\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["14"]}
{"title": "A novel image retrieval system based on BP neural network\n", "abstract": " This paper presents a novel BP-based image retrieval (BPBIR) system, which is based on the observation that the images users need are often similar to a set of images with the same conception instead of one query image and the assumption that there is a nonlinear relationship between different features. If users aren't satisfied with the retrieved results, relevance feedback method is used to enhance the performance of the proposed system by changing the weights of the BP neural networks. In addition, we discuss some divisional methods to give rough information on the spatial color composition. Finally, we compare the performance of the proposed system with other systems. Experimental results show the efficacy of the proposed system.", "num_citations": "21\n", "authors": ["14"]}
{"title": "DiagDroid: Android performance diagnosis via anatomizing asynchronous executions\n", "abstract": " Rapid UI responsiveness is a key consideration to Android app developers. However, the complicated concurrency model of Android makes it hard for developers to understand and further diagnose the UI performance. This paper presents DiagDroid, a tool specifically designed for Android UI performance diagnosis. The key notion of DiagDroid is that UI-triggered asynchronous executions contribute to the UI performance, and hence their performance and their runtime dependency should be properly captured to facilitate performance diagnosis. However, there are tremendous ways to start asynchronous executions, posing a great challenge to profiling such executions and their runtime dependency. To this end, we properly abstract five categories of asynchronous executions as the building basis. As a result, they can be tracked and profiled based on the specifics of each category with a dynamic instrumentation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Software Reliability\n", "abstract": " The demand for complex software systems has increased more rapidly than the ability to design, implement, test, and maintain them, and the reliability of software systems has become a major concern for our modern society. With the last decade of the 20th century, computer software has already become the major source of reported outages in many systems. Consequently, recent literature is replete with horror stories of projects gone awry, generally as a result of problems traced to software.More recently software failures have impaired several high-visibility programs in the health industry, in some cases they even killed people. Described in the book by Lee (1992), the massive Therac-25 radiation therapy machine was hit by software errors in its sophisticated control systems and claimed several patients' lives in 1985 and 1986. South West Thames Regional Health Authority (1993) reported the incidence on October 26, 1992, when the Computer Aided Dispatch system of the London Ambulance Service broke down right after its installation, paralyzing the capability of the world's largest ambulance service to handle 5000 daily requests in carrying patients in emergency situations. In the aviation industry, although the real causes for several airliner crashes in the past few years remained mysteries, experts pointed out that software control could be the chief suspect in some of these incidents due to its inappropriate response to the pilots' desperate inquires during an abnormal flight condition.", "num_citations": "20\n", "authors": ["14"]}
{"title": "P-Tracer: Path-based performance profiling in cloud computing systems\n", "abstract": " In large-scale cloud computing systems, the growing scale and complexity of component interactions pose great challenges for operators to understand the characteristics of system performance. Performance profiling has long been proved to be an effective approach to performance analysis; however, existing approaches do not consider two new requirements that emerge in cloud computing systems. First, the efficiency of the profiling becomes of critical concern; second, visual analytics should be utilized to make profiling results more readable. To address the above two issues, in this paper, we present P-Tracer, an online performance profiling approach specifically tailored for large-scale cloud computing systems. P-Tracer constructs a specific search engine that adopts a proactive way to process performance logs and generates particular indices for fast queries; furthermore, PTracer provides users with a suite of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Automatic string test data generation for detecting domain errors\n", "abstract": " Domain testing is designed to detect domain errors that result from a small boundary shift in a path domain. Although many researchers have studied domain testing, automatic domain test data generation for string predicates has seldom been explored. This paper presents a novel approach for the automatic generation of ON\u0393\u00c7\u00f4OFF test points for string predicate borders, and describes a corresponding test data generator. Our empirical work is conducted on a set of programs with string predicates, where extensive trials have been done for each string predicate, and the results are analysed using the SPSS tool. Conclusions are drawn that: (i) the approach is promising and effective; (ii) there is a strong linear relationship between the performance of the test generator and the length of target string in the predicate tested; and (iii) initial inputs, no shorter than the target string and with characters generated randomly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Extraction and segmentation of tables from Chinese ink documents based on a matrix model\n", "abstract": " This paper presents an approach for extracting and segmenting tables from Chinese ink documents based on a matrix model. An ink document is first modeled as a matrix containing ink rows, including writing and drawing ones. Each row consists of collinear ink lines containing ink characters. Together with their associated drawing rows, adjacent writing rows having an identical distribution of writing lines and\u0393\u00ba\u2563 or the same associated drawing rows if available are extracted to form a table. Row and column headers, nested sub-headers and cells are identified. Experiments demonstrate that the proposed approach is more effective and robust.", "num_citations": "20\n", "authors": ["14"]}
{"title": "Extraction of line segments and circular arcs from freehand strokes based on segmental homogeneity features\n", "abstract": " The extraction of component line segments and circular arcs from freehand strokes along with their relations is a prerequisite for sketch understanding. Existing approaches usually take three stages to segment a stroke: first identifying segmentation points, then classifying the substroke between each pair of adjacent segmentation points, and, finally, obtaining graphical representations of substrokes by fitting graphical primitives to them. Since a stroke inevitably contains noises, the first stage may produce wrong or inaccurate segmentation points, resulting in the wrong substroke classification in the second stage and inaccurately fitted parameters in the third stage. To overcome the noise sensitivity of the three-stage method, the segmental homogeneity feature is emphasized in this paper. We propose a novel approach, which first extracts graphical primitives from a stroke by a connected segment growing from a seed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Methods of decreasing the number of support vectors via k-mean clustering\n", "abstract": " This paper proposes two methods which take advantage of k-mean clustering algorithm to decrease the number of support vectors (SVs) for the training of support vector machine (SVM). The first method uses k-mean clustering to construct a dataset of much smaller size than the original one as the actual input dataset to train SVM. The second method aims at reducing the number of SVs by which the decision function of the SVM classifier is spanned through k-mean clustering. Finally, Experimental results show that this improved algorithm has better performance than the standard Sequential Minimal Optimization (SMO) algorithm.", "num_citations": "20\n", "authors": ["14"]}
{"title": "Extraction of karyocytes and their components from microscopic bone marrow images based on regional color features\n", "abstract": " Extracting karyocytes and their components from microscopic bone marrow images is prerequisite for computer-aided early diagnosis of hemopathy. Most existing methods assume all pixels belonging to a karyon region or a cytoplasm region have similar colors. Practically, the color of neither a karyon nor a cytoplasm in a microscopic image is homogeneous in the pixel level. Therefore, the regional color features of a region are emphasized in this paper. Based on this observation, we propose a novel method to karyocyte extraction that first identifies a karyon by 4-connected block growing from a karyon feature block, then identifies feature blocks of its cytoplasm based on the extracted karyon, and finally extracts all cytoplasm regions by growing the cytoplasm feature blocks. Combining the karyon region and the corresponding cytoplasm regions can attain a complete karyocyte. Experimental results show that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Finite mixture model of bounded semi-naive Bayesian networks classifier\n", "abstract": " The Semi-Naive Bayesian network (SNB) classifier, a probabilistic model with an assumption of conditional independence among the combined attributes, shows a good performance in classification tasks. However, the traditional SNBs can only combine two attributes into a combined attribute. This inflexibility together with its strong independency assumption may generate inaccurate distributions for some datasets and thus may greatly restrict the classification performance of SNBs. In this paper we develop a Bounded Semi-Naive Bayesian network (B-SNB) model based on direct combinatorial optimization. Our model can join any number of attributes within a given bound and maintains a polynomial time cost at the same time. This improvement expands the expressive ability of the SNB and thus provide potentials to increase accuracy in classification tasks. Further, aiming at relax the strong independency\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Edge color distribution transform: an efficient tool for object detection in images\n", "abstract": " Object detection in images is a fundamental task in many image analysis applications. Existing methods for low-level object detection always perform the color-similarity analyses in the 2D image space. However, the crowded edges of different objects make the detection complex and error-prone. The paper proposes to detect objects in a new edge color distribution space (ECDS) rather than in the image space. In the 3D ECDS, the edges of different objects are segregated and the spatial relation of a same object is kept as well, which make the object detection easier and less error-prone. Since uniform-color objects and textured objects have different distribution characteristics in ECDS, the paper gives a 3D edge-tracking algorithm for the former and a cuboid-growing algorithm for the latter. The detection results are correct and noise-free, so they are suitable for the high-level object detection. The experimental\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Software reliability simulation\n", "abstract": " Previous chapters discuss the opportunities and bene ts of SRE throughout the entire life cycle, from requirements determination through design, implementation, testing, delivery, and operations. Properly applied, SRE is an important positive in uence on the ultimate quality of all life cycle products. The set of life cycle activities and artifacts, together with their attributes and interrelationships, that are related to reliability1 comprise what we here refer to as the reliability process. The artifacts of the software life cycle include documents, reports, manuals, plans, code, con guration data, test data, ancillary data, and all other tangible products.Software reliability is dynamic and stochastic. In a new or upgraded product, it begins at a low gure with respect to its new intended usage and ultimately reaches a gure near unity in maturity. The exact value of product reliability, however, is never precisely known at any point in its lifetime. The software reliability models described in Chapter 4 attempt to assess expected reliability or future operability using observed failure data and statistical inference techniques. Most of these only treat the exposure and handling of failures during testing or operations. They are restricted in their life cycle scope and adaptability to general use for a number of reasons, including their foundation on oversimpli ed assumptions and their primary focus on testing and operations phases. Modelers have traditionally imposed certain simplifying assumptions in order to obtain closed-form, idealized approximations of software reliability. Some modelers may have relaxed an assumption here or there in attempts to provide more generality, but\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["14"]}
{"title": "Difficulty controllable question generation for reading comprehension\n", "abstract": " We investigate the difficulty levels of questions in reading comprehension datasets such as SQuAD, and propose a new question generation setting, named Difficulty-controllable Question Generation (DQG). Taking as input a sentence in the reading comprehension paragraph and some of its text fragments (ie, answers) that we want to ask questions about, a DQG method needs to generate questions each of which has a given text fragment as its answer, and meanwhile the generation is under the control of specified difficulty labels---the output questions should satisfy the specified difficulty as much as possible. To solve this task, we propose an end-to-end framework to generate questions of designated difficulty levels by exploring a few important intuitions. For evaluation, we prepared the first dataset of reading comprehension questions with difficulty labels. The results show that the question generated by our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["14"]}
{"title": "ARF-predictor: effective prediction of aging-related failure using entropy\n", "abstract": " Even well-designed software systems suffer from chronic performance degradation, also known as \u0393\u00c7\u00a3software aging\u0393\u00c7\u00a5, due to internal (e.g., software bugs) or external (e.g., resource exhaustion) impairments. These chronic problems often fly under the radar of software monitoring systems before causing severe impacts (e.g., system failures). Therefore, it is a challenging issue how to timely predict the occurrence of failures caused by these problems. Unfortunately, the effectiveness of prior approaches are far from satisfactory due to the insufficiency of aging indicators adopted by them. To accurately predict failures caused by software aging which are named as Aging-Related Failure (ARFs), this paper presents a novel entropy-based aging indicator, namely Multidimensional Multi-scale Entropy (MMSE) which leverages the complexity embedded in runtime performance metrics to indicate software aging. To the best of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["14"]}
{"title": "Modeling the Homophily Effect between Links and Communities for Overlapping Community Detection.\n", "abstract": " Overlapping community detection has drawn much attention recently since it allows nodes in a network to have multiple community memberships. A standard framework to deal with overlapping community detection is Matrix Factorization (MF). Although all existing MF-based approaches use links as input to identify communities, the relationship between links and communities is still under-investigated. Most of the approaches only view links as consequences of communities (community-to-link) but fail to explore how nodes\u0393\u00c7\u00d6 community memberships can be represented by their linked neighbors (link-to-community). In this paper, we propose a Homophily-based Nonnegative Matrix Factorization (HNMF) to model both-sided relationships between links and communities. From the community-to-link perspective, we apply a preference-based pairwise function by assuming that nodes with common communities have a higher probability to build links than those without common communities. From the link-to-community perspective, we propose a new community representation learning with network embedding by assuming that linked nodes have similar community representations. We conduct experiments on several real-world networks and the results show that our HNMF model is able to find communities with better quality compared with state-of-the-art baselines.", "num_citations": "19\n", "authors": ["14"]}
{"title": "Learning to suggest questions in social media\n", "abstract": " Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Two representative types are online forums and community-based Q&A services. To enable users to explore the large number of questions and answers in social media systems effectively, it is essential to suggest interesting items to an active user. In this article, we address the problem of question suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework, and propose the topic-enhanced translation-based language model (TopicTRLM), which fuses both the lexical and latent semantic knowledge. This fusing enables TopicTRLM to find semantically related questions to a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["14"]}
{"title": "Character string predicate based automatic software test data generation\n", "abstract": " A character string is an important element in programming. A problem that needs further research is how to automatically generate software test data for character strings. This paper presents a novel approach for automatic test data generation of program paths including character string predicates, and the effectiveness of this approach is examined on a number of programs. Each element of input variable of a character string is determined by using the gradient descent technique to perform function minimization so that the test data of character string can be dynamically generated. The experimental results illustrate that this approach is effective.", "num_citations": "19\n", "authors": ["14"]}
{"title": "Reliability-oriented software engineering: Design, testing and evaluation techniques\n", "abstract": " Software reliability engineering involves techniques for the design, testing and evaluation of software systems, focusing on reliability attributes. Design for reliability is achieved by fault-tolerance techniques that keep the system working in the presence of software faults. Testing for reliability is achieved by fault-removal techniques that detect and correct software faults before the system is deployed. Evaluation for reliability is achieved by fault-prediction techniques that model and measure the reliability of the system during its operation. This paper presents the best current practices in software reliability engineering for design, testing and evaluation purposes. There are descriptions of how fault-tolerant components are designed and applied to software systems, how software testing schemes are performed to show improvement of software reliability, and how reliability quantities are obtained for software systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["14"]}
{"title": "System-level reliability and sensitivity analyses for three fault-tolerant system architectures\n", "abstract": " This paper discusses the modeling and analysis of three major fault-tolerant software system architectures: DRB (Distributed Recovery Blocks), NVP (N-Version Programming) and NSCP (N Self-Checking Programming). In the system-level reliability modeling domain, fault tree analysis techniques and Markov modeling techniques are combined to incorporate transient and permanent hardware faults as well as unrelated and related software faults. These models are parameterized by a real-world fault-tolerant flight control computer application for evaluations and comparisons. In particular, a series of sensitivity analysis is performed to explore the critical components in each fault-tolerant architecture and display their quantitative impacts to the overall system reliability.", "num_citations": "19\n", "authors": ["14"]}
{"title": "DEDIX 87\u0393\u00c7\u00f6A supervisory system for design diversity experiments at UCLA\n", "abstract": " To establish a long-term research facility for further experimental investigations of design diversity as a means of achieving fault-tolerant systems, the DEDIX (DEsign Diversity experiment) system, a distributed supervisor and testbed for multi-version software, was designed and implemented by researchers at the UCLA Dependable Computing and Fault-Tolerant Systems Laboratory. DEDIX is available on the Olympus local network, which utilizes the Locus distributed operating system to operate a set of several VAX 11/750 computers at the UCLA Center for Experimental Computer Science. DEDIX is portable to any machine which runs a Unix operating system. The DEDIX system is described and its applications are discussed in this paper. A review of current research is also presented.", "num_citations": "19\n", "authors": ["14"]}
{"title": "Deep validation: Toward detecting real-world corner cases for deep neural networks\n", "abstract": " The exceptional performance of Deep neural networks (DNNs) encourages their deployment in safety-and dependability-critical systems. However, DNNs often demonstrate erroneous behaviors in real-world corner cases. Existing countermeasures center on improving the testing and bug-fixing practice. Unfortunately, building a bug-free DNN-based system is almost impossible currently due to its black-box nature, so anomaly detection is imperative in practice. Motivated by the idea of data validation in a traditional program, we propose and implement Deep Validation, a novel framework for detecting real-world error-inducing corner cases in a DNN-based system during runtime. We model the specifications of DNNs by resorting to their training data and cast checking input validity of DNNs as the problem of discrepancy estimation. Deep Validation achieves excellent detection results against various corner case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["14"]}
{"title": "Emerging app issue identification from user feedback: Experience on wechat\n", "abstract": " It is vital for popular mobile apps with large numbers of users to release updates with rich features while keeping stable user experience. Timely and accurately locating emerging app issues can greatly help developers to maintain and update apps. User feedback (i.e., user reviews) is a crucial channel between app developers and users, delivering a stream of information about bugs and features that concern users. Methods to identify emerging issues based on user feedback have been proposed in the literature, however, their applicability in industry has not been explored. We apply the recent method IDEA to WeChat, a popular messenger app with over 1 billion monthly active users, and find that the emerging issues detected by IDEA are not stable (i.e., due to its inherent randomness, its results change when run multiple times even for the same inputs), and there are other problems such as long running time. To\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["14"]}
{"title": "Pure exploration of multi-armed bandits with heavy-tailed payoffs\n", "abstract": " Inspired by heavy-tailed distributions in practical scenarios, we investigate the problem on pure exploration of Multi-Armed Bandits (MAB) with heavy-tailed payoffs by breaking the assumption of payoffs with sub-Gaussian noises in MAB, and assuming that stochastic payoffs from bandits are with finite p-th moments, where p\u0393\u00ea\u00ea(1,+\u0393\u00ea\u20a7). The main contributions in this paper are three-fold. First, we technically analyze tail probabilities of empirical average and truncated empirical average (TEA) for estimating expected payoffs in sequential decisions with heavy-tailed noises via martingales. Second, we propose two effective bandit algorithms based on different prior information (ie, fixed confidence or fixed budget) for pure exploration of MAB generating payoffs with finite p-th moments. Third, we derive theoretical guarantees for the proposed two bandit algorithms, and demonstrate the effectiveness of two algorithms in pure exploration of MAB with heavy-tailed payoffs in synthetic data and real-world financial data.", "num_citations": "18\n", "authors": ["14"]}
{"title": "Carp: context-aware reliability prediction of black-box web services\n", "abstract": " Reliability prediction is an important task in software reliability engineering, which has been widely studied in the last decades. However, modelling and predicting user-perceived reliability of black-box services remain an open research problem. Software services, such as Web services and Web APIs, generally provide black-box functionalities to users through the Internet, thus leading to a lack of their internal information for reliability analysis. Furthermore, the user-perceived service reliability depends not only on the service itself, but also heavily on the invocation context (e.g., service workloads, network conditions), whereby traditional reliability models become ineffective and inappropriate. To address these new challenges posed by blackbox services, in this paper, we propose CARP, a new contextaware reliability prediction approach, which leverages historical usage data from users to construct context-aware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["14"]}
{"title": "Complete nucleotide sequence of a Chinese isolate of Grapevine leafroll-associated virus 3 reveals a 5\u0393\u00c7\u2593 UTR of 802 nucleotides\n", "abstract": " Grapevine leafroll-associated virus 3 (GLRaV-3) is widely spread in China. Here we report, for the first time, the complete nucleotide sequence of the Chinese isolate (LN) of GLRaV-3. The 18,563-nt genomic RNA is the largest of the GLRaV-3 genomes reported to date, with a 5\u0393\u00c7\u2593 untranslated region of 802 nt. Its sequence shares 87.99\u0393\u00c7\u00f498.15\u252c\u00e1% identity with those of previously reported isolates, and phylogenetic analysis suggested placing isolate LN in group 3, together with another fully sequenced isolate, PL-20.", "num_citations": "18\n", "authors": ["14"]}
{"title": "Experience report: Detecting poor-responsive ui in android applications\n", "abstract": " Good user interface (UI) design is key to successful mobile apps. UI latency, which can be considered as the time between the commencement of a UI operation and its intended UI update, is a critical consideration for app developers. Current literature still lacks a comprehensive study on how much UI latency a user can tolerate or how to identify UI design defects that cause intolerably long UI latency. As a result, bad UI apps are still common in app markets, leading to extensive user complaints. This paper examines user expectations of UI latency, anddevelops a tool to pinpoint intolerable UI latency in Android apps. To this end, we design an app to conduct a user survey of app UI latency. Through the survey, we find the tendency between user patience and UI latency. Therefore a timely screen update (e.g., loading animations) is critical to heavy-weighted UI operations (i.e. those that incur a long execution time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["14"]}
{"title": "Copyright protection on the web: A hybrid digital video watermarking scheme\n", "abstract": " Video is one of the most popular data shared in the Web, and the protection of video copyright is of vast interest. In this paper, we present a comprehensive approach for protecting and managing video copyrights in the Internet with watermarking techniques. We propose a novel hybrid digital video watermarking scheme with scrambled watermarks and error correction codes. The effectiveness of this scheme is verified through a series of experiments, and the robustness of our approach is demonstrated using the criteria of the latest StirMark test.", "num_citations": "17\n", "authors": ["14"]}
{"title": "The PowerRank Web link analysis algorithm\n", "abstract": " The web graph follows the power law distribution and has a hierarchy structure. But neither the PageRank algorithm nor any of its improvements leverage these attributes. In this paper, we propose a novel link analysis algorithm\" the PowerRank algorithm\", which makes use of the power law distribution attribute and the hierarchy structure of the web graph. The algorithm consists two parts. In the first part, special treatment is applied to the web pages with low\" importance\" score. In the second part, the global\" importance\" score for each web page is obtained by combining those scores together. Our experimental results show that: 1) The PowerRank algorithm computes 10%-30% faster than PageRank algorithm. 2) Top web pages in PowerRank algorithm remain similar to that of the PageRank algorithm.", "num_citations": "17\n", "authors": ["14"]}
{"title": "Graphics recognition from binary images: one step or two steps\n", "abstract": " Recognizing graphic objects from binary images is an important task in many real-life applications. Generally, there are two ways to do the graphics recognition: one-step methods and two-step methods. The former recognizes graphic objects from binary images directly, while the latter consists of vectorization and postprocessing. Neither of them is perfect enough to handle all difficulties. The paper first reviews popular graphics recognition methods to understand their advantages and disadvantages. Next, the performance comparison between two classes of methods is made in two important aspects: the time efficiency and the graphics quality, and the experimental results of time-efficiency comparison of 7 popular methods are also reported. Finally, we propose a new hybrid graphics-recognition paradigm to integrate the advantages of both one-step methods and two-step methods and minimize their\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["14"]}
{"title": "Diversity in the software development process\n", "abstract": " Various methods have been proposed for building fault-tolerant software in an effort to provide substantial improvements in software reliability for critical applications, such as flight control, air-traffic control, patient monitoring or power plant monitoring. The two best-known methods of building fault-tolerant software are n-version programming and recovery blocks. To tolerate faults, both of these techniques rely on design diversity, i.e. the availability of multiple implementations of a specification. Software engineers assume that the different implementations use different designs and, thereby, it is hoped, contain different faults. Our study uses a novel method of incorporating diversity in the development of one version of the software. We term this approach the pipeline method of software development. Its purpose is to eliminate as many software faults as possible before the testing phase. The method was applied to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["14"]}
{"title": "A design paradigm for multi-version software\n", "abstract": " In the multi-version software (MVS) implementation of fault-tolerant software, design faults are detected and masked through the consensus of results from two or more independently designed and functionally equivalent versions. It was the major concern and effort of the author to formulate a set of rigorous guidelines, or a design paradigm, for the application and implementation of design diversity in building MVS systems for practical applications. This design paradigm was developed and formulated based on the results of fault-tolerant software research conducted at UCLA since 1975. Using this design paradigm, the author took a major part in the design, coordination and evaluation of a UCLA/Honeywell joint research project on a large-scale MVS system, employing six different programming languages to create six versions of software for pitch control in an automatic aircraft landing program. The rationale, preparation, execution, and evaluation of this project are reported in detail. Moreover, the assessment and refinement of the proposed design paradigm are also presented as results from this project.", "num_citations": "17\n", "authors": ["14"]}
{"title": "Localizing root causes of performance anomalies in cloud computing systems by analyzing request trace logs\n", "abstract": " It is hard to localize the primary cause of performance anomalies in cloud computing systems because of the complexity of interactions between components. The hidden connections in the huge number of request execution paths in such systems usually contain useful information for diagnosing performance anomalies. We propose an approach to localize anomalous invoked methods and their physical locations by leveraging request trace logs, which involves two steps: (1) firstly, cluster the requests according to their corresponding call sequences, identify anomalous requests with principal component analysis, and then pick out anomalous methods with Mann-Whitney hypothesis test; (2) secondly, compare the behavior similarities of all replicated instances of the anomalous methods with Jensen-Shannon divergence, and select the ones whose behaviors are different from those of others, which will be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["14"]}
{"title": "Communities of Yahoo! answers and Baidu Zhidao: Complementing or competing?\n", "abstract": " Community Question Answering (CQA) attracts increasing volume of research on question retrieval, high quality content discovery and experts finding. However, few studies are focused on community per se of CQA services and also provide an in-depth analysis of them. This paper aims to enrich our knowledge on two of these CQA services, namely Yahoo! Answers and Baidu Zhidao through reviewing their communities, comparing similarities and differences of the two communities, together with analyzing their influence on solving questions. Six data sets are employed for comparative analysis. In this paper: (1) We analyze the social network structures of Yahoo! Answers and Baidu Zhidao; (2) We compare the the social community characteristics of top contributors; (3) We reveal the behaviors of users in different categories in these two portals; (4) We reveal temporal trends of these characteristics; (5) We find that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["14"]}
{"title": "Location-based topic evolution\n", "abstract": " As the advance of mobile technologies, geographical records can be easily embedded in the data to form the location-associated documents. For example, in Twitter, the location of tweets can be identified by the GPS locations or IP addresses from smart phones. In Flickr, photos may be tagged and recorded with GPS locations. With the geographical information, it is more likely to model users' interests in different regions so as to determine the corresponding marketing strategy. Due to its potential in providing personalized and context-aware services, several pieces of work have started to explore in this area. One stream of work tries to discover users' interest topics from location-associated documents. These models work under the assumption that words close in geographical positions are likely to be clustered into the same geographical topic. However, they attain this in a static mode. That is, they do not consider\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["14"]}
{"title": "A simple proof for persistence of snap-back repellers\n", "abstract": " In this article, we show that if f has a snap-back repeller then any small C 1 perturbation of f has a snap-back repeller, and hence has Li\u0393\u00c7\u00f4Yorke chaos and positive topological entropy, by simply using the implicit function theorem. We also give some examples.", "num_citations": "16\n", "authors": ["14"]}
{"title": "Delay-minimized route design for wireless sensor-actuator networks\n", "abstract": " Wireless sensor-actuator networks (WSANs) have recently been suggested as an enhancement to the traditional sensor networks by employing powerful and mobile actuators. Multiple actuators can patrol along different routes and communicate with the static sensors. To minimize the data collection time, an effective route design is crucial for the actuators to travel in the sensing field. In this paper, we present a mathematical formulation of the route design problem, and show that the general problem is computationally intractable. We then develop a practically efficient algorithm to reduce the waiting time for the sensors. Our algorithm adaptively differentiates the actuator visiting frequencies to the sensors according to their relative weights and data generation patterns. Simulation results demonstrate that our algorithm can effectively reduce the overall data collection time.", "num_citations": "16\n", "authors": ["14"]}
{"title": "A new feature of uniformity of image texture directions coinciding with the human eyes perception\n", "abstract": " In this paper we present a new feature of texture images which can scale the uniformity degree of image texture directions. The feature value is obtained by examining the statistic characteristic of the gradient information of the image pixels. Simulation results illustrate that this feature can exactly coincide with the uniformity degree of image texture directions according to the perception of human eyes.", "num_citations": "16\n", "authors": ["14"]}
{"title": "Security modeling and evaluation for the mobile code paradigm\n", "abstract": " There is no well-know model for mobile agent security. One of the few attempts so far is given by [1]. The model is, however, a qualitative model that does not have direct numerical measures. It would be great if there is a quantitative model that can give user an intuitive sense of \u0393\u00c7\u00a3how secure an agent is\u0393\u00c7\u00a5.", "num_citations": "16\n", "authors": ["14"]}
{"title": "Automating app review response generation\n", "abstract": " Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews. Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Manufacturing resilient bi-opaque predicates against symbolic execution\n", "abstract": " Control-flow obfuscation increases program complexity by semantic-preserving transformation. Opaque predicates are essential gadgets to achieve such transformation. However, we observe that real-world opaque predicates are generally very simple and engage little security consideration. Recently, such insecure opaque predicates have been severely attacked by symbolic execution-based adversaries and jeopardize the security of control-flow obfuscation. This paper, therefore, proposes symbolic opaque predicates which can be resilient to symbolic execution-based adversaries. We design a general framework to compose such opaque predicates, which requires introducing challenging symbolic analysis problems (e.g., symbolic memory) in each opaque predicate. In this way, we may mislead symbolic execution engines into reaching false conclusions. We observe a novel bi-opaque property about\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "SpyAware: Investigating the privacy leakage signatures in app execution traces\n", "abstract": " A new security problem on smartphones is the wide spread of spyware nested in apps, which occasionally and silently collects user's private data in the background. The state-of-the-art work for privacy leakage detection is dynamic taint analysis, which, however, suffers usability issues because it requires flashing a customized system image to track the taint propagation and consequently incurs great overhead. Through a real-world privacy leakage case study, we observe that the spyware behaviors share some common features during execution, which may further indicate a correlation between the data flow of privacy leakage and some specific features of program execution traces. In this work, we examine such a hypothesis using the newly proposed SpyAware framework, together with a customized TaintDroid as the ground truth. SpyAware includes a profiler to automatically profile app executions in binder calls\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Service fault tolerance for highly reliable service-oriented systems: an overview\n", "abstract": " Service-oriented systems are widely-employed in e-business, e-government, finance, management systems, and so on. Service fault tolerance is one of the most important techniques for building highly reliable service-oriented systems. In this paper, we provide an overview of various service fault tolerance techniques, including sections on fault tolerance strategy design, fault tolerance strategy selection, and Byzantine fault tolerance. In the first section, we introduce the design of static and dynamic fault tolerance strategies, as well as the major problems when designing fault tolerance strategies. After that, based on various fault tolerance strategies, in the second section, we identify significant components from a complex service-oriented system, and investigate algorithms for optimal fault tolerance strategy selection. Finally, in the third section, we discuss a special type of service fault tolerance techniques, i\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Congestion performance improvement in wireless sensor networks\n", "abstract": " Wireless sensor networks (WSNs) have expanded their monitoring and tracking applications into wide areas, such as civil, military, and aerospace fields. Despite their important role, their performance under harsh conditions still remains to be improved. Specifically, when a WSN suffers congestion, the base station (BS) can hardly receive any data from the faraway sensor nodes while it still gets a moderate amount of data from the near-by nodes. Since the goal of WSN applications is to monitor the whole designated area, such unfairness is not acceptable. In addition, the average latency during congestion is intolerably long, failing the data freshness requirement of WSN applications. Although the fairness and latency performance of congested WSNs is very crucial for WSN applications, their degradation during congestion is usually ignored by most current congestion control methods which focus more on avoiding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Topological entropy for multidimensional perturbations of snap-back repellers and one-dimensional maps\n", "abstract": " We consider a one-parameter family of maps F \u256c\u2557 on\\mathbb {R}^{m}\\times\\mathbb {R}^{n} with the singular map F 0 having one of the two forms (i) F 0 (x, y)=(f (x), g (x)), where f:\\mathbb {R}^{m}\\rightarrow\\mathbb {R}^{m} and g:\\mathbb {R}^{m}\\rightarrow\\mathbb {R}^{n} are continuous, and (ii) F 0 (x, y)=(f (x), g (x, y)), where f:\\mathbb {R}^{m}\\rightarrow\\mathbb {R}^{m} and g:\\mathbb {R}^{m}\\times\\mathbb {R}^{n}\\rightarrow\\mathbb {R}^{n} are continuous and g is locally trapping along the second variable y. We show that if f is one-dimensional and has a positive topological entropy, or if f is high-dimensional and has a snap-back repeller, then F \u256c\u2557 has a positive topological entropy for all \u256c\u2557 close enough to 0.", "num_citations": "15\n", "authors": ["14"]}
{"title": "Local support vector regression for financial time series prediction\n", "abstract": " We consider the regression problem for financial time series. Typically, financial time series are non-stationary and volatile in nature. Because of its good generalization power and the tractability of the problem, the Support Vector Regression (SVR) has been extensively applied in financial time series prediction. The standard SVR adopts the l p -norm (p = 1 or 2) to model the functional complexity of the whole data set and employs a fixed e-tube to tolerate noise. Although this approach has proved successful both theoretically and empirically, it considers data in a global fashion only. Therefore it may lack the flexibility to capture the local trend of data; this is a critical aspect of volatile data, especially financial time series data. Aiming to address this issue, we propose the Local Support Vector Regression (LSVR) model. This novel model is demonstrated to provide a systematic and automatic scheme to adapt the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "On setting up energy-efficient paths with transmitter power control in wireless sensor networks\n", "abstract": " Energy-efficiency is an important design consideration of communication schemes for wireless sensor networks (WSNs). In this paper, we investigate the problem of energy-minimized sensor-to-sink communications with adaptive transmitter power settings. We devise a novel network-and application-aware model for this problem, and present a broadcast-on-update (BOU) solution. However, BOU suffers from the high overhead due to explosive broadcasting in path setup. We then show a waiting scheme, BOU-WA, that effectively mitigates the broadcast explosion. In BOU-WA, the waiting time before each broadcast is proportional to the probability that a node could find a more energy-efficient path to the sink. We provide an efficient approximation algorithm to calculate this probability. The performance of BOU-WA is evaluated under diverse network configurations, and the results demonstrate its superiority in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Message logging and recovery in wireless CORBA using access bridge\n", "abstract": " The emerging mobile wireless environment poses exciting challenges for distributed fault tolerant (FT) computing. This paper proposes a message logging and recovery protocol on the top of Telecom Wireless CORBA and FT-CORBA architectures. It uses the storage available at the access bridge as the stable storage to log messages and checkpoints on behalf of a mobile host. Our approach engages both quasi-sender-based and receiver-based logging methods and makes seamless handoff in the presence of failures. The details of how to tolerate mobile host disconnection, mobile host crash and access bridge crash are described. The normalized execution time of a mobile host engaging our proposed scheme and the handoff effect are evaluated.", "num_citations": "15\n", "authors": ["14"]}
{"title": "A VC-based API for renegotiable QoS in wireless ATM networks\n", "abstract": " Quality of service (QoS) support for multimedia applications has been widely discussed in the context of high speed wired networks. As interest increases in wireless ATM networks that extend the connection to a wireless endpoint, the issue of QoS over a wireless link has to be addressed. We focus on the provision of QoS at the application level in a wireless environment. Our work includes the design of an application programming interface (API) that allows applications to specify and renegotiate the QoS level during a call; as well as the implementation of such API in a wireless ATM testbed: the SWAN system. Experiments are performed to verify the efficiency of this scheme, and the results reveal quality control for multimedia applications despite changing network conditions.", "num_citations": "15\n", "authors": ["14"]}
{"title": "Software fault-tolerance by design diversity DEDIX: A tool for experiments\n", "abstract": " A large number of computing systems require very high levels of reliability, availability, or safety. A fault-avoidance approach is not practical in many eases, and is costly and difficult for software, if not impossible. One way of reducing tiie effects of an error introduced during the design of a program is to use multiple versions of the program, independently designed from a common specification. If these versions are designed by independent programming teams, it is to be expected that a fault in one version will not have the same behavior as any fault in the other versions. Since the errors in the output of the versions will be different and uncorrclated, it is pos-sible to run the versions concurrently, cross-check their results at prespecified points, and mask errors. A Design Diversity experiments (DEDIX) testbed has been implemented at UCLA to study the influence of common mode errors which can result in a failure of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["14"]}
{"title": "Benchmarking the capability of symbolic execution tools with logic bombs\n", "abstract": " Symbolic execution has become an indispensable technique for software testing and program analysis. However, since several symbolic execution tools are presently available off-the-shelf, there is a need for a practical benchmarking approach. This paper introduces a fresh approach that can help benchmark symbolic execution tools in a fine-grained and efficient manner. The approach evaluates the performance of such tools against known challenges faced by general symbolic execution techniques, e.g., floating-point numbers and symbolic memories. We first survey related papers and systematize the challenges of symbolic execution. We extract 12 distinct challenges from the literature and categorize them into two categories:  symbolic-reasoning challenges  and  path-explosion challenges . Next, we develop a dataset of logic bombs and a framework for benchmarking symbolic execution tools automatically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Geo-pairwise ranking matrix factorization model for point-of-interest recommendation\n", "abstract": " Point-of-interest (POI) recommendation that suggests new locations for people to visit is an important application in location-based social networks (LBSNs). Compared with traditional recommendation problems, e.g., movie recommendation, geographical influence is a special feature that plays an important role in recommending POIs. Various methods that incorporate geographical influence into collaborative filtering techniques have recently been proposed for POI recommendation. However, previous geographical models have struggled with a problem of geographically noisy POIs, defined as POIs that follow the geographical influence but do not satisfy users\u0393\u00c7\u00d6 preferences. We observe that users in the same geographical region share many POIs, and thus we propose the co-geographical influence to filter geographically noisy POIs. Furthermore, we propose the Geo-Pairwise Ranking Matrix\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "An EMG enhanced impedance and force control framework for telerobot operation in space\n", "abstract": " Tele-operation is a merging point of modern developments in robotics and communications technologies. Both traditional applications (e.g., mining) and emerging domains (e.g., microsurgery) benefit from the advancement of tele-robotic systems. Combining a local human operator and a remote autonomous robot, the tele-robotics systems could optimally exploit both the intelligence of human operator and the automation of robot. In a tele-operation scenario, the exchange of force and position signals, i.e., haptic feedback, can greatly extend human operator's capability of conducting complicated work through the robot in a remote environment. However, long-range communications usually suffer from the time delay problem caused by the inherent characteristics of communication channels. Delayed transmission of haptic signals may lead to instability in the closed-loop telerobot control system. Although much effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Mining test oracles of web search engines\n", "abstract": " Web search engines have major impact in people's everyday life. It is of great importance to test the retrieval effectiveness of search engines. However, it is labor-intensive to judge the relevance of search results for a large number of queries, and these relevance judgments may not be reusable since the Web data change all the time. In this work, we propose to mine test oracles of Web search engines from existing search results. The main idea is to mine implicit relationships between queries and search results, e.g., some queries may have fixed top 1 result while some may not, and some Web domains may appear together in top 10 results. We define a set of items of queries and search results, and mine frequent association rules between these items as test oracles. Experiments on major search engines show that our approach mines many high-confidence rules that help understand search engines and detect\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Near-duplicate keyframe retrieval by semi-supervised learning and nonrigid image matching\n", "abstract": " Near-duplicate keyframe (NDK) retrieval techniques are critical to many real-world multimedia applications. Over the last few years, we have witnessed a surge of attention on studying near-duplicate image/keyframe retrieval in the multimedia community. To facilitate an effective approach to NDK retrieval on large-scale data, we suggest an effective Multi-Level Ranking (MLR) scheme that effectively retrieves NDKs in a coarse-to-fine manner. One key stage of the MLR ranking scheme is how to learn an effective ranking function with extremely small training examples in a near-duplicate detection task. To attack this challenge, we employ a semi-supervised learning method, semi-supervised support vector machines, which is able to significantly improve the retrieval performance by exploiting unlabeled data. Another key stage of the MLR scheme is to perform a fine matching among a subset of keyframe candidates\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Supervised self-taught learning: Actively transferring knowledge from unlabeled data\n", "abstract": " We consider the task of Self-taught Learning (STL) from unlabeled data. In contrast to semi-supervised learning, which requires unlabeled data to have the same set of class labels as labeled data, STL can transfer knowledge from different types of unlabeled data. STL uses a three-step strategy: (1) learning high-level representations from unlabeled data only, (2) re-constructing the labeled data via such representations and (3) building a classifier over the re-constructed labeled data. However, the high-level representations which are exclusively determined by the unlabeled data, may be inappropriate or even misleading for the latter classifier training process. In this paper, we propose a novel Supervised Self-taught Learning (SSTL) framework that successfully integrates the three isolated steps of STL into a single optimization problem. Benefiting from the interaction between the classifier optimization and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Test selection for result inspection via mining predicate rules\n", "abstract": " It is labor-intensive to manually verify the outputs of a large set of tests that are not equipped with test oracles. Test selection helps to reduce this cost by selecting a small subset of tests that are likely to reveal faults. A promising approach is to dynamically mine operational models as potential test oracles and then select tests that violate them. Existing work mines operational models from verified passing tests based on dynamic invariant detection. In this paper, we propose to mine common operational models, which are not always true in all observed traces, from a set of unverified tests based on mining predicate rules. Specifically, we collect values of simple predicates at runtime and then generate and evaluate predicate rules as potential operational models after running all the tests. We then select tests that violate the mined predicate rules for result inspection. Preliminary results on the Siemens suite and the grep\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Integrating trust in grid computing systems\n", "abstract": " A Grid computing system is a virtual resource framework. Inside the framework, resources are being shared among autonomous domains which can be geographically distributed. One primary goal of such a virtual Grid environment is to encourage domain-to-domain interactions and to increase the confidence of domains to utilize or share resources without losing control and confidentiality. To achieve this goal, a Grid computing system can be viewed more or less as a human community and thus the \u0393\u00c7\u00a3trust\u0393\u00c7\u00a5 notion needs to be addressed. To integrate trust into a Grid, some specific issues need to be considered. In this paper, we view trust in two aspects, identity trust and behavior trust. Further, we briefly present two important issues which help in managing, evolving and interpreting trust. The two issues are grid context and trust tree structure.", "num_citations": "14\n", "authors": ["14"]}
{"title": "Design, testing, and evaluation techniques for software reliability engineering\n", "abstract": " Software reliability is closely influenced by the creation, manifestation and impact of software faults. Consequently, software reliability can be improved by treating software faults properly, using techniques of fault tolerance, fault removal, and fault prediction. Fault tolerance techniques achieve the design for reliability, fault removal techniques achieve the testing for reliability, and fault prediction techniques achieve the evaluation for reliability. We present best current practices in software reliability engineering for the design, testing, and evaluation purposes. We describe how fault tolerant components can be applied in software applications, how software testing can be conducted to show improvement of software reliability, and how software reliability can be modeled and measured for complex systems. We also examine the associated tools for applying these techniques.", "num_citations": "14\n", "authors": ["14"]}
{"title": "Experience in metrics and measurements for N-version programming\n", "abstract": " The N-Version Programming (NVP) approach applies the idea of design diversity to obtain fault-tolerant software units, called N-Version Software (NVS) units. The effectiveness of this approach is examined by the software diversity achieved in the member versions of an NVS unit. We define and formalize the concept of design diversity and software diversity in this paper. Design diversity is a property naturally applicable to the NVP process to increase its fault-tolerance attributes. The baseline design diversity is characterized by the employment of independent programming teams in the NVP. More design diversity investigations could be enforced in the NVP design process, including different languages, different tools, different algorithms, and different methodologies. Software diversity is the resulting dissimilarities appearing in the NVS member versions. We characterize it from four different points of view that are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["14"]}
{"title": "Neuron interaction based representation composition for neural machine translation\n", "abstract": " Recent NLP studies reveal that substantial linguistic information can be attributed to single neurons, ie, individual dimensions of the representation vectors. We hypothesize that modeling strong interactions among neurons helps to better capture complex information by composing the linguistic properties embedded in individual neurons. Starting from this intuition, we propose a novel approach to compose representations learned by different components in neural machine translation (eg, multi-layer networks or multi-head attention), based on modeling strong interactions among neurons in the representation vectors. Specifically, we leverage bilinear pooling to model pairwise multiplicative interactions among individual neurons, and a low-rank approximation to make the model computationally feasible. We further propose extended bilinear pooling to incorporate first-order representations. Experiments on WMT14 English\u0393\u00e7\u00c6 German and English\u0393\u00e7\u00c6 French translation tasks show that our model consistently improves performances over the SOTA Transformer baseline. Further analyses demonstrate that our approach indeed captures more syntactic and semantic information as expected.", "num_citations": "13\n", "authors": ["14"]}
{"title": "Mining business opportunities from location-based social networks\n", "abstract": " Urbanization's rapid progress has modernized a large number of human beings' lives. This urbanization progress is accompanied by the increase of a variety of shops (eg, restaurants and fitness centers) to meet the increasing citizens, which means business opportunities for the investors. Nevertheless, it is difficult for the investors to catch such opportunities because opening what kind of business at which place is not easy to decide. In this paper, we take this challenge and define the business opportunity mining problem, which recommends new business categories at a partitioned business district. Specifically, we exploit the data from location-based social networks (LBSNs) to mine the business opportunities, guiding the business owners to open new commercial shops in certain categories at a particular area. First, we define the properties of a business district and propose a greedy algorithm to partition a city\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["14"]}
{"title": "Delay-aware cost optimization for dynamic resource provisioning in hybrid clouds\n", "abstract": " Hybrid cloud computing paradigm has recently be widely advocated, where Software-as-a-Service (SaaS) providers can extend their local services into the public clouds seamlessly. In this way, dynamic user request workload to a SaaS can be elegantly handled with the rented computing capacity in public cloud. However, although a hybrid cloud may save cost compared with the private cloud, it still introduces considerable renting cost and communication cost. How to optimize such an operational cost becomes one major concern for the SaaS providers to adopt such a hybrid cloud computing paradigm. However, this critical problem remains unanswered in the current state of the art. In this paper, we focus on optimizing the operational cost for the hybrid cloud model by theoretically analyzing the problem with a Lyapunov optimization framework, and accordingly providing an online dynamic provision algorithm. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["14"]}
{"title": "Exploration of instantaneous amplitude and frequency features for epileptic seizure prediction\n", "abstract": " For the purpose of effective epileptic seizure prediction, this paper presents a new representation for the electroencephalogram (EEG) signal by recurring to their primary amplitude and frequency components. This formulation is then applied on an epoch-by-epoch basis to the preictal and interictal states of EEG signals in order to extract the most dominant amplitude and frequency characteristics. Through inspecting and identifying the distinctive EEG signal changes and stage transitions revealed by these extracted feature vectors, upcoming epileptic seizures could be predicted in due course. Machine learning approaches have been employed to construct patient-specific classifiers that can divide the extracted feature vectors into preictal and interictal groups. In this context, our work is distinguished from most currently adopted feature extraction process which employs time-consuming high-dimensional parameter\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["14"]}
{"title": "Automatic 3D face modeling using 2D active appearance models\n", "abstract": " Although a lot of promising research findings have been studied on 3D face modeling in the past years, it is still a challenge to generate realistic 3D human face models and facial animations. This paper presents a novel approach to model 3D faces automatically from still images or video sequences without manual interactions. Our proposed scheme comprises three steps. First, we offline construct 3D shape models using Active Appearance Models (AAMs), which saves large computation costs for online modeling. Second, based on the computed 3D shape models, we propose an efficient algorithm to estimate the parameters of 3D pose and non-rigid shape via local bundle adjustment. Third, we employ the recovered 3D face shape to deform the high resolution face mesh through scattered data interpolation, and extract the face texture maps for rendering the reconstructed face model from various viewpoints. Since the correspondence is built by AAMs fitting, the 3D face model can be constructed effectively even from a single image.", "num_citations": "13\n", "authors": ["14"]}
{"title": "A systematic and comprehensive tool for software reliability modeling and measurement\n", "abstract": " Sufficient work has been done to demonstrate that software reliability models can be used to monitor reliability growth over a useful range of software development projects. However, due to the lack of appropriate tools, the application of software reliability models as a means for project management is not as widespread as it might be. The existing software reliability modeling and measurement programs are either difficult for a nonspecialist to use, or short of a systematic and comprehensive capability in the software reliability measurement practice. To address the ease-of-use and the capability issues, the authors have prototyped a software reliability modeling tool called CASRE, a Computer-Aided Software Reliability Estimation tool. Implemented with a systematic and comprehensive procedure as its framework, CASRE will encourage more widespread use of software reliability modeling and measurement as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["14"]}
{"title": "NV-DNN: towards fault-tolerant DNN systems with N-version programming\n", "abstract": " Employing deep learning algorithms in real-world applications becomes a trend. However, a bottleneck that impedes their further adoption in safety-critical systems is the reliability issue. It is challenging to develop reliable neural network models as the theory of deep learning has not yet been well-established and neural network models are very sensitive to data perturbations. Inspired by the classic paradigm of N-version programming for fault tolerance, this paper investigates the feasibility of developing fault-tolerant deep learning systems through model redundancy. We hypothesize that if we train several simplex models independently, these models are unlikely to produce erroneous results for the same test cases. In this way, we can design a fault-tolerant system whose output is determined by all these models cooperatively. We propose several independence factors that can be introduced for generating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["14"]}
{"title": "QoS Prediction in Cloud and Service Computing: Approaches and Applications\n", "abstract": " Cloud computing provides shared resources (eg, infrastructure, platform, and software) as services. Service-oriented architecture (SOA) is the technical foundation of cloud computing, whereby services offered by different cloud providers are discovered and integrated over the Internet. Quality-of-Service (QoS) is widely employed to represent the non-functional performance of services and has been considered as the key factor to differentiate the qualities of service candidates. It becomes important to evaluate the QoS performance of services. However, QoS evaluation is time-and resource-consuming. Conducting real-world evaluation is difficult in practice. Moreover, in some scenarios, QoS evaluation becomes impossible (eg, the cloud provider may charge for service invocations, too many services to be evaluated). Therefore, it is crucial to study how to build effective and efficient approaches to predict the QoS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["14"]}
{"title": "A hierarchical matrix factorization approach for location-based web service QoS prediction\n", "abstract": " With the rapid growth of population of service-oriented architecture (SOA), services are playing an important role in software development process. One major issue we should consider about Web services is to dig out the one with the best QoS value among all functionally-equivalent candidates. However, since there are a great number of missing QoS values in real world invocation records, we can hardly do a detailed comparison among those selectable Web services. To address this problem, we propose a location-based hierarchical matrix factorization method to make efficient and accurate QoS prediction. In our method, we consider both global context and local information. We first apply matrix factorization (MF) on global user-service records and obtain a global prediction matrix. After that, we use MF to predict QoS values on some user-service groups, which are clustered by K-means algorithm. Then we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["14"]}
{"title": "Robust Filtering for a Class of Complex Networks with Stochastic Packet Dropouts and Time Delays\n", "abstract": " The robust  filtering problem is investigated for a class of complex network systems which has stochastic packet dropouts and time delays, combined with disturbance inputs. The packet dropout phenomenon occurs in a random way and the occurrence probability for each measurement output node is governed by an individual random variable. Besides, the time delay phenomenon is assumed to occur in a nonlinear vector-valued function. We aim to design a filter such that the estimation error converges to zero exponentially in the mean square, while the disturbance rejection attenuation is constrained to a given level by means of the  performance index. By constructing the proper Lyapunov-Krasovskii functional, we acquire sufficient conditions to guarantee the stability of the state detection observer for the discrete systems, and the observer gain is also derived by solving linear matrix inequalities. Finally, an illustrative example is provided to show the usefulness and effectiveness of the proposed design method.", "num_citations": "12\n", "authors": ["14"]}
{"title": "A novel coalitional game model for security issues in wireless networks\n", "abstract": " In this paper, we propose a novel coalitional game model for security issues in wireless networks. The model can be applied to not only mobile ad hoc networks but also wireless sensor networks. We define a new throughput characteristic function, on the basis of which nodes are enforced to cooperate and form coalitions. This function implies the maximal throughput and the most reliable traffic that a coalition can achieve. The fair payoff share inside the coalition is given by Shapley value after proving the feasibility of this method. Then a set of game rules is presented to establish a threatening mechanism to all players. We then describe the coalition formation procedure and explain how to integrate this game theoretic model with available wireless routing protocols. Finally, a theoretical analysis is conducted to illustrate the convergence situation and verify the correctness of the formulation.", "num_citations": "12\n", "authors": ["14"]}
{"title": "Digital video watermarking with a genetic algorithm\n", "abstract": " Due to the explosion of data exchange on the Internet and the extensive use of digital media, it has been of intense interest to digital data owners in multimedia security and multimedia copyright protection. In this paper, a comprehensive approach for protecting and managing video copyrights with watermarking techniques is introduced. We propose a novel digital video watermarking scheme based on the scene change analysis and genetic algorithm. Robustness and fidelity are the essential requirements of a successful watermarking scheme. In pervious work, a robustness scene-based watermarking scheme is proposed. We focus on improving the fidelity of the scheme in this paper. The fidelity of the scheme is enhanced by applying a genetic algorithm, which optimizes the quality of the watermarked video. The effectiveness of this scheme is verified through a series of experiments.", "num_citations": "12\n", "authors": ["14"]}
{"title": "Integrating user feedback log into relevance feedback by coupled svm for content-based image retrieval\n", "abstract": " Relevance feedback has been shown as an important tool to boost the retrieval performance in content-based image retrieval. In the past decade, various algorithms have been proposed to formulate relevance feedback in contentbased image retrieval. Traditional relevance feedback techniques mainly carry out the learning tasks by focusing lowlevel visual features of image content with little consideration on log information of user feedback. However, from a long-term learning perspective, the user feedback log is one of the most important resources to bridge the semantic gap problem in image retrieval. In this paper we propose a novel technique to integrate the log information of user feedback into relevance feedback for image retrieval. Our algorithm\u0393\u00c7\u00d6s construction is based on a coupled support vector machine which learns consistently with the two types of information: the low-level image content and the user\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["14"]}
{"title": "Analysis of privacy and non-repudiation on pay-TV systems\n", "abstract": " Lee-Chang-Lin-Hwang (see IEEE Trans. On Consumer Electronics, vol.46, no.1, p.20-26, 2000) in 2000 proposed a set of protocols for pay-TV systems in order to secure subscriber's privacy and build a fair pay-TV system. However, we have found that an attacker can easily get other subscriber's privacy in watching TV-programs. We analyze the reason and discuss the possible amendments. Moreover, we expose a weakness on non-repudiation and suggest an improvement to support non-repudiation.", "num_citations": "12\n", "authors": ["14"]}
{"title": "ComPARE: A generic quality assessment environment for component-based software systems\n", "abstract": " Component-based technology is gaining popularity in modern software development. This approach helps reduce development cost and time-to-market, as well as improve maintainability and reliability. One of the key problems in component-based software development is finding a way to certify the quality of individual components and that of the integrated component-based software systems. There are several different techniques which have been developed to describe the predictive relationship between software metrics and the reliability of the software components.", "num_citations": "12\n", "authors": ["14"]}
{"title": "Atacobol-a cobol test coverage analysis tool and its applications\n", "abstract": " A coverage testing tool ATACOBOL (Automatic Test Analysis for COBOL) that applies a data flow coverage technique is developed for software development on an IBM System/390 mainframe. We show that the data flow coverage criteria can identify possible problematic paths that map to the actual testing semantic required by Y2K compliance software testing. However, the mainframe environment lacks testing tools that equip the data flow coverage measure. Up to the current implementation, ATACOBOL is able to perform block coverage, decision coverage and all-uses measures. We extend the rules of data flow coverage criteria to adapt data structures that modern high-level languages usually employ.", "num_citations": "12\n", "authors": ["14"]}
{"title": "Software reliability: to use or not to use?\n", "abstract": " Research activities in software reliability engineering have been vigorous in the past two decades since Z. Jelinski and P.B. Moranda (1972) proposed the first software reliability model. Since then, numerous software reliability models and measurement procedures have been proposed for the prediction, estimation, and engineering of software reliability. However, there seems to be a gap among the software engineering practitioners regarding the use of software reliability. Believers advocate the use of software reliability and promote the stories of success, while skeptics doubt the adequacy, validity, and consistency of software reliability both in terms of its concept and in terms of its practicality.< >", "num_citations": "12\n", "authors": ["14"]}
{"title": "Software reliability measurements in N-Version software execution environment.\n", "abstract": " N-Version Programming has been proposed as a means to increasing software reliability by the attributes provided in fault tolerance. In this paper we will quantitatively examine the effectiveness of this approach. We look into the details of an academia/industry joint project employing six programming languages, and study the properties of the resulting program versions. We explain how exhaustive testing was applied to the project, and measure the error probability in different N-Version Software execution configurations. To explore the manifestations of errors resulting from each programming fault, and to study their impact to N-Version Software systems, we apply mutation testing techniques to gain more insights. We further define several reliability-related quantities, including error frequency function, error severity function, error similarity function, and safety coverage factor. With the multiple program versions obtained in the project, we create a total of 93 mutants, and measure the reliability quantities for each of them. Based on these quantities, we estimate the safety coverage factor for the N-Version Software system, which reveals the potential of this technique in improving software reliability. A side observation of this research is that the per fault error rate does not remain constant in this computation-intensive project. The error rates associated with each program fault differ from each other dramatically. However, they tend to decrease as testing progresses. This information might be helpful for practitioners to decide which models to apply for software reliability measurement in a similar project.", "num_citations": "12\n", "authors": ["14"]}
{"title": "Web service QoS prediction via collaborative filtering: A survey\n", "abstract": " With the growing number of competing Web services that provide similar functionality, Quality-of-Service (QoS) prediction is becoming increasingly important for various QoS-aware approaches of Web services. Collaborative filtering (CF), which is among the most successful personalized prediction techniques for recommender systems, has been widely applied to Web service QoS prediction. In addition to using conventional CF techniques, a number of studies extend the CF approach by incorporating additional information about services and users, such as location, time, and other contextual information from the service invocations. There are also some studies that address other challenges in QoS prediction, such as adaptability, credibility, privacy preservation, and so on. In this survey, we summarize and analyze the state-of-the-art CF QoS prediction approaches of Web services and discuss their features and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["14"]}
{"title": "Concolic execution on small-size binaries: Challenges and empirical study\n", "abstract": " Concolic execution has achieved great success in many binary analysis tasks. However, it is still not a primary option for industrial usage. A well-known reason is that concolic execution cannot scale up to large-size programs. Many research efforts have focused on improving its scalability. Nonetheless, we find that, even when processing small-size programs, concolic execution suffers a great deal from the accuracy and scalability issues. This paper systematically investigates the challenges that can be introduced even by small-size programs, such as symbolic array and symbolic jump. We further verify that the proposed challenges are non-trivial via real-world experiments with three most popular concolic execution tools: BAP, Triton, and Angr. Among a set of 22 logic bombs we designed, Angr can solve only four cases correctly, while BAP and Triton perform much worse. The results imply that current tools are still\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["14"]}
{"title": "Can irrelevant data help semi-supervised learning, why and how?\n", "abstract": " Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unlabeled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["14"]}
{"title": "Flow-augmented call graph: A new foundation for taming api complexity\n", "abstract": " Software systems often undergo significant changes in their life cycle, exposing increasingly complex API to their developers. Without methodical guidances, it is easy to become bogged down in a morass of complex API even for the professional software developers. This paper presents the Flow-Augmented Call Graph (FACG) for taming API complexity. Augmenting the call graph with control flow analysis brings us a new insight to capture the significance of the caller-callee linkages in the call graph. We apply the proposed FACG in API recommendation and compare our approach with the state-of-the-art approaches in the same domain. The evaluation result indicates that our approach is more effective in retrieving the relevant APIs with regard to the original API documentation.", "num_citations": "11\n", "authors": ["14"]}
{"title": "Simulation techniques for component-based software reliability modeling with project application\n", "abstract": " In this paper, we consider to combine analytical models with simulation techniques for software reliability measurements. We have implemented a set of failure-rate-based simulation techniques which can capture the characteristic of software process and structure in a way that permits us to obtain quantified results for software reliability measures. We address two methods to take into account the functional dependency and error correlation among components, so that we can treat a software system as a combination of interdependent components. This offers a more appropriate approach for analyzing reliability measurements of component-based software systems. The results from a project application indicate that the incorporation of simulation techniques into analytical models has the advantages of accurate analyses, early predictions, and comprehensive evaluations for software reliability engineering.", "num_citations": "11\n", "authors": ["14"]}
{"title": "On the effectiveness of multiversion software in digital avionics\n", "abstract": " The means for attaining effective tolerance of design faults by means of multiversion software are discussed. A comparison is made between two recent investigations (U. of VirginiUJ. of California at Irvine, and UCLA/Honeywell) that hfie produced differing results. Some likely reasons for the differences are identified.", "num_citations": "11\n", "authors": ["14"]}
{"title": "A survey on automated log analysis for reliability engineering\n", "abstract": " Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems, because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "Maximum margin semi-supervised learning with irrelevant data\n", "abstract": " Semi-supervised learning (SSL) is a typical learning paradigms training a model from both labeled and unlabeled data. The traditional SSL models usually assume unlabeled data are relevant to the labeled data, ie, following the same distributions of the targeted labeled data. In this paper, we address a different, yet formidable scenario in semi-supervised classification, where the unlabeled data may contain irrelevant data to the labeled data. To tackle this problem, we develop a maximum margin model, named tri-class support vector machine (3C-SVM), to utilize the available training data, while seeking a hyperplane for separating the targeted data well. Our 3C-SVM exhibits several characteristics and advantages. First, it does not need any prior knowledge and explicit assumption on the data relatedness. On the contrary, it can relieve the effect of irrelevant unlabeled data based on the logistic principle and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "A distributed framework for spatio-temporal analysis on large-scale camera networks\n", "abstract": " Cameras are becoming ubiquitous. Applications including video-based surveillance and emergency response exploit camera networks to detect anomalies in real time and reduce collateral damage. A well-known technique for detecting anomalies is spatio-temporal analysis -- an inferencing technique employed by domain experts (e.g., vision researchers) to answer spatio-temporal queries. In this paper, we propose a distributed framework that facilitates the development and deployment of spatio-temporal analysis applications on large-scale camera networks and backend computing resources. We make the following contributions: (a) an investigation of the computation/communication costs associated with spatio-temporal analysis, (b) a programming framework designed for large-scale spatio-temporal analysis, and (c) performance evaluations for each step of the spatio-temporal analysis with realistic algorithms.", "num_citations": "10\n", "authors": ["14"]}
{"title": "An online service-oriented performance profiling tool for cloud computing systems\n", "abstract": " The growing scale and complexity of component interactions in cloud computing systems post great challenges for operators to understand the characteristics of system performance. Profiling has long been proved to be an effective approach to performance analysis; however, existing approaches confront new challenges that emerge in cloud computing systems. First, the efficiency of the profiling becomes of critical concern; second, service-oriented profiling should be considered to support separation-of-concerns performance analysis. To address the above issues, in this paper, we present P-Tracer, an online performance profiling tool specifically tailored for cloud computing systems. P-Tracer constructs a specific search engine that proactively processes performance logs and generates a particular index for fast queries; second, for each service, P-Tracer retrieves a statistical insight of performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "The generalized dependency degree between attributes\n", "abstract": " Inspired by the dependency degree \u256c\u2502, a traditional measure in Rough Set Theory, we propose a generalized dependency degree, \u256c\u00f4, between two given sets of attributes, which counts both deterministic and indeterministic rules while \u256c\u2502 counts only deterministic rules. We first give its definition in terms of equivalence relations and then interpret it in terms of minimal rules, and further describe the algorithm for its computation. To understand \u256c\u00f4 better, we investigate its various properties. We further extend \u256c\u00f4 to incomplete information systems. To show its advantage, we make a comparative study with the conditional entropy and \u256c\u2502 in a number of experiments. Experimental results show that the speed of the new C4.5 using \u256c\u00f4 is greatly improved when compared with the original C4.5R8 using conditional entropy, while the prediction accuracy and tree size of the new C4.5 are comparable with the original one. Moreover, \u256c\u00f4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "Two-stage multi-class AdaBoost for facial expression recognition\n", "abstract": " Although AdaBoost has achieved great success, it still suffers from following problems: (1) the training process could be unmanageable when the number of features is extremely large; (2) the same weak classifier may be learned multiple times from a weak classifier pool, which does not provide additional information for updating the model; (3) there is an imbalance between the amount of the positive samples and that of the negative samples for multi-class classification problems. In this paper, we propose a two-stage AdaBoost learning framework to select and fuse the discriminative feature effectively. Moreover, an improved AdaBoost algorithm is developed to select weak classifiers. Instead of boosting in the original feature space, whose dimensionality is usually very high, multiple feature subspaces with lower dimensionality are generated. In the first stage, boosting is carried out in each subspace. Then the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "A wireless handheld multi-modal digital video library client system\n", "abstract": " We developed technologies for transmitting video contents over wireless platforms, and encapsulated these video delivery and presentation technologies into a client system for accessing a multi-modal digital video library. The mobile access system, iVIEW client, provides a user interface that meets the challenge of rich multi-modal information presentation on wireless hand-held devices. An XML schema is employed to organize the multi-modal metadata for better data interoperability. Furthermore, we investigated a context awareness mechanism complementary to the XML schema to facilitate scalable degradation under restricted resources in wireless application environment. This paper presents the design, implementation, and evaluation of the iVIEW system and its associated technologies for video information management and delivery on pervasive devices over wireless networks.", "num_citations": "10\n", "authors": ["14"]}
{"title": "PVCAIS: A personal videoconference archive indexing system\n", "abstract": " Wilh the rapid deployment of videoconference, the fast-accumulated personal videoconference archives need to be effectively indexed. This paper proposes a well-designed indexing system - PVCAIS that integrates many multimedia-indexing techniques to manage personal videoconference archives. Firstly, the contents of video, audio, text and whiteboard communications are stored after removing the redundancies. Next, more information, e.g., participants, title, keywords and slides, is extracted by face detection and recognition, speech recognition, OCR, automatic title generation, keyword selection, etc. Then, an XML index file containing the summary of the videoconference is also generated. The whole indexing process is automatic except that the face of a new contact needs to be interactively identified for only once. Finally, we demonstrate a graphical user interface which allows the user to search and browse\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "An integrated approach to achieving high software reliability\n", "abstract": " In this paper we address the development, testing, and evaluation schemes for software reliability, and the integration of these schemes into a unified and consistent paradigm. Specifically, techniques and tools for the three software reliability engineering phases are described. The three phases are (1) modeling and analysis, (2) design and implementation, and (3) testing and measurement. In the modeling and analysis phase we describe Markov modeling and fault-tree analysis techniques. We present system-level reliability models based on these techniques, and provide modeling examples for reliability analysis and study. We describe how reliability block diagrams can be constructed for a real-world system for reliability prediction, and how critical components can be identified. We also apply fault tree models to fault tolerant system architectures, and formulate the resulting reliability quantity. Finally, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["14"]}
{"title": "Revisiting parameter sharing for automatic neural channel number search\n", "abstract": " Recent advances in neural architecture search inspire many channel number search algorithms (CNS) for convolutional neural networks. To improve searching efficiency, parameter sharing is widely applied, which reuses parameters among different channel configurations. Nevertheless, it is unclear how parameter sharing affects the searching process. In this paper, we aim at providing a better understanding and exploitation of parameter sharing for CNS. Specifically, we propose affine parameter sharing (APS) as a general formulation to unify and quantitatively analyze existing channel search algorithms. It is found that with parameter sharing, weight updates of one architecture can simultaneously benefit other candidates. However, it also results in less confidence in choosing good architectures. We thus propose a new strategy of parameter sharing towards a better balance between training efficiency and architecture discrimination. Extensive analysis and experiments demonstrate the superiority of the proposed strategy in channel configuration against many state-of-the-art counterparts on benchmark datasets.", "num_citations": "9\n", "authors": ["14"]}
{"title": "Deepobfuscation: Securing the structure of convolutional neural networks via knowledge distillation\n", "abstract": " This paper investigates the piracy problem of deep learning models. Designing and training a well-performing model is generally expensive. However, when releasing them, attackers may reverse engineer the models and pirate their design. This paper, therefore, proposes deep learning obfuscation, aiming at obstructing attackers from pirating a deep learning model. In particular, we focus on obfuscating convolutional neural networks (CNN), a widely employed type of deep learning architectures for image recognition. Our approach obfuscates a CNN model eventually by simulating its feature extractor with a shallow and sequential convolutional block. To this end, we employ a recursive simulation method and a joint training method to train the simulation network. The joint training method leverages both the intermediate knowledge generated by a feature extractor and data labels to train a simulation network. In this way, we can obtain an obfuscated model without accuracy loss. We have verified the feasibility of our approach with three prevalent CNNs, i.e., GoogLeNet, ResNet, and DenseNet. Although these networks are very deep with tens or hundreds of layers, we can simulate them in a shallow network including only five or seven convolutional layers. The obfuscated models are even more efficient than the original models. Our obfuscation approach is very effective to protect the critical structure of a deep learning model from being exposed to attackers. Moreover, it can also thwart attackers from pirating the model with transfer learning or incremental learning techniques because the shallow simulation network bears poor learning ability. To\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["14"]}
{"title": "Learning to rank using localized geometric mean metrics\n", "abstract": " Many learning-to-rank~(LtR) algorithms focus on query-independent model, in which query and document do not lie in the same feature space, and the rankers rely on the feature ensemble about query-document pair instead of the similarity between query instance and documents. However, existing algorithms do not consider local structures in query-document feature space, and are fragile to irrelevant noise features. In this paper, we propose a novel Riemannian metric learning algorithm to capture the local structures and develop a robust LtR algorithm. First, we design a concept called ideal candidate document to introduce metric learning algorithm to query-independent model. Previous metric learning algorithms aiming to find an optimal metric space are only suitable for query-dependent model, in which query instance and documents belong to the same feature space and the similarity is directly computed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["14"]}
{"title": "N-version obfuscation\n", "abstract": " Although existing for decades, software tampering attack is still a main threat to systems, such as Android, and cyber physical systems. Many approaches have been proposed to thwart specific procedures of tampering, eg, obfuscation and self-checksumming. However, none of them can achieve theoretically tamper-proof without the protection of hardware circuit. Rather than proposing new tricks against tampering attacks, we focus on impeding the replication of software tampering via program diversification, and thus pose a scalability barrier against the attacks. Our idea, namely N-version obfuscation (NVO), is to automatically generate and deliver same featured, but functionally nonequivalent software copies to different machines or users.", "num_citations": "9\n", "authors": ["14"]}
{"title": "Persisdroid: Android performance diagnosis via anatomizing asynchronous executions\n", "abstract": " Android applications (apps) grow dramatically in recent years. Apps are user interface (UI) centric typically. Rapid UI responsiveness is key consideration to app developers. However, we still lack a handy tool for profiling app performance so as to diagnose performance problems. This paper presents PersisDroid, a tool specifically designed for this task. The key notion of PersisDroid is that the UI-triggered asynchronous executions also contribute to the UI performance, and hence its performance should be properly captured to facilitate performance diagnosis. However, Android allows tremendous ways to start the asynchronous executions, posing a great challenge to profiling such execution. This paper finds that they can be grouped into six categories. As a result, they can be tracked and profiled according to the specifics of each category with a dynamic instrumentation approach carefully tailored for Android. PersisDroid can then properly profile the asynchronous executions in task granularity, which equips it with low-overhead and high compatibility merits. Most importantly, the profiling data can greatly help the developers in detecting and locating performance anomalies. We code and open-source release PersisDroid. The tool is applied in diagnosing 20 open-source apps, and we find 11 of them contain potential performance problems, which shows its effectiveness in performance diagnosis for Android apps.", "num_citations": "9\n", "authors": ["14"]}
{"title": "RealProct: reliable protocol conformance testing with real nodes for wireless sensor networks\n", "abstract": " Despite the various applications of wireless sensor network (WSN), experiences from real WSN deployments show that protocol implementations in sensor nodes are susceptible to software failures, which may cause network failures or even breakdown. Pre-deployment protocol conformance testing is essential to ensure reliable communications for WSNs. Unfortunately, existing solutions with simulators cannot test the exact hardware and implementation environment as real sensors, whereas testbeds are expensive and limited to small scale networks and topologies. In this paper, we present RealProct, a novel and reliable framework for testing protocol implementations against their specifications in WSNs. RealProct utilizes real sensors for protocol conformance testing to ensure that the results are close to the real deployment. Using different techniques from those in simulations and real deployments, RealProct\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["14"]}
{"title": "Classification based on Gabor filter using RBPNN classification\n", "abstract": " In this paper, a color pattern recognition technique that is suitable for multicolor images of bark has been analyzed and evaluated. To extract the bark texture features, Gabor filter the image has been filtered with four orientations and six scales filters, and then the mean and standard deviation of the image output are computed. In addition, the obtained Gabor feature vectors are fed up into radial basis function neural network (RBPNN) for classification. The performance of color space features is found to be better than that of the features which just extracted from gray image. The experimental results show this approach can be used to automatically identify the plant categories more effective", "num_citations": "9\n", "authors": ["14"]}
{"title": "Merging Results from Different Media: Lic2m Experiments at ImageCLEF 2005.\n", "abstract": " In the ImageCLEF 2005 campaign, the LIC2M participated in the ad hoc task, the medical task and the annotation task. For both ad hoc and medical task, we perform experiments on merging the results of two independent search systems: a crosslanguage information retrieval system exploiting the text part of the query and a content-based image retrieval system exploiting the example images given with the query. The results show that a well-tuned merging may improve performance, but the tuning is made difficult because the performance of each system highly depends on the corpus and queries. Annotation task has been performed using a KNN classifier with the image indexes of our CBIR system.", "num_citations": "9\n", "authors": ["14"]}
{"title": "Layered obfuscation: a taxonomy of software obfuscation techniques for layered security\n", "abstract": " Software obfuscation has been developed for over 30 years. A problem always confusing the communities is what security strength the technique can achieve. Nowadays, this problem becomes even harder as the software economy becomes more diversified. Inspired by the classic idea of layered security for risk management, we propose layered obfuscation as a promising way to realize reliable software obfuscation. Our concept is based on the fact that real-world software is usually complicated. Merely applying one or several obfuscation approaches in an ad-hoc way cannot achieve good obscurity. Layered obfuscation, on the other hand, aims to mitigate the risks of reverse software engineering by integrating different obfuscation techniques as a whole solution. In the paper, we conduct a systematic review of existing obfuscation techniques based on the idea of layered obfuscation and develop a novel taxonomy of obfuscation techniques. Following our taxonomy hierarchy, the obfuscation strategies under different branches are orthogonal to each other. In this way, it can assist developers in choosing obfuscation techniques and designing layered obfuscation solutions based on their specific requirements.", "num_citations": "8\n", "authors": ["14"]}
{"title": "Simple and efficient parallelization for probabilistic temporal tensor factorization\n", "abstract": " Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm to model the temporal tensor data. It leverages a time constraint to capture the evolving properties of tensor data. Nowadays the exploding dataset demands a large scale PTTF analysis, and a parallel solution is critical to accommodate the trend. Whereas, the parallelization of PTTF still remains unexplored. In this paper, we propose a simple yet efficient Parallel Probabilistic Temporal Tensor Factorization, referred to as P 2 T 2 F, to provide a scalable PTTF solution. P 2 T 2 F is fundamentally disparate from existing parallel tensor factorizations by considering the probabilistic decomposition and the temporal effects of tensor data. It adopts a new tensor data split strategy to subdivide a large tensor into independent sub-tensors, the computation of which is inherently parallel. We train P 2 T 2 F with an efficient algorithm of stochastic Alternating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "Budget constrained non-monotonic feature selection\n", "abstract": " Feature selection is an important problem in machine learning and data mining. We consider the problem of selecting features under the budget constraint on the feature subset size. Traditional feature selection methods suffer from the \u0393\u00c7\u00a3monotonic\u0393\u00c7\u00a5 property. That is, if a feature is selected when the number of specified features is set, it will always be chosen when the number of specified feature is larger than the previous setting. This sacrifices the effectiveness of the non-monotonic feature selection methods. Hence, in this paper, we develop an algorithm for non-monotonic feature selection that approximates the related combinatorial optimization problem by a Multiple Kernel Learning (MKL) problem. We justify the performance guarantee for the derived solution when compared to the global optimal solution for the related combinatorial optimization problem. Finally, we conduct a series of empirical evaluation on both\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "Granger causality-aware prediction and diagnosis of software degradation\n", "abstract": " Software that continuously runs over a long period has been frequently reported encountering \"gradual degradation\" issues. As time progresses, software tends to exhibit degraded performance, deflated capacity, exhausted physical resource or deteriorated QoS (Quality of Service). Different from transient software anomalies, this issue is a chronic degrading process and usually persists until the software is eventually unavailable. We name it \"Software Degradation\" or \"Degradation\" for short. In this paper, we propose a framework GVAR, utilizing Granger Causalities to predict and diagnose software degradation. GVAR is evaluated via an 8-day experiment on a VoD (Video on Demand) platform Helix-Serv. The experimental results show that GVAR can predict the TTF (Time to Failure) of degraded software in an accuracy of 80.1%, remarkably outweighing the widely used ARMA and Sen's Slope Estimator\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "A hierarchical entity-based approach to structuralize user generated content in social media: A case of Yahoo! answers\n", "abstract": " Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This \u0393\u00c7\u00a3list-based\u0393\u00c7\u00a5 approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called \u0393\u00c7\u00a3cluster entity tree (CET)\u0393\u00c7\u00a5. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (ie, vector space model and query likelihood language model).", "num_citations": "8\n", "authors": ["14"]}
{"title": "Fault detection of networked control systems based on sliding mode observer\n", "abstract": " This paper is concerned with the network-based fault detection problem for a class of nonlinear discrete-time networked control systems with multiple communication delays and bounded disturbances. First, a sliding mode based nonlinear discrete observer is proposed. Then the sufficient conditions of sliding motion asymptotical stability are derived by means of the linear matrix inequality (LMI) approach on a designed surface. Then a discrete-time sliding-mode fault observer is designed that is capable of guaranteeing the discrete-time sliding-mode reaching condition of the specified sliding surface. Finally, an illustrative example is provided to show the usefulness and effectiveness of the proposed design method.", "num_citations": "8\n", "authors": ["14"]}
{"title": "Semi-nonnegative matrix factorization with global statistical consistency for collaborative filtering\n", "abstract": " Collaborative Filtering, considered by many researchers as the most important technique for information filtering, has been extensively studied by both academic and industrial communities. One of the most popular approaches to collaborative filtering recommendation algorithms is based on low-dimensional factor models. The assumption behind such models is that a user's preferences can be modeled by linearly combining item factor vectors using user-specific coefficients. In this paper, aiming at several aspects ignored by previous work, we propose a semi-nonnegative matrix factorization method with global statistical consistency. The major contribution of our work is twofold:(1) We endow a new understanding on the generation or latent compositions of the user-item rating matrix. Under the new interpretation, our work can be formulated as the semi-nonnegative matrix factorization problem.(2) Moreover, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "An index-based sensor-grouping mechanism for efficient field-coverage wireless sensor networks\n", "abstract": " This paper discusses a point-distribution index, l, which measures the normalized minimum distance between sensors. Maximizing l of a set of points causes the Delaunay triangulation graph of these points to be a net of equilateral triangles. Such a structure indicates the lowest redundancy of coverage if each point represents the center of a disc. Thus l can serve as a promising measure for solving a critical problem in field coverage: How to group a set of sensor nodes into disjoint subsets so that each subset can cover the entire field? Based on the l index, we develop an effective algorithm, MAXINE (MAXimizing-l Node-redundancy Exploiting), for the sensor- grouping problem. We evaluate the performance of MAXINE through extensive simulations and compare it with existing algorithms. The results demonstrate the effectiveness of MAXINE and verify the superiority of employing i for the sensor-grouping problem.", "num_citations": "8\n", "authors": ["14"]}
{"title": "A framework for inheritance testing from VDM++ specifications\n", "abstract": " The benefits offered by the use of formal methods are not limited to avoidance of specification errors and elimination of ambiguities only - a formal specification also provides a sound basis for generating test suites. Inheritance is a powerful mechanism in object-oriented paradigm by which a subclass inherits data and functionality of a super class. Testing of inheritance relationships is crucial in object-oriented testing, as an inheritance error may lead to subtle bugs such as due to overridden functionality. In this paper, we introduce a technique to generate test cases for inheritance testing, using a VDM++ formal specification. The proposed technique is based on the flattening of a VDM++ specification class, and then generating operation sequences from the trace structure specified in the VDM++ specification. The input space for each operation is partitioned, and a test model is constructed from the operation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "A point-distribution index and its application to sensor-grouping in wireless sensor networks\n", "abstract": " We propose \u256c\u2563 a novel index for evaluation of point-distribution. \u256c\u2563 is the minimum distance between each pair of points normalized by the average distance between each pair of points. We find that a set of points that achieve a maximum value of \u256c\u2563 result in a honeycomb structure. We propose that \u256c\u2563 can serve as a good index to evaluate the distribution of the points, which can be employed in coverage-related problems in wireless sensor networks (WSNs). To validate this idea, we formulate a general sensorgrouping problem for WSNs and provide a general sensing model. We show that locally maximizing \u256c\u2563 at sensor nodes is a good approach to solve this problem with an algorithm called Maximizing-\u256c\u2563 Node-Deduction (MIND). Simulation results verify that MIND outperforms a greedy algorithm that exploits sensor-redundancy we design. This demonstrates a good application of employing \u256c\u2563 in coverage-related problems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "Domain Testing Based on Character String Predicate.\n", "abstract": " Domain testing is a well-known software testing technique. Although research tasks have been initiated in domain testing, automatic test data generation based on character string predicates has not yet been reported. This paper presents a novel approach to automatically generate ON-OFF test points for character string predicate borders associated with program paths, and describes a corresponding test data generator. Slices with respect to predicates on paths are constructed to calculate the current values of variables in the predicates via program slicing techniques. Each character element of variables in a character string predicate is dynamically determined in turn by function minimization so that the ON-OFF test points for the predicate border can be automatically generated. The preliminary experimental results show that this approach is promising and effective.", "num_citations": "8\n", "authors": ["14"]}
{"title": "ADVISE: Advanced digital video information segmentation engine\n", "abstract": " This paper describes the design of ADVISE, Advanced Digital Video Information Segmentation Engine, which is a web-based video retrieval system. The system aims at providing a visual summarization of video contents, such that users can efficiently determine whether they are interested in the video before they have downloaded it from the Internet. ADVISE consists of three major modules. The first module is responsible for automatic structuring of videos. The second module stores the structure as a Video Table-Of-Contents (VTOC) in XML format, and presents the VTOC on the Web with XSL. The third module provides users options to personalize a video into its summary using SMIL. The system architecture and some implementation screen shots are presented.", "num_citations": "8\n", "authors": ["14"]}
{"title": "Design, Implementation, and Experimentation on Mobile Agent Security for Electronic Commerce Applications.\n", "abstract": " Agent System (SIAS) is built based on mobile agent technology. It sends out agents to different hosts in an electronic marketplace. The agents collect and report information such as prices and availabilities about products specified by users. Snapshots of the system in use are shown. Security is a major problem of mobile agent systems, especially when money transactions are concerned. Possible security attacks by malicious hosts to agents in SIAS are discussed, and a solution to prevent these attacks is presented. Finally, security of the solution is analyzed, and the performance overhead introduced is evaluated.", "num_citations": "8\n", "authors": ["14"]}
{"title": "Testing, reliability, and interoperability issues in the CORBA programming paradigm\n", "abstract": " CORBA (Common Object Request Broker Architecture) is widely perceived as an emerging platform for distributed systems development. We discuss CORBA's testing, reliability and interoperability issues among multiple program versions implemented by different languages (Java and C++) based on different vendor platforms (Iona Orbix and Visibroker). We engage 19 independent programming teams to develop a set of CORBA programs from the same requirement specifications, and measure the reliability of these programs. We design the required test cases and develop the operational profile of the programs. After running the test, we classify the detected faults and evaluate the reliability of the programs according to the operational profile. We also discuss how to test the CORBA programs based on their specification and interface design language (IDL). We measure the interoperability of these programs by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "Web-CASRE: A web-based tool for software reliability modeling\n", "abstract": " CASRE (Computer-Aided Software Reliability Estimation) is a software tool for software reliability measurement and evaluation. With systematic and comprehensive modeling schemes shrink-wrapped with a user-friendly interface, CASRE has drawn a significant attention as a project management tool for automatic software reliability measurement. CASRE was originally designed as a stand-alone tool running on Windows. Due to the popularity of the World Wide Web, many users demand that software tools be brought up and made available in the Web. There are challenging problems we have to solve to meet this demand. This paper discusses the architectural issues and technical decisions involved in porting a stand-alone tool so that it can run on the Web interactively. We examine the general architectures required for Web-based tools, and present our experience in bringing CASRE into the presence of the Web. The resulting software, called Web-CASRE, takes advantage of the Tcl/Tk Web browser plugin to download a Tcl script for its front-end user interface, while communicating with its back-end model solvers. The design decisions regarding communications protocol, security policy, data storage, and consistent computation environment are also discussed in this paper.", "num_citations": "8\n", "authors": ["14"]}
{"title": "Model validation using simulated data\n", "abstract": " Effective and accurate reliability modeling requires the collection of comprehensive, homogeneous, and consistent data sets. Failure data required for software reliability modeling is difficult to collect, and even the available data tends to be noisy, distorted and unpredictable. Also, the complexity of the real world data might obscure the properties of the reliability models which are based on simpler assumptions. These properties may be revealed by evaluating the models using simpler data sets. Towards this end, we have created 20 sequences of interfailure times each from five software reliability models using rate-based simulation technique, and validated the models using the simulated data sets. In this paper we describe the experimental setup, model validation results, and the lessons learned during the experiment. Having established the credibility of simulation to generate failure data, we also show how the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["14"]}
{"title": "CDS: A Cross\u0393\u00c7\u00f4Version Software Defect Prediction Model With Data Selection\n", "abstract": " Over the past decade, a large number of software defect prediction approaches have been proposed to identify the defect-prone modules by mining software repositories. Recently, a novel scenario called Cross-Version Defect Prediction (CVDP) begins to draw increasing research interests, as it is more reasonable and applicable in practice to adopt the labeled defect data of previous versions to predict defects in the current version of the same project. As a software project often has multiple previous versions, CVDP on this kind of projects will face the following two critical but seldom reported issues, namely, data distribution difference and class overlapping. In this paper, we address these two issues by solving a version selection problem via a Cross-version model with Data Selection (CDS). The proposed CDS is a novel framework which treats the defect prediction of existing and new files in different ways. For\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Infar: Insight extraction from app reviews\n", "abstract": " App reviews play an essential role for users to convey their feedback about using the app. The critical information contained in app reviews can assist app developers for maintaining and updating mobile apps. However, the noisy nature and large-quantity of daily generated app reviews make it difficult to understand essential information carried in app reviews. Several prior studies have proposed methods that can automatically classify or cluster user reviews into a few app topics (eg, security). These methods usually act on a static collection of user reviews. However, due to the dynamic nature of user feedback (ie, reviews keep coming as new users register or new app versions being released) and multiple analysis dimensions (eg, review quantity and user rating), developers still need to spend substantial effort in extracting contrastive information that can only be teased out by comparing data from multiple time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Exploring the effects of ad schemes on the performance cost of mobile phones\n", "abstract": " Advertising is an important revenue source for mobile app development, especially for free apps. However, ads also carry costs to users. Displaying ads can interfere user experience, and lead to less user retention and reduced earnings ultimately. Although there are recent studies devoted to directly mitigating ad costs, for example, by reducing the battery or memory consumed, comprehensive analysis on ad embedded schemes (eg, ad sizes and ad providers) has rarely been conducted. In this paper, we focus on analyzing three types of performance cost, ie, cost of memory/CPU, traffic, and battery. We explore 12 ad schemes used in 104 popular Android apps and compare their performance consumption. We show that the performance costs of the ad schemes we analyzed are significantly different. We also summarize the ad schemes that would generate low resource cost to users. Our summary is endorsed by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Variance-constrained resilient H\u0393\u00ea\u20a7 filtering for time-varying nonlinear networked systems subject to quantization effects\n", "abstract": " This paper deals with the resilient variance-constrained H\u0393\u00ea\u20a7 finite-horizon filtering problem for a class of discrete time-varying nonlinear networked system with quantization effects. The nonlinearity enters the system in a probabilistic way that is characterized by a binary sequence with known distribution. The system parameters under investigation are all time-varying and the randomly occurring filter gain variations are modeled by utilizing a random variable obeying prespecified binary distribution which is uncorrelated with other stochastic variables. The nonlinearities and exogenous disturbances we adopt are non-zero mean, which makes the variance analysis become more difficult. Furthermore, the quantization effects are also taken into account to describe the unavoidable constraints imposed on the signal during the transmission in networked systems. Sufficient conditions are established for the finite-horizon\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Exploiting homophily-based implicit social network to improve recommendation performance\n", "abstract": " Social information between users has been widely used to improve the traditional Recommender System in many previous works. However, in many websites such as Amazon and eBay, there is no explicit social graph that can be used to improve the recommendation performance. Hence in this work, in order to make it possible to employ social recommendation methods in those non-social information websites, we propose a general framework to construct a homophily-based implicit social network by utilizing both the rating and comments of items given by the users. Our scalable framework can be easily extended to enhance the performance of any recommender systems without social network by replacing the homophily-based implicit social relation definition. We propose four methods to extract and analyze the implicit social links between users, and then conduct the experiments on Amazon dataset\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Online learning for big data analytics\n", "abstract": " Online Learning for Big Data Page 1 Online Learning for Big Data Analytics Irwin King, Michael R. Lyu and Haiqin Yang Department of Computer Science & Engineering The Chinese University of Hong Kong Tutorial presentation at IEEE Big Data, Santa Clara, CA, 2013 1 Page 2 Outline \u0393\u00c7\u00f3 Introduction (60 min.) \u0393\u00c7\u00f4 Big data and big data analytics (30 min.) \u0393\u00c7\u00f4 Online learning and its applications (30 min.) \u0393\u00c7\u00f3 Online Learning Algorithms (60 min.) \u0393\u00c7\u00f4 Perceptron (10 min.) \u0393\u00c7\u00f4 Online non-sparse learning (10 min.) \u0393\u00c7\u00f4 Online sparse learning (20 min.) \u0393\u00c7\u00f4 Online unsupervised learning (20. min.) \u0393\u00c7\u00f3 Discussions + Q & A (5 min.) 2 Page 3 Outline \u0393\u00c7\u00f3 Introduction (60 min.) \u0393\u00c7\u00f4 Big data and big data analytics (30 min.) \u0393\u00c7\u00f4 Online learning and its applications (30 min.) \u0393\u00c7\u00f3 Online Learning Algorithms (60 min.) \u0393\u00c7\u00f4 Perceptron (10 min.) \u0393\u00c7\u00f4 Online non-sparse learning (10 min.) \u0393\u00c7\u00f4 Online sparse learning (20 min.) \u0393\u00c7\u00f4 Online unsupervised learning (20. min.) \u0393\u00c7\u00f3 + Q (..\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "T-Morph: Revealing buggy behaviors of tinyos applications via rule mining and visualization\n", "abstract": " TinyOS applications for Wireless Sensor Networks (WSNs) typically run in a complicated concurrency model. It is difficult for developers to precisely predict the dynamic execution process of a TinyOS application by its static source codes. Such a conceptual gap frequently incurs software bugs, due to unexpected system behaviors caused by unknown execution patterns. This paper presents T-Morph (TinyOS application tomography), a novel tool to mine, visualize, and verify the execution patterns of TinyOS applications. T-Morph abstracts the dynamic execution process of a TinyOS application into simple, structured application behavior models, which well reflect how the static source codes are executed. Furthermore, T-Morph visualizes them in a user-friendly manner. Therefore, WSN developers can readily see if their source codes run as intended by simply verifying the correctness of the models. Finally, the verified models allow T-Morph to automatically check the application behaviors during a long-term testing execution. The suggested model violations can unveil potential bugs and direct developers to suspicious locations in the source codes. We have implemented T-Morph and applied it to verify a series of representative real-life TinyOS applications and find several bugs, including a new bug in the latest release of TinyOS. It shows T-Morph can provide substantial help to verify TinyOS applications.", "num_citations": "7\n", "authors": ["14"]}
{"title": "A reliable and efficient MAC protocol for underwater acoustic sensor networks\n", "abstract": " Underwater acoustic sensor networks (UWASNs) are playing a key role in ocean applications. Unfortunately, the efficiency of UWASNs is inferior to that of the terrestrial sensor networks (TWSNs). The main reasons are as follows: (1) UWASNs suffer long propagation delay; (2) UWASNs are limited by the narrow bandwidth. Many MAC protocols are proposed to improve the efficiency of UWASNs. However, their improvement is not enough. Moreover, few of them consider the reliability of UWASNs even though the packet loss can fail the applications. Actually, a few of the protocols employ the traditional acknowledgment (ACK) mechanism, but they suffer the throughput degradation a lot. In this paper, first, we propose a protocol called RAS, a priority scheduling approach for multihop topologies. RAS is more efficient in throughput and delay performance. Then, we propose a reliable RAS called RRAS that obtains a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "Topological dynamics for multidimensional perturbations of maps with covering relations and Liapunov condition\n", "abstract": " In this paper, we study topological dynamics of high-dimensional systems which are perturbed from a continuous map on R m\u251c\u00f9 R k of the form (f (x), g (x, y)). Assume that f has covering relations determined by a transition matrix A. If g is locally trapping, we show that any small C 0 perturbed system has a compact positively invariant set restricted to which the system is topologically semi-conjugate to the one-sided subshift of finite type induced by A. In addition, if the covering relations satisfy a strong Liapunov condition and g is a contraction, we show that any small C 1 perturbed homeomorphism has a compact invariant set restricted to which the system is topologically conjugate to the two-sided subshift of finite type induced by A. Some other results about multidimensional perturbations of f are also obtained. The strong Liapunov condition for covering relations is adapted with modification from the cone condition in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "LOFT: a latency-oriented fault tolerant transport protocol for wireless sensor-actuator networks\n", "abstract": " Wireless sensor-actuator networks, or WSANs, refer to a group of sensors and actuators which collect data from the environment and perform application-specific actions in response. To act responsively and accurately, an efficient and reliable data transport protocol is crucial for the sensors to inform the actuators about the environmental events. Unfortunately, the low-power multi-hop communications in WSANs are inherently unreliable; the frequent sensor and link failures as well as the excessive delays due to congestion further aggravate the problem. In this paper, we propose a latency-oriented fault tolerant data transport protocol in WSANs. We argue that reliable data transport in such a real-time system should resist to the transmission failures, and should also consider the importance and freshness of the reported data. We articulate this argument and provide a cross-layer two-step data transport protocol for on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "A unified learning paradigm for large-scale personalized information management\n", "abstract": " Statistical-learning approaches such as unsupervised learning, supervised learning, active learning, and reinforcement learning have generally been separately studied and applied to solve application problems. In this paper, we provide an overview of our newly proposed unified learning paradigm (ULP), which combines these approaches into one synergistic framework. We outline the architecture and the algorithm of ULP, and explain benefits of employing this unified learning paradigm on personalizing information management.", "num_citations": "7\n", "authors": ["14"]}
{"title": "XVIP: an XML-based video information processing system\n", "abstract": " We describe XVIP, an XML-based video information processing system, which extracts information from video and stores the information in a multimedia digital video library. XVIP encapsulates a number of extraction techniques, including scene change detection, video optical character detection and recognition, and geometric coding. It also provides a seamless approach to scale up the contents created in and delivered by the target multimedia digital video library. Furthermore, XVIP can handle multilingual contents. XVIP is based on a multi-modal concept, which treats each content extraction component as a modality, and users can easily add new modalities into XVIP. The information extracted from the video is then stored in a flexible, scalable and reusable way based on a generic XML structure, providing a convenient mechanism for data representation on Web browsers. Also, the content in the XML file can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["14"]}
{"title": "iview: An intelligent video over internet and wireless access system\n", "abstract": " We describe the design and implementation of a digital video content management system, iVIEW, for intelligent searching and access of video contents over Internet and wireless devices. The iVIEW system allows full content indexing, searching and retrieval of multilingual text, audio and video material. iVIEW integrates image processing techniques for scenes and scene changes analyses, speech processing techniques for audio signal transcriptions, and multilingual natural language processing techniques for word relevance determination. iVIEW is composed of three subsystems: Video Information Processing (VIP) Subsystem for multimodal information processing of rich video media, Searching and Indexing Subsystem for constructing XML-based multimedia representation in enhancing multimodal indexing and searching capabilities, and Visualization and Presentation Subsystem for flexible and seamless delivery of multimedia contents in various browsing tools and devices. We present overall and detailed infrastructure of iVIEW, describe its system characteristics, and evaluate the customer view on its performance and functionality. Integrating image, speech, and natural language processing techniques into Web-based environments for friendly user interface and seamless browsing, iVIEW provides a unified, end-to-end management of video-based media contents, from their creation to delivery, over WWW and mobile Web designed for today and tomorrow.", "num_citations": "7\n", "authors": ["14"]}
{"title": "Towards intelligent incident management: why we need it and how we make it\n", "abstract": " The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Multi-task learning with shared encoder for non-autoregressive machine translation\n", "abstract": " Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties and representations of source sentences. Therefore, we propose to adopt the multi-task learning to transfer the AT knowledge to NAT models through the encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English->German and WMT16 English->Romanian datasets show that the proposed multi-task NAT achieves significant improvements over the baseline NAT models. In addition, experimental results demonstrate that our multi-task NAT is complementary to the standard knowledge transfer method, knowledge distillation. Code is publicly available at https://github.com/yongchanghao/multi-task-nat", "num_citations": "6\n", "authors": ["14"]}
{"title": "Personalized sequential check-in prediction: Beyond geographical and temporal contexts\n", "abstract": " Check-in prediction is an important task for location-based systems, which maps a noisy estimate of a user's current location to a semantically meaningful point-of-interest (POI), such as a restaurant or store. In this paper, we leverage the personalized preference and sequential check-in pattern to improve the traditional methods that base on the geographical and temporal contexts. In our approach, we propose a Gaussian mixture model and a histogram distribution estimation model to learn the contextual features from relevant spatial and temporal information, respectively. Furthermore, we employ user and POI embeddings to model the personalized preference and leverage a stacked Long-Short Term Memory (LSTM) model to learn the sequential check-in pattern. Combining the contextual features and the personalized sequential patterns together, we propose a wide and deep neural network for the check-in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Sparse Poisson coding for high dimensional document clustering\n", "abstract": " Document clustering plays an important role in large scale textual data analysis, which generally faces with great challenge of the high dimensional textual data. One remedy is to learn the high-level sparse representation by the sparse coding techniques. In contrast to traditional Gaussian noise-based sparse coding methods, in this paper, we employ a Poisson distribution model to represent the word-count frequency feature of a text for sparse coding. Moreover, a novel sparse-constrained Poisson regression algorithm is proposed to solve the induced optimization problem. Different from previous Poisson regression with the family of \u0393\u00e4\u00f4 1 -regularization to enhance the sparse solution, we introduce a sparsity ratio measure which make use of both \u0393\u00e4\u00f4 1 -norm and \u0393\u00e4\u00f4 2 -norm on the learned weight. An important advantage of the sparsity ratio is that it bounded in the range of 0 and 1. This makes it easy to set for practical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Towards a top-down and bottom-up bidirectional approach to joint information extraction\n", "abstract": " Most high-level information extraction (IE) consists of compound and aggregated subtasks. Such IE problems are generally challenging and they have generated increasing interest recently. We investigate two representative IE tasks:(1) entity identification and relation extraction from Wikipedia, and (2) citation matching, and we formally define joint optimization of information extraction. We propose a joint paradigm integrating three factors--segmentation, relation, and segmentation-relation joint factors, to solve all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting bidirectional rich dependencies and interactions between relevant subtasks to capture mutual benefits. Since exact parameter estimation is prohibitively intractable, we present a general, highly-coupled learning algorithm based on variational expectation maximization (VEM) to perform parameter estimation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Developing Aerospace Applications with a ReliableWeb Services Paradigm\n", "abstract": " One of the latest achievements of the Internet usage is the availability of Web services technology. Web services provide an efficient and convenient way for service provisioning, exchanging and aggregating, which facilitates a resourceful platform for the aerospace industry. The aerospace industry usually involves products of complex synthesis of various technologies and sciences. These different technical resources can be provided in the form of Web services to increase their availability, efficiency and performance. However, in aerospace area, reliability is an ultimately important issue. In this paper, we target on providing a reliable Web service paradigm for the industry. We describe the methods of reliability enhancement by redundancy in space and redundancy in time, identify parameters impacting Web service reliability, and present a Web service composition algorithm. The replication algorithm and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Graphic object recognition from binary images: a survey and an integrated paradigm\n", "abstract": " Abstract\u0393\u00ea\u00c6 Recognizing graphic objects from binary images is a prerequisite in many real-life graphicsprocessing applications that take binary images as input. Generally, there are two ways to perform the graphic object recognition: direct-recognition paradigm and vectorization-based paradigm. The former recognizes graphic objects from binary images directly, while the latter first converts the raster image into a low-level vector format, and then recognizes graphic objects from these vectors. However, both paradigms suffer from their own unsolved difficulties. This paper first reviews the popular graphic object recognition methods of each paradigm to realize their advantages and disadvantages. The performance comparison between these two classes of methods is made in terms of the time efficiency and the quality of graphics result. The experimental result of comparing the time efficiency of seven popular methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "A generic environment for COTS testing and quality prediction\n", "abstract": " In this chapter, we first survey current component technologies and discuss the features they inherit. Quality assurance (QA) characteristics of component systems and the life cycle of component-based software development (CBSD) are also addressed. Based on the characteristics of the life cycle, we propose a QA model for CBSD. The model covers the eight main processes in component-based software systems (CBS) development. A Component-based Program Analysis and Reliability Evaluation (ComPARE) environment is established for evaluation and prediction of quality of components. ComPARE provides a systematic procedure for predicting the quality of software components and assessing the reliability of the final system developed using CBSD. Using different quality prediction techniques, ComPARE has been applied to a number of component-based programs. The prediction results and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "A new software testing approach based on domain analysis of specifications and programs\n", "abstract": " Partition testing is a well-known software testing technique. This paper shows that partition testing strategies are relatively ineffective in detecting faults related to small shifts in input domain boundary. We present an innovative software testing approach based on input domain analysis of specifications and programs, and propose the principle and procedure of boundary test case selection in functional domain and operational domain. The differences of the two domains are examined by analyzing the set of their boundary test cases. To automatically determine the operational domain of a program, the ADSOD system is prototyped. The system supports not only the determination of input domain of integer and real data types, but also non-numeric data types such as characters and enumerated types. It consists of several modules in finding illegal values of input variables with respect to specific expressions. We apply\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["14"]}
{"title": "Speech Recognition Techniques for Digital Video Library\n", "abstract": " Automatic Speech Recognition (ASR) is the process by which computer (or other type of machine) identifies spoken words and recognize what you are saying. It has been an interest of research by nearly four decades. It is amazing because it has a wide area of applications. If eventually computer can get 100 percent of recognizing words, it will bloom a new era of computer technology and even change people, s daily life. However, there are many difficulties to overcome and speech recognition still has a long way to go.", "num_citations": "6\n", "authors": ["14"]}
{"title": "Integrating digital libraries by CORBA, XML and Servlet\n", "abstract": " In this paper, we describe how we use a mediator-based architecture for integrating digital libraries. We discuss how we tackle the obstacles of firewalls in the expansion of our system by using XML and Java Servlet, which are used to achieve CORBA general communications and callback features across the firewalls.", "num_citations": "6\n", "authors": ["14"]}
{"title": "Why an Android app is classified as malware? Towards malware classification interpretation\n", "abstract": " Machine learning (ML) based approach is considered as one of the most promising techniques for Android malware detection and has achieved high accuracy by leveraging commonly-used features. In practice, most of the ML classifications only provide a binary label to mobile users and app security analysts. However, stakeholders are more interested in the reason why apps are classified as malicious in both academia and industry. This belongs to the research area of interpretable ML but in a specific research domain (i.e., mobile malware detection). Although several interpretable ML methods have been exhibited to explain the final classification results in many cutting-edge Artificial Intelligent (AI) based research fields, till now, there is no study interpreting why an app is classified as malware or unveiling the domain-specific challenges. In this paper, to fill this gap, we propose a novel and interpretable ML-based approach (named XMal) to classify malware with high accuracy and explain the classification result meanwhile. (1) The first classification phase of XMal hinges multi-layer perceptron (MLP) and attention mechanism, and also pinpoints the key features most related to the classification result. (2) The second interpreting phase aims at automatically producing neural language descriptions to interpret the core malicious behaviors within apps. We evaluate the behavior description results by comparing with the existing interpretable ML-based methods (i.e., Drebin and LIME) to demonstrate the effectiveness of XMal. We find that XMal is able to reveal the malicious behaviors more accurately. Additionally, our experiments show that XMal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Geo-teaser: Geo-temporal sequential embedding rank for POI recommendation\n", "abstract": " This chapter proposes a Geo-Temporal sequential embedding rank (Geo-Teaser) model for POI recommendation. Inspired by the success of the word2vec framework to model the sequential contexts, a temporal POI embedding model is proposed to learn POI representations under some particular temporal state. The temporal POI embedding model captures the contextual check-in information in sequences and the various temporal characteristics on different days as well. Furthermore, a new way of incorporating the geographical influence into the pairwise preference ranking method through discriminating the unvisited POIs according to geographical information, is employed to develop a geographically hierarchical pairwise preference ranking model. Finally, a unified framework is proposed to recommend POIs combining these two models. Experimental results on two real-life datasets show that the Geo\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "A data set for user request trace-oriented monitoring and its applications\n", "abstract": " User request trace-oriented monitoring is an effective method to improve the reliability of cloud services. However, there are some difficulties in getting useful traces in practice, which hinder the development of trace-oriented monitoring research. In this paper, we release a fine-grained user request-centric open trace data set, called TraceBench, which is collected in a real-world cloud storage service deployed in a real environment. When collecting, we consider different scenarios, involving multiple scales of clusters, different kinds of user requests, various speeds of workloads, many types of injected faults, etc. To validate the usability and authenticity, we have employed TraceBench in several trace-oriented monitoring topics, such as anomaly detection, performance problem diagnosis, and temporal invariant mining. The results show that TraceBench well supports these research topics. In addition, we have also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Guest editorial: Recommendation techniques for services computing and cloud computing\n", "abstract": " As the number of Web services surges rapidly, recommending the right service for various users on demand has become one of the most challenging research issues in the fields of services computing and cloud computing. This special issue focuses on the recommendation techniques for services computing and cloud computing including classic recommendation algorithms for services computing and cloud computing, emerging recommendation techniques for service selection and composition, and applications of recommendation techniques in composition of complex service mashups, ad-hoc social networks, and green cloud environment. The papers introduced in this special issue illustrate the efficiency and effectiveness of services computing and cloud computing, while demonstrating a variety of challenges that arise. It is expected that this special issue, as a whole, will provide integrated and synthesized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Building Reliable Web Services: Methodology, Composition, Modeling and Experiment\n", "abstract": " One of the latest achievements of the Internet usage is the availability of Web services technology and its dependability is becoming one of the most critical goals in Web related research. In this thesis, we propose a design paradigm for reliable Web services and a Web service composition algorithm. We describe the methods of dependability enhancement by redundancy in space and redundancy in time, using Round-robin scheduling technique, N-version programming and recovery block. The Web services are coordinated by a replication manager. It provides a Round-robin algorithm for scheduling the workload of the Web services and keeps updating the availability of each Web service. The replication algorithm and the detailed system configuration are described. In the paradigm, N-version programming technique is applied to increase the diversity of the system. As different versions of Web services or even different versions of their components are abundantly", "num_citations": "5\n", "authors": ["14"]}
{"title": "Maximum margin based semi-supervised spectral kernel learning\n", "abstract": " Semi-supervised kernel learning is attracting increasing research interests recently. It works by learning an embedding of data from the input space to a Hilbert space using both labeled data and unlabeled data, and then searching for relations among the embedded data points. One of the most well-known semi-supervised kernel learning approaches is the spectral kernel learning methodology which usually tunes the spectral empirically or through optimizing some generalized performance measures. However, the kernel designing process does not involve the bias of a kernel-based learning algorithm, the deduced kernel matrix cannot necessarily facilitate a specific learning algorithm. To supplement the spectral kernel learning methods, this paper proposes a novel approach, which not only learns a kernel matrix by maximizing another generalized performance measure, the margin between two classes of data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "A multi-scale Tikhonov regularization scheme for implicit surface modelling\n", "abstract": " Kernel machines have recently been considered as a promising solution for implicit surface modelling. A key challenge of machine learning solutions is how to fit implicit shape models from large-scale sets of point cloud samples efficiently. In this paper, we propose a fast solution for approximating implicit surfaces based on a multi-scale Tikhonov regularization scheme. The optimization of our scheme is formulated into a sparse linear equation system, which can be efficiently solved by factorization methods. Different from traditional approaches, our scheme does not employ auxiliary off-surface points, which not only saves the computational cost but also avoids the problem of injected noise. To further speedup our solution, we present a multi-scale surface fitting algorithm of coarse to fine modelling. We conduct comprehensive experiments to evaluate the performance of our solution on a number of datasets of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Fault\u0393\u00c7\u00c9Tolerant Software\n", "abstract": " Fault tolerance is the survival attribute of a system or component to continue operating as required despite the manifestation of hardware or software faults. Fault\u0393\u00c7\u00c9tolerant software delivers continuous service complying with the relevant specification in the presence of faults typically by employing either single\u0393\u00c7\u00c9version software techniques or multiple\u0393\u00c7\u00c9version software techniques. We address four key perspectives for fault\u0393\u00c7\u00c9tolerant software: historical background, techniques, modeling schemes, and applications.", "num_citations": "5\n", "authors": ["14"]}
{"title": "An energy-efficient mechanism for self-monitoring sensor web\n", "abstract": " A Sensor Web is a network of spatially distributed sensor platforms, which is especially well suited for environmental monitoring. Although sensor nodes of a Sensor Web are critical devices that perform the monitoring work, the low-cost implementation of sensor nodes poses that they are subject to failures and permanent damage. A life-condition monitoring mechanism for sensor nodes is therefore required to ensure the function of a Sensor Web. This paper studies this sensor-node monitoring problem in the domain where in-network sensor nodes are self-monitoring, i.e., the status of each sensor node is monitored by another node. To be energy-efficient, a mechanism for implementing self-monitoring Sensor Webs should minimize the energy required. We propose a formal formulation of this problem and show that it can be solved by finding a minimum spanning tree of the graph constructed by in-network nodes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Adaptive nearest neighbor classifier based on supervised ellipsoid clustering\n", "abstract": " Nearest neighbor classifier is a widely-used effective method for multi-class problems. However, it suffers from the problem of the curse of dimensionality in high dimensional space. To solve this problem, many adaptive nearest neighbor classifiers were proposed. In this paper, a locally adaptive nearest neighbor classification method based on supervised learning style which works well for the multi-classification problems is proposed. In this method, the ellipsoid clustering learning is applied to estimate an effective metric. This metric is then used in the K-NN classification. Finally, the experimental results show that it is an efficient and robust approach for multi-classification.", "num_citations": "5\n", "authors": ["14"]}
{"title": "How should software reliability engineering (SRE) be taught?\n", "abstract": " This article on teaching software reliability engineering (SRE) represents a consensus of views of experienced software reliability engineering leaders from diverse backgrounds but with ties to education: directors of software reliability and software reliability training in industry, a consultant who teaches SRE practice to industry, and university professors. The first topic covered is how to attract participants to SRE courses. We then analyze the job-related educational needs of current and future (those now university students) software practitioners, SRE practitioners, researchers, and nonsoftware professionals. Special needs relating to backgrounds, limited proficiency in the course language, and work conflicts are outlined. We discuss how the needs presented should influence course content and structure, teaching methods, and teaching materials. Finally, we cover our experiences with distance learning and its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Nonlinear blind source separation using hybrid neural networks\n", "abstract": " This paper proposes a novel algorithm based on minimizing mutual information for a special case of nonlinear blind source separation: post-nonlinear blind source separation. A network composed of a set of radial basis function (RBF) networks, a set of multilayer perceptron and a linear network is used as a demixing system to separate sources in post-nonlinear mixtures. The experimental results show that our proposed method is effective, and they also show that the local character of the RBF network\u0393\u00c7\u00d6s units allows a significant speedup in the training of the system.", "num_citations": "5\n", "authors": ["14"]}
{"title": "Network Analysis of the Protein Chain Tertiary Structures of Heterocomplexes+\n", "abstract": " In this paper, the tertiary structures of protein chains of heterocomplexes were mapped to 2D networks; based on the mapping approach, statistical properties of these networks were systematically studied. Firstly, our experimental results confirmed that the networks derived from protein structures possess small-world properties. Secondly, an interesting relationship between network average degree and the network size was discovered, which was quantified as an empirical function enabling us to estimate the number of residue contacts of the protein chains accurately. Thirdly, by analyzing the average clustering coefficient for nodes having the same degree in the network, it was found that the architectures of the networks and protein structures analyzed are hierarchically organized. Finally, network motifs were detected in the networks which are believed to determine the family or superfamily the networks belong to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Improving naive Bayesian classifier by discriminative training\n", "abstract": " Discriminative classifiers such as Support Vector Machines (SVM) directly learn a discriminant function or a posterior probability model to perform classification. On the other hand, generative classifiers often learn a joint probability model and then use the Bayes rule to construct a posterior classifier. In general, generative classifiers are not as accurate as discriminative classifiers. However generative classifiers provide a principled way to deal with the missing information problem, which discriminative classifiers cannot easily handle. To achieve good performance in various classification tasks, it is better to combine these two strategies. In this paper, we develop a method to train one of the popular generative classifiers, the Naive Bayesian classifier (NB) in a discriminative way. We name this new model as the Discriminative Naive Bayesian classifier. We provide theoretic justifications, outline the algorithm, and perform a serious of experiments on benchmark real-world datasets to demonstrate our model\u0393\u00c7\u00d6s advantages. Its performance outperforms NB in classification tasks and outperforms SVM in handling missing information tasks.", "num_citations": "5\n", "authors": ["14"]}
{"title": "NHDC and PHDC: Non-propagating and propagating heat diffusion classifiers\n", "abstract": " By imitating the way that heat flows in a medium with a geometric structure, we propose two novel classification algorithms, Non-propagating Heat Diffusion Classifier (NHDC) and Propagating Heat Diffusion Classifier (PHDC). In NHDC, an unlabelled data is classified into the class that diffuses the most heat to the unlabelled data after one local diffusion from time 0 to a small time period, while in PHDC, an unlabelled data is classified into the class that diffuses the most heat to the unlabelled data in the propagating effect of the heat flow from time 0 to time t, which means that in PHDC, the heat diffuses infinitely many times from time 0 and each time period is infinitely small. In other words, we measure the similarity between an unlabelled data and a class by the heat amount that the unlabelled data receives from the set of labelled data in the class, and then classify the unlabelled data into the class with the most similarity. Unlike the traditional method, in which the heat kernel is applied to a kernel-based classifier we employ the heat kernel to construct the classifier directly; moreover, instead of imitating the way that the heat flows along a linear or nonlinear manifold, we let the heat flow along a graph formed by the k-nearest neighbors. An important and special feature in both NHDC and PHDC is that the kernel is not symmetric. We show theoretically that PWA (Parzen Window Approach when the window function is a multivariate normal kernel) and KNN are actually special cases of NHDC model, and that PHDC has the ability to approximate NHDC. Experiments show that NHDC performs better than PWA and KNN in prediction accuracy, and that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["14"]}
{"title": "Automatic generation of dubbing video slides for mobile wireless environment\n", "abstract": " Mobile wireless video delivery is still challenging due to its limited bandwidth and dynamic channel status. In this paper, a novel approach named dubbing video slides (DVS) is proposed to cope with the bandwidth limitation problem. Based on a statistical video content importance analysis, DVS method can dynamically select and transmit representative video frames which are relatively more important, and discard others according to current network status feedback. To save bandwidth, we can use these representative frames as substitutes for those adjacent video intervals and synchronize with the original audio track during playback. The visual simulation shows DVS works well for video summary in mobile network delivery.", "num_citations": "5\n", "authors": ["14"]}
{"title": "Securing mobile agents for electronic commerce: an experiment\n", "abstract": " Mobile software agents are becoming a major trend of distributed systems in the next decade. Electronic commerce and information retrieval are two prospective applications of mobile agents. Nevertheless, security is a crucial concern for such systems. Attacks to agents by malicious hosts are the most challenging part of the problem unsolved. In this paper, a Shopping Information Agent System (SIAS) is built based on mobile agent technology. Possible security attacks by malicious hosts to agents in the system are discussed, and solutions to prevent these attacks are presented. Security of the solutions is analysed, and the performance overhead introduced is evaluated.", "num_citations": "5\n", "authors": ["14"]}
{"title": "Visual Techniques for Parallel Processing\n", "abstract": " This Comprehensive Examination consists of an accumulation and analysis of research on the use of visualization in computing systems over the past decade, as well as recent efforts specifically in the area of software development for parallel processing. The goal of the examination is to determine the relationships among the references located, and their cumulative effect in directing the course of future research in the field of visualization. The examination includes a creative portion in which the various uses and approaches for visualization are to be classified via a taxonomical system. This classification will identify the central issues which differentiate the visualization environments for developing parallel software. In addition, a quantitative assessment of these environments will be constructed which presents a more concrete evaluation and categorization technique. The following sections constitute my solution to the examination in the following manner. Section II describes briefly the broad literature survey, including the approach to the search and a summary of the literature references.Section III presents a detailed analysis and summary of recent work on selected visualization tools and environments for parallel software development. These tools and environments will be analyzed with respect to their range of utility or capabilities, as well as cost-effectiveness or usefulness.", "num_citations": "5\n", "authors": ["14"]}
{"title": "Textout: Detecting Text-Layout Bugs in Mobile Apps via Visualization-Oriented Learning\n", "abstract": " Layout bugs commonly exist in mobile apps. Due to the fragmentation issues of smartphones, a layout bug may occur only on particular versions of smartphones. It is quite challenging to detect such bugs for state-of-the-art commercial automated testing platforms, although they can test an app with thousands of different smartphones in parallel. The main reason is that typical layout bugs neither crash an app nor generate any error messages. In this paper, we present our work for detecting text-layout bugs, which account for a large portion of layout bugs. We model text-layout bug detection as a classification problem. This then allows us to address it with sophisticated image processing and machine learning techniques. To this end, we propose an approach which we call Textout. Textout takes screenshots as its input and adopts a specifically-tailored text detection method and a convolutional neural network (CNN\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Making online sketching hashing even faster\n", "abstract": " Data-dependent hashing methods have demonstrated good performance in various machine learning applications to learna low-dimensional representation from the original data. However, they still meet several obstacles: First, most of existing hashingmethods are trained in a batch mode, yielding inefficiency for training streaming data. Second, the computational cost and the memoryconsumption increase extraordinarily in the big data era, which hinders the training procedure. Third, the scarcity of label informationhinders the improvement of the model performance. To address these difficulties, we utilize online sketching hashing (OSH) andpresent a FasteR Online Sketching Hashing (FROSH) algorithm to sketch the data in a more compact form via an independenttransformation. Theoretical justification is provided to guarantee that our proposed FROSH consumes less time and achieves acomparable sketching\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Risk control of best arm identification in multi-armed bandits via successive rejects\n", "abstract": " Best arm identification in stochastic Multi-Armed Bandits (MAB) has become an essential variant in the research line of bandits for decision-making problems. In previous work, the best arm usually refers to an arm with the highest expected payoff in a given decision-arm set. However, in many practical scenarios, it would be more important and desirable to incorporate the risk of an arm into the best decision. In this paper, motivated by practical applications with risk via bandits, we investigate the problem of Risk Control of Best Arm Identification (RCBAI) in stochastic MAB. Based on the technique of Successive Rejects (SR), we show that the error resulting from the mean-variance estimation is sub-Gamma by setting mild assumptions on stochastic payoffs of arms. Besides, we develop an algorithm named as RCMAB. SR, and derive an upper bound for the probability of error for RCBAI in stochastic MAB. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "QoS-aware Byzantine fault tolerance\n", "abstract": " Cloud computing is becoming a popular and important solution for building highly reliable applications on distributed resources. However, it is a critical challenge to guarantee the system reliability of applications especially in voluntary-resource cloud due to the highly dynamic environment. In this chapter, we present Byzantine fault-tolerant cloud (BFTCloud ), a Byzantine fault tolerance framework for building robust systems in voluntary-resource cloud environments. BFTCloud guarantees robustness of systems when up to f of totally 3f + 1 resource providers are faulty, including crash faults and arbitrary behaviors faults. BFTCloud is evaluated in a large-scale real-world experiment which consists of 257 voluntary-resource providers located in 26 countries. The experimental results show that BFTCloud guarantees high reliability of systems built on the top of voluntary-resource cloud infrastructure and ensure\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Assessing the security properties of software obfuscation\n", "abstract": " The battle between software obfuscation and reverse engineering has been ongoing for decades. Yet the degree of security that obfuscation actually guarantees remains unclear. Obfuscation's security properties and challenges to achieving a valid security property are evaluated.", "num_citations": "4\n", "authors": ["14"]}
{"title": "Trace bench: An open data set for trace-oriented monitoring\n", "abstract": " User request trace-oriented monitoring is an effective method to improve the reliability of cloud systems. However, there are some difficulties in getting traces in practice, which hinder the development of trace-oriented monitoring research. In this paper, we release a fine-grained user request-centric open trace data set, called Trace Bench, collected on a real world cloud storage system deployed in a real environment. During collecting, many aspects are considered to simulate different scenarios, including cluster size, request type, workload speed, etc. Besides recording the traces when the monitored system is running normally, we also collect the traces under the situation with faults injected. With a mature injection tool, 14 faults are introduced, including function faults and performance faults. The traces in Trace Bench are clustered in different files, where each file corresponds to a certain scenario. The whole\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Fault detection for wireless network control systems with stochastic uncertainties and time delays\n", "abstract": " The fault detection problem is investigated for a class of wireless network control systems which has stochastic uncertainties in the state-space matrices, combined with time delays and nonlinear disturbance. First, the system error observer is proposed. Then, by constructing proper Lyapunov-Krasovskii functional, we acquire sufficient conditions to guarantee the stability of the fault detection observer for the discrete system, and observer gain is also derived by solving linear matrix inequalities. Finally, a simulation example shows that when a fault happens, the observer residual rises rapidly and fault can be quickly detected, which demonstrates the effectiveness of the proposed method.", "num_citations": "4\n", "authors": ["14"]}
{"title": "Fault detection for network control systems with multiple communication delays and stochastic missing measurements\n", "abstract": " This paper is concerned with fault detection problem for a class of network control systems (NCSs) with multiple communication delays and stochastic missing measurements. The missing measurement phenomenon occurs in a random way and the occurrence probability for each measurement output is governed by an individual random variable. Besides, the multiple communication delay phenomenon reflects that networked control systems have different communication delays when the signals are transferred via different channels. We aim to design a fault detection filter so that the overall fault detection dynamics is exponentially stable in the mean square. By constructing proper Lyapunov-Krasovskii functional, we acquire sufficient conditions to guarantee the stability of the fault detection filter for the discrete systems, and the filter parameters are also derived by solving linear matrix inequality. Finally, an illustrative example is provided to show the usefulness and effectiveness of the proposed design method.", "num_citations": "4\n", "authors": ["14"]}
{"title": "MDiag: Mobility-assisted diagnosis for wireless sensor networks\n", "abstract": " Though widely employed in various applications, wireless sensor networks (WSNs) are liable to failures, especially after deployment. Since the on-site failures are difficult to reproduce, it is of critical importance to perform in-situ diagnosis. Current in-situ diagnosis methods are either intrusive or inefficient, because they either inject diagnosis agents into each sensor node or build up another network for diagnosis purpose. To tackle these issues, we propose MDiag, a mobility-assisted diagnosis approach that employs smartphones to patrol the WSNs and diagnose failures. Diagnosing with a smartphone which is not a component of WSNs does not intrude the execution of the WSNs. Moreover, patrolling the smartphone in the WSNs to investigate failures is more efficient than deploying another diagnosis network. Statistical rules are designed to guide the detection of abnormal cases. Aiming at improving the patrol\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Cmap: effective fusion of quality and relevance for multi-criteria recommendation\n", "abstract": " The research issue of recommender systems has been treated as a classical regression problem over the decades and has obtained a great success. In the next generation of recommender systems, multi-criteria recommendation has been predicted as an important direction. Different from traditional recommender systems that aim particularly at recommending high-quality items evaluated by users' ratings, inmulti-criteria recommendation, quality only serves as one criterion, and many other criteria such as relevance, coverage, and diversity should be simultaneously optimized. Although recently there is work investigating each single criterion, there is rarely any literature that reports how each single criterion impacts each other and how to combine them in real applications. Thus in this paper, we study the relationship of two criteria, quality and relevance, as a preliminary work in multi-criteria recommendation. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "A volume-based heat-diffusion classifier\n", "abstract": " Heat-diffusion models have been successfully applied to various domains such as classification and dimensionality-reduction tasks in manifold learning. One critical local approximation technique is employed to weigh the edges in the graph constructed from data points. This approximation technique is based on an implicit assumption that the data are distributed evenly. However, this assumption is not valid in most cases, so the approximation is not accurate in these cases. To solve this challenging problem, we propose a volume-based heat-diffusion model (VHDM). In VHDM, the volume is theoretically justified by handling the input data that are unevenly distributed on an unknown manifold. We also propose a novel volume-based heat-diffusion classifier (VHDC) based on VHDM. One of the advantages of VHDC is that the computational complexity is linear on the number of edges given a constructed graph\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Efficient minimax clustering probability machine by generalized probability product kernel\n", "abstract": " Minimax Probability Machine (MPM), learning a decision function by minimizing the maximum probability of misclassification, has demonstrated very promising performance in classification and regression. However, MPM is often challenged for its slow training and test procedures. Aiming to solve this problem, we propose an efficient model named Minimax Clustering Probability Machine (MCPM). Following many traditional methods, we represent training data points by several clusters. Different from these methods, a Generalized Probability Product Kernel is appropriately defined to grasp the inner distributional information over the clusters. Incorporating clustering information via a non-linear kernel, MCPM can fast train and test in classification problem with promising performance. Another appealing property of the proposed approach is that MCPM can still derive an explicit worst-case accuracy bound for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "A survey of fault tolerant CORBA systems\n", "abstract": " CORBA is an OMG standard for distributed object computing; but despite being a standard and wide scale acceptance in the industry it lacks the ability to meet high demands of quality of service (QoS) required for building a reliable fault tolerant distributed system. To tackle these issues, in 2001, OMG incorporated fault tolerance mechanisms, QoS policies and services in its standard interfaces as mentioned in its Fault Tolerant CORBA (FT-CORBA) specification. FT-CORBA Architecture used the notion of object replication to provide reliable and fault tolerant services. In this paper, we surveyed the different approaches for building FT-CORBA based distributed systems with their merits and limitations. We gave an overview of FT-CORBA specification; its requirements and limitations, and FT-CORBA Architecture. We have also revised the existing categorization of FT-CORBA systems by incorporating a fourth\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "A new learning algorithm for function approximation incorporating a priori information into extreme learning machine\n", "abstract": " In this paper, a new algorithm for function approximation is proposed to obtain better generalization performance and faster convergent rate. The new algorithm incorporates the architectural constraints from a priori information of the function approximation problem into Extreme Learning Machine. On one hand, according to Taylor theorem, the activation functions of the hidden neurons in this algorithm are polynomial functions. On the other hand, Extreme Learning Machine is adopted which analytically determines the output weights of single-hidden layer FNN. In theory, the new algorithm tends to provide the best generalization at extremely fast learning speed. Finally, several experimental results are given to verify the efficiency and effectiveness of our proposed learning algorithm.", "num_citations": "4\n", "authors": ["14"]}
{"title": "Improving Chow-Liu tree performance by mining association rules\n", "abstract": " We present a novel approach to construct a kind of tree belief network, in which the \u0393\u00c7\u00a3nodes\u0393\u00c7\u00a5 are subsets of variables of dataset. We call this model Large Node Chow-Liu Tree (LNCLT). This technique uses the concept of the association rule as found in the database literature to guide the construction of the LNCLT. Similar to the Chow-Liu Tree (CLT), the LNCLT is also ideal for density estimation and classification applications. More importantly, our novel model partially solves the disadvantages of the CLT, i.e., the inability to represent non-tree structures, and is shown to be superior to the CLT theoretically. Moreover, based on the MNIST hand-printed digit database, we conduct a series of digit recognition experiments to verify our approach. From the result we find that both the approximation accuracy and the recognition rate on the data are improved with the LNCLT structure, when compared with the CLT.", "num_citations": "4\n", "authors": ["14"]}
{"title": "A generic color-distribution-based approach to detect edges, corners and junctions simultaneously in color images, submitted to\n", "abstract": " Low-level feature detection from color images is crucial to computer vision tasks. Existing methods have at least one of the following four disadvantages:(1) requiring different passes or methods in detecting edges, corners and junctions;(2) unable to handle textured regions;(3) unable to detect the boundary direction of corners and junctions; and (4) holding an assumption on the junction shape. We believe that the pixel type should be determined only by the color distribution in its neighborhood. Unlike the previous methods which only focus on the statistic color distribution, we emphasize both spatial and statistic color distributions. Accordingly, a generic neighborhood model is proposed to classify edges, corners and junctions clearly by their color distributions. Based on this model, we propose a low-level feature detection approach which avoids these four disadvantages and is adjustable to the application\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Web-based education techniques: Workflow, collaboration, and quality of service\n", "abstract": " In this paper we describe and evaluate a set of Web-based and workflow-sensitive educational techniques that can increase the effectiveness of teaching, collaboration, and resource sharing. These techniques allow educators to facilitate the goal of developing high quality teaching and learning, using the fast growing Internet technologies. We construct a system that supports implementation of these techniques in a broad range of institutions of higher learning. The system operates in the context of an efficient state-of-the-art, network-based engine that supports advanced virtual laboratory concepts, and collaborative content capture, development and delivery mechanisms. This support system, called\" Multimedia Web-Presentation System\", is easy-to-use, adaptable to user workflow, profiles, and quality of service needs, and is affordable for wide distribution and adoption. We also discuss Web-based education issues and describe how our system can be used to address these issues.", "num_citations": "4\n", "authors": ["14"]}
{"title": "A generalized software reliability process simulation technique and tool\n", "abstract": " The paper describes the structure and rationale of the generalized software reliability process and a set of simulation techniques that may be applied for the purpose of software reliability modeling. These techniques establish a convenient means for studying a realistic, end-to-end software life cycle that includes intricate subprocess interdependencies, multiple defect categories, many factors of influence, and schedule and resource dependencies, subject to only a few fundamental assumptions, such as the independence of causes of failures. The goals of this research are dual: first, to generate data for truly satisfying the simplified assumptions of various existing models for the purpose of studying their comparative merits, and second, to enable these models to extend their merits to a less idealized, more realistic reliability life cycle. This simulation technique has been applied to data from a spacecraft project at the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "The Construction of Meta-Tools for Program Visualization of Parallel Software\n", "abstract": " Program visualization has been successfully applied to alleviate the complexities of parallel software development. However, those program visualization tools which utilize the more advanced visualization techniques tend to be too complex and specialpurpose to be generally useful. The use of meta-tools, or tools that construct and manipulate other tools, can be applied to simplify the use of complex environments, but meta-tools for program visualization can still require extensive specification for views and information processing details.This proposal provides a design methodology for program visualization meta-tools for parallel software that simplifies the use of such tools while maintaining a high degree of flexibility and expressive power. The approach is based on a meta-tool circulation architecture model that organizes the details of the user specification, and provides a circulation of information which supports a formal means for indicating relationships among that information. The overall user specification is divided into independent modules containing distinct entities, and the relationships among these module entities are identified using a powerful relationship mapping language. This language maps conditions on selected entities to manipulations that modify the entities, allowing the state of an entity to be controlled in terms of the state of any other entity or itself. The mapping language supports arbitrary levels of abstraction in manipulating entities, allowing a full range of possible detail. As a result, visual analyses can be specified efficiently, utilizing only the minimum level of detail necessary. To demonstrate the feasibility and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["14"]}
{"title": "Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing\n", "abstract": " Millions of mobile apps are available in app stores, such as Apple's App Store and Google Play. For a mobile app, it would be increasingly challenging to stand out from the enormous competitors and become prevalent among users. Good user experience and well-designed functionalities are the keys to a successful app. To achieve this, popular apps usually schedule their updates frequently. If we can capture the critical app issues faced by users in a timely and accurate manner, developers can make timely updates, and good user experience can be ensured. There exist prior studies on analyzing reviews for detecting emerging app issues. These studies are usually based on topic modeling or clustering techniques. However, the short-length characteristics and sentiment of user reviews have not been considered. In this paper, we propose a novel emerging issue detection approach named MERIT to take into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "What changed your mind: The roles of dynamic topics and discourse in argumentation process\n", "abstract": " In our world with full of uncertainty, debates and argumentation contribute to the progress of science and society. Despite of the increasing attention to characterize human arguments, most progress made so far focus on the debate outcome, largely ignoring the dynamic patterns in argumentation processes. This paper presents a study that automatically analyzes the key factors in argument persuasiveness, beyond simply predicting who will persuade whom. Specifically, we propose a novel neural model that is able to dynamically track the changes of latent topics and discourse in argumentative conversations, allowing the investigation of their roles in influencing the outcomes of persuasion. Extensive experiments have been conducted on argumentative conversations on both social media and supreme court. The results show that our model outperforms state-of-the-art models in identifying persuasive arguments via\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Data-driven robust non-fragile filtering for cyber-physical systems\n", "abstract": " Filtering or state estimation plays an important role in the cyber-physical systems (CPSs). This paper aims to solve the data-driven non-fragile filtering problem for the cyber-physical system. Randomly occurring gain variations are considered so as to account for the parameter fluctuations occurring during the filter implementation. The data-driven communication mechanism is utilized to reduce the measurement transmission frequency and save energy for the CPSs. Therefore, a unified H \u0393\u00ea\u20a7  filtering framework that combines the data-driven communication mechanism and the non-fragility of filters is constructed. Based on this unified framework, the influence of the simultaneous presence of networked-induced packet dropouts, quantization, randomly occurring nonlinearities and randomly occurring parameter uncertainties in the CPS is investigated. A modified dropouts model is proposed under the data-driven\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "QoS-aware web service searching\n", "abstract": " Web services are becoming prevalent nowadays. Finding desired Web services is becoming an emergent and challenging research problem. In this chapter, we present WSExpress (Web Service Express), a novel Web service search engine to expressively find expected Web services. WSExpress ranks the publicly available Web services not only by functional similarities to user queries, but also by non-functional QoS characteristics of Web services. WSExpress provides three searching styles, which can adapt to the scenario of finding an appropriate Web service and the scenario of automatically replacing a failed Web service with a suitable one. WSExpress is implemented by Java language, and large-scale experiments employing real-world Web services are conducted. Totally, 3738 Web services (15,811 operations) from 69 countries are involved in our experiments. The experimental results show that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Distributed information-theoretic metric learning in apache spark\n", "abstract": " Distance metric learning (DML) is an effective similarity learning tool to learn a distance function from examples to enhance the model performance in applications of classification, regression, and ranking, etc. Most DML algorithms need to learn a Mahalanobis matrix, a positive semidefinite matrix that scales quadratically with the number of dimensions of input data. This brings huge computational cost in the learning procedure, and makes all proposed algorithms infeasible for extremely high-dimensional data even with the low-rank approximation. Differently, in this paper, we take advantage of the power of parallel computation and propose a novel distributed distance metric learning algorithm based on a state-of-the-art DML algorithm, Information-Theoretic Metric Learning (ITML).More specifically, we utilize the property that each positive semidefinite matrix can be decomposed into a combination of rank-one and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Intelliad understanding in-app ad costs from users perspective\n", "abstract": " Ads are an important revenue source for mobile app development, especially for free apps, whose expense can be compensated by ad revenue. The ad benefits also carry with costs. For example, too many ads can interfere the user experience, leading to less user retention and reduced earnings ultimately. In the paper, we aim at understanding the ad costs from users perspective. We utilize app reviews, which are widely recognized as expressions of user perceptions, to identify the ad costs concerned by users. Four types of ad costs, i.e., number of ads, memory/CPU overhead, traffic usage, and bettery consumption, have been discovered from user reviews. To verify whether different ad integration schemes generate different ad costs, we first obtain the commonly used ad schemes from 104 popular apps, and then design a framework named IntelliAd to automatically measure the ad costs of each scheme. To demonstrate whether these costs indeed influence users reactions, we finally observe the correlations between the measured ad costs and the user perceptions. We discover that the costs related to memory/CPU overhead and battery consumption are more concerned by users, while the traffic usage is less concerned by users in spite of its obvious variations among different schemes in the experiments. Our experimental results provide the developers with suggestions on better incorporating ads into apps and, meanwhile, ensuring the user experience.", "num_citations": "3\n", "authors": ["14"]}
{"title": "CGA-based deadlock solving strategies towards vehicle sensing systems\n", "abstract": " Vehicle sensing system is an important research topic in the research field of Internet-of-Vehicles (IoV). Reliability and real-time performance of vehicle sensing systems are greatly influenced when deadlock happens. When a deadlock is detected, identifying the optimal deadlock solving strategy can ensure that the system goes back to normal state quickly. In order to address this issue, this paper proposes an efficient deadlock solving method. Firstly, the deadlock problem in a vehicle sensing system is analyzed based on four deadlock occurring conditions (i.e., mutual exclusion, hold and wait, no preemption, and circular wait). Secondly, an optimization model is built to combine the quantity and cost of tasks in vehicle sensing systems. After that, a co-evolutionary genetic algorithm (CGA) is developed to search the optimal deadlock solving strategy. Finally, experiments by simulation are conducted and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "A machine learning framework for space medicine predictive diagnostics with physiological signals\n", "abstract": " Prognostics and health management (PHM) in the context of space missions focuses on the fundamental issues of system failures in an attempt to predict when the failures may occur, and links these issues to system life cycle management. Space missions that are targeting for aerospace exploration or aviation also pose great challenges on the health conditions of people involved, such like astronauts, crew members, aviators, etc. Considering the inherent risks of space missions and the difficulty of direct communications between crew and ground support medical specialists, we see that greater autonomy in medical operations for crew is required. Namely, there is an urgent call for an effective onboard medical system to predict and prevent health problems in a timely manner, rather than following reactive approaches which are inherent to conventional medicine.", "num_citations": "3\n", "authors": ["14"]}
{"title": "Scientific application deployment on cloud: a topology-aware method\n", "abstract": " Effective scientific applications deploying is crucial for provide good services to cloud users. Scientific applications are usually topology-aware applications. Therefore, considering the communication topology of a scientific application during the development will benefit the performance of the application. However, it is challenging to automatically discover and make use of the communication pattern of a scientific application while deploying the application on cloud. To attack this challenge, in this paper, we propose a framework to discover the communication topology of a scientific application by pre-execution and multi-scale graph clustering, based on which the deployment can be optimized. In addition, instead of implement collective operations algorithm or MPI library, we present a set of efficient collective operations on cloud based on the common interconnect topology. Comprehensive experiments are conducted by employing a wellknown MPI benchmark and comparing the performance of our method with those of other methods. The experimental results show the effectiveness of our topology-aware deployment method. Copyright c\u0393\u00e2\u00a5 2012 John Wiley & Sons, Ltd.", "num_citations": "3\n", "authors": ["14"]}
{"title": "Mitigate the bottleneck of underwater acoustic sensor networks via priority scheduling\n", "abstract": " Underwater acoustic sensor networks (UWASNs) are composed of underwater sensors that use sound to transmit information collected in the ocean. Since the sound speed is lower than radio wave, UWASNs suffer from much lower throughput and higher delay compared with terrestrial wireless sensor networks (TWSNs). Current methods manage to alleviate the bottleneck by replacing mutual handshakes with reservation mechanisms that consume lower overhead. However, their throughput improvement and delay reduction are very limited (e.g., the throughput is only 30% of the theoretical maximum for TLohi in 8-node networks), and most of their analysis and simulations are based on single-hop communication. In this work, we tackle the above challenges by proposing a priority scheduling approach for multi-hop topologies. First, we find that the scheduling problem of UWASNs is very different from that of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Surviving holes and barriers in geographic data reporting for Wireless Sensor Networks\n", "abstract": " Geographic forwarding is a favorable scheme for data reporting in wireless sensor networks (WSNs) due to its simplicity and low-overhead. However, WSNs are usually subject to complicated environmental factors. Network holes (i.e., the areas where no nodes inside) and barriers (i.e., those blocking the communication between two close nodes) are inevitable in practical deploying environments. These issues pose an obstacle to adopting geographic forwarding in WSNs, while current approaches lack an efficient method to tolerate such negative factors. In this paper we specifically tailor a waypoint-based geographic data reporting protocol (GDRP) for WSNs. Inherited from geographic forwarding, GDRP is light-weighted and hence well-suits WSNs. But unlike current approaches that often find suboptimal paths, GDRP adopts an intelligent strategy to select a best set of waypoints via which packets can efficiently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Getting a Grip on Emotions in Negotiations: the Possibilities of ICT\n", "abstract": " Negotiation is a daily occurring process at all levels of society. Emotion plays an important role in negotiation between humans. In this paper we discuss to what extent and in which form ICT techniques can be used to get a grip on the emotional processes that play a role during negotiations. We focus on emotion recognition and measurement. Our analysis shows that current emotion recognition & measurement technology is mainly usable in the preparation for negotiations (including training sessions) and during offline moments of the negotiation (e.g., time-outs). The main arguments for this conclusion are: (1) valid and reliable emotion recognition and measurement techniques are usually invasive, and, (2) it is unclear if participants in a negotiation accept the technology.", "num_citations": "3\n", "authors": ["14"]}
{"title": "An Automated Approach to Inheritance and Polymorphic Testing using a VDM++ Specification\n", "abstract": " The use of formal methods is growing with the rapidly increasing applications of safety-critical systems in such fields as aviation, medicine, railways etc. The benefits of using formal methods are not limited to avoidance of specification errors and elimination of ambiguities only - a formal specification also provides a sound basis for generating test suites. However, most of the work in this area has focused on unit testing only. In object-oriented paradigm, inheritance and polymorphism are powerful features, yet they present new challenges to the testers. In this paper, the authors present a novel approach to automated generation of test cases from a VDM++ specification. The authors base the testing technique on Offutt et al.'s fault model for subtype inheritance and polymorphic testing.", "num_citations": "3\n", "authors": ["14"]}
{"title": "Constructing robust and resilient framework for cooperative video streaming\n", "abstract": " Peer-to-peer based streaming has been a promising solution for large-scale video broadcasting over the Internet. In a peer-to-peer video streaming framework, peers cooperate with each other for content distribution, so that the burden of the central server is greatly alleviated. Moreover, the peer-to-peer overlay is highly scalable to support a very large number of users. However, to support video streaming service, some strict performance issues need to be addressed, e.g., reliability, resilience and robustness to network dynamics. In this paper, we investigate several goals that a peer overlay should achieve in order to support good-quality video streaming. We then describe our work on organizing peers into such a robust and resilient framework. We have implemented a fully functional video broadcasting system based on the proposed peer-to-peer infrastructure. The prototype system has been successfully\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "A novel shrinkage technique based on the normal inverse Gaussian density model\n", "abstract": " This paper proposes a novel image denoising technique based on the normal inverse Gaussian (NIG) density model using an extended non-negative sparse coding (NNSC) algorithm proposed by us. This algorithm can converge to feature basis vectors, which behave in the locality and orientation in spatial and frequency domain. Here, we demonstrate that the NIG density provides a very good fitness to the non-negative sparse data. In the denoising process, by exploiting a NIG-based maximum a posteriori estimator (MAP) of an image corrupted by additive Gaussian noise, the noise can be reduced successfully. This shrinkage technique, also referred to as the NNSC shrinkage technique, is self-adaptive to the statistical properties of image data. This denoising method is evaluated by values of the normalized signal to noise rate (SNR). Experimental results show that the NNSC shrinkage approach is indeed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "CUHK at imageclef 2005: Cross-language and cross-media image retrieval\n", "abstract": " In this paper, we describe our studies of cross-language and cross-media image retrieval at the ImageCLEF 2005. This is the first participation of our CUHK (The Chinese University of Hong Kong) group at ImageCLEF. The task in which we participated is the \u0393\u00c7\u00a3bilingual ad hoc retrieval\u0393\u00c7\u00a5 task. There are three major focuses and contributions in our participation. The first is the empirical evaluation of language models and smoothing strategies for cross-language image retrieval. The second is the evaluation of cross-media image retrieval, i.e., combining text and visual contents for image retrieval. The last is the evaluation of bilingual image retrieval between English and Chinese. We provide an empirical analysis of our experimental results, in which our approach achieves the best mean average precision result in the monolingual query task in the campaign. Finally we summarize our empirical experience and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "Expected-reliability analysis for wireless CORBA with imperfect components\n", "abstract": " Reliability analysis has long been an important area of research for wired networks. However, little reliability analysis has been conducted on wireless networks. Wireless networks, such as wireless CORBA, inherit the unique handoff characteristic which leads to different communication structures with various types and numbers of components and links. Therefore, the traditional definition of two-terminal reliability is not applicable any more. We propose a new term, two-terminal expected-reliability, to integrate those different communication structures into one metric, which includes not only the failure parameters but also the service parameters. Nevertheless, the two-terminal expected-reliability is still a monotonically decreasing function of time t. The expected-reliability and the corresponding MTTF are evaluated quantitatively in different communication schemes. To observe the gains in reliability improvement, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["14"]}
{"title": "A DWT-based digital video watermarking scheme with error correcting\n", "abstract": " In this paper, a digital video watermarking algorithm is proposed. We present a novel DWT-based blind digital video watermarking scheme with scrambled watermark and error correcting code. Our scheme embeds different parts of a single watermark into different scenes of a video under the wavelet domain. To increase robustness of the scheme, the watermark is refined by the error correcting code, while the correcting code is embedded as watermark in audio channel. Our video watermarking algorithm is robust against the attacks of frame dropping, averaging and statistical analysis, which were not solved effectively in the past. Furthermore, it allows blind retrieval of embedded watermark which does not need the original video; and the watermark is perceptually invisible. The algorithm design, evaluation, and experimentation of the proposed scheme are described in this paper. 1.", "num_citations": "3\n", "authors": ["14"]}
{"title": "Component-based software engineering: Technologies, quality assurance schemes, and risk analysis tools\n", "abstract": " Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is much different from the traditional approach, quality assurance (QA) for component-based software development is a new topic in the software engineering community. In this paper, we survey current component-based software technologies, describe their advantages and disadvantages, and discuss the features they inherit. We also address QA issues for component-based software. As a major contribution, we propose a QA model for component-based software development, which covers component requirement analysis, component development, component certification, component customization, and system architecture design, integration, testing, and maintenance.We also look at the advantages of the Analyzer for Reducing Module Operational Risk (RMOR) tool, and collect some widely adopted Java metrics and tool suites. As our future work we will upgrade ARMOR to windows platformed, off-shelf commercial components based, Java source code oriented risk analysis and evaluation tool.", "num_citations": "3\n", "authors": ["14"]}
{"title": "\u0393\u00c7\u00ffA Linear Combination Software Reliability Modeling Tool with A Graphically-Oriented User Interface\n", "abstract": " In our recent work, we have shown that forming linear combination of model results tends to yield more accurate predictions of software reliability. Using linear combinations also simplifies the practitioner's task of deciding which model or models to apply to a particular development effort. Currently, no commercially available tools permit such combinations to be formed within the environment provided by the tool. Most software reliability modeling tools also do not take advantage of the high-resolution displays available today. Performing actions within the tool may be awkward, and the output of the tools may be understandable only to a specialist. We propose a software reliability modeling tool that allows users to formulate linear combination models, that can be operated by non-specialists, and that produces results in a form undetstandable by software developers and managements", "num_citations": "3\n", "authors": ["14"]}
{"title": "CRaDLe: Deep code retrieval based on semantic Dependency Learning\n", "abstract": " Code retrieval is a common practice for programmers to reuse existing code snippets in the open-source repositories. Given a user query (i.e., a natural language description), code retrieval aims at searching the most relevant ones from a set of code snippets. The main challenge of effective code retrieval lies in mitigating the semantic gap between natural language descriptions and code snippets. With the ever-increasing amount of available open-source code, recent studies resort to neural networks to learn the semantic matching relationships between the two sources. The statement-level dependency information, which highlights the dependency relations among the program statements during the execution, reflects the structural importance of one statement in the code, which is favorable for accurately capturing the code semantics but has never been explored for the code retrieval task. In this paper, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Assessing the bilingual knowledge learned by neural machine translation models\n", "abstract": " Machine translation (MT) systems translate text between different languages by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples. Although neural machine translation (NMT) has led the field of MT, we have a poor understanding on how and why it works. In this paper, we bridge the gap by assessing the bilingual knowledge learned by NMT models with phrase table -- an interpretable table of bilingual lexicons. We extract the phrase table from the training examples that an NMT model correctly predicts. Extensive experiments on widely-used datasets show that the phrase table is reasonable and consistent against language pairs and random seeds. Equipped with the interpretable phrase table, we find that NMT models learn patterns from simple to complex and distill essential bilingual knowledge from the training examples. We also revisit some advances that potentially affect the learning of bilingual knowledge (e.g., back-translation), and report some interesting findings. We believe this work opens a new angle to interpret NMT with statistic models, and provides empirical supports for recent advances in improving NMT models.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Incorporating geographical location for team formation in social coding sites\n", "abstract": " With the proliferation of open source software and community, more and more developers from different background (e.g., culture, language, location, skill) prefer to work collaboratively and release their works in social coding sites (e.g., Github). Given a collaborative project with a set of required skills, it is an important and challenging task to form a team of developers that have not only the required skills but also the minimal communication cost. Previous works mainly leverage historical collaboration records among team members to model the communication cost, while ignoring the impact of geographical location of each developer. In this paper, we aim to exploit and incorporate the geographical information to improve the performance of team formation in social coding sites. Specifically, we conduct two objective functions for the collaboration records and geographical proximity correspondingly, and propose two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Overlapping community detection with preference and locality information: a non-negative matrix factorization approach\n", "abstract": " Community detection plays an important role in understanding structures and patterns in complex networks. In real-world networks, a node in most cases belongs to multiple communities, which makes communities overlap with each other. One popular technique to cope with overlapping community detection is matrix factorization (MF). However, existing MF approaches only make use of the existence of a link, but ignore the implicit preference information inside it. In this paper, we first propose a Preference-based Non-negative Matrix Factorization (PNMF) model to take link preference information into consideration. Distinguished from traditional value approximation-based matrix factorization approaches, our model maximizes the likelihood of the preference order for each node so that it overcomes the indiscriminate penalty problem in which non-linked pairs inside one community are equally penalized in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Communication-efficient distributed deep metric learning with hybrid synchronization\n", "abstract": " Deep metric learning is widely used in extreme classification and image retrieval because of its powerful ability to learn the semantic low-dimensional embedding of high-dimensional data. However, the heavy computational cost of mining valuable pair or triplet of training data and updating models frequently in existing deep metric learning approaches becomes a barrier to apply such methods to a large-scale real-world context in a distributed environment. Moreover, existing distributed deep learning framework is not designed for deep metric learning tasks, because it is difficult to implement a smart mining policy of valuable training data. In this paper, we introduce a novel distributed framework to speed up the training process of the deep metric learning using multiple machines. Specifically, we first design a distributed sampling method to find the hard-negative samples from a broader scope of candidate samples\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "From mutual friends to overlapping community detection: A non-negative matrix factorization approach\n", "abstract": " Community detection provides a way to unravel complicated structures in complex networks. Overlapping community detection allows nodes to be associated with multiple communities. Matrix Factorization (MF) is one of the standard tools to solve overlapping community detection problems from a global view. Existing MF-based methods only exploit link information revealed by the adjacency matrix, but ignore other critical information. In fact, compared with the existence of a link, the number of mutual friends between two nodes can better reflect their similarity regarding community membership. In this paper, based on the concept of mutual friend, we incorporate Mutual Density as a new indicator to infer the similarity of community membership between two nodes in the MF framework for overlapping community detection. We conduct data observation on real-world networks with ground-truth communities to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "IntelliAd: assisting mobile app developers in measuring ad costs automatically\n", "abstract": " In-app mobile advertising serves as a primary source of revenue for most free apps. Such apps are embedded with third-party SDKs for ads displaying and are monetized by user impressions. However, ad placement can sometimes spoil user experience, for example, by too much memory consumption and battery drainage, thus leading to app uninstalling and unfavorable user feedback. Therefore, ensuring user perceptions of mobile ads can be greatly beneficial to app developers. Furthermore, various ad networks and formats make ads selection a great challenge. To achieve this, we design a tool named IntelliAd to automatically measure the ads-related consumption on mobile phones. Based on the measured costs, developers can optimize the ad-embedding schemes for their apps.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Entropy-based service selection with uncertain QoS for mobile cloud computing\n", "abstract": " With the prevalence of mobile computing and its convergence with cloud computing, there is an increasing trend of composing existing cloud services for rapid development of cloud-based mobile applications. It is vital for developers to find services not only satisfying their functionality requirements, but also meeting the requirements on non-functional quality of services (QoS). These QoS requirements, such as throughput, delay, reliability and security, are critical for the success of cloud-based mobile applications. In this paper, a QoS-based service ranking and selection approach is proposed to help developers select the service that best satisfies developers' QoS requirements from a set of services having already satisfied developers' functionality requirements in mobile cloud computing. Compared with state-of-the-art service ranking and selection techniques, our approach has the following advantages: 1) it uses\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "A new multi-factor risk score system for predicting the outcome after allogenic hematopoietic stem cell transplantation\n", "abstract": " The aim of this study was to develop and investigate the significance of a new multi-factor risk score system to predict the outcome of patients with hematological malignancies received allogeneic hematopoietic stem cell transplantation (allo-HSCT). The impact of pre-, peri-, and post-transplant factors on the outcome including overall survival (OS), disease-free survival (DFS), relapse and transplant-related mortality (TRM) after allo-HSCT were retrospectively analyzed in 122 patients with hematological malignancies at our center. A new risk score system based on the independent risk factors was established and tested. The results showed that absolute monocyte count at day 30 after transplantation (AMC-30,\u0393\u00eb\u00d1 536 cells/\u252c\u2561l)[hazard ratio (HR)= 0.313, 95% confidential interval (CI): 0.156-0.63], WT1 (\u0393\u00eb\u00d1 1.0%)(HR= 3.268, 95% CI: 1.644-6.499), pre-transplant risk grouping (HR= 1.999, 95% CI= 0.993-4.023) were independent prognostic factors of OS and DFS. Patients were divided into 3 groups based on the risk scoring system: group A (no risk factor; score 0), group B (1 risk factor; score 1) and group C (2-3 risk factors; score 2-3). OS at 5 years were 95.1%\u252c\u25923.4%, 62.9%\u252c\u25926.6% and 36.1%\u252c\u25929.6%, respectively (P< 0.0001). DFS at 5 years were 92.6%\u252c\u25924.9%, 60.4%\u252c\u25926.8% and 15.4%\u252c\u25927.1%, respectively (P< 0.0001). The akaike information criterion (AIC) value of the new score system for OS was 331, less than those of AMC-30, WT1, and pre-transplant risk group (346, 343, 346), AIC value for DFS and relapse were 378 and 231, both less than the three single elements (417, 397, 411 and 268, 238, 257). It is concluded that the risk scoring system based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "A comparison of lasso-type algorithms on distributed parallel machine learning platforms\n", "abstract": " Due to tremendous increase of data, scalability becomes a challenging issue for many modern machine learning algorithms. Various distributed machine learning platforms, eg, GraphLab, Spark and Petuum, are proposed to tackle this issue. Lasso algorithm, as its effectiveness in performing regression tasks while selecting the important features simultaneously, has become a benchmark machine learning method deployed in these platforms. However, rare work discusses the performance of the Lasso algorithm systematically and many other lasso-type algorithms are not well-studied yet. In addition, how to relieve the \u0393\u00c7\u00a3Ninja performance gap\u0393\u00c7\u00a5 between optimized code and most of these frameworks is a difficult task. To resolve the above tasks, we present a detailed deployment of the lassotype algorithms in several state-of-the-art distributed machine learning platforms. We characterize the performance of the native implementation and identify the potential parts to reduce the performance gap. We give a comprehensive comparison on running time, easy-of-deployment and capability of handling big data, which will enable end-users to choose platforms based on their goals.", "num_citations": "2\n", "authors": ["14"]}
{"title": "A topology-aware method for scientific application deployment on cloud\n", "abstract": " Nowadays, more and more scientific applications are moving to cloud computing. The optimal deployment of scientific applications is critical for providing good services to users. Scientific applications are usually topology-aware applications. Therefore, considering the topology of a scientific application during the development will benefit the performance of the application. However, it is challenging to automatically discover and make use of the communication pattern of a scientific application while deploying the application on cloud. To attack this challenge, in this paper, we propose a framework to discover the communication topology of a scientific application by pre-execution and multi-scale graph clustering, based on which the deployment can be optimised. In addition, we present a set of efficient collective operations for cloud based on the common interconnect topology. Comprehensive experiments are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Accurate extraction of human faces and their components from color digital images based on a hierarchical model\n", "abstract": " Many human face-based applications depend on accurate extraction of human faces and their components from complex color digital images. Most existing methods are only based on human skin regions and their adjacency relations, but do not utilize their sub-division and grouping at multiple levels. This paper presents a hierarchical model to address this. The model contains pixels, runs, regions, and their respective relations for a digital image. Human skin regions are identified by combining multiple color spaces. Adjacent human skin regions are grouped as a patch. Human face candidates are extracted from each refined human skin patch by un-linking adjacent regions of some regions based on a human facial shape model. Non-human skin patches in a human face candidate are identified as human facial internal component candidates according to the attributes and configurations of human facial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Positive topological entropy for multidimensional perturbations of topologically crossing homoclinicity\n", "abstract": " In this paper, we consider a one-parameter family F\u256c\u2557 of continuous maps on Rm or Rm\u251c\u00f9 Rk with the singular map F0 having one of the forms (i) F0 (x)= f (x),(ii) F0 (x, y)=(f (x), g (x)), where g: Rm\u0393\u00e5\u00c6 Rk is continuous, and (iii) F0 (x, y)=(f (x), g (x, y)), where g: Rm\u251c\u00f9 Rk\u0393\u00e5\u00c6 Rk is continuous and locally trapping along the second variable y. We show that if f: Rm\u0393\u00e5\u00c6 Rm is a C1 diffeomorphism having a topologically crossing homoclinic point, then F\u256c\u2557 has positive topological entropy for all \u256c\u2557 close enough to 0.1. Introduction. Following [6], we study multidimensional perturbations from a continuous map f on a phase space, say Rm, to a one-parameter family of continuous maps F\u256c\u2557 on a high-dimensional space, say Rm or Rm\u251c\u00f9 Rk, where \u256c\u2557\u0393\u00ea\u00ea Rl is a parameter, such that at \u256c\u2557= 0, the singular map F0 is one of the following forms:(i) F0 (x)= f (x)\u0393\u00ea\u00ea Rm;", "num_citations": "2\n", "authors": ["14"]}
{"title": "Department of Computer Science and Engineering The Chinese University of Hong Kong\n", "abstract": " Videoconferencing becomes more and more popular in business activities, personal communications and educations, because of the rapid growth of Internet bandwidth and technical advances in multimedia coding. The participant of a videoconference usually wishes to keep the video archive for later reference. However, existing videoconferencing clients either do not record or only record the personal conference as ordinary audio and video files, in which the file name is the only place, but not the ideal place, to indicate the subject. Therefore, we can imagine that, in the near future, when a person accumulates many videoconference archives in his/her work, study and daily life, he/she may not be able to recall the details of a conference without watching it again. Since the visual and aural information is not directly searchable like text, it is also difficult to search out those archives with content of interest by normal searching methods. Therefore, the research of effective indexing personal videoconference archives is of timely importance. However, current efforts on multimedia indexing still focus on digital video libraries or distance learning. Reports on indexing videoconference archives have not been found so far as we investigated.Our final year project will develop a well-designed Personal VideoConference Archives Indexing System-PVCAIS, which integrates many multimedia-indexing techniques to manage personal videoconference archives. Firstly, the contents of video, audio, text chat and whiteboard communications are received from the videoconferencing client and stored after removing the redundancies. Next, more information, eg\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "A coalitional game model for heat diffusion based incentive routing and forwarding scheme\n", "abstract": " We propose an incentive routing and forwarding scheme that integrates a reputation system into a monetary payment mechanism to encourage nodes cooperation in wireless ad hoc networks. For the first time in the literature, we build our reputation system based on a heat diffusion model. The heat diffusion model provides us a way of combining the direct and indirect reputation together and propagating the reputation from locally to globally. Further, we model and analyze our incentive scheme using a coalitional game, which is not the usual non-cooperative game like others. We further prove that under a proper condition this game has a non-empty stable core. From the evaluation we can see that the cumulative utility of nodes increases when nodes stay in the core.", "num_citations": "2\n", "authors": ["14"]}
{"title": "On sensor network reconfiguration for downtime-free system migration\n", "abstract": " Many state-of-the-art wireless sensor networks have been equipped with reprogramming modules, e.g., those for software/firmware updates. System migration tasks such as software reprogramming, however, will interrupt normal sensing and data processing operations of a sensor node. Although such tasks are occasionally invoked, the long time of such tasks may disable the network from detecting critical events, posing a severe threat to many sensitive applications. In this paper, we present the first formal study on the problem of downtime-free migration. We demonstrate that the downtime can be effectively eliminated, by partitioning the sensors into subsets, and let them perform migration tasks successively with the rest still performing normal services. We then present a series of effective algorithms, and further extend our solution to a practical distributed and localized implementation. The performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Learning with unlabeled data\n", "abstract": " Machine learning is a subfield of artificial intelligence that is concerned with the design and development of algorithms and techniques that allow computers to make inductions or deduction [102]. In general, machine learning studies a variety of different types of problems. In terms of the different settings and ways of dealing with data, machine learning algorithms can typically be categorized as unsupervised learning, supervised learning, semisupervised learning, and reinforcement learning, and others. We give a simple description of these learning algorithms in the following:\u0393\u00c7\u00f3 Supervised learning that generates a function that maps inputs to desired outputs. In supervised learning, each instance of the training data consists of a data vector and it corresponding output. In terms of the output, there are two supervised supervised learning tasks: classification where the output is discrete and regression where the output is continuous.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Modeling Data Locally and Globally. Advanced Topics in Science and Tecnology in China: Machine Learning\n", "abstract": " Machine Learning-Modeling Data Locally and Globally presents a novel and unified theory that tries to seamlessly integrate different algorithms. Specifically, the book distinguishes the inner nature of machine learning algorithms as either\" local learning\" or\" global learning.\" This theory not only connects previous machine learning methods, or serves as roadmap in various models, but\u0393\u00c7\u00f4more importantly\u0393\u00c7\u00f4it also motivates a theory that can learn from data both locally and globally. This would help the researchers gain a deeper insight and comprehensive understanding of the techniques in this field. The book reviews current topics, new theories and applications.", "num_citations": "2\n", "authors": ["14"]}
{"title": "A novel discriminative naive Bayesian network for classification\n", "abstract": " Naive Bayesian network (NB) is a simple yet powerful Bayesian network. Even with a strong independency assumption among the features, it demonstrates competitive performance against other state-of-the-art classifiers, such as support vector machines (SVM). In this chapter, we propose a novel discriminative training approach originated from SVM for deriving the parameters of NB. This new model, called discriminative naive Bayesian network (DNB), combines both merits of discriminative methods (eg, SVM) and Bayesian networks. We provide theoretic justifications, outline the algorithm, and perform a series of experiments on benchmark real-world datasets to demonstrate our model\u0393\u00c7\u00d6s advantages. Its performance outperforms NB in classification tasks and outperforms SVM in handling missing information tasks.", "num_citations": "2\n", "authors": ["14"]}
{"title": "A modified learning algorithm incorporating additional functional constraints into neural networks\n", "abstract": " In this paper, a modified learning algorithm to obtain better generalization performance is proposed. The cost terms of this new algorithm are selected based on the second-order derivatives of the neural activation at the hidden layers and the first-order derivatives of the neural activation at the output layer. It can be guaranteed that in the course of training, the additional cost terms for this algorithm can penalize both the input-to-output mapping sensitivity and the high frequency components to obtain better generalization performance. Finally, theoretical justifications and simulation results are given to verify the efficiency and effectiveness of the proposed learning algorithm.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Semantic video summarization using mutual reinforcement and shot arrangement patterns\n", "abstract": " We propose a novel semantic video summarization framework, which generates video skimmings that guarantee both the balanced content coverage and the visual coherence. First, we collect video semantic information with a semi-automatic video annotation tool. Secondly, we analyze the video structure and determine each video scene\u0393\u00c7\u00d6s target skim length. Then, mutual reinforcement principle is used to compute the relative importance value and cluster the video shots according to their semantic descriptions. Finally, we analyze the arrangement pattern of the video shots, and the key shot arrangement patterns are extracted to form the final video skimming, where the video shot importance value is used as guidance. Experiments are conducted to evaluate the effectiveness of our proposed approach. 1.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Message queueing analysis in wireless networks with mobile station failures and handoffs\n", "abstract": " Access points play an essential role in fault tolerant architectures for mobile computing environments which engage wireless networks. They are the performance bottleneck in the presence of failures and handoffs of mobile stations. Different message dispatch strategies impose different effects on the message sojourn time in access points. In this paper, we study five dispatch models which are the basic queueing model, the static and the dynamic processor-sharing models, the round-robin model, and the feedback model. We derive the expected message sojourn time in access points under steady state. We observe that the basic model and the static processor-sharing model demonstrate the worst performance. The other three models cut down the sojourn time by dynamically reducing the probability of message blocking which is introduced by failures and handoffs of mobile stations; however, which one is the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Extendable and interchangeable architecture description of distributed systems using UML and XML\n", "abstract": " Software Architecture can help people to better understand the gross structure and, with powerful analysis techniques, to evaluate the properties of a software system. To accommodate the dynamic changes and facilitate interoperation of tools, an architectural description of the distributed system should be extensible and interchangeable. In this paper, we utilize the built-in extension mechanism of the Unified Modeling Language (UML) to describe the architectures of distributed systems, with the underlying architectural metadata represented in XML. In particular, the approach has been applied to describe the architectural model of distributed software in the Graph-Oriented Programming framework. The proposed approach has many desirable features, characterized by being visual, easily extendable and interchangeable, and well supported by tools.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Biased Minimax Probability Machine\n", "abstract": " A recently-proposed novel classifier called Minimax Probability Machine (MPM) achieves the comparative performance with a state-of-the-art classifier, the Support Vector Machine. MPM trains the classifier by minimizing the worst-case probability of misclassification of future data points. However, this model assumes the unbiased weight for each class and thus lacks the ability to deal with biased classification tasks. In this paper, aiming at solving this problem, we develop an extension named Biased Minimax Probability Machine (BMPM). By setting an acceptable lower bound to the accuracy for the less important class, we maximize the worst-case probability of the accuracy for the biased class. This model is the first quantitative method to control how the decision hyperplane moves in favor of the classification of the more important class. We present the formulation and implement a series of experiments on both a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Design and Implementation of XML-Based Digital Video Library System.\n", "abstract": " In this paper, we describe the design and implementation of an XML-Based Digital Video Library (XDVL) System. The system includes automatic processes from video creation through video delivery, which involves storing, searching, indexing and retrieval of video contents. It also allows users to search for videos based on various kinds of information using a multi-lingual approach. We adopt a multi-tier model of four components in our system, namely Video Server, Indexing Server, Query Server and Client Application, resulting in a more extensible and usable system. For data management, we employed XML as the enabling technology for metadata structure repository and component\u0393\u00c7\u00d6s messaging interface. With XDVL, we can maintain the compatibility between different digital library developers, and its open-standard solution will contribute to the popularity of digital video libraries for their creation, integration and subscription.", "num_citations": "2\n", "authors": ["14"]}
{"title": "A phase-based approach to creating highly reliable software\n", "abstract": " Software reliability engineering includes: software reliability measurement, which includes estimation and prediction, with the help of software reliability models established in the literature; the attributes and metrics of product design, development process, system architecture, software operational environment, and their implications on reliability; and the application of this knowledge in specifying and guiding system software architecture, development, testing, acquisition, use, and maintenance. My position is that we should attack the problem of software reliability engineering in three phases: modeling and analysis phase; design and implementation phase; and testing and measurement phase. All these phases deal with the management of software faults and failures.", "num_citations": "2\n", "authors": ["14"]}
{"title": "SIAS: a secure shopping information agent system\n", "abstract": " In this paper, we build a Shopping Information Agent System (SIAS) based on mobile agent technology. We discuss possible security attacks by malicious hosts to agents in the system, and present our solutions to prevent these attacks. We analyze the security of our solutions, and evaluate the performance overhead introduced.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Guest editor's introduction to the special issue on web technologies\n", "abstract": " INCE its first official definition in 1989, the World Wide Web (or simply \u0393\u00c7\u00a3the Web\u0393\u00c7\u00a5) has drawn enormous attention among not only academic professionals and the industry workforce, but also the general public\u252c\u00fa most of whom are computer illiterates. The Web is the universe of networkaccessible data and the embodiment of human knowledge. Information engineering began a new era with the birth of Web technologies. The Web\u0393\u00c7\u00d6s phenomenal popularity in the past decade stems from the way in which it enables individuals to navigate their own paths through the abundant information repository provided by any other individuals or cooperating entities. People share ideas, hold discussions, discover theories, exchange information, and learn new techniques freely and openly in the Web. The Web is also highly interactive, making everyone potential desk-top publishers, forum practitioners, and information explorers\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "A summary of Qos support in SWAN\n", "abstract": " SWAN (Seamless Wireless ATM Network) uses ATM technology to provide integrated services to mobile terminals in an indoor setting. Variability caused by wireless access and mobility make this a very different environment for ATM, specifically for supporting QoS. This paper provides a summary of QoS support in SWAN, including a scheme for renegotiating QoS in response to changes in available bandwidth. Experimental results are presented to demonstrate its operation and effectiveness.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Software Reliability Measurement Experience\n", "abstract": " The key components in the SRE process, as described in Chap. 6, include reliability objective specification, operational profile determination, reliability modeling and measurement, and reliability validation. These techniques were applied to several internal projects developed within Jet Propulsion Laboratory (JPL) and Bell Communications Research (Bellcore). The project background, reliability engineering procedures, data collection efforts, modeling results, data analyses, and reliability measurements for these projects are presented in this chapter. Model comparisons for the software reliability applications, lessons learned with regard to the engineering effort, and directions for current and future software reliability investigations are also provided. One major thing we observed is that for the failure data we analyzed, no one model was consistently the best. It was frequently the case that a model that had\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["14"]}
{"title": "Lucent Technologies\n", "abstract": " Discriminative classifiers such as Support Vector Machines (SVM) directly learn a discriminant function or a posterior probability model to perform classification. On the other hand, generative classifiers often learn a joint probability model and then use the Bayes rule to construct a posterior classifier. In general, generative classifiers are not as accurate as discriminative classifiers. However generative classifiers provide a principled way to deal with the missing information problem, which discriminative classifiers cannot easily handle. To achieve good performance in various classification tasks, it is better to combine these two strategies. In this paper, we develop a method to train one of the popular generative classifiers, the Naive Bayesian classifier (NB) in a discriminative way. We name this new model as the Discriminative Naive Bayesian classifier. We provide theoretic justifications, outline the algorithm, and perform a serious of experiments on benchmark real-world datasets to demonstrate our model\u0393\u00c7\u00d6s advantages. Its performance outperforms NB in classification tasks and outperforms SVM in handling missing information tasks. I.", "num_citations": "2\n", "authors": ["14"]}
{"title": "Memory-Safety Challenge Considered Solved? An In-Depth Study with All Rust CVEs\n", "abstract": " Rust is an emerging programming language that aims at preventing memory-safety bugs without sacrificing much efficiency. The claimed property is very attractive to developers, and many projects start using the language. However, can Rust achieve the memory-safety promise? This article studies the question by surveying 186 real-world bug reports collected from several origins, which contain all existing Rust common vulnerability and exposures (CVEs) of memory-safety issues by 2020-12-31. We manually analyze each bug and extract their culprit patterns. Our analysis result shows that Rust can keep its promise that all memory-safety bugs require unsafe code, and many memory-safety bugs in our dataset are mild soundness issues that only leave a possibility to write memory-safety bugs without unsafe code. Furthermore, we summarize three typical categories of memory-safety bugs, including automatic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "On the diversity of multi-head attention\n", "abstract": " Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we propose two approaches to better exploit such diversity for multi-head attention, which are complementary to each other. First, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Second, we propose to better capture the diverse information distributed in the extracted partial-representations with the routing-by-agreement algorithm. The routing algorithm iteratively updates the proportion of how much a part (i.e. the distinct information learned from a specific subspace\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Learning by Distillation: A Self-Supervised Learning Framework for Optical Flow Estimation\n", "abstract": " We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Do users care about ad\u0393\u00c7\u00d6s performance costs? Exploring the effects of the performance costs of in-app ads on user experience\n", "abstract": " Context: In-app advertising is the primary source of revenue for many mobile apps. The cost of advertising (ad cost) is non-negligible for app developers to ensure a good user experience and continuous profits. Previous studies mainly focus on addressing the hidden performance costs generated by ads, including consumption of memory, CPU, data traffic, and battery. However, there is no research on analyzing users\u0393\u00c7\u00d6 perceptions of ads\u0393\u00c7\u00d6 performance costs to our knowledge.Objective: To fill this gap and better understand the effects of performance costs of in-app ads on user experience, we conduct a study on analyzing user concerns about ads\u0393\u00c7\u00d6 performance costs.Method: First, we propose RankMiner, an approach to quantify user concerns about specific app issues, including performance costs. Then, based on the usage traces of 20 subject apps, we measure the performance costs of ads. Finally, we conduct\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Why an Android App Is Classified as Malware: Toward Malware Classification Interpretation\n", "abstract": " Machine learning\u0393\u00c7\u00f4(ML) based approach is considered as one of the most promising techniques for Android malware detection and has achieved high accuracy by leveraging commonly used features. In practice, most of the ML classifications only provide a binary label to mobile users and app security analysts. However, stakeholders are more interested in the reason why apps are classified as malicious in both academia and industry. This belongs to the research area of interpretable ML but in a specific research domain (i.e., mobile malware detection). Although several interpretable ML methods have been exhibited to explain the final classification results in many cutting-edge Artificial Intelligent\u0393\u00c7\u00f4based research fields, until now, there is no study interpreting why an app is classified as malware or unveiling the domain-specific challenges. In this article, to fill this gap, we propose a novel and interpretable ML\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "AIOps Innovations of Incident Management for Cloud Services\n", "abstract": " While remarkable advances have been achieved in cloud computing infrastructure, the way incidents (unplanned interruptions or outages of a service/product) are managed needs to be as agile and dynamics as the cloud itself. In practice, incident management is conducted through analysing a huge amount of monitoring data collected at the runtime of services. Given its data-driven nature, we deem AIOps innovations as essential to empowering cloud systems to provide more reliable online services and applications by incorporating more intelligence into the entire workflow of incident management. This paper presents a project showcase of our AIOps practices towards these goals at Microsoft. First, we brief the incident management procedure and its corresponding real-world challenges. Then, we elaborate the ML & AI techniques used for mitigating such challenges and share some application results to demonstrate the intelligence and benefits conveyed to Microsoft service products.", "num_citations": "1\n", "authors": ["14"]}
{"title": "Discrete Sliding Mode Control for a Class of WSN Based IoT with Time-delay and Nonlinear Characteristic\n", "abstract": " This paper applies discrete sliding mode control to the WSN based IoT, to ensure its robust stability under different types of communication time-delay and nonlinear characteristic. Due to the limited sampling frequency, the system will approach a quasi-sliding surface in finite time. Linear matrix inequality (LMI) approach is used to prove the quadratically stability of the sliding surface and suitable Lyapunov function is constructed to obtain the appropriate control law. Two situation will be investigated in this paper, the system with signal time-delay and system with multiple time-delays. Furthermore, nonlinear characteristic of system has been taken into consideration. By employing non-singular transformation and reduced order processing, the proof of sliding surface stability and finite time approaching is simplified. Finally, two numerical simulations are employed to demonstrate the effectiveness and applicability of the proposed approach.", "num_citations": "1\n", "authors": ["14"]}
{"title": "AI Techniques in Software Engineering Paradigm.\n", "abstract": " \u03c3\u2563\u2557\u03c4\u00fc\u00bb\u03c4\u00eb\u00e7 1 Page 1 AI Techniques in Software Engineering Paradigm Computer Science & Engineering Department The Chinese University of Hong Kong Rung-Tsong Michael LYU ICPE 2018 Berlin April 12, 2018 Page 2 2 Introduction AI Techniques Operational Phase Analysis Phase \u0398\u00aa\u00d6\u00b5\u2555\u00bb\u03a3\u2555\u00a1\u00b5\u00fb\u00e7\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa Chinese University of Hong Kong Development Phase Conclusion Page 3 A Brief History of the IT World ENIAC Birth of Internet The MITS Altair Apple II IBM Desktop PC Time Magazine Person of the Year Apple Macintosh Birth of WWW Birth of XML Birth of Web 2.0 Attention Age Time Magazine Person of the Year 3 Birth of iPhone 1750 1945 1969 1975 1981 1983 1984 1989 1996 2004 2006 Industrial Revolution Information Age WWW Age Internet Age 2007 Page 4 A Brief History of AI Development Representation Language of Logic Birth of \u0393\u00c7\u00a3AI\u0393\u00c7\u00a5 Basic Structure Symbolics Machine Deep Blue vs Kasparov The Age of AI \u0393\u00c7\u00d6-\u0393\u00c7\u00d6\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Ternary solvent boosts two-dimensional perovskites\n", "abstract": " In the past few years, organic-inorganic/inorganic lead halide perovskites, most notably CH3NH3PbI3 and its mixed cation/anion compounds, have attracted tremendous research interests in a wide range of different disciplines, which is largely initialized by their unprecedented success in low-cost and highly efficient solar cells. With certified efficiencies of over 22% in the lab-scale devices, lead halide perovskite solar cells have been considered as a serious rival of crystalline Si solar cells in the future photovoltaic market (http://www. nrel. gov/ncpv/images/efficiency_chart. jpg). The rapid progress in the lead halide perovskite solar cells has been relying on the confluence factors of their extraordinary intrinsic properties, including high light-absorption, low exciton binding energy, long charge carrier diffusion lengths, dominant shallow point defects, benign grain boundaries and ease of film crystallization via various low-temperature solution methods.[1-2] These excellent optoelectronic properties together with their prominent advantage of ease of fabrication have also endowed lead halide perovskites in a number of applications beyond photovoltaics, such as field-effect transistors, light-emitting diodes, photodetectors, lasers, X-ray detectors and memory devices, to name but a few.[3]While the typical lead halide perovskites under the spotlight of research frontier adopt a three-dimensional (3D) structure, there is another class of two-dimensional (2D) counterparts that have recently attracted increasing research attention owing to their intriguing optoelectronic properties as well as superior ambient stability.[4] These layered perovskites share a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Time-Aware Model-Based QoS Prediction\n", "abstract": " The exponential growth of Web service makes building high-quality service-oriented applications an urgent and crucial research problem. User-side QoS evaluations of Web services are critical for selecting the optimal Web service from a set of functionally equivalent service candidates. Since QoS performance of Web services is highly related to the service status and network environments which are variable against time, service invocations are required at different instances during a long time interval for making accurate Web service QoS evaluation. However, invoking a huge number of Web services from user-side for quality evaluation purpose is time-consuming, resource-consuming, and sometimes even impractical (e.g., service invocations are charged by service providers). To address this critical challenge, this chapter proposes a Web service QoS prediction framework, called WSPred, to provide time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Neighborhood-based QoS prediction\n", "abstract": " With the increasing popularity of cloud computing as a solution for building high-quality applications on distributed components, efficiently evaluating user-side quality of cloud components becomes an urgent and crucial research problem. However, invoking all the available cloud components from user-side for evaluation purpose is expensive and impractical. To address this critical challenge, we propose a neighborhood-based approach, called CloudPred, for collaborative and personalized quality prediction of cloud components. CloudPred is enhanced by feature modeling on both users and components. Our approach CloudPred requires no additional invocation of cloud components on behalf of the cloud application designers. The extensive experimental results show that CloudPred achieves higher QoS prediction accuracy than other competing methods. We also publicly release our large-scale QoS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Online QoS Prediction\n", "abstract": " The exponential growth of Web service makes building high-quality service-oriented systems an urgent and crucial research problem. Performance of the service-oriented systems highly depends on the remote Web services as well as the unpredictability of the Internet. Performance prediction of service-oriented systems is critical for automatically selecting the optimal Web service composition. Since the performance of Web services is highly related to the service status and network environments which are variable over time, it is an important task to predict the performance of service-oriented systems at runtime. To address this critical challenge, this chapter proposes an online performance prediction framework, called OPred, to provide personalized service-oriented system performance prediction efficiently. Based on the past usage experience from different users, OPred builds feature models and employs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Online non-negative dictionary learning via moment information for sparse poisson coding\n", "abstract": " Online dictionary learning for sparse coding is an effective tool for data analysis. It incrementally learns a set of basis vectors with sparse linear combinations of these vectors when new samples appear. Previous work assumes that the samples embed Gaussian noises, which weaken the power of these methods in handling real applications with non-negative data (e.g., frequency data in word counts). Differently, in this paper, we concentrate on online learning for non-negative dictionary by using moment information for sparse Poisson coding. We exploit the non-negativity of Poisson models to learn a set of non-negative basis vectors and a non-negative sparse linear combination for the moment information of samples. Specifically, we first formulate the online learning problem via the maximum-a-posteriori (MAP) framework. We then propose a novel online algorithm which alternatively updates the sparse\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Non-monotonic feature selection for regression\n", "abstract": " Feature selection is an important research problem in machine learning and data mining. It is usually constrained by the budget of the feature subset size in practical applications. When the budget changes, the ranks of features in the selected feature subsets may also change due to nonlinear cost functions for acquisition of features. This property is called non-monotonic feature selection. In this paper, we focus on non-monotonic selection of features for regression tasks and approximate the original combinatorial optimization problem by a Multiple Kernel Learning (MKL) problem and show the performance guarantee for the derived solution when compared to the global optimal solution for the combinatorial optimization problem. We conduct detailed experiments to demonstrate the effectiveness of the proposed method. The empirical results indicate the promising performance of the proposed framework\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Service Reliability Engineering: Performance Evaluation, Fault Tolerance, and Reliability Prediction\n", "abstract": " Service Reliability Engineering: Performance Evaluation, Fault Tolerance, and Reliability Prediction Page 1 Service Reliability Engineering: Performance Evaluation, Fault Tolerance, and Reliability Prediction Michael R. Lyu lyu@cse.cuhk.edu.hk Department of Computer Science & Engineering The Chinese University of Hong Kong December, 2011 Page 2 2 Outline \u0393\u00c7\u00f3 Introduction \u0393\u00c7\u00f3 Performance evaluation of Web services \u0393\u00c7\u00f3 Fault-tolerant Web services \u0393\u00c7\u00f3 Reliability prediction of Web services \u0393\u00c7\u00f3 Conclusion Page 3 3 Introduction Web applications are becoming more and more important! Page 4 4 Introduction \u0393\u00c7\u00f3 The age of Web 2.0 \u0393\u00c7\u00f4 Web pages and Web services \u0393\u00c7\u00f3 Web services (WS) are Web APIs that can be accessed over a network and executed on remote systems \u0393\u00c7\u00f4 Open standards \u0393\u00c7\u00f4 Interoperability Page 5 5 Introduction \u0393\u00c7\u00f3 Service-oriented systems \u0393\u00c7\u00f4 Composed by distributed Web services Page 6 Fault Lifecycle Fault //-\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Energy-efficient On-demand Active Contour Service for sensor networks\n", "abstract": " Contour mapping is an important technique for Wireless Sensor Networks (WSNs) in environmental monitoring to abstract the information of a monitored field. State-of-the-art approaches for contour mapping, however, are neither energy-optimized, nor capable of handling heterogeneous user requests. In this paper, we develop a novel energy-efficient On-demand Active Contour Service (OACS) for power-constrained WSNs. OACS regresses the field intensity function with kernel Support Vector Regression (SVR), a novel machine learning tool that flexibly handles both contour line and contour map requests. OACS also adaptively accommodates a wide range of contour line/map precision requirements: (1) For applications of low precision, only a minimum set of nodes are scheduled in working mode while others are sleeping for conserving energy. (2) For applications of high precision, through an active and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Maximal-robustness-minimal-fragility controller: a compromise between robustness and fragility of biochemical networks\n", "abstract": " Establishing a trade-off between robustness and fragility has been an active research topic in constructing biochemical networks. In this paper, we formulate a compromise between robustness and fragility as a cooperative game, based on which a dynamical incomplete information game is constructed. In addition, three channels are chosen as players, and eight pure control strategies are created. Algorithms in seeking the perfect Bayesian-Nash equilibrium are consequently constructed. Based on the perfect Bayesian-Nash equilibrium, Maximal-Robustness-Minimal-Fragility controller (MRMFc) is derived, and MRMFc is effectively applied to biochemical networks. And computer simulations demonstrate that the biochemical network achieves a good balance between robust stability and dynamical performance. Consequently, an attractive solution in attacking the problem of the trade-offs between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "POWER-SPEED: A Power-Controlled Real-Time Data Transport Protocol for Wireless Sensor-Actuator Networks\n", "abstract": " This paper investigates the data transport problem for reporting delay-sensitive events in wireless sensor-actuator networks (WSANs). We specifically tailor the protocol design according to the features of WSANs and propose POWERSPEED, a real-time data transport protocol for WSANs to achieve energy-efficient data transport for delay-sensitive event reporting. In POWER-SPEED, sensor nodes select the next-hop neighbor to actuators according to the spatio-temporal historic data of the upstream QoS condition, which completely avoids control packets. With an adaptive transmitter power control scheme, POWER-SPEED conveys packets in an energy-efficient manner while maintaining soft real-time packet transport. It thus reduces the energy consumption of data transport while ensuring the QoS requirement in timeliness domain. We demonstrate the effectiveness of POWER-SPEED through simulations with NS2.", "num_citations": "1\n", "authors": ["14"]}
{"title": "Reliable Reporting of Delay-Senstive Events in Wireless Sensor-Actuator Networks\n", "abstract": " Wireless sensor-actuator networks, or WSANs, greatly enhance the existing wireless sensor network architecture by introducing powerful and even mobile actuators. The actuators work with the sensor nodes, but can perform much richer application-specific actions. To act responsively and accurately, an efficient and reliable reporting scheme is crucial for the sensors to inform the actuators about the environmental events. Unfortunately, the low-power multi-hop communications in a WSAN are inherently unreliable; the frequent sensor failures and the excessive delays due to congestion or in-network data aggregation further aggravate the problem.In this paper, we propose a general reliability-centric framework for event reporting in WSANs. We argue that the reliability in such a real-time system depends not only on the accuracy, but also the importance and freshness of the reported data. Our design follows this argument and seamlessly integrates three key modules that process the event data, namely, an efficient and fault-tolerant event data aggregation algorithm, a delay-aware data transmission protocol, and an adaptive actuator allocation algorithm for unevenly distributed events. Our transmission protocol also adopts smart priority scheduling that differentiates the event data of non-uniform importance. We evaluate our framework through extensive simulations, and the results demonstrate that it achieves desirable reliability with minimized delay.", "num_citations": "1\n", "authors": ["14"]}
{"title": "Improvements to the conventional layer-by-layer BP algorithm\n", "abstract": " This paper points out some drawbacks and proposes some modifications to the conventional layer-by-layer BP algorithm. In particular, we present a new perspective to the learning rate, which is to use a heuristic rule to define the learning rate so as to update the weights. Meanwhile, to pull the algorithm out of saturation area and prevent it from converging to a local minimum, a momentum term is introduced to the former algorithm. And finally the effectiveness and efficiency of the proposed method are demonstrated by two benchmark examples.", "num_citations": "1\n", "authors": ["14"]}
{"title": "Comparative studies on the CT image reconstruction based on the RBF neural network\n", "abstract": " To reconstruct two-dimensional computerized tomography (CT) images from a small amount of projection data is a very difficult task. In this paper, two methods based on radial basis function (RBF) neural network are investigated to perform such a work. In the first method, we take projection data as the input and original image as the output of the network, after trained with some samples, the network can be applied to reconstruct CT image in the same class. In the second method, we adopt coordinate and a cross-section image as the input and the output respectively. For converting the image to its projections, an additional integral module is cascaded with the network. To evaluate these two methods, a comparative study is presented. A pixel-wise error estimator is adopted to calculate the overall error of the reconstructed images. Experiments show that the second method is the best for moderate projection data in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Using smart agent-based method to implement dynamic information security policy\n", "abstract": " Today, networks are becoming more complex and consequently more vulnerable to various kinds of attacks. So management of networks requires more smart and sophisticated methods. Smart agent technology provides a powerful method for the modeling and development of complex networks. So, we propose an smart agent-based dynamic information security management method to adapt to networks\u0393\u00c7\u00d6 vulnerability and complexity, In this paper, we describe our method of dynamic information security policy of networks, which is based on security policies, security events and security concepts. And we proposed a new generation of flexible and adaptable systems for efficient dynamically security management. The smart agent-based approach is innovative because it makes dynamic information security management system to adapt to unpredicted complex evolution of both network environments and intrusions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "A Framework for Indexing Personal Videoconference\n", "abstract": " The rapid technical advance of multimedia communication has enabled more and more people to enjoy videoconferences. Traditionally, the personal videoconference is either not recorded or only recorded as ordinary audio and video files that only allow linear access. Moreover, in addition to video and audio channels, other videoconferencing channels, including text chat, file transfer, and whiteboard, also contain valuable information. Therefore, it is not convenient to search or recall the content of videoconference from the archives. However, there exists little research on the management and automatic indexing of personal videoconferences. The existing methods for video indexing, lecture indexing, and meeting support systems cannot be applied to personal videoconference straightforwardly. This chapter discusses important issues unique to personal videoconference and proposes a comprehensive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["14"]}
{"title": "Digital Video Watermarking Techniques for Secure Multimedia Creation and Delivery\n", "abstract": " With the rapid growth of the Internet and multimedia systems in distributed environments, it is easier for digital data owners to transfer multimedia documents across the Internet. Therefore, there is an increase in concern over copyright protection of digital content. However, current technology dose not protect their copyright properly. In the early days, encryption and control access techniques were used to protect the ownership of media. They do not protect against unauthorized copying after the media have been successfully transmitted and decrypted. Recently, the watermark techniques are utilized to maintain the copyright. In this paper, we focus on using the digital watermarking techniques to protect digital multimedia intellectual copyright and propose a new algorithm for video watermarking. We present a novel DWT-based blind digital video watermarking scheme with scrambled watermark. Our scheme embeds different parts of a signal watermark into different scenes of a video under the wavelet domain. To increase robustness of the scheme, the watermark is refined by the error correcting code, which is embedded as watermark in the audio channel. The features of our video watermarking algorithm are:(1) it allows blind retrieval of embedded watermark which does not need the original video;(2) the watermark is perceptually invisible;(3) it is robust against the attack of frame dropping, averaging and statistical analysis;(4) it is resistant to lossy compression; and (5) visual-audio watermark is applied. The algorithm design, evaluation, and experimentation of the proposed scheme are described in this paper.", "num_citations": "1\n", "authors": ["14"]}
{"title": "Multimedia security digital video watermarking\n", "abstract": " With the rapid growth of the Internet and the multimedia systems in distributed environments, access to multimedia data has become much easier. However, current technology does not protect their copyrights effectively. So, multimedia security becomes important issue in nowadays Internet world. In the early days, encryption and control access techniques were used to protect the ownership of media. Recently, the watermark techniques are utilized to keep the copyright. In this report, it will focus on using digital watermarking techniques to protect the digital multimedia intellectual copyright. A new algorithm of digital video watermarking is proposed. In the watermarking scheme, the watermark image in scrambled and embedded into different scene. This scheme supposes to be robust against the attack of frame dropping, averaging, statistical analysis and lossy compression.", "num_citations": "1\n", "authors": ["14"]}
{"title": "The Design, Implementation and Evaluation of an Internet Payment System\n", "abstract": " In this paper, we propose an Internet payment system which uses a payment gateway to handle the credit card payment transaction between customers, merchants and banks. To test and evaluate the payment system, we build an online travel agency called TravelNet, which simulates an real-life E-commerce application. On-line travel services including flight reservation, selling of travel accessories, tour guides, and hotel reservation are provided in TravelNet. TravelNet makes use of the proposed payment system to handle the payments transferred between customers and merchants. We implement the payment model as well as TravelNet, and conduct performance evaluation on the payment system. The performance results show that our payment system is easy-to-use, secure, and cost-effective.", "num_citations": "1\n", "authors": ["14"]}
{"title": "The mobile code paradigm and its security issues\n", "abstract": " Mobile code emerges as a new paradigm for developing non-interactive distributed applications. It requires less network communication compared with the conventional client/server paradigm, thus could reduce the bandwidth requirements of many applications, and ease network management in the ever-growing Internet. Active research is undertaken to bring the paradigm to realization, yet security concerns appear to be the major factor of its general acceptance.In this paper, we examine qualitatively the security considerations and challenges in application development with the mobile code paradigm. We identify a simple but crucial security requirement for the general acceptance of the mobile code paradigm, and evaluate the current status of mobile code development in meeting this requirement. We find that the mobile agent approach is the most interesting and challenging branch of mobile code in the security context. Therefore, we built a simple agent-based information retrieval application, the Traveling Information Agent (TIA) system, and discuss the security issues of the system in particulars.", "num_citations": "1\n", "authors": ["14"]}
{"title": "IN SEARCH OF EFFECTIVE DIVERSITY: A SIX-LANGUAGE STUDY OF FAULT-TOLERANT FLIGHT CONTROL SOFTWARE t\n", "abstract": " Multi-version software systems achieve fault tolerance through somare redundancy and diversity. In order to investigate this approach, this joint UCLAIHoneywell research project investigated multi-version sojhvare systems, employing six different programming languages to create six versions of sopare for an automatic landing program. The rationale, preparation. execution, and evaluation of this investigation are reported.", "num_citations": "1\n", "authors": ["14"]}