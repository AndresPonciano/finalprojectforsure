{"title": "A hybrid discrete firefly algorithm for solving multi-objective flexible job shop scheduling problems\n", "abstract": " Firefly algorithm (FA) is a nature-inspired optimisation algorithm that can be successfully applied to continuous optimisation problems. However, lot of practical problems are formulated as discrete optimisation problems. In this paper a hybrid discrete firefly algorithm (HDFA) is proposed to solve the multi-objective flexible job shop scheduling problem (FJSP). FJSP is an extension of the classical job shop scheduling problem that allows an operation to be processed by any machine from a given set along different routes. Three minimisation objectives - the maximum completion time, the workload of the critical machine and the total workload of all machines are considered simultaneously. This paper also proposes firefly algorithms discretisation which consists of constructing a suitable conversion of the continuous functions as attractiveness, distance and movement, into new discrete functions. In the proposed\u00a0\u2026", "num_citations": "178\n", "authors": ["1228"]}
{"title": "A hybrid discrete firefly algorithm for multi-objective flexible job shop scheduling problem with limited resource constraints\n", "abstract": " In this paper, a hybrid discrete firefly algorithm is presented to solve the multi-objective flexible job shop scheduling problem with limited resource constraints. The main constraint of this scheduling problem is that each operation of a job must follow a process sequence and each operation must be processed on an assigned machine. These constraints are used to balance between the resource limitation and machine flexibility. Three minimisation objectives\u2014the maximum completion time, the workload of the critical machine and the total workload of all machines\u2014are considered simultaneously. In this study, discrete firefly algorithm is adopted to solve the problem, in which the machine assignment and operation sequence are processed by constructing a suitable conversion of the continuous functions as attractiveness, distance and movement, into new discrete functions. Meanwhile, local search method\u00a0\u2026", "num_citations": "95\n", "authors": ["1228"]}
{"title": "SVM ranking with backward search for feature selection in type II diabetes databases\n", "abstract": " Clinical databases have accumulated large quantities of information about patients and their clinical history. Data mining is the search for relationships and patterns within this data that could provide useful knowledge for effective decision-making. Classification analysis is one of the widely adopted data mining techniques for healthcare applications to support medical diagnosis, improving quality of patient care, etc. Usually medical databases are high dimensional in nature. If a training dataset contains irrelevant features (i.e., attributes), classification analysis may produce less accurate results. Data pre-processing is required to prepare the data for data mining and machine learning to increase the predictive accuracy. Feature selection is a preprocessing technique commonly used on high-dimensional data and its purposes include reducing dimensionality, removing irrelevant and redundant features, reducing the\u00a0\u2026", "num_citations": "58\n", "authors": ["1228"]}
{"title": "Metaheuristic algorithms and probabilistic behaviour: a comprehensive analysis of Ant Colony Optimization and its variants\n", "abstract": " The application of metaheuristic algorithms to combinatorial optimization problems is on the rise and is growing rapidly now than ever before. In this paper the historical context and the conducive environment that accelerated this particular trend of inspiring analogies or metaphors from various natural phenomena are analysed. We have implemented the Ant System Model and the other variants of ACO including the 3-Opt, Max\u2013Min, Elitist and the Rank Based Systems as mentioned in their original works and we converse the missing pieces of Dorigo\u2019s Ant System Model. Extensive analysis of the variants on Travelling Salesman Problem and Job Shop Scheduling Problem shows how much they really contribute towards obtaining better solutions. The stochastic nature of these algorithms has been preserved to the maximum extent to keep the implementations as generic as possible. We observe that\u00a0\u2026", "num_citations": "47\n", "authors": ["1228"]}
{"title": "Application based brokering algorithm for optimal resource provisioning in multiple heterogeneous clouds\n", "abstract": " In recent years, adoption of cloud computing for computational needs is growing significantly due to various factors such as no upfront cost and access to latest service. In general, cloud infrastructure providers offer a wide range of services with different pricing models, instance types and a host of value-added features. Efficient selection of cloud services constitutes significant management challenges for cloud consumer, which is tedious and involves large information processing. To overcome this, the cloud brokers provide resource provisioning options that ease the task of choosing the best services based on consumers requirements and also provide a uniform management interface to access cloud services. This paper proposes a novel cloud brokering architecture that provides an optimal deployment plan for placement of virtual resources in multiple clouds. The objective of the deployment plan is to\u00a0\u2026", "num_citations": "24\n", "authors": ["1228"]}
{"title": "Metaheuristic algorithms and polynomial turing reductions: A case study based on ant colony optimization\n", "abstract": " Nowadays, there is an increasing dependence on metaheuristic algorithms for solving combinatorial optimization problems. This paper discusses various metaheuristic algorithms, their similarities and differences and how Ant Colony Optimization algorithm is found to be much more suitable for providing a generic implementation. We start with the solution for Travelling Salesman Problem using Ant Colony Optimization (ACO) and show how Polynomial Turing Reduction helps us solve Job Shop Scheduling and Knapsack Problems without making considerable changes in the implementation. The probabilistic nature of metaheuristic algorithms, especially ACO helps us to a greater extent in avoiding parameter fine-tuning. Through Sensitivity analysis we find that ACO exhibits better resilience to changes in parameter values in comparison to other metaheuristic algorithms.", "num_citations": "17\n", "authors": ["1228"]}
{"title": "Enhancing the performance of LIBSVM classifier by kernel f-score feature selection\n", "abstract": " Medical Data mining is the search for relationships and patterns within the medical datasets that could provide useful knowledge for effective clinical decisions. The inclusion of irrelevant, redundant and noisy features in the process model results in poor predictive accuracy. Much research work in data mining has gone into improving the predictive accuracy of the classifiers by applying the techniques of feature selection. Feature selection in medical data mining is appreciable as the diagnosis of the disease could be done in this patient-care activity with minimum number of significant features. The objective of this work is to show that selecting the more significant features would improve the performance of the classifier. We empirically evaluate the classification effectiveness of LibSVM classifier on the reduced feature subset of diabetes dataset. The evaluations suggest that the feature subset selected\u00a0\u2026", "num_citations": "17\n", "authors": ["1228"]}
{"title": "A conditional tree based novel algorithm for high utility itemset mining\n", "abstract": " In recent years, the problem of high utility pattern mining become one of the most important research area in data mining. The problem is challenging, due to non applicability of anti-monotone property. The existing high utility mining algorithm generates large number of candidate itemsets, which takes much time to find utility value of all candidate itemsets, especially for dense datasets. In this paper, a novel conditional high utility tree (CHUT) is proposed to compress transactional databases in two stages to reduce search space and a new algorithm called HU-Mine is proposed to mine complete set of high utility item sets. The proposed algorithm needs only two database scans in contrasts to many scans of existing algorithm. The results of the proposed work are compared with existing benchmark algorithm.", "num_citations": "14\n", "authors": ["1228"]}
{"title": "Cloud service evaluation and selection using fuzzy hybrid MCDM approach in marketplace\n", "abstract": " Cloud services are offered independently or combining two or more services to satisfy consumer requirements. Different types of cloud service providers such as direct sellers, resellers and aggregators provide services with different level of service features and quality. The selection of best suitable services involves multi-criteria nature of services to be compared with the presence of both qualitative and quantitative factors, which make it considerably more complex. To overcome this complexity, a fuzzy hybrid multi-criteria decision making approach has been proposed, which includes both qualitative and quantitative factors. Triangular fuzzy numbers are used in all pairwise comparison matrices in the Fuzzy ANP and the criteria weights are utilized by Fuzzy TOPSIS and Fuzzy ELECTRE methods to rank the alternatives. This strategy is demonstrated with selection of cloud based collaboration tool for designers\u00a0\u2026", "num_citations": "12\n", "authors": ["1228"]}
{"title": "Solving flexible job-shop scheduling problem using hybrid particle swarm optimisation algorithm and data mining\n", "abstract": " Flexible job-shop scheduling problem (FJSSP) is an extension of the classical job-shop scheduling problem that allows an operation to be processed by any machine from a given set along different routes. It is very important in both fields of production management and combinatorial optimisation. This paper presents a new approach based on a hybridisation of the particle swarm optimisation (PSO) algorithm with data mining (DM) technique to solve the multi-objective flexible job-shop scheduling problem. Three minimisation objectives \u2013 the maximum completion time, the total workload of machines and the workload of the critical machines are considered simultaneously. In this study, PSO is used to assign operations and to determine the processing order of jobs on machines. The objectives are optimised by data mining technique which extracts the knowledge from the solution sets to find the near optimal solution\u00a0\u2026", "num_citations": "12\n", "authors": ["1228"]}
{"title": "A study on optimized resource provisioning in federated cloud\n", "abstract": " Cloud computing changed the way of computing as utility services offered through public network. Selecting multiple providers for various computational requirements improves performance and minimizes cost of cloud services than choosing a single cloud provider. Federated cloud improves scalability, cost minimization, performance maximization, collaboration with other providers, multi-site deployment for fault tolerance and recovery, reliability and less energy consumption. Both providers and consumers could benefit from federated cloud where providers serve the consumers by satisfying Service Level Agreement, minimizing overall management and infrastructure cost; consumers get best services with less deployment cost and high availability. Efficient provisioning of resources to consumers in federated cloud is a challenging task. In this paper, the benefits of utilizing services from federated cloud, architecture with various coupling levels, different optimized resource provisioning methods and challenges associated with it are discussed and a comparative study is carried out over these aspects.", "num_citations": "10\n", "authors": ["1228"]}
{"title": "Novel local restart strategies with hyper-populated ant colonies for dynamic optimization problems\n", "abstract": " The emergence of novel metaheuristic algorithms and the impracticality of exact algorithms led to the increased application of stochastic optimization techniques to solve combinatorial optimization problems. Determination of population size, stopping criteria, selection of optimal parameter values, getting out of the local optima and most importantly the interplay between various parameters are yet to be addressed. In this work, the significance of population size and how it interplays with other parameters in determining the effective convergence of the system in both static and dynamic scenarios for travelling salesman problem (TSP) are explained. This work utilizes a more complex variant of introducing dynamism in TSP, by swapping existing nodes with new nodes. This work proposes novel local restart strategies for efficient search space reset during node replacements in dynamic TSP. The proposed local\u00a0\u2026", "num_citations": "9\n", "authors": ["1228"]}
{"title": "Fuzzy based quantum genetic algorithm for project team formation\n", "abstract": " Formation of an effective project team plays an important role in successful completion of the projects in organizations. As the computation involved in this task grows exponentially with the growth in the size of personnel, manual implementation is of no use. Decision support systems (DSS) developed by specialized consultants help large organizations in personnel selection process. Since, the given problem can be modelled as a combinatorial optimization problem, Genetic Algorithmic approach is preferred in building the decision making software. Fuzzy descriptors are being used to facilitate the flexible requirement specifications that indicates required team member skills. The Quantum Walk based Genetic Algorithm (QWGA) is proposed in this paper to identify near optimal teams that optimizes the fuzzy criteria obtained from the initial team requirements. Efficiency of the proposed design is tested on a variety of\u00a0\u2026", "num_citations": "7\n", "authors": ["1228"]}
{"title": "Clustering based imputation algorithm using unsupervised neural network for enhancing the quality of healthcare data\n", "abstract": " Historical and real-time healthcare data sets are valuable sources of information for predictive data analytics. However, most of the historical healthcare data sets are overloaded with challenges. One of the most frequently faced challenge is the problem of missing values, occurring because of the inaccuracies in data transmission or data entry processes. An appropriate technique for handling missing values is required to generate good quality data sets for achieving better prediction results. Removing the records with missing values, known as marginalization, poses an easy way out to this challenge. But, this will lessen the data volume of the historical data set and disturb the class balance of the data set. An alternative to marginalization is replacing missing values with plausible values, known as imputation. This paper proposes a missing value imputation technique, CLUSTIMP, using an unsupervised neural\u00a0\u2026", "num_citations": "4\n", "authors": ["1228"]}
{"title": "An efficient secure data deduplication method using radix trie with bloom filter (SDD-RT-BF) in cloud environment\n", "abstract": " Recent advancements in the domain of cloud computing (CC) and big data technologies leads to an exponential increase in cloud data, huge replica data utilized the available memory space and maximum computation brought a major issue to the restricted cloud storage space. This paper develops an effective radix trie (RT) with Bloom Filter (BF) based secure data deduplication model, abbreviated as SDD-RT-BD. The proposed SDD-RT-BF model involves three major stages namely, authorized deduplication, proof of ownership and role key update. Initially, a convergent encryption approach is applied for preventing the leakage of data and employed role re-encryption process for attaining authorized deduplication resourcefully. Specifically, management centre handles the authorized request, and establish a RT structure to map the relationship among roles and keys. Besides, BF is applied for the\u00a0\u2026", "num_citations": "3\n", "authors": ["1228"]}
{"title": "An unsupervised neural network approach for imputation of missing values in univariate time series data\n", "abstract": " Handling missing values in time series data plays a key role in predicting and forecasting, as complete and clean historical data help to achieve higher accuracy. Numerous research works are present in multivariate time series imputation, but imputation in univariate time series data is least considered due to correlated variables unavailability. This article aims to propose an iterative imputation algorithm by clustering univariate time series data, considering the trend, seasonality, cyclical, and residue features of the data. The proposed method uses a similarity based nearest neighbor imputation approach on each clusters for filling missing values. The proposed method is evaluated on publicly available data set from the data market repository and UCI repository by randomly simulating missing patterns under low, moderate, and high missingness rates throughout the data series. The proposed method's outcome is\u00a0\u2026", "num_citations": "3\n", "authors": ["1228"]}
{"title": "K\u03bcRB: Kernel microreboot mechanism\n", "abstract": " Microreboot is a widely implemented autonomic technique that imparts self-healing capability to distributed systems. This paper presents K\u03bcRB, a kernel microreboot mechanism that is proposed as a remedy to prevent application state loss after kernel crash. K\u03bcRB addresses the most significant limitations of previous approaches such as outdated application state restoration by Checkpoints and prolonged memory reservation to accommodate crash kernel by Otherworld. The framework of this mechanism builds on Kexec and KDump with minimal modifications, which enables rebooting the system with higher probability of the data being preserved. On kernel failure, modified Kexec loads fresh kernel image into main memory, KDump supports booting from nondefault boot location. The fresh kernel restores the application data and eventually replaces the memory occupied by crashed kernel.", "num_citations": "3\n", "authors": ["1228"]}
{"title": "ESKEA: Enhanced Symmetric Key Encryption Algorithm Based Secure Data Storage in Cloud Networks with Data Deduplication\n", "abstract": " Data deduplication approach is utilized in cloud storage to decrease the bandwidth of communication and storage space by eliminating the data copies from the cloud service provider (CSP). However, one of the main problems of cloud storage is data deduplication with secure data storage. To overcome this issue, the researchers presented symmetric data storage methods based on an encryption algorithm. Nonetheless, the Enhanced Symmetric Key Encryption Algorithm (ESKEA) based on secure data storage with data deduplication is proposed in this research to further improve data confidentiality. In this approach, the block-level deduplication of data is performed using the Convergent Encryption (CE) algorithm to check the CSP duplicate copies of data. Then, ESKEA algorithm is presented for secure storage of data. In ESKEA, Spider Monkey Optimization Algorithm (SMOA) optimally selects the secret\u00a0\u2026", "num_citations": "2\n", "authors": ["1228"]}
{"title": "Quantum Walk Algorithm to Compute Subgame Perfect Equilibrium in Finite Two-player Sequential Games\n", "abstract": " Subgame Perfect Equilibrium (SGPE) is a refined version of Nash equilibrium used in games of sequential nature. Computational complexity of classical approaches to compute SGPE grows exponentially with the increase in height of the game tree. In this paper, we present a quantum algorithm based on discrete-time quantum walk to compute Subgame Perfect Equilibrium (SGPE) in a finite two-player sequential game. A fullwidth game tree of average branching factor b and height h has() h n O b= nodes in it. The proposed algorithm uses (/) On b oracle queries to backtrack to the solution. The resultant speed-up is () O b times better than the best known classical approach, Zermelo's algorithm.", "num_citations": "2\n", "authors": ["1228"]}
{"title": "Firefly algorithm for static task scheduling problem'\n", "abstract": " Effective resource utilization can be achieved through proper scheduling of application tasks, which helps to attain better performance in the heterogeneous environment. Firefly algorithm, an efficient meta-heuristic algorithm is used in this paper to solve the task scheduling problem. This approach aims to generate optimal task schedule so as to get minimum completion time while executing application tasks. The efficiency of the algorithm with respect to makespan is compared with the existing particle swarm intelligence algorithm. The experimental results show that the firefly algorithm based approach outperforms PSO algorithm.", "num_citations": "2\n", "authors": ["1228"]}
{"title": "Feature subset selection using Nomogram in Type II Diabetes databases\n", "abstract": " Advancement in data mining and machine learning has promoted computer-based approaches such as Computer-aided diagnosis, Expert systems and Prognostic studies in medical applications. Medical data are processed and analyzed using data mining techniques to derive useful knowledge. These data are multidimensional, and represented by a large number of features. The irrelevant and redundant features among them may negatively impact the performance of the data mining algorithms. Feature selection identifies the features that improve the predictive accuracy of the classifiers. The proposed work focuses on identifying the significant features that influence the predictive accuracy of the Na\u00efve Bayes Classifier using the visualization tool, Nomogram. The effect of each feature on the performance of the classifier is analyzed using nomogram and an optimal feature subset that enhances the predictive accuracy is derived. The proposed method, Nomogram-RFE, is experimented with Pima Indians Diabetes Dataset and the performance of the classifier is evaluated on five criteria: classification accuracy, sensitivity, specificity, the area under the receiver operating characteristic and Brier score. The experimental results show that the optimal feature subset derived enhances the predictive power of a classifier and reduces false positive and false negative rates as measured by the sensitivity and specificity of the classifier. A low Brier score for the optimal feature subset indicates lower deviation between the predicted probability and the actual outcome.", "num_citations": "2\n", "authors": ["1228"]}
{"title": "Quantum Walk based genetic algorithm for 0\u20131 quadratic knapsack problem\n", "abstract": " This paper proposes a genetic quantum algorithm based on discrete time quantum walk (QWGA) to solve 0-1 quadratic knapsack problem. Genetic Quantum Algorithms makes use of the qubit representation and superposition phenomenon which are the counter-intuitive characteristics of quantum mechanics. Discrete Quantum Walk (DQW) on a hypercube is used in the place of genetic operators like mutation, crossover, etc. Achievement of rapid convergence and avoidance of local optima with the help of the quantum principles is explained in this paper. The possibilities of extending the proposed algorithm to various combinatorial optimization problems is discussed in detail. Superiority of the proposed algorithm over genetic quantum algorithm based on rotation operators is evident from the results.", "num_citations": "1\n", "authors": ["1228"]}
{"title": "A Novel Algorithm for Mining Hybrid-Dimensional Association Rules\n", "abstract": " Association rule mining is a fundamental and vital functionality of data mining. Most of the existing real time transactional databases are multidimensional in nature. In this paper, a novel algorithm is proposed for mining hybrid-dimensional association rules which are very useful in business decision making. The proposed algorithm uses multi index structures to store necessary details like item combination, support measure and transaction IDs, which stores all frequent 1-itemsets after scanning the entire database first time. Frequent k-itemsets are generated with previous level data, without scanning the database further. Compared to traditional algorithms, this algorithm efficiently finds association rules in multidimensional datasets, by scanning the database only once, thus enhancing the process of data mining.", "num_citations": "1\n", "authors": ["1228"]}
{"title": "Imputation of Missing Data: A Semi-Supervised Clustering methodology.\n", "abstract": " Data mining is being applied with success in different fields of human endeavor including Marketing, Customer Relationship Management and Healthcare. Real world data sets are always accompanied by missing data, a major factor affecting data quality. Missing data has been a pervasive problem in data analysis since the origin of data collection. In the knowledge discovery process, missing data introduces bias in the model evaluation and leads to inaccurate mining results. The objective of this research is to propose a Semi-Supervised Clustering methodology for the imputation of missing data in databases. For this purpose, missing data are simulated on the complete Pima Indians Type II Diabetes dataset in order to evaluate the performance of the proposed algorithm. The performance is compared with other existing imputation methods. The comparative analysis shows that the proposed method produces\u00a0\u2026", "num_citations": "1\n", "authors": ["1228"]}
{"title": "Genetic Chromo Dynamic Framework for Imputation of Missing Data in Type II Diabetes Databases\n", "abstract": " Genetic Chromo dynamic framework GCFIT_MIS_IMPUTE is proposed for the imputation of missing data. The framework is experimented on Pima Indian Type II Diabetes Dataset and the performance is evaluated using Average Imputation Error.", "num_citations": "1\n", "authors": ["1228"]}