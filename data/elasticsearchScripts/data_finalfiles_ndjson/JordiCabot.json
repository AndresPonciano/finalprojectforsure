{"title": "Verification of UML/OCL class diagrams using constraint programming\n", "abstract": " In the MDD and MDA approaches, models become the primary artifacts of the development process. Therefore, assessment of the correctness of such models is a key issue to ensure the quality of the final application. In that sense, this paper presents an automatic method that uses the Constraint Programming paradigm to verify UML class diagrams extended with OCL constraints. In our approach, both class diagrams and OCL constraints are translated into a Constraint Satisfaction Problem. Then, compliance of the diagram with respect to several correctness proper- ties such as weak and strong satisfiability or absence of constraint redundancies can be formally verified.", "num_citations": "282\n", "authors": ["835"]}
{"title": "MoDisco: a generic and extensible framework for model driven reverse engineering\n", "abstract": " Nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. Many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. As a consequence, reverse engineering still remains a major challenge for software engineering today. This paper introduces MoDisco, a generic and extensible open source reverse engineering solution. MoDisco intensively uses MDE principles and techniques to improve existing approaches for reverse engineering.", "num_citations": "253\n", "authors": ["835"]}
{"title": "Modisco: A model driven reverse engineering framework\n", "abstract": " ContextMost companies, independently of their size and activity type, are facing the problem of managing, maintaining and/or replacing (part of) their existing software systems. These legacy systems are often large applications playing a critical role in the company\u2019s information system and with a non-negligible impact on its daily operations. Improving their comprehension (e.g., architecture, features, enforced rules, handled data) is a key point when dealing with their evolution/modernization.ObjectiveThe process of obtaining useful higher-level representations of (legacy) systems is called reverse engineering (RE), and remains a complex goal to achieve. So-called Model Driven Reverse Engineering (MDRE) has been proposed to enhance more traditional RE processes. However, generic and extensible MDRE solutions potentially addressing several kinds of scenarios relying on different legacy technologies are\u00a0\u2026", "num_citations": "249\n", "authors": ["835"]}
{"title": "UMLtoCSP: a tool for the formal verification of UML/OCL models using constraint programming\n", "abstract": " We present UMLtoCSP, a tool for the formal verification of UML/OCL models. Given a UML class diagram annotated with OCL constraints, UMLtoCSP is able to automatically check several correctness properties, such as the strong and weak satisfiability of the model or the lack of redundant constraints. The tool uses Constraint Logic Programming as the underlying formalism and the constraint solver ECL i PS e as the verification engine.", "num_citations": "228\n", "authors": ["835"]}
{"title": "Verification and validation of declarative model-to-model transformations through invariants\n", "abstract": " In this paper we propose a method to derive OCL invariants from declarative model-to-model transformations in order to enable their verification and analysis. For this purpose we have defined a number of invariant-based verification properties which provide increasing degrees of confidence about transformation correctness, such as whether a rule (or the whole transformation) is satisfiable by some model, executable or total. We also provide some heuristics for generating meaningful scenarios that can be used to semi-automatically validate the transformations.As a proof of concept, the method is instantiated for two prominent model-to-model transformation languages: Triple Graph Grammars and QVT.", "num_citations": "192\n", "authors": ["835"]}
{"title": "On the verification of UML/OCL class diagrams using constraint programming\n", "abstract": " Assessment of the correctness of software models is a key issue to ensure the quality of the final application. To this end, this paper presents an automatic method for the verification of UML class diagrams extended with OCL constraints. Our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. The method works by translating the UML/OCL model into a Constraint Satisfaction Problem (CSP) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. Our approach is particularly relevant to current MDA and MDD methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application.", "num_citations": "138\n", "authors": ["835"]}
{"title": "From UML/OCL to SBVR specifications: A challenging transformation\n", "abstract": " UML is currently the most widely used modeling language for the specification of the conceptual schema (CS) of an information system (IS). However, UML falls short when it comes to allow business people to define in their own language (e.g. using their own terms in natural language) the policies and rules by which they run their business. To this purpose, the semantics of business vocabulary and business rules (SBVR) metamodel specification was proposed. SBVR is conceptualized optimally for business people and it is designed to be used for business purposes, independently of information systems designs.Clearly, SBVR and unified modeling language (UML) cannot be considered as isolated languages. Many of the business rules specified by business people must be automatically executed by the underlying information system, and thus, they must also appear in its UML CS. In this sense, the main goal of\u00a0\u2026", "num_citations": "122\n", "authors": ["835"]}
{"title": "How do software architects consider non-functional requirements: An exploratory study\n", "abstract": " Dealing with non-functional requirements (NFRs) has posed a challenge onto software engineers for many years. Over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. Knowing more about the state of the practice on these topics may benefit both practitioners' and researchers' daily work. A few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that NFRs have on daily architects' practices. This paper presents some of the findings of an empirical study based on 13 interviews with software architects. It addresses questions such as: who decides the NFRs, what types of NFRs matter to architects, how are NFRs documented, and how are NFRs validated. The results are contextualized with existing previous work.", "num_citations": "120\n", "authors": ["835"]}
{"title": "EMFtoCSP: A tool for the lightweight verification of EMF models\n", "abstract": " The increasing popularity of MDE results in the creation of larger models and model transformations, hence converting the specification of MDE artefacts in an error-prone task. Therefore, mechanisms to ensure quality and absence of errors in models are needed to assure the reliability of the MDE-based development process. Formal methods have proven their worth in the verification of software and hardware systems. However, the adoption of formal methods as a valid alternative to ensure model correctness is compromised for the inner complexity of the problem. To circumvent this complexity, it is common to impose limitations such as reducing the type of constructs that can appear in the model, or turning the verification process from automatic into user assisted. Since we consider these limitations to be counterproductive for the adoption of formal methods, in this paper we present EMFtoCSP, a new tool for the\u00a0\u2026", "num_citations": "117\n", "authors": ["835"]}
{"title": "Incremental integrity checking of UML/OCL conceptual schemas\n", "abstract": " Integrity constraints play a key role in the specification and development of software systems since they state conditions that must always be satisfied by the system at runtime. Therefore, software systems must include some kind of integrity checking component that ensures that all constraints still hold after the execution of any operation that modifies the system state. Integrity checking must be as efficient as possible not to seriously slow down the system performance at runtime. In this sense, this paper proposes a set of techniques to facilitate the efficient integrity checking of UML-based software specifications, usually complemented with a set of integrity constraints defined in Object Constraint Language (OCL) to express all rules that cannot be graphically defined. In particular, our techniques are able to determine, at design-time, when and how each constraint must be checked at runtime to avoid irrelevant\u00a0\u2026", "num_citations": "112\n", "authors": ["835"]}
{"title": "Dealing with non-functional requirements in model-driven development\n", "abstract": " The impact of non-functional requirements (NFRs) over software systems has been widely documented. Consequently, cost-effective software production method shall provide means to integrate this type of requirements into the development process. In this vision paper we analyze this assumption over a particular type of software production paradigm: model-driven development (MDD). We report first the current state of MDD approaches with respect to NFRs and remark that, in general, NFRs are not addressed in MDD methods and processes, and we discuss the effects of this situation. Next, we outline a general framework that integrates NFRs into the core of the MDD process and provide a detailed comparison among all the MDD approaches considered. Last, we identify some research issues related to this framework.", "num_citations": "105\n", "authors": ["835"]}
{"title": "A systematic mapping study of software development with GitHub\n", "abstract": " Context: GitHub, nowadays the most popular social coding platform, has become the reference for mining Open Source repositories, a growing research trend aiming at learning from previous software projects to improve the development of new ones. In the last years, a considerable amount of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe that it is worthwhile to reflect on how research papers have addressed the task of mining GitHub and what findings they have reported. Objective: The main objective of this paper is to identify the quantity, topic, and empirical methods of research works, targeting the analysis of how software development practices are influenced by the use of a distributed social coding platform like GitHub\u00a0\u2026", "num_citations": "103\n", "authors": ["835"]}
{"title": "Non-functional requirements in architectural decision making\n", "abstract": " Software architects often must work with incomplete or ill-specified non-functional requirements (NFRs) and use them to make decisions. Through this process, existing NFRs are refined or modified and new ones emerge. Although much research has centered on how software architects treat NFRs, no empirical studies have investigated the state of the practice. A survey based on interviews with 13 software architects addressed two fundamental issues: how do architects face NFRs from an engineering perspective, and how do NFRs influence their decision-making? The survey revealed that architects usually elicit NFRs themselves in an iterative process; they usually don't document the NFRs and only partially validate them.", "num_citations": "91\n", "authors": ["835"]}
{"title": "Combining model-driven engineering and cloud computing\n", "abstract": " Service-orientation and model-driven engineering are two of the most dominant software engineering paradigms nowadays. This position paper explores the synergies between them and show how they can benefit from each other. In particular, the paper introduces the notion of Modeling as a Service (MaaS) as a way to provide modeling and model-driven engineering services from the cloud.", "num_citations": "88\n", "authors": ["835"]}
{"title": "Feature-based classification of bidirectional transformation approaches\n", "abstract": " Bidirectional model transformation is a key technology in model-driven engineering (MDE), when two models that can change over time have to be kept constantly consistent with each other. While several model transformation tools include at least a partial support to bidirectionality, it is not clear how these bidirectional capabilities relate to each other and to similar classical problems in computer science, from the view update problem in databases to bidirectional graph transformations. This paper tries to clarify and visualize the space of design choices for bidirectional transformations from an MDE point of view, in the form of a feature model. The selected list of existing approaches are characterized by mapping them to the feature model. Then, the feature model is used to highlight some unexplored research lines in bidirectional transformations.", "num_citations": "82\n", "authors": ["835"]}
{"title": "Transformation techniques for OCL constraints\n", "abstract": " Constraints play a key role in the definition of conceptual schemas. In the UML, constraints are usually specified by means of invariants written in the OCL. However, due to the high expressiveness of the OCL, the designer has different syntactic alternatives to express each constraint. The techniques presented in this paper assist the designer during the definition of the constraints by means of generating equivalent alternatives for the initially defined ones. Moreover, in the context of the MDA, transformations between these different alternatives are required as part of the PIM-to-PIM, PIM-to-PSM or PIM-to-code transformations of the original conceptual schema.", "num_citations": "76\n", "authors": ["835"]}
{"title": "On verifying ATL transformations using \u2018off-the-shelf\u2019SMT solvers\n", "abstract": " MDE is a software development process where models constitute pivotal elements of the software to be built. If models are well-specified, transformations can be employed for various purposes, e.g., to produce final code. However, transformations are only meaningful when they are \u2018correct\u2019: they must produce valid models from valid input models. A valid model has conformance to its meta-model and fulfils its constraints, usually written in OCL. In this paper, we propose a novel methodology to perform automatic, unbounded verification of ATL transformations. Its main component is a novel first-order semantics for ATL transformations, based on the interpretation of the corresponding rules and their execution semantics as first-order predicates. Although, our semantics is not complete, it does cover a significant subset of the ATL language. Using this semantics, transformation correctness can be automatically\u00a0\u2026", "num_citations": "73\n", "authors": ["835"]}
{"title": "Verifying UML/OCL operation contracts\n", "abstract": " In current model-driven development approaches, software models are the primary artifacts of the development process. Therefore, assessment of their correctness is a key issue to ensure the quality of the final application. Research on model consistency has focused mostly on the models\u2019 static aspects. Instead, this paper addresses the verification of their dynamic aspects, expressed as a set of operations defined by means of pre/postcondition contracts.               This paper presents an automatic method based on Constraint Programming to verify UML models extended with OCL constraints and operation contracts. In our approach, both static and dynamic aspects are translated into a Constraint Satisfaction Problem. Then, compliance of the operations with respect to several correctness properties such as operation executability or determinism are formally verified.", "num_citations": "72\n", "authors": ["835"]}
{"title": "Formal verification of static software models in MDE: A systematic review\n", "abstract": " ContextModel-driven Engineering (MDE) promotes the utilization of models as primary artifacts in all software engineering activities. Therefore, mechanisms to ensure model correctness become crucial, specially when applying MDE to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. Clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. Formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years.ObjectiveThe objective of this paper is to analyze the state of the art in the field of formal verification of models, restricting the\u00a0\u2026", "num_citations": "68\n", "authors": ["835"]}
{"title": "Findings from GitHub: methods, datasets and limitations\n", "abstract": " GitHub, one of the most popular social coding platforms, is the platform of reference when mining Open Source repositories to learn from past experiences. In the last years, a number of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe it is worthwhile to reflect how research papers have addressed the task of mining GitHub repositories over the last years. In this regard, we present a meta-analysis of 93 research papers which addresses three main dimensions of those papers: i) the empirical methods employed, ii) the datasets they used and iii) the limitations reported. Results of our meta-analysis show some concerns regarding the dataset collection process and size, the low level of replicability, poor sampling techniques, lack of\u00a0\u2026", "num_citations": "67\n", "authors": ["835"]}
{"title": "Towards a general composition semantics for rule-based model transformation\n", "abstract": " As model transformations have become an integral part of the automated software engineering lifecycle, reuse, modularisation, and composition of model transformations becomes important. One way to compose model transformations is to compose modules of transformation rules, and execute the composition as one transformation (internal composition). This kind of composition can provide fine-grained semantics, as it is part of the transformation language. This paper aims to generalise two internal composition mechanisms for rule-based transformation languages, module import and rule inheritance, by providing executable semantics for the composition mechanisms within a virtual machine. The generality of the virtual machine is demonstrated for different rule-based transformation languages by compiling those languages to, and executing them on this virtual machine. We will discuss how ATL and\u00a0\u2026", "num_citations": "67\n", "authors": ["835"]}
{"title": "Improving higher-order transformations support in ATL\n", "abstract": " In Model-Driven Engineering (MDE), Higher-Order Transformations (HOTs) are model transformations that analyze, produce or manipulate other model transformations. In a previous survey we classified them, and showed their usefulness in different MDE scenarios. However, writing HOTs is generally considered a time-consuming and error-prone task, and often results in verbose code.               In this paper we present several proposals to facilitate the definition of HOTs in ATL. Each proposal focuses on a specific kind of scenario. We validate our proposals by assessing their impact over the full list of HOTs described in the survey.", "num_citations": "67\n", "authors": ["835"]}
{"title": "Discovering implicit schemas in JSON data\n", "abstract": " JSON has become a very popular lightweigth format for data exchange. JSON is human readable and easy for computers to parse and use. However, JSON is schemaless. Though this brings some benefits (e.g., flexibility in the representation of the data) it can become a problem when consuming and integrating data from different JSON services since developers need to be aware of the structure of the schemaless data. We believe that a mechanism to discover (and visualize) the implicit schema of the JSON data would largely facilitate the creation and usage of JSON services. For instance, this would help developers to understand the links between a set of services belonging to the same domain or API. In this sense, we propose a model-based approach to generate the underlying schema of a set of JSON documents.", "num_citations": "65\n", "authors": ["835"]}
{"title": "ATLTest: A white-box test generation approach for ATL transformations\n", "abstract": " MDE is being applied to the development of increasingly complex systems that require larger model transformations. Given that the specification of such transformations is an error-prone task, techniques to guarantee their quality must be provided. Testing is a well-known technique for finding errors in programs. In this sense, adoption of testing techniques in the model transformation domain would be helpful to improve their quality. So far, testing of model transformations has focused on black-box testing techniques. Instead, in this paper we provide a white-box test model generation approach for ATL model transformations.", "num_citations": "65\n", "authors": ["835"]}
{"title": "UMLtoGraphDB: mapping conceptual schemas to graph databases\n", "abstract": " The need to store and manipulate large volume of (unstructured) data has led to the development of several NoSQL databases for better scalability. Graph databases are a particular kind of NoSQL databases that have proven their efficiency to store and query highly interconnected data, and have become a promising solution for multiple applications. While the mapping of conceptual schemas to relational databases is a well-studied field of research, there are only few solutions that target conceptual modeling for NoSQL databases and even less focusing on graph databases. This is specially true when dealing with the mapping of business rules and constraints in the conceptual schema. In this article we describe a mapping from UML/OCL conceptual schemas to Blueprints, an abstraction layer on top of a variety of graph databases, and Gremlin, a graph traversal language, via an intermediate Graph\u00a0\u2026", "num_citations": "64\n", "authors": ["835"]}
{"title": "Constraint support in MDA tools: A survey\n", "abstract": " The growing interest in the MDA (Model-Driven Architecture) and MDD (Model-Driven Development) approaches has largely increased the number of tools and methods including code-generation capabilities. Given a platform-independent model (PIM) of an application, these tools generate (part of) the application code either by defining first a platform-specific model or by executing a direct PIM to code transformation. However, current tools present several limitations regarding code generation of the integrity constraints defined in the PIMs. This paper compares these tools and shows that they lack expressiveness in the kind of constraints they can handle or efficiency in the code generated to verify them. Based on this evaluation, the features of an ideal code-generation method for integrity constraints are established. We believe such a method is required to extend MDA adoption in the development of\u00a0\u2026", "num_citations": "62\n", "authors": ["835"]}
{"title": "A feature-based survey of model view approaches\n", "abstract": " When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to define and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designing and applying such solutions in order to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and propose a corresponding research\u00a0\u2026", "num_citations": "59\n", "authors": ["835"]}
{"title": "Assessing the bus factor of Git repositories\n", "abstract": " Software development projects face a lot of risks (requirements inflation, poor scheduling, technical problems, etc.). Underestimating those risks may put in danger the project success. One of the most critical risks is the employee turnover, that is the risk of key personnel leaving the project. A good indicator to evaluate this risk is to measure the concentration of information in individual developers. This is also popularly known as the bus factor (\u201cnumber of key developers who would need to be incapacitated, i.e. hit by a bus, to make a project unable to proceed\u201d). Despite the simplicity of the concept, calculating the actual bus factor for specific projects can quickly turn into an error-prone and time-consuming activity as soon as the size of the project and development team increase. In order to help project managers to assess the bus factor of their projects, in this paper we present a tool that, given a Git-based repository\u00a0\u2026", "num_citations": "58\n", "authors": ["835"]}
{"title": "Incremental evaluation of OCL constraints\n", "abstract": " Integrity checking is aimed at determining whether an operation execution violates a given integrity constraint. To perform this computation efficiently, several incremental methods have been developed. The main goal of these methods is to consider as few of the entities in an information base as possible, which is generally achieved by reasoning from the structural events that define the effect of the operations. In this paper, we propose a new method for dealing with the incremental evaluation of the OCL integrity constraints specified in UML conceptual schemas. Since our method works at a conceptual level, its results are useful in efficiently evaluating constraints regardless of the technology platform in which the conceptual schema is to be implemented.", "num_citations": "58\n", "authors": ["835"]}
{"title": "Exploring the use of labels to categorize issues in open-source software projects\n", "abstract": " Reporting bugs, asking for new features and in general giving any kind of feedback is a common way to contribute to an Open-Source Software (OSS) project. This feedback is generally reported in the form of new issues for the project, managed by the so-called issue-trackers. One of the features provided by most issue-trackers is the possibility to define a set of labels/tags to classify the issues and, at least in theory, facilitate their management. Nevertheless, there is little empirical evidence to confirm that taking the time to categorize new issues has indeed a beneficial impact on the project evolution. In this paper we analyze a population of more than three million of GitHub projects and give some insights on how labels are used in them. Our preliminary results reveal that, even if the label mechanism is scarcely used, using labels favors the resolution of issues. Our analysis also suggests that not all projects use\u00a0\u2026", "num_citations": "57\n", "authors": ["835"]}
{"title": "Emf views: A view mechanism for integrating heterogeneous models\n", "abstract": " Modeling complex systems involves dealing with several heterogeneous and interrelated models defined using a variety of languages (UML, ER, BPMN, DSLs, etc.). These models must be frequently combined in different cross-domain perspectives to provide stakeholders the view of the system they need to best perform their tasks. Several model composition approaches have already been proposed addressing this problem. Nevertheless, they present some important limitations concerning efficiency, interoperability and synchronization between the base models and the composed ones. As an alternative we introduce EMF Views, an approach coming with a dedicated language and tooling for defining views on potentially heterogeneous models. Similarly to views in databases, model views are not materialized but instead redirect all model access and manipulation requests to the base models from\u00a0\u2026", "num_citations": "56\n", "authors": ["835"]}
{"title": "A Catalogue of Refactorings for Model-to-Model Transformations.\n", "abstract": " In object-oriented programming, continuous refactorings are used as the main mechanism to increase the maintainability of the code base. Unfortunately, in the field of model transformations, such refactoring support is so far missing. This paper tackles this limitation by adapting the notion of refactorings to model-to-model (M2M) transformations. In particular, we present a dedicated catalogue of refactorings for improving the quality of M2M transformations. The refactorings have been explored by analyzing existing transformation examples defined in ATL. However, the refactorings are not specifically tailored to ATL, but applicable also to other M2M transformation languages.", "num_citations": "52\n", "authors": ["835"]}
{"title": "EMF-REST: generation of RESTful APIs from models\n", "abstract": " In the last years, there has been an increasing interest for Model-Driven Engineering (MDE) solutions in the Web. Web-based modeling solutions can leverage on better support for distributed management (ie, the Cloud) and collaboration. However, current modeling environments and frameworks are usually restricted to desktop-based scenarios and therefore their capabilities to move to the Web are still very limited. In this paper we present an approach to generate Web APIs out of models, thus paving the way for managing models and collaborating on them online. The approach, called EMF-REST, takes Eclipse Modeling Framework (EMF) data models as input and generates Web APIs following the REST principles and relying on well-known libraries and standards, thus facilitating its comprehension and maintainability. Also, EMF-REST integrates model and Web-specific features to provide model validation and\u00a0\u2026", "num_citations": "49\n", "authors": ["835"]}
{"title": "NeoEMF: A multi-database model persistence framework for very large models\n", "abstract": " The growing role of Model Driven Engineering (MDE) techniques in industry has emphasized scalability of existing model persistence solutions as a major issue. Specifically, there is a need to store, query, and transform very large models in an efficient way. Several persistence solutions based on relational and NoSQL databases have been proposed to achieve scalability. However, they often rely on a single data store, which suits a specific modeling activity, but may not be optimized for other use cases. This paper presents NeoEMF, a tool that tackles this issue by providing a multi-database model persistence framework. Tool website: http://www.neoemf.com", "num_citations": "47\n", "authors": ["835"]}
{"title": "Xatkit: a multimodal low-code chatbot development framework\n", "abstract": " Chatbot (and voicebot) applications are increasingly adopted in various domains such as e-commerce or customer services as a direct communication channel between companies and end-users. Multiple frameworks have been developed to ease their definition and deployment. While these frameworks are efficient to design simple chatbot applications, they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs (e.g. it is typically impossible to change the NL engine provider). In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, especially back-end connections, increasing the development and maintenance costs. In this paper, we introduce the Xatkit framework. Xatkit tackles these issues by providing a set of Domain Specific Languages to define chatbots (and voicebots and\u00a0\u2026", "num_citations": "44\n", "authors": ["835"]}
{"title": "Lazy execution of model-to-model transformations\n", "abstract": " The increasing adoption of Model-Driven Engineering in industrial contexts highlights scalability as a critical limitation of several MDE tools. Most of the current model-to-model transformation engines have been designed for one-shot translation of input models to output models, and present efficiency issues when applied to very large models. In this paper, we study the application of a lazy-evaluation approach to model transformations. We present a lazy execution algorithm for ATL, and we empirically evaluate a prototype implementation. With it, the elements of the target model are generated only when (and if) they are accessed, enabling also transformations that generate infinite target models. We achieve our goal on a significant subset of ATL by extending the ATL compiler.", "num_citations": "43\n", "authors": ["835"]}
{"title": "From declarative to imperative UML/OCL operation specifications\n", "abstract": " An information system maintains a representation of the state of the domain in its Information Base (IB). The state of the IB changes due to the execution of the operations defined in the behavioral schema. There are two different approaches for specifying the effect of an operation: the imperative and the declarative approaches. In conceptual modeling, the declarative approach is preferable since it allows a more abstract and concise definition of the operation effect and conceals all implementation issues. Nevertheless, in order to execute the conceptual schema, declarative specifications must be transformed into equivalent imperative ones.               Unfortunately, declarative specifications may be non-deterministic. This implies that there may be several equivalent imperative versions for the same declarative specification, which hampers the transformation process. The main goal of this paper is to provide a\u00a0\u2026", "num_citations": "43\n", "authors": ["835"]}
{"title": "Distributed Model-to-model Transformation with ATL on MapReduce\n", "abstract": " Efficient processing of very large models is a key requirement for the adoption of Model-Driven Engineering (MDE) in some industrial contexts. One of the central operations in MDE is rule-based model transformation (MT). It is used to specify manipulation operations over structured data coming in the form of model graphs. However, being based on computationally expensive operations like subgraph isomorphism, MT tools are facing issues on both memory occupancy and execution time while dealing with the increasing model size and complexity. One way to overcome these issues is to exploit the wide availability of distributed clusters in the Cloud for the distributed execution of MT. In this paper, we propose an approach to automatically distribute the execution of model transformations written in a popular MT language, ATL, on top of a well-known distributed programming model, MapReduce. We show how the\u00a0\u2026", "num_citations": "42\n", "authors": ["835"]}
{"title": "Automatic generation of test cases for REST APIs: A specification-based approach\n", "abstract": " The REpresentation State Transfer (REST) has gained momentum as the preferred technique to design Web APIs. REST allows building loosely coupled systems by relying on HTTP and the Web-friendly format JSON. However, REST is not backed by any standard or specification to describe how to create/consume REST APIs, thus creating new challenges for their integration, testing and verification. To face this situation, several specification formats have been proposed (e.g., OpenAPI, RAML, and API Blueprint), which can help automate tasks in REST API development (e.g., testing) and consumption (e.g., SDKs generation). In this paper we focus on automated REST API testing relying on API specifications, and particularly the OpenAPI one. We propose an approach to generate specification-based test cases for REST APIs to make sure that such APIs meet the requirements defined in their specifications. We\u00a0\u2026", "num_citations": "41\n", "authors": ["835"]}
{"title": "Map-based transparent persistence for very large models\n", "abstract": " The progressive industrial adoption of Model-Driven Engineering (MDE) is fostering the development of large tool ecosystems like the Eclipse Modeling project. These tools are built on top of a set of base technologies that have been primarily designed for small-scale scenarios, where models are manually developed. In particular, efficient runtime manipulation for large-scale models is an under-studied problem and this is hampering the application of MDE to several industrial scenarios.                 In this paper we introduce and evaluate a map-based persistence model for MDE tools. We use this model to build a transparent persistence layer for modeling tools, on top of a map-based database engine. The layer can be plugged into the Eclipse Modeling Framework, lowering execution times and memory consumption levels of other existing approaches. Empirical tests are performed based on a typical\u00a0\u2026", "num_citations": "41\n", "authors": ["835"]}
{"title": "Towards model driven tool interoperability: Bridging eclipse and microsoft modeling tools\n", "abstract": " Successful application of model-driven engineering approaches requires interchanging a lot of relevant data among the tool ecosystem employed by an engineering team (e.g., requirements elicitation tools, several kinds of modeling tools, reverse engineering tools, development platforms and so on). Unfortunately, this is not a trivial task. Poor tool interoperability makes data interchange a challenge even among tools with a similar scope. This paper presents a model-based solution to overcome such interoperability issues. With our approach, the internal schema/s (i.e., metamodel/s) of each tool are explicited and used as basis for solving syntactic and semantic differences between the tools. Once the corresponding metamodels are aligned, model-to-model transformations are (semi)automatically derived and executed to perform the actual data interchange. We illustrate our approach by bridging the\u00a0\u2026", "num_citations": "41\n", "authors": ["835"]}
{"title": "JSONDiscoverer: Visualizing the schema lurking behind JSON documents\n", "abstract": " The so-called API economy is pushing more and more companies to provide open Web APIs to access their data, typically using the JavaScript Object Notation (JSON) as interchange data format. While JSON has been designed to be easy to read and parse, their structure is implicit. This poses a serious problem when consuming and integrating Web APIs from different sources since it forces us to manually analyze each individual API in detail. This paper presents JSONDiscoverer, a tool that alleviates this problem by discovering (and visualizing) the implicit schema of JSON documents as well as possible composition links among JSON-based Web APIs. Tool website: http://som-research. uoc. edu/tools/jsonDiscoverer", "num_citations": "40\n", "authors": ["835"]}
{"title": "Mogwa\u00ef: a framework to handle complex queries on large models\n", "abstract": " While Model Driven Engineering is gaining more industrial interest, scalability issues when managing large models have become a major problem in current modeling frameworks. Scalable model persistence has been achieved by using NoSQL backends for model storage, but existing modeling framework APIs have not evolved accordingly, limiting NoSQL query performance benefits. In this paper we present the Mogwa\u00ef, a scalable and efficient model query framework based on a direct translation of OCL queries to Gremlin, a query language supported by several NoSQL databases. Generated Gremlin expressions are computed inside the database itself, bypassing limitations of existing framework APIs and improving overall performance, as confirmed by our experimental results showing an improvement of execution time up to a factor of 20 and a reduction of the memory overhead up to a factor of 75 for large\u00a0\u2026", "num_citations": "38\n", "authors": ["835"]}
{"title": "Management of stateful firewall misconfiguration\n", "abstract": " Firewall configurations are evolving into dynamic policies that depend on protocol states. As a result, stateful configurations tend to be much more error prone. Some errors occur on configurations that only contain stateful rules. Others may affect those holding both stateful and stateless rules. Such situations lead to configurations in which actions on certain packets are conducted by the firewall, while other related actions are not. We address automatic solutions to handle these problems. Permitted states and transitions of connection-oriented protocols (in essence, on any layer) are encoded as automata. Flawed rules are identified and potential modifications are provided in order to get consistent configurations. We validate the feasibility of our proposal based on a proof of concept prototype that automatically parses existing firewall configuration files and handles the discovery of flawed rules according to our\u00a0\u2026", "num_citations": "36\n", "authors": ["835"]}
{"title": "Enabling the collaborative definition of DSMLs\n", "abstract": " Software development processes are collaborative in nature. Neglecting the key role of end-users leads to software that does not satisfy their needs. This collaboration becomes specially important when creating Domain-Specific Modeling Languages (DSMLs), which are (modeling) languages specifically designed to carry out the tasks of a particular domain. While end-users are actually the experts of the domain for which a DSML is developed, their participation in the DSML specification process is still rather limited nowadays. In this paper we propose a more community-aware language development process by enabling the active participation of all community members (both developers and end-users of the DSML) from the very beginning. Our proposal is based on a DSML itself, called Collaboro, which allows representing change proposals on the DSML design and discussing (and tracing back\u00a0\u2026", "num_citations": "36\n", "authors": ["835"]}
{"title": "On validation of ATL transformation rules by transformation models\n", "abstract": " Model-to-model transformations constitute an important ingredient in model-driven engineering. As real world transformations are complex, systematic approaches are required to ensure their correctness. The ATLAS Transformation Language (ATL) is a mature transformation language which has been successfully applied in several areas. However, the executable nature of ATL is a barrier for the validation of transformations. In contrast, transformation models provide an integrated structural description of the source and target metamodels and the transformation between them. While not being executable, transformation models are well-suited for analysis and verification of transformation properties. In this paper, we discuss (a) how ATL transformations can be translated into equivalent transformation models and (b) illustrate how these surrogates can be employed to validate properties of the original transformation.", "num_citations": "36\n", "authors": ["835"]}
{"title": "MoScript: A DSL for querying and manipulating model repositories\n", "abstract": " Growing adoption of Model-Driven Engineering has hugely increased the number of modelling artefacts (models, metamodels, transformations, ...) to be managed. Therefore, development teams require appropriate tools to search and manipulate models stored in model repositories, e.g. to find and reuse models or model fragments from previous projects. Unfortunately, current approaches for model management are either ad-hoc (i.e., tied to specific types of repositories and/or models), do not support complex queries (e.g., based on the model structure and its relationship with other modelling artefacts) or do not allow the manipulation of the resulting models (e.g., inspect, transform). This hinders the probability of efficiently reusing existing models or fragments thereof. In this paper we introduce MoScript, a textual domain-specific language for model management. With MoScript, users can write scripts\u00a0\u2026", "num_citations": "35\n", "authors": ["835"]}
{"title": "From UML profiles to EMF profiles and beyond\n", "abstract": " Domain-Specific Modeling Languages (DSMLs) are getting more and more attention as a key element of Model Driven Engineering. As any other software artefact, DSMLs should continuously evolve to adapt to the changing needs of the domain they represent. Unfortunately, right now evolution of DSMLs is a costly process that requires changing its metamodel and re-creating the complete modeling environment.             In this paper we advocate for the use of EMF Profiles, an adaptation of the UML profile concept to DSMLs. Profiles have been a key enabler for the success of UML by providing a lightweight language-inherent extension mechanism which is expressive enough to cover an important subset of adaptation scenarios. We believe a similar concept for DSMLs would provide an easier extension mechanism which has been so far neglected by current metamodeling tools. Apart from direct\u00a0\u2026", "num_citations": "34\n", "authors": ["835"]}
{"title": "Representing temporal information in UML\n", "abstract": " The UML is a non-temporal conceptual modeling language. Conceptual schemas in the UML assume that the information base contains the current instances of entity and relationship types. For many information systems, the above assumption is acceptable. However, there are some information systems for which that assumption is a severe limitation. This happens when the functions of the information system require the knowledge of past states of the information base. In this paper we extend the UML to define a set of temporal features of entity and relationship types, and to provide notational devices to refer to any past state of the information base. Using this extension, a designer may use the UML/OCL as if it were a temporal conceptual modeling language. We also present a method for the transformation of a conceptual schema in this extended language into a conventional one. The method can be\u00a0\u2026", "num_citations": "34\n", "authors": ["835"]}
{"title": "Cognifying model-driven software engineering\n", "abstract": " The limited adoption of Model-Driven Software Engineering (MDSE) is due to a variety of social and technical factors, which can be summarized in one: its (real or perceived) benefits do not outweigh its costs. In this vision paper we argue that the cognification of MDSE has the potential to reverse this situation. Cognification is the application of knowledge (inferred from large volumes of information, artificial intelligence or collective intelligence) to boost the performance and impact of a process. We discuss the opportunities and challenges of cognifying MDSE tasks and we describe some potential scenarios where cognification can bring quantifiable and perceivable advantages. And conversely, we also discuss how MDSE techniques themselves can help in the improvement of AI, Machine learning, bot generation and other cognification techniques.", "num_citations": "32\n", "authors": ["835"]}
{"title": "Verifying action semantics specifications in UML behavioral models\n", "abstract": " MDD and MDA approaches require capturing the behavior of UML models in sufficient detail so that the models can be automatically implemented/executed in the production environment. With this purpose, Action Semantics (AS) were added to the UML specification as the fundamental unit of behavior specification. Actions are the basis for defining the fine-grained behavior of operations, activity diagrams, interaction diagrams and state machines. Unfortunately, current proposals devoted to the verification of behavioral schemas tend to skip the analysis of the actions they may include. The main goal of this paper is to cover this gap by presenting several techniques aimed at verifying AS specifications. Our techniques are based on the static analysis of the dependencies between the different actions included in the behavioral schema. For incorrect specifications, our method returns a meaningful feedback\u00a0\u2026", "num_citations": "32\n", "authors": ["835"]}
{"title": "Example-driven web api specification discovery\n", "abstract": " REpresentational State Transfer (REST) has become the dominant approach to design Web APIs nowadays, resulting in thousands of public REST Web APIs offering access to a variety of data sources (e.g., open-data initiatives) or advanced functionalities (e.g., geolocation services). Unfortunately, most of these APIs do not come with any specification that developers (and machines) can rely on to automatically understand and integrate them. Instead, most of the time we have to rely on reading its ad-hoc documentation web pages, despite the existence of languages like Swagger or, more recently, OpenAPI that developers could use to formally describe their APIs. In this paper we present an example-driven discovery process that generates model-based OpenAPI specifications for REST Web APIs by using API call examples. A tool implementing our approach and a community-driven repository for the\u00a0\u2026", "num_citations": "31\n", "authors": ["835"]}
{"title": "Automatic code generation for cross-platform, multi-device mobile apps: Some reflections from an industrial experience\n", "abstract": " With the continuously increasing adoption of mobile devices, software development companies have new business opportunities through direct sales in app stores and delivery of business to employee (B2E) and business to business (B2B) solutions. However, cross-platform and multi-device development is a barrier for today's IT solution providers, especially small and medium enterprises (SMEs), due to the high cost and technical complexity of targeting development to a wide spectrum of devices, which differ in format, interaction paradigm, and software architecture. So far, several authors have proposed the application of model driven approaches to mobile apps development following a variety of strategies. In this paper we present the results of a research study conducted to find the best strategy for WebRatio, a software development company, interested in producing a MDD tool for designing and developing\u00a0\u2026", "num_citations": "31\n", "authors": ["835"]}
{"title": "API2MoL: Automating the building of bridges between APIs and Model-Driven Engineering\n", "abstract": " ContextA software artefact typically makes its functionality available through a specialized Application Programming Interface (API) describing the set of services offered to client applications. In fact, building any software system usually involves managing a plethora of APIs, which complicates the development process. In Model-Driven Engineering (MDE), where models are the key elements of any software engineering activity, this API management should take place at the model level. Therefore, tools that facilitate the integration of APIs and MDE are clearly needed.ObjectiveOur goal is to automate the implementation of API\u2013MDE bridges for supporting both the creation of models from API objects and the generation of such API objects from models. In this sense, this paper presents the API2MoL approach, which provides a declarative rule-based language to easily write mapping definitions to link API specifications\u00a0\u2026", "num_citations": "31\n", "authors": ["835"]}
{"title": "On lightweight metamodel extension to support modeling tools agility\n", "abstract": " Modeling in real industrial projects implies dealing with different models, metamodels and supporting tools. They continuously have to be adapted to changing requirements, involving (often costly) problems in terms of traceability, coherence or interoperability. To this intent, solutions ensuring a better adaptability and flexibility of modeling tools are needed. As metamodels are cornerstones in such tools, metamodel extension capabilities are fundamental. However, current modeling frameworks are not flexible or dynamic enough. Thus, following the ongoing OMG MOF Extension Facility (MEF) RFP, this paper proposes a generic lightweight metamodel extension mechanism developed as part of the MoNoGe collaborative project. A base list of metamodel extension operators as well as a DSL for easily using them are introduced. Two different implementations of this extension mechanism (including a model\u00a0\u2026", "num_citations": "30\n", "authors": ["835"]}
{"title": "A model driven reverse engineering framework for extracting business rules out of a java application\n", "abstract": " In order to react to the ever-changing market, every organization needs to periodically reevaluate and evolve its company policies. These policies must be enforced by its Information System (IS) by means of a set of business rules that drive the system behavior and data. Clearly, policies and rules must be aligned at all times but unfortunately this is a challenging task. In most ISs implementation of business rules is scattered among the code so appropriate techniques must be provided for the discovery and evolution of evolving business rules.               In this paper we describe a model driven reverse engineering framework aiming at extracting business rules out of Java source code. The use of modeling techniques facilitate the representation of the rules at a higher-abstraction level which enables stakeholders to understand and manipulate them.", "num_citations": "29\n", "authors": ["835"]}
{"title": "Analysing graph transformation rules through OCL\n", "abstract": " In this paper we present an approach to the analysis of graph transformation rules based on an intermediate OCL representation. We translate different semantics of rules into OCL, together with the properties of interest (like rule applicability, conflict or independence). The intermediate representation serves three purposes: (i) allows the seamless integration of graph transformation rules with the MOF and OCL standards, and enables taking into account meta-model and OCL constraints when verifying the correctness of the rules; (ii) permits the interoperability of graph transformation concepts with a number of standards-based model-driven development tools; and (iii) makes available a plethora of OCL tools to actually perform the rule analysis.", "num_citations": "29\n", "authors": ["835"]}
{"title": "Computing the relevant instances that may violate an OCL constraint\n", "abstract": " Integrity checking is aimed at efficiently determining whether the state of the information base is consistent after the application of a set of structural events. One possible way to achieve efficiency is to consider only the relevant instances that may violate an integrity constraint instead of the whole population of the information base. This is the approach we follow in this paper to automatically check the integrity constraints defined in a UML conceptual schema. Since the method we propose uses only the standard elements of the conceptual schema to process the constraints, its efficiency improvement can benefit any implementation of the schema regardless the technology used.", "num_citations": "29\n", "authors": ["835"]}
{"title": "VirtualEMF: a model virtualization tool\n", "abstract": " Specification of complex systems involves several heterogeneous and interrelated models. Model composition is a crucial (and complex) modeling activity that allows combining different system perspectives into a single cross-domain view. Current composition solutions fail to fully address the problem, presenting important limitations concerning efficiency, interoperability, and/or synchronization. To cope with these issues, in this demo we introduce VirtualEMF: a model composition tool based on the concept of a virtual model, i.e., a model that do not hold concrete data, but that redirects all its model manipulation operations to the set of base models from which it was generated.", "num_citations": "28\n", "authors": ["835"]}
{"title": "Determining the structural events that may violate an integrity constraint\n", "abstract": " Any implementation of an information system must ensure that an operation is only applied if its execution does not lead to a violation of any of the integrity constraints defined in its conceptual schema. In this paper we propose a method to automatically determine the operations that may potentially violate an OCL integrity constraint in conceptual schemas defined in the UML. This is done by determining the structural events that may violate the constraint and checking whether those events appear in the operation specification. In this way, our method helps to improve efficiency of integrity checking since its results can be used to discard many irrelevant tests.", "num_citations": "28\n", "authors": ["835"]}
{"title": "Collaboro: a collaborative (meta) modeling tool\n", "abstract": " Software development is becoming more and more collaborative, emphasizing the role of end-users in the development process to make sure the final product will satisfy customer needs. This is especially relevant when developing Domain-Specific Modeling Languages (DSMLs), which are modeling languages specifically designed to carry out the tasks of a particular domain. While end-users are actually the experts of the domain for which a DSML is developed, their participation in the DSML specification process is still rather limited nowadays. In this paper, we propose a more community-aware language development process by enabling the active participation of all community members (both developers and end-users) from the very beginning. Our proposal, called Collaboro, is based on a DSML itself enabling the representation of change proposals during the language design and the discussion (and trace back) of possible solutions, comments and decisions arisen during the collaboration. Collaboro also incorporates a metric-based recommender system to help community members to define high-quality notations for the DSMLs. We also show how Collaboro can be used at the model-level to facilitate the collaborative specification of software models. Tool support is available both as an Eclipse plug-in a web-based solution.", "num_citations": "27\n", "authors": ["835"]}
{"title": "Test data generation for model transformations combining partition and constraint analysis\n", "abstract": " Model-Driven Engineering (MDE) is a software engineering paradigm where models play a key role. In a MDE-based development process, models are successively transformed into other models and eventually into the final source code by means of a chain of model transformations. Since writing model transformations is an error-prone task, mechanisms to ensure their reliability are greatly needed. One way of achieving this is by means of testing. A challenging aspect when testing model transformations is the generation of adequate input test data. Most existing approaches generate test data following a black-box approach based on some sort of partition analysis that exploits the structural features of the source metamodel of the transformation. However, these analyses pay no attention to the OCL invariants of the metamodel or do it very superficially. In this paper, we propose a mechanism that\u00a0\u2026", "num_citations": "26\n", "authors": ["835"]}
{"title": "A UML/OCL framework for the analysis of graph transformation rules\n", "abstract": " In this paper we present an approach for the analysis of graph transformation rules based on an intermediate OCL representation. We translate different rule semantics into OCL, together with the properties of interest (like rule applicability, conflicts or independence). The intermediate representation serves three purposes: (1) it allows the seamless integration of graph transformation rules with the MOF and OCL standards, and enables taking the meta-model and its OCL constraints (i.e. well-formedness rules) into account when verifying the correctness of the rules; (2) it permits the interoperability of graph transformation concepts with a number of standards-based model-driven development tools; and (3) it makes available a plethora of OCL tools to actually perform the rule analysis. This approach is especially useful to analyse the operational semantics of Domain Specific Visual Languages. We have\u00a0\u2026", "num_citations": "26\n", "authors": ["835"]}
{"title": "Paraphrasing OCL expressions with SBVR\n", "abstract": " A conceptual schema (CS) should be explained to the stakeholders to validate that it is an appropriate representation of all knowledge of the domain. One of the best ways to explain the CS is to describe it by means of natural language expressions (paraphrasing). Even though paraphrasing has been studied for the most typical elements of a CS, current methods are, in general, unable to cope with the textual business rules that complement the CS. In this paper, we cover this gap by presenting a method that generates natural language explanations for business rules expressed in OCL (Object Constraint Language), the standard language to specify business rules on UML-based CSs. As an intermediate step, our method translates the OCL expression into a SBVR (Semantics of Business Vocabulary and Business Rules) representation.", "num_citations": "26\n", "authors": ["835"]}
{"title": "Ambiguity issues in OCL postconditions\n", "abstract": " There are two different approaches to specify the behavior of the operations of an Information System. In the imperative approach, the operation effect is defined by means of specifying the set of actions (creation of objects and links, attribute updates\u2026) to apply over the system state. With the declarative approach, the effect is defined by means of contracts stating the conditions that the system state must satisfy before (precondition) and after (postcondition) the operation execution.From a specification point of view, the declarative approach is preferable. The main issue regarding declarative specifications is their ambiguity. Commonly, there are many different system states that satisfy an operation postcondition. However, in general, only one of them is the one the designer had in mind when defining the operation, and thus, that state should be the only one considered valid at the end of the operation execution. In this paper, we identify some of the common ambiguities appearing in OCL postconditions and provide a default interpretation for each of them in order to improve the usefulness of declarative specifications.", "num_citations": "26\n", "authors": ["835"]}
{"title": "WordPress: A content management system to democratize publishing\n", "abstract": " WordPress aims to democratize publishing, ensuring that any nontechnical person can create a website, while building a product that can scale all the way up to enterprise clients with complex needs. The richness and importance of the WordPress code base and ecosystem pose many interesting challenges for the research community.", "num_citations": "25\n", "authors": ["835"]}
{"title": "Initiating a benchmark for UML and OCL analysis tools\n", "abstract": " The Object Constraint Language (OCL) is becoming more and more popular for model-based engineering, in particular for the development of models and model transformations. OCL is supported by a variety of analysis tools having different scopes, aims and technological corner stones. The spectrum ranges from treating issues concerning formal proof techniques to testing approaches, from validation to verification, and from logic programming and rewriting to SAT-based technologies. This paper is a first step towards a well-founded benchmark for assessing validation and verification techniques on UML and OCL models. The paper puts forward a set of UML and OCL models together with particular questions for these models roughly characterized by the notions consistency, independence, consequences, and reachability. The paper sketches how these questions are handled by two OCL tools, USE and\u00a0\u2026", "num_citations": "25\n", "authors": ["835"]}
{"title": "Lightweight string reasoning for OCL\n", "abstract": " Models play a key role in assuring software quality in the model-driven approach. Precise models usually require the definition of OCL expressions to specify model constraints that cannot be expressed graphically. Techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. Several tools to check model satisfiability have been developed but to our knowledge, none of them yet supports the analysis of OCL expressions including operations on Strings in general terms. As, in contrast, many industrial models do contain such operations, there is evidently a gap.             There has been much research on formal reasoning on strings in general, but so far the results could not be included into model finding approaches. For model finding, string reasoning only contributes a sub-problem, therefore, a\u00a0\u2026", "num_citations": "25\n", "authors": ["835"]}
{"title": "Refining models with rule-based model transformations\n", "abstract": " Several model-to-model transformation languages have been primarily designed to easily address the syntactic and semantic translation of read-only input models towards write-only output models. While this approach has been proven successful in many practical cases, it is not directly applicable to transformations that need to modify their source models, like refactorings. In this paper we investigate the application of a model-to-model transformation language to in-place transformations, by providing a systematic view of the problem, comparing alternative solutions and proposing a transformation semantics to address this problem in ATL.", "num_citations": "25\n", "authors": ["835"]}
{"title": "UML/OCL verification in practice\n", "abstract": " In the MDD approaches, models become the primary artifact of the development process and the basis for code generation. Identifying defects early, at the model-level, can help to reduce development costs and improve software quality. There is an emerging need for verification techniques usable in practice, ie able to find and notify defects in real-life models without requiring a strong verification background or extensive model annotations. Some promising approaches revolve around the satisfiability property of a model, ie deciding whether it is possible to create a well-formed instantiation of the model. We will discuss existing solutions to this problem in the UML/OCL context. Our claim is that this problem has not yet been satisfactorily addressed.", "num_citations": "25\n", "authors": ["835"]}
{"title": "Conceptual modelling patterns for roles\n", "abstract": " Roles are meant to capture dynamic and temporal aspects of real-world objects. The role concept has been used with many semantic meanings: dynamic class, aspect, perspective, interface or mode. This paper identifies common semantics of different role models found in the literature. Moreover, it presents a set of conceptual modelling patterns for the role concept that include both the static and dynamic aspects of roles. In particular, we propose the Role as Entity Types conceptual modelling pattern to deal with the full role semantics. A conceptual modelling pattern is aimed at representing a specific structure of knowledge that appears in different domains. The use of these patterns eases the definition of roles in conceptual schemas. In addition, we describe the design of schemas defined by using the patterns in order to implement them in any object-oriented language.", "num_citations": "25\n", "authors": ["835"]}
{"title": "Community-driven language development\n", "abstract": " Software development processes are becoming more collaborative, trying to integrate end-users as much as possible. The idea is to advance towards a community-driven process where all actors (both technical and nontechnical) work together to ensure that the system-to-be will satisfy all expectations. This seems specially appropriate in the field of Domain-Specific Languages (DSLs) typically designed to facilitate the development of software for a particular domain. DSLs offer constructs closer to the vocabulary of the domain which simplifies the adoption of the DSL by endusers. Interestingly enough, the development of DSLs is not a collaborative process itself. In this sense, the goal of this paper is to propose a collaborative infrastructure for the development of DSLs where end-users have a direct and active participation in the evolution of the language. This infrastructure is based on Collaboro, a DSL to\u00a0\u2026", "num_citations": "24\n", "authors": ["835"]}
{"title": "A research agenda for conceptual schema-centric development\n", "abstract": " Conceptual schema-centric development (CSCD) is a research goal that reformulates the historical aim of automating information systems development. In CSCD, conceptual schemas would be explicit, executable in the production environment and the basis for the system\u2019s evolution. To achieve the CSCD goal, several research problems must be solved. In this paper we identify and comment on sixteen problems that should be included in a research agenda for CSCD.", "num_citations": "24\n", "authors": ["835"]}
{"title": "Evolution of the Teacher Roles and Figures in E-learning Environments\n", "abstract": " The use of information and communication technologies (ICT) in education has made feasible distance learning in the form of e-learning. To succeed in e-learning we cannot just try to replicate traditional educational methods. ICT force us to review some long-held assumptions in relation to pedagogical principles and practices. In this sense, a key issue is the evolution of the teacher roles in such new ICT environment. This is the main focus of this paper. The results are based on the author experience in both conventional and e-learning environments", "num_citations": "24\n", "authors": ["835"]}
{"title": "Automatic generation of workflow-extended domain models\n", "abstract": " The specification of business processes is becoming a more and more critical aspect for organizations. Such processes are specified as workflow models expressing the logical precedence among the different business activities (i.e. the units of work). Up to now, workflow models have been commonly managed through specific subsystems, called workflow management systems. In this paper we advocate for the integration of the workflow specification in the system domain model. This workflow-extended domain model is automatically derived from the initial workflow specification. Then, model-driven development methods may depart from the extended domain model to automatically generate an implementation of the system enforcing the business processes in any final technology platform, thus avoiding the need of basing the implementation on a dedicated workflow engine.", "num_citations": "23\n", "authors": ["835"]}
{"title": "Gitana: a SQL-based git repository inspector\n", "abstract": " Software development projects are notoriously complex and difficult to deal with. Several support tools such as issue tracking, code review and Source Control Management (SCM) systems have been introduced in the past decades to ease development activities. While such tools efficiently track the evolution of a given aspect of the project (e.g., bug reports), they provide just a partial view of the project and often lack of advanced querying mechanisms limiting themselves to command line or simple GUI support. This is particularly true for projects that rely on Git, the most popular SCM system today.                 In this paper, we propose a conceptual schema for Git and an approach that, given a Git repository, exports its data to a relational database in order to (1) promote data integration with other existing SCM tools and (2) enable writing queries on Git data using standard SQL syntax. To ensure efficiency, our\u00a0\u2026", "num_citations": "22\n", "authors": ["835"]}
{"title": "Backwards reasoning for model transformations: Method and applications\n", "abstract": " Model transformations are key elements of model driven engineering. Current challenges for transformation languages include improving usability (i.e., succinct means to express the transformation intent) and devising powerful analysis methods.In this paper, we show how backwards reasoning helps in both respects. The reasoning is based on a method that, given an OCL expression and a transformation rule, calculates a constraint that is satisfiable before the rule application if and only if the original OCL expression is satisfiable afterwards.With this method we can improve the usability of the rule execution process by automatically deriving suitable application conditions for a rule (or rule sequence) to guarantee that applying that rule does not break any integrity constraint (e.g. meta-model constraints). When combined with model finders, this method facilitates the validation, verification, testing and diagnosis of\u00a0\u2026", "num_citations": "21\n", "authors": ["835"]}
{"title": "How are UML class diagrams built in practice? A usability study of two UML tools: Magicdraw and Papyrus\n", "abstract": " Software modeling is a key activity in software development, especially when following any kind of Model Driven Software Engineering (MDSE) process. In this context, standard modeling languages, like the Unified Modeling Language (UML), and tools for supporting the modeling activities become essential.The aim of this study is to analyze how modelers build UML models and how good modeling tools are in supporting this task. Our goal is to draw some useful lessons that help to improve the (UML) modeling process both by recommending changes on the tools themselves and on how UML is taught so that theory and practice of UML modeling are better aligned.Our study employs two research approaches. The main one is an empirical experiment (which analyzes screen recordings registered by undergraduate students during the construction of a UML class diagram). An analytical analysis complements the\u00a0\u2026", "num_citations": "20\n", "authors": ["835"]}
{"title": "A model-based approach to gamify the learning of modeling\n", "abstract": " Conceptual modeling is an essential activity in the development of any information system. But at the same time, it is a difficult one which may discourage some professionals that may be tempted to skip it altogether in favor of more code-oriented tasks. In recent years, gamification has emerged as a new approach to increase learner engagement and has been successfully applied to a wide range of training and educational scenarios. We believe gamification can play a key supportive role in teaching and learning conceptual modeling, an area where gamification has not been really applied so far. In this sense, this paper presents a model-based approach for gamifying scenarios for learning modeling. Our approach includes a new language for modeling the gamification process itself and an environment where this new language can be embedded in current modeling tools to allow instructors and students design and use gaming scenarios all within a full modeling infrastructure.", "num_citations": "20\n", "authors": ["835"]}
{"title": "Two Basic Correctness Properties for ATL Transformations: Executability and Coverage.\n", "abstract": " Model transformations play a cornerstone role with the emergence of Model Driven Engineering (MDE), where models are transformed from higher to lower levels of abstraction. Unfortunately, a quick and easy way to check the correctness of model transformations is still missing, which compromises their quality (and in turn, the quality of the target models generated from them). In this paper we propose a lightweight and efficient method that performs a static analysis of the ATL rules with respect to two correctness properties we define:(1) weak executability, which determines if there is some scenario in which an ATL rule can be safely applied without breaking the target metamodel integrity constraints; and (2) coverage, which ensures a set of ATL rules allow addressing all elements of the source and target metamodels. In both cases, our method returns meaningful feedback that helps repairing the possible detected inconsistencies.", "num_citations": "20\n", "authors": ["835"]}
{"title": "The Future of Model Transformation Languages: An Open Community\n", "abstract": " Abstract Model transformations are the key element that brings life to model-driven engineering. Animation, simulations, VV, code-generation, etc. all depend on some kind of model transformation to work. Model transformations are typically defined via specialized model transformation languages but this is now in question due to the lack of convincing evidence that specialised languages are substantially better than generalpurpose languages for model transformation specification, and the rise of artificial intelligence. We report on the results of an open discussion with the model transformation community on the future of these languages, including whether such a future exists at all.", "num_citations": "19\n", "authors": ["835"]}
{"title": "A metric for measuring the complexity of OCL expressions\n", "abstract": " Despite the importance of OCL in the specification of UML models, few existing metrics are devoted to measure the complexity of OCL expressions. The proposed ones are based on the syntactic structure of the expressions (number of referred attributes, number of navigations,\u2026) and on the constructs used in their definition (as the number of forAll and select iterators). Indeed, these metrics are helpful to determine, for instance, the understandability of the expressions but we believe they fall short when providing a precise measure of their complexity.In this paper we propose a new metric to compute the complexity of OCL expressions. Our metric is based on the number of objects involved in the evaluation of the expression. An expression e1 is more complex than an expression e2 if the number of objects required to evaluate e1 is greater than the number of objects required to evaluate e2.", "num_citations": "19\n", "authors": ["835"]}
{"title": "Constraint tuning and management for web applications\n", "abstract": " Since the Web is becoming a platform for implementing complex B2C and B2B applications instead of simple content publishing sites, the need arises of imposing constraints on the navigation and on the managed information. Web conceptual modeling languages allow to a small extent to specify constraints on the application, by means of some extensions to their basic primitives; however these approaches do not provide a comprehensive framework for efficient and effective constraint management. The goal of this paper is to present a general framework to facilitate the integration of efficient integrity checking methods in Web applications. Our main contribution is the study of a set of parameters that can be inferred from high level Web application modeling, for allowing the tuning of the constraint optimization techniques for each specific Web application. From these parameters we derive the optimal enforcement\u00a0\u2026", "num_citations": "19\n", "authors": ["835"]}
{"title": "Towards a language server protocol infrastructure for graphical modeling\n", "abstract": " The development of modern IDEs is still a challenging and time-consuming task, which requires implementing the support for language-specific features such as syntax highlighting or validation. When the IDE targets a graphical language, its development becomes even more complex due to the rendering and manipulation of the graphical notation symbols. To simplify the development of IDEs, the Language Server Protocol (LSP) proposes a decoupled approach based on language-agnostic clients and language-specific servers. LSP clients communicate changes to LSP servers, which validate and store language instances. However, LSP only addresses textual languages (ie, character as atomic unit) and neglects the support for graphical ones (ie, nodes/edges as atomic units). In this paper, we present our vision to decouple graphical language IDEs discussing the alternatives for integrating LSP's ideas in their\u00a0\u2026", "num_citations": "18\n", "authors": ["835"]}
{"title": "OpenAPItoUML: a tool to generate UML models from OpenAPI definitions\n", "abstract": " REpresentational State Transfer (REST) has become the prominent architectural style for designing Web APIs. This increasing adoption has triggered the creation of languages to formally describe REST APIs, thus facilitating and promoting their usage. In particular, a consortium of companies has created the OpenAPI Initiative, which aims at creating a vendor neutral, portable, standard and open specification for describing REST APIs. OpenAPI specification has become the choice of reference for describing REST APIs, and its adopters can benefit from a plethora of tools for documenting, developing and integrating REST APIs. However, current documentation tools for OpenAPI only describe REST APIs in HTML pages using text and code samples, thus requiring a considerable effort to visualize and understand what the APIs offer. In this paper, we propose a tool called OpenAPItoUML, which generates\u00a0\u2026", "num_citations": "18\n", "authors": ["835"]}
{"title": "fREX: fUML-based reverse engineering of executable behavior for software dynamic analysis\n", "abstract": " Reverse engineering is still a challenging process, notably because of the growing number, heterogeneity, complexity, and size of software applications. While the analysis of their structural elements has been intensively investigated, there is much less work covering the reverse engineering of their behavioral aspects. To further stimulate research on this topic, we propose fREX as an open framework for reverse engineering of executable behaviors from existing software code bases. fREX currently provides model discovery support for behavior embedded in Java code, employs the OMG's fUML standard language as executable pivot format for dynamic analysis, and uses model transformations to bridge Java and fUML. Thus, fREX also aims at contributing to explore the relationship between programming languages (e.g., Java) and executable modeling languages (e.g., fUML). In this paper, we describe the\u00a0\u2026", "num_citations": "18\n", "authors": ["835"]}
{"title": "Extracting business rules from COBOL: A model-based framework\n", "abstract": " Organizations rely on the logic embedded in their Information Systems for their daily operations. This logic implements the business rules in place in the organization, which must be continuously adapted in response to market changes. Unfortunately, this evolution implies understanding and evolving also the underlying software components enforcing those rules. This is challenging because, first, the code implementing the rules is scattered throughout the whole system and, second, most of the time documentation is poor and out-of-date. This is specially true for older systems that have been maintained and evolved for several years (even decades). In those systems, it is not even clear which business rules are enforced nor whether rules are still consistent with the current organizational policies. In this sense, the goal of this paper is to facilitate the comprehension of legacy systems (in particular COBOL-based\u00a0\u2026", "num_citations": "18\n", "authors": ["835"]}
{"title": "Towards an access-control metamodel for web content management systems\n", "abstract": " Out-of-the-box Web Content Management Systems (WCMSs) are the tool of choice for the development of millions of enterprise web sites but also the basis of many web applications that reuse WCMS for important tasks like user registration and authentication. This widespread use highlights the importance of their security, as WCMSs may manage sensitive information whose disclosure could lead to monetary and reputation losses. However, little attention has been brought to the analysis of how developers use the content protection mechanisms provided by WCMSs, in particular, Access-control (AC). Indeed, once configured, knowing if the AC policy provides the required protection is a complex task as the specificities of each WCMS need to be mastered. To tackle this problem, we propose here a metamodel tailored to the representation of WCMS AC policies, easing the analysis and manipulation tasks\u00a0\u2026", "num_citations": "18\n", "authors": ["835"]}
{"title": "Lightweight verification of executable models\n", "abstract": " Executable models play a key role in many development methods by facilitating the immediate simulation/implementation of the software system under development. This is possible because executable models include a fine-grained specification of the system behaviour.               Unfortunately, a quick and easy way to check the correctness of behavioural specifications is still missing, which compromises their quality (and in turn the quality of the system generated from them). In this paper, a lightweight verification method to assess the strong executability of fine-grained behavioural specifications (i.e. operations) at design-time is provided. This method suffices to check that the execution of the operations is consistent with the integrity constraints defined in the structural model and returns a meaningful feedback that helps correcting them otherwise.", "num_citations": "18\n", "authors": ["835"]}
{"title": "Multi-platform chatbot modeling and deployment with the jarvis framework\n", "abstract": " Chatbot applications are increasingly adopted in various domains such as e-commerce or customer services as a direct communication channel between companies and end-users. Multiple frameworks have been developed to ease their definition and deployment. They typically rely on existing cloud infrastructures and artificial intelligence techniques to efficiently process user inputs and extract conversation information. While these frameworks are efficient to design simple chatbot applications, they still require advanced technical knowledge to define complex conversations and interactions. In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, increasing the development and maintenance costs. In this paper we introduce the Jarvis framework, that tackles these issues by providing a Domain Specific Language (DSL) to define chatbots in a\u00a0\u2026", "num_citations": "17\n", "authors": ["835"]}
{"title": "Ingenier\u00eda del software\n", "abstract": " La ingenier\u00eda del software (IS) estudia cu\u00e1l es la mejor manera de producir software de calidad. Con esta finalidad, la IS propone la aplicaci\u00f3n de una serie de m\u00e9todos, notaciones, t\u00e9cnicas, etc., que permiten asegurar la calidad final del software desarrollado.Desgraciadamente, hoy d\u00eda la aplicaci\u00f3n de los principios de la IS durante el desarrollo de software no est\u00e1 todav\u00eda generalizada dentro de la comunidad de desarrolladores. Frecuentemente, el mismo concepto de qu\u00e9 es la IS no se tiene claro. A veces se asocia la IS con alguna de las t\u00e9cnicas espec\u00edficas que se utilizan o se reduce la IP al arte de hacer \u201cdibujos\u201d o \u201cplanos\u201d del software (con rect\u00e1ngulos, rayas, \u00f3valos...) sin acabar de entender cu\u00e1l es su importancia ni su utilidad dentro del proceso de desarrollo. As\u00ed, pues, este cap\u00edtulo pretende exponer al lector los conocimientos necesarios para entender qu\u00e9 es y para qu\u00e9 sirve la IS, as\u00ed como algunos\u00a0\u2026", "num_citations": "16\n", "authors": ["835"]}
{"title": "Transforming OCL constraints: a context change approach\n", "abstract": " Integrity constraints (ICs) play a key role in the definition of conceptual schemas. In the UML, ICs are usually specified as invariants written in the OCL. However, due to the high expressiveness of the OCL, the designer has different syntactic alternatives to express each IC, mainly depending on the type used as a context of the constraint. The method presented in this paper assists the designer during the definition of ICs by means of automatically transforming the initially defined constraints into equivalent alternatives. The method is also useful in the context of the MDA, where the choice of a particular alternative has a direct effect on the efficiency of the automatically generated implementation.", "num_citations": "16\n", "authors": ["835"]}
{"title": "Enabling the definition and enforcement of governance rules in open source systems\n", "abstract": " Governance rules in software development projects help to prioritize and manage their development tasks, and contribute to the long-term sustainability of the project by clarifying how core and external contributors should collaborate in order to advance the project during its whole lifespan. Despite their importance, specially in Open Source Software (OSS) projects, these rules are usually implicit or scattered in the project documentation/tools (e.g., Tracking-systems or forums), hampering the correct understanding of the development process. We propose to enable the explicit definition and enforcement of governance rules for OSS projects. We believe this brings several important benefits, including improvements in the transparency of the process, its traceability and the semi-automation of the governance itself. Our approach has been implemented on top of My Lyn, a project-management Eclipse plug-in\u00a0\u2026", "num_citations": "15\n", "authors": ["835"]}
{"title": "Model-driven extraction and analysis of network security policies\n", "abstract": " Firewalls are a key element in network security. They are in charge of filtering the traffic of the network in compliance with a number of access-control rules that enforce a given security policy. In an always-evolving context, where security policies must often be updated to respond to new security requirements, knowing with precision the policy being enforced by a network system is a critical information. Otherwise, we risk to hamper the proper evolution of the system and compromise its security. Unfortunately, discovering such enforced policy is an error-prone and time consuming task that requires low-level and, often, vendor-specific expertise since firewalls may be configured using different languages and conform to a complex network topology. To tackle this problem, we propose a model-driven reverse engineering approach able to extract the security policy implemented by a set of firewalls in a working\u00a0\u2026", "num_citations": "15\n", "authors": ["835"]}
{"title": "Virtual composition of EMF models\n", "abstract": " Model composition is a very important modeling task as it allows to combine various perspectives of a system (represented by various models) into a single specialized view (a composed model). Several approaches have been proposed to tackle this problem, but they present some important limitations concerning efficiency, interoperability, and/or synchronization issues (mainly due to the element cloning mechanism used to create the composed model). In this paper we propose a new model composition method based on the virtualization of the composition mechanism. In our approach, the composed model is in fact created as a virtual model that redirects all its model access and manipulation requests directly to the set of base models from which it was generated. This is done transparently for the designer. Our mechanism improves the composition process with relation to the limitations mentioned above. The solution has been implemented and validated in a prototype tool on top of EMF.", "num_citations": "15\n", "authors": ["835"]}
{"title": "Tools for teams: A survey of web-based software project portals\n", "abstract": " If you pick a software development project at random, the odds are good that its source code, bug reports, mailing list archives, and so on reside in a web-based portal. Whether it's a hosted service like SourceForge or an installed system like Trac, in many ways that portal is the project: individual developers might come and go, but the portal's contents live on, and with them, the project.Despite their widespread use, software project portals have been studied much less than individual-oriented tools such as integrated development environments (IDEs)[IEEESoftware08]. To begin to rectify this, in July--September 2008 we examined the features of several representative portals and interviewed their developers. Our research goals were to determine what needs those tools were intended to serve, how their feature sets had been chosen (eg, how their developers had translated perceived needs into functionality) and how they were likely to evolve. To our knowledge, this is the first general study of web-based software project management tools, though their importance was pointed out in [Alshawi03; for comparisons of pre-web tools see [Terry98].", "num_citations": "15\n", "authors": ["835"]}
{"title": "An invariant-based method for the analysis of declarative model-to-model transformations\n", "abstract": " In this paper we propose a method to derive OCL invariants from declarative specifications of model-to-model transformations. In particular we consider two of the most prominent approaches for specifying such transformations: Triple Graph Grammars and QVT. Once the specification is expressed in the form of invariants, the transformation developer can use such description to verify properties of the original transformation (e.g. whether it defines a total, surjective or injective function), and to validate the transformation by the automatic generation of valid pairs of source and target models.", "num_citations": "15\n", "authors": ["835"]}
{"title": "Deriving operation contracts from UML class diagrams\n", "abstract": " Class diagrams must be complemented with a set of system operations that describes how users can modify and evolve the system state. To be useful, such a set must be complete (i.e. through these operations, users should be able to modify the population of all elements in the class diagram) and executable (i.e. for each operation, there must exist a system state over which the operation can be successfully applied). Manual specification of these operations is an error-prone and time-consuming activity. Therefore, the goal of this paper is to automatically provide a basic set of system operations that verify these two properties. Operations are drawn from the elements (classes, attributes, etc) of the class diagram and take into account the possible dependencies between the different change events (i.e. inserts/updates/deletes) that may be applied to them. Afterwards, the designer could reuse our proposal to\u00a0\u2026", "num_citations": "15\n", "authors": ["835"]}
{"title": "An LSTM-based neural network architecture for model transformations\n", "abstract": " Model transformations are a key element in any model-driven engineering approach. But writing them is a time-consuming and error-prone activity that requires specific knowledge of the transformation language semantics. We propose to take advantage of the advances in Artificial Intelligence and, in particular Long Short-Term Memory Neural Networks (LSTM), to automatically infer model transformations from sets of input-output model pairs. Once the transformation mappings have been learned, the LSTM system is able to autonomously transform new input models into their corresponding output models without the need of writing any transformation-specific code. We evaluate the correctness and performance of our approach and discuss its advantages and limitations.", "num_citations": "14\n", "authors": ["835"]}
{"title": "Smart bound selection for the verification of UML/OCL class diagrams\n", "abstract": " Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification techniques, e.g., SAT or constraint programming (CP) solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, bounded verification tools provide little support in the bound selection process. In this paper, we present a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification\u00a0\u2026", "num_citations": "14\n", "authors": ["835"]}
{"title": "Gremlin-ATL: a scalable model transformation framework\n", "abstract": " Industrial use of Model Driven Engineering techniques has emphasized the need for efficiently store, access, and transform very large models. While scalable persistence frameworks, typically based on some kind of NoSQL database, have been proposed to solve the model storage issue, the same level of performance improvement has not been achieved for the model transformation problem. Existing model transformation tools (such as the well-known ATL) often require the input models to be loaded in memory prior to the start of the transformation and are not optimized to benefit from lazy-loading mechanisms, mainly due to their dependency on current low-level APIs offered by the most popular modeling frameworks nowadays. In this paper we present Gremlin-ATL, a scalable and efficient model-to-model transformation framework that translates ATL transformations into Gremlin, a query language supported by\u00a0\u2026", "num_citations": "14\n", "authors": ["835"]}
{"title": "Towards domain refinement for UML/OCL bounded verification\n", "abstract": " Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification, e.g. SAT solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, existing tools provide little support in this choice.                 This paper presents a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification tools and improve the efficiency of the verification process.", "num_citations": "14\n", "authors": ["835"]}
{"title": "The role of foundations in open source projects\n", "abstract": " In the last years, a number of Open-Source Systems (OSS) have created parallel foundations, as legal instruments to better articulate the structure, collaboration and financial model for the project. Some examples are Apache, Linux, Mozilia, Eclipse or Django foundations. Nevertheless, foundations largely differ in the kind of mission they have and the support they provide to their project/s. In this paper we study the role of foundations in open source software development. We analyze the nature of 89 software foundations and then focus on the 18 most relevant ones to study their openness and influence in the development practices taking place in the endorsed projects. Our results reveal the existence of a significant number of foundations with the sole purpose of promoting the importance of the free software movement and/or that limit them selves to core legal aspects but do not play any role in the day-to-day\u00a0\u2026", "num_citations": "13\n", "authors": ["835"]}
{"title": "Prefetchml: a framework for prefetching and caching models\n", "abstract": " Prefetching and caching are well-known techniques integrated in database engines and file systems in order to speed-up data access. They have been studied for decades and have proven their efficiency to improve the performance of I/O intensive applications. Existing solutions do not fit well with scalable model persistence frameworks because the prefetcher operates at the data level, ignoring potential optimizations based on the information available at the metamodel level. Furthermore, prefetching components are common in relational databases but typically missing (or rather limited) in NoSQL databases, a common option for model storage nowadays. To overcome this situation we propose PrefetchML, a framework that executes prefetching and caching strategies over models. Our solution embeds a DSL to precisely configure the prefetching rules to follow. Our experiments show that PrefetchML provides a\u00a0\u2026", "num_citations": "13\n", "authors": ["835"]}
{"title": "Composing JSON-based web APIs\n", "abstract": " The development of Web APIs has become a discipline that companies have to master to succeed in the Web. The so-called API economy is pushing companies to provide access to their data by means of Web APIs, thus requiring web developers to study and integrate such APIs into their applications. The exchange of data with these APIs is usually performed by using JSON, a schemaless data format easy for computers to parse and use. While JSON data is easy to read, its structure is implicit, thus entailing serious problems when integrating APIs coming from different vendors. Web developers have therefore to understand the domain behind each API and study how they can be composed. We tackle this issue by presenting an approach able to both discover the domain of JSON-based Web APIs and identify composition links among them. Our approach allows developers to easily visualize what is\u00a0\u2026", "num_citations": "13\n", "authors": ["835"]}
{"title": "A catalogue of refactorings for navigation models\n", "abstract": " The evolution of web applications (from read-only applications for browsing the data to full-fledged content-modification applications) has increased the complexity of navigation models describing the set of web pages included in a web application. In this paper, we propose adopting the refactoring technique to reorganize and improve the quality of such models. This technique was initially proposed to improve the structure of source code without changing its external observable behaviour. We adapt the refactoring technique to the navigation models context and present a catalogue of refactorings specific for this particular kind of models.", "num_citations": "13\n", "authors": ["835"]}
{"title": "Roles as entity types: A conceptual modelling pattern\n", "abstract": " Roles are meant to capture dynamic and temporal aspects of real-world objects. The role concept has been used with many semantic meanings: dynamic class, aspect, perspective, interface or mode. This paper identifies common semantics of different role models found in the literature. Moreover, it presents a conceptual modelling pattern for the role concept that includes both the static and dynamic aspects of roles. A conceptual modelling pattern is aimed at representing a specific structure of knowledge that appears in different domains. In particular, we adapt the pattern to UML. The use of this pattern eases the definition of roles in conceptual schemas. In addition, we describe the design of schemas defined using our pattern in order to implement them in any object-oriented language. We also discuss the advantages of our approach over previous ones.", "num_citations": "13\n", "authors": ["835"]}
{"title": "Applying graph kernels to model-driven engineering problems\n", "abstract": " Machine Learning (ML) can be used to analyze and classify large collections of graph-based information, eg images, location information, the structure of molecules and proteins,... Graph kernels is one of the ML techniques typically used for such tasks.", "num_citations": "12\n", "authors": ["835"]}
{"title": "Scalable queries and model transformations with the mogwai tool\n", "abstract": " Scalability of modeling frameworks has become a major issue hampering MDE adoption in the industry. Specifically, scalable model persistence, as well as efficient query and transformation engines, are two of the key challenges that need to be addressed to enable the support for very large models in current applications. In this paper we demonstrate Mogwa\u00ef, a tool designed to efficiently compute queries and transformations (expressed in OCL and ATL) over models stored in NoSQL databases. Mogwa\u00ef relies on a translational approach that maps constructs of the supported input languages to Gremlin, a generic NoSQL query language, and a model to datastore mapping allowing to compute the generated query on top of several datastores. The produced queries are computed on the database side, benefiting of all its optimizations, improving the execution time and reducing the memory footprint\u00a0\u2026", "num_citations": "12\n", "authors": ["835"]}
{"title": "A model-driven approach for the extraction of network access-control policies\n", "abstract": " Network security constitutes a critical concern when developing and maintaining nowadays corporate information systems. Firewalls are a key element of network security by filtering the traffic of the network in compliance with a number of access control rules that enforce a given security policy. Unfortunately, once implemented, and due to the complexity of firewall configuration languages and the underlying network topology, knowing which security policy is actually being enforced by the network system is a complex and time consuming task that requires low-level and, often, vendor-specific expertise. In an always-evolving context, where security policies are often updated to respond to new security requirements, this discovery phase becomes critical since it could hamper the proper evolution of the system and compromise its security. To tackle this problem, our approach generates an abstract model of the firewall\u00a0\u2026", "num_citations": "12\n", "authors": ["835"]}
{"title": "Architecture quality revisited\n", "abstract": " There is a common belief in the software community that nonfunctional quality is fundamentally important for architecture sustainability and project success. A recent study, however, suggests that nonfunctional quality is of little relevance for users and customers, but instead mainly a concern for architects. Nontechnical constraints, such as licenses and technology providers, appear to be driving design as prominently as quality requirements. Quality requirements, such as performance, are mainly defined by architects on the basis of their experiences, and are often poorly documented and validated. This column explores whether the software community actually overestimates the relevance of nonfunctional qualities or whether the study's observations indicate a valid position on nonfunctional quality for certain types of application domains, development approaches, and organizational setups.", "num_citations": "12\n", "authors": ["835"]}
{"title": "Automatic integrity constraint evolution due to model subtract operations\n", "abstract": " When evolving Conceptual Schemas (CS) one of the most common operations is the removal of some model elements. This removal affects the set of integrity constraints (IC) defined over the CS. Most times they must be modified to remain consistent with the evolved CS. The aim of this paper is to define an automatic evolutionary method to delete only the minimum set of constraints (or some of their parts) needed to keep the consistency with the CS after subtract operations. We consider that a set of constraints is consistent with an evolved CS when: 1) none of them refer to an element removed from the original CS and 2) the set of constraints is equal or less restrictive than the original one. In this paper we present our method assuming CS defined in UML with ICs specified in OCL, but it can be applied to other languages with similar results.", "num_citations": "12\n", "authors": ["835"]}
{"title": "Stepwise adoption of continuous delivery in model-driven engineering\n", "abstract": " Continuous Delivery (CD) and, in general, Continuous Software Engineering (CSE) is becoming the norm. Still, current practices and available integration platforms are too code-oriented. They are not well adapted to work with other, non text-based, software artifacts typically produced during early phases of the software engineering life-cycle. This is especially problematic for teams adopting a Model-Driven Engineering (MDE) approach to software development where several (meta)models (and model transformations) are built and executed as part of the development process. Typically, (part of) the code is automatically generated from such models. Therefore, in a complete CD process, changes in a model should trigger changes on the generated code when appropriate.               A step further would be to apply CD practices to the development of modeling artefacts themselves. Analogously to \u201ctraditional\u00a0\u2026", "num_citations": "11\n", "authors": ["835"]}
{"title": "An empirical study on the maturity of the eclipse modeling ecosystem\n", "abstract": " Since the early days of Model-driven Engineering (MDE), our community has been discussing the reasons why MDE had not quickly became mainstream. It is now clear the answer is a mix of technical and social factors, but among the former, the lack of maturity of MDE tools is often mentioned. The goal of this paper is to explore the question of whether this lack of maturity is actually true. We do so by comparing the maturity of over a hundred modeling and non-modeling projects living together in the Eclipse ecosystem. In both cases, we use the word project to refer to a variety of tools, libraries and other artefacts to build and manipulate software components, either at the model or code level. Our maturity model is based on code-centric and community metrics that we evaluate on the repository data for both kinds of projects. Their incubation status is also considered in the assessment. Results show that there are\u00a0\u2026", "num_citations": "11\n", "authors": ["835"]}
{"title": "Lightweight and static verification of UML executable models\n", "abstract": " Executable models play a key role in many software development methods by facilitating the (semi)automatic implementation/execution of the software system under development. This is possible because executable models promote a complete and fine-grained specification of the system behaviour. In this context, where models are the basis of the whole development process, the quality of the models has a high impact on the final quality of software systems derived from them. Therefore, the existence of methods to verify the correctness of executable models is crucial. Otherwise, the quality of the executable models (and in turn the quality of the final system generated from them) will be compromised. In this paper a lightweight and static verification method to assess the correctness of executable models is proposed. This method allows us to check whether the operations defined as part of the behavioural model\u00a0\u2026", "num_citations": "11\n", "authors": ["835"]}
{"title": "The MDE Diploma: first international postgraduate specialization in model-driven engineering\n", "abstract": " Model-Driven Engineering (MDE) is changing the way we build, operate, and maintain our software-intensive systems. Several projects using MDE practices are reporting significant improvements in quality and performance but, to be able to handle these projects, software engineers need a set of technical and interpersonal skills that are currently not widely available. The MDE postgraduate diploma intends to fill this gap by offering a full-time one year formation onMDE. The course syllabus is designed to teach students how to work at a higher abstraction level by the rigorous use of (software) models as the main artifacts in all software engineering activities. Contents include the conceptual framework of MDE plus all techniques and tools (e.g. for defining modeling languages, models, and model transformations) required to successfully complete software engineering projects following a MDE approach. The\u00a0\u2026", "num_citations": "11\n", "authors": ["835"]}
{"title": "Extending conceptual schemas with business process information\n", "abstract": " The specification of business processes is becoming a more and more critical aspect for organizations. Such processes are specified as workflow models expressing the logical precedence among the different business activities (i.e., the units of work). Typically, workflow models are managed through specific subsystems, called workflow management systems, to ensure a consistent behavior of the applications with respect to the organization business process. However, for small organizations and/or simple business processes, the complexity and capabilities of these dedicated workflow engines may be overwhelming. In this paper, we therefore, advocate for a different and lightweight approach, consisting in the integration of the business process specification within the system conceptual schema. We show how a workflow-extended conceptual schema can be automatically obtained, which serves both to enforce the organization business process and to manage all its relevant domain data in a unified way. This extended model can be directly processed with current CASE tools, for instance, to generate an implementation of the system (including its business process) in any technological platform.", "num_citations": "11\n", "authors": ["835"]}
{"title": "Elecci\u00f3n del modelo de evaluaci\u00f3n: caso pr\u00e1ctico para asignaturas de ingenier\u00eda del software\n", "abstract": " La variedad de sistemas de formaci\u00f3n comporta la coexistencia de diversos modelos de evaluaci\u00f3n, donde la elecci\u00f3n del modelo de evaluaci\u00f3n de una asignatura tiene que ser coherente con la metodolog\u00eda utilizada. En este art\u00edculo describimos diferentes tipos de modelos de evaluaci\u00f3n (basados en los modelos usados en el \u00e1rea de ingenier\u00eda del software de los estudios de Inform\u00e1tica de la Universitat Oberta de Catalunya (UOC)) y presentamos una gu\u00eda que puede ayudar en la elecci\u00f3n del modelo m\u00e1s adecuado para cada asignatura, en funci\u00f3n de sus caracter\u00edsticas. Esta experiencia puede ser \u00fatil para docentes interesados en revisar los modelos de evaluaci\u00f3n de sus asignaturas y/o para encontrar alternativas al tradicional modelo de evaluaci\u00f3n basado en examen final.", "num_citations": "11\n", "authors": ["835"]}
{"title": "Towards scalable model views on heterogeneous model resources\n", "abstract": " When engineering complex systems, models are used to represent various systems aspects. These models are often heterogeneous in terms of modeling language, provenance, number or scale. They can be notably managed by different persistence frameworks adapted to their nature. As a result, the information relevant to engineers is usually split into several interrelated models. To be useful in practice, these models need to be integrated together to provide global views over the system under study. Model view approaches have been proposed to tackle such an issue. They provide an unification mechanism to combine and query heterogeneous models in a transparent way. These views usually target specific engineering tasks such as system design, monitoring, evolution, etc. In our present context, the [email protected] industrially-supported European initiative defines a set of large-scale use cases where\u00a0\u2026", "num_citations": "10\n", "authors": ["835"]}
{"title": "Distributing relational model transformation on MapReduce\n", "abstract": " MDE has been successfully adopted in the production of software for several domains. As the models that need to be handled in MDE grow in scale, it becomes necessary to design scalable algorithms for model transformation (MT) as well as suitable frameworks for storing and retrieving models efficiently. One way to cope with scalability is to exploit the wide availability of distributed clusters in the Cloud for the parallel execution of MT. However, because of the dense interconnectivity of models and the complexity of transformation logic, the efficient use of these solutions in distributed model processing and persistence is not trivial.This paper exploits the high level of abstraction of an existing relational MT language, ATL, and the semantics of a distributed programming model, MapReduce, to build an ATL engine with implicitly distributed execution. The syntax of the language is not modified and no primitive for\u00a0\u2026", "num_citations": "10\n", "authors": ["835"]}
{"title": "On watermarking for collaborative model-driven engineering\n", "abstract": " Collaborative development scenarios often require models to be shared among the different stakeholders. These stakeholders are mostly remote with communication typically taking place over untrusted networks. This raises the need for effective intellectual property (IP) protection mechanisms for the shared models. Watermarking, an information hiding technique aimed at providing the means to verify the authenticity, integrity, and ownership of digital assets, has proved useful to provide IP protection in both media (images, audio, and video) and non-media domains (databases, XML documents, and graphs). In this paper, we explore the adaptation of the concept of watermarking to the modeling domain. We provide a novel and robust labeling mechanism based in the use of locality sensitive hashing and error correction codes. This labeling mechanism enables the integration of state-of-the-art watermarking\u00a0\u2026", "num_citations": "10\n", "authors": ["835"]}
{"title": "Conceptual Modeling Perspectives\n", "abstract": " When we decided to lead the design of this book for Prof. Antoni Oliv\u00e9, we did not guess that writing a Preface for our Professor first, our forever Friend later, would be such a hard job. The reality is that summarizing in a few words our respect and admiration for him is probably the most complicated Conceptual Modeling activity that we have ever faced! But we can try to do it, honoring the conceptual modeling passion that we have inherited from him. Always with Conceptual Modeling in mind, there are a few conceptual patterns that he has shown us with the most efficient strategy: his example. He has shown us the value of understanding carefully others\u2019 opinions. He has shown us how important is to listen to, before deciding what to do. He has shown us that a brilliant research is modest. He has shown us how a real leader is the one recognized as such by all his pupils, as we-the Editorsall are. He has shown us\u00a0\u2026", "num_citations": "10\n", "authors": ["835"]}
{"title": "Traceability mappings as a fundamental instrument in model transformations\n", "abstract": " Technological importance of traceability mappings for model transformations is well-known, but they have often been considered as an auxiliary element generated during the transformation execution and providing accessory information. This paper argues that traceability mappings should instead be regarded as a core aspect of the transformation definition, and a key instrument in the transformation management.                 We will show how a transformation can be represented as the result of execution of a metamodel mapping, which acts as a special encoding of the transformation definition. Since mappings enjoy Boolean operations (as sets of links) and sequential composition (as sets of directed links), encoding transformations by mappings makes it possible to define these operations for transformations as well, which can be useful for model transformation reuse, compositional design, and chaining.", "num_citations": "10\n", "authors": ["835"]}
{"title": "Three metrics to explore the openness of GitHub projects\n", "abstract": " Open source software projects evolve thanks to a group of volunteers that help in their development. Thus, the success of these projects depends on their ability to attract (and keep) developers. We believe the openness of a project, i.e., how easy is for a new user to actively contribute to it, can help to make a project more attractive. To explore the openness of a software project, we propose three metrics focused on: (1) the distribution of the project community, (2) the rate of acceptance of external contributions and (3) the time it takes to become an official collaborator of the project. We have adapted and applied these metrics to a subset of GitHub projects, thus giving some practical findings on their openness.", "num_citations": "10\n", "authors": ["835"]}
{"title": "Extracting business rules from COBOL: A model-based tool\n", "abstract": " This paper presents a Business Rule Extraction tool for COBOL systems. Starting from a COBOL program, we derive a model-based representation of the source code and we provide a set of model transformations to identify and visualize the embedded business rules. In particular, the tool facilitates the definition of an application vocabulary and the identification of relevant business variables. In addition, such variables are used as starting point to slice the code in order to identify business rules, that are finally represented by means of textual and graphical artifacts. The tool has been developed as an Eclipse plug-in in collaboration with IBM France.", "num_citations": "10\n", "authors": ["835"]}
{"title": "Synthesis of OCL pre-conditions for graph transformation rules\n", "abstract": " Graph transformation (GT) is being increasingly used in Model Driven Engineering (MDE) to describe in-place transformations like animations and refactorings. For its practical use, rules are often complemented with OCL application conditions. The advancement of rule post-conditions into pre-conditions is a well-known problem in GT, but current techniques do not consider OCL. In this paper we provide an approach to advance post-conditions with arbitrary OCL expressions into pre-conditions. This presents benefits for the practical use of GT in MDE, as it allows: (i) to automatically derive pre-conditions from the meta-model integrity constraints, ensuring rule correctness, (ii) to derive pre-conditions from graph constraints with OCL expressions and (iii) to check applicability of rule sequences with OCL conditions.", "num_citations": "10\n", "authors": ["835"]}
{"title": "Model driven tool interoperability in practice\n", "abstract": " Model Driven Engineering (MDE) advocates the use of models, metamodels and model transformations to revisit some of the classical operations in software engineering. MDE has been mostly used with success in forward and reverse engineering (for software development and better maintenance, respectively). Supporting system interoperability is a third important area of applicability for MDE. The particular case of tool interoperability is currently receiving a lot of interest. In this paper, we describe some experiments in this area that have been performed in the context of open source modeling efforts. Taking stock of these achievements, we propose a general framework where various tools are associated to implicit or explicit metamodels. One of the interesting properties of such an organization is that it allows designers starting some software engineering activity with an informal light-weight tool and carrying it out later on in a more complete or formal context. We analyze such situations and discuss the advantages of using MDE to build a general tool interoperability framework.", "num_citations": "10\n", "authors": ["835"]}
{"title": "A simple yet useful approach to implementing UML Profiles in current CASE tools\n", "abstract": " UML Profiles allow designers to customize the UML to their particular domain or purpose. Moreover, they play an important role in MDA. These profiles must be implemented in the CASE tools the designer employs to gain the benefits of their use. This paper identifies the limitations of current CASE tools with respect to profile definitions and presents an approach that overcomes them. The basic idea of our method is to extend the CASE tool with a set of profile operations that act as an interface with designers when they want to use the elements of the profile in their models.", "num_citations": "10\n", "authors": ["835"]}
{"title": "Towards automating the synthesis of chatbots for conversational model query\n", "abstract": " Conversational interfaces (also called chatbots) are being increasingly adopted in various domains such as e-commerce or customer service, as a direct communication channel between companies and end-users. Their advantage is that they can be embedded within social networks, and provide a natural language (NL) interface that enables their use by non-technical users. While there are many emerging platforms for building chatbots, their construction remains a highly technical, challenging task.               In this paper, we propose the use of chatbots to facilitate querying domain-specific models. This way, instead of relying on technical query languages (e.g., OCL), models are queried using NL as this can be more suitable for non-technical users. To avoid manual programming, our solution is based on the automatic synthesis of the model query chatbots from a domain meta-model. These chatbots\u00a0\u2026", "num_citations": "9\n", "authors": ["835"]}
{"title": "Online division of labour: emergent structures in open source software\n", "abstract": " The development Open Source Software fundamentally depends on the participation and commitment of volunteer developers to progress on a particular task. Several works have presented strategies to increase the on-boarding and engagement of new contributors, but little is known on how these diverse groups of developers self-organise to work together. To understand this, one must consider that, on one hand, platforms like GitHub provide a virtually unlimited development framework: any number of actors can potentially join to contribute in a decentralised, distributed, remote, and asynchronous manner. On the other, however, it seems reasonable that some sort of hierarchy and division of labour must be in place to meet human biological and cognitive limits, and also to achieve some level of efficiency. These latter features (hierarchy and division of labour) should translate into detectable structural\u00a0\u2026", "num_citations": "9\n", "authors": ["835"]}
{"title": "Runtime support for rule-based access-control evaluation through model-transformation\n", "abstract": " Access-control policies, often the mechanism of choice to implement the security requirements of confidentiality and integrity, can be found in a wide range of application scenarios. Although there are standard languages for access-control and a plethora of works devoted to assure the well-formedness of access-control policies, little attention has been paid to the problem of providing robust and adaptable runtime evaluation engines for the integration of access-control in new DSL's and platforms. Indeed, the integration of access-control requires the development of critical infrastructure facilities around it, so that the policies can be: 1) analyzed and validated and 2) efficiently evaluated against run-time access requests.", "num_citations": "9\n", "authors": ["835"]}
{"title": "Reverse engineering of database security policies\n", "abstract": " Security is a critical concern for any database. Therefore, database systems provide a wide range of mechanisms to enforce security constraints. These mechanisms can be used to implement part of the security policies requested of an organization. Nevertheless, security requirements are not static, and thus, implemented policies must be changed and reviewed. As a first step, this requires to discover the actual security constraints being enforced by the database and to represent them at an appropriate abstraction level to enable their understanding and reenginering by security experts. Unfortunately, despite the existence of a number of techniques for database reverse engineering, security aspects are ignored during the process. This paper aims to cover this gap by presenting a security metamodel and reverse engineering process that helps security experts to visualize and manipulate security policies in\u00a0\u2026", "num_citations": "9\n", "authors": ["835"]}
{"title": "Positioning of the low-code movement within the field of model-driven engineering\n", "abstract": " Low-code is being promoted as the key infrastructure for the digital transformation of our society. But is there something fundamentally new behind the low-code movement? How does it relate to other concepts like Model-Driven Engineering or Model-Driven development? And what are the implications for researchers in the modeling community?. This position paper tries to shed some light on these issues.", "num_citations": "8\n", "authors": ["835"]}
{"title": "TemporalEMF: A Temporal Metamodeling Framework\n", "abstract": " Existing modeling tools provide direct access to the most current version of a model but very limited support to inspect the model state in the past. This typically requires looking for a model version (usually stored in some kind of external versioning system like Git) roughly corresponding to the desired period and using it to manually retrieve the required data. This approximate answer is not enough in scenarios that require a more precise and immediate response to temporal queries like complex collaborative co-engineering processes or runtime models.                 In this paper, we reuse well-known concepts from temporal languages to propose a temporal metamodeling framework, called TemporalEMF, that adds native temporal support for models. In our framework, models are automatically treated as temporal models and can be subjected to temporal queries to retrieve the model contents at different\u00a0\u2026", "num_citations": "8\n", "authors": ["835"]}
{"title": "Model-based analysis of Java EE web security configurations\n", "abstract": " The widespread use of Java EE web applications as a means to provide distributed services to remote clients imposes strong security requirements, so that the resources managed by these applications remain protected from unauthorized disclosures and manipulations. For this purpose, the Java EE framework provides developers with mechanisms to define access-control policies. Unfortunately, the variety and complexity of the provided security configuration mechanisms cause the definition and manipulation of a security policy to be complex and error prone. As security requirements are not static, and thus, implemented policies must be changed and reviewed often, discovering and representing the policy at an appropriate abstraction level to enable their understanding and reenginering appears as a critical requirement. To tackle this problem, this paper presents a (model-based) approach aimed to help\u00a0\u2026", "num_citations": "8\n", "authors": ["835"]}
{"title": "Lightweight string reasoning in model finding\n", "abstract": " Models play a key role in assuring software quality in the model-driven approach. Precise models usually require the definition of well-formedness rules to specify constraints that cannot be expressed graphically. The Object Constraint Language (OCL) is a de-facto standard to define such rules. Techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. Several tools for these activities have been developed, but to our knowledge, none of them supports OCL string operations on scale that is sufficient for, e.g., model-based testing. As, in contrast, many industrial models do contain such operations, there is evidently a gap. We present a lightweight solver that is specifically tailored to generate large solutions for tractable string constraints in model finding, and that is suited to directly express the\u00a0\u2026", "num_citations": "8\n", "authors": ["835"]}
{"title": "Automating inference of OCL business rules from user scenarios\n", "abstract": " User Scenarios have been advocated as an effective means to capture requirements by describing the system-to-be at the instance or example level. This instance-level information is then used to infer a possible software specification consistent with the provided valid and invalid scenarios. So far existing approaches have often focused on the generation of static models but have omitted the inference of business rules that could complement the static models and improve the precision of the software specification. In this sense this paper provides a first set of invariant inference patterns that are applied on valid and invalid snapshots in order to generate OCL~(Object Constraint Language) integrity constraints that the system should always satisfy. We strengthen the confidence of inferred results based on the user's feedback of generated examples and counterexamples for the considered constraint. The approach is\u00a0\u2026", "num_citations": "8\n", "authors": ["835"]}
{"title": "Alf-Verifier: an eclipse plugin for verifying Alf/UML executable models\n", "abstract": " In this demonstration we present an Eclipse plugin that implements a lightweight method for verifying fine-grained operations at design time. This tool suffices to check that the execution of the operations (specified in Alf Action Language) is consistent with the integrity constraints defined in the class diagram (specified in UML) and returns a meaningful feedback that helps correcting them otherwise.", "num_citations": "8\n", "authors": ["835"]}
{"title": "Non-functional requirements in software architecture practice\n", "abstract": " Dealing with non-functional requirements (NFRs) has posed a challenge onto software engineers for many years. Over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. Knowing more about the state of the practice on these topics may benefit both practitioners\u2019 and researchers\u2019 daily work. A few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that NFRs have on daily architects\u2019 practices. This paper presents some of the findings of an empirical study based on 13 interviews with software architects. It addresses questions such as: who decides the NFRs, what types of NFRs matter to architects, how are NFRs documented, and how are NFRs validated. The results are contextualized with existing previous work.", "num_citations": "8\n", "authors": ["835"]}
{"title": "Industrialization of research tools: The atl case\n", "abstract": " Research groups develop plenty of tools aimed at solving real industrial problems. Unfortunately, most of these tools remain as simple proof-of-concept tools that companies consider too risky to use due to their lack of proper user interface, documentation, completeness, support, etc that companies expect from commercial-quality level tools. Based on our tool development experience in the AtlanMod research team, specially regarding the evolution of our ATL model transformation tool, we argue in this paper that the best solution for research teams aiming to create high-quality and widely-used tools is to industrialize their research prototypes through a partnership with a technology provider.", "num_citations": "8\n", "authors": ["835"]}
{"title": "Towards an Integrated Framework for Model-Driven Security Engineering.\n", "abstract": " Security is a major issue in developing software systems. It is widely recognized that security aspects must be considered in all the phases of the development process from the analysis of the organizational context to the final implementation of the software system. However, current approaches for designing secure systems only target particular security aspects at specific stages of the development process. A unified process combining these different approaches is still missing. This paper surveys several existing techniques and discuss the need of a general framework for integrating them into a single development process.", "num_citations": "8\n", "authors": ["835"]}
{"title": "\u00bf Podemos darle la vuelta a la ense\u00f1anza del desarrollo del software?\n", "abstract": " Con la llegada del Espacio Europeo de Educaci\u00f3n Superior (EEES) a las universidades espa\u00f1olas el \u00e1mbito de conocimiento de desarrollo del software, al igual que otros, tiene una interesante oportunidad para replantear su organizaci\u00f3n curricular, as\u00ed como las competencias que pretende desarrollar en los estudiantes. Un grupo pluridisciplinar de once profesores de \u00e9ste \u00e1mbito (de las \u00e1reas de programaci\u00f3n, bases de datos, ingenier\u00eda del software y sistemas de informaci\u00f3n) de la Universitat Oberta de Catalunya se ha planteado la posibilidad de reordenar, invirti\u00e9ndola, la aproximaci\u00f3n a las competencias de este \u00e1mbito, incorporando nuevos contenidos y eliminando solapamientos y conceptos que se alejan de las necesidades actuales de formaci\u00f3n a nivel universitario. El proceso de an\u00e1lisis descrito en este art\u00edculo as\u00ed como los resultados propuestos no pretenden ser definitivos sino estimular el desarrollo de reflexiones similares que ayuden a mejorar la calidad de la ense\u00f1anza en el contexto de cambio actual.", "num_citations": "8\n", "authors": ["835"]}
{"title": "Openapi bot: A chatbot to help you understand rest apis\n", "abstract": " REST APIs are an essential building block in many Web applications. The lack of a standard machine-readable format to describe these REST APIs triggered the creation of several specification languages to formally define REST APIs, with the OpenAPI specification currently taking the lead. OpenAPI definitions are consumed by a growing ecosystem of tools aimed at automating tasks such as generating server/client SDKs and API documentations. However, current OpenAPI documentation tools mostly provide simple descriptive Web pages enumerating all the API operations and corresponding parameters, but do not offer interactive capabilities to help navigate the API and ask relevant information. Therefore, learning how to use an API and how its different parts are interrelated still requires a considerable time investment. To overcome this situation we present our OpenAPI Bot, a chatbot able to read an OpenAPI\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "WAPIml: Towards a modeling infrastructure for Web APIs\n", "abstract": " Web APIs are becoming key assets for any business. Most of these Web APIs are \"REST-like\", meaning that they adhere partially to the Representational State Transfer (REST) architectural style. The OpenAPI Initiative (OAI) was launched with the objective of creating a vendor neutral, portable, and open specification for describing REST APIs. The initiative has succeeded in attracting major companies and the OpenAPI specification has become de facto format for describing REST APIs. However, there is currently a lack of tools to provide modeling facilities for developers who want to manage and visualize their OpenAPI definitions as models and integrate them into model-based processes. In this paper, we propose WAPIml an OpenAPI round-trip tool that leverages model-driven techniques to create, visualize, manage, and generate OpenAPI definitions. WAPIml embeds an OpenAPI metamodel but also an\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "Model-based analysis of Java EE web security misconfigurations\n", "abstract": " The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "Analysis of co-authorship graphs of CORE-ranked software conferences\n", "abstract": " In most areas of computer science (CS), and in the software domain in particular, international conferences are as important as journals as a venue to disseminate research results. This has resulted in the creation of rankings to provide quality assessment of conferences (specially used for academic promotion purposes) like the well-known CORE ranking created by the Computing Research and Education Association of Australasia. In this paper we analyze 102 CORE-ranked conferences in the software area (covering all aspects of software engineering, programming languages, software architectures and the like) included in the DBLP dataset, an online reference for computers science bibliographic information. We define a suite of metrics focusing on the analysis of the co-authorship graph of the conferences, where authors are represented as nodes and co-authorship relationships as edges. Our aim is\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "Human factors in the adoption of model-driven engineering: an educator\u2019s perspective\n", "abstract": " This paper complements previous empirical studies on teaching Model-driven Engineering (MDE) by reporting on the authors\u2019 attempt at introducing MDE to undergrad students. This is important because: (1) today\u2019s students are tomorrow\u2019s professionals and industrial adoption depends also on the availability of trained professionals and (2) observing problems in the introduction of MDE in the more controlled environment of a classroom setting allows us to identify additional adoption factors, more at the individual level, to be taken into account after in industrial settings. As we report herein, this attempt was largely unsuccessful. We will analyze what went wrong, what we learned from the process and the implications this has for both future endeavors of introducing MDE in both educational and professional environments, particularly regarding human/socio-technical factors to be considered.", "num_citations": "7\n", "authors": ["835"]}
{"title": "Continuing a benchmark for UML and OCL design and analysis tools\n", "abstract": " UML and OCL are frequently employed languages in model-based engineering. OCL is supported by a variety of design and analysis tools having different scopes, aims and technological corner stones. The spectrum ranges from treating issues concerning formal proof techniques to testing approaches, from validation to verification, and from logic programming and rewriting to SAT-based technology.                 This paper presents steps towards a well-founded benchmark for assessing UML and OCL validation and verification techniques. It puts forward a set of UML and OCL models together with particular questions centered around OCL and the notions consistency, independence, consequences, and reachability. Furthermore aspects of integer arithmetic and aggregations functions (in the spirit of SQL functions as COUNT or SUM) are discussed. The claim of the paper is not to present a complete\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "An empirical study on simplification of business process modeling languages\n", "abstract": " The adaptation, specially by means of a simplification process, of modeling languages is a common practice due to the overwhelming complexity of most standard languages (like UML or BPMN), not needed for typical usage scenarios while at the same time companies don't want to go to the extremes of defining a brand new domain specific language. Unfortunately, there is a lack of examples of such simplification experiences that can be used as a reference for future projects. In this paper we report on a field study aimed at the simplification of a business process modeling language (namely, BPMN) for making it suitable to end users. Our simplification process relies on a set of steps that encompass the selection of the language elements to simplify, generation of a set of language variants for them, measurement of effectiveness of the variants through user modeling sessions and extraction of quantitative and\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "Enabling the reuse of stored model transformations through annotations\n", "abstract": " With the increasing adoption of MDE, model transformations, one of its core concepts together with metamodeling, stand out as a valuable asset. Therefore, a mechanism to annotate and store existing model transformations appears as a critical need for their efficient exploitation and reuse. Unfortunately, although several reuse mechanisms have been proposed for software artifacts in general and models in particular, none of them is specially tailored to the domain of model transformations. In order to fill this gap, we present here such a mechanism. Our approach is composed by two elements (1) a new DSL specially conceived for describing model transformations in terms of their functional and non-functional properties (2) a semi-automatic process for annotating and querying (repositories of) model transformations using as criteria the properties of our DSL. We validate the feasibility of our approach\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "An adapter-based approach to co-evolve generated SQL in model-to-text transformations\n", "abstract": " Forward Engineering advocates for code to be generated dynamically through model-to-text transformations that target a specific platform. In this setting, platform evolution can leave the transformation, and hence the generated code, outdated. This issue is exacerbated by the perpetual beta phenomenon in Web 2.0 platforms where continuous delta releases are a common practice. Here, manual co-evolution becomes cumbersome. This paper looks at how to automate \u2014fully or in part\u2014the synchronization process between the platform and the transformation. To this end, the transformation process is split in two parts: the stable part is coded as a MOFScript transformation whereas the unstable side is isolated through an adapter that is implicitly called by the transformation at generation time. In this way, platform upgrades impact the adapter but leave the transformation untouched. The work\u00a0\u2026", "num_citations": "7\n", "authors": ["835"]}
{"title": "P ORTOLAN: a Model-Driven Cartography Framework\n", "abstract": " Processing large amounts of data to extract useful information is an essential task within companies. To help in this task, visualization techniques have been commonly used due to their capacity to present data in synthesized views, easier to understand and manage. However, achieving the right visualization display for a data set is a complex cartography process that involves several transformation steps to adapt the (domain) data to the (visualization) data format expected by visualization tools. To maximize the benefits of visualization we propose Portolan, a generic model-driven cartography framework that facilitates the discovery of the data to visualize, the specification of view definitions for that data and the transformations to bridge the gap with the visualization tools. Our approach has been implemented on top of the Eclipse EMF modeling framework and validated on three different use cases.", "num_citations": "7\n", "authors": ["835"]}
{"title": "Reverse Engineering of OO constructs in Object-Relational Database Schemas\n", "abstract": " Reverse engineering applied to databases permits to extract a conceptual schema that represents, at a higher level of abstraction, the database implementation. This resulting conceptual schema may be used to facilitate, among others, system maintenance, evolution and reuse. In the last years, the use of object-relational constructs was incorporated into database development. However, reverse engineering techniques for these specific constructs have not been yet provided. In this sense, the main goal of this paper is to present a method that considers these new constructs in the reverse engineering of an existing object-relational database. As a result of the process, our method returns an equivalent conceptual schema specified in UML (extended with a set of OCL integrity constraints) that represents, at a conceptual level, the database schema. We provide a prototype tool that implements our method for the Oracle9i database management system.", "num_citations": "7\n", "authors": ["835"]}
{"title": "Advanced prefetching and caching of models with PrefetchML\n", "abstract": " Caching and prefetching techniques have been used for decades in database engines and file systems to improve the performance of I/O-intensive application. A prefetching algorithm typically benefits from the system\u2019s latencies by loading into main memory elements that will be needed in the future, speeding up data access. While these solutions can bring a significant improvement in terms of execution time, prefetching rules are often defined at the data level, making them hard to understand, maintain, and optimize. In addition, low-level prefetching and caching components are difficult to align with scalable model persistence frameworks because they are unaware of potential optimizations relying on the analysis of metamodel-level information and are less present in NoSQL databases, a common solution to store large models. To overcome this situation, we propose PrefetchML, a framework that\u00a0\u2026", "num_citations": "6\n", "authors": ["835"]}
{"title": "Are CS conferences (too) closed communities?\n", "abstract": " Assessing whether newcomers have a more difficult time achieving paper acceptance at established conferences.", "num_citations": "6\n", "authors": ["835"]}
{"title": "Better call the crowd: using crowdsourcing to shape the notation of domain-specific languages\n", "abstract": " Crowdsourcing has emerged as a novel paradigm where humans are employed to perform computational tasks. In the context of Domain-Specific Modeling Language (DSML) development, where the involvement of end-users is crucial to assure that the resulting language satisfies their needs, crowdsourcing tasks could be defined to assist in the language definition process. By relying on the crowd, it is possible to show an early version of the language to a wider spectrum of users, thus increasing the validation scope and eventually promoting its acceptance and adoption. We propose a systematic method for creating crowdsourcing campaigns aimed at refining the graphical notation of DSMLs. The method defines a set of steps to identify, create and order the questions for the crowd. As a result, developers are provided with a set of notation choices that best fit end-users' needs. We also report on an experiment\u00a0\u2026", "num_citations": "6\n", "authors": ["835"]}
{"title": "Towards a UML and IFML Mapping to GraphQL\n", "abstract": " Web APIs have become first-class citizens on the Web, in particular, to provide a more unified access to heterogeneous data sources that organizations want to make publicly available. While REST APIs have become the norm to structure web APIs, they can be regarded as a server-side solution, offering default limited query capabilities and therefore forcing developers to implement ad-hoc solutions for clients requiring to perform complex queries on the data. Lately, GraphQL has gained popularity as a way to simplify this work. GraphQL is a query language for Web APIs specially designed to build client applications by providing an intuitive and flexible syntax for describing their data schema, requirements and interactions. In this paper we propose an approach for the generation of GraphQL schemas from UML class diagrams and IFML interaction models, two well-known standard modeling languages in\u00a0\u2026", "num_citations": "6\n", "authors": ["835"]}
{"title": "Software modernization revisited: Challenges and prospects\n", "abstract": " The authors discuss important factors to consider when migrating software to the cloud and offer recommendations to maximize the chance of success.", "num_citations": "6\n", "authors": ["835"]}
{"title": "UMLto [No] SQL: Mapping Conceptual Schemas to Heterogeneous Datastores\n", "abstract": " The growing need to store and manipulate large volumes of data has led to the blossoming of various families of data storage solutions. Software modelers can benefit from this growing diversity to improve critical parts of their applications, using a combination of different databases to store the data based on access, availability, and performance requirements. However, while the mapping of conceptual schemas to relational databases is a well-studied field of research, there are few works that target the role of conceptual modeling in a multiple and diverse data storage settings. This is particularly true when dealing with the mapping of constraints in the conceptual schema. In this paper we present the UMLto[No]SQL approach that maps conceptual schemas expressed in UML/OCL into a set of logical schemas (either relational or NoSQL ones) to be used to store the application data according to the data partition\u00a0\u2026", "num_citations": "5\n", "authors": ["835"]}
{"title": "Robust hashing for models\n", "abstract": " The increased adoption of model-driven engineering (MDE) in complex industrial environments highlights the value of a company's modeling artefacts. As such, any MDE ecosystem must provide mechanisms to both, protect, and take full advantage of these valuable assets.", "num_citations": "5\n", "authors": ["835"]}
{"title": "APIComposer: Data-driven composition of REST APIs\n", "abstract": " More and more companies and governmental organizations are publishing data on the Web via REST APIs. The increasing number of REST APIs has promoted the creation of specialized applications aiming to combine and reuse different data sources to generate and deduce new information. However, creating such applications is a tedious and error-prone process since developers must invest much time in discovering the data model behind each candidate REST API, define the composition strategy, and manually implement such strategy. To facilitate this process, we propose an approach to automatically compose and orchestrate data-oriented REST APIs. For an initial set of REST APIs, we discover the data models, identify matching concepts, obtain a global model, and make the latter available on the Web as a global REST API. A prototype tool relying on OpenAPI for describing APIs and on OData for\u00a0\u2026", "num_citations": "5\n", "authors": ["835"]}
{"title": "A UML profile for OData web APIs\n", "abstract": " More and more individuals and organizations are making their data available online publicly, resulting in a growing market of technologies and services to help consume data and extract its real value. One of the several ways to publish data on the Web is via Web APIs. Unlike other approaches like RDF, Web APIs provide a simple way to query structured data by relying only on the HTTP protocol. Standards and frameworks such as Open API or API Blueprint offer a way to create Web APIs but OData stands out from the rest as it is specifically tailored to deal with data sources. However, creating an OData Web API is a hard and time-consuming task for data providers as they have to choose between relying on commercial solutions, which are heavy and require a deep knowledge of their corresponding platforms, or create a customized solution to share their data. We propose an approach that leverages on\u00a0\u2026", "num_citations": "5\n", "authors": ["835"]}
{"title": "Attracting contributions to your github project\n", "abstract": " Most Open Source Software projects can only progress thanks to developers willing to voluntarily contribute. Therefore, their vitality and success largely depend on their ability to attract developers. Code hosting platforms like GitHub aim at making software development more collabo-rative and attractive for contributors by providing facilities such as issue-tracking, code review or team management on top of a Git repository following a pull-based model to handle external contributions. We study whether the use of these facilities actually help to get more contributions based on a quantitative analysis over a dataset composed by all the GitHub projects created in the last two years. We discovered that most projects actually ignore them and that, those that don't, do not advance faster either. A manual analysis of the most successful projects suggests that other factors like clear description of the contribution and gover-nance rules for the project have a greater impact.", "num_citations": "5\n", "authors": ["835"]}
{"title": "On Automating Inference of OCL Constraints from Counterexamples and Examples\n", "abstract": " Within model-based approaches, defining domains and domain restrictions for conceptual models or metamodels is significant. Recently, a domain is often presented as a class diagram, and domain restrictions are expressed using the Object Constraint Language\u00a0(OCL). An effective method to define a domain is based on a description of the domain at the instance and example level. So far such a method has often focused on the generation of structure aspects, but have omitted the inference of OCL restrictions that could complement the domain structure and improve the precision of the domain. This paper proposes an approach to automating the inference of OCL restrictions from a domain description in terms of counter- and examples. Candidates are generated by a problem solving, and irrelevant ones are eliminated using the user feedback on generated counter- and examples. Our approach is\u00a0\u2026", "num_citations": "5\n", "authors": ["835"]}
{"title": "On developing open source mde tools: Our eclipse stories and lessons learned\n", "abstract": " Tool development has always been a fundamental activity of Software Engineering. Nowadays, open source is changing the way this is done in many organizations. Traditional ways of doing things are progressively enhanced or even sometimes replaced by new organizational schemes, benefiting as much as possible from the properties of open source (OS). This is especially true in innovative areas such as Model Driven Engineering (MDE) in which new tools are constantly created, developed and disseminated, many of them coming from research teams. This poses some hard questions: What is the actual impact of OS in terms of tool development? How to best take advantage of OS communities? And what are the opportunities for research teams in this context? Capitalizing on experiences in developing MDE OS tools on top of the Eclipse platform and its license model, we try to give some insights on these questions in this paper.", "num_citations": "5\n", "authors": ["835"]}
{"title": "Improving the scalability of Web applications with runtime transformations\n", "abstract": " The scalability of modern Web applications has become a key aspect for any business in order to support thousands of concurrent users while reducing its computational costs. If a Web application does not scale, a few hundred users can take the application down and as a consequence cause business problems in their companies. In addition, being able to scale a Web application is not an easy task, as it involves many technical aspects such as architecture design, performance, monitoring and availability that are completely ignored by current Model Driven Web Engineering approaches. In this paper we present a model-based approach that uses runtime transformations for overcoming scalability problems in the applications derived from them. We present our approach by \u201cscaling up\u201d a WebML application under a stress scenario, proving that it provides a \u201cframework\u201d for overcoming scalability issues.", "num_citations": "5\n", "authors": ["835"]}
{"title": "How to Deal with your IT Legacy? What is Coming up in MoDisco\n", "abstract": " The Eclipse-MDT MoDisco open source project is part of the Indigo Eclipse Simultaneous Release. Here we describe how MoDisco can play a role in the evolution of (legacy) software, focusing on the latest project news.", "num_citations": "5\n", "authors": ["835"]}
{"title": "Lightweight Executability Analysis of Graph Transformation Rules\n", "abstract": " Domain Specific Visual Languages (DSVLs) play a cornerstone role in Model-Driven Engineering (MDE), where (domain specific) models are used to automate the production of the final application. Graph Transformation is a formal, visual, rule-based technique, which is increasingly used in MDE to express in-place model transformations like refactorings, animations and simulations. However, there is currently a lack of methods able to perform static analysis of rules, taking into account the DSVL meta-model integrity constraints. In this paper we propose a lightweight, efficient technique that performs static analysis of the weak executability of rules. The method determines if there is some scenario in which the rule can be safely applied, without breaking the meta-model constraints. If no such scenario exists, the method returns meaningful feedback that helps repairing the detected inconsistencies.", "num_citations": "5\n", "authors": ["835"]}
{"title": "Implementing advanced RBAC administration functionality with USE\n", "abstract": " Role-based access control (RBAC) is a powerful means for laying out and developing higher-level organizational policies such as separation of duty, and for simplifying the security management process. One of the important aspects of RBAC is authorization constraints that express such organizational policies. While RBAC has generated a great interest in the security community, organizations still seek a flexible and effective approach to impose role-based authorization constraints in their security-critical applications. In particular, today often only basic RBAC concepts have found their way into commercial RBAC products; specifically, authorization constraints are not widely supported. In this paper, we present an RBAC administration tool that can enforce certain kinds of role-based authorization constraints such as separation of duty constraints. The authorization constraint functionality is based upon the OCL validation tool USE. We also describe our practical experience that we gained on integrating OCL functionality into a prototype of an RBAC administration tool that shall be extended to a product in the future.", "num_citations": "5\n", "authors": ["835"]}
{"title": "An NLP-Based Architecture for the Autocompletion of Partial Domain Models\n", "abstract": " Domain models capture the key concepts and relationships of a business domain. Typically, domain models are manually defined by software designers in the initial phases of a software development cycle, based on their interactions with the client and their own domain expertise. Given the key role of domain models in the quality of the final system, it is important that they properly reflect the reality of the business.", "num_citations": "4\n", "authors": ["835"]}
{"title": "Participation Inequality and the 90-9-1 Principle in Open Source\n", "abstract": " Participation inequality is a major challenge in any shared-resource system. This is known as the\" volunteer's dilemma\": everybody wants to benefit from a resource without contributing, expecting others will do the work. This paper explores whether this problem also arises in open source development. In particular, we analyze the behaviour of GitHub users to assess whether the 90-9-1 principle applies to open source. We study it both from a qualitative (ratio of activity types) and a quantitative (total number of activities) perspective and we show that the principle does not hold if we consider the GitHub platform as a whole. Surprisingly, results are reversed depending on the specific projects we look at. We believe these results are useful to project managers to better understand and optimize the behaviour of the community around their projects and, as a side effect, they show the importance of diversity in sample\u00a0\u2026", "num_citations": "4\n", "authors": ["835"]}
{"title": "Scalable model views over heterogeneous modeling technologies and resources\n", "abstract": " When engineering complex systems, models are typically used to represent various systems aspects. These models are often heterogeneous in terms of modeling languages, provenance, number or scale. As a result, the information actually relevant to engineers is usually split into different kinds of interrelated models. To be useful in practice, these models need to be properly integrated to provide global views over the system. This has to be made possible even when those models are serialized or stored in different formats adapted to their respective nature and scalability needs. Model view approaches have been proposed to tackle this issue. They provide unification mechanisms to combine and query various different models in a transparent way. These views usually target specific engineering tasks such as system design, monitoring and evolution. In an industrial context, there can be many large-scale use cases\u00a0\u2026", "num_citations": "4\n", "authors": ["835"]}
{"title": "A systematic approach to generate diverse instantiations for conceptual schemas\n", "abstract": " Generating valid instantiations for a conceptual schema is instrumental in ensuring its quality by means of verification, validation or testing. This problem becomes even more challenging when we also require that the computed instantiations exhibit significant differences among them, i.e., they are diverse. In this work, we propose an automatic method that guarantees synthesizing a diverse set of instantiations from a conceptual schema by combining model finders, classifying terms and constraint strengthening techniques. This technique has been implemented in the USE tool for UML/OCL.", "num_citations": "4\n", "authors": ["835"]}
{"title": "On the need for intellectual property protection in model-driven co-engineering processes\n", "abstract": " We live in an increasingly complex world where all systems tend to include heterogeneous and interconnected components. To cope with these systems, industry is shifting towards co-engineering development processes where partners with very different roles and access needs must collaborate together. Therefore, protecting the intellectual property (IP) of the shared assets is a must. Model-Driven Engineering (MDE) may play a key role in the successful enactment of industrial co-engineering processes but only if it succeeds at integrating at its core the concern for IP protection, that has been up\u00a0to the date largely ignored. In order to advance in this direction, we provide in this paper an initial roadmap towards the holistic protection of IP in collaborative modeling scenarios and we discuss how existing technologies such as Cryptography, Access-control (AC) or Digital Rights Management (DRM) are\u00a0\u2026", "num_citations": "4\n", "authors": ["835"]}
{"title": "Metascience: An holistic approach for research modeling\n", "abstract": " Conferences have become primary sources of dissemination in computer science research, in particular, in the software engineering and database fields. Assessing the quality, scope and community of conferences is therefore crucial for any researcher. However, digital libraries and online bibliographic services offer little help on this, thus providing only basic metrics. Researchers are instead forced to resort to the tedious task of manually browsing different sources (e.g., DBLP, Google Scholar or conference sites) to gather relevant information about a given venue. In this paper we propose a conceptual schema providing a holistic view of conference-related information (e.g., authors, papers, committees and topics). This schema is automatically and incrementally populated with data available online. We show how this schema can be used as a single information source for a variety of complex queries and\u00a0\u2026", "num_citations": "4\n", "authors": ["835"]}
{"title": "Towards a Corpus of Use-Cases for Model-Driven Engineering Courses.\n", "abstract": " Having taught Model-Driven Engineering courses for a number of years, in this paper we reflect on the importance of selecting appropriate use-cases for students to explore related principles and technologies. We discuss examples on both ends of the spectrum and we present guidelines for selecting use-cases that are pragmatic and motivating without being excessively complex.", "num_citations": "4\n", "authors": ["835"]}
{"title": "ice Model-Driven Software Engineering in Practice\n", "abstract": " This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE).MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, eg, due to the convergence of software development and business analysis.", "num_citations": "4\n", "authors": ["835"]}
{"title": "An MDE-based approach for solving configuration problems: An application to the Eclipse platform\n", "abstract": " Most of us have experienced configuration issues when installing new software applications. Finding the right configuration is often a challenging task since we need to deal with many dependencies between plug-ins, components, libraries, packages, etc; sometimes even regarding specific versions of the involved artefacts. Right now, most configuration engines are adhoc tools designed for specific configuration scenarios. This makes their reuse in different contexts very difficult.In this paper we report on our experience in following a MDE-based approach to solve configuration problems. In our approach, the configuration problem is represented as a model that abstracts all irrelevant technological details and facilitates the use of generic (constraint) solvers to find optimal solutions. This approach has been applied by an industrial partner to the management of plug-ins in the Eclipse framework, a big issue\u00a0\u2026", "num_citations": "4\n", "authors": ["835"]}
{"title": "Modelling safe interface interactions in web applications\n", "abstract": " Current Web applications embed sophisticated user interfaces and business logic. The original interaction paradigm of the Web based on static content pages that are browsed by hyperlinks is, therefore, not valid anymore. In this paper, we advocate a paradigm shift for browsers and Web applications, that improves the management of user interaction and browsing history. Pages are replaced by States as basic navigation nodes, and Back/Forward navigation along the browsing history is replaced by a full-fledged interactive application paradigm, supporting transactions at the interface level and featuring Undo/Redo capabilities. This new paradigm offers a safer and more precise interaction model, protecting the user from unexpected behaviours of the applications and the browser.", "num_citations": "4\n", "authors": ["835"]}
{"title": "On the Quality of Navigation Models with Content-Modification Operations\n", "abstract": " Initially, web development methods focused on the generation of read-only web applications for browsing the data stored in relational database systems. Lately, many have evolved to include content-modification functionalities. As a consequence, we believe that existing quality properties for web model designs must be complemented with new property definitions. In particular, we propose two new quality properties that take the relationship between navigation models and the related data models into account. The properties check if navigation models include all necessary content-modification operations and whether all possible navigation paths modify the underlying data in a consistent way. In this paper, we show how to determine if a navigation model verifies both properties and also how to, given a data model, automatically generate a preliminary navigation model satisfying them.", "num_citations": "4\n", "authors": ["835"]}
{"title": "An OpenAPI-Based Testing Framework to Monitor Non-functional Properties of REST APIs\n", "abstract": " REST APIs have become key assets for any company willing to have online presence and provide access to its services. Several approaches have been proposed to describe this kind of APIs, being OpenAPI the dominant proposal in the last years. OpenAPI allows any consumer to understand the operations and data elements of a REST API. However, it does not cover any kind of non-functional properties, such as performance and availability. In this paper we present Gadolinium, a framework that leverages the OpenAPI specification to test non-functional properties of REST APIs. Gadolinium automatically tests performance and availability in different geographical locations by means of a master/slave architecture. The results of the test can eventually be injected in the original OpenAPI definition of the REST API. Demo: http://hdl. handle. net/20.500. 12004/1/C/ICWE/2020/001", "num_citations": "3\n", "authors": ["835"]}
{"title": "Efficient plagiarism detection for software modeling assignments\n", "abstract": " Background and Context Reports suggest plagiarism is a common occurrence in universities. While plagiarism detection mechanisms exist for textual artifacts, this is less so for non-code related ones such as software design artifacts like models, metamodels or model transformations.Objective To provide an efficient mechanism for the detection of plagiarism in repositories of Model-Driven Engineering (MDE) assignments.Method Our approach is based on the adaptation of the Locality Sensitive Hashing, an approximate nearest neighbor search mechanism, to the modeling technical space. We evaluate our approach on a real use case consisting of two repositories containing 10 years of student answers to MDE course assignments.Findings We have found that:  effectively, plagiarism occurred on the aforementioned course assignments  our tool was able to efficiently detect them.Implications Plagiarism\u00a0\u2026", "num_citations": "3\n", "authors": ["835"]}
{"title": "Temporal Models on Time Series Databases\n", "abstract": " With the emergence of Cyber-Physical Systems (CPS), several sophisticated runtime monitoring solutions have been proposed in order to deal with extensive execution logs. One promising development in this respect is the integration of time series databases that support the storage of massive amounts of historical data as well as to provide fast query capabilities to reason about runtime properties of such CPS.In this paper, we discuss how conceptual modeling can benefit from time series databases, and vice versa. In particular, we present how metamodels and their instances, ie, models, can be partially mapped to time series databases. Thus, the traceability between design and simulation/runtime activities can be ensured by retrieving and accessing runtime information, ie, time series data, in design models. On this basis, the contribution of this paper is four-fold. First, a dedicated profile for annotating design models for time series databases is presented. Second, a mapping for integrating the metamodeling framework EMF with InfluxDB is introduced as a technology backbone enabling two distinct mapping strategies for model information. Third, we demonstrate how continuous time series queries can be combined with the Object Constraint Language (OCL) for navigation through models, now enriched with derived runtime properties. Finally, we also present an initial evaluation of the different mapping strategies with respect to data storage and query performance. Our initial results show the efficiency of applying derived runtime properties as time series queries also for large model histories.", "num_citations": "3\n", "authors": ["835"]}
{"title": "Towards modeling framework for DevOps: requirements derived from industry use case\n", "abstract": " To succeed with the development, deployment, and operation of the new generation of complex systems, organizations need the agility to adapt to constantly evolving environments. In this context, DevOps has emerged as an evolution of the agile approaches. It focuses on optimizing the flow of activities involved in the creation of end-user value, from idea to deployed functionality and operating systems. However, in spite of its popularity, DevOps still lacks proper engineering frameworks to support continuous improvement. One of our key objectives is to contribute to the development of a DevOps engineering framework composed of process, methods, and tools. A core part of this framework relates to the modeling of the different aspects of the DevOps system. To better understand the requirements of modeling in a DevOps context, we focus on a Product Build use case provided by an industry partner.", "num_citations": "3\n", "authors": ["835"]}
{"title": "Enabling performance modeling for the masses: Initial experiences\n", "abstract": " Performance problems such as sluggish response time or low throughput are especially annoying, frustrating and noticeable to users. Fixing performance problems after they occur results in unplanned expenses and time. Our vision is an MDE-intensive software development paradigm for complex systems in which software designers can evaluate performance early in development, when the analysis can have the greatest impact. We seek to empower designers to do the analysis themselves by automating the creation of performance models out of standard design models. Such performance models can be automatically solved, providing results meaningful to them. In our vision, this automation can be enabled by using model-to-model transformations: First, designers create UML design models embellished with the Modeling and Analysis of Real Time and Embedded systems (MARTE) design\u00a0\u2026", "num_citations": "3\n", "authors": ["835"]}
{"title": "Fixing defects in integrity constraints via constraint mutation\n", "abstract": " Defining appropriate integrity constraints (ICs) for the domain model of a software system is a complex and error-prone task. Both over-constraining and under-constraining the information base are undesirable. In this paper, we consider a systematic approach to explore the most suitable ICs for a software system. The inputs of this approach are an initial tentative set of ICs described in OCL (Object Constraint Language) plus a sample information base which is incorrectly forbidden (allowed) by them. Then, this method generates candidate weaker (stronger) versions of the ICs by mutating them in an structured way. Modelers can then replace the original defective set with the alternative versions to improve the quality of the domain model.", "num_citations": "3\n", "authors": ["835"]}
{"title": "Model-driven development of OData services: An application to relational databases\n", "abstract": " Open Data Protocol (OData) is a protocol to facilitate the publication and consumption of queryable and interoperable data-driven online services. OData is based on the use of RESTful APIs derived from a data model plus a URL-based query language to identify and filter the data described in such model. Due to its maturity and ease of use for end-users and client applications, OData has become the natural choice to publish datasets online. Still, creating OData services is a tedious and time-consuming task, since data providers should (1) represent their data models in OData format, (2) implement the business logic to transform OData requests to SQL statements (or the target storage technology of choice), and (3) de/serialize the exchanged messages conforming to the OData protocol. This paper presents a model-based approach aimed at (semi)automating all these steps. From an initial UML class diagram, we\u00a0\u2026", "num_citations": "3\n", "authors": ["835"]}
{"title": "Gitana: A software project inspector\n", "abstract": " The development of software projects entails significant implementation and collaboration activities, typically supported by tools such as issue trackers, code review tools and Version Control Systems. However, these tools only provide a partial view of the project and often lack of advanced querying mechanisms, thus hampering the analysis of the status of the project and endangering the decision making process on the best way to drive the development process. We present Gitana, a software project inspector able to import the activity of the different support tools into a single relational database, thus providing a central point to perform all kinds of cross-cutting analysis on the software project data. Tool website: https://github. com/SOM-Research/gitana.", "num_citations": "3\n", "authors": ["835"]}
{"title": "An LSP infrastructure to build EMF language servers for web-deployable model editors.\n", "abstract": " The development of modern IDEs is still a challenging and timeconsuming task, which requires implementing the support for language-specific features such as syntax highlighting or validation. When the IDE targets a graphical language, its development becomes even more complex due to the rendering and manipulation of the graphical notation symbols. To simplify the development of IDEs, the Language Server Protocol (LSP) proposes a decoupled approach based on language-agnostic clients and language-specific servers. LSP clients communicate changes to LSP servers, which validate and store language instances. However, LSP only addresses textual languages (ie, character as atomic unit) and neglects the support for graphical ones (ie, nodes/edges as atomic units). In this paper, we introduce a novel LSP infrastructure to simplify the development of new graphical modeling tools, in which Web technologies may be used for editor front-ends while leveraging existing modeling frameworks to build language servers. More concretely, in this work, we present the architecture of our LSP infrastructure, based on LSP4J, to build EMF-based graphical language servers.", "num_citations": "3\n", "authors": ["835"]}
{"title": "Une approximation de la pr\u00e9vision saisonni\u00e8re des \u00e9tiages et s\u00e9cheresses en Catalogne\n", "abstract": " Les \u00e9tiages et les s\u00e9cheresses sont une caract\u00e9ristique hydro-climatique en Espagne. La derni\u00e8re s\u00e9cheresse qui a affect\u00e9 l\u2019Espagne a \u00e9t\u00e9 extraordinairement grave en Catalogne, o\u00f9 elle s\u2019est prolong\u00e9e entre l\u2019ann\u00e9e 2004 et le printemps de 2008. L\u2019objectif de cette contribution est de montrer l\u2019\u00e9volution de cette derni\u00e8re s\u00e9cheresse dans le cadre de la caract\u00e9risation des s\u00e9cheresses en Espagne, ainsi qu\u2019une approximation pour faire une pr\u00e9vision saisonni\u00e8re du d\u00e9bit des rivi\u00e8res qui nourrissent les principaux barrages des Bassins Internes de la Catalogne et la ville de Barcelone. Etant donn\u00e9 que les principales rivi\u00e8res naissent dans les Pyr\u00e9n\u00e9es et Pr\u00e9-Pyr\u00e9n\u00e9es, nous avons centr\u00e9 l\u2019analyse pluviom\u00e9trique sur cette r\u00e9gion. En premier lieu, nous avons caract\u00e9ris\u00e9 les situations m\u00e9t\u00e9orologiques associ\u00e9es au d\u00e9ficit de pr\u00e9cipitation en la r\u00e9gion d\u2019\u00e9tude. Ensuite, nous avons cr\u00e9\u00e9 des s\u00e9ries de pr\u00e9cipitation\u00a0\u2026", "num_citations": "3\n", "authors": ["835"]}
{"title": "Verifying action semantics specifications in UML behavioral models (Extended version)\n", "abstract": " MDD and MDA approaches require capturing the behavior of UML models in sufficient detail so that the models can be automatically implemented/executed in the production environment. With this purpose, Action Semantics (AS) were added to the UML specification as the fundamental unit of behavior specification. Actions are the basis for defining the fine-grained behavior of operations, activity diagrams, interaction diagrams and state machines. Unfortunately, current proposals devoted to the verification of behavioral schemas tend to skip the analysis of the actions they may include. The main goal of this paper is to cover this gap by presenting several techniques aimed at verifying AS specifications. Our techniques are based on the static analysis of the dependencies between the different actions included in the behavioral schema. For incorrect specifications, our method returns a meaningful feedback that helps repairing the inconsistency.", "num_citations": "3\n", "authors": ["835"]}
{"title": "UMLtoSBVR: An SBVR-based tool to validate UML conceptual schemas\n", "abstract": " The UMLtoSBVR tool facilitates the interaction between designers and business people in order to refine and validate the information modeled in a UML conceptual schema (CS) to make sure that the CS is correct before starting the implementation phase. To this end, the tool is able to paraphrase the CS, i.e. to describe the CS elements using natural language expressions that can be understood by the business people lacking of the technical background to directly understand the UML notation. As an intermediate step, the tool transforms the UML CS into a SBVR (Semantics of Business Vocabulary and Business Rules) specification. The SBVR standard facilitates the expression of the CS in different natural language styles (as Structured English or Rule Speak).", "num_citations": "3\n", "authors": ["835"]}
{"title": "Towards the optical character recognition of DSLs\n", "abstract": " OCR engines aim to identify and extract text strings fromdocuments or images. While current efforts focus mostly inmainstream languages, there is little support for program-ming or domain-specific languages (DSLs). In this paper, wepresent our vision about the current state of OCR recognitionfor DSLs and its challenges. We discuss some strategies toimprove the OCR quality applied to DSL textual expressionsby leveraging DSL specifications and domain data. To bettersupport our ideas we present the preliminary results of anempirical study and outline a research roadmap.", "num_citations": "2\n", "authors": ["835"]}
{"title": "A model-based approach for developing event-driven architectures with AsyncAPI\n", "abstract": " In this Internet of Things (IoT) era, our everyday objects have evolved into the so-called cyber-physical systems (CPS). The use and deployment of CPS has especially penetrated the industry, giving rise to the Industry 4.0 or Industrial IoT (IIoT). Typically, architectures in IIoT environments are distributed and asynchronous, communication being guided by events such as the publication of (and corresponding subscription to) messages.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Papyrus for gamers, let's play modeling\n", "abstract": " Gamification refers to the exploitation of gaming mechanisms for serious purposes, like learning hard-to-train skills such as modeling.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Scalable modeling technologies in the wild: an experience report on wind turbines control applications development\n", "abstract": " Scalability in modeling has many facets, including the ability to build larger models and domain-specific languages (DSLs) efficiently. With the aim of tackling some of the most prominent scalability challenges in model-based engineering (MBE), the MONDO EU project developed the theoretical foundations and open-source implementation of a platform for scalable modeling and model management. The platform includes facilities for building large graphical DSLs, for splitting large models into sets of smaller interrelated fragments, to index large collections of models to speed-up their querying, and to enable the collaborative construction and refinement of complex models, among other features. This paper reports on the tools provided by MONDO that Ikerlan, a medium-sized technology center which in the last decade has embraced the MBE paradigm, adopted in order to improve their processes. This experience\u00a0\u2026", "num_citations": "2\n", "authors": ["835"]}
{"title": "Recent developments in OCL and textual modelling\n", "abstract": " The panel session of the 16th OCL workshop featured a lightning talk session for discussing recent developments and open questions in the area of OCL and textual modelling. During this session, the OCL community discussed, stimulated through short presentations by OCL experts, tool support, potential future extensions, and suggested initiatives to make the textual modelling community even more successful. This collaborative paper, to which each OCL expert contributed one section, summarises the discussions as well as describes the recent developments and open questions presented in the lightning talks.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Composing JSON-based web APIs\n", "abstract": " The development of Web APIs has become a discipline that companies have to master to succeed in the Web. The so-called API economy is pushing companies to provide access to their data by means of Web APIs, thus requiring web developers to study and integrate such APIs into their applications. The exchange of data with these APIs is usually performed by using JSON, a schemaless data format easy for computers to parse and use. While JSON data is easy to read, its structure is implicit, thus entailing serious problems when integrating APIs coming from di erent vendors. Web developers have therefore to understand the domain behind each API and study how they can be composed. We tackle this issue by presenting an approach able to both discover the domain of JSON-based Web APIs, and identify composition links among them. Our approach allows developers to easily visualize what is behind APIs and how they can be composed to be used in their applications.", "num_citations": "2\n", "authors": ["835"]}
{"title": "MDE Support for Enterprise Architecture in an Industrial Context: the TEAP Framework Experience\n", "abstract": " Model Driven Engineering (MDE) is often applied to support software engineering processes (i.e., from reverse to forward engineering, including maintenance and/or evolution tasks). However, as promoted by the Model Driven Organization (MDO) initiative, it can also be relevant in more business-oriented and strategic decision-making activities such as Enterprise Architecture (EA). EA is the process of translating business vision and strategy into effective change by better describing the enterprise's future state and thus enable its evolution. Even if several approaches have already proposed different kinds of support to deal with the company's EA, an integrated MDE framework combining EA data federation, EA standard adaptation and multiple viewpoint support is still missing. This paper reports on our ongoing experience of building the TEAP MDE framework (based on the TOGAF standard and SmartEA tooling) notably addressing these three challenges in an industrial EA context.", "num_citations": "2\n", "authors": ["835"]}
{"title": "On verifying ATL transformations using \u2018off-the-shelf\u2019SMT solvers: Examples\n", "abstract": " MDE is a software development process where models constitute piv-otal elements of the software to be built. If models are well-specified, transfor-mations can be employed for various purposes, eg, to produce final code. How-ever, transformations are only meaningful when they are \u2018correct\u2019: they must pro-duce valid models from valid input models. A valid model has conformance to its meta-model and fulfils its constraints, usually written in OCL. In this paper, we propose a novel methodology to perform automatic, unbounded verification of ATL transformations. Its main component is a novel first-order semantics for ATL transformations, based on the interpretation of the corresponding rules and their execution semantics as first-order predicates. Although, our semantics is not complete, it does cover a significant subset of the ATL language. Using this se-mantics, transformation correctness can be automatically verified with respect to non-trivial OCL pre-and postconditions by using SMT solvers, eg Z3 and Yices. 1", "num_citations": "2\n", "authors": ["835"]}
{"title": "Bridging the gap among academics and practitioners in non-functional requirements management: some reflections and proposals for the future\n", "abstract": " The software engineering community has paid a lot of attention to the study of non-functional requirements (NFRs). Along time, framing NFRs into an articulated framework has become an elusive target. As a consequence, prac-titioners usually integrate NFRs in the different system life-cycle activities in an ad-hoc manner. In this work, we summarise the results of a recent empirical study involving 13 software architects from the Spanish. These results serve as the basis for discussion about possible ways to bridge the gap between academics and practitioners in the management of NFRs.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Tools for Modeling and Generating Safe Interface Interactions in Web Applications\n", "abstract": " Modern Web applications that embed sophisticated user interfaces and business logic have rendered the original interaction paradigm of the Web obsolete. In previous work, we have advocated a paradigm shift from static content pages that are browsed by hyperlinks to a state-based model where back and forward navigation is replaced by a full-fledged interactive application paradigm, featuring undo and redo capabilities, with support for exception management policies and transactional properties. In this demonstration, we present an editor and code generator designed to build applications based on our approach.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Relationship-based change propagation: a case study\n", "abstract": " Software development is an evolutionary process. Requirements of a system are often incomplete or inconsistent, and hence need to be extended or modified over time. Customers may demand new services or goals that often lead to changes in the design and implementation of the system. These changes are typically very expensive. Even if only local modifications are needed, manually applying them is time-consuming and error-prone. Thus, it is essential to assist users in propagating changes across requirements, design, and implementation artifacts.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Verificaci\u00f3n de la ejecutabilidad de operaciones definidas con Action Semantics\n", "abstract": " MDD y MDA requieren definir el modelo de comportamiento del sistema software con un nivel de detalle suficiente para permitir su implementaci\u00f3n autom\u00e1tica. Con este fin, la especificaci\u00f3n UML incorpor\u00f3 el lenguaje Action Semantics (AS) para especificar de forma precisa el comportamiento de un sistema software. Una vez complementadas con estructuras de control de flujo, las acciones de AS permiten especificar de manera completa el efecto de las operaciones definidas en un diagrama de clases. Lamentablemente, las propuestas actuales dedicadas a la verificaci\u00f3n de la calidad de los esquemas de comportamiento no son suficientemente potentes como para garantizar la correcci\u00f3n de las especificaciones basadas en AS. El objetivo de este art\u00edculo es complementar estos trabajos con la propuesta de una nueva t\u00e9cnica para la verificaci\u00f3n de la ejecutabilidad de este tipo de especificaciones. En concreto, la t\u00e9cnica que se propone est\u00e1 basada en el an\u00e1lisis est\u00e1tico de las dependencias entre las acciones que definen el comportamiento de la operaci\u00f3n.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Generating Extended Conceptual Schemas from Business Process Models.\n", "abstract": " The specification of business processes is becoming a more and more critical aspect for organizations. Such processes are specified as workflow models expressing the logical precedence among the different business activities (ie, the units of work). Up to now, workflow models have been commonly managed through specific subsystems, called workflow management systems. In this paper we advocate for an integration of the workflow specification in the domain conceptual schema. This workflow-extended schema is automatically derived from the workflow model and comprises some new entity and relationship types for describing the workflow elements and a set of integrity constraints to ensure the workflow precedence rules.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Entity types derived by symbol-generating rules\n", "abstract": " We review the definition of entity types derived by symbol-generating rules. These types appear frequently in conceptual schemas. However, up to now they have received very little attention in the field of conceptual modeling of information systems. Most conceptual modeling languages, like the UML and ORM, do not allow their formal definition.               In this paper, we propose a new method for the definition of entity types derived by symbol-generating rules. Our method is based on the fact that these types can always be expressed as the result of the reification of a derived relationship type. Many languages, like the UML and ORM, allow defining derived relationship types and, at the same time, provide support for reification. Using our method, these languages can directly deal with those derived entity types.", "num_citations": "2\n", "authors": ["835"]}
{"title": "La relaci\u00f3n de materializaci\u00f3n en UML\n", "abstract": " La relaci\u00f3n de materializaci\u00f3n es un tipo de relaci\u00f3n gen\u00e9rico de la modelizaci\u00f3n conceptual. Permite relacionar una clase que representa una categor\u00eda con la clase o clases que representan los objetos concretos de esa categor\u00eda. UML no ofrece esta construcci\u00f3n como parte de su metamodelo, por lo que no se pueden usar relaciones de materializaci\u00f3n en la definici\u00f3n de esquemas conceptuales en UML. En este trabajo se propone una extensi\u00f3n est\u00e1ndar al UML para permitir la especificaci\u00f3n de esquemas que incluyan relaciones de materializaci\u00f3n.", "num_citations": "2\n", "authors": ["835"]}
{"title": "Papers and researchers: an example of an unsatisfiable uml/ocl model\n", "abstract": " Papers and researchers: An example of an unsatisfiable UML/OCL model Page 1 Papers and researchers: An example of an unsatisfiable UML/OCL model Jordi Cabot, Robert Claris\u00f3, and Daniel Riera {jcabot,rclariso,drierat}@uoc.edu Paper title: String wordCount: int studentPaper: boolean 1 Writes 1..2 manuscript author 1 Reviews 3 submission referee Researcher name: String isStudent: boolean context Researcher inv NoSelfReviews: { Nobody can review his own paper } self.submission\u2212>excludes(self.manuscript) context Paper inv PaperLength: { Papers should have less than 10000 words } self.wordCount < 10000 context Paper inv AuthorsOfStudentPaper: { A paper is a student paper iff any of its authors is a student } self.studentPaper = self.author\u2212>exists(x | x.isStudent ) context Paper inv NoStudentReviewers: { Students cannot review papers } self.referee\u2212>forAll(r | not r.isStudent) context Paper inv \u2026", "num_citations": "2\n", "authors": ["835"]}
{"title": "Self-adaptive Architectures in IoT Systems: A Systematic Literature Review\n", "abstract": " Over the past few years, the relevance of the Internet of Things (IoT) has grown significantly and is now a key component of many industrial processes and even a transparent participant in various activities performed in our daily life. IoT systems are subjected to changes in the dynamic environments they operate in. These changes (e.g. variations in the bandith consumption or new devices joining/leaving) may impact the Quality of Service (QoS) of the IoT system. A number of self-adaptation strategies for IoT architectures to better deal with these changes have been proposed in the literature. Nevertheless, they focus on isolated types of changes. We lack a comprehensive view of the trade-offs of each proposal and how they could be combined to cope with dynamic situations involving simultaneous types of events. In this paper, we identify, analyze, and interpret relevant studies related to IoT adaptation and develop a comprehensive and holistic view of the interplay of different dynamic events, their consequences on the architecture QoS, and the alternatives for the adaptation. To do so, we have conducted a systematic literature review of existing scientific proposals and defined a research agenda for the near future based on the findings and weaknesses identified in the literature.", "num_citations": "1\n", "authors": ["835"]}
{"title": "All Researchers Should Become Entrepreneurs\n", "abstract": " We often complain about the challenges associated with a fruitful research-industry collaboration. The coronavirus pandemic has just aggravated them as, clearly, companies face difficult times and have mostly paused their R&I activities. In this context, we propose that researchers become entrepreneurs and play both roles at the same time. Right now, this is much more the exception than the rule in the academic system. However, we argue this is the quickest way to get real feedback on the quality and impact of our research.", "num_citations": "1\n", "authors": ["835"]}
{"title": "A Model-based Chatbot Generation Approach to Converse with Open Data Sources\n", "abstract": " The Open Data movement promotes the free distribution of data. More and more companies and governmental organizations are making their data available online following the Open Data philosophy, resulting in a growing market of technologies and services to help publish and consume data. One of the emergent ways to publish such data is via Web APIs, which offer a powerful means to reuse this data and integrate it with other services. Socrata, CKAN or OData are examples of popular specifications for publishing data via Web APIs. Nevertheless, querying and integrating these Web APIs is time-consuming and requires technical skills that limit the benefits of Open Data movement for the regular citizen. In other contexts, chatbot applications are being increasingly adopted as a direct communication channel between companies and end-users. We believe the same could be true for Open Data as a way to bridge the gap between citizens and Open Data sources. This paper describes an approach to automatically derive full-fledged chatbots from API-based Open Data sources. Our process relies on a model-based intermediate representation (via UML class diagrams and profiles) to facilitate the customization of the chatbot to be generated.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering\n", "abstract": " Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.                 Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.", "num_citations": "1\n", "authors": ["835"]}
{"title": "A Survey of Software Foundations in Open Source\n", "abstract": " A number of software foundations have been created as legal instruments to better articulate the structure, collaboration and financial model of Open Source Software (OSS) projects. Some examples are the Apache, Linux, or Mozilla foundations. However, the mission and support provided by these foundations largely differ among them. In this paper we perform a study on the role of foundations in OSS development. We analyze the nature, activities, role and governance of 101 software foundations and then go deeper on the 27 having as concrete goal the development and evolution of specific open source projects (and not just generic actions to promote the free software movement or similar). Our results reveal the existence of a significant number of foundations with the sole purpose of promoting the free software movement and/or that limit themselves to core legal aspects but do not play any role in the day-to-day operations of the project (e.g., umbrella organizations for a large variety of projects). Therefore, while useful, foundations do not remove the need for specific projects to develop their own specific governance, contribution and development policies. A website to help projects to choose the foundation that best fits their needs is also available.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Incremental Verification of UML/OCL Models.\n", "abstract": " ABSTRACT Model-Driven Development employs models as core artifacts of the software development process. This requires ensuring the correctness of models, an analysis which is computationally complex. However, models may evolve over time and these changes usually require re-checking models from scratch. To this end, this paper proposes techniques for the incremental verification of a fundamental correctness property: internal consistency of UML class diagrams annotated with OCL constraints. These techniques allow modelers to significantly reduce (or even avoid altogether) the cost of re-verifying a class diagram after model updates.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Low-code vs model-driven: are they the same?\n", "abstract": " Since low-code became the new buzzword, I wondered whether there was anything really different in the low-code movement compared to what we used to call model-driven engineering/development. The 1st Low-code workshop (part of the Models 2020 conference) was the perfect excuse to take some time to reflect and write down my thoughts on this topic.What you can read next (you can also download the pdf version), it is the result of my thinking sessions. Also embedded the slides of the talk I prepared to present the paper (see at the bottom). Both include some of the feedback I got when publishing the first version of this post (thanks to all for the great feedback you gave me!). I do believe this (the positioning of low-code in the model-driven world) is a discussion we need to keep having as a community. Even if we don\u2019t reach any consensus.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Analysis and modeling of the governance in general programming languages\n", "abstract": " General Programming Languages (GPLs) continuously evolve to adapt to the ever changing technology landscape. The evolution is rooted on technical aspects but it is ultimately decided by the group of people governing the language and working together to solve, vote and approve the new language extensions and modifications. As in any community, governance rules are used to manage the community, help to prioritize their tasks and come to a decision. Typically, these rules specify the decision-making mechanism used in the project, thus contributing to its long-term sustainability by clarifying how core language developers (external contributors and even end-users of the language) can work together. Despite their importance, this core topic has been largely ignored in the study of GPLs. In this paper we study eight well-known GPLs and analyze how they govern their evolution. We believe this study helps to\u00a0\u2026", "num_citations": "1\n", "authors": ["835"]}
{"title": "Analyzing rich-club behavior in open source projects\n", "abstract": " The network of collaborations in an open source project can reveal relevant emergent properties that influence its prospects of success. In this work, we analyze open source projects to determine whether they exhibit a rich-club behavior, ie., a phenomenon where contributors with a high number of collaborations (ie, strongly connected within the collaboration network) are likely to cooperate with other well-connected individuals. The presence or absence of a rich-club has an impact on the sustainability and robustness of the project.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Automatic generation of security compliant (virtual) model views\n", "abstract": " The increased adoption of model-driven engineering in collaborative development scenarios raises new security concerns such as confidentiality and integrity. In a collaborative setting, the model, or fragments of it, should only be accessed and manipulated by authorized parties. Otherwise, important knowledge could be unintentionally leaked or shared artifacts corrupted. In this paper we explore the introduction of access-control mechanisms for models. Our approach relies on the definition of a domain specific language tailored to the definition of access-control rules on models and on its enforcement thanks to the automatic generation of security compliant (virtual) views.", "num_citations": "1\n", "authors": ["835"]}
{"title": "30 years of contributions to conceptual modeling\n", "abstract": " This chapter is aimed at summarizing the contribution of Antoni Oliv\u00e9 to the field of conceptual modeling over the last three decades. It starts with his initial proposals around the year 1986 and it finishes with his most recent, not to say current, work on the field. The summary encompasses different topics, beginning with the deductive approach to conceptual modeling and its application to deductive databases, evolving later to object-oriented conceptual schemas and, more recently, to conceptual-schema centric development. All in all, the trajectory covers a wide range of topics, all of them of great importance at the time they were treated, and has meant an important advance of the knowledge in this area during all these years.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Model-driven integration and analysis of access-control policies in multi-layer information systems\n", "abstract": " Security is a critical concern for any information system. Security properties such as confidentiality, integrity and availability need to be enforced in order to make systems safe. In complex environments, where information systems are composed of a number of heterogeneous subsystems, each must participate in their achievement. Therefore, security integration mechanisms are needed in order to 1) achieve the global security goal and 2) facilitate the analysis of the security status of the whole system. For the specific case of access-control, access-control policies may be found in several components (databases, networks and applications) all, supposedly, working together in order to meet the high level security property. In this work we propose an integration mechanism for access-control policies to enable the analysis of the system security. We rely on model-driven technologies and the XACML standard to\u00a0\u2026", "num_citations": "1\n", "authors": ["835"]}
{"title": "Un support IDM pour l'architecture d'entreprise dans un contexte industriel: l'exemple du framework TEAP\n", "abstract": " L'application de l'Ing\u00e9nierie Dirig\u00e9e par les Mod\u00e8les (IDM) est souvent r\u00e9serv\u00e9e aux processus de g\u00e9nie logiciel (par exemple, sp\u00e9cification, g\u00e9n\u00e9ration de code, maintenance, r\u00e9tro-ing\u00e9nierie, \u00e9volution). Cependant, elle peut aussi \u00eatre b\u00e9n\u00e9fique pour des initiatives davantage orient\u00e9es m\u00e9tier ou li\u00e9es \u00e0 la prise de d\u00e9cisions strat\u00e9giques, telles que l'Architecture d'Entreprise (AE). L'AE est le processus de traduction de la vision m\u00e9tier/strat\u00e9gie d'une entreprise en un changement effectif, via la description de son \u00e9tat pr\u00e9sent et futur (par exemple, concernant son syst\u00e8me d'information). M\u00eame si diff\u00e9rentes approches ont d\u00e9j\u00e0 propos\u00e9 plusieurs sortes de m\u00e9thodes et d'outillages pour l'AE dans une entreprise, il n'existe pas v\u00e9ritablement \u00e0 l'heure actuelle de framework IDM int\u00e9gr\u00e9 combinant f\u00e9d\u00e9ration de donn\u00e9es d'AE, adaptabilit\u00e9 du standard de repr\u00e9sentation et support pour des points de vue multiples. Le pr\u00e9sent article rapporte notre exp\u00e9rience en cours de construction du framework IDM TEAP (bas\u00e9 sur le standard TOGAF et l'outillage SmartEA) visant notamment \u00e0 traiter ces trois d\u00e9fis dans un contexte industriel d'AE.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Creaci\u00f3n Colaborativa de Lenguajes Espec\u00edficos del Dominio\n", "abstract": " El desarrollo de sofware es un proceso donde participan muchos actores, principalmente los desarrolladores y los clientes del producto. En la actualidad, procesos de desarrollo como los basados en metodolog\u00edas \u00e1giles proponen la participaci\u00f3n de forma directa de los usuarios o clientes. La idea clave es definir procesos guiados por la comunidad donde todos los participantes (t\u00e9cnicos y no t\u00e9cnicos) colaboran para que el producto satisfaga los requisitos. Esta aproximaci\u00f3n es especialmente interesante en el \u00e1mbito del desarrollo de lenguajes espec\u00edficos de dominio (DSL). Sin embargo, aunque estos lenguajes est\u00e1n destinados a una comunidad de usuarios expertos de un dominio concreto, actualmente dichos usuarios tienen poca (o nula) participaci\u00f3n en el desarrollo. Nuestra propuesta consiste en incorporar el aspecto colaborativo en los procesos de desarrollo de DSLs, permitiendo a la comunidad de usuarios del lenguaje participar activamente en su creaci\u00f3n y evoluci\u00f3n. Para ello proponemos adaptar Collaboro, un lenguaje para representar las actividades de colaboraci\u00f3n que surgen durante el desarrollo de DSLs, para ser utilizado a lo largo de todo el proceso.", "num_citations": "1\n", "authors": ["835"]}
{"title": "MDE 2.0: Pragmatical formal model verification and other challenges\n", "abstract": " This document presents a synthesis of the research results conducted in the eld of software veri cation for model-driven engineering (MDE). MDE is becoming one of the dominant software engineering paradigms in the industry. The main characteristic of MDE is the use of software models and model manipulation operations as main artifacts in all software engineering activities. This change of perspective implies that correctness of models (and model manipulation operations) becomes a key factor in the quality of the nal software product. The problem of ensuring software correctness is still considered to be a Grand Challenge for the software engineering community. At the modellevel, we are still missing a set of tools and methods that helps in the detection of defects and smoothly integrates in existing MDE-based tool-chains without an excessive overhead. Characteristics of existing tools, which require designer interaction, deep knowledge of formal methods or extensive manual model annotations seriously impair its usability in practice. In this document, we present our pragmatic set of techniques for formal model veri cation to overcome these limitations. We call our techniques pragmatic because they try to nd the best trade-o between completeness of the veri cation and the usability of the process.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Proceedings of the Workshop on OCL and Textual Modelling (OCL 2010)\n", "abstract": " Guest Editors: Jordi Cabot, Tony Clark, Manuel Clavel, Martin Gogolla Managing Editors: Tiziana Margaria, Julia Padberg, Gabriele Taentzer ECEASST Home Page: http://www. easst. org/eceasst/ ISSN 1863-2122", "num_citations": "1\n", "authors": ["835"]}
{"title": "Lightweight Verification of Executable Models (Extended Version)\n", "abstract": " Executable models play a key role in many development methods by facilitating the immediate simulation/implementation of the software system under development. This is possible because executable models include a fine-grained specification of the system behaviour. Unfortunately, a quick and easy way to check the correctness of behavioural specifications is still missing, which compromises their quality (and in turn the quality of the system generated from them). In this paper, a lightweight verification method to assess the strong executability of fine-grained behavioural specifications (ie operations) at design-time is provided. This method suffices to check that the execution of the operations is consistent with the structural integrity constraints defined in the structural model and returns a meaningful feedback that helps correcting them otherwise.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Eighth International Workshop on OCL Concepts and Tools\n", "abstract": " This paper reports on the 8th OCL workshop held at the MODELS conference in 2008. The workshop focussed on how to evaluate, compare and select the right OCL tools for a given purpose and how to deal with the expressiveness and complexity of OCL. The workshop included sessions with paper presentations as well as a special tool demo session.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Natural Language and Information Systems: 13th International Conference on Applications of Natural Language to Information Systems, NLDB 2008 London, UK, June 24-27, 2008\u00a0\u2026\n", "abstract": " This volume contains the papers presented at NLDB 2008, the 13th Inter-tional Conference on Natural Language and Information Systems, held June 25\u201327, 2008. It also containssome of the best researchproposalsas submitted to theNLDB2008doctoralsymposiumheldonJune24, 2008. Theprogrammealso includes three invited talks covering the main perspectives of the application of naturallanguageto informationsystems: the wayhumansprocess, communicate and understand natural language, what are the implications and challenges-wardssemanticsearchforthenewWebgeneration, hownaturallanguageapplies to the well-established database way of querying as a means to unlock data and information for end users. We received 68 papers as regular papers for the main conference and 14 short papers for the doctoral symposium. Each paper for the main conference was assigned four reviewers based on the preferences expressed by the Program Committee members. We ensured that every paper had at least two reviewers that expressedinterest in reviewing it or indicated that they could reviewit. We ensured that each paper got at least three reviews. As a result, only 10% of the papers were reviewed by three reviewers. The Conference Chair and the two Program Committee Co-chairs acted as Meta-Reviewers. Eachofthemtookroughly1/3ofthepapers (obviouslyrespe-ing con? icts of interest), for which s/he was responsible. This included studying the reviews, launching discussions and asking for clari? cations whenever nec-sary, as well as studying the papers whenever a need for an informed additional opinion arose or when the\u00a0\u2026", "num_citations": "1\n", "authors": ["835"]}
{"title": "Incremental Integrity Checking in UML/OCL Conceptual Schemas\n", "abstract": " Integrity constraints play a fundamental role in the definition of conceptual schemas (CSs) of information systems. An integrity constraint defines a condition that must be satisfied in each state of the information base (IB). Hence, the information system must guarantee that the state of the IB is always consistent with respect to the integrity constraints of the CS. This process is known as integrity checking. Unfortunately, current methods and tools do not provide adequate integrity checking mechanisms since most of them only admit some predefined types of constraints. Moreover, the few ones supporting a full expressivity in the constraint definition language present a lack of efficiency regarding the verification of the IB.In this thesis, we propose a new method to deal with the incremental evaluation of the integrity constraints defined in a CS. We consider CSs specified in the UML with constraints defined as OCL invariants. We say that our method is incremental since it adapts some of the ideas of the well-known methods developed for incremental integrity checking in deductive and relational databases. The main goal of these incremental methods is to consider as few entities of the IB as possible during the evaluation of an integrity constraint. This is achieved in general by reasoning from the structural events that modify the contents of the IB. Our method is fully automatic and ensures an incremental evaluation of the integrity constraints regardless their concrete syntactic definition.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Design and implementation of partition profile-an experience report\n", "abstract": " The definition of UML profiles is an important research area in information systems engineering. In this report, we aim at contributing to the area in twofold. On the one hand, we study the limitations of CASE tools when a profile definition is considered. On the other hand, we select one of them and show, using the partition profile with the specification of partition evolution operations defined in [GoOl02], how a profile can be implemented with the selected tool.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Bridging the gap among academics and practitioners in Non-Functional Requirements management: Some reflections and proposals for the future\n", "abstract": " The software engineering community has paid a lot of attention to the study of non-functional requirements (NFRs). Along time, framing NFRs into an articulated framework has become an elusive target. As a consequence, practitioners usually integrate NFRs in the different system life-cycle activities in an ad-hoc manner. In this work, we summarise the results of a recent empirical study involving 13 software architects from the Spanish. These results serve as the basis for discussion about possible ways to bridge the gap between academics and practitioners in the management of NFRs.", "num_citations": "1\n", "authors": ["835"]}
{"title": "Automatic Generation of Worfklow-extended Domain Models (extended version)\n", "abstract": " The specification of business processes is becoming a more and more critical aspect for organizations. Such processes are specified as workflow models expressing the logical precedence among the different business activities (ie the units of work). Up to now, workflow models have been commonly managed through specific subsystems, called workflow management systems. In this paper we advocate for the integration of the workflow specification in the system domain model. This workflow-extended domain model is automatically derived from the initial workflow specification. Then, model-driven development methods may depart from the extended domain model to automatically generate an implementation of the system enforcing the business processes in any final technology platform, thus avoiding the need of basing the implementation on a dedicated workflow engine.", "num_citations": "1\n", "authors": ["835"]}