{"title": "Herding cats: Modelling, simulation, testing, and data mining for weak memory\n", "abstract": " We propose an axiomatic generic framework for modelling weak memory. We show how to instantiate this framework for Sequential Consistency (SC), Total Store Order (TSO), C++ restricted to release-acquire atomics, and Power. For Power, we compare our model to a preceding operational model in which we found a flaw. To do so, we define an operational model that we show equivalent to our axiomatic model. We also propose a model for ARM. Our testing on this architecture revealed a behaviour later acknowledged as a bug by ARM, and more recently, 31 additional anomalies. We offer a new simulation tool, called herd, which allows the user to specify the model of his choice in a concise way. Given a specification of a model, the tool becomes a simulator for that model. The tool relies on an axiomatic description; this choice allows us to outperform all previous simulation tools. Additionally, we confirm that\u00a0\u2026", "num_citations": "339\n", "authors": ["693"]}
{"title": "Improving the confidence in measurement-based timing analysis\n", "abstract": " Measurement-based timing analysis (MBTA) is a hybrid approach that combines execution-time measurements with static program analysis techniques to obtain an estimate of the worst-case execution time (WCET) of a program. The most challenging part of MBTA is test data generation. Choosing an adequate set of test vectors determines safety and efficiency of the overall analysis. So far, there are no feasible criteria that determine how well the worst-case temporal behavior of program parts is covered by a given test-suite. In this paper we introduce a relative safety metric that compares test suites with respect to how well the observed worst-case behavior of program parts is exercised. Using this metric, we empirically show that common code coverage criteria from the domain of functional testing can produce unsafe WCET estimates in the context of MBTA for systems with a processor like the TriCore 1796\u00a0\u2026", "num_citations": "40\n", "authors": ["693"]}
{"title": "Optimizing automatic deployment using non-functional requirement annotations\n", "abstract": " Model-driven development has become common practice in design of safety-critical real-time systems. High-level modeling constructs help to reduce the overall system complexity apparent to developers. This abstraction caters for fewer implementation errors in the resulting systems. In order to retain correctness of the model down to the software executed on a concrete platform, human faults during implementation must be avoided. This calls for an automatic, unattended deployment process including allocation, scheduling, and platform configuration.               In this paper we introduce the concept of a systems compiler using non-functional requirements (NFR) as a guidance for deployment of real-time systems. The postulated requirements are then used to optimize the allocation decision, i.e., the process of mapping model entities to available computing nodes, as well as the subsequent generation of\u00a0\u2026", "num_citations": "39\n", "authors": ["693"]}
{"title": "VerifyThis 2015\n", "abstract": " VerifyThis 2015 was a one-day program verification competition which took place on April 12th, 2015 in London, UK, as part of the European Joint Conferences on Theory and Practice of Software (ETAPS 2015). It was the fourth instalment in the VerifyThis competition series. This article provides an overview of the VerifyThis 2015 event, the challenges that were posed during the competition, and a high-level overview of the solutions to these challenges. It concludes with the results of the competition and some ideas and thoughts for future instalments of VerifyThis.", "num_citations": "19\n", "authors": ["693"]}
{"title": "Compatibility and reuse in component-based systems via type and unit inference\n", "abstract": " In many branches of industry, the component-based approach to systems design is predominant, e. g., as in embedded control systems which are often modelled using MATLAB/Simulink. In order to facilitate reuse, and to raise the level of abstraction for future designs and frequently used functions, the employed tool sets offer built-in mechanisms to create sophisticated component libraries. For large, real-world designs, however, it is not always clear, whether or not a certain context violates even the most basic design assumptions of employed library components, thus often leading to expensive runtime errors. This paper introduces a practical method for checking compatibility of large designs, statically. This method not only ensures that large component-based designs provide a context such that all (library) components have well defined types, but it also ensures that transmitted physical units, such as m2, km/h\u00a0\u2026", "num_citations": "19\n", "authors": ["693"]}
{"title": "From cola models to distributed embedded systems code\n", "abstract": " Model driven development has become state of the art in embedded systems software design. To take the resulting models to the designated hardware platform, automated code generation is sought for. The code obtained thereby must match the semantics of the model as closely as possible. In this paper we present an approach to translate Component Language (COLA) models to C code in an automated manner. This process involves handling of target speci? c sensors and actuators, and distribution of softwa...\u00bb", "num_citations": "16\n", "authors": ["693"]}
{"title": "Short regular expressions from finite automata: Empirical results\n", "abstract": " We continue our work [H.\u00a0Gruber, M.\u00a0Holzer: Provably shorter regular expressions from deterministic finite automata (extended abstract). In Proc. DLT, LNCS\u00a05257, 2008] on the problem of finding good elimination orderings for the state elimination algorithm, one of the most popular algorithms for the conversion of finite automata into equivalent regular expressions. Here we tackle this problem both from the theoretical and from the practical side. First we show that the problem of finding optimal elimination orderings can be used to estimate the cycle rank of the underlying automata. This gives good evidence that the problem under consideration is difficult, to a certain extent. Moreover, we conduct experiments on a large set of carefully chosen instances for five different strategies to choose elimination orderings, which are known from the literature. Perhaps the most surprising result is that a simple greedy\u00a0\u2026", "num_citations": "13\n", "authors": ["693"]}
{"title": "A model driven development approach for implementing reactive systems in hardware\n", "abstract": " To deal with the increasing complexity of digital systems, the model driven development approach has proven to be beneficial. This paper presents a model driven hardware design process that is dedicated to reactive embedded systems. The approach is based on the component language (COLA), a synchronous data flow language with formal semantics. COLA follows the hypothesis of perfect synchrony. Models thus do not assume specific timing properties and remain deterministic as long as data flow requirements are retained. This is an essential feature for modeling safety-critical systems. Further, the well-defined semantics not only allows that the resulting models can be formally reasoned about, but is also the key to translation to domain-specific languages. This paper describes the approach of translating the models to VHDL descriptions from their graphical representations. As COLA is well-adapted to both\u00a0\u2026", "num_citations": "12\n", "authors": ["693"]}
{"title": "Automatic generation of SystemC models from component-based designs for early design validation and performance analysis\n", "abstract": " In this paper we present an approach of generating SystemC executable models from software designs captured in a new component-based modeling language, COLA, which follows the paradigm of synchronous dataflow. COLA has rigorous semantics and specification mechanisms. Due to its well-founded semantics, it is possible to establish an integrated development process, the artifacts of which can be formally reasoned about and are dealt with in automated tools such as model checkers and code generators. However, the resulting models remain abstract and cannot be executed immediately. Therefor SystemC offers executable models of a component-based flavor. Establishing an automated translation procedure from COLA to SystemC thus allows for design validation and performance analysis during early design phases. We have validated our approach on a case study taken from the automotive domain.", "num_citations": "12\n", "authors": ["693"]}
{"title": "Seamless model-driven development put into practice\n", "abstract": " Model-driven development (MDD) today is the most promising approach to handle the complexity of software development for distributed embedded systems. Still, no single tool-chain exists that meets all needs of companies employing MDD. Moving back and forth between the tools in today\u2019s iterative development processes thus requires manual integration steps, which are error-prone and hamper reuse and refinement of models. A possible workaround is developing adapters for each pair of tools. Despite its large overhead, industry started pursuing this approach because of a lack of better alternatives. A proper solution is a tool-chain building on an integrated modeling language. We have realized this in cooperation with BMW Research and Technology. To increase the degree of automation during development, the modeling language builds upon a core featuring a rigorous semantics. This enables\u00a0\u2026", "num_citations": "10\n", "authors": ["693"]}
{"title": "Running cola on embedded systems\n", "abstract": " Model driven development has become state of the art in embedded systems software design. To take the resulting models to the designated hardware platform, automated code generation is sought for. The code obtained thereby must match the semantics of the model as closely as possible. In this paper we show how to map models specified using the Component Language (COLA) to C code in an automated manner. In addition we present our concepts for interfacing the effective hardware platform, which in c...\u00bb", "num_citations": "9\n", "authors": ["693"]}
{"title": "Optimizing API implementer programs using fine-grained code analysis\n", "abstract": " Based on source code analysis of an API-invoker program, an expendable set of source code sections of an API-implementer program is identified. The expendable set corresponds to operations which are not expected to be performed on behalf of the API-invoker program at a particular computing environment. An optimized binary version of the API-implementer program is generated, which does not include executable code corresponding to the expendable set. The optimized binary version is transmitted to the computing environment for deployment.", "num_citations": "7\n", "authors": ["693"]}
{"title": "A Benchmarking Suite for Measurement-Based WCET Analysis Tools\n", "abstract": " Worst case execution time analysis based on measurements requires large test suites to obtain reliable numbers. We are thus developing tools to efficiently generate these test sets in a whitebox-testing approach. To make project progress measurable and guard against regressions, a benchmarking suite is sought for. We present a set of requirements that have been collected and outline the design of a benchmarking tool-set. While we are currently developing our domain-specific tool chain, we assume the presented architecture to be sufficiently general.", "num_citations": "7\n", "authors": ["693"]}
{"title": "CBMC Path: A Symbolic Execution Retrofit of the C Bounded Model Checker\n", "abstract": " We gave CBMC the ability to explore and model check single program paths, as opposed to its default whole-program model-checking behaviour. This means that CBMC, when invoked with the         flag, symbolically executes one program path at a time\u2014saving unexplored paths for later\u2014and attempts to prove properties for only that path. By doing this repeatedly for each path that CBMC encounters, CBMC can detect property violations in a scalable and incremental way. Implementing single-path exploration raises the question of which order the paths should be explored in. Our implementation makes it easy for researchers to implement and investigate alternative path exploration strategies. Our competition contribution uses a breadth-first strategy, where diverging paths are each pushed onto a queue at program decision points, and the path to explore next is gotten by dequeueing the oldest path to have been\u00a0\u2026", "num_citations": "4\n", "authors": ["693"]}
{"title": "Query-Driven Program Testing\n", "abstract": " In this dissertation we describe a new method for fully automatic test case generation following formal specifications given by test engineers. We build upon a well-defined mathematical core that captures the semantics of coverage criteria. On top of this framework we define the declarative test specification language FQL, the FShell query language. These formal specifications are supplemented with an engine that generates test cases in response to FQL queries. We chose this overall design of a mathematical core, a query language and an efficient back end in analogy to databases and hence refer to our method as query-driven program testing. The full workflow is implemented for ANSI C programs in a tool called FShell, which uses components of the C Bounded Model Checker (CBMC).", "num_citations": "4\n", "authors": ["693"]}
{"title": "Timely time estimates\n", "abstract": " Estimations of execution time are essential for design and development of safety critical embedded real-time systems, such as avionics, automotive and aerospace systems. In such systems, execution time is part of the functional specification, hence correct behaviour requires sufficiently powerful target hardware to meet deadlines or achieve required polling rates, etc. Yet, grossly overestimated resource usage results in excessive cost per unit. For a proper choice of the target platform, qualitatively good execution time estimates are required at an early stage of the development process.               In this paper we propose a framework which provides software engineers with execution time estimates of the software under development in a demand-driven manner, i. e. the engineers ask for timing information at program or function level with respect to different target hardware platforms. In a platform\u00a0\u2026", "num_citations": "4\n", "authors": ["693"]}
{"title": "Navigating the requirements jungle\n", "abstract": " Research on validation and verification of requirements specifications has thus far focused on functional properties. Yet, in embedded systems, functional requirements constitute only a small fraction of the properties that must hold to guarantee proper and safe operation of the system under design.               In this paper we try to shine some light on the kinds of requirements occurring in current embedded systems design processes. We present a set of categories together with real-life examples. For each of them, we briefly describe possible approaches towards formal modeling and automated verification of the respective properties.", "num_citations": "4\n", "authors": ["693"]}
{"title": "Concurrent program verification with invariant-guided underapproximation\n", "abstract": " Automatic verification of concurrent programs written in low-level languages like ANSI-C is an important task as multi-core architectures are gaining widespread adoption. Formal verification, although very valuable for this domain, rapidly runs into the state-explosion problem due to multiple thread interleavings. Recently, Bounded Model Checking (BMC) has been used for this purpose, which does not scale in practice. In this work, we develop a method to further constrain the search space for BMC techniques using underapproximations of data flow of shared memory and lazy demand-driven refinement of the approximation. A novel contribution of our method is that our underapproximation is guided by likely data-flow invariants mined from dynamic analysis and our refinement is based on proof-based learning. We have implemented our method in a prototype tool. Initial experiments on benchmark\u00a0\u2026", "num_citations": "3\n", "authors": ["693"]}
{"title": "One click from model to reality\n", "abstract": " One Click from Model to Reality - ORA - Oxford University Research Archive Logos Header links Search History Bookmarks 0 New Search Deposit Help Footer links Deposit Agreements Disclaimer Privacy Policy Cookies Accessibility Statement Take-down Policy Copyright API Contact Skip to main NEW SEARCH Deposit HELP 0 Back to Search CONTACT Name Email Comment Send message Actions Authors Bibliographic Details Terms of Use Stats Export BibTeX EndNote RefWorks Conference item icon Conference item One Click from Model to Reality Actions Email \u00d7 Email this record Send the bibliographic details of this record to your email address. Your Email Please enter the email address that the record information will be sent to. Your message (optional) Please add any additional information to be included within the email. Send Cite \u00d7 Cite this record APA Style Haberl, W., Herrmannsdoerfer, M., Kugele, S.\u2026", "num_citations": "2\n", "authors": ["693"]}