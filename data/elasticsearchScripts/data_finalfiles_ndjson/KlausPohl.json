{"title": "Software product line engineering: foundations, principles, and techniques\n", "abstract": " I. Software Product Line Engineering Are you interested in producing software products or software-intensive systems at lower costs, in shorter time, and with higher quality? If so, you are holding the right book in your hands. Software product line engineering has proven to be the methodology for Higher quality, lower developing a diversity of software products and software-intensive systems cost, and shorter at lower costs, in shorter time, and with higher quality. Numerous reports development times document the significant achievements and experience gained by introducing software product lines in the software industry. Chapter 21 of this book summarises several cases. Concerning the terminology, there is an almost synonymous use of the terms Software product line \u201csoftware product family\u201d and \u201csoftware product line\u201d. Whereas in Europe vs. software product the term software product family is used more often\u00a0\u2026", "num_citations": "4950\n", "authors": ["1447"]}
{"title": "Requirements engineering: fundamentals, principles, and techniques\n", "abstract": " Requirements engineering is the process of eliciting individual stakeholder requirements and needs and developing them into detailed, agreed requirements documented and specified in such a way that they can serve as the basis for all other system development activities. In this textbook, Klaus Pohl provides a comprehensive and well-structured introduction to the fundamentals, principles, and techniques of requirements engineering. He presents approved techniques for eliciting, negotiating and documenting as well as validating, and managing requirements for software-intensive systems. The various aspects of the process and the techniques are illustrated using numerous examples based on his extensive teaching experience and his work in industrial collaborations. His presentation aims at professionals, students, and lecturers in systems and software engineering or business applications development\u00a0\u2026", "num_citations": "1167\n", "authors": ["1447"]}
{"title": "Process-centered requirements engineering\n", "abstract": " From the Publisher: This book presents frameworks, models, and tool environments for process-centered requirements engineering. First, it gives a comprehensive overview of the state of research and practice in the field. Then, a two-pronged solution strategy is developed. A conceptual modelling framework describes the process along the three dimensions of representation (from informal to formal), specification completeness, and stakeholder agreement. From the special problems found in creative processes such as requirements engineering, the book derives a novel characterization of process-centered engineering environments and elaborates the architecture of a new generation of process-integrated CASE tools. The concepts have been validated in PRO-ART, a prototypical process-centered requirements engineering environment developed in the Esprit project NATURE.", "num_citations": "451\n", "authors": ["1447"]}
{"title": "The three dimensions of requirements engineering: a framework and its applications\n", "abstract": " There is an increasing number of contributions on how to solve the various problems within requirements engineering (RE). The purpose of this paper is to identify the main goals to be reached during the RE process in order to develop a framework for RE. This framework consists of three dimensions: \u2022\u2022 the specification dimension\u2022\u2022 the representation dimension\u2022\u2022 the agreement dimension.We show how this framework can be used to classify and clarify current RE research as well as RE support offered by methods and tools. In addition, the framework can be applied to the analysis of existing RE practise and the establishment of suitable process guidance. Last but not least, the framework offers a first step towards a common understanding of RE.", "num_citations": "424\n", "authors": ["1447"]}
{"title": "Requirements engineering fundamentals: a study guide for the certified professional for requirements engineering exam-foundation level-IREB compliant\n", "abstract": " Requirements engineering tasks have become increasingly complex. In order to ensure a high level of knowledge and competency among requirements engineers, the International Requirements Engineering Board (IREB) developed a standardized qualification called the Certified Professional for Requirements Engineering (CPRE). The certification defines the practical skills of a requirements engineer on various training levels. This book is designed for self-study and covers the curriculum for the Certified Professional for Requirements Engineering Foundation Level exam as defined by the IREB. The 2nd edition has been thoroughly revised and is aligned with the curriculum Version 2.2 of the IREB. In addition, some minor corrections to the 1st edition have been included. About IREB: The mission of the IREB is to contribute to the standardization of further education in the fields of business analysis and requirements engineering by providing syllabi and examinations, thereby achieving a higher level of applied requirements engineering. The IRE Board is comprised of a balanced mix of independent, internationally recognized experts in the fields of economy, consulting, research, and science. The IREB is a non-profit corporation. For more information visit www. certified-re. com", "num_citations": "404\n", "authors": ["1447"]}
{"title": "A journey to highly dynamic, self-adaptive service-based applications\n", "abstract": " Future software systems will operate in a highly dynamic world. Systems will need to operate correctly despite of unespected changes in factors such as environmental conditions, user requirements, technology, legal regulations, and market opportunities. They will have to operate in a constantly evolving environment that includes people, content, electronic devices, and legacy systems. They will thus need the ability to continuously adapt themselves in an automated manner to react to those changes. To realize dynamic, self-adaptive systems, the service concept has emerged as a suitable abstraction mechanism. Together with the concept of the service-oriented architecture (SOA), this led to the development of technologies, standards, and methods to build service-based applications by flexibly aggregating individual services. This article discusses how those concepts came to be by taking two\u00a0\u2026", "num_citations": "349\n", "authors": ["1447"]}
{"title": "Communicating the variability of a software-product family to customers\n", "abstract": " Variability is a central concept in software product family development. Variability empowers constructive reuse and facilitates the derivation of different, customer specific products from the product family. If many customer specific requirements can be realised by exploiting the product family variability, the reuse achieved is obviously high. If not, the reuse is low. It is thus important that the variability of the product family is adequately considered when eliciting requirements from the customer.               In this paper we sketch the challenges for requirements engineering for product family applications. More precisely we elaborate on the need to communicate the variability of the product family to the customer. We differentiate between variability aspects which are essential for the customer and aspects which are more related to the technical realisation and need thus not be communicated to the customer\u00a0\u2026", "num_citations": "302\n", "authors": ["1447"]}
{"title": "Variability modeling to support customization and deployment of multi-tenant-aware software as a service applications\n", "abstract": " More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about\u00a0\u2026", "num_citations": "287\n", "authors": ["1447"]}
{"title": "Adapting traceability environments to project-specific needs\n", "abstract": " INCOSE (INternational COuncil on Systems Engineering) studied the technical facilities of traceability environments, such as system environment, user interfaces, support and maintenance, and their capabilities for managing requirements (see www. incose. org/workgrps/tools/tooltax. html). The survey included the following market-leading products: CORE", "num_citations": "283\n", "authors": ["1447"]}
{"title": "Requirements elicitation and validation with real world scenes\n", "abstract": " A requirements specification defines the requirements for the future system at a conceptual level (i.e., class or type level). In contrast, a scenario represents a concrete example of current or future system usage. In early RE phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system. In many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage. To support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e.g., video, speech, pictures, etc.) and to interrelate those observations with the goal definitions. Thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable. We relate the\u00a0\u2026", "num_citations": "245\n", "authors": ["1447"]}
{"title": "The three dimensions of requirements engineering\n", "abstract": " Requirements engineering (RE) is perceived as an area of growing importance. Due to the increasing effort spent for research in this area many contributions to solve different problems within RE exist. The purpose of this paper is to identify the main goals to be reached during the requirements engineering process in order to develop a framework for RE. This framework consists of the three dimensions:                                    the specification dimension                                                     the representation dimension                                                     the agreement dimension                                              Looking at the RE research using this framework, the different approaches can be classified and therefore their interrelationships become much clearer. Additionally the framework offers a first step towards a common understanding of RE.", "num_citations": "245\n", "authors": ["1447"]}
{"title": "Basiswissen requirements engineering: Aus-und Weiterbildung nach IREB-Standard zum certified professional for requirements engineering foundation level\n", "abstract": " Kompaktes Grundlagenwerk f\u00fcr den Requirements Engineer Dieses Lehrbuch umfasst den erforderlichen Stoff zum Ablegen der Pr\u00fcfung\" Certified Professional for Requirements Engineering (Foundation Level)\" nach IREB-Standard. Es vermittelt das Grundlagenwissen und behandelt die wesentlichen Prinzipien und Praktiken sowie wichtige Begriffe und Konzepte. Die Themen im Einzelnen:-Grundlegende Prinzipien des Requirements Engineering-Arbeitsprodukte und Dokumentationspraktiken-Praktiken f\u00fcr die Erarbeitung von Anforderungen-Prozess und Arbeitsstruktur-Praktiken f\u00fcr das Requirements Management-Werkzeugunterst\u00fctzung Das Buch eignet sich gleicherma\u00dfen f\u00fcr das Selbststudium, zur Vorbereitung auf die Zertifizierung sowie als kompaktes Basiswerk zum Thema in der Praxis und an Hochschulen. Die 5. Auflage wurde komplett \u00fcberarbeitet, ist konform zum IREB-Lehrplan Foundation Level Version 3.0 und wurde angereichert mit interaktiven Elementen wie animierte Grafiken und Videos.", "num_citations": "201\n", "authors": ["1447"]}
{"title": "Model-based engineering of embedded systems: the SPES 2020 methodology\n", "abstract": " Embedded systems have long become essential in application areas in which human control is impossible or infeasible. The development of modern embedded systems is becoming increasingly difficult and challenging because of their overall system complexity, their tighter and cross-functional integration, the increasing requirements concerning safety and real-time behavior, and the need to reduce development and operation costs. This book provides a comprehensive overview of the Software Platform Embedded Systems (SPES) modeling framework and demonstrates its applicability in embedded system development in various industry domains such as automation, automotive, avionics, energy, and healthcare. In SPES 2020, twenty-one partners from academia and industry have joined forces in order to develop and evaluate in different industrial domains a modeling framework that reflects the current state of the art in embedded systems engineering. The content of this book is structured in four parts. Part I \u201cStarting Point\u201d discusses the status quo of embedded systems development and model-based engineering, and summarizes the key requirements faced when developing embedded systems in different application domains. Part II \u201cThe SPES Modeling Framework\u201d describes the SPES modeling framework. Part III \u201cApplication and Evaluation of the SPES Modeling Framework\u201d reports on the validation steps taken to ensure that the framework met the requirements discussed in Part I. Finally, Part IV \u201cImpact of the SPES Modeling Framework\u201d summarizes the results achieved and provides an outlook on future work. The book is mainly\u00a0\u2026", "num_citations": "193\n", "authors": ["1447"]}
{"title": "Software product line engineering and variability management: achievements and challenges\n", "abstract": " Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.", "num_citations": "190\n", "authors": ["1447"]}
{"title": "Model checking of domain artifacts in product line engineering\n", "abstract": " In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover\u00a0\u2026", "num_citations": "180\n", "authors": ["1447"]}
{"title": "PRO-ART: Enabling requirements pre-traceability\n", "abstract": " Requirements traceability is essential for developing software systems of high quality. Whereas the traceability of the refinement, deployment, and use of a requirement is called post-traceability, the traceability of a requirement back to its origin is named pre-traceability. We present a requirements engineering environment, called PRO-ART, which enables requirements pre-traceability, PRO-ART is based on three main contributions: a three-dimensional framework for requirements engineering which defines the kind of information to be recorded; a trace-repository for structuring the trace information and enabling selective trace retrieval; a novel tool interoperability approach which enables (almost) automated trace capture. In addition, we report on experiences made with the first prototypical implementation of PRO-ART and the resulting redesign and re-implementation, called PRO-ART 2.0, which mainly addresses\u00a0\u2026", "num_citations": "176\n", "authors": ["1447"]}
{"title": "Initiating software product lines.\n", "abstract": " A growing number of software development organizations are adopting approaches that emphasize proactive reuse, interchangeable components, and multiproduct planning cycles to construct high-quality products faster and cheaper. McGregor et al discuss the technical, managerial, and organizational activities related to introducing these practices.", "num_citations": "149\n", "authors": ["1447"]}
{"title": "Software product line testing\n", "abstract": " Exploring principles and potential solutions.", "num_citations": "142\n", "authors": ["1447"]}
{"title": "Modelling requirements variability across product lines\n", "abstract": " The explicit definition of variability in software product lines is a key difference between the development of single software systems and software product line engineering. More and more companies maintain several software product lines which focus on different types of products, market segments, and/or domains. Those product lines typically share commonalities and variability. The companies thus face the problem of managing communality and variability across different product lines. In this paper, we identify essential requirements for the documentation of requirements variability across product lines. We propose a meta model for structuring the variability information, sketch a prototypical realisation for managing variability across product lines in DOORS, and illustrate the use of the meta model in a small example. We further report on experiences made with the proposed variability modelling approach.", "num_citations": "133\n", "authors": ["1447"]}
{"title": "Model-based system testing of software product families\n", "abstract": " In software product family engineering reusable artifacts are produced during domain engineering and applications are built from these artifacts during application engineering. Modeling variability of current and future applications is the key for enabling reuse. The proactive reuse leads to a reduction in development costs and a shorter time to market. Up to now, these benefits have been realized for the constructive development phases, but not for testing. This paper presents the ScenTED technique (Scenario based  TEst case  Derivation), which aims at reducing effort in product family testing. ScenTED is a model-based, reuse-oriented technique for test case derivation in the system test of software product families. Reuse of test cases is ensured by preserving variability during test case derivation. Thus, concepts known from model-based testing in single system engineering, e.g., coverage metrics, must be\u00a0\u2026", "num_citations": "129\n", "authors": ["1447"]}
{"title": "Requirements engineering: An overview\n", "abstract": " Traditionally, requirements engineering (RE) has been seen as the first phase of the software life cycle in which a specification is produced from informal ideas. During RE, the functional and nonfunctional requirements to be met by the system, as well as the criteria for measuring the degree of their satisfaction, must be elicited and documented in a requirements specification. If the specification describes both hardware and software, it is called system requirements specification; if it describes only software, it is called software requirements specification (cf.[IEEE-830, 1984]). The process of developing a requirements specification has been called requirements engineering (RE). Since the establishment of RE as a distinct field in the mid 1970s (see [TSE, 1977]) a great deal of progress has been made.Nowadays, RE is seen as a key issue for the development of software systems with the responsibility for maintaining the requirements of a system over time and across traditional and organizational boundaries (cf.[Jarke and Pohl, 1994; Loucopoulos and Karakostas, 1995]). Correct understanding (elicitation), documentation (specification) and validation of user/customer needs are becoming more and more crucial as the ultimative measurement for systems quality is the degree of user satisfaction, ie the ability of the system to meet the user needs. Thus, RE is becoming the essential activity within", "num_citations": "124\n", "authors": ["1447"]}
{"title": "Industry needs and research directions in requirements engineering for embedded systems\n", "abstract": " The industry has a strong demand for sophisticated requirements engineering (RE) methods in order to manage the high complexity of requirements specifications for software-intensive embedded systems and ensure a high requirements quality. RE methods and techniques proposed by research are only slowly adopted by the industry. An important step to improve the adoption of novel RE approaches is to gain a detailed understanding of the needs, expectations, and constraints that RE approaches must satisfy. We have conducted an industrial study to gain an in-depth understanding of practitioners\u2019 needs concerning RE research and method development. The study involved qualitative interviews as well as quantitative data collection by means of questionnaires. We report on the main results of our study related to five aspects of RE approaches: the use of requirements models, the support for high\u00a0\u2026", "num_citations": "101\n", "authors": ["1447"]}
{"title": "An automated technique for risk-based test case generation and prioritization\n", "abstract": " In practice, available testing budgets limit the number of test cases that can be executed. Thus, a representative subset of all possible test cases must be chosen to guarantee adequate coverage of a test object. In risk-based testing, the probability of a fault and the damage that this fault can cause when leading to a failure is considered for test case prioritization. Existing approaches for risk-based testing provide guidelines for deriving test cases. However, those guidelines lack the level of detail and precision needed for automation. In this contribution, we introduce the risk-based testing technique RiteDAP, which automatically generates system test cases from activity diagrams and prioritizes those test cases based on risk. The results of applying the technique to a practical example are presented and the ability of different prioritization strategies to uncover faults is evaluated.", "num_citations": "94\n", "authors": ["1447"]}
{"title": "Variability management in software product line engineering\n", "abstract": " By explicitly modeling and managing variability, software product line engineering provides a systematic approach for creating a diversity of similar products at low cost, in short time, and with high quality. This tutorial focuses on the two principle differences of software product line engineering when compared to single systems development: The differentiation of two key development processes (domain engineering and application engineering) and the explicit representation and management of variability. We characterize the two processes and their main activities and introduce the orthogonal variability modeling approach (OVM). We further illustrate the OVM approach in the product line requirements engineering and product line testing activities.", "num_citations": "87\n", "authors": ["1447"]}
{"title": "Why is it not sufficient to model requirements variability with feature models\n", "abstract": " Variability is a central concept in software productline engineering. Feature Models are a well accepted technique to model variants and invariants within the automotive industry. Feature models are able to express variability through mandatory, optional, and alternative features of a product. Unfortunately the variability information is localised in the feature models and difficult to transfer to other requirements artefacts (eg use cases) to express dependencies between different requirements artefacts of a variant. This paper presents a selfcontained model to explicitly represent the product-line variability in one central model, and further enables the definition of variability in different requirements models.", "num_citations": "87\n", "authors": ["1447"]}
{"title": "Modelling contextual information about scenarios\n", "abstract": " Scenario-based approaches have proven useful for requirements elicitation, validation and negotiation. Besides the direct and indirect stated requirements current scenario-based approaches capture also contextual information about the existing or future system, but lack in a systematic support for representing and reasoning about this information. Based on a literature survey we define a comprehensive set of concepts needed to represent contextual usage knowledge of scenarios. In contrast to existing approaches, we propose to relate contextual knowledge not only to the whole scenario, but also to the scenario components, eg single or sets of interactions between the system and the user of the system. Consequently, we propose two contextual models, a scenario context model (SCM) and an interaction context model (ICM).", "num_citations": "85\n", "authors": ["1447"]}
{"title": "Guiding requirements engineering for software-intensive embedded systems in the automotive industry\n", "abstract": " Over the past decade, a dramatic increase of functionality, quantity, size, and complexity of software-intensive embedded systems in the automotive industry can be observed. In particular, the growing complexity drives current requirements engineering practices to the limits. In close cooperation between partners from industry and academia, the recently completed REMsES (Requirements Engineering and Management for software-intensive Embedded Systems) project has developed a guideline to support requirements engineering processes in the automotive industry. The guideline enables the requirements engineers to cope with the challenges that arise due to quantity, size and complexity of software-intensive systems. This article presents the major results of the project, namely, the fundamental principles of the approach, the guideline itself, the tool support, and the major findings obtained during the\u00a0\u2026", "num_citations": "78\n", "authors": ["1447"]}
{"title": "Testing variabilities in use case models\n", "abstract": " The derivation of system test cases for product families is difficult due to variability in the requirements, since each variation point multiplies the number of possible behaviors to be tested. This paper proposes an approach to develop domain test cases from use cases that contain variabilities and to derive application test cases from them. The basic idea to avoid combinatorial explosion is to preserve the variability in domain test cases. New strategies to capture variability in test cases are suggested, which in combination help dealing with all basic types of variability in a use case and in its relationships (e.g., <>).", "num_citations": "74\n", "authors": ["1447"]}
{"title": "Variability management in software product line engineering\n", "abstract": " Software product line engineering (SPLE [2], [6]) has proven to be the paradigm for developing a diversity of similar software applications and software-intensive systems at low costs, in short time, and with high quality. Numerous reports document the significant achievements of introducing software product lines in industry [6].", "num_citations": "71\n", "authors": ["1447"]}
{"title": "Method for defined derivation of software tests from use cases\n", "abstract": " A method is provided for deriving software tests from use cases. Thereby an activity digram is used to represent the possible use case scenarios. The test case scenarios are derived by applying coverage metrics on the activity diagram. The activities of the diagram are matched with an appertaining test. A test idea document is produced from the test case scenario. System test scenarios are created by concatenating the test case scenarios to a walk-through of the system. The system test cases are further enriched with activities to ensure the test precondition and to verify the test case scenario result. A test design document is produced for the system test.", "num_citations": "70\n", "authors": ["1447"]}
{"title": "Modeling dependencies between variation points in use case diagrams\n", "abstract": " Software product family variability facilitates the constructive and pro-active reuse of assets during the development of software applications. The variability is typically represented by variation points, the variants and their interdependencies. Those variation points and their variants have to be considered when defining the requirements for the applications of the software product family. To facilitate the communication of the variability to the customer, an extension to UML-use case diagrams has been proposed in [9]. In this paper we identify common dependency types used within the feature modelling community to express interdependencies between variation points and variants. We then propose a differentiation of those interdependencies to be used to reduce complexity and to facilitate selective retrieval of interdependencies between variation points and the variants. Finally, we extend the notations proposed in [9] for representing interdependencies between variation points and variants in use case diagrams and use a simple example to illustrate those extensions.", "num_citations": "68\n", "authors": ["1447"]}
{"title": "Towards a Research Method for Theory-driven Design Research.\n", "abstract": " In this paper we outline a new methodical approach for integrating theories into the design research process. Incorporating theories in design projects allows design researchers to reason on the effects of the IT artifact prior to its realization. We argue that design decisions should be transparent claims of utility based on theory-grounded arguments. Documenting design decisions requires the design researcher to integrate appropriate theories and document the rationale behind a particular design decision. Overall, we demonstrate on the example of constructing a new modeling grammar how to integrate theories in the design research process and discuss conflicts which occur when applying these theories.", "num_citations": "67\n", "authors": ["1447"]}
{"title": "Service research challenges and solutions for the future internet: S-cube-towards engineering, managing and adapting service-based systems\n", "abstract": " S-Cube\u2019s Foundations for the Internet of Services Today\u2019s Internet is standing at a crossroads. The Internet has evolved from a source of information to a critical infrastructure which underpins our lives and economies. The demand for more multimedia content, more interconnected devices, more users, a richer user experience, services available any time and anywhere increases the pressure on existing networks and service platforms. The Internet needs a fundamental rearrangement to be ready to meet future needs. One of the areas of research for the Future Internet is the Internet of S-vices, a vision of the Internet where everything (eg, information, software, platforms and infrastructures) is available as a service. Services available on the Internet of Services can be used by anyone (if they are used according to the policies de? ned by the provider) and they can be extended with new services by anyone. Advantages of the Internet of Services include the p-sibility to build upon other people\u2019se? orts and the little investment needed upfront to develop an application. The risk involved in pursuing new business ideas is diminished, and might lead to more innovative ideas being tried out in practice. It will lead to the appearance of new companies that are able to operate in niche areas, providing services to other companies that will be able to focus on their core business.", "num_citations": "64\n", "authors": ["1447"]}
{"title": "The scented method for testing software product lines\n", "abstract": " In current practice, a significant problem of testing software product lines is the immense effort required. However, this effort can be reduced by applying the systematic reuse concepts of product line engineering to the reuse of test artifacts. Such a reuse is established by defining and preserving variability throughout generic test artifacts in domain engineering, and by reusing these generic test artifacts in application engineering to derive product-specific test case scenarios. In this contribution, the ScenTED method (Scenario based TEst Case Deriva-tion) is presented. The ScenTED method is based on the systematic refinement of generic use case scenarios to generic system and integration test case scenarios. The method includes activities in domain engineering for preserving the variability in the test artifacts as well as activities in application engineering for binding the variability of the generic test artifacts. In\u00a0\u2026", "num_citations": "63\n", "authors": ["1447"]}
{"title": "A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models\n", "abstract": " The formalization of variability models (e.g. feature models) is a prerequisite for the automated analysis of these models. The efficient execution of the analysis operations depends on the selection of well-suited solver implementations. Regarding feature models, on the one hand, the formalization with Boolean expressions enables the use of SAT or BDD solvers. On the other hand, feature models can be transformed into a Constraint-Satisfaction Problem (CSP) in order to use CSP solvers for validation. This paper presents a performance comparison regarding nine contemporary high-performance solvers, three for each base problem structure (BDD, CSP, and SAT). Four operations on 90 feature models are run on each solver. The results will in turn clear the way for new improvements regarding the automatic verification of software product lines, since the efficient execution of analysis operations is essential to\u00a0\u2026", "num_citations": "60\n", "authors": ["1447"]}
{"title": "Usage-based online testing for proactive adaptation of service-based applications\n", "abstract": " Increasingly, service-based applications (SBAs) are composed of third-party services available over the Internet. Even if third-party services have shown to work during design-time, they might fail during the operation of the SBA due to changes in their implementation, provisioning, or the communication infrastructure. As a consequence, SBAs need to dynamically adapt to such failures during run-time to ensure that they maintain their expected functionality and quality. Ideally the need for an adaptation is proactively identified, i.e., failures are predicted before they can lead to consequences such as costly compensation and roll-back activities. Currently, approaches to predict failures are based on monitoring. Due to its passive nature, however, monitoring might not cover all relevant service executions, which can diminish the ability to correctly predict failures. In this paper we demonstrate how online testing, as an\u00a0\u2026", "num_citations": "58\n", "authors": ["1447"]}
{"title": "Towards pro-active adaptation with confidence: Augmenting service monitoring with online testing\n", "abstract": " Service-based applications need to operate in a highly dynamic and distributed world. As those applications are composed of individual services, they have to react to failures of those services to ensure that the applications maintain their expected functionality and quality. Self-adaptation is one solution to this problem, as it allows applications to autonomously react to failures. Currently, monitoring is typically used to identify failures, thus triggering adaptation. However, monitoring only observes failures after they have occurred, which means that adaptation based on monitoring is reactive. This can lead to shortcomings like user dissatisfaction, increased execution times, and late response to critical events. Pro-active adaptation addresses those shortcomings, because in such a setting, the application detects the need for adaptation and thus can adapt before a failure will occur. However, it is important to avoid\u00a0\u2026", "num_citations": "56\n", "authors": ["1447"]}
{"title": "Trustworthiness attributes and metrics for engineering trusted internet-based software systems\n", "abstract": " Trustworthiness of Internet-based software systems, apps, services and platform is a key success factor for their use and acceptance by organizations and end-users. The notion of trustworthiness, though, is subject to individual interpretation and preference, e.g., organizations require confidence about how their business critical data is handled whereas end-users may be more concerned about usability. As one main contribution, we present an extensive list of software quality attributes that contribute to trustworthiness. Those software quality attributes have been identified by a systematic review of the research literature and by analyzing two real-world use cases. As a second contribution, we sketch an approach for systematically deriving metrics to measure the trustworthiness of software system. Our work thereby contributes to better understanding which software quality attributes should be considered and\u00a0\u2026", "num_citations": "51\n", "authors": ["1447"]}
{"title": "Dynamic consistency checking of domain requirements in product line engineering\n", "abstract": " The domain requirements specification (DRS) of a product line comprises the common and variable requirements of all products of the product line. Due to the variability defined for a product line, the DRS may contain contradicting requirements. For example, it may contain requirements A and not(A) which can be included in different products. Checking the consistency of DRS in product line engineering is thus not straightforward. Variability information has to be incorporated into the consistency checks to ensure that contradicting requirements do not become part of the same product requirements specification. In this paper, we present a consistency checking technique for dynamic properties of DRS based on model checking techniques. We present a proof of correctness for the technique, sketch our tool environment, and report on the application of the approach to an industrial example.", "num_citations": "49\n", "authors": ["1447"]}
{"title": "Integration testing in software product line engineering: a model-based technique\n", "abstract": " The development process in software product line engineering is divided into domain engineering and application engineering. As a consequence of this division, tests should be performed in both processes. However, existing testing techniques for single systems cannot be applied during domain engineering, because of the variability in the domain artifacts. Existing software product line test techniques only cover unit and system tests. Our contribution is a model-based, automated integration test technique that can be applied during domain engineering. For generating integration test case scenarios, the technique abstracts from variability and assumes that placeholders are created for variability. The generated scenarios cover all interactions between the integrated components, which are specified in a test model. Additionally, the technique reduces the effort for creating placeholders by minimizing the\u00a0\u2026", "num_citations": "47\n", "authors": ["1447"]}
{"title": "Advanced model-based engineering of embedded systems\n", "abstract": " The markets for embedded systems are characterized by high innovation pressure, steadily decreasing times to market, and the omnipresent need to reduce development costs. This trend is accompanied by the necessity of developing innovative products with greater functionality and more features that can be sold to customers. In the joint research project \"Software Platform Embedded Systems XT\" (SPES XT), a group of 21 partners from industry and academia came together to improve the engineering processes for embedded systems in the automation, automotive and avionic industry. In this chapter we give an introduction to the SPES XT modeling framework supporting the seamless model-based engineering of embedded systems and addressing core challenges in todays embedded systems engineering.", "num_citations": "46\n", "authors": ["1447"]}
{"title": "Project-based learning with examples from industry in university courses: an experience report from an undergraduate requirements engineering course\n", "abstract": " A significant challenge within university education, especially with regard to the teaching of highly theoretical topics like requirements engineering, is to maintain students' interest and motivation whilst addressing the core concepts that will enable students to work in industry upon graduation. It has long been established that experience-based learning can aid in both these feats: On the one hand, providing students with industrial case examples rather than \"dry\" academic assignments can increase student interest and motivation. On the other hand, a case example-centric classroom approach can yield a rich learning environment which fosters collaboration, communication, and self-directed exploration of the instructed principles. In previous work, we have reported on our experience in changing a graduate requirements engineering course towards using case examples based on real industry projects. As more\u00a0\u2026", "num_citations": "44\n", "authors": ["1447"]}
{"title": "COSMOD-RE: Supporting the co-design of requirements and architectural artifacts\n", "abstract": " The need for co-designing requirements and architecture for innovative software-intensive systems is widely accepted. In this paper, we present the key ideas of our method COSMOD-RE for supporting the co-design of requirements and architectural artifacts. The backbone of COSMOD-RE is a hierarchy of four abstraction layers. At each layer, a requirements viewpoint and an architectural viewpoint are co-developed and aligned using a goal- and scenario-based approach. COSMOD-RE defines three co-design processes and five sub-processes for each co-design process to structure and guide the co-development.", "num_citations": "43\n", "authors": ["1447"]}
{"title": "Analyzing requirements engineering processes: a case study\n", "abstract": " Thorough process improvement starts with an analysis of the current situation. This is also true for requirements engineering processes. The goal of cooperation between DaimlerChrysler and the department of Software Systems Engineering at the University of Essen is to establish a framework for such RE process analysis in the area of car manufacturing. In this paper, we report on our first analysis using a traditional interview technique and the results obtained. We compare the major findings with existing research and other experiences, identify a set of challenges and provide an outlook of our future investigations.", "num_citations": "41\n", "authors": ["1447"]}
{"title": "Avoiding redundant testing in application engineering\n", "abstract": " Many software product line testing techniques have been presented in the literature. The majority of those techniques address how to define reusable test assets (such as test models or test scenarios) in domain engineering and how to exploit those assets during application engineering. In addition to test case reuse however, the execution of test cases constitutes one important activity during application testing. Without a systematic support for the test execution in application engineering, while considering the specifics of product lines, product line artifacts might be tested redundantly. Redundant testing in application engineering, however, can lead to an increased testing effort without increasing the chance of uncovering failures. In this paper, we propose the model-based ScenTED-DF technique to avoid redundant testing in application engineering. Our technique builds on data flow-based testing\u00a0\u2026", "num_citations": "40\n", "authors": ["1447"]}
{"title": "Requirements engineering for embedded systems: An investigation of industry needs\n", "abstract": " [Context and Motivation] Requirements engineering (RE) research is expected to provide methods that address the specific challenges of industrial systems engineering. [Question/problem] For this purpose, researchers need a detailed understanding of the needs and expectations that the industry has regarding RE methods. [Principal ideas/results] To identify the key industry needs, we have conducted an in-depth study with representatives from large, internationally operating companies in the domain of embedded systems in Germany. [Contribution] This paper reports on the identified industry needs related to the topics natural language vs. requirements models, support for high system complexity, quality assurance of requirements, and intertwining of RE and design.", "num_citations": "39\n", "authors": ["1447"]}
{"title": "Accurate proactive adaptation of service-oriented systems\n", "abstract": " As service-oriented systems are increasingly composed of third-party services accessible over the Internet, self-adaptation capabilities promise to make these systems become robust and resilient against third-party service failures that may negatively impact on system quality. In such a setting, proactive adaptation capabilities will provide significant benefits by predicting pending service failures and mitigating their negative impact on system quality. Proactive adaptation requires accurate quality prediction techniques; firstly, because executing unnecessary proactive adaptations (due to false positive predictions) might lead to additional costs or follow-up-failures; secondly, because proactive adaptation opportunities may be missed (due to false negative predictions). This book chapter reviews solutions for measuring and ensuring the accuracy of online service quality predictions. It critically analyses their\u00a0\u2026", "num_citations": "38\n", "authors": ["1447"]}
{"title": "Communicating the variability of a software-product family to customers\n", "abstract": " Variability is a central concept in software product family development. Variability empowers constructive reuse and facilitates the derivation of different, customer specific products from the product family. If many customer specific requirements can be realised by exploiting the product family variability, the reuse achieved is obviously high. If not, the reuse is low. It is thus important that the variability of the product family is adequately considered when eliciting requirements from the customer. In this paper we sketch the challenges for requirements engineering for product family applications. More precisely we elaborate on the need to communicate the variability of the product family to the customer. We differentiate between variability aspects which are essential for the customer and aspects which are more related to the technical realisation and need thus not be communicated to the customer. Motivated by the\u00a0\u2026", "num_citations": "38\n", "authors": ["1447"]}
{"title": "Learning and evolution in dynamic software product lines\n", "abstract": " A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies\u00a0\u2026", "num_citations": "35\n", "authors": ["1447"]}
{"title": "Towards automated consistency checks of product line requirements specifications\n", "abstract": " A requirements specification for an individual software system should be consistent, ie free of contradictions. In product line engineering, the product line requirements specification comprises all the requirements common to all products of the product line as well as the variable requirements used to derive individual products from the product line. The set of requirements (common and all the variable ones) of a product line is typically inconsistent since variable requirements can contradict each other. This is not a problem as long as contradicting requirements are not included in a product derived from the product line. Thus, the set of requirements realized in each individual product has to be consistent. Employing techniques used in single system development to check the consistency of product line requirements will thus produce false positive results, since there can be contradiction in the product line\u00a0\u2026", "num_citations": "34\n", "authors": ["1447"]}
{"title": "Integrating requirement and architecture information: A scenario and meta-model approach\n", "abstract": " Recording and maintaining traces about the history of a (software) product is a prerequisite for managing its evolution. Establishing traceability from requirements down to implementation and vice versa has thus long ago been recognized as an essential development activity. A fundamental problem faced with relating requirements and architectural artefacts is the large conceptual distance and the so-called structural gap between requirements and architectural components. In this", "num_citations": "34\n", "authors": ["1447"]}
{"title": "An Analysis of Software Quality Attributes and Their Contribution to Trustworthiness.\n", "abstract": " Whether a software, app, service or infrastructure is trustworthy represents a key success factor for its use and adoption by organizations and end-users. The notion of trustworthiness, though, is actually subject to individual interpretation, eg organizations require confidence about how their business critical data is handled whereas end-users may be more concerned about the usability. These concerns manifest as trustworthiness requirements towards modern apps and services. Understanding which Software Quality Attributes (SQA) foster trustworthiness thus becomes an increasingly important piece of knowledge for successful software development. To this end, this paper provides a first attempt to identify SQA, which contribute to trustworthiness. Based on a survey of the literature, we provide a structured overview on SQA and their contribution to trustworthiness. We also identify potential gaps with respect to attributes whose relationship to trustworthiness is understudied such as eg accessibility, level of service, etc. Further, we observe that most of the literature studies trustworthiness from a security perspective while there exist limited contributions in studying the social aspects of trustworthiness in computing. We expect this work to contribute to a better understanding of which attributes and characteristics of a software system should be considered to build trustworthy systems.", "num_citations": "31\n", "authors": ["1447"]}
{"title": "Introduction to the SPES modeling framework\n", "abstract": " The aim of model-based development is to use models as main development artifacts in all phases of the development process.", "num_citations": "31\n", "authors": ["1447"]}
{"title": "Measuring the structural complexity of feature models\n", "abstract": " The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural\u00a0\u2026", "num_citations": "30\n", "authors": ["1447"]}
{"title": "Structuring the Co-design of Requirements and Architecture\n", "abstract": " The need to co-develop requirements and architectural artefacts, especially for innovative solutions, is widely recognised and accepted. Surprisingly, no comprehensive approach exists to structure the co-design process and to support the stakeholders, requirements engineers, and system architects in co-developing innovative requirements and architectural artefacts. In this paper, we propose a method for the co-design of requirements and architectural artefacts based on two viewpoints, the system usage viewpoint and the system architecture viewpoint. Initially, the two viewpoints are nearly decoupled. The method consists of five sub-processes that support the development of each viewpoint, the comparison of the two viewpoints, the consolidation of the viewpoints, and the definition of detailed system requirements based on the two viewpoints. The consolidation of system usage and coarse-grained\u00a0\u2026", "num_citations": "27\n", "authors": ["1447"]}
{"title": "A reuse technique for performance testing of software product lines\n", "abstract": " Testing that the applications of a software product line comply with their functional as well as with their nonfunctional requirements (for example performance) is important for achieving the desired product quality. Existing approaches for software product line testing only deal with testing an application against its functional requirements. In this paper we present a technique that supports the development of reusable performance test case scenarios in domain engineering and the reuse of these scenarios in application engineering. The technique is an extension of the ScenTED technique for system testing from our previous work. The technique focuses on load testing and performance profiling, two types of performance testing, and it has been validated in a case study at Siemens Medical Solutions HS IM.", "num_citations": "27\n", "authors": ["1447"]}
{"title": "A contextual approach for process-integrated tools\n", "abstract": " Research in process-centered environments (PCEs) has focused on project management support and has been dominated by the search for suitable process modelling languages and enactment mechanisms. The consequences of the process orientation on the tools used during process performance, and for offering fine-grained, method-based support to the engineers performing the process have been studied much less.             In this paper, we discuss the requirements for a tighter integration of interactive engineering tools and present a contextual approach for the process-integration of those tools. To achieve process integration we argue that tools, like processes, should be explicitly defined. The integration of the tool models with the process definitions forms an environment model which is interpreted during tool execution. Based on this interpretation tool behavior is adjusted according to the process\u00a0\u2026", "num_citations": "27\n", "authors": ["1447"]}
{"title": "Industrial case studies in graduate requirements engineering courses: The impact on student motivation\n", "abstract": " University education in software engineering instructs sound theoretical concepts together with method competence. It seeks to provide hands-on experience with the learning content along with insights into its application in practice. Even theoretical disciplines are beginning to adopt more experience-oriented instruction as opposed to passive, lecture-oriented instruction. One favored way for experience-oriented instructions is using case studies in lecture-accompanying assignments and/or tutorials. Compared with real-world scenarios, such case studies are often simplified in order to illustrate specific challenges related to the instructed material. This paper reports on our experience in using realistic industry-oriented case studies in a requirements engineering course with graduate students. The experience indicates a strong positive effect on student motivation as well as the degree of comprehension of the\u00a0\u2026", "num_citations": "26\n", "authors": ["1447"]}
{"title": "Derivation of domain test scenarios from activity diagrams\n", "abstract": " Requirements are often reported as not suitable for testing, because they are, for instance, incomplete. We argue in this paper for early steps in requirements engineering to ensure the testability of requirements in the context of product families. This paper describes the early derivation of test scenarios from use cases represented as activity diagrams. Use cases are often supplemented with activity diagrams if the control structure of the use case includes loops or branches. The use of activity diagrams allows defining a coverage criterion to ensure a particular degree of completeness of the test scenarios. The approach described in this paper is intended for use cases at the domain engineering level. It is discussed how variability in these use cases can be captured in activity diagrams, and, most important, how to address variability while deriving test scenarios so that a particular degree of completeness is reached. For this purpose, we adapt the existing branch coverage criterion to the needs of product families and provide an operational procedure that helps in deriving a set of test scenarios that fulfills our extended coverage criterion. Eventually, the derivation of test scenarios gives an early feedback to the requirements engineer when performed from the tester\u2019s perspective. This increases the requirements quality.", "num_citations": "26\n", "authors": ["1447"]}
{"title": "Goal-driven alignment of services and business requirements\n", "abstract": " The independence of services in service oriented architectures allows different providers to provide different business functionalities. As services can be used independently from each other the requirements engineer is able to quickly align the requirements closely to the current service provision. The organisation, which achieves this close alignment benefits optimally from the current service provision. In this paper we propose a requirements engineering approach based on goal model comparison, which allows finding services for given requirements and which facilitates the adjustment of requirements to current service provision. The approach rests on the idea of describing requirements and services with Tropos goal models and to use model comparison techniques to find services and to adjust the requirements to fit these services.", "num_citations": "25\n", "authors": ["1447"]}
{"title": "Towards exploiting the full adaptation potential of cloud applications\n", "abstract": " Current technology for cloud application adaptation fails to capture two fundamental aspect of cloud environments: multiple adaptation options and interferences and dependencies among these multiple mechanisms. Addressing these aspects requires a significant extension of existing cloud tools and frameworks for engineering and executing cloud application adaptations. They should explicitly take into account: all entities of the cloud environment relevant for adaptation decisions; the concrete adaptation actions that these cloud entities may perform; and the mutual dependencies between those entities and actions. In this paper we provide the first insights towards such novel technology. As main contribution, we systematically elicit the key entities related to adaptations inside a cloud environment and explicitly document those in a conceptual model. To build this model we surveyed the literature, discussed with\u00a0\u2026", "num_citations": "24\n", "authors": ["1447"]}
{"title": "Creating a reference architecture for service-based systems\u2013a pattern-based approach\n", "abstract": " The variety of technologies and standards in the domain of service-based systems makes it complex to build architectures which fit specific project contexts. A reference architecture accompanied by guidelines for deriving context-specific architectures for service-based systems can ease this problem. The NEXOF-RA project is defining a reference architecture for service-based systems that serves as a construction kit to derive architectures for a particular project context. Experience in developing the reference architecture over the last two years has shown that the service-oriented context results in different and sometimes contradicting demands for the reference architecture. Therefore, the development of a single and integrated reference architecture is not feasible. Instead, for constructing the reference architecture, the project has chosen a pattern-based approach that allows the consideration of different types and\u00a0\u2026", "num_citations": "23\n", "authors": ["1447"]}
{"title": "Hydra: A hypertext model for structuring informal requirements representations\n", "abstract": " The ultimate measurement for software quality is the degree to which user needs are satisfied by the system. User needs are an essential input for developing a requirements specification and, in the first place, are most often represented using natural language, pictures, or graphics (informal representations). The consideration of user needs as a driving force throughout the development process is only possible if requirements traceability is assured. Therefore, the specified requirements must be related with their sources, eg the user needs. Hypertext offers a technology for enabling this interrelation.We propose a formal hypertext model, called HYDRA, for structuring informal requirements information. HYDRA enriches the quasi standard hypertext model Dexter by introducing typed hypertext nodes and links. HYDRA is used to structure informal information during the requirements engineering process by creating formal hypertext objects which refer to the informal representations (or part of it). These formal objects are used to relate informal information with other representations (eg entity relationship diagrams, first order logic constraints). Moreover, the formal structure enables situated and selective retrieval of informal information. The creation of the formal objects as well as their relation with other representations is supported by the PRO-ART environment.", "num_citations": "23\n", "authors": ["1447"]}
{"title": "A framework for combining problem frames and goal models to support context analysis during requirements engineering\n", "abstract": " Quality requirements, like security requirements, are difficult to elicit, especially if they cross multiple domains. Understanding these domains is an important issue in the requirements engineering process for the corresponding systems. Well-known requirements engineering approaches, such as goal-oriented techniques provide a good starting point in capturing security requirements in the form of soft-goals in the early stage of the software engineering process. However, such approaches are not sufficient for context and problem analysis. On the other hand, the context and problem modeling approaches like e.g., problem frames, do not address the system goals. Integrating the relevant context knowledge into goal models is a promising approach to address the mutual limitations. In this paper, we propose a framework for combining goal models and problem frames. The framework makes it possible to\u00a0\u2026", "num_citations": "22\n", "authors": ["1447"]}
{"title": "The co-development of system requirements and functional architecture\n", "abstract": " It is widely recognized that in system development, innovative requirements and innovative architectural solutions need to be co-developed. Yet, no comprehensive method exists to support the co-development of requirements and architecture. This chapter describes the COSMOD-RE method for supporting the co-development of requirements and architectural artefacts at four distinct levels of abstraction. An overview on the method is provided, and the activities for supporting the development of system requirements and the functional system architecture are described.", "num_citations": "22\n", "authors": ["1447"]}
{"title": "The impact of students' skills and experiences on empirical results: a controlled experiment with undergraduate and graduate students\n", "abstract": " In empirical software engineering research, graduate students are often seen as legitimate substitutes for industry professionals. It has been also argued in the literature that the generalizability of empirical results from experiments with undergraduate students as participants holds to a much lower extent. In this paper, we report on a controlled experiment conducted separately with graduate students and undergraduate students in order to gain deeper insights whether the results from experiments with graduates and undergraduates in the software engineering field are equal or significantly different with respect to the conclusions that can be drawn. During the experiment, the students apply a specific validation technique for behavioral requirements of embedded software. We observed that graduates were significantly more effective, efficient, and confident in their tasks than the undergraduates. Nevertheless, the\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Studien zu einer Theorie der Gesetzgebung\n", "abstract": " Auf den ersten Blick mag es vielleicht uberraschen, daB eine GroBfor schungseinrichtung auf dem Gebiete der Mathematik und Datenverarbeitung sich mit Gesetzgebungstheorie befaBt. Die Gesellschaft fur Mathematik und Datenverarbeitung hat ihre Aufgabe jedoch von Anfang an nicht nur auf ihr engeres Arbeitsgebiet angelegt, sondern als das verstanden, was sich gerade in letzter Zeit als bedeutsam erwiesen hat, narnlich logische und mathematische Methoden zur Analyse von Informations-und Kornrnunika tionsstrukturen mit dem Ziel einzusetzen, sie ebenso fur aile Beteilig ten transparent zu machen wie auch einer rationalen und zweckentspre chenden Gestaltung zuzufuhren. Besonders dringend scheint eine Reali sierung dieser Zielsetzung auf dem Gebiet des Rechtswesens, da hier in ganz besonderem MaBe aile Mitglieder einer sozialen Gemeinschaft unter undurchsichtiger Rechtssetzung und Rechtsauslegung (und-anwendung) zu leiden h~ tten. Daher besitzt die GMD ein eigenes Institut fur Datenver arbeitung im Rechtswesen. In zahlreichen Einzelprojekten befaBt sich dieses mit der DV-bezogenen Systemanalyse komplexer Normgefuge, wie auf dem Gebiete des Datenschutzes, des Gerichtskosten-und-kassenwesens, des Zustellungsrechts oder der Datenubertragung in der offentlichen Ver waltung. Insbesondere das Thema der\" automationsgerechten Rechtssetzung\" zeigt den Zusammenhang zwischen Rechtstheorie, Logik und Datenverarbeitung. Die im Herbst 1975 durchgefuhrte Tagung zur Theorie der Gesetzgebung, in der zum erstenrnal in der Bundesrepublik versucht worden ist, die hierzu international\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Quality assurance in the presence of variability\n", "abstract": " Software Product Line Engineering (SPLE) is a reuse-driven development paradigm that has been applied successfully in information system engineering and other domains. Quality assurance of the reusable artifacts of the product line (e.g. requirements, design, and code artifacts) is essential for successful product line engineering. As those artifacts are reused in several products, a defect in a reusable artifact can affect several products of the product line. A central challenge for quality assurance in product line engineering is how to consider product line variability. Since the reusable artifacts contain variability, quality assurance techniques from single-system engineering cannot directly be applied to those artifacts. Therefore, different strategies and techniques have been developed for quality assurance in the presence of variability. In this chapter, we describe those strategies and discuss in more detail\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Scenario-based application requirements engineering\n", "abstract": " In product line engineering, the application requirements engineers have to ensure both a high degree of reuse and the satisfaction of stakeholder needs. The vast number of possible variant combinations and the influences of the selection of one variant on different requirements models is a challenge for the consistent reuse of product line requirements. Only if the requirements engineers are aware of all product line capabilities (variabilities and commonalities), they are able to decide whether a stakeholder requirement can be satisfied by the product line or not. In this chapter we present a novel approach for the development of application requirements specifications. For this approach, we use an orthogonal variability model with associated requirements scenarios to support requirements engineers during the elicitation, negotiation, documentation, and validation of product line requirements. The presented\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "A framework for software product line engineering\n", "abstract": " Our framework for software product line engineering incorporates the central concepts of traditional product line engineering, namely the use of platforms and the ability to provide mass customisation.A platform is, in the software context, a collection of reusable artefacts (Definition 1-4). These artefacts have to be reused in a consistent and systematic way in order to build applications. Reusable artefacts encompass all types of software development artefacts such as requirements models, architectural models, software components, test plans, and test designs.", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Defining requirements at different levels of abstraction\n", "abstract": " Requirements engineering for complex software intensive systems has become a major challenge in many software development projects. Especially the automotive industry experiences the increasing complexity of software in vehicles, during the last years. An actual premium vehicle, for instance embodies up to hundred electronic control units (ECU) with easily a few hundred features, each. Beneath the definition of abstract features, an electronic control unit is described by different goals, scenarios, requirements, and constraints. To create a manageable and traceable requirements specification for complex systems that enables the change and reuse of requirements, many companies claim assistance for a seamless specification of requirements. To satisfy this claim necessary requirements-artefacts and abstraction levels have to be defined. Further the interrelations between different requirements-artefacts of\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Guest Editors' Introduction: RE 02--A Major Step toward a Mature Requirements Engineering Community\n", "abstract": " Gap between requirements and software. A continuum should exist from the problem domain to the solution domain, as a problem\u2019s structure usually differs from that of the intended solution. Some presenters discussed solutions for guaranteeing this continuum based on systematic consistency checks between the two levels. Others emphasized the definition and use of well-understood reusable problem structures (problem frames, requirements patterns) that can then be systematically associated with well-defined architectures. Managed RE processes. There was clear evidence that RE processes are defined, deployed, and enacted within large and (sometimes geographically) distributed multisite organizations. The article by Stewart A. Higgins, Maurice de Laat, Paul MC Gieles, and Emilienne Geurts illustrates the importance of requirements process management at Philips Medical Systems. It reports on the\u00a0\u2026", "num_citations": "21\n", "authors": ["1447"]}
{"title": "Research challenges on adaptive software and services in the future internet: towards an S-Cube research roadmap\n", "abstract": " This paper introduces research challenges on future service-oriented systems and software services. Those research challenges have been identified in a coordinated effort by researchers under the umbrella of the EU FP7 Network of Excellence S-Cube. We relate this effort to previous and related research roadmap activities and discuss the approach and results on identifying and assessing those challenges.", "num_citations": "19\n", "authors": ["1447"]}
{"title": "Considering Feature Interactions in Product Lines: Towards the Automatic Derivation of Dependencies between Product Variants.\n", "abstract": " Employing a software product line presents a systematic approach for the reuse of software assets. This is achieved by the explicit modeling of variability, ie, the description of points of variation and variants within the development artifacts. To derive concrete products from the reusable assets, dependencies between variants have to be considered when binding the overall variability. Consequently, being aware of the dependencies between variants is essential, as otherwise incorrect products will result. However, determining such dependencies is far from trivial because in reasonably complex systems many such dependencies might have to be elicitated. Therefore, an approach for systematically and semi-automatically deriving variant dependencies from product line assets is introduced in this paper. Upon recognizing that many of the variants can be understood as features, this approach is realized by extending existing solutions for the automatic detection of feature interactions in single products.", "num_citations": "19\n", "authors": ["1447"]}
{"title": "Considering variabilities during component selection in product family development\n", "abstract": " Within the last decade, software engineering research and practice has enforced the reuse of existing components and COTS (commercial of the shelf systems). Various processes for evaluating and selecting components and COTS during system design and implementation have been proposed. In this paper we discuss the shortcomings of existing component/COTS selection processes. In contrast to all existing COTS selection processes, we argue that three important facets have to be considered when selecting a COTS for a product family, namely:                                         the variability to be offered by the product family,                                                           the architectural concerns and                                                           the functional and quality requirements defined for the product family. We discuss the interplay between the component/COTS selection process and the three facets and sketch our selection\u00a0\u2026", "num_citations": "18\n", "authors": ["1447"]}
{"title": "Concurrent engineering: enabling traceability and mutual understanding\n", "abstract": " Concurrent engineering requires the cooperation of people coming from different phases of the engineering process. Traceability between the different views (products), which exist in such cross-functional teams, is essential for enabling mutual understanding. Moreover, the different views must be related to each other and must be presented in a suitable way to support finding and resolving of inconsistencies, conflicts, and different opportunities. We have developed and implemented a concurrent engineering environment, called PRO-ART, which is based on four main ideas:\u2022 record, use, and maintain the various products using formal product models;\u2022 capture the relationships between the products detected during process performance;\u2022 present the relations based on the ideas of the House of Quality (Co Decide);\u2022 provide a computer based environment, which hides the formal models and automates the\u00a0\u2026", "num_citations": "18\n", "authors": ["1447"]}
{"title": "Runtime model-based privacy checks of big data cloud services\n", "abstract": " Cloud services have to comply with privacy policies when storing or processing data. As cloud services become increasingly data-intensive, e.g., in the case of big data analytics, data privacy concerns become more critical and challenging to address. In particular, data may only be processed at certain geo-locations. However, the actual geo-locations of the many storage and compute nodes involved in big data processing is dynamically selected during runtime. In addition, the execution of concrete data processing tasks may change data classifications from, e.g., personal to anonymized data. Thus, privacy policy checks for big data cloud services have to consider information about the actual nodes and data processing tasks at runtime. The proposed approach R-PRIS monitors cloud services to derive and maintain typed runtime models providing the aforementioned information. R-PRIS checks the typed\u00a0\u2026", "num_citations": "17\n", "authors": ["1447"]}
{"title": "Towards the next generation of service-based systems: The S-cube research framework\n", "abstract": " Research challenges for the next generation of service-based systems often cut across the functional layers of the Service-Oriented Architecture (SOA). Providing solutions to those challenges requires coordinated research efforts from various disciplines, including service engineering, service composition, software engineering, business process management, and service infrastructure. The FP7 Network of Excellence on Software Services and Systems (S-Cube) performs cross-discipline research to develop solutions for those challenges. Research in S-Cube is organised around the S-Cube research framework, which we briefly introduce in this paper. Moreover, we outline the envisioned types of interactions between the key elements of the S-Cube research framework, which facilitate the specification and design as well as the operation and adaptation of future service-based systems.", "num_citations": "17\n", "authors": ["1447"]}
{"title": "Documenting application-specific adaptations in software product line engineering\n", "abstract": " Software product line engineering distinguishes between two types of development processes: domain engineering and application engineering. In domain engineering software artefacts are developed for reuse. In application engineering domain artefacts are reused to create specific applications.               Application engineers often face the problem that individual customer needs cannot be satisfied completely by reusing domain artefacts and thus application-specific adaptations are required. Either the domain artefacts or the application artefacts need to be modified to incorporate the application-specific adaptations. We consider the case that individual customer needs are realised by adapting the application artefacts and propose a technique for maintaining traceability between the adapted application artefacts and the domain artefacts. The traceable documentation of application-specific adaptations is\u00a0\u2026", "num_citations": "17\n", "authors": ["1447"]}
{"title": "Online reinforcement learning for self-adaptive information systems\n", "abstract": " A self-adaptive information system is capable of maintaining its quality requirements in the presence of dynamic environment changes. To develop a self-adaptive information system, information system engineers have to create self-adaptation logic that encodes when and how the system should adapt itself. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning (RL) addresses design time uncertainty by learning the effectiveness of adaptation actions through interactions with the system\u2019s environment at run time, thereby automating the development of self-adaptation logic. Existing online RL approaches for self-adaptive information systems exhibit two shortcomings that limit the degree of automation: they require manually fine-tuning the\u00a0\u2026", "num_citations": "15\n", "authors": ["1447"]}
{"title": "Proactive process adaptation using deep learning ensembles\n", "abstract": " Proactive process adaptation can prevent and mitigate upcoming problems during process execution. Proactive adaptation decisions are based on predictions about how an ongoing process instance will unfold up\u00a0to its completion. On the one hand, these predictions must have high accuracy, as, for instance, false negative predictions mean that necessary adaptations are missed. On the other hand, these predictions should be produced early during process execution, as this leaves more time for adaptations, which typically have non-negligible latencies. However, there is an important tradeoff between prediction accuracy and earliness. Later predictions typically have a higher accuracy, because more information about the ongoing process instance is available. To address this tradeoff, we use an ensemble of deep learning models that can produce predictions at arbitrary points during process execution\u00a0\u2026", "num_citations": "15\n", "authors": ["1447"]}
{"title": "Variabilit\u00e4tsmanagement in software-produktlinien\n", "abstract": " Die Software-Produktlinienentwicklung erlaubt die Entwicklung a\u0308hnli- cher Software-Systeme zu geringen Kosten, in kurzer Zeit und zudem mit steigender Qualit\u00e4t. Zahlreiche Erfahrungen aus der Industrie belegen diese Vorteile der Produktlinienentwicklung gegen\u00fcber der Entwicklung von Einzel-Software-Systemen. Der Schl\u00fcssel fu\u0308r die Software-Produktlinienentwicklung ist die Unterscheidung zwischen zwei Entwicklungsprozessen (\u201eEntwicklung fu\u0308r Wiederverwendung\u201c und \u201eEntwicklung unter Wiederverwendung\u201c) sowie der systematische Umgang mit den Unterschieden (der Variabilita\u00e4t) sowie den Gemeinsamkeiten der Produkte einer Produktlinie. In diesem Beitrag diskutieren wir die Notwendigkeit der expliziten Dokumentation der Produktlinienvariabilit\u00e4t und stellen Ans\u00e4tze zur expliziten Dokumentation von Produktlinienvariabilit\u00e4t in unterschiedlichen Entwicklungsmodellen vor (z.B. UML-Diagramme oder Feature-Modelle). Darauf aufbauend erl\u00e4utern wir den Vorteil der Dokumentation der Variabilit\u00e4t in dedizierten Modellen und stellen als eine m\u00f6gliche Form dieser Variabilit\u00e4tsmodellierung den in Essen entwickelten Ansatz zur Orthogonalen Variabilit\u00e4tsmodellierung (OVM) vor. Die Vorteile des OVM-Ansatzes, insbesondere bez\u00fcglich der Handhabung von Variabilit\u00e4t in verschiedenen Entwicklungsartefakten, illustrieren wir anhand von Beispielen.", "num_citations": "15\n", "authors": ["1447"]}
{"title": "Traceability between cross-functional-teams\n", "abstract": " Traceability between different views, which exist in cross-functional teams, is essential for concurrent engineering. The views (products) of the various teams must be interrelated to each other and presented in a suitable way to emphasize inconsistencies, conflicts, different opportunities. Moreover, decisions together with their rationales must be made explicit. We have developed and implemented a concurrent engineering environment which is based on four basic ideas:(1) record, use, and maintain product interrelations during the concurrent engineering process;(2) capture the decisions and their rationale made during the process;(3) use formal product models to enable product interrelation;(4) provide suitable computer supported tools which hide the formal models and automate the recording of interrelations. First experiences show, that the use of our environment enables traceability of the product interrelations and the decision made within a concurrent engineering process and leads to improved and more consistent process results (products).", "num_citations": "15\n", "authors": ["1447"]}
{"title": "A runtime model approach for data geo-location checks of cloud services\n", "abstract": " Organizations have to comply with geo-location policies that prescribe geographical locations at which personal data may be stored or processed. When using cloud services, checking data geo-location policies during design-time is no longer possible - data geo-location policies need to be checked during run-time. Cloud elasticity mechanisms dynamically replicate and migrate virtual machines and services among data centers, thereby affecting the geo-location of data. Due to the dynamic nature of such replications and migrations, the actual, concrete changes to the deployment of cloud services and thus to the data geo-locations are not known. We propose a policy checking approach utilizing runtime models that reflect the deployment and interaction structure of cloud services and components. By expressing privacy policy checks as an st-connectivity problem, potential data transfers that violate the\u00a0\u2026", "num_citations": "14\n", "authors": ["1447"]}
{"title": "Exploiting assumption-based verification for the adaptation of service-based applications\n", "abstract": " Service-based applications (SBAs) need to operate in a highly dynamic world, in which their constituent services could fail or become unavailable. Monitoring is typically used to identify such failures and, if needed, to trigger an adaptation of the SBA to compensate for those failures.", "num_citations": "14\n", "authors": ["1447"]}
{"title": "Justifying design decisions with theory-based design principles\n", "abstract": " Although the role of theories in design research is recognized, we show that little attention has been", "num_citations": "14\n", "authors": ["1447"]}
{"title": "Principles of Variability\n", "abstract": " We introduce variability modelling in order to support the development and the reuse of variable development artefacts. In software product line engineering, variability is an essential property of domain artefacts. Hence, we use variability modelling in this book to capture the variability of domain requirements, architecture, components, and tests (the artefacts highlighted in Fig. 4-1).Variability is introduced during the product management sub-process when common and variable features of the software product line applications are identified. As domain requirements detail the features defined in product management, variability is carried over to domain requirements. Similarly, this holds for design, realisation, and testing. Requirements engineering, design, and realisation deal with models of a system at different levels of abstraction. At each level, variability from the previous level is refined and additional variability is\u00a0\u2026", "num_citations": "14\n", "authors": ["1447"]}
{"title": "Detecting and correcting outdated requirements in function-centered engineering of embedded systems\n", "abstract": " [Context and Motivation] In function-centered engineering of embedded systems, changes of stakeholder intentions are often directly incorporated in the functional design without updating the behavioral requirements accordingly. [Question/Problem] As a consequence, it is likely that the behavioral requirements of the system become outdated over the course of the engineering process. [Principal Ideas/Results] We propose a validation technique that aids the requirements engineer in detecting and correcting outdated behavioral requirements. The approach relies on a dedicated review model that represents a consolidated view of behavioral requirements and functional design. [Contributions] This paper reports on a semi-automated approach and presents first experimental results showing that our technique can significantly aid the requirements engineer in the detection and correction of\u00a0\u2026", "num_citations": "13\n", "authors": ["1447"]}
{"title": "Validating the functional design of embedded systems against stakeholder intentions\n", "abstract": " In the embedded systems industry, function-centered engineering is commonly applied to address the increasing number and complexity of system functions. During function-centered engineering, the functional design that is created based on the defined requirements for the system is the main artifact that serves as a basis for subsequent development activities. If stakeholder intentions change and modifications become necessary, they are frequently incorporated directly into the functional design without updating the behavioral requirements accordingly. As a consequence, the correctness of the interplay of system functions as defined in the functional design cannot be assessed by checking it against the defined requirements (since they are outdated) but needs to be checked against the current stakeholder intentions. More precisely, the requirements engineer has to validate the functional design against the\u00a0\u2026", "num_citations": "13\n", "authors": ["1447"]}
{"title": "Produktionsmanagement mit sap r/3\n", "abstract": " Das Buch ist eine \u00fcbersichtliche Darstellung aller relevanten Fragen eines effizienten Produktionsmanagements mit Hilfe der SAP R/3 Software. St\u00e4rken und Grenzen des Produktionsplanungsmoduls PP werden klar aufgezeigt und teilweise durch Fallbeispiele erl\u00e4utert. Dabei werden auch die m\u00f6glichen Erweiterungen in einem modernen Supply Chain Management beschrieben. Informationstechnologie und Organisation werden zusammen betrachtet, da nur abgestimmte Konzepte Erfolg bringen. Eine Reihe praktischer Tipps, die im Anhang nochmals chronologisch zusammengefasst sind, helfen dem Management die Weichen in einem PP-Projekt richtig zu stellen. Das Buch richtet sich gleicherma\u00dfen an Praktiker im Bereich Produktionsmanagement sowie an Studierende.", "num_citations": "13\n", "authors": ["1447"]}
{"title": "Modellierung der Variabilit\u00e4t einer Software-Produktfamilie\n", "abstract": " Variabilit\u00e4t ist das zentrale Konzept in der Produktfamilienentwicklung. Durch Ausnutzung der Produktfamilien-Variabilit\u00e4t k\u00f6nnen, basierend auf dem Produktfamilienkern, kundenspezifische Produkte realisiert werden. Der Erfolg einer Produktfamilie h\u00e4ngt neben der zu erreichenden Kundenakzeptanz davon ab, m\u00f6glichst viele kundenspezifische Anforderungen unter Ausnutzung der Variabilit\u00e4t realisieren zu k\u00f6nnen. Hierf\u00fcr ist u.a. die Kommunikation der Variabilit\u00e4t zum Kunden eine wesentliche Voraussetzung. In diesem Beitrag erl\u00e4utern wir die wesentlichen technischen und fachlichen Aspekte der Produktfamilien-Variabilit\u00e4t f\u00fcr eine Software-Produktfamilie und beschreiben ihre zentralen Eigenschaften. Wir zeigen zudem, wie Teile der fachlichen Variabilit\u00e4t durch Use Cases ausgedr\u00fcckt und somit leichter zum Kunden kommuniziert werden k\u00f6nnen.", "num_citations": "13\n", "authors": ["1447"]}
{"title": "Hazard Relation Diagrams: a diagrammatic representation to increase validation objectivity of requirements-based hazard mitigations\n", "abstract": " When developing safety-critical embedded systems, it is necessary to ensure that the system under development poses no harm to human users or external systems during operation. To achieve this, potential hazards are identified and potential mitigations for those hazards are documented in requirements. During requirements validation, the stakeholders assess if the documented hazard-mitigating requirements can avoid the identified hazards. Requirements validation is highly subjective. Among others, validation depends on the stakeholders\u2019 understanding of the involved processes, their familiarity with the system under development, and the information available. In consequence, there is the risk that stakeholders judge the adequacy of hazard-mitigating requirements based on their individual opinions about the hazards, rather than on the documented information about the system\u2019s hazards. To\u00a0\u2026", "num_citations": "12\n", "authors": ["1447"]}
{"title": "Runtime management of multi-level SLAs for transport and logistics services\n", "abstract": " SLA management of non-computational services, such as transport and logistics services, may differ from SLA management of computational services, such as cloud or web services. As an important difference, SLA management for transport and logistics services has to consider so called frame SLAs. A frame SLA is a general agreement that constitutes a long-term contract between parties. The terms and conditions of the frame SLA become the governing terms and conditions for all specific SLAs established under such a frame SLA. Not considering the relationships between frame SLAs, specific SLAs and QoS monitoring information may lead to partial conclusions and decisions, thereby resulting in avoidable penalties. Based on a real industry case in the transport and logistics domain, this paper elaborates on a multi-level run-time SLA management approach for non-computational services that takes\u00a0\u2026", "num_citations": "12\n", "authors": ["1447"]}
{"title": "Extending Software Development Methodologies to Support Trustworthiness-by-Design.\n", "abstract": " People are increasingly concerned about the trustworthiness of software that they use when acting within socio-technical systems. Ideally, software development projects have to address trustworthiness requirements from the very early stages of development using constructive methods to enable trustworthiness-by-design. We analyze the development methodologies with respect to their capabilities for supporting the development of trustworthy software. Our analysis reveals that well-established development methodologies do not specifically support the realization of trustworthy software. Based on findings, we propose a generic mechanism for extending development methodologies by incorporating process chunks that represent best practices and explicitly address the systematical design of trustworthy software. We demonstrate the application of our approach by extending a design methodology to foster the development of trustworthy software for socio-technical systems.", "num_citations": "11\n", "authors": ["1447"]}
{"title": "Supporting the consistent specification of scenarios across multiple abstraction levels\n", "abstract": " [Context and motivation] In scenario-based requirements engineering for complex software-intensive systems, scenarios must be specified and kept consistent across several levels of abstraction such as system and component level. [Question/problem] Existing scenario-based approaches do not provide a systematic support for the transitions between different abstraction levels such as defining component scenarios based on the system scenarios and the system architecture or checking whether the component scenarios are consistent with the system scenarios. [Principal ideas/results] This paper presents a systematic approach for developing scenarios at multiple abstraction levels supported by automated consistency checks of scenarios across these abstraction levels. [Contribution] We have implemented the consistency check in a tool prototype and evaluated our approach by applying it to a\u00a0\u2026", "num_citations": "11\n", "authors": ["1447"]}
{"title": "Software product line variability management\n", "abstract": " Software product line engineering (SPLE) is the approach for creating a diversity of similar products at low cost, in short time, and with high quality. Explicitly documenting product line variability is essential for variability management as it significantly supports the following activities: Defining the commonality and the variability of the product line during domain engineering; Realizing reusable artifacts (the domain artifacts) with variability; Defining the binding of variability during application engineering; Deriving individual applications by exploiting the variability in the domain artifacts. This tutorial is based on our text book on software product line engineering [1]. The tutorial is structured along an SPLE framework, which has been defined based on our experiences and the results of the European software product line research projects ESAPS, CAF\u00c9, and FAMILIES [2].", "num_citations": "11\n", "authors": ["1447"]}
{"title": "Considering Product Family Assets when Defining Customer Requirements\n", "abstract": " The success of a product family heavily depends on the degree of reuse achieved when developing product family based customer specific applications. If a significant amount of the customer requirements can be realized by using the communality and the variability defined for the product family, the reuse level is high; if not, the reuse level is low.In this paper we elaborate on the influence of customer requirements on the degree of reuse achieved within a product family. We argue that the level of reuse can be increased by considering the capabilities of the product family when defining the requirements for the customer specific application. We finally show how this can be supported by product family specific use cases.", "num_citations": "11\n", "authors": ["1447"]}
{"title": "Scenario-based change integration in product family development\n", "abstract": " A product family defines a framework for developing customer specific applications in a particular domain. When defining a customer specific application the developers have to take the product family assets as well as application specific assets into account. In the case of a change, eg a requirements change based on customer specific needs, the developers must consider all relevant assets of both the customer specific application and the generic parts of the product family.As already experienced in traditional software development errors made during change integration cause costly rework, especially if they are made at the requirements or architecture level. In addition, the integration of a change into a product family or a derived application is by far more difficult than the integration of changes into a single product. The main reason for this is that, in comparison with single product development, the interrelations among product family assets and between the product family assets and the derived costumer specific applications are much more complex and thus the number of assets being potentially affected by the change is much larger.", "num_citations": "11\n", "authors": ["1447"]}
{"title": "Architectural runtime models for privacy checks of cloud applications\n", "abstract": " Cloud providers as well as cloud customers are obliged to comply with privacy regulations. In particular, these regulations prescribe compliance to geo-location policies that define at which geographical locations personal data may be stored or processed. However, cloud elasticity dynamically adapts computing resources to workload changes by replicating and migrating components as well as included data among data centers. As a result, data might be moved to different geographical locations, thereby violating geo-location policies. Current approaches for cloud monitoring and compliance fall short in detecting relevant cases of such policy violations, particularly cases that involve data transfers among data centers. We address this gap by exploiting runtime models for the analysis of privacy violations during runtime. In this paper, we introduce architectural runtime models that reflect information about application\u00a0\u2026", "num_citations": "10\n", "authors": ["1447"]}
{"title": "A requirements reference model for model-based requirements engineering in the automotive domain\n", "abstract": " [Context and motivation] The use of conceptual models in automotive requirements engineering is impaired due to the lack of appropriate modelling guidelines. [Question/problem] The goal of this paper is to propose a requirements reference model that serves as the basis for defining such guidelines. [Principal ideas/results] The reference model distinguishes three abstraction layers and three content categories for requirements models. [Contribution] The reference model has been successfully applied in the REMsES project to support the development of a model-based requirements engineering approach for the automotive domain.", "num_citations": "10\n", "authors": ["1447"]}
{"title": "Anforderungsmanagement in der Automobilindustrie: Variabilit\u00e4t in Zielen, Szenarien und Anforderungen\n", "abstract": " Der zunehmende Anteil von Software im Automobil stellt neue Herausforderungen an das Anforderungsmanagement in der Automobilindustrie: Dokumentation und Management unterschiedlicher Anforderungsartefakte auf unterschiedlichen Abstraktionsstufen, sowie die Wiederverwendung von Anforderungen unter Ber\u00fccksichtigung gegebener Abh\u00e4ngigkeiten.", "num_citations": "10\n", "authors": ["1447"]}
{"title": "Improving manual reviews in function-centered engineering of embedded systems using a dedicated review model\n", "abstract": " In model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements\u00a0\u2026", "num_citations": "9\n", "authors": ["1447"]}
{"title": "Software product lines\n", "abstract": " Software product line engineering (SPLE) has proven to empower industry to develop a diversity of similar systems at lower cost, in shorter time, and with higher quality when compared with the development of single systems [1, 2]. A software product line (also sometimes called software product family) is \u201ca set of software-intensive systems that share a common, managed set of features satisfying the specific needs of a particular market segment or mission and that are developed from a common set of core assets [artifacts] in a prescribed way\u201d[3]. SPLE exploits the commonalities of the different systems (typically called applications) belonging to the product line and systematically handles the variation (ie, the differences) among those applications. Commonality is invariant for (ie, shared by) all product line applications [4]; for example, all mobile phones allow users to make calls. Product line variability defines how the different applications of the product line may vary [5]. Product line applications may differ in terms of features and functional and quality requirements they fulfill; for example, some tablet computers may include mobile broadband connectivity, while others may not. The SPLE paradigm has a strong track record of success in industry. Success stories can be found in textbooks (such as [1, 3, 6]) or in the product line hall of fame of the leading international software product line conference (http://splc. net/fame. html). Reported benefits of SPLE include improved productivity by as much as a factor of 10, increased quality by as much as a factor of 10, decreased cost by as much as 60%, decreased labor needs by as much as 87%, decreased\u00a0\u2026", "num_citations": "9\n", "authors": ["1447"]}
{"title": "S-Cube: Addressing Multidisciplinary Research Challenges for the Internet of Services.\n", "abstract": " The Service Oriented Architecture (SOA) is increasingly adopted by industry as a paradigm for building distributed software applications. Yet, the SOA has currently several serious limitations and many crucial service issues are not addressed, including, for example, how to establish, monitor and enforce quality in an end-to-end fashion, as well as how to build service-based applications that proactively adapt to dynamically changing requirements and context conditions. This paper provides an overview of the service research challenges identified in S-Cube, the European Network of Excellence on Software Services and Systems. S-Cube strives to address those challenges by bringing together researchers from leading research institutions across diverse disciplines. The S-Cube researchers are joining their competences to develop foundations and theories, as well as novel mechanisms, techniques and methods for service-based applications, thereby enabling the future Internet of Services.", "num_citations": "9\n", "authors": ["1447"]}
{"title": "Goal-based configuration analysis for networks of collaborative cyber-physical systems\n", "abstract": " Networks of collaborative cyber-physical systems can achieve goals individual systems are incapable of achieving on their own. However, which goals such a network can achieve depends, in part, on the networks current configuration, ie its composition of partaking individual systems. As networks of collaborative cyber-physical systems are of a dynamic nature, the composition of such a network can change during runtime, leading to a plethora of often similar albeit slightly different configurations. Due to the huge number of possible configurations and their various dependencies to the different goals of the network, it is infeasible to handle this amount of information manually. Hence, to provide support for reasoning about dependencies between different configurations and the goals they can achieve, this paper contributes an automated model-based reasoning approach using view generations. Our approach\u00a0\u2026", "num_citations": "8\n", "authors": ["1447"]}
{"title": "Supporting the validation of adequacy in requirements-based hazard mitigations\n", "abstract": " [Context and motivation] In practice, validating functional safety requirements is mainly done by means of reviews, which require large amounts of contextual information about hazards, such as safety goals or the operational conditions under which the hazard occurs. [Question/problem] This information is often scattered across a plethora of artifacts produced particularly during requirements engineering and safety assessment. In consequence, there is a risk that not all relevant information is considered during reviews, leading to subjective and misjudged results. [Principal ideas/results] In order to improve the consideration of all relevant information necessary to validate functional safety requirements, we propose a diagrammatic representation integrating all relevant contextual information. [Contribution] We hypothesize that reviewers are more likely to base their judgment on the relevant contextual\u00a0\u2026", "num_citations": "8\n", "authors": ["1447"]}
{"title": "Adaptive future internet applications: Opportunities and challenges for adaptive web services technology\n", "abstract": " Adaptive capabilities are essential to guarantee the proper execution of Web services and service-oriented applications once dynamic changes are not exceptions but the rule. The importance of adaptive capabilities significantly increases in the context of Future Internet (FI) applications will have to autonomously adapt to changes on service provisioning, availability of things and content, computing resources, and network connectivity. Current solutions for adaptive Web services and adaptive service-based applications will be challenged in such a setting because they fall short to support essential characteristics of FI applications. This chapter analyzes and justifies the need for the transition from adaptive Web services and service-based applications to adaptive FI applications. Based on two real-world use cases from multimedia and logistics, the authors examine where current solutions fall short to properly\u00a0\u2026", "num_citations": "8\n", "authors": ["1447"]}
{"title": "The three dimensions of requirements engineering: 20 years later\n", "abstract": " Requirements engineering is the process of eliciting stakeholder needs and desires and developing them into an agreed set of detailed requirements that can serve as a basis for all other subsequent development activities. In order to structure this field, we identified in 1993 three key dimensions which drive the requirements engineering (RE) process, namely, the specification, the representation, and the agreement dimension. In this chapter, we revisit the three dimensions of RE and sketch their evolution into our comprehensive RE framework in the past 20 years.", "num_citations": "8\n", "authors": ["1447"]}
{"title": "The S-cube research vision\n", "abstract": " This chapter sets the scene and gives the background for S-Cube\u2019s research vision and activities described in the remainder of the book. It does this by describing, in Section\u00a01.1, how the anticipated growth in services and service-based systems that together form the Internet of Services will have a profound effect on business and society. Section\u00a01.2 discusses in more detail some selected, fundamental cross-cutting research challenges and how the cooperation of different research disciplines plays an important role. In Section\u00a01.3 we describe the research framework S-Cube has adopted to assist in unifying research communities and agendas across Europe to meet the challenges faced in realizing the Future Internet.", "num_citations": "8\n", "authors": ["1447"]}
{"title": "Considering Variability in a System Family\u2019s Architecture During COTS Evaluation\n", "abstract": " COTS (commercial off-the-shelf) component designers and developers often envision different usage contexts for their component and, therefore, provide it with adaptation possibilities. These adaptation possibilities are especially important when considering system families. System family engineering is currently an emerging discipline. Variability is a core property of system families which allows deriving different customer-specific applications from a core artifact base. A system family\u2019s core artifact base may also be populated with COTS components. These COTS components then need to support the system family\u2019s variability, i.e. they have to offer the possibility to adapt them to different customer-specific applications. Through their adaptation possibilities COTS components are able to meet this requirement. During COTS evaluation, a system family\u2019s requirements and architecture need to be taken into\u00a0\u2026", "num_citations": "8\n", "authors": ["1447"]}
{"title": "Teaching conceptual modeling in online courses: Coping with the need for individual feedback to modeling exercises\n", "abstract": " Educational approaches for computer science proposing the use of complete online courses or traditional courses employing some kind of online material have received much attention recently. The integration of online materials into traditional courses or the replacement of entire courses offer huge possibilities, including increased teaching quality and better study and work alignment. However, researchers and teachers also identified some drawbacks of using online material, including the lack of interaction between students and teachers, and the need to discuss and provide feedback of the students' exercise results. A solution for providing such feedback are automated assessment tools which can generate feedback. However, these tools are not applicable in all situations, e.g. for providing feedback to conceptual modeling exercises. In this paper, we report on the design and implementation of an online course\u00a0\u2026", "num_citations": "7\n", "authors": ["1447"]}
{"title": "Coordinated run-time adaptation of variability-intensive systems: an application in cloud computing\n", "abstract": " Distributed systems, such as cloud systems or cyber-physical systems, involve the orchestration of different variability-intensive, adaptive sub-systems. Each of these sub-systems may perform adaptations simultaneously and independently from each other. Yet, if dependencies between the adaptations of the sub-systems are not considered, this may lead to conflicting adaptations or untapped synergies among adaptations. This paper introduces FCORE, a model-based approach, which facilitates coordinating adaptations among variability-intensive systems. The permissible run-time reconfigurations of each system is specified by an FCORE model, which combines feature models used in Dynamic Software Product Lines with goal models. FCORE models are mapped to constraint satisfaction problems to determine conflicts and synergies among the adaptations of the systems during execution. We demonstrate the\u00a0\u2026", "num_citations": "7\n", "authors": ["1447"]}
{"title": "Extending development methodologies with trustworthiness-by-design for socio-technical systems\n", "abstract": " Socio-Technical Systems (STS) include humans, organizations, and the information systems that they use to achieve certain goals [1]. They are increasingly relevant for society, since advances in ICT technologies, such as cloud computing, facilitate their integration in our daily life. Due to the difficulty in preventing malicious attacks, vulnerabilities, or the misuse of sensitive information, users might not trust these systems. Trustworthiness in general can be defined as the assurance that the system will per-form as expected, or meets certain requirements (cf., eg [2]). We consider trustworthiness as a multitude of quality attributes. As a means of constructive quality assurance, development methodologies should explicitly address the different challenges of building trustworthy software as well as evaluating trustworthiness, which is not supported by development methodologies, such as User-Centered Design (UCD)[3].", "num_citations": "7\n", "authors": ["1447"]}
{"title": "Structuring variability in the context of embedded systems during software engineering\n", "abstract": " During the development of embedded software, the system context (mechanical, electronical, business, etc.) has to be considered. Typically, this context is diverse and highly complex. Moreover, the context in which the system is embedded can vary. For example, the system can be used in different technical environments or in different countries. This variability in the context influences the software to be developed and typically leads to system variability. This paper systematically analyses the impact of context variability on the system development, more precisely, on the variability of the system. Related work is discussed and an example from the automotive domain is presented to identify open issues that need to be addressed.", "num_citations": "7\n", "authors": ["1447"]}
{"title": "Integrating perfective and corrective adaptation of service-based applications\n", "abstract": " Service-based Applications (SBAs) can be dynamically adapted to address various goals, which include (1) aiming to better achieve the users\u2019 requirements (perfective adaptation), and (2) repairing and preventing failures (corrective adaptation). When building applications which aim at addressing more than of such goals, it is important to understand the interplay of these different adaptation goals. Otherwise this can lead to conflicting adaptations. This chapter introduces a framework to integrate and align perfective and corrective adaptations, while addressing the problems that are due to the interactions between these two kinds of adaptation. The framework uses requirements engineering techniques to trigger perfective adaptation and online testing techniques to trigger corrective adaptations. Based on the above techniques, this chapter investigates the interplay and interaction between the two types of\u00a0\u2026", "num_citations": "6\n", "authors": ["1447"]}
{"title": "Requirements engineering in complex domains\n", "abstract": " Complexity in the application domains of software-intensive systems is continuously growing due to at least two reasons. Firstly, technical complexity grows as hardware and software have to interact in individual or even communicating embedded systems. Secondly, social complexity grows as the process organizations of the 1990\u2019s are gradually being replaced by loosely coupled networks of actors, often organized around community platforms. In this chapter, we discuss recent solution attempts for these two issues individually, and end with speculating about their possible future interaction.", "num_citations": "6\n", "authors": ["1447"]}
{"title": "Agreeing Upon SOA Terminology-Lessons Learned.\n", "abstract": " Building service-based systems with the Service Oriented Architecture (SOA) requires knowledge and experience from diverse domains, including user interaction, service-oriented computing, as well as service platforms and infrastructures. These domains are addressed by different research communities. Therefore, joint research activities between these communities are key to provide novel service technologies for the Future Internet. As each community uses its own language, this poses significant communication challenges. To foster a common understanding of researchers, this paper reports on the process, the results and the lessons learned in devising an agreed terminology within the context of the NEXOF initiative. This terminology is freely accessible on the Web.", "num_citations": "6\n", "authors": ["1447"]}
{"title": "A filter-mechanism for method-driven trace capture\n", "abstract": " Traceability is a prerequisite for developing high quality (software) systems. Recording and maintaining all available information is too labor intensive and thus by far too expensive. A project-specific definition of the trace information to be recorded and the method fragments (so called trace fragments) to be executed for recording the information provides a solution for this problem. But the amount of traces to be recorded does not only vary from project to project. It also varies between project phases and even within a project phase. As a consequence project-specific trace fragments need to be adapted according to the actual project phase.             In this paper we propose a model-based filter mechanism to significantly reduce the required effort to adapt trace fragments. By defining appropriate filters the project manager is able to (dynamically) adapt the project-specific trace fragments to the actual needs. We\u00a0\u2026", "num_citations": "6\n", "authors": ["1447"]}
{"title": "Modellierung\u201998\n", "abstract": " Auf der Programmkomitee-Sitzung am 12.12. 97 in Aachen wurden von den eingereichten Papieren 17 f\u00fcr die Pr\u00e4sentation auf dem Workshop angenommen. Um Diskussionen und konstruktives Arbeiten zu erm\u00f6glichen hat das Programmkomitee die Teilnehmerzahl am Workshop sehr stark begrenzt. Aufgrund der \u00fcberaus hohen Resonanz im Vorfeld des Workshops haben wir aber bereits \u00fcber eine Folgeveranstaltung im Jahre 1999 nachgedacht.Wir hoffen, da\u00df die eingeladenen \u00dcbersichtsvortr\u00e4ge aus den einzelnen Fachgruppen, die themenzentrierten Sitzungen, sowie die gro\u00dfz\u00fcgig eingeplante Zeit f\u00fcr Diskussionen und das nette Ambiente in M\u00fcnster die Voraussetzungen f\u00fcr eine erfolgreiche Modellierung'98 schaffen.", "num_citations": "6\n", "authors": ["1447"]}
{"title": "CASE environment adaptability: Bridging the islands of automation\n", "abstract": " In current CASE environments a user has to choose between efficient computerized support using a fixed methodical framework which may not fit his situation, or the freedom to do what seems appropriate in the given circumstance, but at the cost of losing efficient technological support. In this paper we examine adaptable metamodel based environments as a means to resolve this dilemma. Metamodel based environments provide means to represent and modify knowledge about development products, processes, and representation schemes to improve the designer-task fit. Metamodel based adaptability is not a new innovation. Yet, earlier metamodel based approaches have tended to create islands of automation that focus on improving adaptability either in ontologies, notations, or process definitions. The paper applies a framework which integrates these aspects and thereby increases environment adaptability that covers a wider spectrum of development situations. The metalevel based integration is demonstrated by describing how two metamodel based tools focusing on different aspects of adaptability can be integrated. The resulting integrated environment encompasses both product (ontology/notation) and process aspects.", "num_citations": "6\n", "authors": ["1447"]}
{"title": "Workshop summary second international workshop on requirements engineering: foundation of software quality\n", "abstract": " As achieving high quality means the realization of customers needs, requirements engineering (RE) is the most crucial phase within software development. During RE not only the functional requirements but also the so-called 'non-functional' requirements of the planned software system have to be elicited from the customer and represented in a requirements document in order to provide the software designer with a complete and correct specification. Conventional RE methods normally support only parts of this process or help stating only specific kinds of requirements.These methodological problems are the prime motivation for the REFSQ workshop series held in conjunction with the CAiSE Conference on Advanced Information Systems Engineering. In order to find solutions which handle the described deficiencies it is the goal of this workshop series to improve the understanding of the relations between RE and\u00a0\u2026", "num_citations": "6\n", "authors": ["1447"]}
{"title": "Model-Based Engineering of Collaborative Embedded Systems: Extensions of the SPES Methodology\n", "abstract": " This Open Access book presents the results of the \"Collaborative Embedded Systems\" (CrESt) project, aimed at adapting and complementing the methodology underlying modeling techniques developed to cope with the challenges of the dynamic structures of collaborative embedded systems (CESs) based on the SPES development methodology. In order to manage the high complexity of the individual systems and the dynamically formed interaction structures at runtime, advanced and powerful development methods are required that extend the current state of the art in the development of embedded systems and cyber-physical systems. The methodological contributions of the project support the effective and efficient development of CESs in dynamic and uncertain contexts, with special emphasis on the reliability and variability of individual systems and the creation of networks of such systems at runtime. The project was funded by the German Federal Ministry of Education and Research (BMBF), and the case studies are therefore selected from areas that are highly relevant for Germany\u2019s economy (automotive, industrial production, power generation, and robotics). It also supports the digitalization of complex and transformable industrial plants in the context of the German government's \"Industry 4.0\" initiative, and the project results provide a solid foundation for implementing the German government's high-tech strategy \"Innovations for Germany\" in the coming years.", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services\n", "abstract": " A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service\u2019s adaptation actions\u00a0\u2026", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Eine Referenzstrukturierung zur modellbasierten Kontextanalyse im Requirements Engineering softwareintensiver eingebetteter Systeme\n", "abstract": " Dem Requirements Engineering (RE) kommt im Entwicklungsprozess die Aufgabe zu, die Anforderungen an das zu entwickelnde System zu spezifizieren. Der Ursprung von Anforderungen liegt in der Umgebung des zu entwickelnden Systems \u2013 dem Systemkontext. Um sicherzustellen, dass das entwickelte System den Erwartungen, Bedingungen und Reglementierungen seiner Umgebung gen\u00fcgt, muss der Systemkontext in den Anforderungen vollst\u00e4ndig und fehlerfrei ber\u00fccksichtigt werden. Dies setzt voraus, dass der Kontext des zu entwickelnden Systems in seiner Gesamtheit richtig erfasst wird. Die Erfassung des Systemkontexts im RE ist Gegenstand der Kontextanalyse. Das RE in der Entwicklung softwareintensiver eingebetteter Systeme sieht sich immer h\u00e4ufiger der Situation gegen\u00fcber, den zunehmend komplexer werdenden Kontext solcher Systeme unter bisweilen rigidesten Budget- und Zeitvorgaben vollst\u00e4ndig und korrekt erfassen zu m\u00fcssen, um die Anforderungen an das zu entwickelnde System spezifizieren zu k\u00f6nnen. In diesem Beitrag wird eine Referenzstrukturierung fu\u0308r den Systemkontext softwareintensiver eingebetteter Systeme vorgeschlagen, die ein Rahmenwerk zur strukturierten Erfassung und Dokumentation des Systemkontexts definiert und eine Komplexit\u00e4tsreduktion der Kontextbetrachtung in der modellbasierten Kontextanalyse verspricht.", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Domain requirements engineering\n", "abstract": " The main goals of domain requirements engineering are the development of common and variable domain requirements and their precise documentation. Domain requirements engineering is a continuous process of proactively defining the requirements for all foreseeable applications to be developed in the software product line. A particular issue for domain requirements engineering is to identify and explicitly document the external variability. The sub-processes and artefacts closely related to the domain requirements engineering sub-process are highlighted in Fig. 10-1.Domain requirements engineering has to adhere to the specification of the product line\u2019s major features provided by product management. Based on these features, it creates detailed common and variable requirements sufficient to guide domain design (and thereby also realisation as well as testing). In addition, domain requirements\u00a0\u2026", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Experiences with software product line engineering\n", "abstract": " Platform: ABB gained plenty of experience with different software product lines. The first example is the ABB Gas Turbine Family, which covers the power range of 35 to 270 MW with five basic turbine types varying in size, combustion technologies, and equipment [Ganz and Layes 1998]. The second example is the Semantic Graphics Framework. It supports the development of graphical applications that realise special requirements in the engineering domain [R\u00f6sel 1998]. The third example is ABB\u2019s train control product line which is an embedded real-time software system for controlling train movement [Eixelsberger and Beckman 2000].Experience: The experiences of ABB with the software product line approach are positive. The Semantic Graphics Framework has been in use for several years in different business units. More than ten industrial applications have been derived from it [R\u00f6sel 1998]. The reference\u00a0\u2026", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Product management\n", "abstract": " The goal of product management is to make a major contribution to entrepreneurial success by integrating the development, production, and marketing of products that meet customer needs. 29 Product management is responsible for enforcing entrepreneurial goals throughout the software engineering process. Therefore it has an influence on requirements engineering, design, realisation, and testing. The sub-processes and artefacts closely related to product management are highlighted in Fig. 9-1.The major result of product management with respect to the software product line framework is the product roadmap. Note that we did not include the product roadmap in the framework picture as it is no development artefact in the common sense (Section 2.5. 1). The product roadmap outlines the product line as far as it is foreseeable at a given point in time. It defines the major common and variable features of all\u00a0\u2026", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Documenting variability in requirements artefacts\n", "abstract": " We describe the way of documenting variability in different kinds of requirements artefacts in order to provide the reader with the basic knowledge that is necessary to document requirements variability in the domain artefacts of a software product line. The sub-processes and artefacts closely related to documenting variability in domain requirements are highlighted in Fig. 5-1.Domain requirements are created in the domain requirements engineering sub-process. They encompass requirements common to all applications of the software product line as well as variable requirements enabling the creation of different applications. Domain requirements artefacts are the input for the domain design sub-process, which is concerned with developing the domain architecture. Domain testing uses domain requirements artefacts to provide reusable test artefacts for the software product line. Application requirements artefacts\u00a0\u2026", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Interrelating goal models and multimedia scenes: An empirical investigation\n", "abstract": " Conceptual goal models are used to express intentional aspects of the system under development. Among others, goal models facilitate stakeholder discussions and agreement about main system aspects during early requirements engineering phases. As experiences from participatory design indicates, the use of multimedia representations (especially videos) leads to better stakeholder involvement and, as a consequence, the produced conceptual (goal) models and specifications respectively are of higher quality. In this paper, we report on our empirical investigation which shows that the use of associations between goals and video parts documenting goal achievements and goal failures improve the performance of typical requirements engineering tasks. More precisely, they lead to more correct and complete results.", "num_citations": "5\n", "authors": ["1447"]}
{"title": "Learning and Evolution in Dynamic Software Product Lines\n", "abstract": " A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Model-based verification of event-driven business processes\n", "abstract": " Event-driven business processes employ event-processing capabilities to analyze internal and external event streams for complex situations that may impact on process execution. An open issue is the verification of such event-driven business processes in order to detect, among others, deadlocks. Individual verification approaches for business processes and event processing networks exist. However, even if the business processes and the event processing networks individually are deadlock-free, deadlocks may arise due to the integration of event processing networks and business processes into event-driven business processes. This paper introduces a verification technique for event-driven business processes. The main aspects of the technique are (1) to formalize business process models and event processing networks as an integrated petri-net, and (2) to perform reachability analyses of the integrated petri\u00a0\u2026", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Informatik\u201997 Informatik als Innovationsmotor: 27. Jahrestagung der Gesellschaft f\u00fcr Informatik Aachen, 24.\u201326. September 1997\n", "abstract": " Der Band enth\u00e4lt die Tagungsbeitr\u00e4ge zur 27. Jahrestagung der Gesellschaft f\u00fcr Informatik 1997. Schwerpunkte der Darstellung sind zentrale Forschungsergebnisse aus Hochschulen, Gro\u00dfforschungseinrichtungen und Industrie, wichtige Trends aus Hersteller-und Anwendersicht, Kooperation zwischen Schule, Hochschule und Praxis sowie Resultate, Chancen und Probleme europ\u00e4ischer Informatikprojekte.", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Verification and testing at run-time for online quality prediction\n", "abstract": " This paper summarizes two techniques for online failure prediction allowing to anticipate the need for adaptation of service-oriented systems: (1) SPADE, employing run-time verification to predict failures of service compositions. (2) PROSA, building on online testing to predict failures of individual services.", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Szenario-basierter Systemtest von Software-Produktfamilien\n", "abstract": " In der Produktfamilienentwicklung werden durch zwei Entwicklungsprozesse, Domain und Application Engineering, zun\"achst wiederverwendbare Entwicklungsartefakte produziert, um diese anschlie\u00dfend zur Konstruktion von kundenspezifischen Applikationen einzusetzen. Die Wiederverwendbarkeit wird durch die explizite Definition der Variabilit\"at der geplanten Applikationen einer Produktfamilie erzielt. Diese proaktive Wiederverwendung ist bisher in den konstruktiven Entwicklungsphasen realisiert, jedoch noch nicht im Test. Mit ScenTED (Scen_ario based TE_st Case D_erivation) wird in diesem Beitrag eine wiederverwendungsorientierte Technik zur Testfallerstellung f\"ur den Systemtest, dem Test eines ausf\"uhrbaren Systems gegen spezifizierte Use-Cases, von Produktfamilien vorgestellt. ScenTED basiert auf zwei Kernideen: der Erhaltung der Variabilit\"at in Testf\"allen und der Szenario\u00a0\u2026", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Domain testing\n", "abstract": " The goal of domain testing is to validate the output of the other domain engineering sub-processes. Our main focus is on the validation of the realisation artefacts. The derivation of test cases is based on the input from domain requirements engineering, domain design, and domain realisation. The goal of domain testing is to establish an efficient overall testing process. This involves testing early and often what can be tested within the domain engineering process and providing reusable test artefacts. Testing aspects have to be considered right from the beginning of the development, eg to ensure that requirements and design support testing. For instance, testing requires that the state of a component can be evaluated at run-time to be able to compare the expected results of an action with the actual results. Consequently, component interfaces need to be designed to enable the introspection into a component\u2019s state\u00a0\u2026", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Bridging the islands of automation\n", "abstract": " In current CASE environments a user has to choose between efficient computerized support using a fixed methodical framework which may not fit his situation, or the freedom to do what seems appropriate in the given circumstance, but at the cost of losing efficient technological support. In this paper we examine adaptable metamodel based environments as a means to resolve this dilemma. Metamodel based environments provide means to represent and modify knowledge about development products, processes, and representation schemes to improve the designer-task fit. Metamodel based adaptability is not a new innovation. Yet, earlier metamodel based approaches have tended to create islands of automation that focus on improving adaptability either in ontologies, notations, or process definitions. The paper applies a framework which integrates these aspects and thereby increases environment adaptability that covers a wider spectrum of development situations. The metalevel based integration is demonstrated by describing how two metamodel based tools focusing on different aspects of adaptability can be integrated. The resulting integrated environment encompasses both product (ontology/notation) and process aspects. 1", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Report on the first international IEEE symposium on requirements engineering (RE93) San Diego, Jan. 4\u20136, 1993\n", "abstract": " The IEEE Computer Society held its first international symposium on requirements engineering with about two hundred participants in the beautiful Hotel Coronado close to San Diego, California. The audience was spread across many different countries, with the majority of papers coming from the US and the UK. The program committee, led by Anthony Finkelstein (Imperial College London) and Steve Fickas (University of Oregon, USA), had decided on a very selective high-quality paper strategy with at most two parallel sessions, a very good approach to achieve coherence in this wide open field.", "num_citations": "4\n", "authors": ["1447"]}
{"title": "Evaluation eines modellbasierten Requirements-Engineering-Ansatzes f\u00fcr den Einsatz in der Motorsteuerungs-Dom\u00e4ne\n", "abstract": " Modellbasierte Entwicklungsans\u00e4tze f\u00fcr softwareintensive, eingebettete Systeme sollen dazu beitragen, Fehler im Entwicklungsprozess zu vermeiden, die Entwicklungsdauer zu verk\u00fcrzen und die hohe Systemkomplexit\u00e4t besser beherrschbar zu machen. Besonders vielversprechend erscheint der Einsatz von Modellen bereits in fr\u00fchen Entwicklungsaktivit\u00e4ten, in denen die Anforderungen und die Architektur definiert werden. COSMOD-RE ist ein zielund szenariobasierter Requirements-Engineering-Ansatz, der eine verzahnte Entwicklung von Anforderungsund Architekturmodellen \u00fcber mehrere Abstraktionsstufen eines softwareintensiven, eingebetteten Systems hinweg unterst\u00fctzt. In diesem Beitrag werden die Erkenntnisse und Erfahrungen beschrieben, die bei der Evaluation von COSMOD-RE in einem spezifischen Entwicklungskontext, der Entwicklung von Anforderungen f\u00fcr die Anwendungssoftware einer Motorsteuerung, gesammelt wurden. Hauptmotivation f\u00fcr diese Evaluation war es, Erkenntnisse \u00fcber den Nutzen und die Anwendbarkeit von COSMOD-RE in dieser Dom\u00e4ne zu gewinnen.", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Entwicklung eines Leitfadens f\u00fcr das Requirements Engineering softwareintensiver Eingebetteter Systeme\n", "abstract": " Gerade in den letzten Jahren haben die Funktionsvielfalt und die Komplexit\u00e4t der einzelnen Funktionen bei Eingebetteten Systemen au\u00dferordentlich zugenommen. Moderne Kraftfahrzeuge verf\u00fcgen \u00fcber mehr als 3000 softwaregesteuerte Funktionen. Diese leiten sich aus immer umfangreicher werdenden Spezifikationen ab. So umfasst ein typisches Spezifikationsdokument eines modernen Kombiinstruments mittlerweile mehr als 20.000 Anforderungen. Eine systematische und zielgerichtete Erfassung, Strukturierung, Dokumentation und Verwaltung dieser Anforderungen ist daher von entscheidender Bedeutung. W\u00e4hrend derzeit oftmals noch zu beobachten ist, dass einzelne lokale Helden mit reichem Erfahrungswissen Projekterfolge erm\u00f6glichen, ist zu erwarten, dass die derzeit eingesetzten ad-hoc Ans\u00e4tze zur Spezifikation von Anforderungen an ihre Grenzen sto\u00dfen. Aufgrund der gro\u00dfen und weiter wachsenden Bedeutung Eingebetteter Systeme f\u00fcr die deutsche Automobilindustrie ist die Beherrschung der Requirements-Engineering und-Management-Prozesse eine immer wichtigere F\u00e4higkeit, die zunehmend den wirtschaftlichen Erfolg bestimmen wird. Dies gilt umso mehr aufgrund der stark arbeitsteiligen Entwicklung mit vielen kleinen und mittleren Zulieferern. Nur mit geeigneten Techniken und Methoden werden in Zukunft die Komplexit\u00e4t von Systemen und Systemverb\u00fcnden beherrschbar bleiben. In diesem Spannungsfeld wurde im Rahmen der F\u00f6rderinitiative Software Engineering 2006 des Bundesministeriums f\u00fcr Bildung und Forschung (BMBF) das Verbundprojekt REMsES im August 2006 mit einer Laufzeit von drei Jahren\u00a0\u2026", "num_citations": "3\n", "authors": ["1447"]}
{"title": "S-Cube: Enabling the Next Generation of Software Services\n", "abstract": " The Service Oriented Architecture (SOA) paradigm is increasingly adopted by industry for building distributed software systems. However, when designing, developing and operating innovative software services and servicebased systems, several challenges exist. Those challenges include how to manage the complexity of those systems, how to establish, monitor and enforce Quality of Service (QoS) and Service Level Agreements (SLAs), as well as how to build those systems such that they can proactively adapt to dynamically changing requirements and context conditions. Developing foundational solutions for those challenges requires joint efforts of different research communities such as Business Process Management, Grid Computing, Service Oriented Computing and Software Engineering. This paper provides an overview of S-Cube, the European Network of Excellence on Software Services and\u00a0\u2026", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Model-based testing of software product lines\n", "abstract": " Due to the rising demand for individualised software products and software-intensive systems (e.g.,mobile phone or automotive software), organizations are faced with the challenge to provide a diversity of software systems at low costs, in short time, and with high quality. Software product line engineering is the approach for tackling this challenge and has proven its effectiveness in numerous industrial success stories, including Siemens, ABB, Boeing, Hewlett-Packard, Philips, and Bosch [Pohl et al. 2005].", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Application testing\n", "abstract": " The goal of the application testing sub-process is to achieve a sufficient quality of the application under test. Application testing thus complements the testing activities of domain testing.The sub-processes and artefacts closely related to the application testing subprocess are highlighted in Fig. 18-1. Application testing reuses domain test artefacts. The unit test in application testing requires input from application realisation. The integration test requires input from application design, and the system test is performed on the basis of application requirements. The results of application testing are provided as feedback to the related subprocesses. Figure 18-2 shows the information flows between application testing and its related sub-processes.", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Szenario-basiertes systemtesten von software-produktfamilien mit ScenTED\n", "abstract": " Beim Testen von Software-Produktfamilien stellt der Testaufwand ein gro\u00dfes Problem in der Praxis dar. Durch Bewahrung der Variabilit\u00e4t in Testartefakten des Domain Engineerings kann mit Hilfe der ScenTED-Methode (Scenario based TEst Case Derivation) durch Wiederverwendung im Application Engineering der Testaufwand verringert werden. Die Variabilit\u00e4t erm\u00f6glicht die Ableitung von produktspezifischen Testf\u00e4llen mit Hilfe von beschriebenen Varianten. Die ScenTED-Methode basiert auf der systematischen Verfeinerung von Use-Case-Szenarien zu Testf\u00e4llen f\u00fcr den Systemund Integrationstest. Durch diese Verfeinerung der Szenarien wird die Nachvollziehbarkeit zwischen Entwicklungsartefakten und Testartefakten unterst\u00fctzt, wodurch die Wiederverwendung von Testf\u00e4llen vereinfacht und ein sp\u00e4teres Change-Management erm\u00f6glicht wird. In diesem Artikel liegt der Fokus auf der Erstellung und Wiederverwendung von Testartefakten f\u00fcr den Systemtest.", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Abstraction Guides: Interrelating Conceptual Models with Real World Scenes.\n", "abstract": " The role of the requirements engineer is to establish a complete consistent and unambiguous requirements specification which defines the requirements at a conceptual level. Many traditional modelling approaches from Structured Analysis [6],[14] to UML-based methods [7] supporting him in this task neglect the use of concrete examples about current or future system usage.In this paper, we present so called Abstraction Guides which assist the requirements engineer in establishing and applying an interrelating structure between conceptual models and persistent recorded usages of existing systems called Real World Scenes (RWS). This structure is used to improve traceability, understandability and negotiation of conceptual models. We show how Abstraction Guides define support for eliciting requirements from RWS, validating requirements against RWS, explanation of conceptual models, and comparing scenes and models.", "num_citations": "3\n", "authors": ["1447"]}
{"title": "Incremental verification of complex event processing applications for system monitoring\n", "abstract": " Complex Event Processing (CEP) facilitates monitoring large-scale, distributed systems. CEP applications analyze real-time streams of events to detect patterns that indicate problems that may require an adaptation of the running system. Like for any software system, developers may introduce faults when designing and implementing CEP applications. Such faults may imply that true problems may not be detected during systems operation, or false alarms may be raised even though no problem exists. Therefore, verifying the CEP application during design time is critical to ensure correct system monitoring at run-time. To address the scalability problem of verifying CEP applications, we propose an incremental verification approach building on recent advances in model checking. Results of an initial evaluation indicate under which assumptions our approach scales better than standard model checking.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Advanced information systems engineering workshops\n", "abstract": " A continuous challenge in modern information systems engineering (ISE) is to provide significant aid toward the improvement of the design, implementation, and fielding of advanced information systems. However, a timely daunting task is to employ ISE approaches to real-world, large-scale, adaptable systems that can have a potential impact in various diverse aspects of people\u2019s life. All of these topics and potential roadmaps toward innovation that might lead to development and welfare were discussed in the workshops that took place under the framework of the 26th CAiSE held in Thessaloniki, Greece, June 16\u201320. It is a long-standing tradition of the International Conference on Advanced Information Systems Engineering to be accompanied by an ensemble of highquality workshops. Their aim is to serve as a discussion forum between stakeholders in this domain, to exchange innovative ideas on new\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Leitfaden f\u00fcr modellbasiertes Requirements-Engineering und-Management softwareintensiver Eingebetteter Systeme\u2014REMsES\u2014\n", "abstract": " Die immer gr\u00f6\u00dfer werdende Menge von softwaregesteuerten Funktionen f\u00fcr Eingebettete Systeme zB in modernen Kraftfahrzeugen erfordert ein f\u00fcr die jeweilige Dom\u00e4ne angepasstes und optimiertes Requirements Engineering und\u2013Management (REM). Ohne geeignete Techniken im REM l\u00e4sst sich die zunehmende Komplexit\u00e4t der Gewinnung, Dokumentation und Verwaltung der immer gr\u00f6\u00dfer werdenden Anzahl von Anforderungen und ihrer Abh\u00e4ngigkeiten nicht mehr bew\u00e4ltigen. In der Praxis existierende ad-hoc Ans\u00e4tze k\u00f6nnen die REM-Prozesse nicht ausreichend unterst\u00fctzen. Dies f\u00fchrt zu einer fehleranf\u00e4lligen und ineffizienten Entwicklung Eingebetteter Systeme. Ziel des REMsES-Projektes ist die Erarbeitung eines praxistauglichen Leitfadens f\u00fcr ein systematisches REM Eingebetteter Systeme, insbesondere im Automobilbereich. Die praxistaugliche Ausrichtung des Leitfadens, zB durch eine Sammlung\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Towards a Service-Based Internet: First European Conference, ServiceWave 2008, Madrid, Spain, December 10-13, 2008, Proceedings\n", "abstract": " Today it is almost impossible to remember what life was like with no computer, no mobile phone, and no Internet for accessing information, performing tra-actions or exchanging emails and data. New technology is bringing wave after wave of new bene? ts to daily life: organisations are doing business with each other via the Internet; people are? lling in tax declarations online and booking their next vacation through the Internet. In general we are all progressively-ing (and dependent on) software and services running on computers, connecting mobile phones and other devices, and exchanging information on the Internet. People like to shop around and exercise choice. So do businesses and public administrations. Today they can buy a complete software package that best suits their needs, even though they may never use some of the tools it o? ers, or other desirable tools are not available. In the future they may no longer have to compromise on choice. Alternative approaches like \u201cSoftware as a Service\u201d and \u201cComputing Resources as a Service\u201d are emerging. Software is provided on-line as a service when and where it is needed, and the same for computing resources needed to run software. Such an approach allows individuals and organisations totapintoande? ectivelyharnesstheimmensewealthofinformation, knowledge and analytical resources when they need them, paying only for what they use. Customersareboundtobene? twhenthereisasu? cientlyrichchoiceofservices.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Wiederverwendung von integrationstestf\u00e4llen in der software-produktlinienentwicklung\n", "abstract": " Bei der Software-Produktlinienentwicklung ist der Entwicklungsprozess aufgeteilt in die Dom\u00e4nen-   und die Applikationsentwicklung. Durch die Wiederverwendung von Testartefakten aus der Dom\u00e4nenentwicklung   kann in der Applikationsentwicklung der Testaufwand signifikant reduziert werden. Existierende Ans\u00e4tze   zum Testen von Software-Produktlinien beschr\u00e4nken sich auf den Modul- und den Systemtest. In diesem   Beitrag wird eine durchg\u00e4ngige und modellbasierte Technik f\u00fcr die automatisierte Ableitung von   logischen Integrationstestf\u00e4llen bei der Entwicklung von Software-Produktlinien vorgestellt. Die Technik   umfasst sowohl die Generierung von wiederverwendbaren Integrationstestf\u00e4llen in der Dom\u00e4nenentwicklung,   als auch die Ableitung von Applikationstestf\u00e4llen unter Wiederverwendung der Dom\u00e4nentestf\u00e4lle.   Mit Hilfe der wiederverwendbaren Integrationstestf\u00e4lle wird zus\u00e4tzlich\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Ableitung von Systemfunktionen aus Zielen und Szenarien\n", "abstract": " Die in der Literatur beschriebenen Ans\u00e4tze zur Ableitung von l\u00f6sungsorientierten Anforderungen fokussieren typischer Weise entweder nur auf Ziele oder nur auf Szenarien als Basis f\u00fcr die Ableitung. Beispielsweise basiert die Ableitung von l\u00f6sungsorientierten Anforderungen in der Methode KAOS (siehe zB [La01]) auf einem Zielmodell. Zudem sind existierende Ans\u00e4tze typischer Weise nicht f\u00fcr die Einhaltung vorgegebener Abstraktionsebenen konzipiert.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Overview of the Example Domain: Home Automation\n", "abstract": " In recent times, smart homes have moved into the focus of scientific and technological research and development. Most everyday-life technical devices are controlled by microprocessors. Home automation integrates such devices into a network. The network allows the devices to coordinate their behaviour in order to fulfil complex tasks without human intervention. Intuitive user interfaces allow easy access to the functionality of a smart home.A variety of domains contribute to the evolution of smart homes. An overview is given in Fig. 3-1. The relation between home automation and these domains becomes clearer in the course of this chapter. For example,\u2018web technology\u2019allows to access home functions remotely through the Internet.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Selecting High-Level COTS Components\n", "abstract": " In order to select a COTS component, candidate components that are available in the market or which exist in the organisation have to be evaluated and ranked according to defined criteria. We distinguish between high-and low-level component selection. The key discriminator is the fraction of functionality that a COTS component is supposed to provide with respect to the overall functionality of the software product line. Low-level components provide a minor part of the overall functionality and have little influence on the reference architecture. They are selected during domain realisation. The focus of this chapter is on the high-level components. Since they provide a significant fraction of the overall functionality they must be considered in the design right from the beginning. When we speak about COTS selection in this book, we refer to the high-level COTS selection process. The subprocesses and artefacts closely\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Documenting variability in test artefacts\n", "abstract": " Test artefacts contain the instructions for testers what to test, when to test, how to test, and how to document the test results. The test results themselves are test artefacts, too. Test artefacts enable repeatable and traceable tests. Testing is performed in domain engineering as well as in application engineering and thus test artefacts are created in both processes. A major task of domain testing is to develop test artefacts that can be reused efficiently in application testing. This is achieved with a clear and unambiguous documentation of variability in test artefacts. In this chapter, we focus on that documentation. We provide a brief description of important test artefacts and show how to employ the orthogonal variability model. The sub-processes and artefacts closely related to documenting variability in domain tests are highlighted in Fig. 8-1.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Anforderungsorientierte Variabilit\u00e4tsmodellierung f\u00fcr Software-Produktfamilien\n", "abstract": " Die Entwicklung von Software-Produktfamilien hat zum Ziel, durch Ausnutzung von Variabilit\u00e4t unterschiedliche Produkte auf der Basis einer gemeinsamen Plattform effizient und mit hoher Qualit\u00e4t zu entwickeln. Variabilit\u00e4t ist daher ein zentrales Konzept der Software-Produktfamilien Entwicklung. F\u00fcr die Realisierung und \u00c4nderung von Produkten auf der Basis einer Software-Produktfamilie ist u. a. eine f\u00fcr alle Entwicklungsphasen (z.B. Requirements Engineering, Architekturdesign, Implementierung) durchg\u00e4ngige Repr\u00e4sentation der Produktfamilien-Variabilit\u00e4t eine wesentliche Voraussetzung. Im Rahmen des Requirements Engineerings ist es notwendig, die Repr\u00e4sentation der Variabilit\u00e4t mit unterschiedlichen Anforderungsmodellen wie beispielsweise Zielmodellen oder Szenarien zu verkn\u00fcpfen und damit die verschiedenen Sichten der Anforderungsmodellierung einflie\u00dfen zu lassen. In diesem Beitrag beschreiben wir drei Arten von Anforderungsmodellen und deren Beziehungen untereinander. Im Weiteren erl\u00e4utern wir, wie Produktfamilien- Variabilit\u00e4t in Bezug auf diese drei Arten von Anforderungsmodellen repr\u00e4sentiert und die Produktdefinition durch die Beziehungen der Modelle untereinander vereinfacht werden kann.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Modellbasiertes Requirements Engineering\u2013Eine Situationsanalyse zum Stand der Praxis\n", "abstract": " Neue Produkteigenschaften werden in zahlreichen technischen Dom\u00e4nen (wie zum Beispiel in der Automobildom\u00e4ne) in zunehmendem Ma\u00dfe softwarebasiert realisiert. Die Anzahl von verteilten, miteinander vernetzten (softwarebasierten) Systemfunktionen steigt stetig an, wodurch die Komplexit\u00e4t der Systeme ebenfalls rapide zunimmt. Folglich wachsen auch die Herausforderungen f\u00fcr die Entwicklung dieser Systeme und insbesondere f\u00fcr das Requirements Engineering (RE). Die Anforderungen an die Systeme werden stetig komplexer und m\u00fcssen in immer k\u00fcrzeren Zyklen gewonnen, analysiert und spezifiziert werden. Dabei m\u00fcssen h\u00e4ufig strikte Qualit\u00e4tsstandards eingehalten werden, die bei sicherheitsrelevanten Systemfunktionen durch Normen und Standards reglementiert werden. Der Einsatz von Modellen erscheint aufgrund positiver Erfahrungen in anderen Disziplinen als ein vielversprechender L\u00f6sungsansatz, um den oben genannten Herausforderungen im RE geeignet zu begegnen [2],[6]. Modelle k\u00f6nnen unter anderem dazu beitragen, eine hohe Systemkomplexit\u00e4t zu bew\u00e4ltigen, die Qualit\u00e4t von Anforderungen zu steigern, die Nachvollziehbarkeit zu verbessern und die Kommunikation zwischen Requirements-Ingenieuren und Kunden sowie Entwicklern zu unterst\u00fctzen [5]. Damit die Forschung modellbasierte RE-Ans\u00e4tze bereitstellen kann, die in der Praxis Akzeptanz finden, ist ein detailliertes und fundiertes Verst\u00e4ndnis n\u00f6tig, welche Erwartungen die Praxis an den Einsatz von Modellen im RE hat, welche Rahmenbedingungen zu beachten sind und welche konkreten Anforderungen an einen solchen Ansatz gestellt werden\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Workshopbericht\" Modellierung'98\n", "abstract": " Das Programm des ersten Tages umfa\u00dfte drei eingeladene und drei angenommene Beitr\u00e4ge. HC Mayr (Universit\u00e4t Klagenfurt) er\u00f6ffnete als Vertreter der EMISA mit\" Entwicklungsmethodologie f\u00fcr Informationssysteme: Wunsch und Wirklichkeit\". Eine Betrachtung der fast 20-j\u00e4hrigen Geschichte der FG EMISA zeigt einerseits deutliche Entwicklungen: von der eher technisch orientierten Datenmodellierung \u00fcber eine Einbeziehung der sogenannten\" fr\u00fchen Phasen\" des Informationssystementwurfs bis hin zu einer immer st\u00e4rker und wichtiger werdenden Einbeziehung des Anwenders in den Entwurfs-und Modellierungsproze\u00df. Andererseits fehlt nach wie vor eine\" Konstruktionslehre\" f\u00fcr Informationssysteme. Dom\u00e4nenwissen ist noch zu wenig vorhanden und kann dementsprechend noch nicht in angemessenem Umfang bei der Modellierung ber\u00fccksichtigt werden. Abhilfe k\u00f6nnten hier zB\" kleine\" Modellierungssprachen schaffen, die auf einen bestimmten Anwendungsbereich hin abgestimmt sind. Entsprechendes gilt f\u00fcr einen Re-Use von Modellen, die besser an den Anwender, weniger an den Informatiker angepa\u00dft werden m\u00fcssen. Es zeigt sich heute ferner, da\u00df unternehmensweite Datenmodellierung im wesentlichen gescheitert ist. Hauptgr\u00fcnde hierf\u00fcr sind das Problem der Integration der verschiedenen unterschiedlichen Sichten auf das entstehende Schema sowie die kaum bew\u00e4ltigbare Komplexit\u00e4t der Aufgabenstellung.", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Workshop summary first international workshop on requirements engineering: foundation of software quality (REFSQ; 94)\n", "abstract": " As achieving high quality means the realization of customers needs, requirements engineering (RE) is the most crucial phase within software development. In the RE process not only the functional requirements but also the so-called 'non-functional' or 'quality' requirements of the planned software system have to be elicited from the customer and represented in a requirements document in order to provide the software designer a complete and correct specification. Conventional RE methods usually support only parts of this process or help stating only specific kinds of requirements.These methodological problems were the prime motivation for the REFSQ'94 workshop held in conjunction with the CAiSE '94 Conference on Advanced Information Systems Engineering in Utrecht, The Netherlands on June 6th and 7th 1994. In order to find solutions which handle the described deficiencies, it was the goal of the workshop\u00a0\u2026", "num_citations": "2\n", "authors": ["1447"]}
{"title": "Model-Based Engineering of Collaborative Embedded Systems: Extensions of the SPES Methodology\n", "abstract": " This Open Access book presents the results of the\" Collaborative Embedded Systems\"(CrESt) project, aimed at adapting and complementing the methodology underlying modeling techniques developed to cope... with the challenges of the dynamic structures of collaborative embedded systems (CESs) based on the SPES development methodology. In order to manage the high complexity of the individual systems and the dynamically formed interaction structures at runtime, advanced and powerful development methods are required that extend the current state of the art in the development of embedded systems and cyber-physical systems. The methodological contributions of the project support the effective and efficient development of CESs in dynamic and uncertain contexts, with special emphasis on the reliability and variability of individual systems and the creation of networks of such systems at runtime. The project was funded by the German Federal Ministry of Education and Research (BMBF), and the case studies are therefore selected from areas that are highly relevant for Germany's economy (automotive, industrial production, power generation, and robotics). It also supports the digitalization of complex and transformable industrial plants in the context of the German government's\" Industry 4.0\" initiative, and the project results provide a solid foundation for implementing the German government's high-tech strategy\" Innovations for Germany\" in the coming years. \u7d9a\u304d\u3092\u898b\u308b", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Situativer Datenschutz im Fog-Computing\n", "abstract": " Fog-Computing erlaubt, Software-Code oder Daten dynamisch von ressourcenschwachen Endger\u00e4ten an leistungsst\u00e4rkere Ger\u00e4te am Rande des Netzwerks und in der Cloud auszulagern. Eine solche dynamische Auslagerung erm\u00f6glicht eine performante Ausf\u00fchrung rechenintensiver Aufgaben, bei gleichzeitig geringer Latenzzeit f\u00fcr die Daten\u00fcbertragung. Beim Datenschutz ergeben sich im Fog-Computing jedoch spezifische Herausforderungen. Wir beschreiben die wesentlichen Herausforderungen des Datenschutzes im Fog-Computing und diskutieren, wie diese Herausforderungen durch die situative Kombination verschiedener Datenschutztechniken zur Laufzeit adressiert werden k\u00f6nnen.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Advanced Information Systems Engineering: 29th International Conference, CAiSE 2017, Essen, Germany, June 12-16, 2017, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 29th International Conference on Advanced Information Systems Engineering, CAiSE 2017, held in Essen, Germany, in June 2017. The 37 papers presented together with 3 keynote papers in this volume were carefully reviewed and selected from 175 submissions. The papers are organized in topical sections on information systems architecture; business process alignment; user knowledge discovery; business process performance; big data exploration; process variability management; information systems transformation and evolution; business process modeling readability; business process adaption; data mining; process discovery; business process modeling notation.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Real-time cargo volume recognition using internet-connected 3D scanners\n", "abstract": " Transport and logistics faces fluctuations in cargo volume that statistically can only be captured with a large error. Observing and managing such dynamic volume fluctuations more effectively promises many benefits such as reducing unused transport capacity and ensuring timely delivery of cargo. This paper introduces an approach that combines user-friendly mobile devices with internet-connected sensors to deliver up-to-date, timely, and precise information about parcel volumes inside containers. In particular, we present (1) RCM, a mobile app for unique identification of containers, and (2) SNAP, a novel approach for employing internet-connected low-cost, off-the-shelf 3D scanners for capturing and analyzing actual cargo volumes. We have evaluated the accuracy of SNAP in controlled experiments indicating that cargo volume can be measured with high accuracy. We have further evaluated RCM together with\u00a0\u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Modellierung\u201999: Workshop der Gesellschaft f\u00fcr Informatik eV (GI), M\u00e4rz 1999 in Karlsruhe\n", "abstract": " Bei der Entwicklung von Software-und Informationssystemen werden verschiedene Aspekte von Struktur und Verhalten eines Systems modelliert. Dazu stehen unterschiedliche Modellierungssprachen zur Verf\u00fcgung. Das Fachgebiet Modellierung befa\u00dft sich mit derartigen Modellen sowie mit ihren Beziehungen untereinander und auch mit dem Proze\u00df der Modellerstellung. Der Workshop Modellierung wird 1999 zum zweiten Mal von sieben Fachgruppen der Gesellschaft f\u00fcr Informatik veranstaltet. Dieser Tagungsband enth\u00e4lt 11 Fachbeitr\u00e4ge, die repr\u00e4sentativ f\u00fcr die verschiedenen Aspekte des Themas Modellierung stehen. Zus\u00e4tzlich sind Diskussionsbeitr\u00e4ge zu drei sehr aktuellen Fragestellungen im Zusammenhang mit Modellierung enthalten.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Variabilit\u00e4t als eine eigenst\u00e4ndige Sicht auf Produktlinien\n", "abstract": " Die integrierte Dokumentation von Variabilit\u00e4t ist aus verschiedenen Gr\u00fcnden von Nachteil. Unter anderem ist die Variabilit\u00e4tsinformation \u00fcber verschiedene Modelle verstreut und Abh\u00e4ngigkeiten zwischen verschiedenen Artefakten (zB zwischen der Variabilit\u00e4t in den Use Cases und den Klassenmodellen) k\u00f6nnen nur schwierig dokumentiert und gepr\u00fcft werden. Ebenfalls kann die Variabilit\u00e4t nur mit Hilfe der", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Positive Effekte von Szenarien und Features in einem Softwarepraktikum.\n", "abstract": " In diesem Beitrag beschreiben wir die von uns beobachteten Probleme bei der Durchf\u00fchrung der Softwareentwicklungsaktivit\u00e4ten durch die Studierenden und pr\u00e4sentieren einen szenario-und featurebasierten Ansatz zur Verminderung dieser Probleme. Wir erl\u00e4utern die positiven Effekte von Szenarien und Features auf das SEP und belegen die Akzeptanz von Szenarien und Features anhand einer Umfrage unter den Teilnehmern.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Eine Methode f\u00fcr das Co-Design von Anforderungs-und Entwurfsartefakten.\n", "abstract": " Inhalt Page 1 (c) Prof. Dr. Klaus Pohl 1 Software Systems Engineering Institute for Computer Science and Business Information Systems (ICB) University of Duisburg-Essen, Germany www.sse.uni-due.de Eine Methode f\u00fcr das Co-Design von Anforderungs- und Entwurfsartefakten Klaus Pohl, Ernst Sikora Ernst Sikora \u2013 SE 2007, Hamburg \u00a9 Prof. Dr. K. Pohl \u2013 2/7 Inhalt \u25aa Motivation f\u00fcr Co-Design \u25aa Ziele des Co-Design \u25aa Abstraktionsebenen \u25aa Teilprozesse Page 2 (c) Prof. Dr. Klaus Pohl 2 Ernst Sikora \u2013 SE 2007, Hamburg \u00a9 Prof. Dr. K. Pohl \u2013 3/7 Motivation: Entwicklung innovativer softwareintensiver Systeme \u25aa Kritik am \u201cAbstract Design\u201d Paradigma aus Praxis und Forschung \u2013 Erst vollst\u00e4ndige, abstrakte Spezifikation, dann Entwurf nicht praktikabel \u25aa Stakeholder k\u00f6nnen ohne Architekturwissen keine detaillierten Anforderungen spezifizieren \u2013 Beispiel: Definition von Anforderungen bzgl. Netzwerksicherheit bedingt \u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Anforderungsbasierte Erkennung von Feature-Interaktionen in der Produktlinienentwicklung\n", "abstract": " Feature-Interaktionen sind unerw\u00fcnschte Wechselwirkungen zwischen Produktmerkmalen. Wegen der typischerweise sehr hohen Zahl an Applikationen, die aus den wiederverwendbaren Artefakten einer Produktlinie abgeleitet werden k\u00f6nnen, ist es nicht m\u00f6glich, alle potenziellen Applikationen einzeln auf Feature-Interaktionen hin zu analysieren. Der hier vorgestellte RAFINA-Ansatz zur anforderungsbasierten Erkennung von Feature-Interaktionen nutzt die Eigenschaften von Produktlinienvariabilit\u00e4t und von speziellen Feature-Interaktionen, um eine sehr kleine Teilmenge repr\u00e4sentativer Applikationen auszuw\u00e4hlen, die einen R\u00fcckschluss auf die Feature-Interaktionen der gesamten Produktlinie erlauben.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Dokumentation spezifischer Anforderungen im Application Requirements Engineering der Produktlinienentwicklung\n", "abstract": " Das Ziel eines Requirements Engineering Prozesses im Application Engineering einer Produktlinienentwicklung ist die Spezifikation eines Produktes unter Wiederverwendung von Anforderungen aus dem Domain Engineering. Die Dokumentation spezifischer Anforderungen, die nicht oder nicht komplett durch Wiederverwendung definiert werden k\u00f6nnen, f\u00fchrt zu speziellen Anforderungen an den Application Requirements Engineering (ARE) Prozess. Bisher gibt es keine Ans\u00e4tze, welche diese speziellen Prozessanforderungen an das ARE beschreiben. In diesem Beitrag erl\u00e4utern wir zun\u00e4chst Anforderungs- und Variabilit\u00e4tsdeltas, welche durch spezifische Anforderungen entstehen. Darauf aufbauend werden Prozessanforderungen an das ARE definiert. Zur Umsetzung der Prozessanforderungen wird die Definition eines produktspezifischen Variabilit\u00e4tsmodells vorgeschlagen, welches nur f\u00fcr das betrachtete Produkt g\u00fcltig ist. Das produktspezifische Variabilit\u00e4tsmodell dient als Basis f\u00fcr die weiteren Entwicklungsphasen im Application Engineering und f\u00fcr eine systematische R\u00fcckkopplung spezifischer Anforderungen an das Domain Engineering.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Software Engineering 2005\n", "abstract": " Software Engineering 2005 - Digitale Bibliothek - Gesellschaft f\u00fcr Informatik eV GI Logo GI Logo Login Digital Library All of DSpace Communities & Collections Titles Authors By Issue Date Subjects This Collection Titles Authors By Issue Date Subjects Toggle navigation Digital Library Gesellschaft f\u00fcr Informatik eV GI-DL English Deutsch English English Deutsch View Item DSpace Home Lecture Notes in Informatics Proceedings Software Engineering P064 - Software Engineering 2005 View Item DSpace Home Lecture Notes in Informatics Proceedings Software Engineering P064 - Software Engineering 2005 View Item Software Engineering 2005 Author: Liggesmeyer, Peter [DBLP] Pohl, Klaus [DBLP] Goedicke, Michael [DBLP] Citation BibTeX Unknown author (2005). Software Engineering 2005. In: Liggesmeyer, P., Pohl, K. & Goedicke, M. (Hrsg.), Bonn: Gesellschaft f\u00fcr Informatik eV. Haben Sie fehlerhafte Angaben \u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Application Requirements Engineering\n", "abstract": " The goal of application requirements engineering is to elicit and to document the requirements artefacts for a particular application and at the same time reuse, as much as possible, the domain requirements artefacts. The reuse of domain requirements artefacts for each application supports the overall goal of obtaining a high degree of domain artefact reuse.The sub-processes and artefacts closely related to the application requirements engineering sub-process are highlighted in Fig. 15-1. Application requirements engineering is related to product management, domain requirements engineering, and application design. Product management defines the major features of the applications to be developed. Domain requirements engineering creates the domain requirements artefacts, which are reused for the application under consideration. The application requirements engineering sub-process reuses the domain\u00a0\u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Sixth International Workshop on Requirements Engineering: Foundation for Software Quality (REFSQ\u201d 00)\n", "abstract": " The goal of the REFSQ workshop series is to improve the understanding of the relationship between requirements engineering and software quality. REFSQ especially aims at having intensive discussion provoked by brief presentations about solutions to known RE problems and shortcomings, innovative research ideas and research directions, industrial problem statements and generalisation from individual industrial experiences. For REFSQ\u201900 twelve full and three position papers were selected for presentation at the workshop out of over 25 submissions. Twenty-five participants from all over the world joined lively discussion at REFSQ\u201900 held in conjunction with CAiSE\u201900 in Stockholm, Sweden on June 5th and 6th 2000 (see www. sse. uni-essen. de for more information about REFSQ\u201900 and the REFSQ workshop series). The papers of the workshop as well as the summaries of the intensive discussions are\u00a0\u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Die drei Bereiche des kontinuierlichen Anforderungsmanagements\n", "abstract": " Unsere Anforderungen bleiben w\u00e4hrend eines Entwicklungsprojektes weitgehend stabil, da wir die eigentlichen Kundenprobleme systematisch ermittelt und priorisiert haben und fr\u00fchzeitig eine klare Produktvision und -abgrenzung existierte. Nat\u00fcrlich gibt es auch \u00c4nderungen. Diese werden in einem kontrollierten, f\u00fcr alle Beteiligten transparenten Prozess in die Umsetzung gef\u00fchrt. Das Projektteam kann sich auf die Detaillierung und Umsetzung der Anforderungen konzentrieren. Es existieren klare Auftraggeber-/Auftragnehmer-Strukturen zwischen Kunden, Produktmanagern und Entwicklung. Da wir ein kontinuierliches Risiko-und Umsetzungsmanagement verfolgen, kennen wir jeweils den aktuellen Projektstatus und k\u00f6nnen aktiv Problemen entgegenwirken. Das Verh\u00e4ltnis zwischen Entwicklung, Kunden und Anwendern ist gut, alle Gruppen arbeiten eng zusammen. a (ein zufriedener IT-Leiter)", "num_citations": "1\n", "authors": ["1447"]}
{"title": "REFSQ'97 workshop summary\n", "abstract": " The REFSQ (Requirements Engineering: Foundation for Software Quality) 1997 workshop was held in conjunction with CAiSE'97 in Barcelona, Spain on June 16th and 17th, 1997. It was organised by Eric Dubois, Andreas L. Opdahl and Klaus Pohl. In this workshop summary we, the organisers, will provide an overview of the workshop and of the presentations given, and present our subjective view of the various and fruitful discussions that took place.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Tool integration support for modeling in open-CAPE environments\n", "abstract": " The development of modeling tools within the context of future open computer-aided process engineering environments requires careful consideration of tool integration strategies such that the modeling process is optimally supported, and the resulting models achieve their full usage potential. We differentiate between five kinds of tool integration, describe the kinds of integration offered by existing process engineering environments, and thereby identify the major shortcomings of current approaches. To indicate a possible solution to these shortcomings we sketch the tool integration approach realized in PRO-ART/CE, a prototypical process engineering environment.", "num_citations": "1\n", "authors": ["1447"]}
{"title": "PRO-ART: Erfassung und Verwaltung von Anforderungshistorien\n", "abstract": " Die Nachvollziehbarkeit von Anforderungen (Requirements Traceability) ist eine essentielle Grundvoraussetzung f\u00fcr die Entwicklung hochwertiger Softwaresysteme. Hierbei wird allgemein zwischen der Nachvollziehbarkeit der Verwendung einer Anforderung (Post-Traceability oder Vorw\u00e4rts-Nachvollziehbarkeit) und der Entstehung einer Anforderung (Pre-Traceability oder R\u00fcckw\u00e4rts-Nachvollziehbarkeit) unterschieden. F\u00fcr die Unterst\u00fctzung von Pre-Traceability sind drei Fragen von zentraler Bedeutung:(1) Welche Informationen m\u00fcssen aufgezeichnet werden?(2) Wie werden diese Informationen strukturiert?(3) Wie werden die Informationen aufgezeichnet? In diesem Beitrag stellen wir einerseits drei Rahmenwerke vor, die generische Antworten auf diese Fragen geben. Andererseits skizzieren wir eine detaillierte Ausarbeitung dieser Rahmenwerke, auf der die Requirements Engineering Umgebung PRO-ART basiert. PRO-ART unterst\u00fctzt die Erfassung und Strukturierung von Trace-Informationen entlang der drei Dimensionen des Requirements Engineerings. Die Aufzeichnung der Daten in einem Trace-Repository erfolgt durch die Werkzeuge der Entwicklungsumgebung, die gem\u00e4\u00df einem neuen Ansatzes f\u00fcr proze\u00dfintegrierte Werkzeuge implementiert wurden. Wir berichten von unseren Erfahrungen mit PRO-ART 1.0, einer ersten prototypischen Implementierung unserer Ans\u00e4tze, dem daraus resultierenden Re-Design sowie der Re-Implementierung der Umgebung (PRO-ART 2.0).", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Structuring Variability in the Context of Embedded Systems during Software Engineering: Problem Statement\n", "abstract": " Structuring Variability in the Context of Embedded Systems during Software Engineering \u2013 Problem Statement Page 1 Structuring Variability in the Context of Embedded Systems during Software Engineering \u2013 Problem Statement Andr\u00e9 Heuer, Klaus Pohl VaMoS 2014 \u2013 Nice, France Page 2 Variable Software of Embedded Systems Context of Embedded Software Example for Context Variability Related Work Open Issues and Future Work Agenda 2 Page 3 Software of Embedded Systems \u25aa Todays embedded systems are often part of a system of systems \u25aa Main functionality of Embedded Systems is realized by software \u25aa Software is deployed in highlycomplex and safety critical environments\u2026 \u25aa \u2026and has a High interaction with environment by sensors and actuators cf. [Weyer & Pohl 2008] 3 Page 4 \u25aa Use of different hardware in supply chain of the embedded system \u25aa Use of a system (and software) at different OEMs \u2026", "num_citations": "1\n", "authors": ["1447"]}
{"title": "Einsatz von Modellen f\u00fcr das risikominimierende, anforderungsbasierte Testen von Softwaresystemen (ranTEST)\n", "abstract": " Testen ist die stichprobenartige Ausf\u00fchrung von Software zum Zweck der Fehlerfindung oder zum Nachweis bestimmter Qualit\u00e4tseigenschaften. Selbst dann, wenn m\u00f6glichst repr\u00e4sentative und fehlersensitive Testf\u00e4lle ausgew\u00e4hlt wurden, kann es im Feldeinsatz des Systems zu Fehlverhalten kommen. Die H\u00e4ufigkeit und die Folgeschwere des Auftretens von Fehlverhalten w\u00e4hrend des Betrachtungszeitraums bestimmen das mit dem Einsatz einer Software verbundene Risiko. Risikobasierte Testans\u00e4tze ber\u00fccksichtigen das mit dem Einsatz des Softwaresystems verbundene Risiko der Nichterf\u00fcllung der Qualit\u00e4tsanforderungen. Existierende Ans\u00e4tze zur Risikobestimmung nutzen Expertensch\u00e4tzungen zur Bestimmung der H\u00e4ufigkeit und Schwere von Softwarefehlverhalten. Eine \u00c4nderung der Schadenswahrscheinlichkeit w\u00e4hrend der Entwicklung (zB wenn Defekte in der Software korrigiert werden) erfordert eine\u00a0\u2026", "num_citations": "1\n", "authors": ["1447"]}