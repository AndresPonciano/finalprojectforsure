{"title": "A brief account of runtime verification\n", "abstract": " In this paper, a brief account of the field of runtime verification is given. Starting with a definition of runtime verification, a comparison to well-known verification techniques like model checking and testing is provided, and applications in which runtime verification brings out its distinguishing features are pointed out. Moreover, extensions of runtime verification such as monitor-oriented programming, and monitor-based runtime reflection are sketched and their similarities and differences are discussed. Finally, the use of runtime verification for contract enforcement is briefly pointed out.", "num_citations": "893\n", "authors": ["1138"]}
{"title": "Detecting malicious code by model checking\n", "abstract": " The ease of compiling malicious code from source code in higher programming languages has increased the volatility of malicious programs: The first appearance of a new worm in the wild is usually followed by modified versions in quick succession. As demonstrated by Christodorescu and Jha, however, classical detection software relies on static patterns, and is easily outsmarted. In this paper, we present a flexible method to detect malicious code patterns in executables by model checking. While model checking was originally developed to verify the correctness of systems against specifications, we argue that it lends itself equally well to the specification of malicious code patterns. To this end, we introduce the specification language CTPL (Computation Tree Predicate Logic) which extends the well-known logic CTL, and describe an efficient model checking algorithm. Our practical experiments\u00a0\u2026", "num_citations": "223\n", "authors": ["1138"]}
{"title": "OXPath: A language for scalable data extraction, automation, and crawling on the deep web\n", "abstract": " The evolution of the web has outpaced itself: A growing wealth of information and increasingly sophisticated interfaces necessitate automated processing, yet existing automation and data extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements for web data extraction, automation, and (focused) web crawling: (1) interact with sophisticated web application interfaces, (2) precisely capture the relevant data to be extracted, (3) scale with the number of visited pages, and (4) readily embed into existing web technologies. We introduce OXPath as an extension of XPath for interacting with web applications and extracting data thus revealed\u2014matching all the above requirements. OXPath\u2019s page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We experimentally validate the\u00a0\u2026", "num_citations": "110\n", "authors": ["1138"]}
{"title": "DIADEM: thousands of websites to a single database\n", "abstract": " The web is overflowing with implicitly structured data, spread over hundreds of thousands of sites, hidden deep behind search forms, or siloed in marketplaces, only accessible as HTML. Automatic extraction of structured data at the scale of thousands of websites has long proven elusive, despite its central role in the \"web of data\". Through an extensive evaluation spanning over 10000 web sites from multiple application domains, we show that automatic, yet accurate full-site extraction is no longer a distant dream. diadem is the first automatic full-site extraction system that is able to extract structured data from different domains at very high accuracy. It combines automated exploration of websites, identification of relevant data, and induction of exhaustive wrappers. Automating these components is the first challenge. diadem overcomes this challenge by combining phenomenological and ontological knowledge\u00a0\u2026", "num_citations": "91\n", "authors": ["1138"]}
{"title": "14 tools for test case generation\n", "abstract": " The preceding parts of this book have mainly dealt with test theory, aimed at improving the practical techniques which are applied by testers to enhance the quality of soft- and hardware systems. Only if these academic results can be efficiently and successfully transferred back to practice, they were worth the effort.", "num_citations": "76\n", "authors": ["1138"]}
{"title": "Model-based runtime analysis of distributed reactive systems\n", "abstract": " Reactive distributed systems have pervaded everyday life and objects, but often lack measures to ensure adequate behaviour in the presence of unforeseen events or even errors at runtime. As interactions and dependencies within distributed systems increase, the problem of detecting failures which depend on the exact situation and environment conditions they occur in grows. As a result, not only the detection of failures is increasingly difficult, but also the differentiation between the symptoms of a fault, and the actual fault itself, i.e., the cause of a problem. In this paper, we present a novel and efficient approach for analysing reactive distributed systems at runtime, in that we provide a framework for detecting failures as well as identifying their causes. Our approach is based upon monitoring safety-properties, specified in the linear time temporal logic LTL (respectively, TLTL) to automatically generate monitor\u00a0\u2026", "num_citations": "73\n", "authors": ["1138"]}
{"title": "Proactive detection of computer worms using model checking\n", "abstract": " Although recent estimates are speaking of 200,000 different viruses, worms, and Trojan horses, the majority of them are variants of previously existing malware. As these variants mostly differ in their binary representation rather than their functionality, they can be recognized by analyzing the program behavior, even though they are not covered by the signature databases of current antivirus tools. Proactive malware detectors mitigate this risk by detection procedures that use a single signature to detect whole classes of functionally related malware without signature updates. It is evident that the quality of proactive detection procedures depends on their ability to analyze the semantics of the binary. In this paper, we propose the use of model checking-a well-established software verification technique-for proactive malware detection. We describe a tool that extracts an annotated control flow graph from the binary and\u00a0\u2026", "num_citations": "63\n", "authors": ["1138"]}
{"title": "DIADEM: domain-centric, intelligent, automated data extraction methodology\n", "abstract": " Search engines are the sinews of the web. These sinews have become strained, however: Where the web's function once was a mix of library and yellow pages, it has become the central marketplace for information of almost any kind. We search more and more for objects with specific characteristics, a car with a certain mileage, an affordable apartment close to a good school, or the latest accessory for our phones. Search engines all too often fail to provide reasonable answers, making us sift through dozens of websites with thousands of offers--never to be sure a better offer isn't just around the corner. What search engines are missing is understanding of the objects and their attributes published on websites.", "num_citations": "59\n", "authors": ["1138"]}
{"title": "CBMC-GC: an ANSI C compiler for secure two-party computations\n", "abstract": " Secure two-party computation (STC) is a computer security paradigm where two parties can jointly evaluate a program with sensitive input data, provided in parts from both parties. By the security guarantees of STC, neither party can learn any information on the other party\u2019s input while performing the STC task. For a long time thought to be impractical, until recently, STC has only been implemented with domain-specific languages or hand-crafted Boolean circuits for specific computations. Our open-source compiler CBMC-GC is the first ANSI C compiler for STC. It turns C programs into Boolean circuits that fit the requirements of garbled circuits, a generic STC approach based on circuits. Here, the size of the resulting circuits plays a crucial role since each STC step involves encryption and network transfer and is therefore extremely slow when compared to computations performed on modern hardware\u00a0\u2026", "num_citations": "51\n", "authors": ["1138"]}
{"title": "Oxpath: A language for scalable, memory-efficient data extraction from web applications\n", "abstract": " The evolution of the web has outpaced itself: The growing wealth of information and the increasing sophistication of interfaces necessitate automated processing. Web automation and extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements of web extraction: (1) Interact with sophisticated web application interfaces, (2) Precisely capture the relevant data for most web extraction tasks, (3) Scale with the number of visited pages, and (4) Readily embed into existing web technologies. We introduce OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed. It addresses all the above requirements. OXPath's page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We validate experimentally the theoretical complexity and\u00a0\u2026", "num_citations": "44\n", "authors": ["1138"]}
{"title": "Opal: automated form understanding for the deep web\n", "abstract": " Forms are our gates to the web. They enable us to access the deep content of web sites. Automatic form understanding unlocks this content for applications ranging from crawlers to meta-search engines and is essential for improving usability and accessibility of the web. Form understanding has received surprisingly little attention other than as component in specific applications such as crawlers. No comprehensive approach to form understanding exists and previous works disagree even in the definition of the problem. In this paper, we present OPAL, the first comprehensive approach to form understanding. We identify form labeling and form interpretation as the two main tasks involved in form understanding. On both problems OPAL pushes the state of the art: For form labeling, it combines signals from the text, structure, and visual rendering of a web page, yielding robust characterisations of common design\u00a0\u2026", "num_citations": "34\n", "authors": ["1138"]}
{"title": "COLA--The component language\n", "abstract": " In this paper we introduce the component language COLA for the design and development of embedded systems. We present the formal syntax and semantics of COLA which is based upon synchronous dataflow. Utilizing the abstraction provided by this paradigm, the designer is freed from implementation details and is able to focus on the core-functionality to be modeled and implemented.\\\\Due to the well-founded semantics of the language, it is possible to establish an integrated development process, the...\u00bb", "num_citations": "34\n", "authors": ["1138"]}
{"title": "The ontological key: automatically understanding and integrating forms to access the deep Web\n", "abstract": " Forms are our gates to the Web. They enable us to access the deep content of Web sites. Automatic form understanding provides applications, ranging from crawlers over meta-search engines to service integrators, with a key to this content. Yet, it has received little attention other than as component in specific applications such as crawlers or meta-search engines. No comprehensive approach to form understanding exists, let alone one that produces rich models for semantic services or integration with linked open data. In this paper, we present opal, the first comprehensive approach to form understanding and integration. We identify form labeling and form interpretation as the two main tasks involved in form understanding. On both problems, opal advances the state of the art: For form labeling, it combines features from the text, structure, and visual rendering of a Web page. In extensive experiments on the ICQ\u00a0\u2026", "num_citations": "27\n", "authors": ["1138"]}
{"title": "Towards Formal Semantics for ODRL.\n", "abstract": " We give a brief overview of a new way to model the semantics of ODRL permissions in a formal manner by using finite-automata like structures. The constructed automata capture the sequence of actions that a user is allowed to perform according to a specific permission. In contrast to previous approaches, our semantics is able to model sell and lend permissions.", "num_citations": "23\n", "authors": ["1138"]}
{"title": "Impartial anticipation in runtime-verification\n", "abstract": " In this paper, a uniform approach for synthesizing monitors checking correctness properties specified in linear-time logics at runtime is provided. Therefore, a generic three-valued semantics is introduced reflecting the idea that prefixes of infinite computations are checked. Then a conceptual framework to synthesize monitors from a logical specification to check an execution incrementally is established, with special focus on resorting to the automata-theoretic approach. The merits of the presented framework are shown by providing monitor synthesis approaches for a variety of different logics such as LTL, the linear-time \u03bc-calculus, PLTL                 mod, SiS, and RLTL.", "num_citations": "22\n", "authors": ["1138"]}
{"title": "Real understanding of real estate forms\n", "abstract": " Finding an apartment is a lengthy and tedious process. Once decided, one can never be sure not to have missed an even better offer which would have been just one click away. Form understanding is key to automatically access and process all the relevant---and nowadays readily available---data.", "num_citations": "19\n", "authors": ["1138"]}
{"title": "Runtime verification revisited\n", "abstract": " In this paper, we address a typical obstacle in runtime verification of linear temporal logic (LTL) formulae: standard models of linear temporal logic are infinite traces, whereas run-time verification has to deal with only finite system behaviours. This problem is usually addressed by defining an LTL semantics for finite traces, which, however, does usually not fit well to the infinite trace semantics. We define a 3-valued semantics (true, false, inconclusive) for LTL on finite traces that resemb...\u00bb", "num_citations": "19\n", "authors": ["1138"]}
{"title": "Little knowledge rules the web: Domain-centric result page extraction\n", "abstract": " Web extraction is the task of turning unstructured HTML into structured data. Previous approaches rely exclusively on detecting repeated structures in result pages. These approaches trade intensive user interaction for precision. In this paper, we introduce the Amber (\u201cAdaptable Model-based Extraction of Result Pages\u201d) system that replaces the human interaction with a domain ontology applicable to all sites of a domain. It models domain knowledge about (1) records and attributes of the domain,(2) low-level (textual) representations of these concepts, and (3) constraints linking representations to records and attributes. Parametrized with these constraints, otherwise domain-independent heuristics exploit the repeated structure of result pages to derive attributes and records. Amber is implemented in logical rules to allow an explicit formulation of the heuristics and easy adaptation to different domains. We apply\u00a0\u2026", "num_citations": "18\n", "authors": ["1138"]}
{"title": "Effective web scraping with oxpath\n", "abstract": " Even in the third decade of the Web, scraping web sites remains a challenging task: Most scraping programs are still developed as ad-hoc solutions using a complex stack of languages and tools. Where comprehensive extraction solutions exist, they are expensive, heavyweight, and proprietary.", "num_citations": "14\n", "authors": ["1138"]}
{"title": "Turn the page: automated traversal of paginated websites\n", "abstract": " Content-intensive web sites, such as Google or Amazon, paginate their results to accommodate limited screen sizes. Thus, human users and automatic tools alike have to traverse the pagination links when they crawl the site, extract data, or automate common tasks, where these applications require access to the entire result set. Previous approaches, as well as existing crawlers and automation tools, rely on simple heuristics (e.g., considering only the link text), falling back to an exhaustive exploration of the site where those heuristics fail. In particular, focused crawlers and data extraction systems target only fractions of the individual pages of a given site, rendering a highly accurate identification of pagination links essential to avoid the exhaustive exploration of irrelevant pages.               We identify pagination links in a wide range of domains and sites with near perfect accuracy (99%). We obtain these results with a\u00a0\u2026", "num_citations": "14\n", "authors": ["1138"]}
{"title": "Visual OXPath: robust wrapping by example\n", "abstract": " Good examples are hard to find, particularly in wrapper induction: Picking even one wrong example can spell disaster by yielding overgeneralized or overspecialized wrappers. Such wrappers extract data with low precision or recall, unless adjusted by human experts at significant cost.", "num_citations": "14\n", "authors": ["1138"]}
{"title": "How the Minotaur turned into Ariadne: ontologies in Web data extraction\n", "abstract": " Humans require automated support to profit from the wealth of data nowadays available on the web. To that end, the linked open data initiative and others have been asking data providers to publish structured, semantically annotated data. Small data providers, such as most UK real-estate agencies, however, are overburdened with this task\u2014often just starting to move from simple, table- or list-like directories to web applications with rich interfaces.               We argue that fully automated extraction of structured data can help resolve this dilemma. Ironically, automated data extraction has seen a recent revival thanks to ontologies and linked open data to guide data extraction. First results from the DIADEM project illustrate that high quality, fully automated data extraction at a web scale is possible, if we combine domain ontologies with a phenomenology describing the representation of domain concepts. We\u00a0\u2026", "num_citations": "13\n", "authors": ["1138"]}
{"title": "Robust and noise resistant wrapper induction\n", "abstract": " Wrapper induction is the problem of automatically inferring a query from annotated web pages of the same template. This query should not only select the annotated content accurately but also other content following the same template. Beyond accurately matching the template, we consider two additional requirements:(1) wrappers should be robust against a large class of changes to the web pages, and (2) the induction process should be noise resistant, ie, tolerate slightly erroneous (eg, machine generated) samples. Key to our approach is a query language that is powerful enough to permit accurate selection, but limited enough to force noisy samples to be generalized into wrappers that select the likely intended items. We introduce such a language as subset of XPATH and show that even for such a restricted language, inducing optimal queries according to a suitable scoring is infeasible. Nevertheless, our\u00a0\u2026", "num_citations": "12\n", "authors": ["1138"]}
{"title": "Runtime Reflection: Dynamic model-based analyis of component-based distributed embedded systems\n", "abstract": " Distributed embedded systems have pervaded the automotive domain, but often still lack measures to ensure adequate behaviour in the presence of unforeseen events, or even errors at runtime. As interactions and dependencies within distributed automotive systems increase, the problem of detecting failures which depend on the exact situation and environment conditions they occur in grows. As a result, not only the detection of failures is increasingly difficult, but also the differentiation between the symptoms of a fault, and the actual fault itself, ie, the cause of a problem. In this paper, we present a novel and efficient approach built around the notion of a software component similar to AUTOSAR, for dynamically analysing distributed embedded systems in the testing phase or even in standard operation, in that we provide a framework for detecting failures as well as identifying their causes. Our approach is based upon monitoring safety properties, specified in a language that allows to express dynamic system properties. For such specifications so-called monitor components are generated automatically to detect violations of software components. Based on the results of the monitors, a dedicated diagnosis is then performed in order to identify explanations for the misbehaviour of a system. These may be used to store detailed error logs, or to trigger recovery measures.", "num_citations": "12\n", "authors": ["1138"]}
{"title": "Automatically learning gazetteers from the deep Web\n", "abstract": " Wrapper induction faces a dilemma: To reach web scale, it requires automatically generated examples, but to produce accurate results, these examples must have the quality of human annotations. We resolve this conflict with AMBER, a system for fully automated data extraction from result pages. In contrast to previous approaches, AMBER employs domain specific gazetteers to discern basic domain attributes on a page, and leverages repeated occurrences of similar attributes to group related attributes into records rather than relying on the noisy structure of the DOM. With this approach AMBER is able to identify records and their attributes with almost perfect accuracy (> 98%) on a large sample of websites. To make such an approach feasible at scale, AMBER automatically learns domain gazetteers from a small seed set. In this demonstration, we show how AMBER uses the repeated structure of records on deep web\u00a0\u2026", "num_citations": "11\n", "authors": ["1138"]}
{"title": "Do we need sinks?\n", "abstract": " 2 Waste Management & Research 30 (1) via incineration of nearly all sorts of waste in cement kilns and firing of ceramic materials mixed with waste, and industries preparing concrete that contains waste materials. Some of these processes are well suited as sinks for organic materials because they mineralize carbon-containing substances completely. Unfortunately, inorganic materials cannot be \u2018removed\u2019in the same way as organics, and they may accumulate. Inevitably constructions will end their useful life after decades of service and become demolition waste.(Virtually all types of structures have economic and/or useful lives of 40 to 100 years. In the big picture of waste management, this cannot be considered \u2018permanent\u2019.) The recycling of building materials and road surfaces is bound to increase in all societies due to the growth in end-of-life stock, increasing shortages of virgin raw materials, and lack of space\u00a0\u2026", "num_citations": "11\n", "authors": ["1138"]}
{"title": "AMBER: Automatic Supervision for Multi-Attribute Extraction\n", "abstract": " The extraction of multi-attribute objects from the deep web is the bridge between the unstructured web and structured data. Existing approaches either induce wrappers from a set of human-annotated pages or leverage repeated structures on the page without supervision. What the former lack in automation, the latter lack in accuracy. Thus accurate, automatic multi-attribute object extraction has remained an open challenge. AMBER overcomes both limitations through mutual supervision between the repeated structure and automatically produced annotations. Previous approaches based on automatic annotations have suffered from low quality due to the inherent noise in the annotations and have attempted to compensate by exploring multiple candidate wrappers. In contrast, AMBER compensates for this noise by integrating repeated structure analysis with annotation-based induction: The repeated structure limits the search space for wrapper induction, and conversely, annotations allow the repeated structure analysis to distinguish noise from relevant data. Both, low recall and low precision in the annotations are mitigated to achieve almost human quality (more than 98 percent) multi-attribute object extraction. To achieve this accuracy, AMBER needs to be trained once for an entire domain. AMBER bootstraps its training from a small, possibly noisy set of attribute instances and a few unannotated sites of the domain.", "num_citations": "9\n", "authors": ["1138"]}
{"title": "Taking the OXPath down the deep web\n", "abstract": " Although deep web analysis has been studied extensively, there is no succinct formalism to describe user interactions with AJAX-enabled web applications.", "num_citations": "8\n", "authors": ["1138"]}
{"title": "The good\n", "abstract": " CiteSeerX \u2014 The good Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA The good Cached Download as a PDF Download Links [www4.in.tum.de] Save to List Add to Collection Correct Errors Monitor Changes by Andreas Bauer , Martin Leucker , Christian Schallhart Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract the bad, and the ugly, but how ugly is ugly? Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State University \u2026", "num_citations": "7\n", "authors": ["1138"]}
{"title": "Unified Verbalization for Speech Recognition & Synthesis Across Languages.\n", "abstract": " We describe a new approach to converting written tokens to their spoken form, which can be shared by automatic speech recognition (ASR) and text-to-speech synthesis (TTS) systems. Both ASR and TTS need to map from the written to the spoken domain, and we present an approach that enables us to share verbalization grammars between the two systems while exploiting linguistic commonalities to provide simple default verbalizations. We also describe improvements to an induction system for number names grammars. Between these shared ASR/TTS verbalizers and the improved induction system for number names grammars, we achieve significant gains in development time and scalability across languages.", "num_citations": "6\n", "authors": ["1138"]}
{"title": "Bitemporal complex event processing of web event advertisements\n", "abstract": " The web is the largest bulletin board of the world. Events of all types, from flight arrivals to business meetings, are announced on this board. Tracking and reacting to such event announcements, however, is a tedious manual task, only slightly alleviated by email or similar notifications. Announcements are published with human readers in mind, and updates or delayed announcements are frequent. These characteristics have hampered attempts at automatic tracking.                                        PeaCE provides the first integrated framework for event processing on top of web event ads. Given a schema of events to be tracked, the framework populates this schema through compact wrappers for event announcement sources. These wrappers produce events including updates and retractions. PeaCE then queries these events to detect complex events, often combining announcements from multiple sources. To deal\u00a0\u2026", "num_citations": "6\n", "authors": ["1138"]}
{"title": "Opal: a passe-partout for web forms\n", "abstract": " Web forms are the interfaces of the deep web. Though modern web browsers provide facilities to assist in form filling, this assistance is limited to prior form fillings or keyword matching. Automatic form understanding enables a broad range of applications, including crawlers, meta-search engines, and usability and accessibility support for enhanced web browsing. In this demonstration, we use a novel form understanding approach, OPAL, to assist in form filling even for complex, previously unknown forms. OPAL associates form labels to fields by analyzing structural properties in the HTML encoding and visual features of the page rendering. OPAL interprets this labeling and classifies the fields according to a given domain ontology. The combination of these two properties, allows OPAL to deal effectively with many forms outside of the grasp of existing form filling techniques. In the UK real estate domain, OPAL\u00a0\u2026", "num_citations": "6\n", "authors": ["1138"]}
{"title": "Forms form patterns: reusable form understanding\n", "abstract": " Forms are our gates to the web. They enable us to access the deep content of web sites. Automatic form understanding unlocks this content for applications ranging from crawlers to meta-search engines and is essential for improving usability and accessibility of the web. Form understanding has received surprisingly little attention other than as component in specific applications such as crawlers. No comprehensive approach to form understanding exists and previous works disagree even in the definition of the problem. In this paper, we present OPAL, the first comprehensive approach to form understanding. We identify form labeling and form interpretation as the two main tasks involved in form understanding. On both problems OPAL pushes the state of the art: For form labeling, it combines signals from the text, structure, and visual rendering of a web page, yielding robust characterisations of common design patterns. In extensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern web forms OPAL outperforms previous approaches by a significant margin. For form interpretation, we introduce a template language to describe frequent form patterns. These two parts of OPAL combined yield form understanding with near perfect accuracy (> 98%).", "num_citations": "6\n", "authors": ["1138"]}
{"title": "OXPath: little language, little memory, great value\n", "abstract": " Data about everything is readily available on the web-but often only accessible through elaborate user interactions. For automated decision support, extracting that data is essential, but infeasible with existing heavy-weight data extraction systems. In this demonstration, we present OXPath, a novel approach to web extraction, with a system that supports informed job selection and integrates information from several different web sites. By carefully extending XPath, OXPath exploits its familiarity and provides a light-weight interface, which is easy to use and embed. We highlight how OXPath guarantees optimal page buffering, storing only a constant number of pages for non-recursive queries.", "num_citations": "6\n", "authors": ["1138"]}
{"title": "New Challenges in the Development of Critical Embedded Systems\u2014An \u201caeromotive\u201d Perspective\n", "abstract": " During the last decades, embedded systems have become increasingly important in highly safety-critical areas such as power plants, medical equipment, cars, and aeroplanes. The automotive and avionics domains are prominent examples of classical engineering disciplines where conflicts between costs, short product cycles and legal requirements concerning dependability, robustness, security, carbon footprint and spatial demands have become a pressing problem.", "num_citations": "6\n", "authors": ["1138"]}
{"title": "Ensuring Media Integrity on Third-Party Infrastructures\n", "abstract": " In many heterogeneous networked applications the integrity of multimedia data plays an essential role, but is not directly supported by the application. In this paper, we propose a method which enables an individual user to detect tampering with a multimedia file without changing the software application provided by the third party. Our method is based on a combination of cryptographic signatures and fragile watermarks, i.e., watermarks that are destroyed by illegitimate tampering. We show that the proposed system is provably secure under standard cryptographic assumptions.", "num_citations": "6\n", "authors": ["1138"]}
{"title": "Verification across intellectual property boundaries\n", "abstract": " In many industries, the importance of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers\u2019 source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This article presents an approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the \u201camanat\u201d) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We argue\u00a0\u2026", "num_citations": "5\n", "authors": ["1138"]}
{"title": "\u201cShow-Me\u201d\u2014Ein ambientes Informationssystem zur Reduktion des Wasserverbrauchs\n", "abstract": " Der weltweite Wasserverbrauch steigt stetig, w\u00e4hrend die existierenden S\u00fc\u00dfwasserressourcen immer geringer werden. Aus diesem Grund ist es unerl\u00e4sslich, sich auch \u00fcber den t\u00e4glichen Wasserverbrauch Gedanken zu machen und neuartige L\u00f6sungen in diesem Bereich anzuwenden, um insgesamt zu einer Verbrauchsreduktion beizutragen. Ein ausgezeichneter Ansatzpunkt ist dabei die t\u00e4gliche Dusche, die f\u00fcr eine Vielzahl von Menschen ein wichtiges Ritual darstellt. Je nach Badarmatur oder Duschgewohnheit wird so jeden Tag eine gro\u00dfe Menge an Wasser \u2014 meist sogar unbewusst \u2014 verbraucht. Die Dusche birgt daher ein dementsprechendes Einsparungspotential. Um nachhaltiges Verhalten zu f\u00f6rdern und eine permanente Verhaltens\u00e4nderung herbeizuf\u00fchren, hat sich Feedback laut diversen Studien aus unterschiedlichen Bereichen bew\u00e4hrt. Eine Erweiterung des Feedbacks in Form von subtilem\u00a0\u2026", "num_citations": "5\n", "authors": ["1138"]}
{"title": "The good, the bad, and the ugly-but how ugly is ugly?\n", "abstract": " When monitoring a system wrt. a property defined in a temporal logic such as LTL, a major concern is to settle with an adequate interpretation of observable system events; that is, models of temporal logic formulae are usually infinite words of events, whereas at runtime only finite but incrementally expanding prefixes are available.\\\\In this work, we review LTL-derived logics for finite traces from a runtimeverification perspective. In doing so, we establish four maxims to be satisfied by any...\u00bb", "num_citations": "5\n", "authors": ["1138"]}
{"title": "Model-based runtime analysis of distributed reactive systems\n", "abstract": " As interactions and dependencies within distributed reactive systems increase, the problem of detecting failures which depend on the exact situation and environmental conditions they occur in grows. As a result, not only the detection of failures is increasingly difficult, but also the differentiation between the symptoms of a fault, and the actual fault itself, ie, the cause of a failure. This thesis proposes an efficient approach for the analysis of distributed reactive systems at run...\u00bb", "num_citations": "5\n", "authors": ["1138"]}
{"title": "Verification across intellectual property boundaries\n", "abstract": " In many industries, the share of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers\u2019 source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This paper presents a new approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the \u201camanat\u201d) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We\u00a0\u2026", "num_citations": "4\n", "authors": ["1138"]}
{"title": "Eager: extending automatically gazetteers for entity recognition\n", "abstract": " Key to named entity recognition, the manual gazetteering of entity lists is a costly, errorprone process that often yields results that are incomplete and suffer from sampling bias. Exploiting current sources of structured information, we propose a novel method for extending minimal seed lists into complete gazetteers. Like previous approaches, we value WIKIPEDIA as a huge, well-curated, and relatively unbiased source of entities. However, in contrast to previous work, we exploit not only its content, but also its structure, as exposed in DBPEDIA. We extend gazetteers through Wikipedia categories, carefully limiting the impact of noisy categorizations. The resulting gazetteers easily outperform previous approaches on named entity recognition.", "num_citations": "3\n", "authors": ["1138"]}
{"title": "Visualizing Solutions with Viewers.\n", "abstract": " Visualization can be a powerful aid for learning a programming language. It may be used to reinforce central language concepts. In the context of Prolog and CLP-languages, however, most approaches to visualization aim at procedural aspects. Instead of explaining what a relation describes, visualization is used to animate procedural machinery. In this paper we present approaches to visualizing aspects of Prolog programs that try to avoid unnecessary and irritating procedural details. Answer substitutions are visualized with the help of so called viewers. Some procedural aspects are explained with animations. The viewers have been integrated into a side-effect free programming environment and are used in introductory Prolog and CLP courses. The didactical impact of our approaches is discussed.", "num_citations": "3\n", "authors": ["1138"]}
{"title": "Data-Driven Parametric Text Normalization: Rapidly Scaling Finite-State Transduction Verbalizers to New Languages\n", "abstract": " This paper presents a methodology for rapidly generating FST-based verbalizers for ASR and TTS systems by efficiently sourcing language-specific data. We describe a questionnaire which collects the necessary data to bootstrap the number grammar induction system and parameterize the verbalizer templates described in Ritchie et al.(2019), and a machine-readable data store which allows the data collected through the questionnaire to be supplemented by additional data from other sources. This system allows us to rapidly scale technologies such as ASR and TTS to more languages, including low-resource languages.", "num_citations": "2\n", "authors": ["1138"]}
{"title": "Frugal Paradigm Completion\n", "abstract": " Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose Frugal Paradigm Completion, an approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, our Frugal Paradigm Completion approach reduces manual labor by 16-63% and is the most robust to typological variation.", "num_citations": "2\n", "authors": ["1138"]}
{"title": "PeaCE-Ful Web Event Extraction and Processing as Bitemporal Mutable Events\n", "abstract": " The web is the largest bulletin board of the world. Events of all types, from flight arrivals to business meetings, are announced on this board. Tracking and reacting to such event announcements, however, is a tedious manual task, only slightly alleviated by email or similar notifications. Announcements are published with human readers in mind, and updates or delayed announcements are frequent. These characteristics have hampered attempts at automatic tracking. PeaCE provides the first integrated framework for event processing on top of web event ads, consisting of event extraction, complex event processing, and action execution in response to these events. Given a schema of the events to be tracked, the framework populates this schema by extracting events from announcement sources. This extraction is performed by little programs called wrappers that produce the events including updates and retractions\u00a0\u2026", "num_citations": "2\n", "authors": ["1138"]}
{"title": "Big Data\n", "abstract": " This volume contains the papers presented at BNCOD 2013: 29th British National Conference on Databases held during July 7\u20139, 2013, in Oxford. The BNCOD Conference is a venue for the presentation and discussion of research papers on a broad range of topics related to data-centric computation. For some years, every edition of BNCOD has centered around a main theme, acting as a focal point for keynote addresses, tutorials, and research papers. The theme of BNCOD 2013 is Big Data. It encompases a growing need to manage data that is too big, too fast, or too hard for the existing technology. This year, BNCOD attracted 42 complete submissions from 14 different African, European, South and North American countries. Each submission was reviewed by three Program Committee members. The committee decided to accept 20 papers on such topics as query and update processing, relational storage\u00a0\u2026", "num_citations": "2\n", "authors": ["1138"]}
{"title": "DIADEM: Domains to Databases\n", "abstract": " What if you could turn all websites of an entire domain into a single database? Imagine all real estate offers, all airline flights, or all your local restaurants\u2019 menus automatically collected from hundreds or thousands of agencies, travel agencies, or restaurants, presented as a single homogeneous dataset.               Historically, this has required tremendous effort by the data providers and whoever is collecting the data: Vertical search engines aggregate offers through specific interfaces which provide suitably structured data. The semantic web vision replaces the specific interfaces with a single one, but still requires providers to publish structured data.               Attempts to turn human-oriented HTML interfaces back into their underlying databases have largely failed due to the variability of web sources. In this paper, we demonstrate that this is about to change: The availability of comprehensive entity recognition\u00a0\u2026", "num_citations": "2\n", "authors": ["1138"]}
{"title": "PeaCE-Ful Web Event Extraction and Processing\n", "abstract": " PeaCE, our proposed tool, integrates complex event processing and web extraction into a unified framework to handle web event advertisements and to run a notification service atop. Its bitemporal schemata distinguish occurrence and detection time, enabling PeaCE to deal with updates and delayed announcements, as often occurring on the web. To consolidate the arising event streams, PeaCE combines simple events into complex ones. Depending on their occurrence and detection time, these complex events trigger actions to be executed. We demonstrate PeaCE\u2019s capabilities with a business trip scenario, involving as raw events business trips, flight bookings, scheduled flights, and flight arrivals and departures. These events are scrapped from the web and combined into complex events, triggering actions to be executed, such as updating facebook status messages. Our demonstrator records and reruns\u00a0\u2026", "num_citations": "1\n", "authors": ["1138"]}
{"title": "OXPath: Everyone can Automate the Web!\n", "abstract": " Web data easily accessible to everyone is the Holy Grail 2.0. Scientists need data to study, eg, how people interact on web social networks, whereas web companies use profiling data to target online ads or improve search results, and quantitative analyst examine streams of events to predict market variations. We all face daily tasks (eg, planning holidays or searching for a new camera), for which web data (eg, reviews) plays an important role. In principle, all the necessary information is readily available on some web page, yet manually accessing, extracting, and aggregating that information is often infeasible due to the number of different sites and the size of the involved data. This creates a new divide in datadriven research and analysis between governments or large, websavvy companies that can exploit web data at scale and most other entities or persons that do not have that ability. Web data extraction addresses the problem of turning data accessible through existing, human-oriented interfaces, into structured data. For instance, each gray span HTML element with CSS class source on Google News should be recognized as news source. However existing tools for web data extraction are either research prototypes not fit for everyday users or very expensive, commercial applications that require significant resources for large scale data extraction. Furthermore, they are usually not designed with end users in mind, as commercial data extraction is primarily offered as a service these days. Data extraction tools are also quickly outpaced by the growth and change in web technologies.", "num_citations": "1\n", "authors": ["1138"]}
{"title": "Transaction processing for clustered virtual environments\n", "abstract": " This paper introduces Massively Multi-Player Online Role Games (MMORGS) which are currently a main focus of the gaming industry. MMORGS are Networked Virtual Environments (NVES) where a player can navigate his or her character through a large game-world which is populated by thousands of other players and non player characters. Based the problems arising in the context of MMORGS, we will motivate the development of a domain-independent middleware to support MMORG in particular and networked NVES in general. Finally, we will introduce APEIRON, a middleware which is based on a flexible transaction processing framework. APEIRON is designed to serve as basis for the next generation MMORG.", "num_citations": "1\n", "authors": ["1138"]}