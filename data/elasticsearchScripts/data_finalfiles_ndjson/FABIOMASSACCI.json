{"title": "A survey of autonomic communications\n", "abstract": " Autonomic communications seek to improve the ability of network and services to cope with unpredicted change, including changes in topology, load, task, the physical and logical characteristics of the networks that can be accessed, and so forth. Broad-ranging autonomic solutions require designers to account for a range of end-to-end issues affecting programming models, network and contextual modeling and reasoning, decentralised algorithms, trust acquisition and maintenance---issues whose solutions may draw on approaches and results from a surprisingly broad range of disciplines. We survey the current state of autonomic communications research and identify significant emerging trends and techniques.", "num_citations": "898\n", "authors": ["168"]}
{"title": "Logical cryptanalysis as a SAT problem\n", "abstract": " Cryptographic algorithms play a key role in computer security and the formal analysis of their robustness is of utmost importance. Yet, logic and automated reasoning tools are seldom used in the analysis of a cipher, and thus one cannot often get the desired formal assurance that the cipher is free from unwanted properties that may weaken its strength.               In this paper, we claim that one can feasibly encode the low-level properties of state-of-the-art cryptographic algorithms as SAT problems and then use efficient automated theorem-proving systems and SAT-solvers for reasoning about them. We call this approach logical cryptanalysis.               In this framework, for instance, finding a model for a formula encoding an algorithm is equivalent to finding a key with a cryptanalytic attack. Other important properties, such as cipher integrity or algebraic closure, can also be captured as SAT problems or as\u00a0\u2026", "num_citations": "218\n", "authors": ["168"]}
{"title": "EXPTIME tableaux for ALC\n", "abstract": " The last years have seen two major advances in Knowledge Representation and Reasoning. First, many interesting problems (ranging from Semi-structured Data to Linguistics) were shown to be expressible in logics whose main deductive problems are EXPtime-complete. Second, experiments in automated reasoning have substantially broadened the meaning of \u201cpractical tractability\u201d. Instances of realistic size for Pspace-complete problems are now within reach for implemented systems. Still, there is a gap between the reasoning services needed by the expressive logics mentioned above and those provided by the current systems. Indeed, the algorithms based on tree-automata, which are used to prove EXPtime-completeness, require exponential time and space even in simple cases. On the other hand, current algorithms based on tableau methods can take advantage of such cases, but require double\u00a0\u2026", "num_citations": "200\n", "authors": ["168"]}
{"title": "Comparing vulnerability severity and exploits using case-control studies\n", "abstract": " (U.S.) Rule-based policies for mitigating software risk suggest using the CVSS score to measure the risk of an individual vulnerability and act accordingly. A key issue is whether the \u2018danger\u2019 score does actually match the risk of exploitation in the wild, and if and how such a score could be improved. To address this question, we propose using a case-control study methodology similar to the procedure used to link lung cancer and smoking in the 1950s. A case-control study allows the researcher to draw conclusions on the relation between some risk factor (e.g., smoking) and an effect (e.g., cancer) by looking backward at the cases (e.g., patients) and comparing them with controls (e.g., randomly selected patients with similar characteristics). The methodology allows us to quantify the risk reduction achievable by acting on the risk factor. We illustrate the methodology by using publicly available data on vulnerabilities\u00a0\u2026", "num_citations": "132\n", "authors": ["168"]}
{"title": "Strongly analytic tableaux for normal modal logics\n", "abstract": " A strong analytic tableau calculus is presentend for the most common normal modal logics. The method combines the advantages of both sequent-like tableaux and prefixed tableaux. Proper rules are used, instead of complex closure operations for the accessibility relation, while non determinism and cut rules, used by sequent-like tableaux, are totally eliminated. A strong completeness theorem without cut is also given for symmetric and euclidean logics. The system gains the same modularity of Hilbert-style formulations, where the addition or deletion of rules is the way to change logic. Since each rule has to consider only adjacent possible worlds, the calculus also gains efficiency. Moreover, the rules satisfy the strong Church Rosser property and can thus be fully parallelized. Termination properties and a general algorithm are devised. The propositional modal logics thus treated are K, D, T, KB, K4, K5, K45\u00a0\u2026", "num_citations": "129\n", "authors": ["168"]}
{"title": "An access control framework for business processes for web services\n", "abstract": " Business Processes for Web Services are the new paradigm for the lightweight integration of business from different enterprises. Whereas the security and access control policies for basic web services and distributed systems are well studied and almost standardized, there is not yet a comprehensive proposal for an access control architecture for business processes. The major issue is that a business process describe complex services that cross organizational boundaries and are provided by entities that see each other as just partners and nothing else. This calls for a number of differences with traditional aspects of access control architectures such as\u2022 credential vs classical user-based access control,\u2022 interactive and partner-based vs one-server-gathers-all requests of credentials from clients,\u2022 controlled disclosure of information vs all-or-nothing access control decisions,\u2022 abducing missing credentials for fulfilling\u00a0\u2026", "num_citations": "121\n", "authors": ["168"]}
{"title": "Using a security requirements engineering methodology in practice: the compliance with the Italian data protection legislation\n", "abstract": " Extending Requirements Engineering modelling and formal analysis methodologies to cope with Security Requirements has been a major effort in the past decade. Yet, only few works describe complex case studies that show the ability of the informal and formal approaches to cope with the level complexity required by compliance with ISO-17799 security management requirements.In this paper we present a comprehensive case study of the application of the Secure Tropos RE methodology for compliance to the Italian legislation on Privacy and Data Protection by the University of Trento, leading to the definition and analysis of a ISO-17799-like security management scheme.", "num_citations": "116\n", "authors": ["168"]}
{"title": "Combining deduction and model checking into tableaux and algorithms for converse-PDL\n", "abstract": " This paper presents a prefixed tableaux calculus for Propositional Dynamic Logic with Converse based on a combination of different techniques such as prefixed tableaux for modal logics and model checkers for \u03bc-calculus. We prove the correctness and completeness of the calculus and illustrate its features. We also discuss the transformation of the tableaux method (naively NEXPTIME) into an EXPTIME algorithm.", "num_citations": "113\n", "authors": ["168"]}
{"title": "Stadyna: Addressing the problem of dynamic code updates in the security analysis of android applications\n", "abstract": " Static analysis of Android applications can be hindered by the presence of the popular dynamic code update techniques: dynamic class loading and reflection. Recent Android malware samples do actually use these mechanisms to conceal their malicious behavior from static analyzers. These techniques defuse even the most recent static analyzers that usually operate under the\" closed world\" assumption (the targets of reflective calls can be resolved at analysis time; only classes reachable from the class path at analysis time are used at runtime). Our proposed solution allows existing static analyzers to remove this assumption. This is achieved by combining static and dynamic analysis of applications in order to reveal the hidden/updated behavior and extend static analysis results with this information. This paper presents design, implementation and preliminary evaluation results of our solution called StaDynA.", "num_citations": "105\n", "authors": ["168"]}
{"title": "Single step tableaux for modal logics\n", "abstract": " Single Step Tableaux (SST) are the basis of a calculus for modal logics that combines different features of sequent and prefixed tableaux into a simple, modular, strongly analytic, and effective calculus for a wide range of modal logics.               The paper presents a number of the computational results about SST (confluence, decidability, space complexity, modularity, etc.) and compares SST with other formalisms such as translation methods, modal resolution, and Gentzen-type tableaux. For instance, it discusses the feasibility and infeasibility of deriving decision procedures for SST and translation-based methods by replacing loop checking techniques with simpler termination checks.               The complexity of searching for validity and logical consequence with SST and other methods is discussed. Minimal conditions on SST search strategies are proven to yield Pspace (and NPtime for S5 and KD45\u00a0\u2026", "num_citations": "105\n", "authors": ["168"]}
{"title": "Verifying security protocols as planning in logic programming\n", "abstract": " We illustrate ALSP (Action Language for Security Protocol), a declarative executable specification language for planning attacks to security protocols. ALSP is based on logic programming with negation as failure, and with stable model semantics. In ALSP we can give a declarative specification of a protocol with the natural semantics of send and receive actions which can be performed in parallel. By viewing a protocol trace as a plan to achieve a goal, attacks are (possibly parallel) plans achieving goals that correspond to security violations. Building on results from logic programming and planning, we map the existence of an attack into the existence of a model for the protocol that satisfies the specification of an attack. We show that our liberal model of parallel actions can  adequately represent the traditional Dolev-Yao trace-based model used in the formal analysis of security protocols. Specifications in ALSP are\u00a0\u2026", "num_citations": "100\n", "authors": ["168"]}
{"title": "Verifying the SET registration protocols\n", "abstract": " Secure electronic transaction (SET) is an immense e-commerce protocol designed to improve the security of credit card purchases. In this paper, we focus on the initial bootstrapping phases of SET, whose objective is the registration of cardholders and merchants with a SET certificate authority. The aim of registration is twofold: getting the approval of the cardholder's or merchant's bank and replacing traditional credit card numbers with electronic credentials that cardholders can present to the merchant so that their privacy is protected. These registration subprotocols present a number of challenges to current formal verification methods. First, they do not assume that each agent knows the public keys of the other agents. Key distribution is one of the protocols' tasks. Second, SET uses complex encryption primitives (digital envelopes) which introduce dependency chains: the loss of one secret key can lead to\u00a0\u2026", "num_citations": "99\n", "authors": ["168"]}
{"title": "Security-by-contract: Toward a semantics for digital signatures on mobile code\n", "abstract": " In this paper we propose the notion of security-by-contract, a mobile contract that an application carries with itself. The key idea of the framework is that a digital signature should not just certify the origin of the code but rather bind together the code with a contract. We provide a description of the overall life-cycle of mobile code in the setting of security-by-contract, describe a tentative structure for a contractual language and propose a number of algorithms for one of the key steps in the process, the contract-policy matching issue. We argue that security-by-contract would provide a semantics for digital signatures on mobile code thus being a step in the transition from trusted code to trustworthy code.", "num_citations": "94\n", "authors": ["168"]}
{"title": "Generalized XML security views\n", "abstract": " We investigate a generalization of the notion of XML security view introduced by Stoica and Farkas [17] and later refined by Fan et al.[8]. The model consists of access control policies specified over DTDs with XPath expression for data-dependent access control policies. We provide the notion of security views for characterizing information accessible to authorized users. This is a transformed (sanitized) DTD schema that can be used by users for query formulation and optimization. Then we show an algorithm to materialize\" authorized\" version of the document from the view and an algorithm to construct the view from an access control specification. We also propose a number of generalizations for security policies.", "num_citations": "90\n", "authors": ["168"]}
{"title": "The verification of an industrial payment protocol: The SET purchase phase\n", "abstract": " The Secure Electronic Transaction (SET) protocol has been proposed by a consortium of credit card companies and software corporations to secure e-commerce transactions. When the customer makes a purchase, the SET dual signature guarantees authenticity while keeping the customer's account details secret from the merchant and his choice of goods secret from the bank. This paper reports the first verification results for the complete purchase phase of SET. Using Isabelle and the inductive method, we showed that the credit card details do remain confidential and customer, merchant and bank can confirm most details of a transaction even when some of those details are kept from them. The complex protocol construction makes proofs more difficult but still feasible. Though enough goals can be proved to give confidence in SET, a lack of explicitness in the dual signature makes some agreement properties fail\u00a0\u2026", "num_citations": "88\n", "authors": ["168"]}
{"title": "Anatomy of exploit kits\n", "abstract": " In this paper we report a preliminary analysis of the source code of over 30 different exploit kits which are the main tool behind drive-by-download attacks. The analysis shows that exploit kits make use of a very limited number of vulnerabilities and in a rather unsophisticated fashion. Their key strength is rather their ability to support \u201ccustomers\u201d in avoiding detection, monitoring traffic, and managing exploits.", "num_citations": "85\n", "authors": ["168"]}
{"title": "Formal verification of cardholder registration in SET\n", "abstract": " The first phase of the SET protocol, namely Cardholder Registration, has been modelled inductively. This phase is presented in outline and its formal model is described. A number of basic lemmas have been proved about the protocol using Isabelle/HOL, along with a theorem stating that a certification authority will certify a given key at most once. Many ambiguities, contradictions and omissions were noted while formalizing the protocol.", "num_citations": "77\n", "authors": ["168"]}
{"title": "Which is the right source for vulnerability studies? an empirical analysis on mozilla firefox\n", "abstract": " Recent years have seen a trend towards the notion of quantitative security assessment and the use of empirical methods to analyze or predict vulnerable components. Many papers focused on vulnerability discovery models based upon either a public vulnerability databases (eg, CVE, NVD), or vendor ones (eg, MFSA). Some combine these databases. Most of these works address a knowledge problem: can we understand the empirical causes of vulnerabilities? Can we predict them? Still, if the data sources do not completely capture the phenomenon we are interested in predicting, then our predictor might be optimal with respect to the data we have but unsatisfactory in practice.", "num_citations": "75\n", "authors": ["168"]}
{"title": "Using Walk-SAT and Rel-SAT for cryptographic key search\n", "abstract": " Computer security depends heavily on the strength of cryptographic algorithms. Thus, cryptographic key search is often THE search problem for many governments and corporations. In the recent years, AI search techniques have achieved notable successes in solving \u201creal world\u201d problems. Following a recent result which showed that the properties of the US Data Encryption Standard can be encoded in propositional logic, this paper advocates the use of cryptographic key search as a benchmark for propositional reasoning and search. Benchmarks based on the encoding of cryptographic algorithms optimally share the features of \u201creal world\u201d and random problems. In this paper, two state-of-the-art AI search algorithms, Walk-SAT by Kautz & Selman and Rel-SAT by Bayardo & Schrag, have been tested on the encoding of the Data Encryption Standard, to see whether they are up the task, and we discuss what lesson can be learned from the analysis on this benchmark to improve SAT solvers. New challenges in this field conclude the paper.", "num_citations": "74\n", "authors": ["168"]}
{"title": "Security events and vulnerability data for cybersecurity risk estimation\n", "abstract": " Current industry standards for estimating cybersecurity risk are based on qualitative risk matrices as opposed to quantitative risk estimates. In contrast, risk assessment in most other industry sectors aims at deriving quantitative risk estimations (e.g., Basel II in Finance). This article presents a model and methodology to leverage on the large amount of data available from the IT infrastructure of an organization's security operation center to quantitatively estimate the probability of attack. Our methodology specifically addresses untargeted attacks delivered by automatic tools that make up the vast majority of attacks in the wild against users and organizations. We consider two\u2010stage attacks whereby the attacker first breaches an Internet\u2010facing system, and then escalates the attack to internal systems by exploiting local vulnerabilities in the target. Our methodology factors in the power of the attacker as the number of\u00a0\u2026", "num_citations": "71\n", "authors": ["168"]}
{"title": "A preliminary analysis of vulnerability scores for attacks in wild: the ekits and sym datasets\n", "abstract": " NVD and Exploit-DB are the de facto standard databases used for research on vulnerabilities, and the CVSS score is the standard measure for risk. On open question is whether such databases and scores are actually representative of attacks found in the wild. To address this question we have constructed a database (EKITS) based on the vulnerabilities currently used in exploit kits from the black market and extracted another database of vulnerabilities from Symantec's Threat Database (SYM). Our final conclusion is that the NVD and EDB databases are not a reliable source of information for exploits in the wild, even after controlling for the CVSS and exploitability subscore. An high or medium CVSS score shows only a significant sensitivity (ie prediction of attacks in the wild) for vulnerabilities present in exploit kits (EKITS) in the black market. All datasets exhibit a low specificity.", "num_citations": "69\n", "authors": ["168"]}
{"title": "Lotrec: the generic tableau prover for modal and description logics\n", "abstract": " The last years have seen a renewed interest in modal and description logics (MDLs). Better algorithms, coding, and technology have led to effective systems based on tableau and constraint systems [6 7] to DPLL-based implementations", "num_citations": "66\n", "authors": ["168"]}
{"title": "How to integrate legal requirements into a requirements engineering methodology for the development of security and privacy patterns\n", "abstract": " Laws set requirements that force organizations to assess the security and privacy of their IT systems and impose them to implement minimal precautionary security measures. Several IT solutions (e.g., Privacy Enhancing Technologies, Access Control Infrastructure, etc.) have been proposed to address security and privacy issues. However, understanding why, and when such solutions have to be adopted is often unanswered because the answer comes only from a broader perspective, accounting for legal and organizational issues. Security engineers and legal experts should analyze the business goals of a company and its organizational structure and derive from there the points where security and privacy problems may arise and which solutions best fit such (legal) problems. The paper investigates the methodological support for capturing security and privacy requirements of a concrete health care\u00a0\u2026", "num_citations": "64\n", "authors": ["168"]}
{"title": "Verifying the SET purchase protocols\n", "abstract": " SET (Secure Electronic Transaction) is a suite of protocols proposed by a consortium of credit card companies and software corporations to secure e-commerce transactions. The Purchase part of the suite is intended to guarantee the integrity and authenticity of the payment transaction while keeping the Cardholder's account details secret from the Merchant and his choice of goods secret from the Bank. This paper details the first verification results for the complete Purchase protocols of SET. Using Isabelle and the inductive method, we show that their primary goal is indeed met. However, a lack of explicitness in the dual signature makes some agreement properties fail: it is impossible to prove that the Cardholder meant to send his credit card details to the very payment gateway that receives them. A major effort in the verification went into digesting the SET documentation to produce a realistic model. The\u00a0\u2026", "num_citations": "64\n", "authors": ["168"]}
{"title": "Simplification A General Constraint Propagation Technique for Propositional and Modal Tableaux\n", "abstract": " Tableau and sequent calculi are the basis for most popular interactive theorem provers for formal verification. Yet, when it comes to automatic proof search, tableaux are often slower than Davis-Putnam, SAT procedures or other techniques. This is partly due to the absence of a bivalence principle (viz. the cut-rule) but there is another source of inefficiency: the lack of constraint propagation mechanisms.               This paper proposes an innovation in this direction: the rule of simplification, which plays for tableaux the role of subsumption for resolution and of unit for the Davis-Putnam procedure.               The simplicity and generality of simplification make possible its extension in a uniform way from propositional logic to a wide range of modal logics. This technique gives an unifying view of a number of tableaux-like calculi such as DPLL, KE, HARP, hyper-tableaux, BCP, KSAT.               We show its practical impact\u00a0\u2026", "num_citations": "63\n", "authors": ["168"]}
{"title": "Design and results of TANCS-2000 non-classical (modal) systems comparison\n", "abstract": " The aim of the TABLEAUX-2000 Non-Classical (Modal) System Comparisons (TANCS-2000) is to provide a set of benchmarks and a standardized methodology for the assessment and comparison of ATP systems in non-classical logics, as it is done for first-order logic with the CADE System Competition. We believe that TANCS can benefit the scientific community in two ways: by promoting the competition among ATP systems and thus yielding novel solutions, and by providing a scientific design for benchmarking non-classical ATP systems.               This paper reports the main ideas behind the design, the benchmarks, the organization, and the rating of the ATP systems of TANCS-2000.", "num_citations": "60\n", "authors": ["168"]}
{"title": "Then and now: On the maturity of the cybercrime markets the lesson that black-hat marketeers learned\n", "abstract": " Cybercrime activities are supported by infrastructures and services originating from an underground economy. The current understanding of this phenomenon is that the cybercrime economy ought to be fraught with information asymmetry and adverse selection problems. They should make the effects that we observe every day impossible to sustain. In this paper, we show that the market structure and design used by cyber criminals have evolved toward a market design that is similar to legitimate, thriving, online forum markets such as eBay. We illustrate this evolution by comparing the market regulatory mechanisms of two underground forum markets: 1) a failed market for credit cards and other illegal goods and 2) another, extremely active marketplace for vulnerabilities, exploits, and cyber attacks in general. The comparison shows that cybercrime markets evolved from unruly, scam for scammers market\u00a0\u2026", "num_citations": "55\n", "authors": ["168"]}
{"title": "An overview of the verification of SET\n", "abstract": " This paper describes the verification of Secure Electronic Transaction (SET), an e-commerce protocol by VISA and MasterCard. The main tasks are to comprehend the written documentation, to produce an accurate formal model, to identify specific protocol goals, and, finally, to prove them. The main obstacles are the protocol\u2019s complexity (due in part to its use of digital envelopes) and its unusual goals involving partial information sharing. Our verification efforts show that the protocol does not completely satisfy its goals, although the flaws are minor. The primary outcome of the project is experience with verification of enormous and complicated protocols. This paper summarizes the project \u2013 the details appear elsewhere [11, 12 , 13 ] \u2013 focusing on the issues and the conclusions.", "num_citations": "55\n", "authors": ["168"]}
{"title": "Design and results of the TABLEAUX-99 non-classical (modal) systems comparison\n", "abstract": " This paper reports the main ideas behind the design, the benchmarks, the organization, and the rating of the ATP systems of the TABLEAUX-99 Non-Classical (Modal) System Comparisons (TANCS).", "num_citations": "55\n", "authors": ["168"]}
{"title": "Security and trust in it business outsourcing: a manifesto\n", "abstract": " Nowadays many companies understand the benefit of outsourcing. Yet, in current outsourcing practices, clients usually focus primarily on business objectives and security is negotiated only for communication links. It is however not determined how data must be protected after transmission. Strong protection of a communication link is of little value if data can be easily stolen or corrupted while on a supplier's server. The problem raises a number of related challenges such as: identification of metrics which are more suitable for security-level negotiation, client and contractor perspective and security guarantees in service composition scenarios. These challenges and some others are discussed in depth in the article.", "num_citations": "54\n", "authors": ["168"]}
{"title": "An empirical methodology to evaluate vulnerability discovery models\n", "abstract": " Vulnerability discovery models (VDMs) operate on known vulnerability data to estimate the total number of vulnerabilities that will be reported after a software is released. VDMs have been proposed by industry and academia, but there has been no systematic independent evaluation by researchers who are not model proponents. Moreover, the traditional evaluation methodology has some issues that biased previous studies in the field. In this work we propose an empirical methodology that systematically evaluates the performance of VDMs along two dimensions (quality and predictability) and addresses all identified issues of the traditional methodology. We conduct an experiment to evaluate most existing VDMs on popular web browsers' vulnerability data. Our comparison shows that the results obtained by the proposed methodology are more informative than those by the traditional methodology. Among\u00a0\u2026", "num_citations": "52\n", "authors": ["168"]}
{"title": "The (un) reliability of nvd vulnerable versions data: An empirical experiment on google chrome vulnerabilities\n", "abstract": " NVD is one of the most popular databases used by researchers to conduct empirical research on data sets of vulnerabilities. Our recent analysis on Chrome vulnerability data reported by NVD has revealed an abnormally phenomenon in the data where almost vulnerabilities were originated from the first versions. This inspires our experiment to validate the reliability of the NVD vulnerable version data. In this experiment, we verify for each version of Chrome that NVD claims vulnerable is actually vulnerable. The experiment revealed several errors in the vulnerability data of Chrome. Furthermore, we have also analyzed how these errors might impact the conclusions of an empirical study on foundational vulnerability. Our results show that different conclusions could be obtained due to the data errors.", "num_citations": "50\n", "authors": ["168"]}
{"title": "How to capture, model, and verify the knowledge of legal, security, and privacy experts: a pattern-based approach\n", "abstract": " Laws set requirements that force organizations to assess the security and privacy of their IT systems and impose the adoption of the implementation of minimal precautionary security measures. Several frameworks have been proposed to deal with thii issue. For instance, purpose-based access control is normally considered a good solution for meeting the requirements of privacy legislation. Yet, understanding why, how, and when such solutions to security and privacy problems have to be deployed is often unanswered.", "num_citations": "47\n", "authors": ["168"]}
{"title": "Reasoning about security: A logic and a decision method for role-based access control\n", "abstract": " Role-based access control (RBAC) is one of the most promising techniques for the design and implementation of security policies and its diffusion may be enhanced by the development of formal and automated method of analysis.             This paper presents a logic for practical reasoning about role based access control which simplifies and adapts to RBAC the calculus developed at Digital SRC. Beside a language and a formal semantics, a decision method based on analytic tableaux is also given. Analytic tableaux make it possible to reason about logical consequence, model generation and consistency of a formalised role-based security policy.", "num_citations": "47\n", "authors": ["168"]}
{"title": "Security-by-contract-with-trust for mobile devices\n", "abstract": " Security-by-Contract (S\u00d7C) is a paradigm providing security assurances for mobile applications. In this work, we present the an extension of S\u00d7C, called Security-by-Contract-with-Trust (S\u00d7C\u00d7T). Indeed, we enrich the S\u00d7C architecture by integrating a trust model and adding new modules and configurations for managing contracts. Indeed, at deploy-time, our system decides the run-time configuration depending on the credentials of the contract provider. The run-time environment can both enforce a security policy and monitor the declared contract. According to the actual behaviour of the running program our architecture updates the trust level associated with the contract provider. We also present a possible application of our framework in the scenario of a mobile application marketplace, e.g., Apple AppStore, Cydia, Android Market, that, nowadays, are considered as one of the most attractive e-commerce activity for both mobile application developers and industries of mobile devices. Since the number of applications increases, Mobile Applications Marketplace (MAMp) sets up recommendation systems that rank and highlight mobile applications by category, social activity, etc. The S\u00d7C\u00d7T framework we propose is applied in this scenario for providing security on cus- tomers' mobile devices as well as help Mobile Applications Marketplaces to enhance their recommendation systems with security feedback. The main advantage of this method is an automatic management of the level of trust of software and contract releasers and a unified way for dealing with both security and trust.", "num_citations": "46\n", "authors": ["168"]}
{"title": "Interactive access control for web services\n", "abstract": " Business Processes for Web Services (BPEL4WS) are the new paradigms for lightweight enterprise integration. They cross organizational boundaries and are provided by entities that see each other just as business partners. Web services require shift in the access control mechanism: from identity-based access control to trust management and negotiation, but this is not enough for cross organizational business processes. For many businesses no partner may guess a priori what kind of credentials will be sent by clients and clients may not know a priori which credentials are required for completing a business process.", "num_citations": "46\n", "authors": ["168"]}
{"title": "Tableaux and algorithms for propositional dynamic logic with converse\n", "abstract": " This paper presents a prefixed tableaux calculus for Propositional Dynamic Logic with Converse based on a combination of different techniques such as prefixed tableaux for modal logics and model checkers for mu-calculus. We prove the correctness and completeness of the calculus and illustrate its features. We also discuss the transformation of the tableaux method (naively NEXPTIME) into an EXPTIME algorithm.", "num_citations": "46\n", "authors": ["168"]}
{"title": "Do you really mean what you actually enforced?\n", "abstract": " In their works on the theoretical side of Polymer, Ligatti and his co-authors have identified a new class of enforcement mechanisms based on the notion of edit automata that can transform sequences and enforce more than simple safety properties. We show that there is a gap between the edit automata that one can possibly write (e.g., by Ligatti et\u00a0al in their IJIS running example) and the edit automata that are actually constructed according the theorems from Ligatti\u2019s IJIS paper or from Talhi et\u00a0al. \u201cLigatti\u2019s automata\u201d are just a particular kind of edit automata. Thus, we re-open a question which seemed to have received a definitive answer: you have written your security enforcement mechanism (aka your edit automata); does it really enforce the security policy you wanted?", "num_citations": "45\n", "authors": ["168"]}
{"title": "Vulnerable open source dependencies: Counting those that matter\n", "abstract": " Background: Vulnerable dependencies are a known problem in today's open-source software ecosystems because OSS libraries are highly interconnected and developers do not always update their dependencies.Aim: Our paper addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in OSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources.Method: Careful analysis of deployed dependencies, aggregation of dependencies by their projects, and distinction of halted dependencies allow us to obtain a counting method that avoids over-inflation. To understand the industrial impact of a more precise approach, we considered the 200 most popular OSS Java libraries used by SAP in its own software. Our analysis included 10905 distinct GAVs (group, artifact, version) in Maven when\u00a0\u2026", "num_citations": "44\n", "authors": ["168"]}
{"title": "An Experimental Comparison of Two Risk-Based Security Methods\n", "abstract": " A significant number of methods have been proposed to identify and analyze threats and security requirements, but there are few empirical evaluations that show these methods work in practice. This paper reports a controlled experiment conducted with 28 master students to compare two classes of risk-based methods, visual methods (CORAS) and textual methods (SREP). The aim of the experiment was to compare the effectiveness and perception of the two methods. The participants divided in groups solved four different tasks by applying the two methods using a randomized block design. The dependent variables were effectiveness of the methods measured as number of threats and security requirements identified, and perception of the methods measured through a post-task questionnaire based on the Technology Acceptance Model. The experiment was complemented with participants' interviews to\u00a0\u2026", "num_citations": "44\n", "authors": ["168"]}
{"title": "After-life vulnerabilities: a study on firefox evolution, its vulnerabilities, and fixes\n", "abstract": " We study the interplay in the evolution of Firefox source code and known vulnerabilities in Firefox over six major versions (v1.0, v1.5, v2.0, v3.0, v3.5, and v3.6) spanning almost ten years of development, and integrating a numbers of sources (NVD, CVE, MFSA, Firefox CVS). We conclude that a large fraction of vulnerabilities apply to code that is no longer maintained in older versions. We call these after-life vulnerabilities. This complements the Milk-or-Wine study of Ozment and Schechter\u2014which we also partly confirm\u2014as we look at vulnerabilities in the reference frame of the source code, revealing a vulnerabilitiy\u2019s future, while they looked at its past history. Through an analysis of that code\u2019s market share, we also conclude that vulnerable code is still very much in use both in terms of instances and as global codebase: CVS evidence suggests that Firefox evolves relatively slowly.               This is empirical\u00a0\u2026", "num_citations": "44\n", "authors": ["168"]}
{"title": "Predictability of enforcement\n", "abstract": " The current theory of runtime enforcement is based on two properties for evaluating an enforcement mechanism: soundness and transparency. Soundness defines that the output is always good (\u201cno bad traces slip out\u201d) and transparency defines that good input is not changed (\u201cno surprises on good traces\u201d). However, in practical applications it is also important to specify how bad traces are fixed so that the system exhibits a reasonable behavior. We propose a new notion of predictability which can be defined in the same spirit of continuity in real-functions calculus. It defines that there are \u201cno surprises on bad input\u201d. We discuss this idea based on the feedback of an industrial case study on e-Health.", "num_citations": "44\n", "authors": ["168"]}
{"title": "An automatic method for assessing the versions affected by a vulnerability\n", "abstract": " Vulnerability data sources are used by academics to build models, and by industry and government to assess compliance. Errors in such data sources therefore not only are threats to validity in scientific studies, but also might cause organizations, which rely on retro versions of software, to lose compliance. In this work, we propose an automated method to determine the code evidence for the presence of vulnerabilities in retro software versions. The method scans the code base of each retro version of software for the code evidence to determine whether a retro version is vulnerable or not. It identifies the lines of code that were changed to fix vulnerabilities. If an earlier version contains these deleted lines, it is highly likely that this version is vulnerable. To show the scalability of the method we performed a large scale experiments on Chrome and Firefox (spanning 7,236 vulnerable files and approximately 9\u00a0\u2026", "num_citations": "39\n", "authors": ["168"]}
{"title": "Towards black box testing of android apps\n", "abstract": " Many state-of-art mobile application testing frameworks (e.g., Dynodroid [1], EvoDroid [2]) enjoy Emma [3] or other code coverage libraries to measure the coverage achieved. The underlying assumption for these frameworks is availability of the app source code. Yet, application markets and security researchers face the need to test third-party mobile applications in the absence of the source code. There exists a number of frameworks both for manual and automated test generation that address this challenge. However, these frameworks often do not provide any statistics on the code coverage achieved, or provide coarse-grained ones like a number of activities or methods covered. At the same time, given two test reports generated by different frameworks, there is no way to understand which one achieved better coverage if the reported metrics were different (or no coverage results were provided). To address these\u00a0\u2026", "num_citations": "39\n", "authors": ["168"]}
{"title": "Usage control in service-oriented architectures\n", "abstract": " Usage control governs the handling of sensitive data after it has been given away. The enforcement of usage control requirements is a challenge because the service requester in general has no control over the service provider\u2019s information processing devices. We analyze applicable trust models, conclude that observation-based enforcement is often more appropriate than enforcement by direct control over the service provider\u2019s actions, and present a logical architecture that blends both forms of enforcement with the business logics of service-oriented architectures.", "num_citations": "39\n", "authors": ["168"]}
{"title": "The Taming of the (X) OR\n", "abstract": " Many key verification problems such as boundedmodel-checking, circuit verification and logical cryptanalysis are formalized with combined clausal and affine logic (i.e. clauses with xor as the connective) and cannot be efficiently (if at all) solved by using CNF-only provers.               We present a decision procedure to efficiently decide such problems. The Gauss-DPLL procedure is a tight integration in a unifying framework of a Gauss-Elimination procedure (for affine logic) and a Davis-Putnam-Logeman-Loveland procedure (for usual clause logic).               The key idea, which distinguishes our approach from others, is the full interaction bewteen the two parts which makes it possible to maximize (deterministic) simplification rules by passing around newly created unit or binary clauses in either of these parts.We show the correcteness and the termination of Gauss-DPLL under very liberal assumptions.", "num_citations": "38\n", "authors": ["168"]}
{"title": "Extending Security-by-Contract with quantitative trust on mobile devices\n", "abstract": " Security-by-Contract (S\u00d7C) is a paradigm providing security assurances for mobile applications. In this work, we present an extension of S\u00d7C enriched with an automatic trust management infrastructure. Indeed, we enhance the already existing architecture by adding new modules and configurations for contracts managing. At deploy-time, our system decides the run-time configuration depending on the credentials of contract provider. Roughly, the run-time environment can both enforce a security policy and monitor the declared contract. According to the actual behaviour of the running program our architecture updates the trust level associated with the contract provider. The main advantage of this method is an automatic management of the level of trust of software and contract releasers.", "num_citations": "37\n", "authors": ["168"]}
{"title": "Security-by-contract (SxC) for software and services of mobile systems\n", "abstract": " The paradigm of pervasive services (Bacon 2002) envisions a nomadic user traversing a variety of environments and seamlessly and constantly receiving services from other portables, handhelds, embedded, or wearable computers. Bootstrapping and managing security of services in this scenario is a major challenge. We argue that the challenge is bigger than the \u2018\u2018simple\u2019\u2019pervasive service vision because it does not consider the possibilities that open up when we realize that the smart phone in our pocket already has more computing power than the PC on our desk 15 years ago. Current pervasive services, including context-aware services, do not exploit the computational power of the mobile device. Information is provided to the mobile user anywhere, but the computing infrastructure is centralized (Harter et al. 2002). Even when it is decentralized to increase scalability and performance (Chakraborty et al. 2007; Diot and Gautier 1999), it does not exploit the devices\u2019 computing power. We believe that the future of pervasive services will be shaped by pervasive client downloads. When traversing environments the nomadic user not only invokes services according to a Web Service-like fashion (in either push or pull mode) but also downloads new applications that are able to exploit computational power in order to make a better use of the unexpected services available in the environment. A tourist, upon landing at the airport in a historical city might download a tourist guide application that can route her rented car to those touristic hot spots that are among her particular interests. The application is con\ufb01gured with touristic hot spots and, in order to\u00a0\u2026", "num_citations": "37\n", "authors": ["168"]}
{"title": "Malwarelab: Experimentation with cybercrime attack tools\n", "abstract": " Cybercrime attack tools (ie Exploit Kits) are reportedly responsible for the majority of attacks affecting home users. Exploit kits are traded in the black markets at different prices and advertising different capabilities and functionalities. In this paper we present our experimental approach in testing 10 exploit kits leaked from the markets that we deployed in an isolated environment, our MalwareLab. The purpose of this experiment is to test these tools in terms of resiliency against changing software configurations in time. We present our experiment design and implementation, discuss challenges, lesson learned and open problems, and present a preliminary analysis of the results.", "num_citations": "36\n", "authors": ["168"]}
{"title": "An interactive trust management and negotiation scheme\n", "abstract": " Interactive access control allows a server to compute on the fly missing credentials needed to grant access and to adapt its responses on the basis of client's presented and declined credentials. Yet, it may disclose too much information on what credentials a client needs. Automated trust negotiation allows for a controlled disclosure on what credentials a client has during a mutual disclosure process. Yet, it requires pre-arranged policies and sophisticated strategies. How do we bootstrap from simple security policies a comprehensive interactive trust management and negotiation scheme that combines the best of both worlds without their limitations? This is the subject of the paper.", "num_citations": "36\n", "authors": ["168"]}
{"title": "Interactive credential negotiation for stateful business processes\n", "abstract": " Business Processes for Web Services are the new paradigm for lightweight enterprise integration. They cross organizational boundaries, are provided by entities that see each other just as business partners, and require access control mechanisms based on trust management. Stateful Business Processes, enforcing separation of duties or service limitations based on past or current usage, pose additional research challenges. Clients, which may not know the right set of credentials to supply to each partner, may end up in dead-ends and servers should help them find out what must be revoked and what missing is that grant access to a particular resource.                 We propose a logical framework and an interactive algorithm based on negotiation of credentials for access control that works for Stateful Business Processes. We show that our algorithm is sound (no grant is given to unauthorized clients\u00a0\u2026", "num_citations": "35\n", "authors": ["168"]}
{"title": "How to fake an RSA signature by encoding modular root finding as a SAT problem\n", "abstract": " Logical cryptanalysis has been introduced by Massacci and Marraro as a general framework for encoding properties of crypto-algorithms into SAT problems, with the aim of generating SAT benchmarks that are controllable and that share the properties of real-world problems and randomly generated problems.In this paper, spurred by the proposal of Cook and Mitchell to encode the factorization of large integers as a SAT problem, we propose the SAT encoding of another aspect of RSA, namely finding (i.e. faking) an RSA signature for a given message without factoring the modulus.Given a small public exponent e, a modulus n and a message m, we can generate a SAT formula whose models correspond to the eth roots of m modulo n, without encoding the factorization of n or other functions that can be used to factor n. Our encoding can be used to either generate solved instances for SAT or both satisfiable and\u00a0\u2026", "num_citations": "35\n", "authors": ["168"]}
{"title": "Towards systematic achievement of compliance in service-oriented architectures: The MASTER approach\n", "abstract": " Service-oriented architectures (SOA) have been successfully adapted by agile businesses to support dynamic outsourcing of business processes and the maintenance of business ecosystems. Still, businesses need to comply with applicable laws and regulations. Abstract service interfaces, distributed ownership and cross-domain operations introduce new challenges for the implementation of compliance controls and the assessment of their effectiveness.               In this paper, we analyze the challenges for automated support of the enforcement and evaluation of IT security controls in a SOA. We introduce these challenges by means of an example control, and outline a methodology and a high-level architecture that supports the phases of the control lifecycle through dedicated components for observation, evaluation, decision support and reaction. The approach is model-based and features policy-driven\u00a0\u2026", "num_citations": "34\n", "authors": ["168"]}
{"title": "Decision procedures for expressive description logics with intersection, composition, converse of roles and role identity\n", "abstract": " In the quest for expressive description logics for real-world applications, a powerful combination of constructs has so far eluded practical decision procedures: intersection and composition of roles. We propose tableau-based decision procedures for the satisfiability of logics extending ALC with the intersection\u2293, composition\u25e6, union\u2294, converse\u00b7\u2212 of roles and role identity id (\u00b7). We show that 1. the satisfiability of ALC (\u2293,\u25e6,\u2294), for which a 2-EXPTIME upper bound was given by treeautomata techniques, is PSPACE-complete; 2. the satisfiability of ALC (\u2293,\u25e6,\u2294,\u00b7\u2212, id (\u00b7)), an open problem so far, is in NEXPTIME.", "num_citations": "34\n", "authors": ["168"]}
{"title": "The role of catalogues of threats and security controls in security risk assessment: an empirical study with ATM professionals\n", "abstract": " [Context and motivation] To remedy the lack of security expertise, industrial security risk assessment methods come with catalogues of threats and security controls. [Question/problem] We investigate in both qualitative and quantitative terms whether the use of catalogues of threats and security controls has an effect on the actual and perceived effectiveness of a security risk assessment method. In particular, we assessed the effect of using domain-specific versus domain-general catalogues on the actual and perceived efficacy of a security risk assessment method conducted by non-experts and compare it with the effect of running the same method by security experts but without catalogues.                                [Principal ideas/results] The quantitative analysis shows that non-security experts who applied the method with catalogues identified threats and controls of the same quality of security experts\u00a0\u2026", "num_citations": "33\n", "authors": ["168"]}
{"title": "Quantitative assessment of risk reduction with cybercrime black market monitoring\n", "abstract": " Cybercrime is notoriously maintained and empowered by the underground economy, manifested in black markets. In such markets, attack tools and vulnerability exploits are constantly traded. In this paper, we focus on making a quantitative assessment of the risk of attacks coming from such markets, and investigating the expected reduction in overall attacks against final users if, for example, vulnerabilities traded in the black markets were all to be promptly patched. In order to conduct the analysis, we mainly use the data on (a) vulnerabilities bundled in 90+ attack tools traded in the black markets collected by us; (b) actual records of 9 \u00d7 10 7  attacks collected from Symantec's Data Sharing Programme WINE. Our results illustrate that black market vulnerabilities are an important source of risk for the population of users; we further show that vulnerability mitigation strategies based on black markets monitoring may\u00a0\u2026", "num_citations": "33\n", "authors": ["168"]}
{"title": "HIT4Mal: Hybrid image transformation for malware classification\n", "abstract": " Modern malware evolves various detection avoidance techniques to bypass the state\u2010of\u2010the\u2010art detection methods. An emerging trend to deal with this issue is the combination of image transformation and machine learning models to classify and detect malware. However, existing works in this field only perform simple image transformation methods. These simple transformations have not considered color encoding and pixel rendering techniques on the performance of machine learning classifiers. In this article, we propose a novel approach to encoding and arranging bytes from binary files into images. These developed images contain statistical (eg, entropy) and syntactic artifacts (eg, strings), and their pixels are filled up using space\u2010filling curves. Thanks to these features, our encoding method surpasses existing methods demonstrated by extensive experiments. In particular, our proposed method achieved 93\u00a0\u2026", "num_citations": "32\n", "authors": ["168"]}
{"title": "A security-by-contract architecture for pervasive services\n", "abstract": " Future pervasive environments will be characterised by pervasive client downloads: new (untrusted) clients will be dynamically downloaded in order to exploit the computational power of the nomadic devices to make a better use of the services available in the environment. To address the challenges of this paradigm we propose the notion of security-by-contract (SxC), as in programming-by-contract, based on the notion of a mobile contract that a pervasive download carries with itself. It describes the relevant security features of the application and the relevant security interactions with its nomadic host. In this paper we describe the layered security architecture of the SxC paradigm for pervasive security, the threats and mitigation strategies of security services and sketch some interaction modalities of the security services layer.", "num_citations": "31\n", "authors": ["168"]}
{"title": "A model-driven approach for the specification and analysis of access control policies\n", "abstract": " The last years have seen the definition of many languages, models and standards tailored to specify and enforce access control policies, but such frameworks do not provide methodological support during the policy specification process. In particular, they do not provide facilities for the analysis of the social context where the system operates.               In this paper we propose a model-driven approach for the specification and analysis of access control policies. We build this framework on top of SI*, a modeling language tailored to capture and analyze functional and security requirements of socio-technical systems. The framework also provides formal mechanisms to assist policy writers and system administrators in the verification of access control policies and of the actual user-permission assignment.", "num_citations": "30\n", "authors": ["168"]}
{"title": "A negotiation scheme for access rights establishment in autonomic communication\n", "abstract": " Autonomic computing and communication has become a new paradigm for dynamic service integration and resource sharing in today's ambient networks. Devices and systems need to dynamically collaborate and federate with little known or even unknown parties in order to perform everyday tasks. Those devices and systems act as independent nodes that autonomously manage and enforce their own security policies.               Thus in autonomic pervasive communications clients may not know a priori what access rights they need in order to execute a service nor service providers know a priori what credentials and privacy requirements clients have so that they can take appropriate access decisions.               To solve this problem we propose a negotiation scheme that protects security and privacy interests with respect to information disclosure while still providing effective access control to services. The scheme\u00a0\u2026", "num_citations": "30\n", "authors": ["168"]}
{"title": "How to select a security requirements method? a comparative study with students and practitioners\n", "abstract": " Most Secure Development Software Life Cycles (SSDLCs) start from security requirements. Security Management standards do likewise. There are several methods from industry and academia to elicit and analyze security requirements, but there are few empirical evaluations to investigate whether these methods are effective in identifying security requirements. Most of the papers published in the requirements engineering community report on methods\u2019evaluations that are conducted by the same researchers who have designed the methods.               The goal of this paper is to investigate how successfull academic security requirements methods are when applied by someone different than the method designer. The paper reports on a medium scale qualitative study where master students in computer science and professionals have applied academic security requirements engineering methods to analyze\u00a0\u2026", "num_citations": "29\n", "authors": ["168"]}
{"title": "Matching midlet\u2019s security claims with a platform security policy using automata modulo theory\n", "abstract": " \u2022 Today\u2019s smart phones/nomadic devices have more computing and communication power than PCs 20 years ago, but\u2026\u2022 Not even remotely the amount of third party software available for PCs at that time, and\u2022 A long term market growth cannot be based on selling ring-tones as the only \u201caddedvalue\u201d services.\u00a9 2007 by DoCoMo Communications Laboratories Europe GmbH", "num_citations": "29\n", "authors": ["168"]}
{"title": "Security-by-contract for web services\n", "abstract": " The classical approach to access control of Web Services is to present a number of credentials for the access to a service and possibly negotiate their disclosure using a suitable negotiation protocol and a policy to protect them.", "num_citations": "27\n", "authors": ["168"]}
{"title": "DES: a challenge problem for nonmonotonic reasoning systems\n", "abstract": " The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.", "num_citations": "27\n", "authors": ["168"]}
{"title": "Retaliation: Can we live with flaws?\n", "abstract": " Security protocols intend to give their parties reasonable assurance that certain security properties will protect their communication session. However, the literature confirms that the protocols may suffer subtle and hidden attacks. Flawed protocols are customarily sent back to the design process, but the costs of reengineering a deployed protocol may be prohibitive. This paper outlines the concept of retaliation: who would steal a sum of money today, should this pose significant risks of having twice as much stolen back tomorrow? Attacks are always balanced decisions: if an attack can be retaliated, the economics of security may convince us to live with a flawed protocol. This new perspective requires a new threat model where any party may decide to subvert the protocol for his own sake, depending on the risks of retaliation. This threat model, which for example is also suitable to studying non-repudiation protocols, seems more appropriate than the Dolev-Yao model to the present technological/social setting.", "num_citations": "25\n", "authors": ["168"]}
{"title": "FuturesMEX: secure, distributed futures market exchange\n", "abstract": " In a Futures-Exchange, such as the Chicago Mercantile Exchange, traders buy and sell contractual promises (futures) to acquire or deliver, at some future pre-specified date, assets ranging from wheat to crude oil and from bacon to cash in a desired currency. The interactions between economic and security properties and the exchange's essentially non-monotonic security behavior; a valid trader's valid action can invalidate other traders' previously valid positions, are a challenge for security research. We show the security properties that guarantee an Exchange's economic viability (availability of trading information, liquidity, confidentiality of positions, absence of price discrimination, risk-management) and an attack when traders' anonymity is broken. We describe all key operations for a secure, fully distributed Futures-Exchange, hereafter referred to as simply the 'Exchange'. Our distributed, asynchronous protocol\u00a0\u2026", "num_citations": "24\n", "authors": ["168"]}
{"title": "A method for security governance, risk, and compliance (GRC): a goal-process approach\n", "abstract": " The Governance, Risk, and Compliance (GRC) management process for Information Security is a necessity for any software systems where important information is collected, processed, and used. To this extent, many standards for security managements at operational level exists (e.g., ITIL, ISO27K family etc). What is often missing is a process to govern security at organizational level.               In this tutorial, we present a method to analyze and design security controls that capture the organizational setting of the system and where business goals and processes are the main citizen. The SI*-GRC method is a comprehensive method that is composed of i) a modeling framework based on a requirement engineering framework, with some extensions related to security & GRC concerns, such as: trust, permission, risk, and treatment, 2) a analysis process defining systematical steps in analyzing and design\u00a0\u2026", "num_citations": "24\n", "authors": ["168"]}
{"title": "A self-protecting and self-healing framework for negotiating services and trust in autonomic communication systems\n", "abstract": " In a federation of heterogeneous nodes that organize themselves, the lack of a trusted third party does not allow establishing a priori trust relationships among strangers. Automated trust negotiation (TN) is a promising approach to establish sufficient trust among parties, allowing them to access sensitive data and services in open environments. Although the literature on TN is growing, two key issues have still to be addressed. The first one concerns a typical feature of real-life negotiations: we are usually willing to trade the disclosure of personal attributes in exchange for additional services and only in a particular order (according to our preferences). The second one concerns dependability. By their nature TN systems are used in unreliable contexts where it is important not only to protect negotiations against malicious attack (self-protection), but also against accidental failures (self-healing). In this paper we address\u00a0\u2026", "num_citations": "24\n", "authors": ["168"]}
{"title": "Generalized XML security views\n", "abstract": " We investigate a generalization of the notion of XML security view introduced by Stoica and Farkas (Proceedings of the 16th International Conference on Data and Applications Security (IFIP\u201902). IFIP Conference Proceedings, vol. 256, pp. 133\u2013146. Kluwer, Dordrecht, 2002) and later refined by Fan et al. (Proceedings of the ACM SIG- MOD International Conference on Management of Data (SIGMOD\u201904), pp. 587\u2013598. ACM Press, New York, 2004). The model consists of access control policies specified over DTDs with XPath expressions for data-dependent access control. We provide the notion of security views characterizing information accessible to authorized users. This is a trans- formed DTD schema that can be used by users for query formulation. We develop an algorithm to materialize an authorized version of the document from the view and an algorithm to construct the view from an access control\u00a0\u2026", "num_citations": "23\n", "authors": ["168"]}
{"title": "Matching in security-by-contract for mobile code\n", "abstract": " We propose the notion of security-by-contract, a mobile contract that an application carries with itself. The key idea of the framework is that a digital signature should not just certify the origin of the code but rather bind together the code with a contract.We provide a description of the workflow for the deployment and execution of mobile code in the setting of security-by-contract, describe a structure for a contractual language and propose a number of algorithms for one of the key steps in the process, the contract-policy matching issue.We also describe the prototype for matching policies with security claims of mobile applications that we have currently implemented.We argue that security-by-contract would provide a semantics for digital signatures on mobile code thus being a step in the transition from trusted code to trustworthy code.", "num_citations": "23\n", "authors": ["168"]}
{"title": "Interactive access control for autonomic systems: from theory to implementation\n", "abstract": " Autonomic communication and computing is a new paradigm for dynamic service integration over a network. An autonomic network crosses organizational and management boundaries and is provided by entities that see each other just as partners. For many services no autonomic partner may guess a priori what will be sent by clients nor clients know a priori what credentials are required to access a service. To address this problem we propose a new interactive access control: servers should interact with clients, asking for missing credentials necessary to grant access, whereas clients may supply or decline the requested credentials. Servers evaluate their policies and interact with clients until a decision of grant or deny is taken. This proposal is grounded in a formal model on policy-based access control. It identifies the formal reasoning services of deduction, abduction and consistency. Based on them, the work\u00a0\u2026", "num_citations": "23\n", "authors": ["168"]}
{"title": "From early requirements analysis towards secure workflows\n", "abstract": " Requirements engineering is a key step in the software development process that has little counterpart in the design of secure business processes and secure workflows for web services. This paper presents a methodology that allows a business process designer to derive the skeleton of the concrete coarse grained secure business process, that can be further refined into workflows, from the early requirements analysis.", "num_citations": "23\n", "authors": ["168"]}
{"title": "Contextual reasoning is NP-complete\n", "abstract": " The logic of context with the ist (c; p) modality has been proposed by McCarthy as a foundation for contextual reasoning. This paper shows that propositional logic of context is NP-complete and therefore more tractable than multimodal logics or Multi Language hierarchical logics which are PSPACE-complete. This result is given in a proof-theoretical way by providing a tableau calculus, which can be used as a decision procedure for automated reasoning. The computational gap between logic of context and modal logics is analyzed and some indications for the use of either formalisms are drawn on the basis of the tradeo between compactness of representation and tractability of reasoning.", "num_citations": "23\n", "authors": ["168"]}
{"title": "Security in the Firefox OS and Tizen Mobile Platforms\n", "abstract": " Emerging mobile platforms Firefox OS and Tizen are learning from Android's security successes and trying to avoid its limitations. Although these platforms offer largely novel solutions, they can still learn from one another.", "num_citations": "21\n", "authors": ["168"]}
{"title": "What the heck is this application doing?\u2013A security-by-contract architecture for pervasive services\n", "abstract": " Future pervasive environments are characterized by non-fixed architectures made of users and ubiquitous computers. They will be shaped by pervasive client downloads, i.e. new (untrusted) applications will be dynamically downloaded to make a better use of the computational power available in the ubiquitous computing environment.To address the challenges of this paradigm we propose the notion of security-by-contract (S\u00a0\u00d7\u00a0C), as in programming-by-contract, based on the notion of a mobile contract that a pervasive download carries with itself. It describes the relevant security features of the application and the relevant security interactions with its computing environment. The contract can be used to check it against the device policy for compliance.In this paper we describe the S\u00a0\u00d7\u00a0C concepts, the S\u00a0\u00d7\u00a0C architecture and implementation and sketch some interaction modalities of the S\u00a0\u00d7\u00a0C paradigm.", "num_citations": "21\n", "authors": ["168"]}
{"title": "Single step tableaux for modal logics: Computational properties, complexity and methodology\n", "abstract": " Single Step Tableaux (SST) are the basis of a calculus for modal logics that combines different features of sequent and prefixed tableaux into a simple, modular, strongly analytic, and effective calculus for a wide range of modal logics. The paper presents a number of the computational results about SST (confluence, decidability, space complexity, modularity, etc.) and compares SST with other formalisms such as translation methods, modal resolution, and Gentzen-type tableaux. For instance, it discusses the feasibility and infeasibility of deriving decision procedures for SST and translation-based methods by replacing loop checking techniques with simpler termination checks. The complexity of searching for validity and logical consequence with SST and other methods is discussed. Minimal conditions on SST search strategies are proven to yield PSPACE (and NPTIME for S5 and KD45) decision procedures. The paper also presents the methodology underlying the construction of the correctness and completeness proofs.", "num_citations": "21\n", "authors": ["168"]}
{"title": "Towards a quantitative assessment of security in software architectures\n", "abstract": " Software patterns are key building blocks used to construct the architecture of a software system. Patterns also have an important role during the architecture assessment phase, as they represent the design rationale, which is central to evaluation. This work presents a quantitative approach to assess the security of a pattern-based software architecture. In particular, security patterns are used to measure to what extent an architecture is protected against relevant security threats. To this aim, threat coverage metrics are associated to security patterns and an aggregation algorithm is proposed to compute an overall security indicator. The proposed approach helps in comparing design alternatives and choosing the best candidate.", "num_citations": "20\n", "authors": ["168"]}
{"title": "Privacy is linking permission to purpose\n", "abstract": " The last years have seen a peak in privacy related research. The focus has been mostly on how to protect the individual from being tracked, with plenty of anonymizing solutions.             We advocate another model that is closer to the \u201cphysical\u201d world: we consider our privacy respected when our personal data is used for the purpose for which we gave it in the first place.             Essentially, in any distributed authorization protocol, credentials should mention their purpose beside their powers. For this information to be meaningful we should link it to the functional requirements of the original application.             We sketch how one can modify a requirement engineering methodology to incorporate security concerns so that we explicitly trace back the high-level goals for which a functionality has been delegated by a (human or software) agent to another one. Then one could be directly derive purpose-based trust\u00a0\u2026", "num_citations": "20\n", "authors": ["168"]}
{"title": "A convolutional transformation network for malware classification\n", "abstract": " Modern malware evolves various detection avoidance techniques to bypass the state-of-the-art detection methods. An emerging trend to deal with this issue is the combination of image transformation and machine learning techniques to classify and detect malware. However, existing works in this field only perform simple image transformation methods that limit the accuracy of the detection. In this paper, we introduce a novel approach to classify malware by using a deep network on images transformed from binary samples. In particular, we first develop a novel hybrid image transformation method to convert binaries into color images that convey the binary semantics. The images are trained by a deep convolutional neural network that later classifies the test inputs into benign or malicious categories. Through the extensive experiments, our proposed method surpasses all baselines and achieves 99.14% in terms of\u00a0\u2026", "num_citations": "19\n", "authors": ["168"]}
{"title": "No purpose, no data: Goal-oriented access control forambient assisted living\n", "abstract": " Ambient assisted living is a new interdisciplinary field aiming at supporting senior citizens in their home by means of embedded technologies. This domain offer an interesting challenge for providing dependability and security in a privacy-respecting way: in order to provide services in an emergency we cannot monitor on a second-by-second base a senior citizen. Beside being immoral, it would be illegal (at least in Europe). At the same time if we don't get notified of an emergency the entire system would be useless.", "num_citations": "19\n", "authors": ["168"]}
{"title": "Assessing a requirements evolution approach: Empirical studies in the Air Traffic Management domain\n", "abstract": " Requirements evolution is still a challenging problem in engineering practices. In this paper, we report the results of the empirical evaluation of a novel approach for modeling and reasoning on evolving requirements. We evaluated the effectiveness of the approach in modeling requirements evolution by means of a series of empirical studies in the air traffic management (ATM) domain. As we also wanted to assess whether the knowledge of the method and/or the application domain influences the effectiveness of the approach, the studies involved researchers, master students and domain experts with different level of knowledge of the approach and of the ATM domain. The participants have applied the approach to a real evolutionary scenario which focuses on the introduction of a new queue management tool, the Arrival MANager (AMAN) and a new network for information sharing (SWIM) connecting the main\u00a0\u2026", "num_citations": "18\n", "authors": ["168"]}
{"title": "Computer aided threat identification\n", "abstract": " Recently, there has been an increase of reported security threats hitting organizations. Some of them are originated from the assignments to users of inappropriate permissions on organizational sensitive data. Thus it is crucial for organizations to recognize as early as possible the risks deriving by inappropriate access right management and to identify the solutions that they need to prevent such risks. In this paper, we propose a framework to identify threats during the requirements analysis of organizations' IT systems. With respect to other works which have attempted to include security analysis into requirement engineering process (e.g., KAOS, Elahi et al., Asnar et al.), our framework does not rely on the level of expertise of the security analyst to detect threats but allows to automatically identify threats that derive from inappropriate access management. To capture the organization's setting and the system\u00a0\u2026", "num_citations": "18\n", "authors": ["168"]}
{"title": "Towards practical enforcement theories\n", "abstract": " Runtime enforcement is a common mechanism for ensuring that program executions adhere to constraints specified by a security policy. It is based on two simple ideas: the enforcement mechanism should leave good executions without changes and make sure that the bad ones got amended. From the theory side, a number of papers [6,10,12] provide the precise characterization of good executions that can be captured by a security policy and thus enforced by a specific mechanism. Unfortunately, those theories do not distinguish what happens when an execution is actually bad (the practical case). The theory only says that the outcome of enforcement mechanism should be \u201cgood\u201d but not how far should the bad execution be changed.               If we consider a real-life example of a drug dispensation process in a hospital the notion of security automata or even edit automata would stop all requests by all\u00a0\u2026", "num_citations": "18\n", "authors": ["168"]}
{"title": "Do you really mean what you actually enforced?\n", "abstract": " In the landmark paper on the theoretical side of Polymer, Ligatti and his co-authors have identified a new class of enforcement mechanisms based on the notion of edit automata, that can transform sequences and enforce more than simple safety properties.             We show that there is a gap between the edit automata that one can possibly write (e.g. by Ligatti himself in his running example) and the edit automata that are actually constructed according the theorems from Ligatii\u2019s IJIS paper and IC follow-up papers by Talhi et al. \u201dLigatti\u2019s automata\u201d are just a particular kind of edit automata.             Thus, we re-open a question which seemed to have received a definitive answer: you have written your security enforcement mechanism (aka your edit automata); does it really enforce the security policy you wanted?", "num_citations": "18\n", "authors": ["168"]}
{"title": "A protocol\u2019s life after attacks...\n", "abstract": " I am going to be speaking about protocol verification again; I\u2019m going to take a rather different perspective from the one we normally take, and I\u2019ll be talking about what happens after an attack takes place. Is there a life for a protocol beyond the attacks?             We all know about verification. On the one hand we have the model checking community trying to find a witness of an attack, trying to find if something went wrong and why the specific property of interest failed. On the other hand we have the opposite approach, assuring that there\u2019s no such witness therefore the specific property holds. But the question here is, is this the whole story? It appears that everything is about finding the attack: is there an attack, is there no attack against confidentiality or authentication? It appears kind of weird. Is it only the attack we are really interested in? Is this really all we should look at? I\u2019ll try and convince you that there\u2019s\u00a0\u2026", "num_citations": "18\n", "authors": ["168"]}
{"title": "Efficient approximate deduction and an application to computer security\n", "abstract": " The use of logics and formal methods for specification and verification requires effective deduction methods which can be efficiently implemented. To this aim,", "num_citations": "18\n", "authors": ["168"]}
{"title": "Model comprehension for security risk assessment: an empirical comparison of tabular vs. graphical representations\n", "abstract": " Tabular and graphical representations are used to communicate security risk assessments for IT systems. However, there is no consensus on which type of representation better supports the comprehension of risks (such as the relationships between threats, vulnerabilities and security controls). Cognitive fit theory predicts that spatial relationships should be better captured by graphs. In this paper we report the results of two studies performed in two countries with 69 and 83 participants respectively, in which we assessed the effectiveness of tabular and graphical representations with respect to extraction correct information about security risks. The experimental results show that tabular risk models are more effective than the graphical ones with respect to simple comprehension tasks and in some cases are more effective for complex comprehension tasks. We explain our findings by proposing a simple\u00a0\u2026", "num_citations": "17\n", "authors": ["168"]}
{"title": "An independent validation of vulnerability discovery models\n", "abstract": " The vulnerability discovery process normally refers to the post-release stage where people identify and report security flaws of a released software. Vulnerability discovery models (VDM) operate on the known vulnerability data to estimate the total number of vulnerabilities present in the software. Successful models can be useful hints for both software vendors and users in allocating resources to handle potential breaches, and tentative patch update. For example, we do not exactly know the day of major snow falls but cities expect it to fall in winter and therefore plan resources for road clearing in that period. The effective planning is important because security bugs are different than\" normal\" bugs. A normal bugs might be filed and be scheduled for fixing in the next release. Meanwhile a security vulnerability might required an urgent patch to be shipped to customers lest their browser be subject to rogue campaigns\u00a0\u2026", "num_citations": "17\n", "authors": ["168"]}
{"title": "Can We Support Applications\u2019 Evolution in Multi-Application Smart Cards by Security-by-Contract?\n", "abstract": " Java card technology have progressed at the point of running web servers and web clients on a smart card. Yet concrete deployment of multi-applications smart cards have remained extremely rare because the business model of the asynchronous download and update of applications by different parties requires the control of interactions among possible applications after the card has been fielded. Yet the current security models and techniques do not support this type of evolution. We propose in this paper to apply the notion of security-by-contract (S \u00d7                 C), that is a specification of the security behavior of an application that must be compliant with the security policy of the hosting platform. This compliance can be checked at load time and in this way avoid the need for costly run-time monitoring. We show how the S \u00d7                 C approach can be used to prevent illegal information exchange among\u00a0\u2026", "num_citations": "17\n", "authors": ["168"]}
{"title": "Modelling quality of protection in outsourced business processes\n", "abstract": " There is a large number of research papers and standards dedicated to security for outsourced data. Yet, most papers propose new controls to access and protect the data rather than to assess the level of assurance of the whole process that is currently deployed. The main contributions of the paper is an approach for aggregating security properties of individual tasks of a complex business process in order to receive the level of assurance provided by the whole process. The approach takes into account the fact that some tasks of a business process may be outsourced and thus account for not very reliable partners. The approach chooses the concrete business process offering the highest assurance among several possible design alternatives by building an optimal hyper-path traversing the business process.", "num_citations": "17\n", "authors": ["168"]}
{"title": "Solving QBF with SMV\n", "abstract": " The possibility of solving the Quantified Boolean Formulae (QBF) problems using the SMV system is a consequence of two wellknown theoretical results: the membership of QBF to PSPACE, and the PSPACE-hardness of LTL (and therefore, of SMV). Nevertheless, such results do not imply the existence of a reduction that is also of practical utility. In this paper, we show a reduction from QBF to SMV that is linear (instead of cubic), and uses a constant-size specification.This new reduction has three applications the previous one has not: first, it allows for solving QBF problems using SMV-like systems, which are now more developed than direct QBF solvers; second, we can use it to verify whether the performance behavior of direct QBF solvers is intrinsic of the problem, or rather an effect of the solving algorithm; third, random hard SMV instances can be easily generated by reduction from QBF hard instances (whose generation method is now established).", "num_citations": "17\n", "authors": ["168"]}
{"title": "An executable specification language for planning attacks to security protocols\n", "abstract": " We propose AL/sub SP/ a Declarative Executable Specification Language for Planning Attacks to Security Protocols based on logic programming. In AL/sub SP/ we can give a declarative specification of a protocol with the natural semantics of send and receive actions. We view a protocol trace as a plan to reach a goal, so that attacks are just plans reaching goals that correspond to security violations, which can be also declaratively specified. Building on results from logic programming and planning, we map the existence of an attack to a protocol into the existence of a model for the protocol specification that satisfies the specification of an attack. AL/sub SP/ specifications are executable, as we can automatically search for attacks via any efficient model generator (such as smodels), that implements the stable model semantics of normal logic programs. Thus, we come to a specification language which is easy to use\u00a0\u2026", "num_citations": "17\n", "authors": ["168"]}
{"title": "The seconomics (security-economics) vulnerabilities of decentralized autonomous organizations\n", "abstract": " Traditionally, security and economics functionalities in IT financial services and protocols (FinTech) have been perceived as separate objectives. We argue that keeping them separate is a bad idea for FinTech \u201cDecentralized Autonomous Organizations\u201d (DAOs). In fact, security and economics are one for DAOs: we show that the failure of a security property, e.g. anonymity, can destroy a DAOs because economic attacks can be tailgated to security attacks. This is illustrated by the examples of \u201cTheDAO\u201d (built on the Ethereum platform) and the DAOed version of a Futures Exchange. We claim that security and economics vulnerabilities, which we named seconomics vulnerabilities, are indeed new \u201cbeasts\u201d to be reckoned with.", "num_citations": "16\n", "authors": ["168"]}
{"title": "An experiment on comparing textual vs. visual industrial methods for security risk assessment\n", "abstract": " Many security risk assessment methods have been proposed both from academia and industry. However, little empirical evaluation has been done to investigate how these methods are effective in practice. In this paper we report a controlled experiment that we conducted to compare the effectiveness and participants' perception of visual versus textual methods for security risk assessment used in industry. As instances of the methods we selected CORAS, a method by SINTEF used to provide security risk assessment consulting services, and SecRAM, a method by EUROCONTROL used to conduct security risk assessment within air traffic management. The experiment involved 29 MSc students who applied both methods to an application scenario from Smart Grid domain. The dependent variables were effectiveness of the methods measured as number of specific threats and security controls identified, and\u00a0\u2026", "num_citations": "16\n", "authors": ["168"]}
{"title": "A screening test for disclosed vulnerabilities in foss components\n", "abstract": " Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. Each time a vulnerability is disclosed in a FOSS component, a software vendor using this component in an application must decide whether to update the FOSS component, patch the application itself, or just do nothing as the vulnerability is not applicable to the older version of the FOSS component used. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components and offer more than a decade of support and security fixes for their applications. Moreover, customers expect vendors to react quickly on disclosed vulnerabilities-in case of widely discussed vulnerabilities such as Heartbleed, within hours. To address this challenge, we propose a screening test: a novel, automatic method based on thin slicing, for estimating quickly whether a given\u00a0\u2026", "num_citations": "15\n", "authors": ["168"]}
{"title": "Multi-context exploit test management\n", "abstract": " An input handler receives an exploit test request specifying at least one exploit to be tested against at least one application in at least one execution environment. A deployment engine deploys the at least one execution environment including instantiating a container providing a virtual machine image and configured based on the exploit test request, the instantiated container including the at least one application. A scheduler schedules execution of the at least one execution environment within at least one execution engine, including scheduling an injection of the at least one exploit as specified in the exploit test request. A report generator generates an exploit test report characterizing a result of the at least one exploit being injected into the at least one execution environment of the at least one execution engine.", "num_citations": "15\n", "authors": ["168"]}
{"title": "Economic impacts of rules-versus risk-based cybersecurity regulations for critical infrastructure providers\n", "abstract": " What's the optimal way to regulate cybersecurity for the critical infrastructure operators in charge of electricity transmission? Should regulation follow the US style (a mostly rules-based model), the EU approach (which is mostly risk-based), or a balance of both? The authors discuss the economic issues behind making this choice and present a cybersecurity economics model for public policy in the presence of strategic attackers. They calibrated these models in the field with the support of National Grid, which operates in the UK and on the US East Coast. The model shows that optimal choices are subject to phase transitions: depending on the combination of incentives, operators will stop investing in risk assessment and only care about compliance (and vice versa). This finding suggests that different approaches might be more appropriate in different conditions and that just pushing for more rules could have\u00a0\u2026", "num_citations": "15\n", "authors": ["168"]}
{"title": "Rew-smt: A new approach for rewriting xacml request with dynamic big data security policies\n", "abstract": " Application of dynamic policy has brought benefits to distributed systems, cloud systems, and social network. However, there are no previous studies focused on solving authorization problems in the dynamic policy. In this paper, we focus on analyzing the way of policy change and providing solutions in the dynamic policy environment. The contribution of this paper is two-fold: including the solution for changing policy even when the access request has been granted by the policy and we provide an XACML-based implementation that incorporates the rewriting request model. Experiential results with real-world policies have established the practical and theoretical value of our newly introduced approach.", "num_citations": "14\n", "authors": ["168"]}
{"title": "On the security cost of using a free and open source component in a proprietary product\n", "abstract": " The work presented in this paper is motivated by the need to estimate the security effort of consuming Free and Open Source Software (FOSS) components within a proprietary software supply chain of a large European software vendor. To this extent we have identified three different cost models: centralized (the company checks each component and propagates changes to the different product groups), distributed (each product group is in charge of evaluating and fixing its consumed FOSS components), and hybrid (only the least used components are checked individually by each development team). We investigated publicly available factors (e.\u00a0g., development activity such as commits, code size, or fraction of code size in different programming languages) to identify which one has the major impact on the security effort of using a FOSS component in a larger software product.", "num_citations": "14\n", "authors": ["168"]}
{"title": "Iterative enforcement by suppression: Towards practical enforcement theories\n", "abstract": " Runtime enforcement is a common mechanism for ensuring that program executions adhere to constraints specified by a security policy. It is based on two simple ideas: the enforcement mechanism should leave good executions without changes (transparency) and make sure that the bad ones got amended (soundness). From the theory side, a number of papers (Hamlen et al., Ligatti et al., Talhi et al.) provide the precise characterization of good executions that can be captured by a security policy and thus enforced by mechanisms like security automata or edit automata.", "num_citations": "14\n", "authors": ["168"]}
{"title": "Quality of protection: Security measurements and metrics\n", "abstract": " Quality of Protection: Security Measurements and Metrics is an edited volume based on the Quality of Protection Workshop in Milano, Italy (September 2005). This volume discusses how security research can progress towards quality of protection in security comparable to quality of service in networking and software measurements, and metrics in empirical software engineering. Information security in the business setting has matured in the last few decades. Standards such as IS017799, the Common Criteria (ISO15408), and a number of industry certifications and risk analysis methodologies have raised the bar for good security solutions from a business perspective. Designed for a professional audience composed of researchers and practitioners in industry, Quality of Protection: Security Measurements and Metrics is also suitable for advanced-level students in computer science.", "num_citations": "14\n", "authors": ["168"]}
{"title": "A qualitative study of dependency management and its security implications\n", "abstract": " Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers' overall decision-making strategies for selecting, managing, and updating software dependencies.", "num_citations": "13\n", "authors": ["168"]}
{"title": "Anytime approximate modal reasoning\n", "abstract": " Propositional modal logics have two independent sources of complexity: unbounded logical omniscience and unbounded logical introspection. This paper discusses an approximation method to tame both of them, by merging propositional approximations with a new technique tailored for multi-modal logics. It provides both skeptical and credulous approximations (or approximation that are neither of the two). On this semantics we build an anytime proof procedure with a simple modification to classical modal tableaux. The procedure yields approximate proofs whose precision increases as we have more resources (time, space etc.) and we analyze its semantical and computational \u201cquality guarantees\u201d.", "num_citations": "13\n", "authors": ["168"]}
{"title": "IoT security configurability with security-by-contract\n", "abstract": " Cybersecurity is one of the biggest challenges in the Internet of Things (IoT) domain, as well as one of its most embarrassing failures. As a matter of fact, nowadays IoT devices still exhibit various shortcomings. For example, they lack secure default configurations and sufficient security configurability. They also lack rich behavioural descriptions, failing to list provided and required services. To answer this problem, we envision a future where IoT devices carry behavioural contracts and Fog nodes store network policies. One requirement is that contract consistency must be easy to prove. Moreover, contracts must be easy to verify against network policies. In this paper, we propose to combine the security-by-contract (S\u00d7 C) paradigm with Fog computing to secure IoT devices. Following our previous work, first we formally define the pillars of our proposal. Then, by means of a running case study, we show that we can model communication flows and prevent information leaks. Last, we show that our contribution enables a holistic approach to IoT security, and that it can also prevent unexpected chains of events. View Full-Text", "num_citations": "12\n", "authors": ["168"]}
{"title": "On the equivalence between graphical and tabular representations for security risk assessment\n", "abstract": " Context: Many security risk assessment methods are proposed both in academia (typically with a graphical notation) and industry (typically with a tabular notation).Question: We compare methods based on those two notations with respect to their actual and perceived efficacy when both groups are equipped with a domain-specific security catalogue (as typically available in industry risk assessments).                          Results: Two controlled experiments with MSc students in computer science show that tabular and graphical methods are (statistically) equivalent in quality of identified threats and security controls. In the first experiment the perceived efficacy of tabular method was slightly better than the graphical one, and in the second experiment two methods are perceived as equivalent. Contribution: A graphical notation does not warrant by itself better (security) requirements elicitation than a tabular\u00a0\u2026", "num_citations": "12\n", "authors": ["168"]}
{"title": "Security-by-Contract for the OSGi platform\n", "abstract": " The natural business model of OSGi is dynamic loading and removal of bundles or services on an OSGi platform. If bundles can come from different stakeholders, how do we make sure that one\u2019s services will only be invoked by the authorized bundles? A simple solution is to interweave functional and security logic within each bundle, but this decreases the benefits of using a common platform for service deployment and is a well-known source of errors. Our solution is to use the Security-by-Contract methodology (SxC) for loading time security verification to separate the security from the business logic while controlling access to applications. The basic idea is that each bundle has a contract embedded into its manifest, that contains details on functional requirements and permissions for access by other bundles on the platform. During bundle installation the contract is matched with the platform security policy\u00a0\u2026", "num_citations": "12\n", "authors": ["168"]}
{"title": "GoCoMM: a governance and compliance maturity model\n", "abstract": " Advanced methodologies for compliance such as CobiT identify a number of maturity levels that must be reached: first the existence of an infrastructure for the enforcement of security controls; second, the ability to continuously monitor and audit quantifiable indicators for the controls put in place; and third, the ability to react when a policy violation is detected. In this paper, we go further and define a governance and compliance maturity model (GoCoMM) that is process-oriented. As an instance of the highest level of governance and compliance, we suggest a method of goal correlation that provides measurable indicators of security and compliance by systematically refining business processes and regulatory goals. We also introduce a run-time architecture to support this model.", "num_citations": "12\n", "authors": ["168"]}
{"title": "Simulating midlet's security claims with automata modulo theory\n", "abstract": " Model-carrying code and security-by-contract have proposed to augment mobile code with a claim on its security behavior that could be matched against a mobile platform policy before downloading the code. In order to capture realistic scenarios with potentially infinite transitions (eg\" only connections to urls starting with https\") we have proposed to represent those policies with the notion of Automata Modulo Theory (AMT), an extension of Buchi Automata (BA), with edges labeled by expressions in a decidable theory.", "num_citations": "12\n", "authors": ["168"]}
{"title": "An algorithm for the appraisal of assurance indicators for complex business processes\n", "abstract": " In order to provide certified security services we must provide indicators that can measure the level of assurance that a complex business process can offer. Unfortunately the formulation of security indicators is not amenable to efficient algorithms able to evaluate the level of assurance of complex process from its components. In this paper we show an algorithm based on FD-Graphs (a variant of directed hypergraphs) that can be used to compute in polynomial time (i) the overall assurance indicator of a complex business process from its components for arbitrary monotone composition functions,(ii) the subpart of the business process that is responsible for such assurance indicator (ie the best security alternative).", "num_citations": "12\n", "authors": ["168"]}
{"title": "Verifying the SET purchase protocols\n", "abstract": " The Secure Electronic Transaction (SET) protocol has been proposed by a consortium of credit card companies and software corporations to guarantee the authenticity of e-commerce transactions and the confidentiality of data. When the customer makes a purchase, the SET dual signature keeps his account details secret from the merchant and his choice of goods secret from the bank. This paper reports verification results for the purchase step of SET, using the inductive method. The credit card details do remain confidential. The customer, merchant and bank can confirm most details of a transaction even when some of those details are kept from them. The usage of dual signatures requires repetition in protocol messages, making proofs more difficult but still feasible. The formal analysis has revealed a significant defect. The dual signature lacks explicitness, giving rise to potential vulnerabilities.", "num_citations": "12\n", "authors": ["168"]}
{"title": "Tableau methods for formal verification of multi-agent distributed systems\n", "abstract": " Formal verification is a key step in the development of trusted and reliable multi-agent distributed systems. This is particularly relevant when security concerns such as privacy, integrity and availability impose limitations on the operations that can be performed on sensitive data. The aim of access control is to limit what agents (humans, programs, softbots, etc.) of distributed systems can do directly or indirectly by delegating their powers and tasks. As the size of the systems and the sensitivity of data increase, the availability of automated reasoning methods becomes essential for logical analysis of access control.This paper presents a prefixed tableau method for the calculus of access control developed at the Digital System Research Center. This calculus is particularly interesting for a number of reasons. First it was the basis for the development and the verification of an implemented system. Second, it poses many\u00a0\u2026", "num_citations": "12\n", "authors": ["168"]}
{"title": "The Work\u2010Averse Cyberattacker Model: Theory and Evidence from Two Million Attack Signatures\n", "abstract": " The assumption that a cyberattacker will potentially exploit all present vulnerabilities drives most modern cyber risk management practices and the corresponding security investments. We propose a new attacker model, based on dynamic optimization, where we demonstrate that large, initial, fixed costs of exploit development induce attackers to delay implementation and deployment of exploits of vulnerabilities. The theoretical model predicts that mass attackers will preferably (i) exploit only one vulnerability per software version, (ii) largely include only vulnerabilities requiring low attack complexity, and (iii) be slow at trying to weaponize new vulnerabilities . These predictions are empirically validated on a large data set of observed massed attacks launched against a large collection of information systems. Findings in this article allow cyber risk managers to better concentrate their efforts for vulnerability\u00a0\u2026", "num_citations": "11\n", "authors": ["168"]}
{"title": "Crime Pays If You Are Just an Average Hacker\n", "abstract": " This study investigates the effects of incentive and deterrence strategies that might turn a security researcher into a malware writer, or vice versa. By using a simple game theoretic model, we illustrate how hackers maximize their expected utility. Furthermore, our simulation models show how hackers' malicious activities are affected by changes in strategies employed by defenders. Our results indicate that, despite the manipulation of strategies, average-skilled hackers have incentives to participate in malicious activities, whereas highly skilled hackers who have high probability of getting maximum payoffs from legal activities are more likely to participate in legitimate ones. Lastly, according on our findings, reactive strategies are more effective than proactive strategies in discouraging hackers' malicious activities.", "num_citations": "11\n", "authors": ["168"]}
{"title": "A load time Policy Checker for open multi-application smart cards\n", "abstract": " Applications on multi-application smart cards contain sensitive data and can exchange information. Thus a major concern is that these applications should not exchange data unless permitted by their respective policy. As modern smart cards allow post-issuance installation and removal of applications, traditional approaches for information flow analysis are not suitable. We suggest the Security-by-Contract approach for loading time application certification on the card, that will enable the stakeholders with the means to ensure the compliance of every update of the card with their security policy. We describe an extension of the card security architecture to deal with verification for different types of updates and present a Java Card prototype implementation of the Policy Checker with performance measurements.", "num_citations": "11\n", "authors": ["168"]}
{"title": "Goal-equivalent secure business process re-engineering\n", "abstract": " The introduction of information technologies in health care systems often requires to re-engineer the business processes used to deliver care. Obviously, the new and re-engineered processes are observationally different and thus we cannot use existing model-based techniques to argue that they are somehow \u201cequivalent\u201d. In this paper we propose a method for passing from SI*, a modeling language for capturing and modeling functional, security, and trust organizational and system requirements, to business process specifications and vice versa. In particular, starting from an old secure business process, we reconstruct the functional and security requirements at organizational level that such a business process was supposed to meet (including the trust relations that existed among the members of the organization). To ensure that the re-engineered business process meets the elicited requirements, we\u00a0\u2026", "num_citations": "11\n", "authors": ["168"]}
{"title": "Planning attacks to security protocols: Case studies in logic programming\n", "abstract": " Formal verification of security protocols has become a key issue in computer security. Yet, it has proven to be a hard task often error prone and discouraging for non-experts in formal methods.               In this paper we show how security protocols can be specified and verified efficiently and effectively by embedding reasoning about actions into a logic programming language.               In a nutshell, we view a protocol trace as a plan to achieve a goal, so that protocol attacks are plans achieving goals that correspond to security violations. Building on results from logic programming and planning, we map the existence of an attack to a protocol into the existence of a model for the protocol specification that satisfies the specification of an attack. To streamline such way of modeling security protocols, we use a description language   which makes it possible to describe protocols with declarative ease and to search\u00a0\u2026", "num_citations": "11\n", "authors": ["168"]}
{"title": "Simplification with renaming: A general proof technique for tableau and sequent-based provers\n", "abstract": " Tableau and sequent calculi are the basis for most popular interactive theorem provers for hardware and software veri cation. Yet, when it comes to decision procedures or automatic proof search, tableaux are orders of magnitude slower than Davis-Putnam, SAT based procedures or other techniques based on resolution.To meet this challenge, this paper proposes a theoretical innovation: the rule of simpli cation, which plays for tableaux the same role of subsumption for resolution and unit for Davis-Putnam. This technique gives an unifying view of a number of tableaux-like calculi such as DPLL, KE, HARP, hyper-tableaux etc. For instance, the stand-alone nature of the rst-order Davis-Putnam-Longeman-Loveland procedure can be explained away as a case of Smullyan tableau with propositional simpli cation. Beside its computational e ectiveness, the simplicity and generality of simpli cation make possible its extension in a uniform way. We de ne it for propositional and rst order logic and a wide range of modal logics. For a full-edged rst order simpli cation we combine it with another technique, renaming, which subsumes the use of free universal variables in sequent and tableau calculi. New experimental results are given for random SAT and the IFIP benchmarks for hardware veri cation.", "num_citations": "11\n", "authors": ["168"]}
{"title": "Tableaux methods for access control in distributed systems\n", "abstract": " The aim of access control is to limit what users of distributed systems can do directly or through their programs. As the size of the systems and the sensitivity of data increase formal methods of analysis are often required.             This paper presents a prefixed tableaux method for the calculus of access control in distributed system developed at DEC-SRC by Abadi, Lampson et. al. Beside the applicative interest, the calculus poses interesting technical challenges, since it has not the tree-model property, introduces relations between modalities which cannot be compiled into axiom schemas, and has some features of the universal modality.             As a side-effect we show a tableaux calculus for the universal modality which distinguishes it from S5 (via satisfiability on non tree-models).", "num_citations": "11\n", "authors": ["168"]}
{"title": "Estimating the assessment difficulty of CVSS environmental metrics: an experiment\n", "abstract": " [Context] The CVSS framework provides several dimensions to score vulnerabilities. The environmental metrics allow security analysts to downgrade or upgrade vulnerability scores based on a company\u2019s computing environments and security requirements. [Question] How difficult is for a human assessor to change the CVSS environmental score due to changes in security requirements (let alone technical configurations) for PCI-DSS compliance for networks and systems vulnerabilities of different type? [Results] A controlled experiment with 29\u00a0MSc students shows that given a segmented network it is significantly more difficult to apply the CVSS scoring guidelines on security requirements with respect to a flat network layout, both before and after the network has been changed to meet the PCI-DSS security requirements. The network configuration also impact the correctness of vulnerabilities\u00a0\u2026", "num_citations": "10\n", "authors": ["168"]}
{"title": "Delta-bench: differential benchmark for static analysis security testing tools\n", "abstract": " Background: Static analysis security testing (SAST) tools may be evaluated using synthetic micro benchmarks and benchmarks based on real-world software. Aims: The aim of this study is to address the limitations of the existing SAST tool benchmarks: lack of vulnerability realism, uncertain ground truth, and large amount of findings not related to analyzed vulnerability. Method: We propose Delta-Bench - a novel approach for the automatic construction of benchmarks for SAST tools based on differencing vulnerable and fixed versions in Free and Open Source (FOSS) repositories. To test our approach, we used 7 state of the art SAST tools against 70 revisions of four major versions of Apache Tomcat spanning 62 distinct Common Vulnerabilities and Exposures (CVE) fixes and vulnerable files totalling over 100K lines of code as the source of ground truth vulnerabilities. Results: Our experiment allows us to draw\u00a0\u2026", "num_citations": "10\n", "authors": ["168"]}
{"title": "IT interdependence and the economic fairness of cybersecurity regulations for civil aviation\n", "abstract": " Interviews about emerging cybersecurity threats and a cybersecurity public policy economic model for civil aviation illustrate stakeholders' concerns: interdependency issues can lead to aviation regulations that put smaller airports at a disadvantage.", "num_citations": "10\n", "authors": ["168"]}
{"title": "Realizing trustworthy business services by a new grc approach\n", "abstract": " The trustworthiness of business services is widely recognised as a critical factor for the success of an organization. Businesses are increasing in complexity and unpredictability, while demand for accountability, as well as regulatory compliance is becoming mandatory. Yet, some reports indicate that the level of fraud within an organization is far from decreasing. Thus, a structured approach to Governance, Risk and Compliance (GRC) has become a high priority goal for many organizations. GRC solutions enable organizations to address various business challenges related to risk management and regulatory compliance. For example, GRC solutions provide end-to-end control management, deployment of controls through risk-based approaches and automatic monitoring of controls across different entities and applications. Furthermore, GRC solutions enable standardization of methodologies, vocabulary and measurements across an organization, therefore facilitating the detection of risks, the prioritization of corrective actions and so the enforcement of compliance.", "num_citations": "10\n", "authors": ["168"]}
{"title": "The meaning of logs\n", "abstract": " While logging events is becoming increasingly common in computing, in communication and in collaborative environments, log systems need to satisfy increasingly challenging (if not conflicting) requirements. In this paper we propose a high-level framework for modeling log systems, and reasoning about them. This framework allows one to give a high-level representation of a log system and to check whether it satisfies given audit and privacy properties which in turn can be expressed in standard logic. In particular, the framework can be used for comparing and assessing log systems. We validate our proposal by formalizing a number of standard log properties and by using it to review a number of existing systems. Despite the growing pervasiveness of log systems, we believe this is the first framework of this sort.", "num_citations": "10\n", "authors": ["168"]}
{"title": "Multi-session security monitoring for mobile code\n", "abstract": " There is increasing demand for running multiple times a number of interacting applications in a secure and controllable way on mobile devices. Such demand is not supported by the Java/.NET security models based on trust domains nor by current security monitors or language-based security approaches. Trust domains don\u2019t allow for interactions while language-based security doesn\u2019t support enough customizable policies. A careful analysis of the security requirements in the booming domain of mobile games reveals that most practical security requirements can be represented with an enhanced notion of pure past temporal Logic augmented with the intuitive notion of session. We propose an approach that allows security policies that are i) expressive enough to capture multiple sessions and interacting applications, ii) suitable for efficient monitoring, iii) convenient for a developer to specify them. Since getting all three at once is impossible, we advocate a logical language, 2D-LTL a bi-dimensional temporal logic fit for multiple sessions and for which efficient monitoring algorithms can be given, and a graphical language based on standard UML sequence diagrams with a tight correspondence between the two. In this paper we show a refined formal model for capturing the notion of session and the correctness and completeness of the monitoring algorithm for security policies expressed in 2D-LTL.", "num_citations": "10\n", "authors": ["168"]}
{"title": "An access control system for business processes for Web services\n", "abstract": " Web Services and Business Processes for Web Services are the new paradigms for the lightweight integration of business from different enterprises. Whereas the security and access control policies for basic web services and distributed systems are well studied and almost standardized, there is not yet a comprehensive proposal for an access control architecture for business processes. The major difference is that business process describe complex services that cross organizational boundaries and are provided by entities that sees each other as just partners and nothing else. This calls for a number of differences with traditional aspects of access control architectures such as: - credential vs classical user-based access control, - interactive and partner-based vs one-server-gathers-all requests of credentials from clients, - controlled disclosure of information vs all-or-nothing access control decisions, - abducing missing credentials for fulfilling requests vs deducing entailment of valid requests from credentials in formal models, - \"source-code\" authorization processes vs data describing policies for communicating policies or for orchestrating the work of authorization servers. Looking at the access control field we find good approximation of most components but not their synthesis into one access control architecture for business processes for web services, which is the contribution of this paper.", "num_citations": "10\n", "authors": ["168"]}
{"title": "Exptime Tableaux for ALC.\n", "abstract": " We propose a tableaux calculus requiring simple exponential time for satis ability of an ALC concept C wrt a TBox T containing general axioms of the form C v D. From correspondences with Propositional Dynamic Logic (PDL) it is known that this problem is in EXPTIME Pratt, 1978; Vardi and Wolper, 1986]. However, an algorithm directly derived from the methods used to prove such a result would always require exponential time and space even in simple cases, eg when a simple model satisfying both T and C can be easily found.On the other hand, proposed tableaux methods Buchheit et al., 1993], which explore a space of candidate models for T and C starting from simple ones, can take advantage of such cases. However, there can be an exponential number of possibly exponential-size candidate models. Hence, an algorithm based on tableaux methods requires doubly exponential time in the worst case. We devise a re ned tableaux calculus that integrates the techniques used in PDL with tableaux, thus achieving a tableaux-based procedure working in simple exponential time. In a nutshell, traditional tableaux methods close a branch only by\\rst principles\"(atomic clashes), whereas our enhanced tableau exploits previously proved inconsistencies as additional lemmata to decide that a branch can be closed without having to nd the same atomic clashes again.", "num_citations": "10\n", "authors": ["168"]}
{"title": "Vuln4Real: A Methodology for Counting Actually Vulnerable Dependencies\n", "abstract": " Vulnerable dependencies are a known problem in today's free open-source software ecosystems because FOSS libraries are highly interconnected, and developers do not always update their dependencies. Our paper proposes Vuln4Real, the methodology for counting actually vulnerable dependencies, that addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in FOSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. To understand the industrial impact of a more precise methodology, we considered the 500 most popular FOSS Java libraries used by SAP in its own software. Our analysis included 25767 distinct library instances in Maven. We found that the proposed methodology has visible impacts on both ecosystem view and the individual library developer view of the situation\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "Graphical vs. tabular notations for risk models: on the role of textual labels and complexity\n", "abstract": " [Background] Security risk assessment methods in industry mostly use a tabular notation to represent the assessment results whilst academic works advocate graphical methods. Experiments with MSc students showed that the tabular notation is better than an iconic graphical notation for the comprehension of security risks. [Aim] We investigate whether the availability of textual labels and terse UML-style notation could improve comprehensibility. [Method] We report the results of an online comprehensibility experiment involving 61 professionals with an average of 9 years of working experience, in which we compared the ability to comprehend security risk assessments represented in tabular, UML-style with textual labels, and iconic graphical modeling notations. [Results] Tabular notation are still the most comprehensible notion in both recall and precision. However, the presence of textual labels does improve the\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "Attack potential in impact and complexity\n", "abstract": " Vulnerability exploitation is reportedly one of the main attack vectors against computer systems. Yet, most vulnerabilities remain unexploited by attackers. It is therefore of central importance to identify vulnerabilities that carry a high'potential for attack'. In this paper we rely on Symantec data on real attacks detected in the wild to identify a trade-off in the Impact and Complexity of a vulnerability in terms of attacks that it generates; exploiting this effect, we devise a readily computable estimator of the vulnerability's Attack Potential that reliably estimates the expected volume of attacks against the vulnerability. We evaluate our estimator performance against standard patching policies by measuring foiled attacks and demanded workload expressed as the number of vulnerabilities entailed to patch. We show that our estimator significantly improves over standard patching policies by ruling out low-risk vulnerabilities, while\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "The Work-Averse Attacker Model\n", "abstract": " In this paper we present and validate a novel attacker model based on the economic notion that the attacker has limited resources to forge a new attack. We focus on the vulnerability exploitation case, whereby the attacker has to choose whether to exploit a new vulnerability or keep an old one. We postulate that most vulnerabilities remain unattacked, and that the exploit development cycle relates to software updates rather than to the disclosure of new vulnerabilities. We develop a simple mathematical model to show the mechanisms underlying our observations and name it \u201cThe Work-Averse Attacker Model\u201d. We then leverage Symantec\u2019s data sharing platform WINE to validate our model by analysing records of attacks against more than 1M real systems. We find the \u2018Model of the Work-Averse Attacker\u2019to be strongly supported by the data and, in particular, that:(a) the great majority of attacks per software version is driven by one vulnerability only;(b) an exploit lives two years before being substituted by a new one;(c) the exploit arrival rate depends on the software\u2019s update rate rather than on time or knowledge of the vulnerability.", "num_citations": "9\n", "authors": ["168"]}
{"title": "A relative cost-benefit approach for evaluating alternative airport security policies\n", "abstract": " While careful and prudent settings for airport security policies and strategies are more important than ever, most of them have been implemented as a direct result of terrorist activities rather than motivated by a proper assessment. Furthermore, even if many scholars have proposed ways to assess and evaluate alternative airport security policies particularly by using cost-benefit analysis, they have overlooked two important facets: parameter measurability and social aspects of security policies. In this study, we develop a variant of cost-benefit analysis which we term \"Relative Cost-Benefit Analysis\" and illustrate how we can resolve these problems.", "num_citations": "9\n", "authors": ["168"]}
{"title": "TestREx: a testbed for repeatable exploits\n", "abstract": " Web applications are the target of many known exploits and also a fertile ground for the discovery of security vulnerabilities. Those applications may be exploitable not only because of the vulnerabilities in their source code, but also because of the environments on which they are deployed and run. Execution environments usually consist of application servers, databases and other supporting applications. In order to test whether known exploits can be reproduced in different settings, better understand their effects and facilitate the discovery of new vulnerabilities, we need to have a reliable testbed. In this paper, we present TESTREX, a testbed for repeatable exploits, which has as main features: packing and running applications with their environments; injecting exploits and monitoring their success; and generating security reports. We also provide a corpus of example applications, taken from related works or implemented by us.", "num_citations": "9\n", "authors": ["168"]}
{"title": "My Software has a Vulnerability, should I worry?\n", "abstract": " (U.S) Rule-based policies to mitigate software risk suggest to use the CVSS score to measure the individual vulnerability risk and act accordingly: an HIGH CVSS score according to the NVD (National (U.S.) Vulnerability Database) is therefore translated into a \"Yes\". A key issue is whether such rule is economically sensible, in particular if reported vulnerabilities have been actually exploited in the wild, and whether the risk score do actually match the risk of actual exploitation. We compare the NVD dataset with two additional datasets, the EDB for the white market of vulnerabilities (such as those present in Metasploit), and the EKITS for the exploits traded in the black market. We benchmark them against Symantec's threat explorer dataset (SYM) of actual exploit in the wild. We analyze the whole spectrum of CVSS submetrics and use these characteristics to perform a case-controlled analysis of CVSS scores (similar to those used to link lung cancer and smoking) to test its reliability as a risk factor for actual exploitation. We conclude that (a) fixing just because a high CVSS score in NVD only yields negligible risk reduction, (b) the additional existence of proof of concepts exploits (e.g. in EDB) may yield some additional but not large risk reduction, (c) fixing in response to presence in black markets yields the equivalent risk reduction of wearing safety belt in cars (you might also die but still..). On the negative side, our study shows that as industry we miss a metric with high specificity (ruling out vulns for which we shouldn't worry). In order to address the feedback from BlackHat 2013's audience, the final revision (V3) provides additional data in Appendix A\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "Dealing with known unknowns: towards a game-theoretic foundation for software requirement evolution\n", "abstract": " Requirement evolution has drawn a lot of attention from the community with a major focus on management and consistency of requirements. Here, we tackle the fundamental, albeit less explored, alternative of modeling the future evolution of requirements.               Our approach is based on the explicit representation of controllable evolutions vs observable evolutions, which can only be estimated with a certain probability. Since classical interpretations of probability do not suit well the characteristics of software design, we introduce a game-theoretic approach to give an explanation to the semantic behind probabilities. Based on this approach we also introduce quantitative metrics to support the choice among evolution-resilient solutions for the system-to-be.               To illustrate and show the applicability of our work, we present and discuss examples taken from a concrete case study (the security of the SWIM\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "From hippocratic databases to secure tropos: a computer-aided re-engineering approach\n", "abstract": " Privacy protection is a growing concern in the marketplace. Yet, privacy requirements and mechanisms are usually retro-fitted into a pre-existing design which may not be able to accommodate them due to potential conflicts with functional requirements.         We propose a procedure for automatically extracting privacy requirements from databases supporting access control mechanisms for personal data (hereafter Hippocratic databases) and representing them in the Secure Tropos framework where tools are available for checking the correctness and consistency of privacy requirements. The procedure is illustrated with a case study.", "num_citations": "9\n", "authors": ["168"]}
{"title": "E Pluribus Unum\n", "abstract": " Autonomic Communication is a new paradigm for dynamic network integration. An Autonomic Network crosses organizational boundaries and is provided by entities that see each other just as business partners. Policy-base network anagement already requires a paradigm shift in the access control mechanism (from identity-based access control to trust management and negotiation), but this is not enough for cross organizational autonomic communication. For many services no partner may guess a priori what credentials will be sent by clients and clients may not know a priori which credentials are required for completing a service requiring the orchestration of many different autonomic nodes.               We propose a logical framework and a Web-Service based implementation for reasoning about access control for Autonomic Communication. Our model is based on interaction and exchange of requests for\u00a0\u2026", "num_citations": "9\n", "authors": ["168"]}
{"title": "A uniform tableaux method for nonmonotonic modal logics\n", "abstract": " We present a semantic tableaux calculus for propositional nonmonotonic modal logics, based on possible-worlds characterisations for nonmonotonic modal logics. This method is parametric with respect to both the modal logic and the preference semantics, since it handles in a uniform way the entailment problem for a wide class of nonmonotonic modal logics: McDermott and Doyle's logics and ground logics. It also achieves the computational complexity lower bounds.", "num_citations": "9\n", "authors": ["168"]}
{"title": "Who should pay for interdependent risk? Policy implications for security interdependence among airports\n", "abstract": " We study interdependent risks in security, and shed light on the economic and policy implications of increasing security interdependence in presence of reactive attackers. We investigate the impact of potential public policy arrangements on the security of a group of interdependent organizations, namely, airports. Focusing on security expenditures and costs to society, as assessed by a social planner, to individual airports and to attackers, we first develop a game\u2010theoretic framework, and derive explicit Nash equilibrium and socially optimal solutions in the airports network. We then conduct numerical experiments mirroring real\u2010world cyber scenarios, to assess how a change in interdependence impact the airports' security expenditures, the overall expected costs to society, and the fairness of security financing. Our study provides insights on the economic and policy implications for the United States, Europe, and\u00a0Asia.", "num_citations": "8\n", "authors": ["168"]}
{"title": "Cyberinsurance and public policy: Self-protection and insurance with endogenous adversaries\n", "abstract": " Corporate insurance contracts providing liability coverage in the event of an information security breach are increasingly popular. In addition to the obvious use of \u2018Cyberinsurance\u2019as a risk mitigation tool, a public policy narrative has emerged whereby insurance companies act as a clearing house for information and then provide guidance on appropriate security investment to firms seeking liability coverage. Utilizing few assumptions, our modeling framework demonstrates that this view of cyberinsurance as a delegated policy tool is unlikely to yield the anticipated coordination benefits, and may in fact erode the aggregate level of security investment undertaken by targets.", "num_citations": "8\n", "authors": ["168"]}
{"title": "How CVSS is DOSsing your patching policy (and wasting your money).\n", "abstract": " CVSS score is widely used as the standard-de-facto risk metric for vulnerabilities, to the point that the US Government itself encourages organizations in using it to prioritize vulnerability patching. We tackle this approach by testing the CVSS score in terms of its efficacy as a\" risk score\" and\" prioritization metric.\" We test the CVSS against real attack data and as a result, we show that the overall picture is not satisfactory: the (lower-bound) over-investment by using CVSS to choose what vulnerabilities to patch can as high as 300% of an optimal one. We extend the analysis making sure to obtain statistically significant results. However, we present our results at a practical level, focusing on the question:\" does it make sense for you to use CVSS to prioritize your vulnerabilities?\"", "num_citations": "8\n", "authors": ["168"]}
{"title": "A tool for managing evolving security requirements\n", "abstract": " Management of requirements evolution is a challenging process. Requirements change continuously making the traceability of requirements difficult and the monitoring of requirements unreliable. Moreover, changing requirements might have an impact on the security properties a system design should satisfy: certain security properties that are satisfied before evolution might no longer be valid or new security properties need to be satisfied after changes have been introduced. This paper presents SeCMER, a tool for requirements evolution management developed in the context of the SecureChange project. The tool supports automatic detection of requirement changes and violation of security properties using change-driven transformations. The tool also supports argumentation analysis to check security properties are preserved by evolution and to identify new security properties that should be taken into\u00a0\u2026", "num_citations": "8\n", "authors": ["168"]}
{"title": "Security-By-Contract (SxC) for Mobile Systems\n", "abstract": " We present the notion of Security-by-Contract (S\u00d7 C), a mobile contract that an application carries with itself. The key idea of the framework is that a digital signature should not just certify the origin of the code but rather bind together the code with a contract. In a nutshell, a contract describes the security relevant interactions that the mobile application could have with the mobile device. The contract should be accepted by the platform (if compatible with the policy) at deployment time, and its enforcement guaranteed, for instance by in-line monitoring. A short live demo with real phones will be done at the end of the talk.", "num_citations": "8\n", "authors": ["168"]}
{"title": "Retaliation against protocol attacks\n", "abstract": " Security protocols intend to give their parties reasonable assurance that certain security properties will protect their communication session. However, the literature confirms that the protocols may suffer subtle and hidden attacks. Flawed protocols are customarily sent back to the design process, but the costs of reengineering a deployed protocol may be prohibitive. This paper outlines the concept of retaliation: who would steal a sum of money today, should this pose significant risks of having twice as much stolen back tomorrow? When ethics is left behind, attacks are always balanced decisions: if an attack can be retaliated, the economics of security may convince the attacker to refrain from attacking, and us to live with a flawed protocol. This new perspective requires a new threat model where any party may decide to subvert the protocol for his own sake, depending on the risks of retaliation. This threat model, which for example is also suitable to studying nonrepudiation protocols, seems more appropriate than the Dolev-Yao model to the present technological/social setting. It is demonstrated that machine-assisted protocol verification can can effectively be adapted to the new threat model.", "num_citations": "8\n", "authors": ["168"]}
{"title": "Towards practical security monitors of UML policies for mobile applications\n", "abstract": " There is increasing demand for running interacting applications in a secure and controllable way on mobile devices. Such demand is not fully supported by the Java/.NET security model based on trust domains nor by current security monitors or language-based security approaches. We propose an approach that allows security policies that are i) expressive enough to capture multiple sessions and interacting applications, ii) suitable for efficient monitoring, iii) convenient for a developer to specify them. Since getting all three at once is impossible, we advocate a logical language, 2D-LTL a bi-dimensional temporal logic fit for multiple sessions and for which efficient monitoring algorithms can be given, and a graphical language based on standard UML sequence diagrams with a tight correspondence between the two.", "num_citations": "8\n", "authors": ["168"]}
{"title": "E-Government and on-line services: Security and Legal Patterns\n", "abstract": " E-government refers to the introduction of digital technologies into public administrations and it is assuming a pivotal role in many countries, including Italy. In particular, the supply of on-line services by public administrations represents a rapidly expanding phenomenon. The objective of the paper is to support system designer in the development of IT systems that comply with regulations that govern the use of technologies in public administrations. Thus, taking as running example a tax portal and its authentication issues, we look at the general principles and rules that govern institutional sites and portals, as established in the Italian Public Administration Code. We also show how Security Requirements Engineering methodologies can assist system designers in their activities.", "num_citations": "8\n", "authors": ["168"]}
{"title": "Maintaining privacy on derived objects\n", "abstract": " Protecting privacy means to ensure users that access to their personal data complies with their preferences. However, information can be manipulated in order to derive new objects that may disclose part of the original information. Therefore, control of information flow is necessary for guaranteeing privacy protection since users should know and control not only who access their personal data, but also who access information derived from their data. Actually, current approaches for access control do not provide support for managing propagation of information and for representing user preferences. This paper proposes to extend the Flexible Authorization Framework (FAF) in order to automatically verify whether a subject is entitled to process personal data and derive the authorizations associated with the outcome of data processing. In order to control information flow, users may specify the range of authorizations\u00a0\u2026", "num_citations": "8\n", "authors": ["168"]}
{"title": "Towards Using Source Code Repositories to Identify Software Supply Chain Attacks\n", "abstract": " Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100,000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (eg, those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI\u00a0\u2026", "num_citations": "7\n", "authors": ["168"]}
{"title": "Typosquatting and combosquatting attacks on the python ecosystem\n", "abstract": " Limited automated controls integrated into the Python Package Index (PyPI) package uploading process make PyPI an attractive target for attackers to trick developers into using malicious packages. Several times this goal has been achieved via the combosquatting and typosquatting attacks when attackers give malicious packages similar names to already existing legitimate ones. In this paper, we study the attacks, identify potential attack targets, and propose an approach to identify combosquatting and typosquatting package names automatically. The approach might serve as a basis for an automated system that ensures the security of the packages uploaded and distributed via PyPI.", "num_citations": "7\n", "authors": ["168"]}
{"title": "Protecting the internet of things with security-by-contract and fog computing\n", "abstract": " Nowadays, the Internet of Things (IoT) is a consolidated reality. Smart homes are equipped with a growing number of IoT devices that capture more and more information about human beings lives. However, manufacturers paid little or no attention to security, so that various challenges are still in place. In this paper, we propose a novel approach to secure IoT systems that combines the concept of Security-by-Contract (S\u00d7C) with the Fog computing distributed paradigm. We define the pillars of our approach, namely the notions of IoT device contract, Fog node policy and contract-policy matching, the respective life-cycles, and the resulting S\u00d7C workflow. To better understand all the concepts of the S\u00d7C framework, and highlight its practical feasibility, we use a running case study based on a context-aware system deployed in a real smart home.", "num_citations": "7\n", "authors": ["168"]}
{"title": "Goal-oriented access control model for ambient assisted living\n", "abstract": " Ambient assisted living is a new interdisciplinary field aiming at supporting senior citizens in their home by means of embedded technologies. This domain offer an interesting challenge for providing dependability and security in a privacy-respecting way: in order to provide services in an emergency we cannot monitor on a second-by-second base a senior citizen. Beside being immoral, it would be illegal (at least in Europe). At the same time if we do not get notified of an emergency, the entire system would be useless.               In this paper we present an access control model for this domain that extends RBAC with the notion of organizational model, goals and dependencies. In this model we can associate permission to the objectives that have been assigned to the users of the system and solve the trade-off between security and dependability.", "num_citations": "7\n", "authors": ["168"]}
{"title": "A logical model for security of Web services\n", "abstract": " Business Processes for Web Services are the new paradigm for the lightweight integration of business from different enterprises. Yet, there is not a comprehensive proposal for a logical framework for access control for business processes though logics for access control policies for basic web services are well studied. In this paper we propose a logical framework for reasoning (deduction, abduction, consistency checking) about access control for business processes for web services.", "num_citations": "7\n", "authors": ["168"]}
{"title": "The proof complexity of analytic and clausal tableaux\n", "abstract": " It is widely believed that a family \u03a3 n of unsatisfiable formulae proposed by Cook and Reckhow in their landmark paper (Proc. ACM Symp. on Theory of Computing, 1974) can be used to give a lower bound of 2 \u03a9 (2 n) on the proof size with analytic tableaux. This claim plays a key role in the proof that tableaux cannot polynomially simulate tree resolution. We exhibit an analytic tableau proof for \u03a3 n for whose size we prove an upper bound of O (2 n 2), which, although not polynomial in the size O (2 n) of the input formula, is exponentially shorter than the claimed lower bound. An analysis of the proofs published in the literature reveals that the pitfall is the blurring of n-ary (clausal) and binary versions of tableaux. A consequence of this analysis is that a second widely held belief falls too: clausal tableaux are not just a more efficient notational variant of analytic tableaux for formulae in clausal normal form. Indeed clausal\u00a0\u2026", "num_citations": "7\n", "authors": ["168"]}
{"title": "A bridge between modal logics and contextual reasoning\n", "abstract": " The goal of this paper is to present and discuss a simple and rather e ective tableau calculus which combines modal logics of knowledge and belief with contextual reasoning. The system is made by a multiple combination. For modal proofs, it labels formulae as pre xed tableaux but uses message (knowledge) passing rules similar to those of sequentlike tableaux. For contextual deduction, it merges the metalevel information of the labelling system used by Multicontextual Languages with that used by Labelled Deductive Systems. Its semantics is also simple and intuitively based on a property of Kripke models. The resulting calculus (k-Clusters Tableaux) is e ective for automated proofs, applicable to a wide range of modal logics, and adaptable to many search heuristics. It is also easy to use for proof presentation since its rule have intuitive epistemic interpretation (how knowledge and belief can be inherited up and down possible worlds). It is weak enough to satisfy a KB where two consistent modal statements may globally contradict each other (in classical logic), but strong enough to rule out an inconsistent statement.", "num_citations": "7\n", "authors": ["168"]}
{"title": "Superficial tableaux for contextual reasoning\n", "abstract": " This paper presents a tableaux calculus for the Propositional Logic of Contexts with the ist (c,\u00a2) modality. This approach has a twofold advantage: from the user viewpoint it presents rules which intuitively reflect epistemic properties (lifting, use of assumptions etc.); from a computational perspective it allows local and incremental computation, satisfies strong confluence and can therefore be adapted efficiently to different search heuristics. The modelling of contexts as partial objects is obtained by using superficial assignments. We can define meaningful and meaningless sentences and reason about formulae containing both kind of sentences. Superficial valuations provide us with a sound and incremental approximation of classical logic and make it possible to present a simplified semantics based on layered models.", "num_citations": "7\n", "authors": ["168"]}
{"title": "An Approach for Decision Support on the Uncertainty in Feature Model Evolution\n", "abstract": " Software systems could be seen as a hierarchy of features which are evolving due to the dynamic of the working environments. The companies who build software thus need to make an appropriate strategy, which takes into consideration of such dynamic, to select features to be implemented. In this work, we propose an approach to facilitate such selection by providing a means to capture the uncertainty of evolution in feature models. We also provide two analyses to support the decision makers. The approach is exemplified in the Smart Grid scenario.", "num_citations": "6\n", "authors": ["168"]}
{"title": "A first empirical evaluation framework for security risk assessment methods in the ATM domain\n", "abstract": " Evaluation and validation methodologies are integral parts of Air Traffic Management (ATM). They are well understood for safety, environmental and other business cases for which operational validation guidelines exist which are well defined and widely used. In contrast, there are no accepted methods to evaluate and compare the effectiveness of risk assessment practices for security. The EMFASE project aims to address this gap by providing an innovative framework to compare and evaluate in a qualitative and quantitative manner risk assessment methods for security in ATM. This paper presents the initial version of the framework and the results of the experiments we conducted to compare and assess security risk assessment methods in ATM. The results indicate that participants better perceive graphical methods for security risk assessment. In addition, the use of domain-specific catalogues of threats and security controls seems to have a significant effect on the perceived usefulness of the methods.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Managing evolution by orchestrating requirements and testing engineering processes\n", "abstract": " Change management and change propagation across the various models of the system (such as requirements, design and testing models) are well-known problems in software engineering. For such problems a number of solutions have been proposed that are usually based on the integration of model repositories and on the maintenance of traceability links between the models. We propose to manage the mutual evolution of requirements models and tests models by orchestrating processes based on a minimal shared interface. Thus, requirement and test engineers must only have a basic knowledge about the ``other'' domain, share a minimal set of concepts and can follow their ``own'' respective processes. The processes are orchestrated in the sense that when a change affects a concept of the interface, the change is propagated to the other domain. We illustrate the approach using the evolution of the Global\u00a0\u2026", "num_citations": "6\n", "authors": ["168"]}
{"title": "An idea of an independent validation of vulnerability discovery models\n", "abstract": " Having a precise vulnerability discovery model (VDM) would provide a useful quantitative insight to assess software security. Thus far, several models have been proposed with some evidence supporting their goodness-of-fit. In this work we describe an independent validation of the applicability of these models to the vulnerabilities of the popular browsers Firefox, Google Chrome and Internet Explorer. The result shows that some VMDs do not simply fit the data, while for others there are both positive and negative evidences.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Load time security verification\n", "abstract": " Modern multi-application smart cards can be an integrated environment where applications from different providers are loaded on the fly and collaborate in order to facilitate lives of the cardholders. This initiative requires an embedded verification mechanism to ensure that all applications on the card respect the application interactions policy.               The Security-by-Contract approach for loading time verification consists of two phases. During the first phase the loaded code is verified to be compliant with the supplied contract. Then, during the second phase the contract is matched with the smart card security policy. The paper focuses on the first phase and describes an algorithm for static analysis of the loaded bytecode on Java Card. The paper also reports about implementation of this algorithm that can be embedded on a real smart card.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Security-by-Contract for Applications\u2019 Evolution in Multi-Application Smart Cards\n", "abstract": " \u2022 New application C arrives on the platform. Desired behavior:-C will only call shareable interfaces ID1, ID2, ID3-C will only call shareable interface ID-C will only call ID2 after calling ID3\u2022 Advanced Desired Behavior:-Information flow only TO and FROM service ID1 at any point-Call Flow TO service ID2 only after service call FROM ID3", "num_citations": "6\n", "authors": ["168"]}
{"title": "Pareto-optimal architecture according to assurance indicators\n", "abstract": " In this paper we present an approach and algorithm for selecting the \u201cbest\u201d secure architecture for supporting a business process according to a variety of assurance indicators. The key difficulty is to select an architectural design in presence of multiple indicators that might offer alternative notions of minimality. Therefore we must use the notion of Pareto optimality in order to select alternatives that are not dominated by others.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Towards an independent semantics and verification technology for the HLPSL specification language\n", "abstract": " We present an algorithm for the translation of security protocol specifications in the HLPSL language developed in the framework of the AVISPA project to a dialect of the applied pi calculus. This algorithm provides us with two interesting scientific contributions: at first, it provides an independent semantics of the HLPSL specification language and, second, makes it possible to verify protocols specified in HLPSL with the applied pi calculus-based ProVerif tool. Our technique has been implemented and tested on various security protocols. The translation can handle a large part of the protocols modelled in HLPSL.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Making sense of specifications: the formalization of set\n", "abstract": " The last ten years, since the seminal work on the BAN logic [6], have seen the rapid development of formal methods for the analysis of security protocols. But security protocols have also developed rapidly, becoming more and more complex. Protocols for electronic commerce are the b\u00e9te noir: six pages were enough to describe the Needham-Schroeder protocol in 1978 [15], six hundred pages were not enough to describe the SET protocol of VISA and Mastercard [11, 12, 13] twenty years later.", "num_citations": "6\n", "authors": ["168"]}
{"title": "Perspectives on the SolarWinds Incident\n", "abstract": " A significant cybersecurity event has recently been discovered in which malicious actors gained access to the source code for the Orion monitoring and management software made by the company SolarWinds and inserted malware into that source code. This article contains brief perspectives from a few members of the IEEE Security & Privacy editorial board regarding that incident.", "num_citations": "5\n", "authors": ["168"]}
{"title": "Are we preparing students to build security in? A survey of european cybersecurity in higher education programs\n", "abstract": " We present a review of European master of science programs in cybersecurity and reflect on the presence (and lack) of knowledge and skills needed to build security in.", "num_citations": "5\n", "authors": ["168"]}
{"title": "Measuring the accuracy of software vulnerability assessments: experiments with students and professionals\n", "abstract": " Assessing the risks of software vulnerabilities is a key process of software development and security management. This assessment requires to consider multiple factors (technical features, operational environment, involved assets, status of the vulnerability lifecycle, etc.) and may depend from the assessor\u2019s knowledge and skills. In this work, we tackle with an important part of this problem by measuring the accuracy of technical vulnerability assessments by assessors with different level and type of knowledge. We report an experiment to compare how accurately students with different technical education and security professionals are able to assess the severity of software vulnerabilities with the Common Vulnerability Scoring System (v3) industry methodology. Our results could be useful for increasing awareness about the intrinsic subtleties of vulnerability risk assessment and possibly better compliance\u00a0\u2026", "num_citations": "5\n", "authors": ["168"]}
{"title": "Method and apparatus for distributed, privacy-preserving and integrity-preserving exchange, inventory and order book\n", "abstract": " A method and apparatus for the distributed, privacy preserving and integrity preserving technical implementation of an order book where a group of computing devices belonging to agents (traders, brokers, or groups thereof) wishing to engage in distributed double auctions and markets can (1) keep private the information about the solvency of the individual agent owing the trader's computer;(2) publicly register the values of buy and sell orders while (a) maintaining confidential the information about the owner of the buy (resp. sell) order and (b) still having the possibility to accrue matched buy/sell transactions to the accounts of the computing devices of the individual agents owing those orders;(3) maintain the financial integrity of the market as implemented by a distributed systems of computing devices; and (4) keep private the possible exit from the market by computing devices operated by agents who could no\u00a0\u2026", "num_citations": "5\n", "authors": ["168"]}
{"title": "Agency problems and airport security: Quantitative and qualitative evidence on the impact of security training\n", "abstract": " We analyze the issue of agency costs in aviation security by combining results from a quantitative economic model with a qualitative study based on semi\u2010structured interviews. Our model extends previous principal\u2010agent models by combining the traditional fixed and varying monetary responses to physical and cognitive effort with nonmonetary welfare and potentially transferable value of employees' own human capital. To provide empirical evidence for the tradeoffs identified in the quantitative model, we have undertaken an extensive interview process with regulators, airport managers, security personnel, and those tasked with training security personnel from an airport operating in a relatively high\u2010risk state, Turkey. Our results indicate that the effectiveness of additional training depends on the mix of \u201ctransferable skills\u201d and \u201cemotional\u201d buy\u2010in of the security agents. Principals need to identify on which side of a\u00a0\u2026", "num_citations": "5\n", "authors": ["168"]}
{"title": "Diversity: A Poor Man's Solution to Drone Takeover.\n", "abstract": " Drones are the new targets for hackers. On one hand, their widespread use and security weakness have made them very attractive for the attackers. On the other hand, there are a few security solutions for drones. Out of these few, some are just proposals, and fewer are in the early stage of development. We first assess the requirements of a security solution for drones and then analyse the effect of traditional cryptographic solutions on the drone\u2019s traffic volume and energy consumption. With recourse to Moving Target Defence, we propose a novel instruction diversity solution for drone security that is portable, and has zero overhead.", "num_citations": "5\n", "authors": ["168"]}
{"title": "Security triage: an industrial case study on the effectiveness of a lean methodology to identify security requirements\n", "abstract": " Context: Poste Italiane is a large corporation offering integrated services in banking and savings, postal services, and mobile communication. Every year, it receives thousands of change requests for its ICT services. Applying to each and every request a security assessment\" by the book\" is simply not possible. Goal: We report the experience by Poste Italiane of a lean methodology to identify security requirements that can be inserted in the production cycle of a normal company. Method: The process is based on surveying the overall IT architectures (Security Survey) and then a lean dynamic process (Security Triage) to evaluate individual change requests, so that important changes get the attention they need, minor changes can be quickly implemented, and compliance and security obligations are met. Results: The empirical evaluation conducted for over an year at Poste Italiane shows that the process significantly\u00a0\u2026", "num_citations": "5\n", "authors": ["168"]}
{"title": "Legal Patterns Implement Trust in IT Requirements: When Legal Means are the Best Implementation of IT Technical Goals\n", "abstract": " The traditional approach of computer scientists to Law is that laws (statutes, regulations, etc.) set the requirements, logicians and requirements analysts model them, and finally IT technical solutions or organizational patterns are used to implement them. In this paper we try to answer a radically different question: Can a technical solution (e.g. a requirement in a security and dependability pattern) be implemented by legal means? We show how Legal Patterns, that represent the legal analogy of Software Patterns, can be formally used to implement trust relations required by security and dependability patterns.", "num_citations": "5\n", "authors": ["168"]}
{"title": "A system for interactive authorization for business processes for web services\n", "abstract": " Business Processes for Web Services are the new paradigm for virtual organization. In such cross organizational partnerships no business partner may guess a priori what kind of credentials will be sent by clients nor the clients may know a priori the needed credentials for the successful completion of a business process. This requires an interaction between server and clients.               We propose a framework for managing the authorization interactions for business processes and a BPEL4WS based implementation using Collaxa server. Our model is based on interaction between servers and clients and exchange of requests for supplying or declining missing credentials.", "num_citations": "5\n", "authors": ["168"]}
{"title": "Towards the formal verification of ciphers: Logical cryptanalysis of DES\n", "abstract": " Providing formal assurance of correctness is a key issue for cryptographic algorithms. Yet, automated reasoning tools have only been used for the veri cation of security protocols, and almost never for the veri cation and cryptanalysis of the cryptographic algorithms on which those protocols rely.We claim that one can use logic for encoding the low-level properties of state-of-the-art cryptographic algorithms and then use automated theorem proving for reasoning about them. We call this approach logical cryptanalysis. In this framework, nding a model for a formula encoding an algorithm is equivalent to nding a key with a cryptanalytic attack. Other important properties, such as algebraic closure can also be captured.", "num_citations": "5\n", "authors": ["168"]}
{"title": "A new challenge for automated reasoning: Verification and cryptanalysis of cryptographic algorithms\n", "abstract": " The veri cation of security properties is one of the key issues of computer science and automated reasoning tools play a key role in the high level veri cation of cryptographic protocols. Yet almost nobody uses these reasoning tools for the veri cation and cryptanalysis of the algorithms upon which these protocols are based. In this paper we advocate that it is possible to use logic to encode the low-level properties of state-of-the-art cryptographic algorithms and then use automated theorem proving for reasoning about them. We call this approach logical cryptanalysis.Here we show a case study on the Data Encryption Standard and discuss the techniques that lead to a manageable encoding of this cryptographic algorithm into propositional logic. In this encoding nding a model for the encoded formula is equivalent to nding a key with a cryptanalytic attack.", "num_citations": "5\n", "authors": ["168"]}
{"title": "Mac-A-Mal: macOS malware analysis framework resistant to anti evasion techniques\n", "abstract": " With macOS increasing popularity, the number, and variety of macOS malware are rising as well. Yet, very few tools exist for dynamic analysis of macOS malware. In this paper, we propose a macOS malware analysis framework called Mac-A-Mal. We develop a kernel extension to monitor malware behavior and mitigate several anti-evasion techniques used in the wild. Our framework exploits the macOS features of XPC service invocation that typically escape traditional mechanisms for detection of children processes. Performance benchmarks show that our system is comparable with professional tools and able to withstand VM detection. By using Mac-A-Mal, we discovered 71 unknown adware samples (8 of them using valid distribution certificates), 2 keyloggers, and 1 previously unseen trojan involved in the APT32 OceanLotus.", "num_citations": "4\n", "authors": ["168"]}
{"title": "The effect of security education and expertise on security assessments: The case of software vulnerabilities\n", "abstract": " In spite of the growing importance of software security and the industry demand for more cyber security expertise in the workforce, the effect of security education and experience on the ability to assess complex software security problems has only been recently investigated. As proxy for the full range of software security skills, we considered the problem of assessing the severity of software vulnerabilities by means of a structured analysis methodology widely used in industry (i.e. the Common Vulnerability Scoring System (\\CVSS) v3), and designed a study to compare how accurately individuals with background in information technology but different professional experience and education in cyber security are able to assess the severity of software vulnerabilities. Our results provide some structural insights into the complex relationship between education or experience of assessors and the quality of their assessments. In particular we find that individual characteristics matter more than professional experience or formal education; apparently it is the \\emph{combination} of skills that one owns (including the actual knowledge of the system under study), rather than the specialization or the years of experience, to influence more the assessment quality. Similarly, we find that the overall advantage given by professional expertise significantly depends on the composition of the individual security skills as well as on the available information.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Unruly Innovation: Distributed Ledgers, Blockchains and the Protection of Transactional Rents\n", "abstract": " We present a new conceptual model of disruptive innovation and apply it to distributed ledger technology. Our analysis illustrates the new features of this technology and why there is an argument that, in several respects, the combination of distributed ledger technology and cryptographically enabled contracts changes the economic framework within which individuals, firms and policy makers reside. This foundational level of disruption appears to have several new features, more notably a fundamental change in the game played by economic actors: the ability to self-deregulate. The paper provides a simple, but precise description of a series of complex interactions and illustrates the main points using a recent case study from the Distributed Autonomous Organizations of the Ethereum project.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Which security catalogue is better for novices?\n", "abstract": " Several catalogues of security threats and controls have been proposed to help organizations in identifying critical risks and improve their risk posture against real world threats. But the role that these catalogues play in a security risk assessment has not yet been investigated. In this paper we report an experiment with 18 MSc students conducted to compare the effect of using domain-specific and domain-general catalogues of threats and security controls on the actual efficacy and perception of a security risk assessment method. The experimental results show that there is no difference in the actual efficacy of the method when applied with the two types of catalogues. In contrast, the perceived usefulness of the method is higher for the participants who have used the domain-specific catalogues. In addition, the domain-specific catalogues are perceived as easier to use by the participants.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Security triage: A report of a lean security requirements methodology for cost-effective security analysis\n", "abstract": " Poste Italiane is a large corporation offering integrated services in banking and savings, postal services, and mobile communication. Every year, it receives thousands of change requests for its ICT services. Applying to each and every request a security assessment \u201cby the book\u201dis simply not possible. We report the experience by Poste Italiane of a lean methodology to identify security requirements that can be inserted in the production cycle of a normal company. The process is based on surveying the overall IT architectures (Security Survey) and then a lean dynamic process (Security Triage) to evaluate individual change requests, so that important changes get the attention they need, minor changes can be quickly implemented, and compliance and security obligations are met.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Empirical assessment of security requirements and architecture: Lessons learned\n", "abstract": " Over the past three years, our groups at the University of Leuven and the University of Trento have been conducting a number of experimental studies. In particular, two common themes can be easily identified within our work. First, we have investigated the value of several threat modeling and risk assessment techniques. The second theme relates to the problem of preserving security over time, i.e., security evolution. Although the empirical results obtained in our studies are interesting on their own, the main goal of this chapter is to share our experience. The objective is to provide useful, hands-on insight on this type of research work so that the work of other researchers in the community would be facilitated. The contribution of this chapter is the discussion of the challenges we faced during our experimental work. Contextually, we also outline those solutions that worked out in our studies and could be reused\u00a0\u2026", "num_citations": "4\n", "authors": ["168"]}
{"title": "A Systematically Empirical Evaluation Of Vulnerability Discovery Models: A Study On Browsers' Vulnerabilities\n", "abstract": " A precise vulnerability discovery model (VDM) will provide a useful insight to assess software security, and could be a good prediction instrument for both software vendors and users to understand security trends and plan ahead patching schedule accordingly. Thus far, several models have been proposed and validated. Yet, no systematically independent validation by somebody other than the author exists. Furthermore, there are a number of issues that might bias previous studies in the field. In this work, we fill in the gap by introducing an empirical methodology that systematically evaluates the performance of a VDM in two aspects: quality and predictability. We further apply this methodology to assess existing VDMs. The results show that some models should be rejected outright, while some others might be adequate to capture the discovery process of vulnerabilities. We also consider different usage scenarios of VDMs and find that the simplest linear model is the most appropriate choice in terms of both quality and predictability when browsers are young. Otherwise, logistics-based models are better choices.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Secmer: A tool to gain control of security requirements evolution\n", "abstract": " This paper presents SeCMER, a tool for requirements evolution management developed in the context of the SecureChange project. The tool supports automatic detection of requirement changes and violation of security properties using change-driven transformations. The tool also supports argumentation analysis to check security properties are preserved by evolution and to identify new security properties that should be taken into account.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Computer-aided generation of enforcement mechanisms for error-tolerant policies\n", "abstract": " The basic tenet of security management when actions violate policies is that the former must be forbidden or amended. This requires to specify precisely all possible exceptions and corrections to the default workflow. In many practical e-health business processes this is not feasible: the default clinical or administrative protocol is simple and well understood by clinicians but the precise codification of all possible amendable errors into the policy would transform it from a straight-line to an unreadable spaghetti-graph. In this paper we propose a more practical alternative: the clinician only specifies the default protocol and marks for each protocol step the venial errors and their possible corrections. Given a global bound on the amount of errors in a trace that can be tolerated for each workflow execution, we can automatically generate an edit-automata that can provably enforce the policy with a sufficient degree of\u00a0\u2026", "num_citations": "4\n", "authors": ["168"]}
{"title": "Java card architecture for autonomous yet secure evolution of smart cards applications\n", "abstract": " Open multi-application smart cards that allow post-issuance evolution (i.e. loading of new applets) are very attractive for both smart card developers and card users. Since these applications contain sensitive data and can exchange information, a major concern is the assurance that these applications will not exchange data unless permitted by their respective policies. We suggest an approach for load time application certification on the card, that will enable the card to make autonomous decisions on application and policy updates while ensuring the compliance of every change of the platform with the security policy of each application\u2019s owner.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Goal-equivalent secure business process re-engineering for e-health\n", "abstract": " The introduction of ITs in e-Health often requires to re-engineer the business processes used to deliver care. Obviously the new and re-engineered processes are observationally different and thus we cannot use existing modelbased techniques to argue that they are somehow \u201cequivalent\u201d. In this paper we propose a notion of equivalence over secure business processes based on the notion of goal-equivalence:\u2013start from the old secure business process;\u2013reconstruct from that business process the functional and security requirements at organizational level that the old business process was supposed to meet (including the trust relations that existed among the members of the organization);\u2013compare the re-engineered business process with the requirements and see if they are equally met or possibly improved. To this intent, we present a reasoning method for passing from SI*, a modeling language that captures the functional, security and trust requirements of IT systems and their operational environments, to business processes specifications and vice versa. Both translation processes are complementary, in the sense that SI* models can have multiple business process concretizations, and different business processes can be equivalent in terms of the goals they achieve. We illustrate and motivate the proposed approach using an e-health case study.", "num_citations": "4\n", "authors": ["168"]}
{"title": "A privacy model to support minimal disclosure in virtual organizations\n", "abstract": " The last years have seen an increasing attention on privacy-aware technologies and mechanisms for the negotiation of private information between customers and enterprises. Unfortunately, current proposals are still unsatisfactory since they do not cover the entire spectrum of privacy management. Moreover, they do not provide support for emerging business models such as the inter-organizational business process (also known as virtual organizations). In this paper we propose a privacy model complying with the minimal disclosure principle when a coalition of organizations integrate their efforts to provide services to customers.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Lotrec: the generic tableau prover for modal and description logics\n", "abstract": " The last years have seen a renewed interest in modal and description logics (MDLs). Better algorithms, coding, and technology have led to effective systems based on tableau and constraint systems [6, 7] to DPLL-based implementations [5], first order provers [8] and the inverse method [13]. PSPACE problems such as satisfiability are within reach for realistic instances [10] and potentially EXPTIME problems stemming from real applications can also be solved [3, 7].", "num_citations": "4\n", "authors": ["168"]}
{"title": "Single step tableaux for modal logics\n", "abstract": " Single Step Tableaux (SST) are the basis of a calculus for modal logics which combines di erent features of sequent and pre xed tableaux into a simple, modular, strongly analytic and e ective calculus for a wide range of modal logics. The paper presents a number of the computational results about SST (con uence, decidability, space complexity, query combination etc.) and compares SST with other formalisms such as translation methods, modal resolution and Gentzen-type tableaux. For instance, it discusses the feasibility and unfeasibility of deriving decision procedures for SST and translation based methods by replacing loop checking techniques with simpler termination checks. The complexity of searching for validity and logical consequence with SST and other methods is discussed. Minimal conditions on SST search strategies are proven to require Pspace (and NPtime for S5 and KD45). The paper also presents the methodology underlying the construction of the correctness and completeness proofs.", "num_citations": "4\n", "authors": ["168"]}
{"title": "Cook and Reckhow are wrong: Subexponential tableau proofs for their family of formulae\n", "abstract": " . It is widely believed that a family\\Sigma n of unsatisfiable formulae defined by Cook and Reckhow [Proc. of the ACM Symp. on Theory of Comp. 1974] gives a lower bound of O (2 2 n) on the proof size with analytic tableaux. This claim plays a key role in the proof that tableaux cannot polynomially simulate tree resolution. We show that it is wrong by exhibiting an analytic tableau proof whose size has an upper bound of O (n\\Theta 2 n 2), which, although not polynomial in the size (2 n) of the input formula, is exponentially shorter than the claimed lower bound. We claim that the pitfall, in that and other papers, is due to the blurring of n-ary and binary versions of tableaux. 1 Introduction The study of upper and lower bounds on the proof size of propositional tautologies using resolution and tableaux played a major role in computer science since the ground breaking papers by Cook & Reckhow [1, 2]. This line of research has been quite fruitful in providing a sound computati...", "num_citations": "4\n", "authors": ["168"]}
{"title": "Optimisation of cyber insurance coverage with selection of cost effective security controls.\n", "abstract": " Nowadays, cyber threats are considered among the most dangerous risks by top management of enterprises. One way to deal with these risks is to insure them, but cyber insurance is still quite expensive. The insurance fee can be reduced if organisations improve their cyber security protection, i.e., reducing the insured risk. In other words, organisations need an investment strategy to decide the optimal amount of investments into cyber insurance and self-protection.In this work, we propose an approach to help a risk-averse organisation to distribute its cyber security investments in a cost-efficient way. What makes our approach unique is that next to defining the amount of investments in cyber insurance and self-protection, our proposal also explicitly defines how these investments should be spent by selecting the most cost-efficient security controls. Moreover, we provide an exact algorithm for the control selection\u00a0\u2026", "num_citations": "3\n", "authors": ["168"]}
{"title": "Governance Challenges for European Cybersecurity Policies: Stakeholder Views\n", "abstract": " We outline possible approaches to cybersecurity governance and compare them against the proposed European Union network of competence centers. We survey stakeholders for their opinions about the centers and analyze the results.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Blockchain-based Invoice Factoring: from business requirements to commitments.\n", "abstract": " Europe has the largest global invoice factoring market for over 1.6 BEuro. The purpose of a factoring market is to address delay in payments of commercial invoices by buyers of good and services: sellers bring their still-to-be-paid invoices to financial organizations (factors) which provides an advance payment. The market further growth is hampered by a number of security issues such as the impossibility of factors to check whether an invoice has been already factored (double pledging). A global organization collecting all factored invoices could be a solution but all stakeholders (banks, factors, etc.) have various reasons to not wanting to share such data. In this scenario a distributed, blockchain-based implementation is the only way forward, We describe the security requirements and all key operations for a secure, fully distributed Invoice Factoring Market, hereafter referred to as simply the \u2018Market\u2019. Our distributed, asynchronous protocol simulates the centralized functionality under the assumption of the availability of a distributed ledger. We consider security with abort (in absence of honest majority).", "num_citations": "3\n", "authors": ["168"]}
{"title": "TestREx: a framework for repeatable exploits\n", "abstract": " Web applications are the target of many well-known exploits and also a fertile ground for the discovery of security vulnerabilities. Yet, the success of an exploit depends both on the vulnerability in the application source code and the environment in which the application is deployed and run. As execution environments are complex (application servers, databases and other supporting applications), we need to have a reliable framework to test whether known exploits can be reproduced in different settings, better understand their effects, and facilitate the discovery of new vulnerabilities. In this paper, we present TestREx\u2014a framework that allows for highly automated, easily repeatable exploit testing in a variety of contexts, so that a security tester may quickly and efficiently perform large-scale experiments with vulnerability exploits. It supports packing and running applications with their environments, injecting\u00a0\u2026", "num_citations": "3\n", "authors": ["168"]}
{"title": "Preliminary experiments on the relative comprehensibility of tabular and graphical risk models\n", "abstract": " The ATM SESAR projects have invested a significant effort to define, besides tabular representations, graphical modeling notations to capture ATM architectural elements. A key question is whether this is worth the effort for security risk assessment. It is important to understand which representation provides better comprehension of threats, vulnerabilities, security countermeasures, as well as the relationships between them. In this paper we present a preliminary study on the comprehensibility of two risk modeling notations, involving students from Trento and Oslo universities. In particular, we assessed the effect of using graphical or tabular modeling notation on the actual comprehension of security risk models. The subjects were asked to answer eight comprehension questions about the risk assessment concepts (like threats, vulnerabilities or controls) represented using graphical or tabular notation. The results of the data analysis show no significant difference in actual comprehension. Further studies are required to strengthen the statistical significance and to investigate the extent to which the findings are relevant for more general contexts.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Evaluation of Airport Security Training Programs: Perspectives and Issues\n", "abstract": " While many governments and airport operators have emphasized the importance of security training and committed a large amount of budget to security training programs, the implementation of security training programs was not proactive but reactive. Moreover, most of the security training programs were employed as a demand or a trend-chasing activity from the government. In order to identify issues in airport security training and to develop desirable security training procedures in an airport, this preliminary study aims at providing (1) the description of current state of airport security training and training in general, (2) the study design and interview guide for studying airport security training, and (3) expected outcome from the study.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Load time code validation for mobile phone Java Cards\n", "abstract": " Over-the-air (OTA) application installation and updates have become a common experience for many end-users of mobile phones. In contrast, OTA updates for applications on the secure elements (such as smart cards) are still hindered by the challenging hardware and certification requirements.The paper describes a security framework for Java Card-based secure element applications. Each application can declare a set of services it provides, a set of services it wishes to call, and its own security policy. An on-card checker verifies compliance and enforces the policy; thus an off-card validation of the application is no longer required.The framework has been optimized in order to be integrated with the run-time environment embedded into a concrete card. This integration has been tried and tested by a smart card manufacturer. In this paper we present the architecture of the framework and provide the implementation\u00a0\u2026", "num_citations": "3\n", "authors": ["168"]}
{"title": "MAP-REDUCE runtime enforcement of information flow policies\n", "abstract": " We propose a flexible framework that can be easily customized to enforce a large variety of information flow properties. Our framework combines the ideas of secure multi-execution and map-reduce computations. The information flow property of choice can be obtained by simply changes to a map (or reduce) program that control parallel executions. We present the architecture of the enforcement mechanism and its customizations for non-interference (NI) (from Devriese and Piessens) and some properties proposed by Mantel, such as removal of inputs (RI) and deletion of inputs (DI), and demonstrate formally soundness and precision of enforcement for these properties.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Empirical validation of security methods\n", "abstract": " Copyright\u00a9 by the paper's authors. Security requirements engineering is an important part of many software projects. Practitioners consider security requirements from the early stages of software development processes, but most of them do not use any formal method for security requirements engineering. According to a recent survey, only about 9% security practitioners implement formal process of elicitation and analysis of security requirements and risks. However, a number of methods have been recently proposed in academia to support practitioners in collecting and analysing security requirements. Unfortunately, these methods are not widely adopted in practice because there is a lack of empirical evidence that they work. Only few papers in requirements engineering have a solid empirical evidence of efficiency of proposed solutions. So how can we know that security methods work in practice? In this paper we propose to conduct a series of empirical studies to build a basis that a) will provide security practitioners with guidelines for selection of security requirements methods, and b) will help methods designer understand how to improve their methods.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Unicorn: A tool for modeling and reasoning on the uncertainty of requirements evolution\n", "abstract": " Long-living software systems keep evolving to satisfy changes in their working environment. New requirements may arise, while current requirements may become obsoleted. Such requirements evolution fortunately could be foreseen at some level of (un) certainty. The paper presents UNICORN, a CASE tool for modeling and reasoning on the uncertainty of requirements evolution. The tool provides graphical constructs as well as different views of requirements evolution to assist users to model requirements evolution. The tool also supports the evolution analysis in which facilitate the selection of design alternative.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Attacker economics for internet-scale vulnerability risk assessment\n", "abstract": " Vulnerability risk assessment is a crucial process in security management, and the CVSS score is the standard-de-facto risk metric for software vulnerabilities. In this manuscript I show that current risk assessment methodologies do not fit real\" in the wild\" attack data. I also present my three-steps plan to identify an Internet-scale risk assessment methodology that accounts for attacker economics and opportunities. Eventually, I want to provide answers like the following:\" If we deploy this security measure, the fraction of our users affected by this type of cyber attacks will be less than X%\".", "num_citations": "3\n", "authors": ["168"]}
{"title": "Crime pays if you are only an average hacker\n", "abstract": " This study investigates the effects of incentive and deterrence strategies that might turn a security researcher into a malware writer, or vice versa. By using a simple game theoretic model, we illustrate how hackers maximize their expected utility. Furthermore, our simulation models show how hackers' malicious activities are affected by changes in strategies employed by defenders. Our results indicate that, despite the manipulation of strategies, average-skilled hackers have incentives to participate in malicious activities, whereas highly skilled hackers who have high probability of getting maximum payoffs from legal activities are more likely to participate in legitimate ones. Lastly, according on our findings, we found that reactive strategies are more effective than proactive strategies in discouraging hackers' malicious activities.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Engineering secure future internet services\n", "abstract": " In this paper we analyze the need and the opportunity for establishing a discipline for engineering secure Future Internet Services, typically based on research in the areas of software engineering, of service engineering and security engineering. Generic solutions that ignore the characteristics of Future Internet services will fail, yet it seems obvious to build on best practices and results that have emerged from various research communities.The paper sketches various lines of research and strands within each line to illustrate the needs and to sketch a community wide research plan. It will be essential to integrate various activities that need to be addressed in the scope of secure service engineering into comprehensive software and service life cycle support. Such a life cycle support must deliver assurance to the stakeholders and enable risk and cost management for the business stakeholders in particular. The paper should be considered a call for contribution to any researcher in the related sub domains in order to jointly enable the security and trustworthiness of Future Internet services.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Infringo ergo sum: when will software engineering support infringements?\n", "abstract": " Once upon a time a professor of computing and a father was complaining at a radiology ward. A CD with the X-rays of his son's chest had garbled images. Unfortunately, the CD burning process has been outsourced and, in compliance with e-health security policies, technicians could not see the images on the system. Only doctors could. The nurse had a decision to make: sidestep the father (send him away with empty hands to the pneumology ward) or sidestep the system (give the technician the doctor's password and thus the ability to access all images and not just this one). As a father he was happy of her decision. As a professor, this knowledge was of meager and unsatisfactory kind.", "num_citations": "3\n", "authors": ["168"]}
{"title": "A dynamic security framework for ambient intelligent systems: a smart-home based eHealth application\n", "abstract": " Providing context-dependent security services is an important challenge for ambient intelligent systems. The complexity and the unbounded nature of such systems make it difficult even for the most experienced and knowledgeable security engineers, to foresee all possible situations and interactions when developing the system. In order to solve this problem context based self- diagnosis and reconfiguration at runtime should be provided.             We present in this paper a generic security and dependability framework for the dynamic provision of Security and Dependability (S&D) solutions at runtime. Through out the paper, we use a smart items based e-health scenario to illustrate our approach. The eHealth prototype has been implemented and demonstrated in many scientific and industrial events.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Security views for outsourced business processes\n", "abstract": " The workflow of a Virtual Organization is often divided into fragments that are run by different entities having different clearance level or accessibility permissions. Therefore, an important issue is a decomposition of the overall business process into workflow views that can be outsourced to the side of the corresponding contractors. In this paper, we introduce the notion of business process security view and present an algorithm for the automatic derivation of such views from a security specification that may express conditional accessibility based on the actual data flowing across business process. Our solution borrows the idea of virtual views from relational database views. We also discuss an architecture and an implementation for workflow view synchronization.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Do you really mean what you actually enforced? edit automata revisited\n", "abstract": " In the landmark paper on the theoretical side of Polymer, Ligatti and his co-authors have identied a new class of enforcement mechanisms based on the notion of edit automata, that can transform sequences and enforce more than simple safety properties. We show that there is a gap between the edit automata that one can possibly write (e.g. by Ligatti et al in their running example) and the edit automata that are actually constructed according the theorems from Ligatti's IJIS paper and IC follow-up paper by Talhi et al. \"Ligatti's automata\" are just a particular kind of edit automata. Thus, we re-open a question which seemed to have received a denitive answer: you have written your security enforcement mechanism (aka your edit automata); does it really enforce the security policy you wanted?", "num_citations": "3\n", "authors": ["168"]}
{"title": "From hippocratic databases to secure tropos: a computer-aided re-engineering Approach\n", "abstract": " this paper is to propose a re-engineering approach and algorithms for automatically extracting privacy requirements from policy statements stored in existing Hippocratic databases. These are then represented in a Requirements Engineering framework where tools are available for formal analysis. Specifically, we aim to re-model privacy concerns captured in Hippocratic databases in Secure Tropos and check for their consistency. This approach has two advantages. Firstly, it provides a representation of the enterprise privacy policy in a modeling framework where formal tools are available for model checking (see Ref. 16). Secondly, it o# ers a unifying view of systems built using a structured Requirements Engineering methodology such as Tropos/i* or KAOS and systems directly implemented as Hippocratic databases. Thus, di# erent design decisions can be compared at a level suitable for the designer", "num_citations": "3\n", "authors": ["168"]}
{"title": "Abduction and deduction in logic programming for access control for autonomic systems\n", "abstract": " Autonomic communication and computing is the new paradigm for dynamic service integration over a network. An autonomic network crosses organizational and management boundaries and is provided by entities that see each other just as partners that need to collaborate with little known or even unknown parties. Policy-based network access and management already requires a paradigm shift in the access control mechanism: from identity-based access control to trust management and negotiation, but even this is not enough for cross-organizational autonomic communication. For many services no autonomic partner may guess a priori what will be sent by clients and clients may not know a priori what credentials are demanded for completing a service, which may require the orchestration of many different autonomic nodes. To solve this problem we propose to use interactive access control: servers should be able to get back to clients asking for missing or excessing credentials, whereas the latter may decide to supply or decline requested credentials and so on until a final decision is taken. This proposal is grounded in a formal model on policy-based access control. It identifies the formal reasoning services of deduction, abduction and consistency checking that characterize the problem. It proposes two access control algorithms for stateless and stateful autonomic services and shows their completeness and correctness.", "num_citations": "3\n", "authors": ["168"]}
{"title": "A semantics and a calculi for reasoning about credential-based systems\n", "abstract": " Reasoning about credential-based systems such as SDSI, SPKI is one of today\u2019s security challenges. The representation and reasoning problem for this (simple) public key infrastructure is challenging: we need to represent permissions, naming and identities of agents and complex naming constructions (Blackburn\u2019s office-mate is M4M\u2019s PC-Chair\u2019s Colleague), then we need to reason about intervals of time and metric time for expiration dates and validity intervals.One of the limitation of many formalizations is their folding on Lampson and Rivest\u2019s SDSI and SPKI, the major goal being to show that the proposed logics and semantics captured exactly SPKI behavior or were better in this or that respect. What we find missing is what Syverson termed an\u201d independently motivated semantics\u201d. A semantics where models fitting SDSI would just be a particular subset of logical models and where other proposals could be equally well accomodated.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Breaking security protocols as an AI planning problem\n", "abstract": " Properties like confidentiality, authentication and integrity are of increasing importance to communication protocols. Hence the development of formal methods for the verification of security protocols. This paper proposes to represent the verification of security properties as a (deductive or model-based) logical AI planning problem. The key intuition is that security attacks can be seen as plans. Rather then achieving \u201cpositive\u201d goals a planner must exploit the structure of a security protocol and coordinate the communications steps of the agents and the network (or a potential enemy) to reach a security violation.             The planning problem is formalized with a variant of dynamic logic where actions are explicit computation (such as cryptanalyzing a message) and communications steps between agents. A theory of computational properties is then coupled with a description of the particular communication\u00a0\u2026", "num_citations": "3\n", "authors": ["168"]}
{"title": "A proof theory for tractable approximations of propositional reasoning\n", "abstract": " This paper proposes an uniform framework for the proof theory of tractable approximations of propositional reasoning. The key idea is the introduction of approximate proofs. This makes possible the development of an approximating sequent calculus for propositional deduction where proofs can be sound, complete or multi-directional approximations of classical logic. We show how this calculus subsumes existing approaches to approximation such as the BCP - k family of anytime reasoners by Dalal and S - 1, S - 3 entailments by Cadoli and Schaerf.", "num_citations": "3\n", "authors": ["168"]}
{"title": "How to get better eid and trust services by leveraging eidas legislation on eu funded research results. 2013\n", "abstract": " This work has been partly funded by the European Commission under the FP7 SecCord Project. Opinions expressed here are not necessarily endorsed by the European Commission. The authors would like to thank the coordinators and technical leaders of the EU FP6 and FP7 Research projects on Security and Trust mentioned in this report (ABC4Trust, ASSERT4SOA, CUMULUS, GEMOM, INTERSECTION, MASTER, OpenTC, PICOS, PrimeLife, RASEN, SECONOMICS, SEPIA, SHIELDS, STORK, STORK 2.0, TURBINE, TRESPASS, WOMBAT) for providing information on their research results and their potential impact. Discussions with Ross Anderson, J\u00f6rg Schwenk, Amelia Andersdotter, Arnd Weber, Aljosa Pasic, and Massimo Felici were helpful to shape some of the issues in this report.", "num_citations": "3\n", "authors": ["168"]}
{"title": "Practical witness-key-agreement for blockchain-based dark pools financial trading\n", "abstract": " We introduce a new cryptographic scheme, Witness Key Agreement (WKA), that allows a party to securely agree on a secret key with a counter party holding publicly committed information only if the counter party also owns a secret witness in a desired (arithmetic) relation with the committed information.", "num_citations": "2\n", "authors": ["168"]}
{"title": "On the effort for security maintenance of free and open source components\n", "abstract": " The work presented in this paper is motivated by the need to estimate the security effort of maintaining Free and Open Source Software (FOSS) components within the software supply chain of a large international software vendor. We investigated publicly available factors (from number of active users to commits, from code size to usage of popular programming languages, etc.) to identify which ones impact three potential effort models: Centralized (the company checks each component and propagates changes to the product groups), Distributed (each product group is in charge of evaluating and fixing its consumed FOSS components), and Hybrid (seldom used components are checked individually by each development team, the rest is centralized). We use Grounded Theory to extract the factors from a six months study at the vendor. We report the results on a sample of 152 FOSS components used by the vendor.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Action, inaction, trust, and cybersecurity's common property problem\n", "abstract": " Cybersecurity tends to be viewed as a highly dynamic, continually evolving technology race between attacker and defender. However, economic theory suggests that in many cases doing \"nothing\" is the optimal strategy when substantial fixed adjustment costs are present. Indeed, the authors' anecdotal experience as chief information security officers indicates that uncertain costs that might be incurred by rapid adoption of security updates substantially delay the application of recommended security controls, so the industry does appear to understand this economic aspect quite well. From a policy perspective, the inherently discontinuous adjustment path taken by firms can cause difficulties in determining the most effective public policy remit and the effectiveness of any enacted policies ex post. This article summarizes this type of policy issue in relation to the contemporary cybersecurity agenda.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Controlling application interactions on the novel smart cards with Security-by-Contract\n", "abstract": " In this paper we investigate novel use cases for open multi-application smart card platforms. These use cases require a fine-grained access control mechanism to protect the sensitive functionality of on-card applications. We overview the Security-by-Contract approach that validates at load time that the application code respects the interaction policies of other applications already on the card, and discuss how this approach can be used to address the challenging change scenarios in the target use cases.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Security of the OSGi platform\u22c6\n", "abstract": " In the last few years we have seen how increasing computational power of electronic devices triggers the functionality growth of the software that runs on them. The natural consequence is that modern software is no longer single-pieced, it becomes, instead, the composition of autonomous components that run on the shared platform. The examples of such platforms are web browsers (such as Google Chrome), smartphone and smart card operating systems (eg, Android and Java Card), intelligent vehicle systems or smart homes (usually implemented on OSGi). On one hand, these platforms protect components by isolation, but at the same time, provide methods to share and exchange services. If the components can come from different stakeholders, how do we make sure that one\u2019s services would only be invoked by one\u2019s authorized siblings? In this PhD proposal we illustrate the problems on the example of OSGi platform. We propose to use the security-by-contract methodology (S\u00d7 C) for loading time security verification to separate the security from the business logic while controlling access to applications.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Supporting Software Evolution for Open Smart Cards by Security-by-Contract\n", "abstract": " If all applications could be loaded at the start this would boil down to information flow analysis for which many solutions exist, but this is precisely what we want to overcome. When applications are not known in advance and can be updated asynchronously and possibly without connection to trusted third parties, we must preserve the security policies of the various owners of the applets during such autonomous evolution. This chapter illustrates the extension of the Security-by-Contract approach from mobile phones to smart cards: Security-by-Contract is based on the loading time application certification on the card that will enable the card to make autonomous decisions on application and policy updates while ensuring the compliance of every change of the platform with the security policy of each application\u2019s owner.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Safety and Security in Multiagent Systems: Research Results from 2004-2006\n", "abstract": " As intelligent autonomous agents and multiagent system applications become more pervasive, it becomes increasingly important to understand the risks associated with using these systems. Incorrect or inappropriate agent behavior can have harmful effects, including financial cost, loss of data, and injury to humans or systems. For example, NASA has proposed missions where multiagent systems, working in space or on other planets, will need to do their own reasoning about safety issues that concern not only themselves but also that of their mission. Likewise, industry is interested in agent systems that can search for new supply opportunities and engage in (semi-) automated negotiations over new supply contracts. These systems should be able to securely negotiate such arrangements and decide which credentials can be requested and which credentials may be disclosed. Such systems may encounter\u00a0\u2026", "num_citations": "2\n", "authors": ["168"]}
{"title": "Quantitative Assessment for Organisational Security & Dependability\n", "abstract": " There are numerous metrics proposed to assess security and dependability of technical systems (e.g., number of defects per thousand lines of code). Unfortunately, most of these metrics are too low-level, and lack on capturing high-level system abstractions required for organisation analysis. The analysis essentially enables the organisation to detect and eliminate possible threats by system re-organisations or re-configurations. In other words, it is necessary to assess security and dependability of organisational structures next to implementations and architectures of systems. This paper focuses on metrics suitable for assessing security and dependability aspects of a socio-technical system and supporting decision making in designing processes. We also highlight how these metrics can help in making the system more effective in providing security and dependability by applying socio-technical solutions (i.e\u00a0\u2026", "num_citations": "2\n", "authors": ["168"]}
{"title": "Optimizing IRM with Automata Modulo Theory\u22c6\n", "abstract": " Inlined Reference Monitor (IRM) is a flexible mechanism to enforce the security of untrusted applications. One of the shortcomings of IRM is that it might introduce a significant overhead in otherwise perfectly secure application. In this paper we propose six different framework models for IRM optimization with respect to components that are needed to be trusted or untrusted. Then, we describe an approach for IRM optimization using automata modulo theory. The key idea is that given a policy that represents the desired security behavior of a platform to be inlined, we compute an optimized policy with respect to the (trusted) claims on the security behavior of a application. The optimized policy is the one to be injected into the untrusted code.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Reasoning about naming and time for credential-based systems\n", "abstract": " Reasoning about trust management and credential-based systems such as SDSI/SPKI, is one of today\u2019s security challenges. The representation and reasoning problem for this (simple) public key infrastructure is challenging: we need to represent permissions, complex naming constructions (\u201cMartinelli\u2019s officemate is FAST\u2019s PC-Chair\u2019s Colleague\u201d), intervals of time and metric time for expiration dates and validity intervals.Such problem is only partly solved by current approaches. At first because they focus on Lamport and Rivest\u2019s SDSI and SPKI, the major goal being to show that the proposed logics and semantics captured exactly SPKI behavior or were better in this or that respect. Second, reasoning about time is missing. Complicated logics and algorithms are put in place for name resolution but it is always assumed that just the valid credentials are evaluated. What we find missing is what Syverson termed an \u201cindependently motivated semantics\u201d. Here, we propose such a semantics with annexed logical calculi. The semantics has a natural intuitive interpretation and in particular can represent timing constraints, intersection of validity intervals and naming at the same time. We also provide a logical calculus based on semantic tableaux with the appealing feature that the verification of credentials allows for the direct construction of a counter-model in the semantics when invalid requests are made. This combines semantic tableau method for modal and description logics with systems for reasoning about interval algebra with both qualitative and metric constraints.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Multisession Monitor for .NET Mobile Applications: Theory & Implementation\n", "abstract": " Future mobile platforms will be characterized by pervasive client downloads. Users would like to download new (untrusted) applications on the spot in order to exploit the computational power of their mobile devices to make a better use of the services available in the environment. Such business model is not adequately supported by the current mobile security architecture and our article aims at extending the scope of security monitoring as a viable alternative. Prior work provided solutions to monitoring single instances of applications or to monitor all instances. The novelty of the proposed approach is that it allows to combine the both approaches, an ability that is needed to cope with many realistic policies.", "num_citations": "2\n", "authors": ["168"]}
{"title": "Trust management\n", "abstract": " Pretty Good Privacy (PGP) is based on a web of trust. Everyone may issue public-key certificates. Each user specifies a level of trust in each issuer. Each user specifies the total confidence needed for a public-key\u2194 name relationship to be considered valid. Example: one certificate from an issuer trusted at level 10, or certificates from two issuers trusted at level 5 or higher.", "num_citations": "2\n", "authors": ["168"]}
{"title": "How to Model (and Simplify) the SET Payment Phase for Automated Verification\n", "abstract": " Formal verification of real-world e-commerce protocols such as SET is hindered by the sheer complexity of their descriptions. In this paper we build upon the results of Bella, Massacci, Paulson and Tramontano [ESORICS 00] and propose a number of progressively simplified models of SET Payment Phase. We discuss the rationale behind each simplification step and the potential impact on verification.", "num_citations": "2\n", "authors": ["168"]}
{"title": "TaintBench: Automatic real-world malware benchmarking of Android taint analyses\n", "abstract": " Due to the lack of established real-world benchmark suites for static taint analyses of Android applications, evaluations of these analyses are often restricted and hard to compare. Even in evaluations that do use real-world apps, details about the ground truth in those apps are rarely documented, which makes it difficult to compare and reproduce the results. To push Android taint analysis research forward, this paper thus recommends criteria for constructing real-world benchmark suites for this specific domain, and presents TaintBench, the first real-world malware benchmark suite with documented taint flows. TaintBench benchmark apps include taint flows with complex structures, and addresses static challenges that are commonly agreed on by the community. Together with the TaintBench suite, we introduce the TaintBench framework, whose goal is to simplify real-world benchmarking of Android taint analyses\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "LastPyMile: identifying the discrepancy between sources and packages\n", "abstract": " Open source packages have source code available on repositories for inspection (eg on GitHub) but developers use pre-built packages directly from the package repositories (such as npm for JavaScript, PyPI for Python, or RubyGems for Ruby). Such convenient practice assumes that there are no discrepancies between source code and packages. These differences pose both operational risks (eg making dependent projects unable to compile) and security risks (eg deploying malicious code during package installation) in the software supply chain. Our empirical assessment of 2438 popular packages in PyPI with an analysis of around 10M lines of code shows several differences in the wild: modifications cannot be just attributed to malicious injections. Yet, scanning again all and whole \u2018most likely good but modified\u2019packages is hard to manage for FOSS downstream users. We propose a methodology, LastPyMile\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Secure Software Development in the Era of Fluid Multi-party Open Software and Services\n", "abstract": " Pushed by market forces, software development has become fast-paced. As a consequence, modern development projects are assembled from 3rd-party components. Security & privacy assurance techniques once designed for large, controlled updates over months or years, must now cope with small, continuous changes taking place within a week, and happening in sub-components that are controlled by third-party developers one might not even know they existed. In this paper, we aim to provide an overview of the current software security approaches and evaluate their appropriateness in the face of the changed nature in software development. Software security assurance could benefit by switching from a process-based to an artefact-based approach. Further, security evaluation might need to be more incremental, automated and decentralized. We believe this can be achieved by supporting mechanisms for\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Technical leverage in a software ecosystem: Development opportunities and security risks\n", "abstract": " In finance, leverage is the ratio between assets borrowed from others and one\u2019s own assets. A matching situation is present in software: by using free open-source software (FOSS) libraries a developer leverages on other people\u2019s code to multiply the offered functionalities with a much smaller own codebase. In finance as in software, leverage magnifies profits when returns from borrowing exceed costs of integration, but it may also magnify losses, in particular in the presence of security vulnerabilities. We aim to understand the level of technical leverage in the FOSS ecosystem and whether it can be a potential source of security vulnerabilities. Also, we introduce two metrics change distance and change direction to capture the amount and the evolution of the dependency on third-party libraries.The application of the proposed metrics on 8494 distinct library versions from the FOSS Maven-based Java libraries shows\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "SolarWinds and the Challenges of Patching: Can We Ever Stop Dancing With the Devil?\n", "abstract": " SC A NNIO www. computer. org/security 15 and move responsibility for confining security spillovers to vendors. This would push our community toward better solutions\u2014which we do have but that are less convenient for software companies. Updates are bundled in the interest of vendors, and by adding functionalities, new vulnerabilities are introduced. One could illustrate this with forced updates from Microsoft, Google, Apple, and Facebook, but we will stick to SolarWinds to keep the discussion focused. See \u201cThe SolarWinds Patching Schedule and Its Demise\u201d for a summary; Table S1 provides us with a schedule of updates. Focus on versions 2019.4 and 2019.2. They are not vulnerable to the SUNBURST malicious code. If a user did not need any of the (nine out of 38) products subject to hotfix 5, he or she might not have required an update and thus would have avoided trouble (some users even complained that\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Preliminary Findings on FOSS Dependencies and Security: A Qualitative Study on Developers\u2019 Attitudes and Experience\n", "abstract": " Developers are known to keep third-party dependencies of their projects outdated even if some of them are affected by known vulnerabilities. In this study we aim to understand why they do so. For this, we conducted 25 semi-structured interviews with developers of both large and small-medium enterprises located in nine countries. All interviews were transcribed, coded, and analyzed according to applied thematic analysis. The results of the study reveal important aspects of developers\u2019 practices that should be considered by security researchers and dependency tool developers to improve the security of the dependency management process.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Distributed Financial Exchanges: Security Challenges and Design Principles\n", "abstract": " Implementing secure, distributed, and economically viable financial exchanges radically challenges traditional constructs such as zero knowledge and secure multiparty computation. To boost discussions of such practical challenges, we enucleate the design principles to build a secure, distributed futures exchange.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Selecting a Secure Cloud Provider\u2014An Empirical Study and Multi Criteria Approach\n", "abstract": " Security has become one of the primary factors that cloud customers consider when they select a cloud provider for migrating their data and applications into the Cloud. To this end, the Cloud Security Alliance (CSA) has provided the Consensus Assessment Questionnaire (CAIQ), which consists of a set of questions that providers should answer to document which security controls their cloud offerings support. In this paper, we adopted an empirical approach to investigate whether the CAIQ facilitates the comparison and ranking of the security offered by competitive cloud providers. We conducted an empirical study to investigate if comparing and ranking the security posture of a cloud provider based on CAIQ\u2019s answers is feasible in practice. Since the study revealed that manually comparing and ranking cloud providers based on the CAIQ is too time-consuming, we designed an approach that semi-automates the selection of cloud providers based on CAIQ. The approach uses the providers\u2019 answers to the CAIQ to assign a value to the different security capabilities of cloud providers. Tenants have to prioritize their security requirements. With that input, our approach uses an Analytical Hierarchy Process (AHP) to rank the providers\u2019 security based on their capabilities and the tenants\u2019 requirements. Our implementation shows that this approach is computationally feasible and once the providers\u2019 answers to the CAIQ are assessed, they can be used for multiple CSP selections. To the best of our knowledge this is the first approach for cloud provider selection that provides a way to assess the security posture of a cloud provider in practice. View Full-Text", "num_citations": "1\n", "authors": ["168"]}
{"title": "Cyber insurance and time-to-compromise: An integrated approach\n", "abstract": " Fast-growing numbers of technologies and devices make cyber security landscape more complicated and require more accurate models. This complexity challenges cyber security experts to devise a better solution to manage cyber risks. One of the promising methods is to find the best distribution of security expenditure for risk mitigation and transfer (i.e. cyber insurance) options. In this work, we propose a solution to find the optimal security investments when there is a cyber insurance option by applying a time-to-compromise metric to the probability of attack computation. In particular, we find the best set of countermeasures which decreases the maximum number of vulnerabilities to increase the required time to compromise a system. Our approach is based on a multiple-objective knapsack problem for the selection of countermeasures and we find the best distribution of security expenditure by computing the time\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Non-monotonic Security Protocols and Failures in Financial Intermediation\n", "abstract": " Security Protocols as we know them are monotonic: valid security evidence (e.g. commitments, signatures, etc.) accrues over protocol steps performed by honest parties. Once\u2019s Alice proved she has an authentication token, got some digital cash, or casted a correct vote, the protocol can move on to validate Bob\u2019s evidence. Alice\u2019s evidence is never invalidated by honest Bob\u2019s actions (as long as she stays honest and is not compromised). Protocol failures only stems from design failures or wrong assumptions (such as Alice\u2019s own misbehavior). Security protocol designers can then focus on preventing or detecting misbehavior (e.g. double spending or double voting).               We argue that general financial intermediation (e.g. Market Exchanges) requires us to consider new form of failures where honest Bob\u2019s actions can make honest good standing. Security protocols must be able to deal with non-monotonic\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "On the Effort for Security Maintenance of Open Source Components\n", "abstract": " The work presented in this paper is motivated by the need to estimate the security effort of maintaining Free and Open Source Software (FOSS) components within the software supply chain of a large international software vendor. We investigated publicly available factors (from number of active users to commits, from code size to usage of popular programming languages, etc.) to identify which ones impact three potential effort models: Centralized (the company checks each component and propagates changes to the product groups), Distributed (each product group is in charge of evaluating and fixing its consumed FOSS components), and Hybrid (seldom used components are checked individually by each development team, the rest is centralized). We use Grounded Theory to extract the factors from a six months study at the vendor. We report the results on a sample of 152 FOSS components used by the vendor.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Cyberinsurance and Public Policy: Self-Protection and Insurance with Endogenous Security Risks\n", "abstract": " Corporate insurance contracts providing liability coverage in the event of an information security breach are increasingly popular. In addition to the obvious use of \u2018Cyberinsurance\u2019as a risk mitigation tool, a public policy narrative has emerged whereby insurance companies act as a clearing house for information and then provide guidance on appropriate security investment to \ufb01rms seeking liability coverage. Utilizing few assumptions, our modeling framework demonstrates that this view of cyberinsurance as a delegated policy tool is unlikely to yield the anticipated coordination bene\ufb01ts, and may in fact erode the aggregate level of security investment undertaken by targets.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Hansen-Scheinkman Factorization and Ross Recovery from Option Panels\n", "abstract": " Determining the transition matrix of a discrete Markov process from sequential forecasts of smoothed density functions is an important element of many problems in decision theory and economics. Recent theoretical results have demonstrated that the Perron-Frobenius eigenfunction of a Markov risk neutral state price transition matrix has an interesting economic interpretation and could permit the extraction of physical forward pricing densities from options markets. Yet, the application to actual market prices is challenging. For instance, even at the intraday frequency, option market panels contain substantial gaps and can contain unpredictable levels of noise across strike prices and tenors. This paper derives an exact nonlinear programming framework utilizing the properties of the Drazin inverse of an irreducible matrix. Simulation and fit to actual data demonstrates the consistency and usefulness of the technique.", "num_citations": "1\n", "authors": ["168"]}
{"title": "The work-averse cyber attacker model\n", "abstract": " The typical cyber attacker is assumed to be all powerful and to exploit all possible vulnerabilities. In this paper we present, and empirically validate, a novel and more realistic attacker model. The intuition of our model is that an attacker will optimally choose whether to act and weaponize a new vulnerability, or keep using existing toolkits if there are enough vulnerable users. The model predicts that attackers may i) exploit only one vulnerability per software version, ii) include only vulnerabilities with low attack complexity, and iii) be slow at introducing new vulnerabilities into their arsenal. We empirically test these predictions by conducting a natural experiment on attack data collected against more than one million real systems from Symantec\u2019s WINE platform. Our analysis shows that mass attackers\u2019 fixed costs are indeed significant and that substantial efficiency gains can be made by individuals and organizations by\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Poster: Analysis of exploits in the wild\n", "abstract": " Vulnerability exploitation is a major threat vector for cyber attacks [2], making vulnerability assessment a crucial moment in the security management process. The \u201ccriticality\u201d of a vulnerability is often expressed in the classic form Risk= Impact\u00d7 Likelihood [6]. The more high-risk vulnerabilities affect a system, the higher its final risk assessment. In this manuscript, we identify and analyse two major shortcomings in current software risk assessment approaches. Problem 1. Worrying about every vulnerability. Vulnerability assessment is founded on the classical view of security and is notoriously synthesised in Schneier\u2019s quote \u201csecurity is only as strong as the weakest link\u201d 1. In turn, this derives directly from the classic model of the attacker, assumed to be very powerful [4]. Some variations to this model exist, but the baseline remains the same: if a vulnerability is in my system, then an attacker will, sooner or later, exploit it. However, this is in contrast with trends in attacks reported in literature. For example, cybercrime attack tools such as exploit kits [1] represent two thirds of the threats for the final user [8], and yet they feature about 10 vulnerabilities each, some of which are five years old. If this observation was to hold for most exploits, it may be that the typical attacker is not as powerful as currently assumed to be; this would radically affect current mitigation and remediation strategies. Problem 2. Reliance on a never-checked assessment methodology. The CVSS scoring system [6] is the standardde-facto framework for vulnerability assessment. For example, the US Government SCAP protocol [7] recommends to use it to optimise patching strategies\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Crime Pays If You Are Just an Average Hacker\n", "abstract": " This study investigates the effects of incentive and deterrence strategies that might turn a security researcher into a malware writer, or vice versa. By using a simple game theoretic model, we illustrate how hackers maximize their expected utility. Furthermore, our simulation models show how hackers\u2019 malicious activities are affected by changes in strategies employed by defenders. Our results indicate that, despite the manipulation of strategies, average-skilled hackers have incentives to participate in malicious activities, whereas highly skilled hackers who have high probability of getting maximum payoffs from legal activities are more likely to participate in legitimate ones. Lastly, according on our findings, we found that reactive strategies are more effective than proactive strategies in discouraging hackers\u2019 malicious activities.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Managing changes with legacy security engineering processes\n", "abstract": " Managing changes in Security Engineering is a difficult task: the analyst must keep the consistency between security knowledge such as assets, attacks and treatments to stakeholders' goals and security requirements. Research-wise the usual solution is an integrated methodology in which risk, security requirements and architectural solutions are addressed within the same tooling environment and changes can be easily propagated. This solution cannot work in practice as the steps of security engineering process requires to use artefacts (documents, models, data bases) and manipulate tools that are disjoint and cannot be fully integrated for a variety of reasons (separate engineering domains, outsourcing, confidentiality, etc.). We call such processes legacy security engineering processes. In this paper, we propose a change management framework for legacy security engineering processes. The key idea is to\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Extending security-by-contract with quantitative trust on mobile devices\n", "abstract": " Security-by-Contract (S\u00d7 C) is a paradigm providing security assurances for mobile applications. In this work, we present the an extension of S\u00d7 C, called Security-by-Contract-with-Trust (S\u00d7 C\u00d7 T). Indeed, we enrich the S\u00d7 C architecture by integrating a trust model and adding new modules and configurations for managing contracts. Indeed, at deploy-time, our system decides the run-time configuration depending on the credentials of the contract provider. The run-time environment can both enforce a security policy and monitor the declared contract. According to the actual behaviour of the running program our architecture updates the trust level associated with the contract provider. We also present a possible application of our framework in the scenario of a mobile application marketplace, eg, Apple AppStore, Cydia, Android Market, that, nowadays, are considered as one of the most attractive e-commerce activity\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Requirement evolution: Towards a methodology and framework?\n", "abstract": " Software systems are undergoing continuing changes and rapid revolution. As consequence, requirements that were satisfied may no longer be satisfied or new requirements may be introduced. Thus, a challenging aspect is to develop a methodology and tools to model, manage, and analyze the evolution of requirements. In this paper, we describe our work at UNITN which targets a framework and methodology for requirement evolution. As an evidence for the feasibility of our approach, we describe our steps to achieve the goal as well as our preliminary result. In which, we propose a foundation to model requirement evolution, and concepts to support reasoning on evolutionary model.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Load Time Security Verification: The Claim Checker\n", "abstract": " Modern multi-application smart cards can become an integrated environment where applications from different providers are loaded on the fly and collaborate in order to facilitate lives of the cardholders. This initiative requires an embedded verification mechanism to ensure that all applications on the card respect the application interactions policy. The Security-by-Contract approach for loading time verification consists of two phases. During the first phase the loaded code is verified to be compliant with the supplied contract. Then, during the second phase the contract is matched with the smart card security policy. The report focuses on the first phase and describes an algorithm for static analysis of the loaded bytecode on Java Card. We also report about implementation of this algorithm that can be embedded on a real smart card.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Dynamic resiliency to changes\n", "abstract": " Nowadays IT systems are undergoing an irreversible evolution: we face a socio-technical system where humans are not just \u201cusers\u201d but decision makers whose decisions determine the behavior of the system as a whole. These decisions will not necessarily be system supported, nor planned in advance and sometimes not even informed, but they will nonetheless be taken. Thus, the inclusion of humans as decision makers requires the provision of technical means so that organizations can balance the need of getting the work done in presence of changes to the original course of action without incurring each and every time the risk of unforeseen toxic over-entitlements. In this paper, we consider a particular case of change to the course of action that is when the users availability or the assignment of privileges to users changes. When such changes occur the satisfaction of organizational business goals can\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "SecureChange: security engineering for lifelong evolvable systems\n", "abstract": " The challenge of SecureChange is to re-invent security engineering for \u201ceternal\u201d systems. The project focuses on methods, techniques and tools to make change a first-class citizen in the software development process so that security is built into software-based systems in a resilient and certifiable way. Solving this challenge requires a significant re-thinking of all development phases, from requirements engineering to design and testing.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Satisfaction of control objectives by control processes\n", "abstract": " Showing that business processes comply with regulatory requirements is not easy. We investigate this compliance problem in the case that the requirements are expressed as a directed, acyclic graph, with high-level requirements (called control objectives) at the top and with low-level requirements (called control activities) at the bottom. These control activities are then implemented by control processes. We introduce two algorithms: the first identifies whether a given set of control activities is sufficient to satisfy the top-level control objectives; the second identifies those steps of control processes that contribute to the satisfaction of top-level control objectives. We illustrate these concepts and the algorithms by examples taken from a large healthcare provider.", "num_citations": "1\n", "authors": ["168"]}
{"title": "D5. 1 Evaluation of existing methods and principles\n", "abstract": " Work Package 5 of the SecureChange project will develop four main artefacts: a language, a method, a documentation framework and a tool supporting risk analysis of evolving systems. Change and evolution in risk analysis can be categorised into tree perspectives and four kinds.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Engineering Secure Software and Systems: First International Symposium, ESSoS 2009 Leuven, Belgium, February 4-6, 2009, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the First International Symposium on Engineering Secure Software and Systems, ESSoS 2009, held in Leuven, Belgium, in February 2009. The 10 revised full papers presented together with 7 industry reports and ideas papers were carefully reviewed and selected from 57 submissions. The papers are organized in topical sections on policy verification and enforcement, model refinement and program transformation, secure system development, attack analysis and prevention, as well as testing and assurance.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Logging key assurance indicators in business processes\n", "abstract": " Management of a modern enterprise is based on the assumption that executive reports of lower-layer management are faithful to what is actually happening in the field. As some well-publicised major recent disasters (such as Barings, AllFirst-Allied Irish Bank, ENRON, Societ\u00e9 Generale) have shown, this assumption is not well-founded. Intermediate managers can misrepresent the actual state of their systems in order to hide negative events or to\" doctor\" reports which have been already produced. Existing security approaches which guarantee integrity of logs and related reports do not protect the system against these threats, if they are directly applied to a multi-layered corporate structure. In this paper, we extend existing approaches by constructing a logging scheme which ensures that, at each level, logs are both correct and consistent.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Engineering Secure Software and Systems\n", "abstract": " Engineering Secure Software and Systems - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Engineering Secure Software and Systems Massacci, Fabio ; Redwine, Samuel T. ; Zannone, Nicola Abstract Publication: Lecture Notes in Computer Science Pub Date: 2009 DOI: 10.1007/978-3-642-00199-4 Bibcode: 2009LNCS......M Keywords: Computer Science; Systems and Data Security; Software Engineering; Operating Systems; Algorithm Analysis and Problem Complexity; Management of Computing and Information Systems; Data Encryption full text sources Publisher | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project (\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Trust Management: 4th International Conference, iTrust 2006, Pisa, Italy, May 16-19, 2006, Proceedings\n", "abstract": " This volume constitutes the proceedings of the 4th International Conference on Trust Management, held in Pisa, Italy during 16\u201319 May 2006. The conference followed successful International Conferences in Crete in 2003, Oxford in 2004 and Paris in 2005. The? rst three conferences were organized by iTrust, which was a working group funded as a thematic network by the Future and Emerging Technologies (FET) unit of the Information Society Technologies (IST) program of the European Union. The purpose of the iTrust working group was to provide a forum for cro-disciplinary investigation of the applications of trust as a means of increasing security, building con? dence and facilitating collaboration in dynamic open s-tems. The aim of the iTrust conference series is to provide a common forum, bringing together researchers from di? erent academic branches, such as the technology-oriented disciplines, law, social sciences and philosophy, in order to develop a deeper and more fundamental understanding of the issues and ch-lenges in the area of trust management in dynamic open systems. The response to this conference was excellent; from the 88 papers submitted to the conference, we selected 30 full papers for presentation. The program also included one keynote address, given by Cristiano Castelfranchi; an industrial panel; 7 technology demonstrations; and a full day of tutorials.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Attacking Fair-Exchange Protocols: Parallel Models vs. Trace Models\n", "abstract": " Most approaches to formal protocol verification rely on an operational model based on traces of atomic actions. Modulo CSP, CCS, state-exploration, Higher Order Logic or strand spaces frills, authentication or secrecy are analyzed by looking at the existence or the absence of traces with a suitable property. We introduced an alternative operational approach based on parallel actions and an explicit representation of time. Our approach consists in specifying protocols within a logic language (AL SP), and associating the existence of an attack to the protocol with the existence of a model for the specifications of both the protocol and the attack. In this paper we show that, for a large class of protocols such as authentication and key exchange protocols, modeling in AL SP is equivalent-as far as authentication and secrecy attacks are considered-to modeling in trace based models. We then consider fair exchange\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}
{"title": "Reduction Rules and Universal Variables for First Order Tableaux and DPLL\n", "abstract": " Recent experimental results have shown that the strength of resolution, the propositional DPLL procedure, the KSAT procedure for description logics, or related tableau-like implementations such as DLP, is due to reduction rules which propagate constraints and prune the search space.Yet, for rst order logic such reduction rules are only known for resolution. The lack of reduction rules for rst order tableau calculi (DPLL can be seen as a tableau calculus with semantic branching) is one of the causes behind the lack of e cient rst order DPLL-like procedures.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Automated reasoning and the verification of security protocols\n", "abstract": " The formal verification of security protocols is one of the successful applications of automated reasoning1. Techniques based on belief logics, model checking, and theorem proving have been successful in determining strengths and weaknesses of many protocols, some of which have been even fielded before being discovered badly wrong.               This tutorial presents the problems to the \u201csecurity illiterate\u201d, explaining aims, objectives and tools of this application of automated reasoning.", "num_citations": "1\n", "authors": ["168"]}
{"title": "Approximate reasoning for contextual databases\n", "abstract": " Contextual reasoning has been proposed as a tool for solving the problem of generality in AI and for effectively handling huge knowledge bases, while approximate reasoning has been developed to overcome the computational barrier of classical deduction. This paper combines these approaches to provide an intuitive representation of knowledge and an effective deduction. Its semantics and a tableau calculus are presented. The key computational features are discussed.", "num_citations": "1\n", "authors": ["168"]}
{"title": "From Sound and Complete to Approximate Deduction\n", "abstract": " The computational complexity of reasoning in classical and non-classical logics makes traditional deduction not feasible in practice. This paper advocates the introduction of approximate proofs within automated deduction for classical and non-classical logics and a corresponding intuition of super cial semantics to overcome this limitation.Recent and past years have seen a vigorous development of automated deduction procedures for classical and non-classical logics. Calculi for logics for knowledge and belief, actions and programs, concurrency and temporal reasoning, description logics for knowledge representation have been developed (eg see 2, 3, 6, 7, 10, 11]) and corresponding systems implemented. Classical logic has gone far beyond and competitions between provers are now common. However, the applicability of deduction methods to engineering is hindered by their computational complexity: even propositional logic is NP complete and other decidable logics are PSPACE or EXPTIME. This computational obstacle has lead to heuristics and tactics to prune the search spaces or to techniques based on knowledge compilation. Still, if one looks at tables of experimental articles in conference proceedings or scienti c journals, beside\\theorem\" and\\non-theorem\", a common entry is\\time-out\". As pointed out by Levesque 4],\\human reasoners do not time out\" and engineers do it even less: after computing resources have been consumed (and paid for), some indicative result is anyway expected for action. To turn around this obstacle, approximation techniques have been developed with a particular attention to problems in NP (eg see 1, 8\u00a0\u2026", "num_citations": "1\n", "authors": ["168"]}