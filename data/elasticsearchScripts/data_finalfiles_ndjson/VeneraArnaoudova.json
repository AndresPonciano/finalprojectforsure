{"title": "The Effect of Poor Source Code Lexicon and Readability on Developers' Cognitive Load\n", "abstract": " It has been well documented that a large portion of the cost of any software lies in the time spent by developers in understanding a program's source code before any changes can be undertaken. One of the main contributors to software comprehension, by subsequent developers or by the authors themselves, has to do with the quality of the lexicon, (i.e., the identifiers and comments) that is used by developers to embed domain concepts and to communicate with their teammates. In fact, previous research shows that there is a positive correlation between the quality of identifiers and the quality of a software project. Results suggest that poor quality lexicon impairs program comprehension and consequently increases the effort that developers must spend to maintain the software. However, we do not yet know or have any empirical evidence, of the relationship between the quality of the lexicon and the cognitive load\u00a0\u2026", "num_citations": "77\n", "authors": ["2153"]}
{"title": "Improving source code readability: theory and practice\n", "abstract": " There are several widely accepted metrics to measure code quality that are currently being used in both research and practice to detect code smells and to find opportunities for code improvement. Although these metrics have been proposed as a proxy of code quality, recent research suggests that more often than not, state-of-the-art code quality metrics do not successfully capture quality improvements in the source code as perceived by developers. More specifically, results show that there may be inconsistencies between, on the one hand, the results from metrics for cohesion, coupling, complexity, and readability, and, on the other hand, the interpretation of these metrics in practice. As code improvement tools rely on these metrics, there is a clear need to identify and resolve the aforementioned inconsistencies. This will allow for the creation of tools that are more aligned with developers' perception of quality, and\u00a0\u2026", "num_citations": "26\n", "authors": ["2153"]}
{"title": "Measuring the impact of lexical and structural inconsistencies on developers\u2019 cognitive load during bug localization\n", "abstract": " A large portion of the cost of any software lies in the time spent by developers in understanding a program\u2019s source code before any changes can be undertaken. Measuring program comprehension is not a trivial task. In fact, different studies use self-reported and various psycho-physiological measures as proxies. In this research, we propose a methodology using functional Near Infrared Spectroscopy (fNIRS) and eye tracking devices as an objective measure of program comprehension that allows researchers to conduct studies in environments close to real world settings, at identifier level of granularity. We validate our methodology and apply it to study the impact of lexical, structural, and readability issues on developers\u2019 cognitive load during bug localization tasks. Our study involves 25 undergraduate and graduate students and 21 metrics. Results show that the existence of lexical inconsistencies in the source\u00a0\u2026", "num_citations": "10\n", "authors": ["2153"]}
{"title": "Prolonging the aging of software systems\n", "abstract": " The evolution of programming paradigms and languages allows us to manage the increasing complexity of systems. Furthermore, we have introduced (and demanded) increasingly complex requirements because various paradigms provide mechanisms to support their implementation. As a result, complex requirements constitute a driving factor for the evolution of languages which in turn can support system complexity. In this circular relationship, the maintenance phase of the software life cycle becomes increasingly important and factors which affect maintenance become vital. In this chapter we review the notions of software aging and discuss activities undertaken during maintenance. We also discuss challenges and trends for the development of well-maintained systems as well as for aiding in the maintenance of legacy systems.", "num_citations": "6\n", "authors": ["2153"]}
{"title": "Improving source code quality through the definition of linguistic antipatterns\n", "abstract": " Previous studies showed that linguistic information contained in source code is a valuable source of information and can help to improve program comprehension. The proposed research focuses on improving the quality of source code by studying common negative practices with respect to linguistic information. The definition of the so called linguistic antipatterns are expected to increase the awareness of the existence of such bad practices and to discourage their use. We also propose to study the relation between negative practices in linguistic information (i.e., linguistic antipatterns) and negative practices in structural information (i.e., design antipatterns). We discuss the proposed methodology and some preliminary results.", "num_citations": "3\n", "authors": ["2153"]}
{"title": "VITALSE: visualizing eye tracking and biometric data\n", "abstract": " Recent research in empirical software engineering is applying techniques from neurocognitive science and breaking new grounds in the ways that researchers can model and analyze the cognitive processes of developers as they interact with software artifacts. However, given the novelty of this line of research, only one tool exists to help researchers represent and analyze this kind of multimodal biometric data. While this tool does help with visualizing temporal eyetracking and physiological data, it does not allow for the mapping of physiological data to source code elements, instead projecting information over images of code. One drawback of this is that researchers are still unable to meaningfully combine and map physiological and eye tracking data to source code artifacts. The use of images also bars the support of long or multiple code les, which prevents researchers from analyzing data from experiments\u00a0\u2026", "num_citations": "2\n", "authors": ["2153"]}
{"title": "A Model to Detect Readability Improvements in Incremental Changes\n", "abstract": " Identifying source code that has poor readability allows developers to focus maintenance efforts on problematic code. Therefore, the effort to develop models that can quantify the readability of a piece of source code has been an area of interest for software engineering researchers for several years. However, recent research questions the usefulness of these readability models in practice. When applying these models to readability improvements that are made in practice, ie, commits, they are unable to capture these incremental improvements, despite a clear perceived improvement by the developers. This results in a discrepancy between the models we have built to measure readability, and the actual perception of readability in practice.", "num_citations": "2\n", "authors": ["2153"]}
{"title": "Overcoming comprehension barriers in the AspectJ programming language.\n", "abstract": " It has now been over a decade since the introduction of Aspect-Oriented Programming (AOP). As the AspectJ programming language (being one of the notable technologies of AOP) gains acceptance in industry and academia, its comprehensibility property is an important factor in determining an eventual wide acceptance by practitioners in development and maintenance as well as by educators who aim at introducing AOP into their curricula. Our objective is to improve program comprehension by identifying and addressing potential pitfalls in code which tend to make comprehension not intuitive. In those subtle places, we observe the behavior of the program to see the degree to which it matches the expected results. In cases where a conflict occurs, we provide a reasoning to point out where it would originate from, and a resolution to the conflict where applicable.", "num_citations": "2\n", "authors": ["2153"]}
{"title": "Comprehension and dependency analysis of aspect-oriented programs through declarative reasoning\n", "abstract": " In this paper we discuss an approach to support declarative reasoning over aspect-oriented (AO) programs, adopting AspectJ as a representative technology. The approach is based on the transformation of source code into a set of facts and rules, stored into a Prolog database. Declarative analysis allows us to extract complex information through its rich and expressive syntax. Our approach has two contributions. First, it aims to improve the comprehension of AspectJ programs. The type of knowledge provided is categorized in three main groups: i) general knowledge, ii) bad smells, and iii) quality metrics. The second contribution is the provision of dependency analysis of AspectJ programs. To that end, we identify dependencies in aspect-oriented programs, and translate them into Prolog rules. Expected beneficiaries of our approach include system maintainers who can obtain comprehension and perform\u00a0\u2026", "num_citations": "2\n", "authors": ["2153"]}
{"title": "Reassessing automatic evaluation metrics for code summarization tasks\n", "abstract": " In recent years, research in the domain of source code summarization has adopted data-driven techniques pioneered in machine translation (MT). Automatic evaluation metrics such as BLEU, METEOR, and ROUGE, are fundamental to the evaluation of MT systems and have been adopted as proxies of human evaluation in the code summarization domain. However, the extent to which automatic metrics agree with the gold standard of human evaluation has not been evaluated on code summarization tasks. Despite this, marginal improvements in metric scores are often used to discriminate between the performance of competing summarization models. In this paper, we present a critical exploration of the applicability and interpretation of automatic metrics as evaluation techniques for code summarization tasks. We conduct an empirical study with 226 human annotators to assess the degree to which automatic\u00a0\u2026", "num_citations": "1\n", "authors": ["2153"]}
{"title": "Towards Improving the Code Lexicon and its Consistency\n", "abstract": " Program comprehension is a key activity during software development and maintenance. Although frequently performed\u2014even more often than actually writing code\u2014program comprehension is a challenging activity. The difficulty to understand a program increases with its size and complexity and as a result the comprehension of complex programs, in the bestcase scenario, more time consuming when compared to simple ones but it can also lead to introducing faults in the program. Hence, structural properties such as size and complexity are often used to identify complex and fault prone programs. However, from early theories studying developers\u2019 behavior while understanding a program, we know that the textual information contained in identifiers and comments\u2014ie, the source code lexicon\u2014is part of the factors that affect the psychological complexity of a program, ie, factors that make a program difficult to understand and maintain by humans. In this dissertation we provide evidence that metrics evaluating the quality of source code lexicon are an asset for software fault explanation and prediction. Moreover, the quality of identifiers and comments considered in isolation may not be sufficient to reveal flaws\u2014in his theory about the program understanding process for example, Brooks warns that it may happen that comments and code are contradictory. Consequently, we address the problem of contradictory, and more generally of inconsistent, lexicon by defining a catalog of Linguistic Antipatterns (LAs), ie, poor practices in the choice of identifiers resulting in inconsistencies among the name, implementation, and documentation of a\u00a0\u2026", "num_citations": "1\n", "authors": ["2153"]}
{"title": "A hybrid query engine for the structural analysis of Java and AspectJ programs\n", "abstract": " Graphical representation has long been used in the domain of software engineering to provide comprehension aids. Generic and query-based code browsers have contributed towards program comprehension through the provision of high-level customized structural views of the source code. Some research has already focused on combining the advantages of the two techniques through the representation of high-level query-based views using different data visualization techniques. However, little has been done towards the visualization of the query composition process itself. In this paper we develop a query engine for software structural analysis which provides a visual query interface over a high-level textual query language to eliminate the requirement for understanding the query language syntax. Our implementation is provided as an Eclipse plug-in, namely HyQ4J (which stands for hybrid query for Java).", "num_citations": "1\n", "authors": ["2153"]}
{"title": "Adaptation of refactoring strategies to multiple axes of modularity: characteristics and criteria\n", "abstract": " In object-oriented programming, the adoption of modules as mixins provides a second axis of modularity. Furthermore, support for aspect-oriented programming introduces a third such axis. In this paper we define criteria under which a feature should be placed in a given unit of modularity and investigate the degree to which the presence of multiple dimensions of modularity affects existing refactoring strategies. We reason about the dilemmas involved and we provide guidelines for applying existing refactoring strategies in order to support the above criteria. We adopt Ruby and Aquarium as example technologies in order to provide an environment with classes, modules and aspects.", "num_citations": "1\n", "authors": ["2153"]}