{"title": "The GHTorrent dataset and tool suite\n", "abstract": " During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.", "num_citations": "585\n", "authors": ["180"]}
{"title": "Work Practices and Challenges in Pull-Based Development: The Integrator\u2019s Perspective\n", "abstract": " In the pull-based development model, the integrator has the crucial role of managing and integrating contributions. This work focuses on the role of the integrator and investigates working habits and challenges alike. We set up an exploratory qualitative study involving a large-scale survey of 749 integrators, to which we add quantitative data from the integrator's project. Our results provide insights into the factors they consider in their decision making process to accept or reject a contribution. Our key findings are that integrators struggle to maintain the quality of their projects and have difficulties with prioritizing contributions that are to be merged. Our insights have implications for practitioners who wish to use or improve their pull-based development process, as well as for researchers striving to understand the theoretical implications of the pull-based model in software development.", "num_citations": "292\n", "authors": ["180"]}
{"title": "GHTorrent: Github's data from a firehose\n", "abstract": " A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.", "num_citations": "257\n", "authors": ["180"]}
{"title": "Measuring developer contribution from software repository data\n", "abstract": " Apart from source code, software infrastructures supporting agile and distributed software projects contain traces of developer activity that does not directly affect the product itself but is important for the development process. We propose a model that, by combining traditional contribution metrics with data mined from software repositories, can deliver accurate developer contribution measurements. The model creates clusters of similar projects to extract weights that are then applied to the actions a developer performed on project assets to extract a combined measurement of the developer's contribution. We are currently implementing the model in the context of a software quality monitoring system while we are also validating its components by means of questionnaires.", "num_citations": "158\n", "authors": ["180"]}
{"title": "Open Source Software: A Survey from 10,000 Feet\n", "abstract": " Open source software (OSS), the origins of which can be traced back to the 1950s, is software distributed with a license that allows access to its source code, free redistribution, the creation of derived works, and unrestricted use. OSS applications cover most areas of consumer and business software and their study touches many disciplines, including computer science, information systems, economics, psychology, and law. Behind a successful OSS project lies a community of actors, ranging from core developers to passive users, held together by a flexible governance structure and membership, leadership and contribution policies that align their interests. The motivation behind individuals participating in OSS projects can be, among others, social, ideological, hedonistic, or signaling, while companies gain from their access to high-quality, innovative projects and an increase in their reputation and visibility. Nowadays many business models rely on OSS as a product through the provision of associated services, or in coexistence with proprietary software, hardware, services, or licensing. The numerous OSS licenses mainly differ", "num_citations": "82\n", "authors": ["180"]}
{"title": "Matching GitHub developer profiles to job advertisements\n", "abstract": " GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally \"be seen: by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.", "num_citations": "79\n", "authors": ["180"]}
{"title": "A dataset for pull-based development research\n", "abstract": " Pull requests form a new method for collaborating in distributed software development. To study the pull request distributed development model, we constructed a dataset of almost 900 projects and 350,000 pull requests, including some of the largest users of pull requests on Github. In this paper, we describe how the project selection was done, we analyze the selected features and present a machine learning tool set for the R statistics environment.", "num_citations": "63\n", "authors": ["180"]}
{"title": "Alitheia core: An extensible software quality monitoring platform\n", "abstract": " Research in the fields of software quality and maintainability requires the analysis of large quantities of data, which often originate from open source software projects. Pre-processing data, calculating metrics, and synthesizing composite results from a large corpus of project artefacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. The system has been used to process several projects successfully, forming the basis of an emerging ecosystem of quality analysis tools.", "num_citations": "63\n", "authors": ["180"]}
{"title": "Beautiful Architecture: Leading Thinkers Reveal the Hidden Beauty in Software Design\n", "abstract": " What are the ingredients of robust, elegant, flexible, and maintainable software architecture? Beautiful Architecture answers this question through a collection of intriguing essays from more than a dozen of today's leading software designers and architects. In each essay, contributors present a notable software architecture, and analyze what makes it innovative and ideal for its purpose. Some of the engineers in this book reveal how they developed a specific project, including decisions they faced and tradeoffs they made. Others take a step back to investigate how certain architectural aspects have influenced computing as a whole. With this book, you'll discover: How Facebook's architecture is the basis for a data-centric application ecosystem The effect of Xen's well-designed architecture on the way operating systems evolve How community processes within the KDE project help software architectures evolve from rough sketches to beautiful systems How creeping featurism has helped GNU Emacs gain unanticipated functionality The magic behind the Jikes RVM self-optimizable, self-hosting runtime Design choices and building blocks that made Tandem the choice platform in high-availability environments for over two decades Differences and similarities between object-oriented and functional architectural views How architectures can affect the software's evolution and the developers' engagement Go behind the scenes to learn what it takes to design elegant software architecture, and how it can shape the way you approach your own projects, with Beautiful Architecture.", "num_citations": "47\n", "authors": ["180"]}
{"title": "A platform for software engineering research\n", "abstract": " Research in the fields of software quality, maintainability and evolution requires the analysis of large quantities of data, which often originate from open source software projects. Collecting and preprocessing data, calculating metrics, and synthesizing composite results from a large corpus of project artifacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. Alitheia Core aims to be the basis of an ecosystem of shared tools and research data that will enable researchers to focus on their research questions at hand, rather than spend time on re-implementing\u00a0\u2026", "num_citations": "42\n", "authors": ["180"]}
{"title": "The bug catalog of the maven ecosystem\n", "abstract": " Examining software ecosystems can provide the research community with data regarding artifacts, processes, and communities. We present a dataset obtained from the Maven central repository ecosystem (approximately 265GB of data) by statically analyzing the repository to detect potential software bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version we also stored specific metadata such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results, as we show in specific examples.", "num_citations": "39\n", "authors": ["180"]}
{"title": "Mining software engineering data from GitHub\n", "abstract": " GitHub is the largest collaborative source code hosting site built on top of the Git version control system. The availability of a comprehensive API has made GitHub a target for many software engineering and online collaboration research efforts. In our work, we have discovered that a) obtaining data from GitHub is not trivial, b) the data may not be suitable for all types of research, and c) improper use can lead to biased results. In this tutorial, we analyze how data from GitHub can be used for large-scale, quantitative research, while avoiding common pitfalls. We use the GHTorrent dataset, a queryable offline mirror of the GitHub API data, to draw examples from and present pitfall avoidance strategies.", "num_citations": "27\n", "authors": ["180"]}
{"title": "Debugging Data Flows in Reactive Programs\n", "abstract": " Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console. In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.", "num_citations": "23\n", "authors": ["180"]}
{"title": "Call for quality: Open source software quality observation\n", "abstract": " This paper describes how a Software Quality Observatory works to evaluate and quantify the quality of an Open Source project. Such a quality measurement can be used by organizations intending to deploy an Open Source solution to pick one of the available projects for use. We offer a case description of how the Software Quality Observatory will be applied to the KDE project to document and evaluate its quality practices for outsiders.", "num_citations": "21\n", "authors": ["180"]}
{"title": "Dismal Code: Studying the Evolution of Security Bugs\n", "abstract": " Background. Security bugs are critical programming errors that can lead to serious vulnerabilities in software. Such bugs may allow an attacker to take over an application, steal data or prevent the application from working at all.Aim. We used the projects stored in the Maven repository to study the characteristics of security bugs individually and in relation to other software bugs. Specifically, we studied the evolution of security bugs through time. In addition, we examined their persistence and their relationship with a) the size of the corresponding version, and b) other bug categories.Method. We analyzed every project version of the Maven repository by using FindBugs, a popular static analysis tool. To see how security bugs evolve over time we took advantage of the repository's project history and dependency data.Results. Our results indicate that there is no simple rule governing the number of security bugs as a project evolves. In particular, we cannot say that across projects security-related defect counts increase or decrease significantly over time. Furthermore, security bugs are not eliminated in a way that is particularly different from the other bugs. In addition, the relation of security bugs with a project's size appears to be different from the relation of the bugs coming from other categories. Finally, even if bugs seem to have similar behaviour, severe security bugs seem to be unassociated with other bug categories.Conclusions. Our findings indicate that further research should be done to analyze the evolution of security bugs. Given the fact that our experiment included only Java projects, similar research could be done for another ecosystem\u00a0\u2026", "num_citations": "20\n", "authors": ["180"]}
{"title": "Measuring the Occurrence of Security-Related Bugs through Software Evolution\n", "abstract": " A security-related bug is a programming error that introduces a potentially exploitable weakness into a computer system. This weakness could lead to a security breach with unfortunate consequences. Version control systems provide an accurate historical record of the software code's evolution. In this paper we examine the frequency of the security-related bugs throughout the evolution of a software project by applying the Find Bugs static analyzer on all versions of its revision history. We have applied our approach on four projects and we have come out with some interesting results including the fact that the number of the security-related bugs increase as the project evolves.", "num_citations": "18\n", "authors": ["180"]}
{"title": "A Comparison of Portable Dynamic Web Content Technologies for the Apache Server\n", "abstract": " Apache is considered to be the most extensible, secure and widely used server on the Internet. On our talk we focus on its first characteristic, extensibility, analyzing many techniques used to provide dynamic content. Available solutions are based either on extensions to the web server itself or on the execution of userspace programs. These solutions include, among others, CGI scripts, PHP, mod perl, mod python and Java Servlets. For each technology we present its basic design goals and the development facilities it offers. We compare the efficiency of these technologies by means of custom-made benchmarks we run to measure each solution\u2019s throughput. Finally, we present each technique\u2019s drawbacks, with references to lessons learned during the complete deploy-and-test process.", "num_citations": "18\n", "authors": ["180"]}
{"title": "Conducting quantitative software engineering studies with Alitheia Core\n", "abstract": " Quantitative empirical software engineering research benefits mightily from processing large open source software repository data sets. The diversity of repository management tools and the long history of some projects, renders the task of working with those datasets a tedious and error-prone exercise. The Alitheia Core analysis platform preprocesses repository data into an intermediate format that allows researchers to provide custom analysis tools. Alitheia Core automatically distributes the processing load on multiple processors while enabling programmatic access to the raw data, the metadata, and the analysis results. The tool has been successfully applied on hundreds of medium to large-sized open-source projects, enabling large-scale empirical studies.", "num_citations": "16\n", "authors": ["180"]}
{"title": "A note on rigour and replicability\n", "abstract": " As any empirical science, Software Engineering research should strive towards better research practices. Replication is regrettably not a priority for Software Engineering researchers and, moreover, not afforded by many published studies. Here we report our experience from our encounter with a recent paper in a agship Software Engineering conference. Our experience shows that current publication requirements do not guarantee replicability.", "num_citations": "13\n", "authors": ["180"]}
{"title": "Tuning Java\u2019s memory manager for high performance server applications\n", "abstract": " Java is a strong player in the application server market and thus the performance of its virtual machine is an important aspect of a server\u2019s performance. One of the components that affect the performance of a JVM is the memory manager, which also includes the garbage collector. Modern virtual machines offer a multitude of options for tuning the memory manager, which can have a significant impact on server application performance. In this paper, we examine the effect of tuning the garbage collection in an application server environment. By employing both synthetic and real world application benchmarks, we assess the various garbage collection strategies offered by two popular virtual machines. Finally, we present a comprehensive list of generally applicable garbage collection guidelines.", "num_citations": "12\n", "authors": ["180"]}
{"title": "A Mixed Methods Approach to Mining Code Review Data: Examples and a replication study of multi-commit reviews\n", "abstract": " Software code review has been considered an important quality assurance mechanism for the last 35 years. The techniques for conducting code reviews have evolved along with the software industry and have become progressively incremental and lightweight. We have studied code review in number of contemporary settings, including Apache, Linux, KDE, Microsoft, Android, and GitHub. Code review is an inherently social activity, so we have used both quantitative and qualitative methods to understand the underlying parameters (or measures) of the process, as well as the rich interactions and motivations for doing code review. In this chapter, we describe how we have used a mixed methods approach to triangulate our findings on software code review. We also describe how we use quantitative data to help us sample the most interesting cases from our data to be analyzed qualitatively. To illustrate code review research, we provide new results contrast single and multi-commit reviews. We find that while multi-commit reviews take longer and have more churn than single commit reviews, the same number of people are involved both types of review. To enrich and triangulate our findings, we qualitatively analyze the characteristics of multi-commit reviews and find that there are two types: reviews of branches and revisions to single commits. We also examine the reasons why commits are rejected.", "num_citations": "11\n", "authors": ["180"]}
{"title": "Building an e-business platform: an experience report\n", "abstract": " The PRAXIS project has been designed to facilitate the interchange of data between government and business entities through direct enterprise application interconnection. The system architecture has been entirely based on emerging technologies, including web services and XML, allowing independent client systems to operate asynchronously with a central server orchestrating and controlling the workflow. An original business document exchange protocol has been developed from the ground up in order to address the specific needs of business to government transactions in the Greek business sector. This paper elaborates on the methodologies and tools used for the development of the project\u2019s coordination point (the server) and its clients, and present the experiences gained during the system\u2019s design, as well as results obtained from the preliminary integration and testing phases.", "num_citations": "11\n", "authors": ["180"]}
{"title": "Strong agile metrics: mining log data to determine predictive power of software metrics for continuous delivery teams\n", "abstract": " ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong.", "num_citations": "10\n", "authors": ["180"]}
{"title": "Big data software analytics with Apache Spark\n", "abstract": " At the beginning of every research effort, researchers in empirical software engineering have to go through the processes of extracting data from raw data sources and transforming them to what their tools expect as inputs. This step is time consuming and error prone, while the produced artifacts (code, intermediate datasets) are usually not of scientific value. In the recent years, Apache Spark has emerged as a solid foundation for data science and has taken the big data analytics domain by storm. We believe that the primitives exposed by Apache Spark can help software engineering researchers create and share reproducible, high-performance data analysis pipelines.", "num_citations": "9\n", "authors": ["180"]}
{"title": "Generating the Blueprints of the Java Ecosystem\n", "abstract": " Examining a large number of software artifacts can provide the research community with data regarding quality and design. We present a dataset obtained by statically analyzing 22730 jar files taken from the Maven central archive, which is the de-facto application library repository for the Java ecosystem. For our analysis we used three popular static analysis tools that calculate metrics regarding object-oriented design, program size, and package design. The dataset contains the metrics results that every tool reports for every selected jar of the ecosystem. Our dataset can be used to produce interesting research results, such as measure the domain-specific language usage.", "num_citations": "9\n", "authors": ["180"]}
{"title": "Aquarium: An Extensible Billing Platform for Cloud Infrastructures\n", "abstract": " An important part of public IaaS offerings is resource management and customer billing. In this paper we present the design and implementation of Aquarium, an extensible billing service software. Aquarium associates state changes in cloud resources with respective charges, based on configurable, user-specific and versioned charging policies. The implementation of Aquarium is characterized by pervasive data immutability, actor message passing, and service orientation.", "num_citations": "8\n", "authors": ["180"]}
{"title": "Pr\\\" azi: From Package-based to Call-based Dependency Networks\n", "abstract": " Software reuse has emerged as one of the most crucial elements of modern software development. The standard way to study the dependency networks caused by reuse is to infer relationships between software packages through manifests in the packages' repositories. Such networks can help answer important questions like \"How many packages have dependencies to packages with known security issues?\" or \"What are the most used packages?\". However, an important overlooked aspect of current networks is that manifest-inferred relationships do not necessarily describe how or whether these dependencies are actually used in the code. To better model dependencies between packages, we devise Pr\\\"azi, an approach combining manifests and call graphs of packages. Pr\\\"azi constructs a fine-grained dependency network at the more fine-grained function-level, instead of at the manifest-level. For this paper, we provide a prototypical Pr\\\"azi implementation for the popular system programming language Rust. Using it, we replicate a recent evolution study characterizing Rust's package repository, Cratesio, on the function-level. Our results identify new key characteristics and developments of Cratesio: i) 49% of all function calls in Cratesio target a function in a dependency, suggesting prevalent reuse of dependencies, ii) packages call 40% of their resolved transitive dependencies, iii) package maintainers make nearly 7 new calls to their dependencies biannually, and iv) packages have two to three times more indirect callers than direct callers of their APIs. These results show that current analyses of manifest-level dependency networks are\u00a0\u2026", "num_citations": "6\n", "authors": ["180"]}
{"title": "Distributed Component Architectures Security Issues\n", "abstract": " Enterprise information systems and e-commerce applications are tightly integrated in today's modern enterprises. Component architectures are the base for building such multitier distributed applications. This paper examines the security threats those systems must confront and the solutions proposed by major existing component architectures. A comparative evaluation of both security features and implementation issues is carried out to determine each architecture's strong points and drawbacks.", "num_citations": "5\n", "authors": ["180"]}
{"title": "Jikesnode: A Java operating system\n", "abstract": " Operating system kernel development has been an active area of research since almost the birth of computer science. There are currently two major architectural designs for kernels, namely monolithic and microkernels. This thesis examines the potential of a Java operating system that theoretically combines the strong points of the aforementioned designs. The proposed architecture merges the Jikes Research Virtual Machine with the JNode operating system in order to demonstrate the feasibility of such an approach and to provide the Jamaica project with a tool to further continue the study of parallelism.", "num_citations": "5\n", "authors": ["180"]}
{"title": "Fine-Grained Network Analysis for Modern Software Ecosystems\n", "abstract": " Modern software development is increasingly dependent on components, libraries, and frameworks coming from third-party vendors or open-source suppliers and made available through a number of platforms (or forges). This way of writing software puts an emphasis on reuse and on composition, commoditizing the services that modern applications require. On the other hand, bugs and vulnerabilities in a single library living in one such ecosystem can affect, directly or by transitivity, a huge number of other libraries and applications. Currently, only product-level information on library dependencies is used to contain this kind of danger, but this knowledge often reveals itself too imprecise to lead to effective (and possibly automated) handling policies. We will discuss how fine-grained function-level dependencies can greatly improve reliability and reduce the impact of vulnerabilities on the whole software ecosystem.", "num_citations": "4\n", "authors": ["180"]}
{"title": "Java performance evaluation using external instrumentation\n", "abstract": " The performance of programs written in the Java programming language is not trivial to analyse. The Java Virtual Machine hides the details of bytecode execution while not providing an accessible profiling mechanism. Most tools used for Java performance evaluations are based on sampling and only resent engineers with sampled data aggregations. In this paper, we present the Java DTrace Toolkit, a collection of scripts that is specifically designed to assist engineers in identifying the roots of various performance problems observed with other tools.", "num_citations": "4\n", "authors": ["180"]}
{"title": "The Vulnerability Dataset of a Large Software Ecosystem\n", "abstract": " Security bugs are critical programming errors that can lead to serious vulnerabilities in software. Examining their behaviour and characteristics within a software ecosystem can provide the research community with data regarding their evolution, persistence and others. We present a dataset that we produced by applying static analysis to the Maven Central Repository (approximately 265GB of data) in order to detect potential security bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics' results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version in our data repository, we also store specific metadata, such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results involving security bugs, as we show in specific examples.", "num_citations": "3\n", "authors": ["180"]}
{"title": "How to Analyze Git Repositories with Command Line Tools: We\u2019re not in Kansas Anymore\n", "abstract": " Git repositories are an important source of empirical software engineering product and process data. Running the Git command-line tool and processing its output with other Unix tools allows the incremental construction of sophisticated data processing pipelines. Git data analytics on the command-line can be systematically presented through a pattern that involves fetching, selection, processing, summarization, and reporting. For each part of the processing pipeline, we examine the tools and techniques that can be most effectively used to perform the task at hand. The presented techniques can be easily applied, first to get a feeling of version control repository data at hand and then also for extracting empirical results.", "num_citations": "2\n", "authors": ["180"]}
{"title": "Tools and methods for large scale empirical software engineering research\n", "abstract": " Introduction empirical, adj: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic.", "num_citations": "1\n", "authors": ["180"]}
{"title": "The JikesXen Java Server Platform\n", "abstract": " The purpose of the JVM is to abstract the Java language from the hardware and software platforms it runs on. For this reason, the JVM uses services offered by the host operating system in order to implement identical services for the Java language. The obvious duplication of effort in service provision and resource management between the JVM and the operating system has a measurable cost on the performance of Java programs. In my PhD research, I try to find ways of minimizing the cost of sharing resources between the OS and the JVM, by identifying and removing unnecessary software layers.", "num_citations": "1\n", "authors": ["180"]}
{"title": "Analyze That! Rethinking Questions for Data Scientists in Software Engineering\n", "abstract": " In 2014, Microsoft came up with a list of 145 questions that software engineers would like data scientists to answer. Following this open call to the research community, many studies followed. Now in 2019, five years since the original study came out, no other study checked if the questions posted by Microsoft hold for other software companies (including companies with a different focus). More so, do the problems identified five years ago still hold? Along these lines, this paper presents another list of questions that software engineers at ING would like data scientists to answer. ING is a Netherlandsbased software-defined enterprise providing banking solutions. At present, ING houses over 15,000 employees in its software division providing home-grown software solutions. As a banking company, now dealing with the complexities of software development, we replicated the original Microsoft study at ING and compared the questions. Furthermore, we contrasted the two outcomes, in light of the differences in context and time. The study shows that the two sets of data-driven software engineering questions are remarkably similar, albeit with certain key differences.", "num_citations": "1\n", "authors": ["180"]}