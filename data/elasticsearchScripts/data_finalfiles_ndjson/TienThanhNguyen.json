{"title": "A novel combining classifier method based on variational inference\n", "abstract": " In this paper, we propose a combining classifier method based on the Bayesian inference framework. Specifically, the outputs of base classifiers (called Level1 data or meta-data) are utilized in a combiner to produce the final classification. In our ensemble system, each class in the training set induces a distribution on the Level1 data, which is modeled by a multivariate Gaussian distribution. Traditionally, the parameters of the Gaussian are estimated using a maximum likelihood approach. However, maximum likelihood estimation cannot be applied since the covariance matrix of Level1 data of each class is not full rank. Instead, we propose to estimate the multivariate Gaussian distribution of Level1 data of each class by using the Variational Inference method. Experiments conducted on eighteen UCI Machine Learning Repository datasets and a selected 10-class CLEF2009 medical imaging database demonstrated\u00a0\u2026", "num_citations": "44\n", "authors": ["1086"]}
{"title": "Heterogeneous classifier ensemble with fuzzy rule-based meta learner\n", "abstract": " In heterogeneous ensemble systems, each learning algorithm learns a classifier on a given training set to describe the relationship between a feature vector and its class label. As each classifier outputs different result on an observation, uncertainty is introduced. In this paper, we introduce a heterogeneous ensemble system with a fuzzy IF-THEN rule inference engine as the combiner to capture the uncertainty in the outputs of the base classifiers. In our method, fuzzy rules are generated on the outputs of an ensemble of base classifiers, which can be viewed as the class posterior probability of the observations. The performance of our method was evaluated on thirty datasets and in comparison with nine ensemble methods (AdaBoost, Decision Template, Decision Tree on meta-data, and six fixed combiners) and two single learning algorithms (SVM with L2-loss function and Decision Tree), and was shown to\u00a0\u2026", "num_citations": "31\n", "authors": ["1086"]}
{"title": "Aggregation of classifiers: a justifiable information granularity approach\n", "abstract": " In this paper, we introduced a new approach of combining multiple classifiers in a heterogeneous ensemble system. Instead of using numerical membership values when combining, we constructed interval membership values for each class prediction from the meta-data of observation by using the concept of information granule. In the proposed method, the uncertainty (diversity) of the predictions produced by the base classifiers is quantified by the interval-based information granules. The decision model is then generated by considering both bound and length of the intervals. Extensive experimentation using the UCI datasets has demonstrated the superior performance of our algorithm over other algorithms including six fixed combining methods, one trainable combining method, AdaBoost, bagging, and random subspace.", "num_citations": "26\n", "authors": ["1086"]}
{"title": "A novel genetic algorithm approach for simultaneous feature and classifier selection in multi classifier system\n", "abstract": " In this paper we introduce a novel approach for classifier and feature selection in a multi-classifier system using Genetic Algorithm (GA). Specifically, we propose a 2-part structure for each chromosome in which the first part is encoding for classifier and the second part is encoding for feature. Our structure is simple in the implementation of the crossover as well as the mutation stage of GA. We also study 8 different fitness functions for our GA based algorithm to explore the optimal fitness functions for our model. Experiments are conducted on both 14 UCI Machine Learning Repository and CLEF2009 medical image database to demonstrate the benefit of our model on reducing classification error rate.", "num_citations": "26\n", "authors": ["1086"]}
{"title": "Ensemble selection based on classifier prediction confidence\n", "abstract": " Ensemble selection is one of the most studied topics in ensemble learning because a selected subset of base classifiers may perform better than the whole ensemble system. In recent years, a great many ensemble selection methods have been introduced. However, many of these lack flexibility: either a fixed subset of classifiers is pre-selected for all test samples (static approach), or the selection of classifiers depends upon the performance of techniques that define the region of competence (dynamic approach). In this paper, we propose an ensemble selection method that takes into account each base classifier's confidence during classification and the overall credibility of the base classifier in the ensemble. In other words, a base classifier is selected to predict for a test sample if the confidence in its prediction is higher than its credibility threshold. The credibility thresholds of the base classifiers are found by\u00a0\u2026", "num_citations": "20\n", "authors": ["1086"]}
{"title": "Multi-label classification via incremental clustering on an evolving data stream\n", "abstract": " With the advancement of storage and processing technology, an enormous amount of data is collected on a daily basis in many applications. Nowadays, advanced data analytics have been used to mine the collected data for useful information and make predictions, contributing to the competitive advantages of companies. The increasing data volume, however, has posed many problems to classical batch learning systems, such as the need to retrain the model completely with the newly arrived samples or the impracticality of storing and accessing a large volume of data. This has prompted interest on incremental learning that operates on data streams. In this study, we develop an incremental online multi-label classification (OMLC) method based on a weighted clustering model. The model is made to adapt to the change of data via the decay mechanism in which each sample's weight dwindles away over time. The\u00a0\u2026", "num_citations": "19\n", "authors": ["1086"]}
{"title": "Combining heterogeneous classifiers via granular prototypes\n", "abstract": " In this study, a novel framework to combine multiple classifiers in an ensemble system is introduced. Here we exploit the concept of information granule to construct granular prototypes for each class on the outputs of an ensemble of base classifiers. In the proposed method, uncertainty in the outputs of the base classifiers on training observations is captured by an interval-based representation. To predict the class label for a new observation, we first determine the distances between the output of the base classifiers for this observation and the class prototypes, then the predicted class label is obtained by choosing the label associated with the shortest distance. In the experimental study, we combine several learning algorithms to build the ensemble system and conduct experiments on the UCI, colon cancer, and selected CLEF2009 datasets. The experimental results demonstrate that the proposed framework\u00a0\u2026", "num_citations": "18\n", "authors": ["1086"]}
{"title": "A weighted multiple classifier framework based on random projection\n", "abstract": " In this paper, we propose a weighted multiple classifier framework based on random projections. Similar to the mechanism of other homogeneous ensemble methods, the base classifiers in our approach are obtained by a learning algorithm on different training sets generated by projecting the original up-space training set to lower dimensional down-spaces. We then apply a Least SquarE\u2212based method to weigh the outputs of the base classifiers so that the contribution of each classifier to the final combined prediction is different. We choose Decision Tree as the learning algorithm in the proposed framework and conduct experiments on a number of real and synthetic datasets. The experimental results indicate that our framework is better than many of the benchmark algorithms, including three homogeneous ensemble methods (Bagging, RotBoost, and Random Subspace), several well-known algorithms (Decision\u00a0\u2026", "num_citations": "15\n", "authors": ["1086"]}
{"title": "Variational inference based bayes online classifiers with concept drift adaptation\n", "abstract": " We present VIGO, a novel online Bayesian classifier for both binary and multiclass problems. In our model, variational inference for multivariate distribution technique is exploited to approximate the class conditional probability density functions of data in an online manner. To handle concept drift that could arise in streaming data, we develop 2 new adaptive methods based on VIGO, which we called VIGOw and VIGOd. While VIGOw naturally adapts to any kind of changing environments, VIGOd maximises the benefit of a static environment as long as it does not detect any change. Extensive experiments on big/medium real-world/synthetic datasets demonstrate the superior performance of our algorithms over many state-of-the-art methods in the literature.", "num_citations": "15\n", "authors": ["1086"]}
{"title": "Combining multi classifiers based on a genetic algorithm\u2013a gaussian mixture model framework\n", "abstract": " Combining outputs from different classifiers to achieve high accuracy in classification task is one of the most active research areas in ensemble method. Although many state-of-art approaches have been introduced, no one method performs the best on all data sources. With the aim of introducing an effective classification model, we propose a Gaussian Mixture Model (GMM) based method that combines outputs of base classifiers (called meta-data or Level1 data) resulted from Stacking Algorithm. We further apply Genetic Algorithm (GA) to that data as a feature selection strategy to explore an optimal subset of Level1 data in which our GMM-based approach can achieve high accuracy. Experiments on 21 UCI Machine Learning Repository data files and CLEF2009 medical image database demonstrate the advantage of our framework compared with other well-known combining algorithms such as Decision\u00a0\u2026", "num_citations": "14\n", "authors": ["1086"]}
{"title": "Optimization of ensemble classifier system based on multiple objectives genetic algorithm\n", "abstract": " This paper introduces a mechanism to learn optimal classifier combining algorithms for an ensemble system. By using a genetic algorithm approach that focuses on 3 objectives namely the number of correct classified observations, the number of selected features and the number of selected classifiers, optimal solution can be achieved after several interactions of crossover and mutation. We also employ the Ordered Weighted Averaging operator in which a weight vector is built by a Linear Decreasing (LD) function to find average values of outputs from combining algorithms. Experiments on 2 well-known UCI Machine Learning Repository datasets demonstrate benefits of our approach compared with other state-of-the-art ensemble methods like Decision Template, SCANN and all fixed combining algorithms in the ensemble system.", "num_citations": "14\n", "authors": ["1086"]}
{"title": "A novel 2-stage combining classifier model with stacking and genetic algorithm based feature selection\n", "abstract": " This paper introduces a novel 2-stage classification system with stacking and genetic algorithm (GA) based feature selection. Specifically, Level1 data is first generated by stacking on the original data (called Level0 data) with base classifiers. Level1data is then classified by a second classifier (denoted by C) with feature selection using GA. The advantage of applying GA on Level1 data is that it has lower dimension and is more uniformity than Level0 data. We conduct experiments on both 18 UCI data files and CLEF2009 medical image database to demonstrate superior performance of our model in comparison with several popular combining algorithms.", "num_citations": "11\n", "authors": ["1086"]}
{"title": "An ensemble system with random projection and dynamic ensemble selection\n", "abstract": " In this paper, we propose using dynamic ensemble selection (DES) method on ensemble generated based on random projection. We first construct the homogeneous ensemble in which a set of base classifier is obtained by a learning algorithm on different training schemes generated by projecting the original training set to lower dimensional down spaces. We then develop a DES method on those base classifiers so that a subset of base classifiers is selected to predict label for each test sample. Here competence of a classifier is evaluated based on its prediction results on the test sample\u2019s  nearest neighbors obtaining from the projected data of validation set. Our proposed method, therefore, gains the benefits not only from the random projection in dimensionality reduction and diverse training schemes generation but also from DES method in choosing an appropriate subset of base classifiers for each\u00a0\u2026", "num_citations": "9\n", "authors": ["1086"]}
{"title": "A novel online bayes classifier\n", "abstract": " We present VIGO, a novel online Bayesian classifier for both binary or multiclass problems. In our model, variational inference for multivariate Gaussian distribution technique is exploited to approximate the class conditional probability density functions of data in an online manner. Besides being a conservative learner with a low number of updates compared with many other popular algorithms, VIGO algorithm can be updated in a minibatch of an arbitrary size which makes it robust with noise data. Experiments over a large number of UCI datasets demonstrate the advantage of VIGO with many state-of-the-art methods presented in LIBOL - a prevalent library for online learning algorithms.", "num_citations": "8\n", "authors": ["1086"]}
{"title": "Fuzzy If-Then rules classifier on ensemble data\n", "abstract": " This paper introduces a novel framework that uses fuzzy IF-THEN rules in an ensemble system. Our model tackles several drawbacks. First, IF-THEN rules approaches have problems with high dimensional data since computational cost is exponential. In our framework, rules are operated on outputs of base classifiers which frequently have lower dimensionality than the original data. Moreover, outputs of base classifiers are scaled within the range [0, 1] so it is convenient to apply fuzzy rules directly instead of requiring data transformation and normalization before generating fuzzy rules. The performance of this model was evaluated through experiments on 6 commonly used datasets from UCI Machine Learning Repository and compared with several state-of-art combining classifiers algorithms and fuzzy IF-THEN rules approaches. The results show that our framework can improve the classification accuracy.", "num_citations": "7\n", "authors": ["1086"]}
{"title": "Fusion of classifiers based on a novel 2-stage model\n", "abstract": " The paper introduces a novel 2-Stage model for multi-classifier system. Instead of gathering posterior probabilities resulted from base classifiers into a single dataset called meta-data or Level1 data like in the original 2-Stage model, here we separate data in K Level1 matrices corresponding to the K base classifiers. These data matrices, in turn, are classified in sequence by a new classifier at the second stage to generate output of that new classifier called Level2 data. Next, Weight Matrix algorithm is proposed to combine Level2 data and produces prediction for unlabeled observations. Experimental results on CLEF2009 medical image database demonstrate the benefit of our model in comparison with several existing ensemble learning models.", "num_citations": "7\n", "authors": ["1086"]}
{"title": "Simultaneous meta-data and meta-classifier selection in multiple classifier system\n", "abstract": " In ensemble systems, the predictions of base classifiers are aggregated by a combining algorithm (meta-classifier) to achieve better classification accuracy than using a single classifier. Experiments show that the performance of ensembles significantly depends on the choice of meta-classifier. Normally, the classifier selection method applied to an ensemble usually removes all the predictions of a classifier if this classifier is not selected in the final ensemble. Here we present an idea to only remove a subset of each classifier's prediction thereby introducing a simultaneous meta-data and meta-classifier selection method for ensemble systems. Our approach uses Cross Validation on the training set to generate meta-data as the predictions of base classifiers. We then use Ant Colony Optimization to search for the optimal subset of meta-data and meta-classifier for the data. By considering each column of meta-data, we\u00a0\u2026", "num_citations": "6\n", "authors": ["1086"]}
{"title": "A lossless online Bayesian classifier\n", "abstract": " We are living in a world progressively driven by data. Besides the issue that big data cannot be entirely stored in the main memory as required by traditional offline learning methods, the problem of learning data that can only be collected over time is also very prevalent. Consequently, there is a need of online methods which can handle sequentially arriving data and offer the same accuracy as offline methods. In this paper, we introduce a new lossless online Bayesian-based classifier which uses the arriving data in a 1-by-1 manner and discards each data right after use. The lossless property of our proposed method guarantees that it can reach the same prediction performance as its offline counterpart regardless of the incremental training order. Experimental results demonstrate its superior performance over many well-known state-of-the-art online learning methods in the literature.", "num_citations": "6\n", "authors": ["1086"]}
{"title": "An ensemble-based online learning algorithm for streaming data\n", "abstract": " In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Naive Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "num_citations": "6\n", "authors": ["1086"]}
{"title": "A novel Bayesian framework for online imbalanced learning\n", "abstract": " We present OCSB, a novel online Bayesian framework for imbalance multi-class data streams. To the best of our knowledge, OCSB is the first online method applying both cost-sensitive learning and sampling technique in a single classifier to deal with class imbalance learning. Specifically, an artificial cost matrix is designed and adapted in a sequential manner to not only boost the accuracy of minority classes but also guarantee stable Gmean - geometric mean of the accuracies for all classes. Furthermore, we introduce a new intermediate random sampling strategy with over- sampled minority classes and under-sampled majority classes. This offers twofold benefit: learning the rare classes properly and reducing the cost caused by the redundant data of majority classes. Experimental results show that our OCSB outperforms very recent well-known methods for online imbalanced learning algorithms in the literature.", "num_citations": "6\n", "authors": ["1086"]}
{"title": "Deep heterogeneous ensemble.\n", "abstract": " In recent years, deep neural networks (DNNs) have emerged as a powerful technique in many areas of  machine  learning.  Although  DNNs  have  achieved  great  breakthrough  in  processing  images,  video,  audio and text, it also has some limitations such as needing a large number of labeled data for training and having a large number of parameters. Ensemble learning, meanwhile, provides a learning model by combining many different classifiers such that an ensemble of classifiers is better than using single classifier. In this study, we propose a deep ensemble framework called Deep Heterogeneous Ensemble (DHE) for supervised learning tasks. In each layer of our algorithm, the input data is passed through a feature selection method to remove irrelevant features and prevent overfitting. The cross-validation with K learning algorithms is applied to the selected data, in order to obtain the meta-data and the K\u00a0\u2026", "num_citations": "4\n", "authors": ["1086"]}
{"title": "Evolving an optimal decision template for combining classifiers\n", "abstract": " In this paper, we aim to develop an effective combining algorithm for ensemble learning systems. The Decision Template method, one of the most popular combining algorithms for ensemble systems, does not perform well when working on certain datasets like those having imbalanced data. Moreover, point estimation by computing the average value on the outputs of base classifiers in the Decision Template method is sometimes not a good representation, especially for skewed datasets. Here we propose to search for an optimal decision template in the combining algorithm for a heterogeneous ensemble. To do this, we first generate the base classifier by training the pre-selected learning algorithms on the given training set. The meta-data of the training set is then generated via cross validation. Using the Artificial Bee Colony algorithm, we search for the optimal template that minimizes the empirical 0\u20131 loss\u00a0\u2026", "num_citations": "4\n", "authors": ["1086"]}
{"title": "Evolving interval-based representation for multiple classifier fusion\n", "abstract": " Designing an ensemble of classifiers is one of the popular research topics in machine learning since it can give better results than using each constituent member. Furthermore, the performance of ensemble can be improved using selection or adaptation. In the former, the optimal set of base classifiers, meta-classifier, original features, or meta-data is selected to obtain a better ensemble than using the entire classifiers and features. In the latter, the base classifiers or combining algorithms working on the outputs of the base classifiers are made to adapt to a particular problem. The adaptation here means that the parameters of these algorithms are trained to be optimal for each problem. In this study, we propose a novel evolving combining algorithm using the adaptation approach for the ensemble systems. Instead of using numerical value when computing the representation for each class, we propose to use the\u00a0\u2026", "num_citations": "3\n", "authors": ["1086"]}
{"title": "Multi-layer heterogeneous ensemble with classifier and feature selection\n", "abstract": " Deep Neural Networks have achieved many successes when applying to visual, text, and speech information in various domains. The crucial reasons behind these successes are the multi-layer architecture and the in-model feature transformation of deep learning models. These design principles have inspired other sub-fields of machine learning including ensemble learning. In recent years, there are some deep homogenous ensemble models introduced with a large number of classifiers in each layer. These models, thus, require a costly computational classification. Moreover, the existing deep ensemble models use all classifiers including unnecessary ones which can reduce the predictive accuracy of the ensemble. In this study, we propose a multi-layer ensemble learning framework called MUlti-Layer heterogeneous Ensemble System (MULES) to solve the classification problem. The proposed system works\u00a0\u2026", "num_citations": "3\n", "authors": ["1086"]}
{"title": "An online variational inference and ensemble based multi-label classifier for data streams\n", "abstract": " Recently, multi-label classification algorithms have been increasingly required by a diversity of applications, such as text categorization, web, and social media mining. In particular, these applications often have streams of data coming continuously, and require learning and predicting done on-the-fly. In this paper, we introduce a scalable online variational inference based ensemble method for classifying multi-label data, where random projections are used to create the ensemble system. As a second-order generative method, the proposed classifier can effectively exploit the underlying structure of the data during learning. Experiments on several real-world datasets demonstrate the superior performance of our new method over several well-known methods in the literature.", "num_citations": "3\n", "authors": ["1086"]}
{"title": "WEC: Weighted Ensemble of Text Classifiers\n", "abstract": " Text classification is one of the most important tasks in the field of Natural Language Processing. There are many approaches that focus on two main aspects: generating an effective representation; and selecting and refining algorithms to build the classification model. Traditional machine learning methods represent documents in vector space using features such as term frequencies, which have limitations in handling the order and semantics of words. Meanwhile, although achieving many successes, deep learning classifiers require substantial resources in terms of labelled data and computational complexity. In this work, a weighted ensemble of classifiers (WEC) is introduced to address the text classification problem. Instead of using majority vote as the combining method, we propose to associate each classifier's prediction with a different weight when combining classifiers. The optimal weights are obtained by\u00a0\u2026", "num_citations": "2\n", "authors": ["1086"]}
{"title": "Streaming active deep forest for evolving data stream classification\n", "abstract": " In recent years, Deep Neural Networks (DNNs) have gained progressive momentum in many areas of machine learning. The layer-by-layer process of DNNs has inspired the development of many deep models, including deep ensembles. The most notable deep ensemble-based model is Deep Forest, which can achieve highly competitive performance while having much fewer hyper-parameters comparing to DNNs. In spite of its huge success in the batch learning setting, no effort has been made to adapt Deep Forest to the context of evolving data streams. In this work, we introduce the Streaming Deep Forest (SDF) algorithm, a high-performance deep ensemble method specially adapted to stream classification. We also present the Augmented Variable Uncertainty (AVU) active learning strategy to reduce the labeling cost in the streaming context. We compare the proposed methods to state-of-the-art streaming algorithms in a wide range of datasets. The results show that by following the AVU active learning strategy, SDF with only 70\\% of labeling budget significantly outperforms other methods trained with all instances.", "num_citations": "2\n", "authors": ["1086"]}
{"title": "Automatic image region annotation by genetic algorithm-based joint classifier and feature selection in ensemble system\n", "abstract": " In this paper, we address the image region tagging procedure in which each image region is annotated by a suitable concept. Specifically, we first extract the feature vector for each segmented region. Then we propose a Genetic Algorithm (GA)-based simultaneous classifier and feature selection method working with ensemble system to learn the relationship between the low-level features and high-level concepts. The extensive experiments conducted on two public datasets namely MSRC v1 and MSRC v2 demonstrate the better performance of our method than several well-known ensemble methods, supervised machine learning methods, and sparse coding-based methods in the regions-in-image classification task.", "num_citations": "2\n", "authors": ["1086"]}
{"title": "Combining classifiers based on gaussian mixture model approach to ensemble data\n", "abstract": " Combining multiple classifiers to achieve better performance than any single classifier is one of the most important research areas in machine learning. In this paper, we focus on combining different classifiers to form an effective ensemble system. By introducing a novel framework operated on outputs of different classifiers, our aim is to build a powerful model which is competitive to other well-known combining algorithms such as Decision Template, Multiple Response Linear Regression (MLR), SCANN and fixed combining rules. Our approach is difference from the traditional approaches in that we use Gaussian Mixture Model (GMM) to model distribution of Level1 data and to predict the label of an observation based on maximizing the posterior probability realized through Bayes model. We also apply Principle Component Analysis (PCA) to output of base classifiers to reduce its dimension of what before\u00a0\u2026", "num_citations": "2\n", "authors": ["1086"]}
{"title": "Heterogeneous ensemble selection for evolving data streams\n", "abstract": " Ensemble learning has been widely applied to both batch data classification and streaming data classification. For the latter setting, most existing ensemble systems are homogenous, which means they are generated from only one type of learning model. In contrast, by combining several types of different learning models, a heterogeneous ensemble system can achieve greater diversity among its members, which helps to improve its performance. Although heterogeneous ensemble systems have achieved many successes in the batch classification setting, it is not trivial to extend them directly to the data stream setting. In this study, we propose a novel HEterogeneous Ensemble Selection (HEES) method, which dynamically selects an appropriate subset of base classifiers to predict data under the stream setting. We are inspired by the observation that a well-chosen subset of good base classifiers may outperform\u00a0\u2026", "num_citations": "1\n", "authors": ["1086"]}
{"title": "Evolved ensemble of detectors for gross error detection\n", "abstract": " In this study, we evolve an ensemble of detectors to check the presence of gross systematic errors on measurement data. We use the Fisher method to combine the output of different detectors and then test the hypothesis about the presence of gross errors based on the combined value. We further develop a detector selection approach in which a subset of detectors is selected for each sample. The selection is conducted by comparing the output of each detector to its associated selection threshold. The thresholds are obtained by minimizing the 0-1 loss function on training data using the Particle Swarm Optimization method. Experiments conducted on a simulated system confirm the advantages of ensemble and evolved ensemble approach.", "num_citations": "1\n", "authors": ["1086"]}
{"title": "A Hough Transform-Based Biclustering Algorithm for Gene Expression Data\n", "abstract": " In pattern classification, when the feature space is of high dimensionality or patterns are \u201csimilar\u201d on a subset of features only, the traditional clustering methods do not show good performance. Biclustering is a class of methods that simultaneously carry out grouping on two dimensions and has many applications to different fields, especially gene expression data analysis. Because of simultaneous classification on both rows and columns of a data matrix, the biclustering problem is inherently intractable and computationally complex. One of the most complex models in biclustering problem is linear coherent model. Several biclustering algorithms based on this model have been proposed in recent years. However, none of them is able to perfectly recognize all linear patterns in a bicluster. In this work, we propose a novel algorithm based on Hough transform that can find all linear coherent patterns. In the\u00a0\u2026", "num_citations": "1\n", "authors": ["1086"]}