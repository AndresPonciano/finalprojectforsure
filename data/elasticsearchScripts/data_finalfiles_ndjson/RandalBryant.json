{"title": "Symbolic boolean manipulation with ordered binary-decision diagrams\n", "abstract": " Ordered Binary-Decision Diagrams (OBDDs) represent Boolean functions as directed acyclic graphs. They form a canonical representation, making testing of functional properties such as satisfiability and equivalence straightforward. A number of operations on Boolean functions can be implemented as graph algorithms on OBDD data structures. Using OBDDs, a wide variety of problems can be solved through symbolic analysis. First, the possible variations in system parameters and operating conditions are encoded with Boolean variables. Then the system is evaluated for all variations by a sequence of OBDD operations. Researchers have thus solved a number of problems in digital-system design, finite-state system analysis, artificial intelligence, and mathematical logic. This paper describes the OBDD data structure and surveys a number of applications that have been solved by OBDD-based symbolic analysis.", "num_citations": "3055\n", "authors": ["1779"]}
{"title": "Ef\ufb01cient implementation of a BDD package\n", "abstract": " Efficient manipulation of Boolean functions is an important component of many computer-aided design tasks. This paper describes a package for manipulating Boolean functions based on the reduced, ordered, binary decision diagram (ROBDD) representation. The package is based on an e\ufb01icient implementation of the if-then\u2014elsc (ITE) operator. A hash table is used to maintain a strong canonical form in the ROBDD, and memory use is improved by merging the hash table and the ROBDD into a hybrid data structure. A memory function for the recursive ITE algorithm is implemented using a hash-based cache to decrease memory use. Memory function efficiency is improved by using rules that detect when equivalent functions are computed. The usefulness of the package is enhanced by an automatic and low-cost scheme for recycling memory. Experimental results are given to demonstrate why various\u00a0\u2026", "num_citations": "1793\n", "authors": ["1779"]}
{"title": "On the complexity of VLSI implementations and graph representations of Boolean functions with application to integer multiplication\n", "abstract": " This paper presents lower bound results on Boolean function complexity under two di erent models. The rst is an abstraction of tradeo s between chip area and speed in very large scale integrated (VLSI) circuits. The second is the ordered binary decision diagram (OBDD) representation used as a data structure for symbolically representing and manipulating Boolean functions. These lower bounds demonstrate the fundamental limitations of VLSI as an implementation medium, and OBDDs as a data structure. They also lend insight into what properties of a Boolean function lead to high complexity under these models.Related techniques can be used to prove lower bounds in the two models. That is, the same technique used to prove that any VLSI implementation of a single output Boolean function has area-time complexity AT2=(n2) also proves that any OBDD representation of the function has (cn) vertices for some c> 1. The converse is not true, however. There are functions for which any OBDD representation is of exponential size, but there is a VLSI implementation of complexity AT2= O (n1+). Consider an integer multiplier for word size n with outputs numbered 0 (least signi cant) through 2n? 1 (most signi cant). For the Boolean function representing either output i? 1 or output 2n? i? 1, where 1 i n, the following lower bounds are proved:(1) any VLSI implementation must have AT2=(i2), and (2) any OBDD representation must have (1: 09i) vertices.", "num_citations": "735\n", "authors": ["1779"]}
{"title": "Computer systems: a programmer's perspective\n", "abstract": " This book (CS: APP) is for programmers who want to improve their skills by learning what is going on \u201cunder the hood\u201d of a computer system.Our aim is to explain the enduring concepts underlying all computer systems, and to show you the concrete ways that these ideas affect the correctness, performance, and utility of your application programs. Unlike other systems books, which are written primarily for system builders, this book is written for programmers, from a programmer\u2019s perspective.", "num_citations": "706\n", "authors": ["1779"]}
{"title": "Verification of arithmetic circuits with binary moment diagrams\n", "abstract": " Binary Moment Diagrams (BMDs) provide a canonical representations for linear functions similar to the way Binary Decision Diagrams (BDDs) represent Boolean functions. Within the class of linear functions, we can embed arbitrary functions from Boolean variables to integer values. BMDs can thus model the functionality of data path circuits operating over word-level data. Many important functions, including integermultiplication, that cannot be represented efficiently at the bit level with BDDs have simple representations at the word level with BMDs. Furthermore, BMDs can represent Boolean functions with around the same complexity as BDDs. We propose a hierarchical approach to verifying arithmetic circuits, where componentmodules are first shownto implement their word-level specifications. The overall circuit functionality is then verified by composing the component functions and comparing the result to the\u00a0\u2026", "num_citations": "623\n", "authors": ["1779"]}
{"title": "A switch-level model and simulator for MOS digital systems\n", "abstract": " The switch-level model describes the logical behavior of digital systems implemented in metal oxide semiconductor (MOS) technology. In this model a network consists of a set of nodes connected by transistor \"switches\" with each node having a state 0, 1, or X (for invalid or uninitialized), and each transistor having a state \"open,\" \"closed,\" or \"indeterminate.\" Many characteristics of MOS circuits can be modeled accurately, including: ratioed, complementary, and precharged logic; dynamic and static storage; (bidirectional) pass transistors; buses; charge sharing; and sneak paths. In this paper we present a formal development of the switch-level model starting from a description of circuit behavior in terms of switch graphs. Then we describe an algorithm for a logic simulator based on the switch-level model which computes the new state of the network by solving a set of equations in a simple, discrete algebra. This\u00a0\u2026", "num_citations": "532\n", "authors": ["1779"]}
{"title": "Simulation of packet communication architecture computer systems\n", "abstract": " Simulations of computer systems have traditionally been performed on a single, sequential computer, even if the system to be simulated contains a number of components which operate concurrently. An alternative would be to simulate these systems on a network of processors. With this approach, each processor would simulate one component of the system, hence the component simulations could proceed concurrently. By exploiting the modularity and concurrency in the system to be simulated, the simulation would itself be modular and concurrent. An accurate simulation must model the time behavior of the system as well as its input-output behavior. In order to avoid real-time constraints on the processors and communication network in the simulation facility, the simulation of the timing must use a time-independent algorithm. That is, the simulated behavior of each component should not depend on the speed at which the simulation is performed. With this time-independent approach, additional coordination operations are required to prevent a deadlock of the simulation. This coordination can be provided without any centralized control. Instead, the program for the simulation of each component is modified, so that each component simulation will communicate status information to other component simulations. Additional termination operations are also required to assure that the simulation will terminate under the exact same conditions that the system being simulated would terminate. These operations can also be provided without any centralization of control or real-time constraints. Furthermore, a simulation which uses these coordination and\u00a0\u2026", "num_citations": "501\n", "authors": ["1779"]}
{"title": "Formal verification by symbolic evaluation of partially-ordered trajectories\n", "abstract": " Symbolic trajectory evaluation provides a means to formally verify properties of a sequential system by a modified form of symbolic simulation. The desired system properties are expressed in a notation combining Boolean expressions and the temporal logic \u201cnext-time\u201d operator. In its simplest form, each property is expressed as an assertion [A\u21d2C], where the antecedentA expresses some assumed conditions on the system state over a bounded time period, and the consequentC expresses conditions that should result. A generalization allows simple invariants to be established and proven automatically.               The verifier operates on system models in which the state space is ordered by \u201cinformation content\u201d. By suitable restrictions to the specification notation, we guarantee that for every trajectory formula, there is a unique weakest state trajectory that satisfies it. Therefore, we can verify an assertion [A\u21d2C\u00a0\u2026", "num_citations": "367\n", "authors": ["1779"]}
{"title": "Simulator for MOS Circuits\n", "abstract": " The COSMOS simulator provides fast and accurate switch-level modeling of MOS digital circuits. It attains high performance by preprocessing the transistor network into a functionally equivalent Boolean representation. This description, produced by the symbolic analyzer ANAMOS, captures all aspects of switch-level networks including bidirectional transistors, stored charge, different signal strengths, and indeterminate (X) logic values. The LGCC program translates the Boolean representation into a set of machine language evaluation procedures and initialized data structures. These procedures and data structures are compiled along with code implementing the simulation kernel and user interface to produce the simulation program. The simulation program runs an order of magnitude faster than our previous simulator MOSSIM II.", "num_citations": "360\n", "authors": ["1779"]}
{"title": "Binary decision diagrams and beyond: Enabling technologies for formal verification\n", "abstract": " Ordered Binary Decision Diagrams (OBDDs) have found widespread use in CAD applications such as formal verification, logic synthesis, and test generation. OBDDs represent Boolean functions in a form that is both canonical and compact for many practical cases. They can be generated and manipulated by efficient graph algorithms. Researchers have found that many tasks can be expressed as series of operations on Boolean functions, making them candidates for OBDD-based methods. The success of OBDDs has inspired efforts to improve their efficiency and to expand their range of applicability. Techniques have been discovered to make the representation more compact and to represent other classes of functions. This has led to improved performance on existing OBDD applications, as well as enabled new classes of problems to be solved. This paper provides an overview of the state of the art in graph\u00a0\u2026", "num_citations": "350\n", "authors": ["1779"]}
{"title": "Big-data computing: creating revolutionary breakthroughs in commerce, science and society\n", "abstract": " Advances in digital sensors, communications, computation, and storage have created huge collections of data, capturing information of value to business, science, government, and society. For example, search engine companies such as Google, Yahoo!, and Microsoft have created an entirely new business by capturing the information freely available on the World Wide Web and providing it to people in useful ways. These companies collect trillions of bytes of data every day and continually add new services such as satellite images, driving directions, and image retrieval. The societal benefits of these services are immeasurable, having transformed how people find and make use of information on a daily basis.", "num_citations": "348\n", "authors": ["1779"]}
{"title": "Effective use of boolean satisfiability procedures in the formal verification of superscalar and vliw microprocessors\n", "abstract": " We compare SAT-checkers and decision diagrams on the evaluation of Boolean formulae produced in the formal verification of both correct and buggy versions of superscalar and VLIW microprocessors. The microprocessors are described in a high-level hardware description language, based on the logic of equality with uninterpreted functions and memories (EUFM). The formal verification is done with Burch and Dill\u2019s correctness criterion, using flushing to map the state of the implementation processor to the state of the specification. The EUFM correctness formula is translated to an equivalent Boolean formula by exploiting the property of positive equality, and using the automatic tool EVC. We identify the SAT-checkers Chaff and BerkMin as significantly outperforming the rest of the SAT tools when evaluating the Boolean correctness formulae. We examine ways to enhance the performance of Chaff and BerkMin\u00a0\u2026", "num_citations": "294\n", "authors": ["1779"]}
{"title": "Boolean analysis of MOS circuits\n", "abstract": " The switch-level model represents a digital metal-oxide semiconductor (MOS) circuit as a network of charge storage nodes connected by resistive transistor switches. The functionality of such a network can be expressed as a series of systems of Boolean equations. Solving these equations symbolically yields a set of Boolean formulas that describe the mapping from input and current state to the new network state. This analysis supports the same class of networks as the switch-level simulator MOSSIM II and provides the same functionality, including the handling of bidirectional e ects and indeterminate (X) logic values. In the worst case, the analysis of an n node network can yield a set of formulas containing a total of O (n3) operations. However, all but a limited set of dense, pass-transistor networks give formulas with O (n) total operations. The analysis can serve as the basis of e cient programs for a variety of logic design tasks, including: logic simulation (on both conventional and special purpose computers), fault simulation, test generation, and symbolic veri cation.", "num_citations": "255\n", "authors": ["1779"]}
{"title": "Data-intensive supercomputing: The case for DISC\n", "abstract": " Google and its competitors have created a new class of large-scale computer systems to support Internet search. These \u201cData-Intensive Super Computing\u201d(DISC) systems differ from conventional supercomputers in their focus on data: they acquire and maintain continually changing data sets, in addition to performing large-scale computations over the data. With the massive amounts of data arising from such diverse sources as telescope imagery, medical records, online transaction records, and web pages, DISC systems have the potential to achieve major advances in science, health care, business efficiencies, and information access. DISC opens up many important research topics in system design, resource management, programming models, parallel algorithms, and applications. By engaging the academic research community in these issues, we can more systematically and in a more open forum explore fundamental aspects of a societally important style of computing.", "num_citations": "237\n", "authors": ["1779"]}
{"title": "Symbolic manipulation of boolean functions using a graphical representation\n", "abstract": " In this paper we describe a data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations of Lee and Akers, but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms are quite efficient as long as the graphs being operated on do not grow too large. We present performance measurements obtained while applying these algorithms to problems in logic design verification.", "num_citations": "180\n", "authors": ["1779"]}
{"title": "Formally verifying a microprocessor using a simulation methodology\n", "abstract": " Formal verification is becoming a useful means of validating designs. We have developed a methodology for formally verifying dataintensive circuits (e.g., processors) with sophisticated timing (e.g., pipelining) against high-level declarative specifications. Previously, formally verifying a microprocessor required the use of an automatic theorem prover, but our technique requires little more than a symbolic simulator. We have formally verified a pre-existing 16-bit CISC microprocessor circuit extracted from the fabricated layout.", "num_citations": "166\n", "authors": ["1779"]}
{"title": "Processor verification using efficient reductions of the logic of uninterpreted functions to propositional logic\n", "abstract": " The logic of Equality with Uninterpreted Functions (EUF) provides a means of abstracting the manipulation of data by a processor when verifying the correctness of its control logic. By reducing formulas in this logic to propositional formulas, we can apply Boolean methods such as ordered Binary Decision Diagrams (BDDs) and Boolean satisfiability checkers to perform the verification. We can exploit characteristics of the formulas describing the verification conditions to greatly simplfy the propostional formulas generated. We identify a class of terms we call \u201cp-terms\u201d for which equality comparisons can only be used in monotonically positive formulas. By applying suitable abstractions to the hardware model, we can express the functionality of data values and instruction    addresses flowing through an instruction pipeline with p-terms. A decision procedure can exploit the restricted uses of p-terms by considering only\u00a0\u2026", "num_citations": "164\n", "authors": ["1779"]}
{"title": "Formal hardware verification by symbolic ternary trajectory evaluation\n", "abstract": " Symbolic trajectory evaluation is a new approach to formal hardware verification combining the circuit modeling capabilities of symbolic logic simulation with some of the analytic methods found in temporal logic model checkers. We have created such an evaluator by extending the symbolic switch-level simulator COSMOS. This program gains added efficiency by exploiting the ability of COSMOS to evaluate circuit operation over a ternary logic model, where the third value X represents an unknown logic value. This program can formally verify systems containing complex featurea such as switch-level models, detailed timing, and pipelining.", "num_citations": "154\n", "authors": ["1779"]}
{"title": "An algorithm for MOS logic simulation\n", "abstract": " Randal E. Bryant, Massachusetts Institute of Technology ith the introduction of simplified MOS design techniques by Mead and Conway (1980) we have seen a proliferation of custom LSI systems designed by non-specialists. For most people, however, VI Sl design is still the ultimate\" batch job\"\u2013a very conplex system must be designed, hand debugged and fabricated before it can be tried out, by which time no corrections can be made, If we are to realize the full potential of custom VLSI, we cannot spend the time, patience, and personnel to handverify designs and iterate through several prototypes, as currently done in industry. Fortunately, well-designed sim-ulation tools can eliminate much of this tedium, and greatly increase the chanes of first-time success. Traditionally, MOS designers have been forced to use rither logi-gate simulators or circuit sinulators such as SPICE (Nagel 1975), Neither choice works well\u00a0\u2026", "num_citations": "149\n", "authors": ["1779"]}
{"title": "Formal verification of superscalar microprocessors with multicycle functional units, exception, and branch prediction\n", "abstract": " We extend the Burch and Dill flushing technique [6] for formal verification of microprocessors to be applicable to designs where the functional units and memories have multicycle and possibly arbitrary latency. We also show ways to incorporate exceptions and branch prediction by exploiting the properties of the logic of Positive Equality with Uninterpreted Functions [4][5]. We study the modeling of the above features in different versions of dual-issue superscalar processors.", "num_citations": "143\n", "authors": ["1779"]}
{"title": "Exploiting positive equality in a logic of equality with uninterpreted functions\n", "abstract": " In using the logic of equality with unininterpreted functions to verify hardware systems, specific characteristics of the formula describing the correctness condition can be exploited when deciding its validity.We distinguish a class of terms we call \u201cp-terms\u201d for which equality comparisons can appear only in monotonically positive formulas. By applying suitable abstractions to the hardware model, we can express the functionality of data values and instruction addresses flowing through an instruction pipeline with p-terms. Adecision procedure can exploit the restricted uses of p-terms by considering only \u201cmaximally diverse\u201d interpretations of the associated function symbols, where every function application yields a different value except when constrained by functional consistency.We present a procedure that translates the original formula into one in propositional logic by interpreting the formula over a domain of\u00a0\u2026", "num_citations": "138\n", "authors": ["1779"]}
{"title": "MOSSIM: A switch-level simulator for MOS LSI\n", "abstract": " The logic simulator MOSSIM Is designed specifically to serve the needs of the MOS LSI designer. It models a MOS circuit as a network of field-effect transistor \u201cswitches\u201d, with node states 0, 1, and X (unknown) and transistor states\u201copen\u201d,\u201cclosed\u201d, and \u201cunknown\u201d. MOSSIM has proved quite versatile and accurate In simulating a variety of MOS designs lncludlng ones for which the network was extracted automatically from the mask specifications. Because it models the network at a logical level, It has a performance comparable to conventional logic gate aimuistora. lntroductlonRecently, a new class of logic simulator has emerged speclflcally for the MOS designer. These switch-level simulators, including the author\u2019s MOSSiM4 as well as others*. model a MOS system as a network of nodes connected by transistor\u201cswitches\u201d with each node assuming a state 0, 1, and X (unknown) and each transistor a state\u201copen\u00a0\u2026", "num_citations": "127\n", "authors": ["1779"]}
{"title": "Symbolic simulation-techniques and applications\n", "abstract": " Basic principles of symbolic simulation and its applications in formal circuit verification and automatic test generation are discussed. Historical perspective and recent activities in development of Boolean algebraic approaches, symbolic reasoning methods about continuous systems, and weak signal algebras are addressed.< >", "num_citations": "126\n", "authors": ["1779"]}
{"title": "Superscalar processor verification using efficient reductions of the logic of equality with uninterpreted functions to propositional logic\n", "abstract": " We present a collection of ideas that allows the pipeline verification method pioneered by Burch and Dill [5] to scale very efficiently to dual-issue super- scalar processors. We achieve a significant speedup in the verification of such processors, compared to the result by Burch [6], while using an entirely automatic tool. Instrumental to our success are exploiting the properties of positive equality [3][4] and the simplification capabilities of BDDs.", "num_citations": "116\n", "authors": ["1779"]}
{"title": "Test pattern generation for sequential MOS circuits by symbolic fault simulation\n", "abstract": " The COSMOS symbolic fault simulator generates test sets for combinational and sequential MOS circuits represented at the switch level. All aspects of switch-level networks including bidirectional transistors, stored charge, different signal strengths, and indeterminate (X) logic values are captured. To generate tests for a circuit, the program derives Boolean functions representing the behavior of the good and faulty circuits over a sequence of symbolic input patterns. It then determines a set of assignments to the input variables that will detect all faults. Symbolic simulation provides a natural framework for the user to supply an overall test strategy, letting the program determine the detailed conditions to detect a set of faults. Symbolic preprocessing of switch-level networks, combined with efficient Boolean manipulation makes this approach feasible.", "num_citations": "102\n", "authors": ["1779"]}
{"title": "A survey of switch-level algorithms\n", "abstract": " The switch-level model provides a logical abstraction from the physical structure of a metal-oxide semiconductor(MOS) circuit to its digital behavior. At the switch level, a circuit is modeled as a network of transistor switches connecting a set of charge storage nodes. Node voltages are represented by discrete logic levels, and electrical behavior is modeled in a highly simplified way. Switch-level algorithms have been applied to such tasks as logic and fault simulation, formal hardware verification, timing analysis, and automatic test program generation. They have been implemented on sequential and parallel computers as well as by hardware simulation accelerators.", "num_citations": "102\n", "authors": ["1779"]}
{"title": "Formal verification of digital circuits using symbolic ternary system models\n", "abstract": " Ternary system modeling involves extending the traditional set of binary values {0, 1} with a third value X indicating an unknown or indeterminate condition. By making this extension, we can model a wider range of circuit phenomena. We can also efficiently verify sequential circuits in which the effect of a given operation depends on only a subset of the total system state.             This paper presents a formal methodology for verifying synchronous digital circuits using a ternary system model. The desired behavior of the circuit is expressed as assertions in a notation using a combination of Boolean expressions and temporal logic operators. An assertion is verified by translating it into a sequence of patterns and checks for a ternary symbolic simulator. The methodology has been used to verify a number of full scale designs.", "num_citations": "97\n", "authors": ["1779"]}
{"title": "Algorithmic aspects of symbolic switch network analysis\n", "abstract": " A network of switches controlled by Boolean variables can be represented as a system of Boolean equations. The solution of this system gives a symbolic description of the conducting paths in the network. Gaussian elimination provides an e cient technique for solving sparse systems of Boolean equations. For the class of networks that arise when analyzing digital metal-oxide semiconductor (MOS) circuits, a simple pivot selection rule guarantees that most s switch networks encountered in practice can be solved with O (s) operations. When represented by a directed acyclic graph, the set of Boolean formulas generated by the analysis has total size bounded by the number of operations required by the Gaussian elimination. This paper presents the mathematical basis for systems of Boolean equations, their solution by Gaussian elimination, and data structures and algorithms for representing and manipulating Boolean formulas.", "num_citations": "97\n", "authors": ["1779"]}
{"title": "Simulation on a distributed system\n", "abstract": " The simulation of a discrete event system naturally decomposes into a set of concurrent pro\u00e7e\u015f\u015fe\u015f with each process sinulating the behavior of one system ehement. A distribute i system could beat exploit thls concurrency H the processes interact only by mess nge-passlng, anci this controi of the slnuiation ls det tett rializari. ln this papat a simulatlon method Is tin velopil In whith the pressas sinulata 1hs intaractions of the Rystorir keinerts ty set ding stimutus messages to one onother an coordlnate thair activitlas wlth cle centralizeci control, ir to the procuss programs. The control does not place any real-time speed constralnts on the computation or conmunicatiun and does not require any further communication ilnks between processes. Some of the concepts developed hare could be applied to other types of distributed systems as well,", "num_citations": "96\n", "authors": ["1779"]}
{"title": "Symbolic verification of MOS circuits\n", "abstract": " The program MOSSYM simulates the behavior of a MOS circuit represented as a switch-level network symbolically. That is, during simulator operation the user can set an input to either 0, 1. or a Boolean variable. The simulator then computes the behavior of the circuit as a function of the past and present input variables. By using heuristically efficient Boolean function manipulation algorithms, the verification of a circuit by symbolic simulation can proceed much more quickly than by exhaustive logic simulation. In this paper we present our concept of symbolic simulation, derive an algorithm for switch-level symbolic simulation, and present experimental measurements from MOSSYM.", "num_citations": "94\n", "authors": ["1779"]}
{"title": "A methodology for hardware verification based on logic simulation\n", "abstract": " A logic simulator can prove the correctness of a digital circuit if it can be shown that only circuits fulfilling the system specification will produce a particular response to a sequence of simulation commands.This style of verification has advantages over the other proof methods in being readily automated and requiring less attention on the part of the user to the low-level details of the design. It has advantages over other approaches to simulation in providing more reliable results, often at a comparable cost. This paper presents the theoretical foundations of several related approaches to circuit verification based on logic simulation. These approaches exploit the three-valued modeling capability found in most logic simulators, where the third-value X   indicates a signal with unknown digital value. Although the circuit verification problem is NP-hard as measured in the size of the circuit description, several techniques can\u00a0\u2026", "num_citations": "93\n", "authors": ["1779"]}
{"title": "SetA^*: An Efficient BDD-Based Heuristic Search Algorithm\n", "abstract": " In this paper we combine the goal directed search of A* with the ability of BDDs to traverse an exponential number of states in polynomial time. We introduce a new algorithm, SetA*, that generalizes A* to expand sets of states in each iteration. SetA* has substantial advantages over BDDA*, the only previous BDD-based A* implementation we are aware of. Our experimental evaluation proves SetA* to be a powerful search paradigm. For some of the studied problems it outperforms BDDA*, A*, and BDD-based breadth-first search by several orders of magnitude. We believe exploring sets of states to be essential when the heuristic function is weak. For problems with strong heuristics, SetA* efficiently specializes to single-state search and consequently challenges single-state heuristic search in general.", "num_citations": "83\n", "authors": ["1779"]}
{"title": "Extraction of gate-level models from transistor circuits by four-valued symbolic analysis\n", "abstract": " The program TRANALYZE generates a gate-level representation of an MOS transistor circuit. The resulting model contains only four-valued unit and zero delay logic primitives, suitable for evaluation by conventional gate-level simulators and hardware simulation accelerators. TRANA-LYZE has the same generality and accuracy as switch-level simulation, generating models for a wide range of technologies and design styles, while expressing the detailed effects of bidirectional transistors, stored charge, and multiple signal strengths. It produces models with size comparable to ones generated by hand.", "num_citations": "81\n", "authors": ["1779"]}
{"title": "Formal verification of content addressable memories using symbolic trajectory evaluation\n", "abstract": " In this paper we report on new techniques for verifying contentaddressable memories (CAMs), and demonstrate that these techniqueswork well for large industrial designs. It was shown in [Formal verification of PowerPC (TM) arrays using symbolic trajectory evaluation], that theformal verification technique of symbolic trajectory evaluation (STE) could be used successfully on memory arrays. We have extended thatwork to verify what are perhaps the most combinatorially difficultclass of memory arrays, CAMs. We use new Boolean encodings toverify CAMs, and show that these techniques scale well, in that spacerequirements increase linearly, or sub-linearly, with the various CAMsize parameters. In this paper, we describe the verification of two CAMs froma recentPowerPC microprocessor design, a Block Address Translation unit (BAT), and a Branch Target Address Cache unit (BTAC). The BATis a complex CAM\u00a0\u2026", "num_citations": "79\n", "authors": ["1779"]}
{"title": "Boolean satisfiability with transitivity constraints\n", "abstract": " We consider a variant of the Boolean satisfiability problem where a subset \u03f5 of the propositional variables appearing in formula Fsat encode a symmetric, transitive, binary relation over N elements. Each of these relational variables, ei,j, for 1 \u2264 i < j \u2264 N, expresses whether or not the relation holds between elements i and j. The task is to either find a satisfying assignment to Fsat that also satisfies all transitivity constraints over the relational variables (e.g., e1,2 \u2227 e2,3 \u21d2 e1,3), or to prove that no such assignment exists. Solving this satisfiability problem is the final and most difficult step in our decision procedure for a logic of equality with uninterpreted functions. This procedure forms the core of our tool for verifying pipelined microprocessors.To use a conventional Boolean satisfiability checker, we augment the set of clauses expressing Fsat with clauses expressing the transitivity constraints. We consider methods to\u00a0\u2026", "num_citations": "75\n", "authors": ["1779"]}
{"title": "Data-intensive scalable computing for scientific applications\n", "abstract": " Increasingly, scientific computing applications must accumulate and manage massive datasets, as well as perform sophisticated computations over these data. Such applications call for data-intensive scalable computer (DISC) systems, which differ in fundamental ways from existing high-performance computing systems.", "num_citations": "66\n", "authors": ["1779"]}
{"title": "Bit-level analysis of an SRT divider circuit\n", "abstract": " It is impractical to verify multiplier or divider circuits entirely at the bit-level using ordered Binary Decision Diagrams (BDDs), because the BDD representations for these functions grow exponentially with the word size. It is possible, however, to analyze individual stages of these circuits using BDDs. Such analysis can be helpful when implementing complex arithmetic algorithms. As a demonstration, we show that Intel could have used BDDs to detect erroneous lookup table entries in the Pentium (TM) floating point divider. Going beyond verification, we show that bit-level analysis can be used to generate a correct version of the table.", "num_citations": "66\n", "authors": ["1779"]}
{"title": "Predicate abstraction with indexed predicates\n", "abstract": " Predicate abstraction provides a powerful tool for verifying properties of infinite-state systems using a combination of a decision procedure for a subset of first-order logic and symbolic methods originally developed for finite-state model checking. We consider models containing first-order state variables, where the system state includes mutable functions and predicates. Such a model can describe systems containing arbitrarily large memories, buffers, and arrays of identical processes. We describe a form of predicate abstraction that constructs a formula over a set of universally quantified variables to describe invariant properties of the first-order state variables. We provide a formal justification of the soundness of our approach and describe how it has been used to verify several hardware and software designs, including a directory-based cache coherence protocol.", "num_citations": "63\n", "authors": ["1779"]}
{"title": "* PHDD: An efficient graph representation for floating point circuit verification\n", "abstract": " Data structures such as *BMDs, HDDs, and K*BMDs provide compact representations for functions which map Boolean vectors into integer values, but not floating point values. We propose a new data structure, called Multiplicative Power Hybrid Decision Diagrams (*PHDDs), to provide a compact representation for functions that map Boolean vectors into integer or floating point values. The size of the graph to represent the IEEE floating point encoding is linear with the word size. The complexity of floating point multiplication grows linearly with the word size. The complexity of floating point addition grows exponentially with the size of the exponent part, but linearly with the size of the mantissa part. We applied *PHDDs to verify integer multipliers and floating point multipliers before the rounding stage, based on a hierarchical verification approach. For integer multipliers, our results are at least 6 times faster than\u00a0\u2026", "num_citations": "62\n", "authors": ["1779"]}
{"title": "Formal verification of PowerPC arrays using symbolic trajectory evaluation\n", "abstract": " Verifying memory arrays such as on-chip caches and register files is a difficult part of designing a microprocessor. Current tools cannot verify the equivalence of the arrays to their behavioral or RTL models, nor their correct functioning at the transistor level. It is infeasible to run the number of simulation cycles required, and most formal verification tools break down due to the enormous number of state-holding elements in the arrays. The formal method of symbolic trajectory evaluation (STE) appears to offer a solution, however, STE verifies that a circuit satisfies a formula in a carefully restricted temporal logic. For arrays, it requires only a number of variables approximately logarithmic in the number of memory locations. The circuit is modeled at the switch level, so the verification is done on the actual design. We have used STE to verify two arrays from PowerPC microprocessors: a register file, and a data cache tag unit\u00a0\u2026", "num_citations": "62\n", "authors": ["1779"]}
{"title": "ACV: An arithmetic circuit verifier\n", "abstract": " Based on a hierarchical verification methodology, we present an arithmetic circuit verifier ACV, in which circuits expressed in a hardware description language, also called ACV, are symbolically verified using binary decision diagrams for Boolean functions and multiplicative binary moment diagrams (BMDs) for word-level functions. A circuit is described in ACV as a hierarchy of modules. Each module has a structural definition as an interconnection of logic gates and other modules. Modules may also have functional descriptions, declaring the numeric encodings of the inputs and outputs, as well as specifying their functionality in terms of arithmetic expressions. Verification then proceeds recursively, proving that each module in the hierarchy having a functional description, including the top-level one, realizes its specification. The language and the verifier contain additional enhancements for overcoming some of the\u00a0\u2026", "num_citations": "60\n", "authors": ["1779"]}
{"title": "Formal verification of an ARM processor\n", "abstract": " This paper presents a detailed description of the application of a formal verification methodology to an ARM processor. The processor, a hybrid between the ARM7 and the StrongARM processors, uses features such as a 5-stage instruction pipeline, predicated execution, forwarding logic and multi-cycle instructions. The instruction set of the processor was defined as a set of abstract assertions. An implementation mapping was used to relate the abstract states in these assertions to detailed circuit states in the gate-level implementation of the processor. Symbolic Trajectory Evaluation was used to verify that the circuit fulfills each abstract assertion under the implementation mapping. The verification was done concurrently with the design implementation of the processor. Our verification did uncover 4 bugs that were reported back to the designer.", "num_citations": "59\n", "authors": ["1779"]}
{"title": "A switch-level simulation model for integrated logic circuits\n", "abstract": " Switch-level simulators model a metal oxide semiconductor (MOS) large scale integrated (LSI) circuit as a network of transistor\" switches\". They can simulate many aspects of MOS circuits which cannot be expressed\u00a1 n the Boolean logic gale mode!, such as b\u00a1 directiona! pass transistors, dynamic sun age, and charge sharing Furthermore, Uic logic network can be extracted directly from the mask specification by a relatively straightforward computer program. Unlike analog circuit simulators, however, the nodes are assigned divertite states 0, 1, and X (for unknown), and the transistors, are assigned discrete states\" opcn\"\\\" closed\", and\" unknown\". As a consequence, switch-level simulators operate at speeds comparable to logic gate simulators.In this thesis, it formal model of switch-level networks is developed. rlTtc networks in this model may contain transistors of different strengths and types, as well as nodes of\u00a0\u2026", "num_citations": "59\n", "authors": ["1779"]}
{"title": "Ordered binary decision diagrams\n", "abstract": " Ordered Binary Decision Diagrams (OBDDs) play a key role in the automated synthesis and formal verification of digital systems. They are the state-of-the-art data structure for representing switching functions in various branches of electronic design automation. In the following we discuss the properties of this data structure, characterize its algorithmic behavior, and describe some prominent applications.", "num_citations": "57\n", "authors": ["1779"]}
{"title": "Deciding quantifier-free Presburger formulas using parameterized solution bounds\n", "abstract": " Given a formula in quantifier-free Presburger arithmetic, if it has a satisfying solution, there is one whose size, measured in bits, is polynomially bounded in the size of the formula. In this paper, we consider a special class of quantifier-free Presburger formulas in which most linear constraints are difference (separation) constraints, and the non-difference constraints are sparse. This class has been observed to commonly occur in software verification. We derive a new solution bound in terms of parameters characterizing the sparseness of linear constraints and the number of non-difference constraints, in addition to traditional measures of formula size. In particular, we show that the number of bits needed per integer variable is linear in the number of non-difference constraints and logarithmic in the number and size of non-zero coefficients in them, but is otherwise independent of the total number of linear constraints in the formula. The derived bound can be used in a decision procedure based on instantiating integer variables over a finite domain and translating the input quantifier-free Presburger formula to an equi-satisfiable Boolean formula, which is then checked using a Boolean satisfiability solver. In addition to our main theoretical result, we discuss several optimizations for deriving tighter bounds in practice. Empirical evidence indicates that our decision procedure can greatly outperform other decision procedures.", "num_citations": "55\n", "authors": ["1779"]}
{"title": "Deductive verification of advanced out-of-order microprocessors\n", "abstract": " This paper demonstrates the modeling and deductive verification of out-of-order microprocessors of varying complexities using a logic of Counter Arithmetic with Lambda Expressions and Uninterpreted Functions (CLU). The microprocessors support combinations of out-of-order instruction execution, superscalar operation, branch prediction, execute and memory exceptions, and load-store buffering. We illustrate that the logic is expressive enough to model components found in modern processors. The paper describes the challenges in modeling and verification with the addition of different design features. The paper demonstrates the effective use of automatic decision procedure to reduce the amount of manual guidance required in discharging most proof obligations in the verification. Unlike previous methods, the verification scales well for superscalar processors with wide dispatch and retirement widths.", "num_citations": "54\n", "authors": ["1779"]}
{"title": "Exploiting positive equality and partial non-consistency in the formal verification of pipelined microprocessors\n", "abstract": " We study the applicability of the logic of Positive Equality with Uninterpreted Functions (PEUF)[2][3] to the verification of pipelined microprocessors with very large Instruction Set Architectures (ISAs). Abstraction of memory arrays and functional units is employed, while the control logic of the processors is kept intact from the original gate-level designs. PEUF is an extension of the logic of Equality with Uninterpreted Functions, introduced by Burch and Dill [4], that allows us to use distinct constants for the data operands and instruction addresses needed in the symbolic expression for the correctness criterion. We present several techniques that make PEUF scale very efficiently for the verification of pipelined microprocessors with large ISAs. These techniques are based on allowing a limited form of non-consistency in the uninterpreted functions, representing initial memory state and ALU behaviors. Our tool required\u00a0\u2026", "num_citations": "51\n", "authors": ["1779"]}
{"title": "dbug: Systematic evaluation of distributed systems\n", "abstract": " This paper presents the design, implementation and evaluation of \u201cdBug\u201d\u2013a tool that leverages manual instrumentation for systematic evaluation of distributed and concurrent systems. Specifically, for a given distributed concurrent system, its initial state and a workload, the dBug tool systematically explores possible orders in which concurrent events triggered by the workload can happen. Further, dBug optionally uses the partial order reduction mechanism to avoid exploration of equivalent orders. Provided with a correctness check, the dBug tool is able to verify that all possible serializations of a given concurrent workload execute correctly. Upon encountering an error, the tool produces a trace that can be replayed to investigate the error. We applied the dBug tool to two distributed systems\u2013the Parallel Virtual File System (PVFS) implemented in C and the FAWN-based key-value storage (FAWN-KV) implemented in C++. In particular, we integrated both systems with dBug to expose the non-determinism due to concurrency. This mechanism was used to verify that the result of concurrent execution of a number of basic operations from a fixed initial state meets the high-level specification of PVFS and FAWN-KV. The experimental evidence shows that the dBug tool is capable of systematically exploring behaviors of a distributed system in a modular, practical, and effective manner.", "num_citations": "50\n", "authors": ["1779"]}
{"title": "Boolean satisfiability with transitivity constraints\n", "abstract": " We consider a variant of the Boolean satisfiability problem where a subset  of the propositional variables appearing in formula F                         sat encode a symmetric, transitive, binary relation over N elements. Each of these relational variables, e                            i,j                         , for 1 \u2264i < j \u2264N, expresses whether or not the relation holds between elements i and j. The task is to either find a satisfying assignment to F                         sat that also satisfies all transitivity constraints over the relational variables (e.g., ), or to prove that no such assignment exists. Solving this satisfiability problem is the final and most difficult step in our decision procedure for a logic of equality with uninterpreted functions. This procedure forms the core of our tool for verifying pipelined microprocessors.               To use a conventional Boolean satisfiability checker, we augment the set of clauses expressing F                         sat\u00a0\u2026", "num_citations": "50\n", "authors": ["1779"]}
{"title": "Verification of floating-point adders\n", "abstract": " In this paper, we present a \u201cblack box\u201d version of verification of FP adders. In our approach, FP adders are verified by an extended word-level SMV using reusable specifications without knowing the circuit implementation. Wordlevel SMV is improved by using Multiplicative Power HDDs (PHDDs), and by incorporating conditional symbolic simulation as well as a short-circuiting technique. Based on a case analysis, the adder specification is divided into several hundred implementation-independent sub-specifications. We applied our system and these specifications to verify the IEEE double precision FP adder in the Aurora III Chip from the University of Michigan. Our system found several design errors in this FP adder. Each specification can be checked in less than 5 minutes. A variant of the corrected FP adder was created to illustrate the ability of our system to handle different FP adder designs. For each\u00a0\u2026", "num_citations": "49\n", "authors": ["1779"]}
{"title": "Efficient modeling of memory arrays in symbolic simulation\n", "abstract": " This paper enables symbolic simulation of systems with large embedded memories. Each memory array is replaced with a behavioral model, where the number of symbolic variables used to characterize the initial state of the memory is proportional to the number of memory accesses. The memory state is represented by a list containing entries of the form <c, a, d>, where c is a Boolean expression denoting the set of conditions for which the entry is defined, a is an address expression denoting a memory location, and d is a data expression denoting the contents of this location. Address and data expressions are represented as vectors of Boolean expressions. The list interacts with the rest of the circuit by means of a software interface developed as part of the symbolic simulation engine. The interface monitors the control lines of the memory array and translates read and write conditions into accesses to the list\u00a0\u2026", "num_citations": "49\n", "authors": ["1779"]}
{"title": "Unbounded, fully symbolic model checking of timed automata using boolean methods\n", "abstract": " We present a new approach to unbounded, fully symbolic model checking of timed automata that is based on an efficient translation of quantified separation logic to quantified Boolean logic. Our technique preserves the interpretation of clocks over the reals and can check any property in timed computation tree logic. The core operations of eliminating quantifiers over real variables and deciding the validity of separation logic formulas are respectively translated to eliminating quantifiers on Boolean variables and checking Boolean satisfiability (SAT). We can thus leverage well-known techniques for Boolean formulas, including Binary Decision Diagrams (BDDs) and recent advances in SAT and SAT-based quantifier elimination. We present preliminary empirical results for a BDD-based implementation of our method.", "num_citations": "48\n", "authors": ["1779"]}
{"title": "Bit-level abstraction in the verification of pipelined microprocessors by correspondence checking\n", "abstract": " We present a way to abstract functional units in symbolic simulation of actual circuits, thus achieving the effect of uninterpreted functions at the bit-level. Additionally, we propose an efficient encoding technique that can be used to represent uninterpreted symbols with BDDs, while allowing these symbols to be propagated by simulation with a conventional bit-level symbolic simulator. Our abstraction and encoding techniques result in an automatic symmetry reduction and allow the control and forwarding logic of the actual circuit to be used unmodified. The abstraction method builds on the behavioral Efficient Memory Model [18][19] and its capability to dynamically introduce consistent initial state, which is identical for two simulation sequences. We apply the abstraction and encoding ideas on the verification of pipelined microprocessors by correspondence checking, where a pipelined microprocessor is\u00a0\u2026", "num_citations": "48\n", "authors": ["1779"]}
{"title": "Space-and time-efficient BDD construction via working set control\n", "abstract": " Binary decision diagrams (BDDs) have been shown to be a powerful tool in formal verification. Efficient BDD construction techniques become more important as the complexity of protocol and circuit designs increases. This paper addresses this issue by introducing three techniques based on working set control. First, we introduce a novel BDD construction algorithm based on partial breadth-first expansion. This approach has the good memory locality of the breadth-first BDD construction while maintaining the low memory overhead of the depth-first approach. Second, we describe how memory management on a per-variable basis can improve spatial locality of BDD construction at all levels, including expansion, reduction, and rehashing. Finally, we introduce a memory compacting garbage collection algorithm to remove unreachable BDD nodes and minimize memory fragmentation. Experimental results show that\u00a0\u2026", "num_citations": "48\n", "authors": ["1779"]}
{"title": "Formal verification of memory circuits by switch-level simulation\n", "abstract": " An N-bit RAM can be verified by simulating just O(N log N) patterns. This approach to verification is fast, requires minimal attention on the part of the user to the circuit details, and can utilize more sophisticated circuit models than other approaches to formal verification. The technique has been applied to a CMOS static RAM design using the COSMOS switch-level simulator. By simulating many patterns in parallel, a massively parallel computer can verify a 4K RAM in under 6 min.< >", "num_citations": "48\n", "authors": ["1779"]}
{"title": "Synchronous circuit verification by symbolic simulation: an illustration\n", "abstract": " Synchronous circuit verification by symbolic simulation: an illustration | Proceedings of the sixth MIT conference on Advanced research in VLSI ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsAUSCRYPT '90Synchronous circuit verification by symbolic simulation: an illustration ARTICLE Synchronous circuit verification by symbolic simulation: an illustration Share on Authors: Derek Lee Beatty profile image Derek L. Beatty View Profile , Randal E Bryant profile image Randal E. Bryant View Profile , Carl Johan H Seger profile image Carl-Johan H. Seger View Profile Authors Info & Affiliations Publication: AUSCRYPT '90: \u2026", "num_citations": "44\n", "authors": ["1779"]}
{"title": "Performance evaluation of FMOSSIM, a concurrent switch-level fault simulator\n", "abstract": " This paper presents measurements obtained while performing fault simulations of MOS circuits modeled at the switch level. In this model the transistor structure of the circuit is represented explicitly as a network of charge storage nodes connected by bidirectional transistor switches. Since the logic model of the simulator closely matches the actual structure of MOS circuits, such faults as stuck-open and closed transistors as well as short and open-circuited wires can be simulated. By using concurrent simulation techniques, we obtain a performance level comparable to fault simulators using logic gate models. Our measurements indicate that fault simulation times grow as the product of the circuit size and number of patterns, assuming the number of faults to be simulated is proportional to the circuit size. However, fault simulation times depend strongly on the rate at which the test patterns detect the faults.", "num_citations": "44\n", "authors": ["1779"]}
{"title": "Fault simulation of MOS digital circuits\n", "abstract": " T est engineers use fault simulators to determine how well a sequence of test patterns applied to the inputs of an integrated circuit can distinguish a good chip from a defective one. The fault simulator is given a description of the good circuit, a set of hypothetical faults in the circuit, a specification of the observation points of the test (eg, the output pins of the chip), and a sequence of test patterns. It then simulates how the good circuit and all of the faulty circuits would behave when the test patterns are applied to the inputs. A fault is considered detected if at any time the simulation of that particular faulty circuit produces a different logic value at some observation point than the simulation of the good circuit produces. By keeping track of which faults have been detected and which have not, the fault simulator can determine the fault coverage of the test sequence, which is defined as the ratio of the number of faults detected to the total number simulated. The simulator can also give the user information about which faults have not been detected, either because the test sequence failed to exercise the defective part of the circuit, or because the sequence failed to make the effect of such an exercise visible at an observation point. This information guides the engineer in extending or modifying the test sequence to improve its fault coverage. Such a tool is invaluable in developing test patterns for today's complex digital systems.For a large integrated circuit such as a microprocessor chip, many faults (eg, more than 1000) must be simulated for adequate characterization of the fault coverage of a test sequence. Furthermore, the test sequences can involve\u00a0\u2026", "num_citations": "44\n", "authors": ["1779"]}
{"title": "Fault Tolerant Planning: Toward Probabilistic Uncertainty Models in Symbolic Non-Deterministic Planning.\n", "abstract": " Symbolic non-deterministic planning represents action effects as sets of possible next states. In this paper, we move toward a more probabilistic uncertainty model by distinguishing between likely primary effects and unlikely secondary effects of actions. We consider the practically important case where secondary effects are failures, and introduce n-fault tolerant plans that are robust for up to n faults occurring during plan execution. Fault tolerant plans are more restrictive than weak plans, but more relaxed than strong cyclic and strong plans. We show that optimal n-fault tolerant plans can be generated by the usual strong algorithm. However, due to non-local error states, it is often beneficial to decouple the planning for primary and secondary effects. We employ this approach for two specialized algorithms 1-FTP (blind) and 1-GFTP (guided) and demonstrate their advantages experimentally in significant real-world domains.", "num_citations": "43\n", "authors": ["1779"]}
{"title": "Formal hardware verification by symbolic trajectory evaluation\n", "abstract": " Formal verification uses a set of languages, tools, and techniques to mathematically reason about the correctness of a hardware system. The form of mathematical reasoning is dependent upon the hardware system. This thesis concentrates on hardware systems that have a simple deterministic high-level specification but have implementations that exhibit highly nondeterministic behaviors. A typical example of such hardware systems are processors. At the high level, the sequencing model inherent in processors is the sequential execution model. The underlying implementation, however, uses features such as nondeterministic interface protocols, instruction pipelines, and multiple instruction issue which leads to nondeterministic behaviors.", "num_citations": "43\n", "authors": ["1779"]}
{"title": "Massively parallel switch-level simulation: A feasibility study\n", "abstract": " The feasibility of mapping the COSMOS switch-level simulator onto a computer with thousands of simple processors is addressed. COSMOS preprocesses transistor networks into Boolean behavioral models, capturing the switch-level behavior of a circuit in a set of Boolean formulas. A class of massively parallel computers and a mapping of COSMOS onto these computers are described. The factors affecting the performance of such a massively parallel simulator are discussed, including: the amount of parallelism in the simulation model, performance measures for massively parallel machines, and the impact of event scheduling on simulator performance. Compilation tools that automatically map a MOS circuit onto a massively parallel computer have been developed. Techniques for restructuring Boolean expressions for greater parallelism and mapping Boolean expressions for evaluation on massively parallel\u00a0\u2026", "num_citations": "41\n", "authors": ["1779"]}
{"title": "Concurrent programming\n", "abstract": " Concurrency of activities has long been recognized as an important feature in many computer systems. These systems allow concurrent operations for a number of reasons of which three are particularly common. First, by executing several jobs simultaneously, multiprogramming and time-sharing systems can make fuller use of the computing resources. Second, real-time transaction systems, such as airline reservation and point-of-sale terminal systems~ allow a number of users to access a single database concurrently and to obtain responses in real-time. Finally, high speed parallel computers such as array processors dedicate a number of processors to the execution of a single program to speed up completion of a computation.In developing the software for some of the early multiprogrammfng systems, programmers soon discovered a need for an abstract and machine-independent means of expressing the\u00a0\u2026", "num_citations": "38\n", "authors": ["1779"]}
{"title": "Symbolic simulation with approximate values\n", "abstract": " Symbolic methods such as model checking using binary decision diagrams (BDDs) have had limited success in verifying large designs because BDD sizes regularly exceed memory capacity. Symbolic simulation is a method that controls BDD size by allowing the user to specify the number of symbolic variables in a test. However, BDDs still may blow up when using symbolic simulation in large designs with a large number of symbolic variables. This paper describes techniques for limiting the size of the internal representation of values in symbolic simulation no matter how many symbolic variables are present. The basic idea is to use approximate values on internal nodes; an approximate value is one that consists of combinations of the values 0, 1, and X. If an internal node is known not to affect the functionality being tested, then the simulator can output a value of X for this node, reducing the amount of time\u00a0\u2026", "num_citations": "36\n", "authors": ["1779"]}
{"title": "Exploiting symmetry when verifying transistor-level circuits by symbolic trajectory evaluation\n", "abstract": " We describe the use of symmetry for verification of transistor-level circuits by symbolic trajectory evaluation (STE). We present a new formulation of STE which allows a succinct description of symmetry properties in circuits, Symmetries in circuits are classified as structural symmetries, arising from similarities in circuit structure, data symmetries, arising from similarities in the handling of data values, and mixed structural-data symmetries. We use graph isomorphism testing and symbolic simulation to verify the symmetries in the original circuit, Using conservative approximations, we partition a circuit to expose the symmetries in its components, and construct reduced system models which can be verified efficiently, Introducing X-drivers into switch-level circuits simplifies the task of creating conservative approximations of switch-level circuits, Our empirical results show that exploiting symmetry with conservative\u00a0\u2026", "num_citations": "36\n", "authors": ["1779"]}
{"title": "Binary decision diagrams\n", "abstract": " Binary decision diagrams provide a data structure for representing and manipulating Boolean functions in symbolic form. They have been especially effective as the algorithmic basis for symbolic model checkers. A binary decision diagram represents a Boolean function as a directed acyclic graph, corresponding to a compressed form of decision tree. Most commonly, an ordering constraint is imposed among the occurrences of decision variables in the graph, yielding ordered binary decision diagrams (OBDD). Representing all functions as OBDDs with a common variable ordering has the advantages that (1) there is a unique, reduced representation of any function, (2) there is a simple algorithm to reduce any OBDD to the unique form for that function, and (3) there is an associated set of algorithms to implement a wide variety of operations on Boolean functions represented as OBDDs. Recent work in this area\u00a0\u2026", "num_citations": "35\n", "authors": ["1779"]}
{"title": "TLSim and EVC: a term-level symbolic simulator and an efficient decision procedure for the logic of equality with uninterpreted functions and memories\n", "abstract": " We present a tool flow for high-level design and formal verification of embedded processors. The tool flow consists of the term-level symbolic simulator TLSim, the decision procedure EVC (Equality Validity Checker) for the logic of Equality with Uninterpreted Functions and Memories (EUFM), and any SAT solver. TLSim accepts high-level models of a pipelined implementation processor and its non-pipelined specification, as well as a command file indicating how to simulate them symbolically, and produces an EUFM formula for the correctness of the implementation. EVC exploits the property of Positive Equality and other optimisations in order to translate the EUFM formula to an equivalent Boolean formula that can be solved with any SAT procedure. An earlier version of our tool flow was used to formally verify a model of the M\u2022CORE processor at Motorola, and detected bugs.", "num_citations": "34\n", "authors": ["1779"]}
{"title": "Concurrent fault simulation of MOS digital circuits\n", "abstract": " The concurrent fault simulation technique is widely used to analyse the behavior of digital circuits in the presence of faults. We show how this technique can be applied to metal-oxide-semiconductor (MOS) digital circuits when modeled at the switch-level as a set of charge storage nodes connected by bidirectional transistor switches. The algorithm we present is capable of analysing the behavior of a wide variety of MOS circuit failures, such as stuck-at-zero or stuck-at-one nodes, stuck-open or stuck-closed transistors, or resistive opens or shorts. We have implemented a fault simulator FMOSSIM based on this algorithm. The capabilities and the peformance of this program demonstrate the advantages of combining switch-level and concurrent simulation techniques.", "num_citations": "33\n", "authors": ["1779"]}
{"title": "Deciding CLU logic formulas via Boolean and pseudo-Boolean encodings\n", "abstract": " UCLID is a tool for specifying and verifying systems expressed in a quantifier-free first-order logic, called CLU logic, that includes uninterpreted functions, equality, ordering, constrained lambda expressions, and counter arithmetic. In previous work, we presented different variants of a decision procedure for this logic that are all based on on an efficient translation of a CLU formula to a Boolean formula, which is checked using a Boolean satisfiability solver. In this paper, we present another variant based on the PBS tool that integrates a pseudo-Boolean constraint solver with a SAT solver. We then present an empirical evaluation of all decision procedure variants on a set of benchmark formulas generated from various UCLID models.", "num_citations": "31\n", "authors": ["1779"]}
{"title": "Verification of arithmetic circuits using binary moment diagrams\n", "abstract": " Binary moment diagrams (BMDs) provide a canonical representation for linear functions similar to the way binary decision diagrams (BDDs) represent Boolean functions. Within the class of linear functions, we can embed arbitrary functions from Boolean variables to real, rational, or integer values. BMDs can thus model the functionality of data path circuits operating over word-level data. Many important functions, including integer multiplication, that cannot be represented efficiently at the bit level with BDDs, have simple representations at the word level with BMDs. Furthermore, BMDs can represent Boolean functions as a special case. We propose a hierarchical approach to verifying arithmetic circuits, where component modules are first shown to implement their word-level specifications. The overall circuit functionality is then verified by composing the component functions and comparing the result to the\u00a0\u2026", "num_citations": "31\n", "authors": ["1779"]}
{"title": "Inverter minimization in multi-level logic networks\n", "abstract": " We look at the problem of inverter minimization in multi-level logic networks. The network is specified in terms of a set of base functions and the inversion operation. The library is specified as a set of allowed permutations of phase assignments on each base function. Traditional approaches to this problem have been limited to greedy heuristics based on local information. Our approach takes a more global view and maps the problem of inverter minimization into a problem of removing a minimum of vertices from a graph, so as to make the remaining graph two-colorable. This approach has the flexibility of capturing a variety of design-specific features that are relevant to the problem of inverter minimization. Although, in general the problem is NP-complete, we have developed several good heuristic and branch and bound search techniques.", "num_citations": "30\n", "authors": ["1779"]}
{"title": "Convergence testing in term-level bounded model checking\n", "abstract": " We consider the problem of bounded model checking of systems expressed in a decidable fragment of first-order logic. While model checking is not guaranteed to terminate for an arbitrary system, it converges for many practical examples, including pipelined processors. We give a new formal definition of convergence that generalizes previously stated criteria. We also give a sound semi-decision procedure to check this criterion based on a translation to quantified separation logic. Preliminary results on simple pipeline processor models are presented.", "num_citations": "28\n", "authors": ["1779"]}
{"title": "EVC: a validity checker for the logic of equality with uninterpreted functions and memories, exploiting positive equality, and conservative transformations\n", "abstract": " The property of Positive Equality [2] dramatically speeds up validity checking of formulas in the logic of Equality with Uninterpreted Functions and Memories (EUFM) [4]. The logic expresses correctness of high-level microprocessors. We present EVC (Equality Validity Checker)\u2014a tool that exploits Positive Equality and other optimizations when translating a formula in EUFM to a propositional formula, which can then be evaluated by any Boolean satisfiability (SAT) procedure. EVC has been used for the automatic formal verification of pipelined, superscalar, and VLIW microprocessors.", "num_citations": "28\n", "authors": ["1779"]}
{"title": "Using Ordered Binary-Decision Diagrams for Compressing Images and Image Sequences.\n", "abstract": " The Ordered Binary-Decision Diagram OBDD has been used to reduce the amount of space and computation required for verifying digital circuits by removing redundant copies of subfunctions. Similarly, image compression algorithms attempt to reduce their space requirements by finding replicated patterns or features in images. OBDDs would therefore appear to be a good candidate as a data structure for representing images. We show how images can be encoded using Ordered Binary-Decision Diagrams and compare our results to quadtrees. We also show how using this method extends naturally to compressing sequences of related images such those that comprise movies.Descriptors:", "num_citations": "28\n", "authors": ["1779"]}
{"title": "Offloading health-checking policy\n", "abstract": " Methods and systems for offloading health-checking policy in a distributed management environment are provided. A failure policy is received at a node of a cloud from a cloud health monitor. The node transmits a notification to a health monitor of the node that the node has failed when the failure policy is satisfied. The node reports at least one fault based on the satisfied failure policy to the cloud health monitor.", "num_citations": "27\n", "authors": ["1779"]}
{"title": "Optimizing model checking based on BDD characterization\n", "abstract": " Symbolic model checking has been successfully applied in verification of various finite state systems, ranging from hardware circuits to software protocols. A core technology underlying this success is the Binary Decision Diagram (BDD) representation. Given the importance of BDDs in model checking, it is surprising that there has been little or no work on studying BDD computations in the context of model checking. As a result, the computational aspects of BDDs are not well understood and many BDD-based algorithms tend to be unstable in terms of performance. This thesis addresses the performance instability issue both by developing a general evaluation methodology for studying BDD computations and by proposing new BDD-based optimizations to stabilize and to improve the overall performance.", "num_citations": "27\n", "authors": ["1779"]}
{"title": "For incremental circuit analysis using extracted hierarchy\n", "abstract": " The authors present an algorithm for extracting a two-level subnetwork hierarchy from flat netlists and its application to incremental circuit analysis in the COSMOS compiled switch-level simulator. Incremental operation is achieved by using the file system as a large hash table that retains information over many executions of the incremental analyzer. The hierarchy extraction algorithm computes a hash signature for each subnetwork by coloring vertices in a manner similar to wirelist-comparison programs, then identifies duplicates using standard hash-table techniques. Its application decreases the network preprocessing time for COSMOS by nearly an order of magnitude.< >", "num_citations": "27\n", "authors": ["1779"]}
{"title": "A hardware architecture for switch-level simulation\n", "abstract": " The Mossim Simulation Engine (MSE) is a hardware accelerator for performing switch-level simulation of MOS VLSI circuits [1], [2]. Functional partitioning of the MOSSIM algorithm and specialized circuitry are used by the MSE to achieve a performance improvement of /spl gt/ 300 over a VAX 11/780 executing the MOSSIM II program. Several MSE processors can be connected in parallel to achieve additional speedup. A virtual processor mechanism allows the MSE to simulate large circuits with the size of the circuit limited only by the amount of backing store available to hold the circuit description.", "num_citations": "27\n", "authors": ["1779"]}
{"title": "Introductory computer science education at Carnegie Mellon University: A deans\u2019 perspective\n", "abstract": " The School of Computer Science at Carnegie Mellon University is planning major revisions to its introductory course sequence in ways that will affect not just our own students, but also the many students from across campus who take computer science courses. Major changes include: 1) revising our introductory courses to promote the principles of computational thinking, for both majors and nonmajors, 2) increasing our emphasis on the need to make software systems highly reliable and the means to achieve this, and 3) preparing students for a future in which programs will achieve high performance by exploiting parallel execution.", "num_citations": "25\n", "authors": ["1779"]}
{"title": "State-set branching: Leveraging BDDs for heuristic search\n", "abstract": " In this article, we present a framework called state-set branching that combines symbolic search based on reduced ordered Binary Decision Diagrams (BDDs) with best-first search, such as A* and greedy best-first search. The framework relies on an extension of these algorithms from expanding a single state in each iteration to expanding a set of states. We prove that it is generally sound and optimal for two A* implementations and show how a new BDD technique called branching partitioning can be used to efficiently expand sets of states. The framework is general. It applies to any heuristic function, evaluation function, and transition cost function defined over a finite domain. Moreover, branching partitioning applies to both disjunctive and conjunctive transition relation partitioning. An extensive experimental evaluation of the two A* implementations proves state-set branching to be a powerful framework. The\u00a0\u2026", "num_citations": "25\n", "authors": ["1779"]}
{"title": "Formal verification of a superscalar execution unit\n", "abstract": " Many modern systems are designed as a set of interconnectedreactive subsystems. The subsystem verification task is toverify an implementation of the subsystem against the simple deterministichigh-level specification of the entire system. Our verificationmethodology, based on Symbolic Trajectory Evaluation, is ableto bridge the wide gap between the abstract specification and theimplementation specific details of the subsystem. This paper presentsa detailed description of an industrial application of this methodologyto the fixed point execution unit of the PowerPC processor. We were able to verify a representative instruction under all possiblestall, bypass, pipeline conditions and under all possible timingsfor interface to other functional units in the processor.", "num_citations": "25\n", "authors": ["1779"]}
{"title": "ATLAS: automatic term-level abstraction of RTL designs\n", "abstract": " Abstraction plays a central role in formal verification. Term-level abstraction is a technique for abstracting word-level designs in a formal logic, wherein data is modeled with abstract terms, functional blocks with uninterpreted functions, and memories with a suitable theory of memories. A major challenge for any abstraction technique is to determine what components can be safely abstracted. We present an automatic technique for term-level abstraction of hardware designs, in the context of equivalence and refinement checking problems. Our approach is hybrid, involving a combination of random simulation and static analysis. We use random simulation to identify functional blocks that are suitable for abstraction with uninterpreted functions. Static analysis is then used to compute conditions under which such function abstraction is performed. The generated term-level abstractions are verified using techniques based\u00a0\u2026", "num_citations": "24\n", "authors": ["1779"]}
{"title": "Optimizing symbolic model checking for constraint-rich models\n", "abstract": " This paper presents optimizations for verifying systems with complex time-invariant constraints. These constraints arise naturally from modeling physical systems, e.g., in establishing the relationship between different components in a system. To verify constraint-rich systems, we propose two new optimizations. The first optimization is a simple, yet powerful, extension of the conjunctivepartitioning algorithm. The second is a collection of BDD-based macro-extraction and macro-expansion algorithms to remove state variables.We showthat these two optimizations are essential in verifying constraint-rich problems; in particular, this work has enabled the verification of fault diagnosis models of the Nomad robot (an Antarctic meteorite explorer) and of the NASA Deep Space One spacecraft.", "num_citations": "23\n", "authors": ["1779"]}
{"title": "Efficient modeling of memory arrays in symbolic ternary simulation\n", "abstract": " This paper enables symbolic ternary simulation of systems with large embedded memories. Each memory array is replaced with a behavioral model, where the number of symbolic variables used to characterize the initial state of the memory is proportional to the number of distinct symbolic memory locations accessed. The behavioral model provides a conservative approximation of the replaced memory array, while allowing the address and control inputs of the memory to accept symbolic ternary values. Memory state is represented by a list of entries encoding the sequence of updates of symbolic addresses with symbolic data. The list interacts with the rest of the circuit by means of a software interface developed as part of the symbolic simulation engine. This memory model was incorporated into our verification tool based on Symbolic Trajectory Evaluation. Experimental results show that the new model\u00a0\u2026", "num_citations": "23\n", "authors": ["1779"]}
{"title": "Logic simulation on massively parallel architectures\n", "abstract": " This work examines the mapping of logic simulation onto massively parallel computer architectures. We discuss alternative communication primitives for a massively parallel instruction set architecture and the impact of the choice of communication primitives on logic simulation. We have developed compilation tools to automatically map the simulation of an MOS transistor circuit onto a massively parallel computer. We analyze the efficiency of this mapping as a function of the available communication primitives. The compilation process is illustrated by describing our pilot implementation on a 32k processor Connection Machine.", "num_citations": "23\n", "authors": ["1779"]}
{"title": "Data parallel switch-level simulation\n", "abstract": " Data-parallel simulation involves simulating the behavior of a circuit over a number of test sequences simultaneously. Compared to other parallel simulation techniques, data-parallel simulation requires less overhead for synchronization and communication, and it permits higher degrees of parallelism. Two data-parallel versions of the switch-level simulator COSMOS have been implemented. The first runs on conventional machines, exploiting the bit parallelism of machine-level logic operations. This version runs 20-30 times faster than sequential simulation on the same machine. The second runs on a massively parallel SIMD machine, with each processor simulating the circuit behavior for a single test sequence. A simulator running on a 32768-processor machine runs up to 33000 times faster than a sequential simulator on a workstation computer", "num_citations": "23\n", "authors": ["1779"]}
{"title": "Guided Symbolic Universal Planning.\n", "abstract": " Symbolic universal planning based on the reduced Ordered Binary Decision Diagram (OBDD) has been shown to be an efficient approach for planning in non-deterministic domains. To date, however, no guided algorithms exist for synthesizing universal plans. In this paper, we introduce a general approach for guiding universal planning based on an existing method for heuristic symbolic search in deterministic domains. We present three new sound and complete algorithms for best-first strong, strong cyclic, and weak universal planning. Our experimental results show that guiding the search dramatically can reduce both the computation time and the size of the generated plans.", "num_citations": "22\n", "authors": ["1779"]}
{"title": "Introducing computer systems from a programmer's perspective\n", "abstract": " The course\" Introduction to Computer Systems\" at Carnegie Mellon University presents the underlying principles by which programs are executed on a computer. It provides broad coverage of processor operation, compilers, operating systems, and networking. Whereas most systems courses present material from the perspective of one who designs or implements part of the system, our course presents the view visible to application programmers. Students learn that, by understanding aspects of the underlying system, they can make their programs faster and more reliable. This approach provides immediate benefits for all computer science and engineering students and also prepares them for more advanced systems courses. We have taught our course for five semesters with enthusiastic responses by the students, the instructors, and the instructors of subsequent systems courses.", "num_citations": "22\n", "authors": ["1779"]}
{"title": "Parallel discrete event simulation: The making of a field\n", "abstract": " Originating in the 1970's, the parallel discrete event simulation (PDES) field grew from a group of researchers focused on determining how to execute a discrete event simulation program on a parallel computer while still obtaining the same results as a sequential execution. Over the decades that followed the field expanded, grew, and flourishes to this day. This paper describes the origins and development of the field in the words of many who were deeply involved. Unlike other published work focusing on technical issues, the emphasis here is on historical aspects that are not recorded elsewhere, providing a unique characterization of how the field was created and developed.", "num_citations": "21\n", "authors": ["1779"]}
{"title": "On solving boolean combinations of UTVPI constraints\n", "abstract": " We consider the satisfiability problem for Boolean combinations of unit two variable per inequality (UTVPI) constraints. A UTVPI constraint is linear constraint containing at most two variables with non-zero coefficients, where furthermore those coefficients must be either\u2212 1 or 1. We prove that if a satisfying solution exists, then there is a solution with each variable taking values in [\u2212 n\u00b7(b max+ 1), n\u00b7(b max+ 1)], where n is the number of variables, and b max is the maximum over the absolute values of constants appearing in the constraints. This solution bound improves over previously obtained bounds by an exponential factor. Our result can be used in a finite instantiation-based approach to deciding satisfiability of UTVPI formulas. An experimental evaluation demonstrates the efficiency of such an approach. One of our key results is to show that an integer point inside a UTVPI polyhedron, if one exists, can be obtained\u00a0\u2026", "num_citations": "21\n", "authors": ["1779"]}
{"title": "Chain reduction for binary and zero-suppressed decision diagrams\n", "abstract": " Chain reduction enables reduced ordered binary decision diagrams (BDDs) and zero-suppressed binary decision diagrams (ZDDs) to each take advantage of the others\u2019 ability to symbolically represent Boolean functions in compact form. For any Boolean function, its chain-reduced ZDD (CZDD) representation will be no larger than its ZDD representation, and at most twice the size of its BDD representation. The chain-reduced BDD (CBDD) of a function will be no larger than its BDD representation, and at most three times the size of its CZDD representation. Extensions to the standard algorithms for operating on BDDs and ZDDs enable them to operate on the chain-reduced versions. Experimental evaluations on representative benchmarks for encoding word lists, solving combinatorial problems, and operating on digital circuits indicate that chain reduction can provide significant benefits in terms of both\u00a0\u2026", "num_citations": "20\n", "authors": ["1779"]}
{"title": "Scalable dynamic partial order reduction\n", "abstract": " Systematic testing, first demonstrated in small, specialized cases 15 years ago, has matured sufficiently for large-scale systems developers to begin to put it into practice. With actual deployment come new, pragmatic challenges to the usefulness of the techniques. In this paper we are concerned with scaling dynamic partial order reduction, a key technique for mitigating the state space explosion problem, to very large clusters. In particular, we present a new approach for distributed dynamic partial order reduction. Unlike previous work, our approach is based on a novel exploration algorithm that 1) enables trading space complexity for parallelism, 2) achieves efficient load-balancing through time-slicing, 3) provides for fault tolerance, which we consider a mandatory aspect of scalability, 4) scales to more than a thousand parallel workers, and 5) is guaranteed to avoid redundant exploration of overlapping\u00a0\u2026", "num_citations": "20\n", "authors": ["1779"]}
{"title": "dBug: Systematic Testing of Unmodified Distributed and Multi-threaded Systems\n", "abstract": " In order to improve quality of an implementation of a distributed and multithreaded system, software engineers inspect code and run tests. However, the concurrent nature of such systems makes these tasks challenging. For testing, this problem is addressed by stress                 testing, which repeatedly executes a test hoping that eventually all possible outcomes of the test will be encountered.", "num_citations": "20\n", "authors": ["1779"]}
{"title": "Verifying nondeterministic implementations of deterministic systems\n", "abstract": " Some modern systems with a simple deterministic high-level specification have implementations that exhibit highly nondeterministic behavior. Such systems maintain a simple operation semantics at the high-level. However their underlying implementations exploit parallelism to enhance performance leading to interaction among operations and contention for resources. The deviation from the sequential execution model not only leads to nondeterminism in the implementation but creates the potential for serious design errors. This paper presents a methodology for formal verification of such systems. An abstract specification describes the high-level behavior as a set of operations. A mapping relates the sequential semantics of these operations to the underlying nondeterminism in the implementation. Symbolic Trajectory Evaluation, a modified form of symbolic simulation, is used to perform the actual\u00a0\u2026", "num_citations": "19\n", "authors": ["1779"]}
{"title": "Automatic clock abstraction from sequential circuits\n", "abstract": " Our goal is to transform a low-level circuit design into a more abstract representation. A pre-existing tool, Tranalyze [4], takes a switch-level circuit and generates a functionally equivalent gatelevel representation. This work focuses on taking that gate-level sequential circuit and performing a temporal analysis which abstracts the clocks from the circuit. The analysis generates a cycle-level gate model with the detailed timing abstracted from the original circuit. Unlike other possible approaches, our analysis does not require the user to identify state elements or give the timings of internal state signals. The temporal analysis process has applications in simulation, formal verification, and reverse engineering of existing circuits. Experimental results show a 40%-70% reduction in the size of the circuit and a 3X-150X speedup in simulation time.", "num_citations": "19\n", "authors": ["1779"]}
{"title": "Verification of synchronous circuits by symbolic logic simulation\n", "abstract": " A logic simulator can prove the correctness of a digital circuit when it can be shown that only circuits implementing the system specification will produce a particular response to a sequence of simulation commands. By simulating a circuit symbolically, verification can avoid the combinatorial explosion that would normally occur when evaluating circuit operation over many combinations of input and initial state. In this paper, we describe our methodology for verifying synchronous circuits using the stack circuit of Mead and Conway as an illustrative example.", "num_citations": "19\n", "authors": ["1779"]}
{"title": "Data intensive scalable computing\n", "abstract": " Symbolic Boolean Manipulation with Ordered Binary Decision Diagrams Page 1 Data Intensive Scalable Computing http://www.cs.cmu.edu/~bryant Randal E. Bryant Carnegie Mellon University Finding the Right Programming Models Page 2 \u20132\u2013 Outline Data-Intensive Scalable Computing \u220e Focus on petabytes, not petaflops \u220e Rethinking machine design Map/Reduce Programming Model \u220e Suitability for wide class of computing tasks Strengths & Limitations \u220e Scalability \u220e Performance Beyond Map/Reduce \u220e Small variations \u220e Other programming models Page 3 \u20133\u2013 Our Data-Driven World Science \u220e Data bases from astronomy, genomics, natural languages, seismic modeling, \u2026 Humanities \u220e Scanned books, historic documents, \u2026 Commerce \u220e Corporate sales, stock market transactions, census, airline traffic, \u2026 Entertainment \u220e Internet images, Hollywood movies, MP3 files, \u2026 Medicine \u220e MRI & CT scans, \u2026", "num_citations": "18\n", "authors": ["1779"]}
{"title": "Exploiting symmetry when verifying transistor-level circuits by symbolic trajectory evaluation\n", "abstract": " In this paper we describe the use of symmetry for verification of transistor-level circuits by symbolic trajectory evaluation. We show that exploiting symmetry can allow one to verify systems several orders of magnitude larger than otherwise possible. We classify symmetries in circuits as structural symmetries, arising from similarities in circuit structure, data symmetries, arising from similarities in the handling of data values, and mixed structural-data symmetries. We use graph isomorphism testing and symbolic simulation to verify the symmetries in the original circuit. Using conservative approximations, we partition a circuit to expose the symmetries in its components, and construct reduced system models which can be verified efficiently. We have verified Static Random Access Memory circuits with up to 1.5 Million transistors.", "num_citations": "18\n", "authors": ["1779"]}
{"title": "Learning conditional abstractions\n", "abstract": " Abstraction is central to formal verification. In term-level abstraction, the design is abstracted using a fragment of first-order logic with background theories, such as the theory of uninterpreted functions with equality. The main challenge in using term-level abstraction is determining what components to abstract and under what conditions. In this paper, we present an automatic technique to conditionally abstract register transfer level (RTL) hardware designs to the term level. Our approach is a layered approach that combines random simulation and machine learning inside a counter-example guided abstraction refinement (CEGAR) loop. First, random simulation is used to determine modules that are candidates for abstraction. Next, machine learning is used on the resulting simulation traces to generate candidate conditions under which those modules can be abstracted. Finally, a verifier is invoked. If spurious\u00a0\u2026", "num_citations": "17\n", "authors": ["1779"]}
{"title": "Autograding in the cloud: interview with David O'Hallaron\n", "abstract": " In this installment of Trend Wars, Dejan Milojicic discusses autograding with David O'Hallaron, a professor of computer science and electrical and computer engineering at Carnegie Mellon University. Featured here is an excerpt from the in-depth interview, which ranged from discussions on autograding to cloud computing. The Web extra includes the video, but you can stream it at www.computer.org/portal/web/computingnow/videos/trendwars.", "num_citations": "17\n", "authors": ["1779"]}
{"title": "An efficient graph representation for arithmetic circuit verification\n", "abstract": " In this paper, we propose a new data structure called multiplicative power hybrid decision diagrams (*PHDDs) to provide a compact representation for functions that map Boolean vectors into integer or floating-point (FP) values. The size of the graph to represent the IEEE FP encoding is linear with the word size. The complexity of FP multiplication grows linearly with the word size. The complexity of FP addition grows exponentially with the size of the exponent part, but linearly with the size of the mantissa part. We applied *PHDDs to verify integer multipliers and FP multipliers before the rounding stage, based on a hierarchical verification approach. For integer multipliers, our results are at least six times faster than binary moment diagrams. Previous attempts at verifying FP multipliers required manual intervention, but we verified FP multipliers before the rounding stage automatically.", "num_citations": "17\n", "authors": ["1779"]}
{"title": "Microprocessor verification using efficient decision procedures for a logic of equality with uninterpreted functions\n", "abstract": " Modern processors have relatively simple specifications based on their instruction set architectures. Their implementations, however, are very complex, especially with the advent of performance-enhancing techniques such as pipelining, superscalar operation, and speculative execution. Formal techniques to verify that a processor implements its instruction set specification could yield more reliable results at a lower cost than the current simulation-based verification techniques used in industry.               The logic of equality with uninterpreted functions (EUF) provides a means of abstracting the manipulation of data by a processor when verifying the correctness of its control logic. Using a method devised by Burch and Dill [BD94], the correctness of a processor can be inferred by deciding the validity of a formula in EUF describing the comparative effect of running one clock cycle of processor operation to that of\u00a0\u2026", "num_citations": "17\n", "authors": ["1779"]}
{"title": "Breadth-first with depth-first BDD construction: A hybrid approach\n", "abstract": " This paper presents the technique of operator sifting as a new way of understanding both breadth-first and depth-first approaches to BDD construction. A new algorithm is also proposed to capture the breadth-first approachs advantage of memory access locality, while keeping the depth-first approachs advantage of low memory overhead. Our preliminary experimental results show that our approach is generally faster than other implementations that rely exclusively on either breadth-first or depth-first approaches while keeping memory overhead comparable to that of depth-first approaches.Descriptors:", "num_citations": "17\n", "authors": ["1779"]}
{"title": "Computing logic-stage delays using circuit simulation and symbolic elmore analysis\n", "abstract": " The computation of logic-stage delays is a fundamental sub-problem for many EDA tasks. Although accurate delays can be obtained via circuit simulation, we must estimate the input assignments that will maximize the delay. With conventional methods, it is not feasible to estimate the delay for all input assignments on large sub-networks, so previous approaches have relied on heuristics. We present a symbolic algorithm that enables efficient computation of the Elmore delay under all input assignments and delay refinement using circuit-simulation. We analyze the Elmore estimate with three metrics using data extracted from symbolic timing simulations of industrial circuits.", "num_citations": "16\n", "authors": ["1779"]}
{"title": "Verifying a static RAM design by logic simulation\n", "abstract": " A logic simulator can prove the directness of a digital circuit if it can be shown that only circuits implementing the system specification will produce a particular response to a sequence of simulation commands.", "num_citations": "16\n", "authors": ["1779"]}
{"title": "Set manipulation with boolean functional vectors for symbolic reachability analysis\n", "abstract": " Symbolic techniques usually use characteristic functions for representing sets of states. Boolean functional vectors provide an alternate set representation which is suitable for symbolic simulation. Their use in symbolic reachability analysis and model checking is limited, however, by the lack of algorithms for performing set operations. We present algorithms for set union, intersection and quantification that work with a canonical Boolean functional vector representation and show how this enables efficient symbolic simulation based reachability analysis. Our experimental results for reachability analysis indicate that the Boolean functional vector representation is often more compact than the corresponding characteristic function, thus giving significant performance improvements on some benchmarks.", "num_citations": "14\n", "authors": ["1779"]}
{"title": "Verification of pipelined microprocessors by comparing memory execution sequences in symbolic simulation\n", "abstract": " This paper extends Burch and Dill's pipeline verification method [4] to the bit level. We introduce the idea of memory shadowing, a new technique for providing on-the-fly identical initial memory state to two different memory execution sequences. We also present an algorithm which compares the final states of two memories for equality. Memory shadowing and the comparison algorithm build on the Efficient Memory Model (EMM) [13], a behavioral memory model where the number of symbolic variables used to characterize the initial state of a memory is proportional to the number of distinct symbolic locations accessed. These techniques allow us to verify that a pipelined circuit has equivalent behavior to its unpipelined specification by simulating two memory execution sequences and comparing their final states. Experimental results show the potential of the new ideas.", "num_citations": "14\n", "authors": ["1779"]}
{"title": "Formal verification of memory arrays using symbolic trajectory evaluation\n", "abstract": " Verification of memory arrays is an important part of processor verification. Memory arrays include circuits such as on-chip caches, cache tags, register files, and branch prediction buffers having memory cores embedded within complex logic. These circuits are typically custom designed at the transistor-level to optimize area and performance. This makes it necessary to verify them at the transistor-level. Conventional array verification approaches are based on switch-level simulation. Such approaches do not work for arrays as it is infeasible to simulate the astronomical number of simulation patterns that are required to verify these designs. Therefore, formal methods are required to ensure the correctness of memory arrays. This paper describes the formal verification technique of Symbolic Trajectory Evaluation (STE), and its application to verify memory arrays. The paper describes techniques to overcome the\u00a0\u2026", "num_citations": "14\n", "authors": ["1779"]}
{"title": "Intractability in linear switch-level simulation\n", "abstract": " The linear switch-level model represents a MOS transistor as a voltage-controlled linear resistor and a storage node as a grounded, linear capacitor. For logic simulation, the linear switch-level model offers an attractive tradeoff between resolution/accuracy and computational complexity over gate-level and circuit-level models. However, analysis of MOS networks using the linear switch-level model becomes increasingly difficult in the presence of unknown values, and heuristic methods are often employed. It is shown that the complexity of computing maximum and minimum steady-state voltages of a general MOS network using the linear switch-level model in the presence of unknown values is NP-complete. These results partially justify the use of heuristic methods when unknown values are present.< >", "num_citations": "14\n", "authors": ["1779"]}
{"title": "Modeling and verifying circuits using generalized relative timing\n", "abstract": " We propose a novel technique for modeling and verifying timed circuits based on the notion of generalized relative timing. Generalized relative timing constraints can express not just a relative ordering between events, but also some forms of metric timing constraints. Circuits modeled using generalized relative timing constraints are formally encoded as timed automata. Novel fully symbolic verification algorithms for timed automata are then used to either verify a temporal logic property or to check conformance against an untimed specification. The combination of our new modeling technique with fully symbolic verification methods enables us to verify larger circuits than has been possible with other approaches. We present case studies to demonstrate our approach, including a self-timed circuit used in the integer unit of the Intel/sup /spl reg// Pentium/sup /spl reg//4 processor.", "num_citations": "13\n", "authors": ["1779"]}
{"title": "Revisiting positive equality\n", "abstract": " This paper provides a stronger result for exploiting positive equality in the logic of Equality with Uninterpreted Functions (EUF). Positive equality analysis is used to reduce the number of interpretations required to check the validity of a formula. We remove the primary restriction of the previous approach proposed by Bryant, German and Velev\u00a0[5], where positive equality could be exploited only when all the function applications for a function symbol appear in positive context. We show that the set of interpretations considered by our analysis of positive equality is a subset of the set of interpretations considered by the previous approach. The paper investigates the obstacles in exploiting the stronger notion of positive equality (called robust positive equality) in a decision procedure and provides a solution for it. We present empirical results on some verification benchmarks.", "num_citations": "13\n", "authors": ["1779"]}
{"title": "Tools and Algorithms for the Construction and Analysis of Systems: 10th International Conference, TACAS 2004, Held as Part of the Joint European Conferences on Theory and\u00a0\u2026\n", "abstract": " This volume contains the proceedings of the 10th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2004). TACAS 2004 took place in Barcelona, Spain, from March 29th to April 2nd, as part of the 7th European Joint Conferences on Theory and Practice of Software (ETAPS 2004), whose aims, organization, and history are detailed in a foreword by the ETAPS Steering Committee Chair, Jos \u0301 e Luiz Fiadeiro. TACAS is a forum for researchers, developers, and users interested in ri-rously based tools for the construction and analysis of systems. The conference serves to bridge the gaps between di? erent communities including, but not-mited to, those devoted to formal methods, software and hardware veri? cation, static analysis, programming languages, software engineering, real-time systems, and communication protocols that share common interests in, and techniques for, tool development. In particular, by providing a venue for the discussion of common problems, heuristics, algorithms, data structures, and methodologies, TACAS aims to support researchers in their quest to improve the utility, rel-bility,? exibility, and e? ciency of tools for building systems. TACASseekstheoreticalpaperswithaclearlinktotoolconstruction, papers describingrelevantalgorithmsandpracticalaspectsoftheirimplementation,-pers giving descriptions of tools and associated methodologies, and case studies with a conceptual message.", "num_citations": "13\n", "authors": ["1779"]}
{"title": "Unbounded system verification using decision procedure and predicate abstraction\n", "abstract": " Modeling and analysis of systems with large, infinite or parameterized state spaces has received much attention in the last decade. These systems include microprocessors with unbounded buffers and memories; parameterized cache-coherence and communication protocols with unbounded channels; and distributed algorithms for mutual exclusion. Most previous works have either used general purpose theorem provers with considerable manual guidance or techniques specific to a particular class of systems.", "num_citations": "13\n", "authors": ["1779"]}
{"title": "CMOS circuit verification with symbolic switch-level timing simulation\n", "abstract": " Symbolic switch-level simulation has been extensively applied to the functional verification of complementary metal-oxide-semiconductor (CMOS) circuitry. We have extended this technique to account for real-valued data-dependent delay values and have developed a novel mechanism for symbolically computing data-dependent Elmore delays. We present our symbolic simulation and delay calculation algorithms and discuss their application to the timing and functional verification of full-custom transistor-level CMOS circuitry.", "num_citations": "13\n", "authors": ["1779"]}
{"title": "Formal methods for functional verification\n", "abstract": " Formal hardware verification ranges from proving that two combinational circuits compute the same functions to the much more ambitious task of proving that a sequential circuit obeys some abstract property expressed in temporal logic. In tracing the history of work in this area, we find a few efforts in the 1970s and 1980s, with a big increase in verification capabilities the late 1980s up through today. The advent of efficient Boolean inference methods, starting with Binary Decision Diagrams (BDDs) and more recently with efficient Boolean satisfiability (SAT) checkers has provided the enabling technology for these advances.", "num_citations": "12\n", "authors": ["1779"]}
{"title": "Symbolic functional and timing verification of transistor-level circuits\n", "abstract": " We introduce a new method of verifying the timing of custom CMOS circuits. Due to the exponential number of patterns required, traditional simulation methods are unable to exhaustively verify a medium-sized modern logic block. Static analysis can handle much larger circuits but is not robust with respect to variations from standard circuit structures. Our approach applies symbolic simulation to analyze a circuit over all input combinations without these limitations. We present a prototype simulator (SirSim) and experimental results. We also discuss using SirSim to verify an industrial design which previously required a special-purpose verification methodology.", "num_citations": "12\n", "authors": ["1779"]}
{"title": "Extraction of finite state machines from transistor netlists by symbolic simulation\n", "abstract": " The paper describes a new technique for extracting clock level finite state machines (FSMs) from transistor netlists using symbolic simulation. The transistor netlist is preprocessed to produce a gate level representation of the netlist. Given specifications of the circuit clocking and input and output timing, simulation patterns are derived for a symbolic simulator. The result of the symbolic simulation and extraction process is the next state and output function of the equivalent FSM, represented as Ordered Binary Decision Diagrams. Compared to previous techniques, our extraction process yields an order of magnitude improvement in both space and time, is fully automated and can handle static storage structures and time multiplexed inputs and outputs.", "num_citations": "12\n", "authors": ["1779"]}
{"title": "Mapping switch-level simulation onto gate-level hardware accelerators\n", "abstract": " In this paper, we present a framework for performing switchlevel simulation on hardware accelerators. A symbolic analyzer preprocesses the MOS network into a functionally equivalent Boolean representation. The analyzer thus converts switch-level simulation into a task of evaluating Boolean expressions. Our approach maps the Boolean representation into the instruction set of the hardware accelerator. The resultant framework supports switch level simulation on a class of hardware accelerators that traditionally have been limited to gate-level simulation.", "num_citations": "12\n", "authors": ["1779"]}
{"title": "Third Caltech Conference on Very Large Scale Integration\n", "abstract": " The papers in this book were presented at the Third Caltech Conference on Very Large Scale Integration, held March 21-23, 1983 in Pasadena, California. The conference was organized by the Computer Science Depart ment, California Institute of Technology, and was partly supported by the Caltech Silicon Structures Project. This conference focused on the role of systematic methodologies, theoretical models, and algorithms in all phases of the design, verification, and testing of very large scale integrated circuits. The need for such disciplines has arisen as a result of the rapid progress of integrated circuit technology over the past 10 years. This progress has been driven largely by the fabrica tion technology, providing the capability to manufacture very complex elec tronic systems reliably and at low cost. At this point the capability to manufac ture very large scale integrated circuits has exceeded our capability to develop new product designs quickly, reliably, and at a reasonable cost. As a result new designs are undertaken only if the production volume will be large enough to amortize high design costs, products first appear on the market well past their announced delivery date, and reference manuals must be amended to document design flaws. Recent research in universities and in private industry has created an emerg ing science of very large scale integration.", "num_citations": "11\n", "authors": ["1779"]}
{"title": "Symbolic timing simulation using cluster scheduling\n", "abstract": " We recently introduced symbolic timing simulation (STS) using data-dependent delays as a tool for verifying the timing of full-custom transistor-level circuit designs, and for the functional verification of delay-dependent logic. While STS leverages efficient symbolic encodings to yield huge gains over conventional simulation methodologies, it still suffers from a problem known as event multiplication. We discuss this problem and present an event-list management technique based on event-clusters, and a new simulator which utilizes this technique. Finally, we demonstrate substantial speedups on a wide range of test cases, including exponential improvement on a simple logic chain.", "num_citations": "11\n", "authors": ["1779"]}
{"title": "Incorporating timing constraints in the efficient memory model for symbolic ternary simulation\n", "abstract": " This paper introduces the four timing constraints of setup time, hold time, minimum delay, and maximum delay in the efficient memory model (EMM). The EMM is a behavioral model, where the number of symbolic variables used to characterize the initial state of the memory is proportional to the number of distinct symbolic memory locations accessed. The behavioral model provides a conservative approximation of the replaced memory array, while allowing the address and control inputs of the memory to accept symbolic ternary values. If a circuit has been formally verified with the behavioral model, the system is guaranteed to function correctly with any memory implementation whose timing parameters are bounded by the ones used in the verification.", "num_citations": "11\n", "authors": ["1779"]}
{"title": "Geometric characterization of series-parallel variable resistor networks\n", "abstract": " The range of operating conditions for a series-parallel network of variable linear resistors, voltage sources, and current sources can be represented as a convex polygon in a Thevenin or Norton half-plane. For a network with n elements of which k are variable, these polygons have at most 2k vertices and can be computed in O(nk) time. These half planes are embedded in the real projective plane to represent circuits with potentially infinite Thevenin resistance or Norton conductance. For circuits that have an acyclic structure once all branches to ground are removed, the characteristic polygons for all nodes with respect to ground can be computed simultaneously by an algorithm of complexity O(nk).< >", "num_citations": "11\n", "authors": ["1779"]}
{"title": "Term-level verification of a pipelined CISC microprocessor\n", "abstract": " By abstracting the details of the data representations and operations in a microprocessor, term-level verification can formally prove that a pipelined microprocessor faithfully implements its sequential, instruction-set architecture specification. Previous efforts in this area have focused on reduced instruction set computer (RISC) and very-large instruction word (VLIW) processors. This work reports on the verification of a complex instruction set computer (CISC) processor styled after the Intel IA32 instruction set using the UCLID term-level verifier. Unlike many case studies for term-level verification, this processor was not designed specifically for formal verification. In addition, most of the control logic was given in a simplified hardware description language. We developed a methodology in which the control logic is translated into UCLID format automatically, and the pipelined processor and the sequential reference version were described with as much modularity as possible. The latter feature was made especially difficult by UCLID\u2019s limited support for modularity.", "num_citations": "10\n", "authors": ["1779"]}
{"title": "Verification of pipelined microprocessors by correspondence checking in symbolic ternary simulation\n", "abstract": " This paper makes the idea of memory shadowing (Bryant and Velev, 1997) applicable to symbolic ternary simulation. Memory shadowing, an extension of Burch and Dill's (1994) pipeline verification method to the bit level, is a technique for providing on-the-fly identical initial memory state to two different memory execution sequences. We also present an algorithm which compares the final states of two memories for ternary correspondence, as well as an approach for generating efficiently the initial state of memories. These techniques allow us to verify that a pipelined circuit has behavior corresponding to that of its unpipelined specification by simulating two symbolic ternary execution sequences and comparing their memory states. Experimental results show the potential of the new ideas.", "num_citations": "10\n", "authors": ["1779"]}
{"title": "User\u2019s guide to COSMOS, a compiled simulator for MOS circuits\n", "abstract": " The COSMOS system is a set of programs that transform a logic circuit into an executable simulation program [4]. This compiled approach to logic simulation provides very high performance and allows flexibility in customizing the simulation command environment.The logic circuit consists of a set of nodes connected by MOS transistors and functional blocks, allowing a mixture of structural and behavioral modeling. COSMOS applies a symbolic analyzer to the transistor circuitry to generate a behavioral description of the logical behavior. These descriptions are then transformed into C language evaluation procedures. These procedures are compiled together with C procedures modeling the functional blocks (provided by the user) to generate the simulation program.", "num_citations": "10\n", "authors": ["1779"]}
{"title": "Nonsilicon, Non-von Neumann Computing\u2014Part I [Scanning the Issue]\n", "abstract": " The future of computing is at crossroads. The technological advances that have sustained the exponential growth of computing performance over the last several decades are slowing and the roadmap for future advances is uncertain. The phenomenal expansion of computing power has made computers ubiquitous, spawned a $300 billion semiconductor industry, enabled unprecedented global economic growth, and transformed many aspects of society at large. Emerging technologies are placing an ever-growing and changing demand on computing, especially the profusion of data from the Internet of Things, large-scale scientific experiments (high-energy physics, astronomy, and genomics), autonomous vehicles, social media (including video), national security systems, and the finance sector. Transmitting, storing, processing, and analyzing this data explosion with the requisite speed and performance\u2014and\u00a0\u2026", "num_citations": "9\n", "authors": ["1779"]}
{"title": "Ordered Binary Decision Diagrams In Electronic Design Automation: Foundations, Applications and Innovations\n", "abstract": " Ordered Binary Decision Diagrams (OBDDs) play a key role in the automated synthesis and formal verification of digital systems. They are the state-of-the-art data structure for representing switching functions in various branches of electronic design automation. In the following we discuss the properties of this data structure, characterize its algorithmic behaviour, and describe some prominent applications.", "num_citations": "9\n", "authors": ["1779"]}
{"title": "Symbolic Simulation Using Automatic Abstraction of Internal Node Values\n", "abstract": " In recent years, verification has emerged as a major portion of the effort in designing large, complex chips. Simulation-based methods such as directed and random testing are the most widely used verification methods today. However, there is growing concern that simulation will not be able to keep up with increased design sizes in the future. Research into better verification methods has focussed on symbolic methods such as model checking. These methods have had success in augmenting simulation, but, so far, have not been able to replace simulation as the primary verification method.", "num_citations": "9\n", "authors": ["1779"]}
{"title": "Current and future Ph. D. output will not satisfy demand for faculty\n", "abstract": " This article and the accompanying tables present the results of the 30th CRA Taulbee Survey1 of Ph. D.-granting departments of computer science (CS) and computer engineering (CE) in the United States and Canada. This survey is conducted annually by the Computing Research Association to document trends in student enrollment, employment of graduates, and faculty salaries. Information is gathered during the fall and early winter. The period the data cover varies from table to table. Degree production (Ph. D., Master\u2019s, and Bachelor\u2019s) and total Ph. D. enrollments refer to the previous academic year (1999-2000). Data for new students in all categories and total enrollments for Master\u2019s and Bachelor\u2019s degrees refer to the current academic year (2000-2001). Projected student production and information on faculty salaries and demographics also refer to the current academic year. Faculty salaries are those effective January 1, 2001. Responses received by January 14, 2001 are included in the tables. The survey results represent input from Ph. D.-granting departments only. A total of 214 departments were surveyed, compared with 203 departments last year. Overall, the response rate was 81%(173 departments), a slight improvement over the past several years (Figure 1). The return rate of 6 out of 28 (21%) for CE programs is once again very low. We attribute this low response to two factors: 1) many CE programs are part of an ECE department, and they do not keep separate statistics for CE vs. EE, and 2) many of these departments are not aware of the Taulbee Survey or its importance. The response rates for US CS programs (148 of 163, or\u00a0\u2026", "num_citations": "9\n", "authors": ["1779"]}
{"title": "From data to knowledge to action: Enabling the smart grid\n", "abstract": " Our nation's infrastructure for generating, transmitting, and distributing electricity - \"The Grid\" - is a relic based in many respects on century-old technology. It consists of expensive, centralized generation via large plants, and a massive transmission and distribution system. It strives to deliver high-quality power to all subscribers simultaneously - no matter what their demand - and must therefore be sized to the peak aggregate demand at each distribution point. Ultimately, the system demands end-to-end synchronization, and it lacks a mechanism for storing (\"buffering\") energy, thus complicating sharing among grids or independent operation during an \"upstream\" outage. Recent blackouts demonstrate the existing grid's problems - failures are rare but spectacular. Moreover, the structure cannot accommodate the highly variable nature of renewable energy sources such as solar and wind. Many people are pinning their hopes on the \"smart grid\" - i.e., a more distributed, adaptive, and market-based infrastructure for the generation, distribution, and consumption of electrical energy. This new approach is designed to yield greater efficiency and resilience, while reducing environmental impact, compared to the existing electricity distribution system. Initial plans for the smart grid suggest it will make extensive use of existing information technology. In particular, recent advances in data analytics - i.e., data mining, machine learning, etc. - have the potential to greatly enhance the smart grid and, ultimately, amplify its impact, by helping us make sense of an increasing wealth of data about how we use energy and the kinds of demands that we are placing upon\u00a0\u2026", "num_citations": "8\n", "authors": ["1779"]}
{"title": "Efficient Exploratory Testing of Concurrent Systems\n", "abstract": " In our experience, exploratory testing has reached a level of maturity that makes it a practical and often the most cost-effective approach to testing. Notably, previous work has demonstrated that exploratory testing is capable of finding bugs even in well-tested systems [4, 17, 24, 23]. However, the number of bugs found gives little indication of the efficiency of a testing approach. To drive testing efficiency, this paper focuses on techniques for measuring and maximizing the coverage achieved by exploratory testing. In particular, this paper describes the design, implementation, and evaluation of Eta, a framework for exploratory testing of multithreaded components of a large-scale cluster management system at Google. For simple tests (with millions to billions of possible executions), Eta achieves complete coverage one to two orders of magnitude faster than random testing. For complex tests, Eta adopts a state space reduction technique to avoid the need to explore over 85% of executions and harnesses parallel processing to explore multiple test executions concurrently, achieving a throughput increase of up to 17.5\u00d7.Acknowledgements: The authors would like to thank Brian Grant, John Wilkes, Robert Kennedy, Swapnil Patil, Vijay Vasudevan, and Walfredo Cirne for their technical feedback. Further, the authors are thankful to Google for providing its hardware and software infrastructure for the experimental evaluation presented in this report. Last but not least, we also thank the members and companies of the PDL Consortium (including APC, EMC, Facebook, Google, Hewlett-Packard, Hitachi, IBM, Intel, LSI, Microsoft, NEC, NetApp, Oracle, Panasas\u00a0\u2026", "num_citations": "8\n", "authors": ["1779"]}
{"title": "A theory of consistency for modular synchronous systems\n", "abstract": " We propose a model for modular synchronous systems with combinational dependencies and define consistency using this model. We then show how to derive this model from a modular specification where individual modules are specified as Kripke Structures and give an algorithm to check the system for consistency. We have implemented this algorithm symbolically using BDDs in a tool, SpecCheck. We have used this tool to check an example bus protocol derived from an industrial specification. The counterexamples obtained for this protocol highlight the need for consistency checking.", "num_citations": "8\n", "authors": ["1779"]}
{"title": "Digital circuit verification using partially-ordered state models\n", "abstract": " Many aspects of digital circuit operation can be efficiently verified by simulating circuit operation over \"weakened\" state values. This technique has long been practiced with logic simulators, using the value X to indicate a signal that could be either 0 or 1. This concept can be formally extended to a wider class of circuit models and signal values, yielding lattice-structured state domains. For more precise modeling of circuit operation, these values can be encoded in binary and hence represented symbolically as ordered binary decision diagrams. The net result is a tool for formal verification that can apply a hybrid of symbolic and partially-ordered evaluation.< >", "num_citations": "8\n", "authors": ["1779"]}
{"title": "An efficient BDD-based A* algorithm\n", "abstract": " In this paper we combine the goal directed search of A* with the ability of BDDs to traverse an exponential number of states in polynomial time. We introduce a new algorithm, SetA*, that generalizes A* to expand sets of states in each iteration. SetA* has substantial advantages over BDDA*, the only previous BDD-based A* implementation we are aware of. Our experimental evaluation proves SetA* to be a powerful search paradigm. For some of the studied problems it outperforms BDDA*, A*, and BDD-based breadth-first search by several orders of magnitude. We believe exploring sets of states to be essential when the heuristic function is weak. For problems with strong heuristics, SetA* efficiently specializes to single-state search and consequently challenges single-state heuristic search in general.", "num_citations": "7\n", "authors": ["1779"]}
{"title": "Optimizing symbolic model checking for invariant-rich models\n", "abstract": " This paper presents new symbolic-model-checking optimizations for verifying systems with complex invariants. In model checking, the invariants are conditions that are always true in the model. They are commonly used for making the same non-deterministic choices, and also for establishing the interface between different components in a system. To verify invariant-rich systems, we propose two new optimizations. First optimization is a simple extension of the conjunctive-partitioning algorithm. The other is a collection of BDD-based macro-extraction and macro-expansion algorithms to remove state variables. We show that these two optimizations are essential in verifying invariant-rich problems; in particular, this work enabled the verification of fault diagnosis models for the Nomad robot (an antarctic meteorite explorer)[10] and the NASA Deep Space One spacecraft [1].", "num_citations": "7\n", "authors": ["1779"]}
{"title": "Switch-Level Modeling of MOS Digital Circuits\n", "abstract": " The switch-level model describes the logical  behavior of digital circuits implemented in metal oxide semiconductor (MOS) technology. In this model a network consists of a set of nodes connected by transistor \"switches\" with each node having a state 0, 1, or X, and each transistor having a state open, closed, or unknown. The logic simulator MOSSIM II has been implemented with this model as its basis. MOSSIM II can simulate a wide variety of MOS circuits at speeds approaching those of event-driven logic gate simulators. The simulator can apply additional tests to detect potential timing errors, unrestored logic levels in CMOS, and unrefreshed dynamic charge. This paper provides an overview of the switch-level model and how it is applied in MOSSIM II.", "num_citations": "7\n", "authors": ["1779"]}
{"title": "CS: APP2e Web Aside ASM: SSE: SSE-Based Support for Floating Point\n", "abstract": " The floating-point architecture for a processor consists of the different aspects that affect how programs operating on floating-point data are mapped onto the machine, including:", "num_citations": "6\n", "authors": ["1779"]}
{"title": "Power and politeness in interactions: Admire-a tool for deriving the former from the latter\n", "abstract": " We describe the background theory, implementation and experimental validation of a tool (ADMIRE, for Assessment of Discourse Media Indicators of Relative Esteem) which recognizes and scores politeness behaviors in the textual interactions of multiple individuals in order to derive power relations among them. ADMIRE's implementation architecture and scalability is described, along with an innovative approach to creating and drawing power networks (roughly similar to organizational charts) from the power relationships inferred from politeness usages between speakers and hearers in a large body of interactions. While ADMIRE was developed and preliminarily tested on email, transcribed spoken dialogue and Internet Relay Chat data, it was formally tested in a large, weeklong military simulation exercise (Empire Challenge 2010) where it proved 100% successful at deriving ground truth power relationships in\u00a0\u2026", "num_citations": "6\n", "authors": ["1779"]}
{"title": "From data to knowledge to action: Enabling advanced intelligence and decision-making for America\u2019s security\n", "abstract": " Large-scale machine learning can fundamentally transform the ability of intelligence analysts to efficiently extract important insights relevant to our nation\u2019s security from the vast amounts of intelligence data being generated and collected worldwide. Intelligence organizations can tap into rapid data analytics innovations that Internet industries and university research organizations are making through the use of unclassified research partnerships.", "num_citations": "6\n", "authors": ["1779"]}
{"title": "A User\u2019s Guide to UCLID Version 3.0\n", "abstract": " The UCLID system is used to specify and verify systems that have infinite or unbounded state. The core features of the modeling language include uninterpreted function and predicate symbols, an arithmetic of counters, bit-vector arithmetic and restricted lambda expressions. 1 Verification methods that can be used with UCLID include inductive invariant checking, correspondence checking (such as in the style of Burch and Dill [3]), proving simulation diagrams showing that one machine simulates another, and bounded model checking. All of these verification techniques use symbolic simulation in one form or another. The UCLID system comprises of the UCLID specification language and the verification engine. The UCLID language can be used to specify a state machine, where each state variable falls into one of three classes: Boolean, enumerated, or uninterpreted symbols. The UCLID verification engine comprises of a symbolic simulator that can be \u201cconfigured\u201d for different kinds of verification tasks, and a decision procedure for a logic of Counter Arithmetic with Lambda Expressions and Uninterpreted Functions (CLUF).This document is intended to serve as an overview of UCLID, as a tutorial, and as a reference manual for the UCLID user. In Section 2, we describe the syntax and semantics of the UCLID language. In Section 3, we describe its modeling and verification capabilities. These capabilities are illustrated with examples in Section 4. For theoretical details about how the tool works, we refer the reader to our upcoming technical report.", "num_citations": "6\n", "authors": ["1779"]}
{"title": "Computer system\n", "abstract": " A web page element updating system comprising a central data source, having a processor and a memory, a plurality of primary web pages, an embedded web page within each of the plurality of primary web pages, means for obtaining data from the embedded webpage to an element of the primary webpage, a correlator for correlating an identifier of data within the embedded web page with a label of the element, wherein each primary web page comprises an element with an associated label, each embedded web page contains data with an identifier corresponding to the label of the element, the central data memory is in communication with the embedded web page and the processor is configured such that new data entered into the central data memory is transmitted from the memory to the embedded webpage where it is held with an identifier, the correlator is configured to correlate the data with the appropriate\u00a0\u2026", "num_citations": "6\n", "authors": ["1779"]}
{"title": "Symbolic simulation, model checking and abstraction with partially ordered boolean functional vectors\n", "abstract": " Boolean Functional Vectors (BFVs) are a symbolic representation for sets of bit-vectors that can be exponentially more compact than the corresponding characteristic functions with BDDs. Additionally, BFVs are the natural representation of bit-vector sets for Symbolic Simulation. Recently, we developed set manipulation algorithms for canonical BFVs by interpreting them as totally ordered selections. In this paper we generalize BFVs by defining them with respect to a partial order. We show that partially ordered BFVs can serve as abstractions for bit-vector sets and can be used to compute over-approximations in reachability analysis. In the special case when the underlying graph of the partial order is a forest, we can efficiently compute an abstract interpretation in a symbolic simulation framework. We present circuit examples where we leverage the exponential gap in the representations and inherent structure\u00a0\u2026", "num_citations": "6\n", "authors": ["1779"]}
{"title": "Synthesis of fault-tolerant plans for non-deterministic domains\n", "abstract": " Non-determinism is often caused by infrequent errors that make otherwise deterministic actions fail. In this paper, we introduce fault tolerant planning to address this problem. An \u0433-fault tolerant plan is guaranteed to recover from up to \u0433 errors occurring during its execution. We show how optimal \u0433-fault tolerant plans can be generated via the strong universal planning algorithm. This algorithm uses an implicit search technique based on the reduced Ordered Binary Decision Diagram (OBDD) that is particularly well suited for non-deterministic planning and has outperformed most alternative approaches. However, the OBDDs used to represent the blind backward search of the strong algorithm often blow up. A heuristic version of the algorithm has recently been proposed but is incapable of dynamically guiding the recovery part of the plan toward error states. To address this problem, we introduce two specialized algorithms 1-FTP (blind) and 1-GFTP (guided) for 1-fault tolerant planning that decouples the synthesis of the recovery and nonrecovery part of the plan. Our experimental evaluation includes 7 domains of which 3 are significant real-world cases. It verifies that 1-GFTP efficiently can handle non-local fault states and demonstrates that it due to this property can outperform guided fault tolerant planning via strong planning. In addition, 1-FTP often outperforms strong planning due to an aggressive expansion strategy of the recovery plan.", "num_citations": "6\n", "authors": ["1779"]}
{"title": "Incremental switch-level analysis\n", "abstract": " An algorithm is presented for extracting a two-level subnetwork hierarchy from flat netlists. They discuss the application of this algorithm to incremental circuit analysis in the Cosmos compiled switch-level simulator. The algorithm decreases the network preprocessing time for Cosmos by nearly an order of magnitude. The file system is used as a large hash table that retains information over many executions of the incremental analyzer. The hierarchy-extraction algorithm computes a hash signature for each subnetwork by coloring vertices somewhat the way wirelist-comparison programs do. It then identifies duplicates, using standard hash-table techniques.< >", "num_citations": "6\n", "authors": ["1779"]}
{"title": "A view from the engine room: Computational support for symbolic model checking\n", "abstract": " Symbolic model checking owes much of its success to powerful methods for reasoning about Boolean functions. The first symbolic model checkers used Ordered Binary Decision Diagrams (OBDDs) [1] to represent system transition relations and sets of system states [9]. All of the steps required for checking a model can be expressed as a series of operations on these representations, without ever enumerating individual states or transitions. More recently, bounded [3] and unbounded [10,11] model checkers have been devised that use Boolean satisfiability (SAT) solvers as their core computational engines. Methods having a SAT solver work on a detailed system model and OBDDs operate on an abstracted model have shown that the combination of these two reasoning techniques can be more powerful than either operating on its own [4]. Boolean methods have enabled model checkers to scale to handle\u00a0\u2026", "num_citations": "5\n", "authors": ["1779"]}
{"title": "Data Intensive Super Computing\n", "abstract": " Data Intensive Super Computing Page 1 Data Intensive Super Computing http://www.cs.cmu.edu/~bryant Randal E. Bryant Carnegie Mellon University Page 2 \u20132\u2013 Motivation \u220e 200+ processors \u220e 200+ terabyte database \u220e 1010 total clock cycles \u220e 0.1 second response time \u220e 5\u00a2 average advertising revenue Page 3 \u20133\u2013 Google\u2019s Computing Infrastructure System \u220e ~ 3 million processors in clusters of ~2000 processors each \u220e Commodity parts \u25cf x86 processors, IDE disks, Ethernet communications \u25cf Gain reliability through redundancy & software management \u220e Partitioned workload \u25cf Data: Web pages, indices distributed across processors \u25cf Function: crawling, index generation, index search, document retrieval, Ad placement A Data-Intensive Super Computer (DISC) \u220e Large-scale computer centered around data \u25cf Collecting, maintaining, indexing, computing \u220e Similar systems at Microsoft & Yahoo Barroso, \u2026", "num_citations": "5\n", "authors": ["1779"]}
{"title": "A User\u2019s Guide to UCLID version 1.0\n", "abstract": " The UCLID system is used to specify and verify systems that have infinite or unbounded state. The core features of the modeling language include uninterpreted function and predicate symbols, an arithmetic of counters and restricted lambda expressions. 1 Verification methods that can be used with UCLID include inductive invariant checking, correspondence checking (such as in the style of Burch and Dill [3]), proving simulation diagrams showing that one machine simulates another, and bounded model checking. All of these verification techniques use symbolic simulation in one form or another.The UCLID system comprises of the UCLID specification language and the verification engine. The UCLID language can be used to specify a state machine, where each state variable falls into one of three classes: Boolean, enumerated, or uninterpreted symbols. The UCLID verification engine comprises of a symbolic simulator that can be \u201cconfigured\u201d for different kinds of verification tasks, and a decision procedure for a logic of Counter Arithmetic with Lambda Expressions and Uninterpreted Functions (CLUF).", "num_citations": "5\n", "authors": ["1779"]}
{"title": "x86-64 Machine-Level Programming\n", "abstract": " Intel\u2019s IA32 instruction set architecture (ISA), colloquially known as \u201cx86\u201d, is the dominant instruction format for the world\u2019s computers. IA32 is the platform of choice for most Windows, Linux, and, since 2006, even Macintosh computers. The ISA we use today was defined in 1985 with the introduction of the i386 microprocessor, extending the 16-bit instruction set defined by the original 8086 to 32 bits. Even though subsequent processor generations have introduced new instruction types and formats, many compilers, including GCC, have avoided using these features in the interest of maintaining backward compatibility.A shift is underway to a 64-bit version of the Intel instruction set. Originally developed by Advanced Micro Devices (AMD) and named x86-64, it is now supported by high end processors from AMD (who now call it AMD64) and by Intel, who refer to it as Intel64. Most people still refer to it as \u201cx86-64,\u201d and we follow this convention.(Some vendors have shortened this to simply \u201cx64\u201d). Newer versions of Linux and GCC support this extension. In making this switch, the developers of GCC saw an opportunity to also make use of some of the instruction-set features that had been added in more recent generations of IA32 processors.", "num_citations": "4\n", "authors": ["1779"]}
{"title": "A boolean approach to unbounded, fully symbolic model checking of timed automata\n", "abstract": " We present a new approach to unbounded, fully symbolic model checking of timed automata that is based on an efficient translation of quantified separation logic to quantified Boolean logic. Our technique preserves the interpretation of clocks over the reals and can check any property expressed in the timed mu calculus. The core operations of eliminating quantifiers over real variables and deciding separation logic are respectively translated to eliminating quantifiers on Boolean variables and checking Boolean satisfiability SAT. We can thus leverage well-known techniques for Boolean formulas, including Binary Decision Diagrams BDDs and recent advances in SAT and SAT-based quantifier elimination. We present preliminary empirical results for a BDD-based implementation of our method.Descriptors:", "num_citations": "4\n", "authors": ["1779"]}
{"title": "Symbolic functional and timing verification of transistor-level circuits\n", "abstract": " This thesis presents a new technology called Symbolic Timing Simulation (STS), and applies it to the timing and functional verification of full-custom transistor-level circuits.", "num_citations": "4\n", "authors": ["1779"]}
{"title": "Formal Verification by Symbolic Evaluation of Partially-Ordered Trajectories\n", "abstract": " Symbolic trajectory evaluation provides a means to formally verify properties of a sequential system by a modi ed form of symbolic simulation. The desired system properties are expressed in a notation combining Boolean expressions and the temporal logic\\next-time\" operator. In its simplest form, each property is expressed as an assertion A=) C], where the antecedent A expresses some assumed conditions on the system state over a bounded time period, and the consequent C expresses conditions that should result. A generalization allows simple invariants to be established and proven automatically.The veri er operates on system models in which the state space is ordered by\\information content\". By suitable restrictions to the speci cation notation, we guarantee that for every trajectory formula, there is a unique weakest state trajectory that satis es it. Therefore, we can verify an assertion A=) C] by simulating the system over the weakest trajectory for A and testing adherence to C. Also, establishing invariants correspond to simple xed point calculations.", "num_citations": "4\n", "authors": ["1779"]}
{"title": "Symbolic analysis methods for masks, circuits and systems\n", "abstract": " Symbolic representations of systems can achieve a high degree of compaction relative to more explicit forms. By casting and analysis task in terms of operations on a symbolic representation, large and complex systems can be analyzed efficiently. This paper summarizes research in applying symbolic analysis methods to systems at several levels of abstraction.< >", "num_citations": "4\n", "authors": ["1779"]}
{"title": "Formal hardware verification by symbolic simulation\n", "abstract": " As digital systems become more complex, traditional methods of testing a design by simulation become unreliable. Formal verification overcomes this weakness by proving that a circuit will meet its specification under all possible operating conditions. Symbolic simulation provides an attractive approach to formal verification. In one simulation run, a symbolic simulator can compute what would require many runs of a traditional simulator. This approach to formal verication can deal with many difficult aspects of VLSI circuits including switch-level models, detailed timing, and pipelining.", "num_citations": "4\n", "authors": ["1779"]}
{"title": "Runtime estimation and resource allocation for concurrency testing\n", "abstract": " In the past 15 years, stateless exploration, a collection of techniques for automated and systematic testing of concurrent programs, has experienced wide-spread adoption. As stateless exploration moves into practice, becoming part of testing infrastructures of large-scale system developers, new practical challenges are being identified. In this paper we address the problem of efficient allocation of resources to stateless exploration runs. To this end, this paper presents techniques for estimating the total runtime of stateless exploration runs and policies for allocating resources among tests based on these runtime estimates.Evaluating our techniques on a collection of traces from a real-world deployment at Google, we demonstrate the techniques\u2019 success at providing accurate runtime estimations, achieving estimation accuracy above 60% after as little as 1% of the state space has been explored. We further show that these estimates can be used to implement intelligent resource allocation policies that meet testing objectives more than twice as efficiently as the round-robin policy.", "num_citations": "3\n", "authors": ["1779"]}
{"title": "Abstracting RTL designs to the term level\n", "abstract": " Term-level verification is a formal technique that seeks to verify RTL hardware descriptions by abstracting away details of data representations and operations. The key to making term-level verification automatic and efficient is in deciding what to abstract. We investigate this question in this paper and propose a solution based on the use of type qualifiers. First, we demonstrate through case studies that only selective term-level abstraction can be very effective in reducing the run-time of formal tools while still retaining precision of analysis. Second, the term-level abstraction process can be guided using lightweight type qualifiers. We present an annotation language and type inference scheme that is applied to the formal verification of the Verilog implementation of a chip multiprocessor router. Experimental results indicate type-based selective term-level abstraction is effective at scaling up verification with minimal designer guidance.", "num_citations": "3\n", "authors": ["1779"]}
{"title": "A symbolic simulation-based methodology for generating black-box timing models of custom macrocells\n", "abstract": " We present a methodology for generating black-box timing models for full-custom transistor-level CMOS circuits. Our approach utilizes transistor-level ternary symbolic timing simulation to explore the input arrival time space and determine the input arrival time windows that result in proper operation. This approach integrates symbolic timing simulation into existing static timing analysis flows and allows automated modelling of the timing behavior of aggressive full-custom circuit design styles.", "num_citations": "3\n", "authors": ["1779"]}
{"title": "Alpha Assembly Language Guide\n", "abstract": " This document provides an overview of the Alpha instruction set and assembly language programming conventions. The Alpha architecture was formulated by Digital Equipment Corporation as a second generation reduced instruction set computer (RISC) architecture. It represents a careful balance between providing a sufficient range of instructions to encode common operations while avoiding a lot of features that could slow down the machine or lead to implementation difficulties in the future. Digital Equipment Corporation was subsequently acquired by Compaq in the Summer of 1998. Compaq is continuing to support Alpha. More complete documentation is available from Digital/Compaq [1, 2].", "num_citations": "3\n", "authors": ["1779"]}
{"title": "A Special purpose processor for switch-level simulation\n", "abstract": " As the complexity of VLSI circuits approaches 10% devices, the computational requirements of design verification are exceeding the capacity of general purpose computers. To provide the computing power required to verify these complex VLSI chips, special purpose hardware for performing simulation is required. Existing logic simulation engines (1, 2, 3, 4) are inadequate for MOS VLSI because they cannot accurately model MOS circuits. Switch-level simulation, on the other hand, models the effects of capacitance and transistor ratios at speeds comparable to logic simulation. Existing machines limit the size of a circuit which can be simulated by binding circuit elements to hardware at compile time. Virtual network processing allows circuits of any size to be simulated by binding circuit elements to hardware at run-time.", "num_citations": "3\n", "authors": ["1779"]}
{"title": "Design considerations for a partial differential equation machine\n", "abstract": " Partial differential equation (PDE) simulation provides an attractive area for the application of highly parallel computer systems. The regular and static structures of these problems and the limited data dependencies allow them to be mapped onto a system consisting of many interconnected processors. This paper presents an analysis of a program for simulating the hydrodynamic motion and heat flow in a compressible fluid. Based on this analysis, some of the issues In designing programming languages and Computer architectures for PDE simulations are discussed. The data flow model of computation is seen to provide an attractive means for managing the complexity of highly parallet systems. Data flow concepts can be applled to relatively simple architectures specifically designed for PDE simulation. introduction", "num_citations": "3\n", "authors": ["1779"]}
{"title": "Formal verification of pipelined Y86-64 microprocessors with UCLID5\n", "abstract": " This work reports on the verification of a complex instruction set computer (CISC) processor, named Y86-64, styled after the Intel64 instruction set using the UCLID5 verifier. We developed a methodology in which the control logic is translated into UCLID5 format automatically, and the pipelined processor and the sequential reference version were described with as much modularity as possible. This work provides confidence in the processor designs presented in the Bryant-O\u2019Hallaron textbook on computer systems, and it also provides a case study for the capabilities and performance of UCLID5.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "CS: APP2e web aside MEM: BLOCKING: Using blocking to increase temporal locality\n", "abstract": " There is an interesting technique called blocking that can improve the temporal locality of inner loops. The general idea of blocking is to organize the data structures in a program into large chunks called blocks.(In this context,\u201cblock\u201d refers to an application-level chunk of data, not to a cache block.) The program is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it needs to on that chunk, then discards the chunk, loads in the next chunk, and so on.Unlike the simple loop transformations for improving spatial locality, blocking makes the code harder to read and understand. For this reason, it is best suited for optimizing compilers or frequently executed library routines. Still, the technique is interesting to study and understand because it is a general concept that can produce big performance gains on some systems.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Concurrent systematic testing at scale\n", "abstract": " Systematic testing, first demonstrated in small, specialized cases 15 years ago, has matured sufficiently for large-scale systems developers to begin to put it into practice. With actual deployment comes new, pragmatic challenges to the usefulness of the techniques. In this report we are concerned with scaling dynamic partial order reduction, a key technique for mitigating the state space explosion problem, to very large clusters. In particular, we present a new approach for distributed dynamic partial order reduction. Unlike previous work, our approach is based on a novel exploration algorithm that 1) enables trading space complexity for parallelism, 2) achieves efficient load-balancing through time-slicing, 3) provides for fault tolerance, 4) has been demonstrated to scale to more than a thousand parallel workers, and 5) is guaranteed to avoid redundant exploration of overlapping portions of the state space.Acknowledgements: This research was sponsored by the US Army Research Office under grant number W911NF0910273. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the US government or any other entity. Further, the authors would like to thank Brian Grant, John Wilkes, Robert Kennedy, Todd Wang, and Walfredo Cirne for their continuous technical feedback and support on the ongoing ETA effort. The authors are also thankful to Google for providing its hardware and software infrastructure for the experimental evaluation presented in this report. Last but not least, we also thank the\u00a0\u2026", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Data-Intensive Scalable Computing Harnessing the Power of Cloud Computing\n", "abstract": " Our world is awash in data. Millions of devices generate digital data, an estimated one zettabyte (that\u2019s 1021 bytes) per year. Much of this data gets transmitted over networks and stored on disk drives. With the dramatic cost reductions in magnetic storage technology, we can readily collect and store massive amounts of data. But, what is all this data good for? Consider the following examples:The 6000 Wal-Mart stores worldwide record every purchase by every shopper, totaling around 267 million transactions per day. They collect all this information in a 4-petabyte (that\u2019s 4 X 1015 bytes) data warehouse set up for them by Hewlett-Packard. These transactions are a treasure trove of information about the shopping habits of their customers. How much did that that $200 discount on large-screen TVs increase sales, and how much did the shoppers who bought them spend on other things? How many copies of the upcoming John Grisham novel should we stock? Based on long-term weather forecasts, how many snow shovels should we order for our stores in Iowa? Sophisticated machinelearning algorithms can find answers to this question, given the right data and the right computing power.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Symbolic representation with ordered function templates\n", "abstract": " Binary Decision Diagrams (BDDs) often fail to exploit sharing between Boolean functions that differ only in their support variables. In a memory circuit, for example, the functions for the different bits of a word differ only in the data bit while the address decoding part of the function is identical. We present a symbolic representation approach using ordered function templates to exploit such regularity. Templates specify functionality without being bound to a specific set of variables. Functions are obtained by instantiating templates with a list of variables. We ensure canonicity of the representation by requiring that templates are normalized and argument lists are ordered. We also present algorithms for performing Boolean operations using this representation. Experiments with a prototype implementation built on top of CUDD indicate that function templates can dramatically reduce memory requirements for symbolic\u00a0\u2026", "num_citations": "2\n", "authors": ["1779"]}
{"title": "SetA* applied to channel routing\n", "abstract": " This report describes an application of the SetA* algorithm to VLSI channel routing. We consider an extended form of the classical routing problem where pins of nets can occur anywhere within the channel. The derived algorithm can use general cost functions and heuristics given that the total routing cost equals the sum of routing costs for each column. For this class of problems we show a graph-based approach for deriving an admissible heuristic for any cost function. The approach is evaluated on a subset of classical routing problems generated from ISCAS-84. We obtain results similar to the most efficient current approaches.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Cache Memories\n", "abstract": " Introduction to Computer Systems 15-213/18-243, spring 2009 Page 1 DAC 2001 Tutorial \u00a9RA Rutenbar, 2001 1 Carnegie Mellon 1 Cache Memories Slides Courtesy of: Randy Bryant and Dave O\u2019Hallaron Carnegie Mellon 2 Today \u25a0 Cache memory organization and operation \u25a0 Performance impact of caches \u25aa The memory mountain \u25aa Rearranging loops to improve spatial locality \u25aa Using blocking to improve temporal locality Page 2 DAC 2001 Tutorial \u00a9RA Rutenbar, 2001 2 Carnegie Mellon 3 Cache Memories \u25a0 Cache memories are small, fast SRAM-based memories managed automatically in hardware. \u25aa Hold frequently accessed blocks of main memory \u25a0 CPU looks first for data in caches (eg, L1, L2, and L3), then in main memory. \u25a0 Typical system structure: Main memory I/O bridge Bus interface ALU Register file CPU chip System bus Memory bus Cache memories Carnegie Mellon 4 General Cache \u2026", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Formal verification of pipelined processors\n", "abstract": " Correspondence checking formally verifies that a pipelined microprocessor realizes the serial semantics of the instruction set model. By representing the circuit state symbolically with Ordered Binary Decision Diagrams (OBDDs), this correspondence checking can be performed directly on a logic-level representation of the circuit. Our ongoing research seeks to make his approach practical for real-life microprocessors.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "An analysis of hashing on parallel and vector computers\n", "abstract": " Parallel hashing is well-known in the folklore of parallel computing, but there has been a remarkable dearth of literature describing it in detail. Because many parallel algorithms such as histogramming, set intersection and dictionary lookup can make use of hashing as a core step, hashing is a fundamental parallel operation. This paper sheds light on the performance that may be achieved using parallel hashing algorithms and should lend credibility to their use.", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Submicron systems architecture project\n", "abstract": " The central theme of this research is the architecture and design of VLSI systems appropriate to a microcircuit technology scaled to submicron feature sizes. Our work is focused on VLSI architecture experiments that involve the design, construction, programming, and use of experimental message-passing concurrent computers, and includes related efforts in concurrent computation and VLSI design.Descriptors:", "num_citations": "2\n", "authors": ["1779"]}
{"title": "Binary decision diagrams: an algorithmic basis for symbolic model checking\n", "abstract": " Binary decision diagrams provide a data structure for representing and manipulating Boolean functions in symbolic form. They have been especially effective as the algorithmic basis for symbolic model checkers. A binary decision diagram represents a Boolean function as a directed acyclic graph, corresponding to a compressed form of decision tree. Most commonly, an ordering constraint is imposed among the occurrences of decision variables in the graph, yielding ordered binary decision diagrams (OBDD). Representing all functions as OBDDs with a common variable ordering has the advantages that (1) there is a unique, reduced representation of any function,(2) there is a simple algorithm to reduce any OBDD to the unique form for that function, and (3) there is an associated set of algorithms to implement a wide variety of operations on Boolean functions represented as OB-DDs. Recent work in this area has focused on generalizations to represent larger classes of functions, as well on scaling implementations to handle larger and more complex problems.", "num_citations": "1\n", "authors": ["1779"]}
{"title": "EduPar 2016 Keynote\n", "abstract": " Summary form only given. At Carnegie Mellon University, two core courses in the computer science curriculum are key to preparing students to master parallel and distributed computing: \u00b7 Introduction to Computer Systems emphasizes how the combination of hardware and software work together to support program execution and network communication, as viewed from a programmer's perspective. Among other topics, this course introduces students to concurrency synchronization, and thread-based programming. This course is the inspiration for the textbook \"Computer Systems: A Programmer's Perspective,\" now in its third edition and used by over 290 schools worldwide. \u00b7 Parallel and Sequential Data Structures and Algorithms serves as our introductory algorithms course. It is based on a parallel model of computation, where programs execute on a maximum of \u0131 processors. Sequential algorithms are covered as\u00a0\u2026", "num_citations": "1\n", "authors": ["1779"]}
{"title": "The National Strategic Computing Initiative\n", "abstract": " US President Obama signed an Executive Order creating the National Strategic Computing Initiative (NSCI) on July 31, 2015. In the order, he directed agencies to establish and execute a coordinated Federal strategy in high-performance computing (HPC) research, development, and deployment. The NSCI is a whole-of-government effort to be executed in collaboration with industry and academia, to maximize the benefits of HPC for the United States. The Federal Government is moving forward aggressively to realize that vision. This presentation will describe the NSCI, its current status, and some of its implications for HPC in the US for the coming decade.", "num_citations": "1\n", "authors": ["1779"]}
{"title": "Decision procedures customized for formal verification\n", "abstract": " The uclid verifier models a hardware or software system as an abstract state machine, where the state variables can be Boolean or integer values, or functions mapping integers to integers or Booleans. The core of the verifier consists of a decision procedure that checks the validity of formulas over the combined theories of uninterpreted functions with equality and linear integer arithmetic. It operates by transforming a formula into an equisatisfiable Boolean formula and then invoking a SAT solver. This approach has worked well for the class of logic and the types of formulas encountered in verification.", "num_citations": "1\n", "authors": ["1779"]}
{"title": "On solving Boolean combinations of generalized 2SAT constraints\n", "abstract": " We consider the satisfiability problem for Boolean combinations of generalized 2 SAT constraints, which are linear constraints with at most two, possibly unbounded, integer variables having coefficients in-1, 1.Descriptors:", "num_citations": "1\n", "authors": ["1779"]}
{"title": "Issues and debates in contemporary social and critical philosophy\n", "abstract": " John Rundell, Danielle Petherbridge, Jan Bryant, John Hewitt & Jeremy Smith, Issues and debates in contemporary social and critical philosophy - PhilPapers Sign in | Create an account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home Syntax Advanced Search Syntax Advanced Search Syntax Advanced Search Issues and debates in contemporary social and critical philosophy John Rundell, Danielle Petherbridge, Jan Bryant, John Hewitt & Jeremy Smith Authors Danielle Petherbridge University College Dublin John Rundell University of Melbourne Abstract This article has no associated abstract. (fix it) Keywords No keywords specified (fix it) Categories Philosophy of Social Science, General Works in Philosophy of Social Science (categorize this paper) Options Edit this record Mark as duplicate Export citation Find it on Scholar Request removal from index Revision history Download options \u2026", "num_citations": "1\n", "authors": ["1779"]}
{"title": "System modeling and verification with uclid\n", "abstract": " Formal verification has had a significant impact on the semiconductor industry, particularly for companies that can devote significant resources to creating and deploying internally developed verification tools. Most existing verifiers model system operation at a detailed bit level. We have developed UCLID, a prototype verifier for infinite-state systems. The UCLID modeling language extends that of SMV, a bit-level model checker, to include integer and function state variables, addition by constants, and relational operations. The underlying logic is expressive enough to model a wide range of systems, but it still permits a decision procedure where we transform the formula into propositional logic and then use either binary decision diagrams (BDD) or a Boolean satisfiability (SAT) solver.", "num_citations": "1\n", "authors": ["1779"]}
{"title": "Contingency, Fragility, Difference\n", "abstract": " It can be argued that, among other things, modernity is constituted by conditions of selfreflexivity. This is irrespective of the form that this reflexivity takes, and whether it requires an Archimedean point. In this issue of Critical Horizons, in his \u201cThe Debate about Truth: Pragmaticism without Regulative Ideas,\u201d Albrecht Wellmer addresses the attempt by Habermas, Apel and others to preserve the \u2018internal\u2019or \u2018conceptual\u2019connection between truth and justification, and by so doing provide such a fixed point of orientation. 1 Starting out from difficulties with the correspondence theory of truth, Wellmer argues that a purely epistemic notion of truth denies the indisputable difference between truth and justification. The claim to the \u2018timelessness\u2019 of truth cannot even be captured if truth is equated with justification under ideal conditions. However, as Wellmer, and many others have pointed out, truth is and will always be \u2018disputable\u00a0\u2026", "num_citations": "1\n", "authors": ["1779"]}
{"title": "The Hardness of Approximating Minima in OBDDs, FBDDs and Boolean Functions\n", "abstract": " This paper presents approximation hardness results for three equivalent problems in Boolean function complexity. Consider a Boolean function f on n variables. The first problem is to minimize the level i in the Ordered Binary Decision Diagram OBDD for f at which the number of nodes is less than 2i-1. We show that this problem is not approximable to within the factor 2exp log1-En, for any E 0, unless NP is contained in RQP, the class of all problems solvable in random quasi-polynomial time. This minimization problem is shown to be equivalent to the problem of finding the minimum size subset S of variables so that f has two equivalent cofactors with respect to the variables in S. Both problems are proved equivalent to the analogous problem for Free BDDs and hence the approximation hardness result holds for all three.Descriptors:", "num_citations": "1\n", "authors": ["1779"]}
{"title": "ELECT! W%\n", "abstract": " Arithmetic circuits have received relatively little attention from the verification community, except by those using methods based on theorem proving, eg,[12]. This inattention is due to two main reasons. First, many perceive that arithmetic circuit design is fairly straightforward\u2014the same implementation techniques have been used for years, and designers are confident of their ability to detect errors using conventional simulation. Intel's recent experience with its Pentium floating point divider [11] has exposed the error in this thinking. There are many places one can make mistakes in designing these circuits, some of which may be very hard to detect with the limited number of cases that can be tested by simulation. Second, these circuits are particularly troublesome for methods based on ordered Binary Decision Diagrams (BDDs), the most popular alternative to theorem proving [4]. The BDDs representing the outputs of a multiplier grow exponentially with the word size [3], making them impractical for word sizes much beyond 16 bits. Other arithmetic functions, such as division, also seem to be intractable using BDDs, although this has not been proved formally.In this paper, we demonstrate that BDD-based verification can be usefully applied to complex arithmetic circuits. Even though it is not feasible to verify the overall circuit functionality, just verifying one iteration can uncover many possible design errors. We demonstrate this by showing the desired behavior for one iteration of radix-4 SRT division [1], as used in the Pentium divider, can be specified and verified using BDDs. This verification will detect incorrect table entries such as occurred in the\u00a0\u2026", "num_citations": "1\n", "authors": ["1779"]}
{"title": "Digital frequency control of satellite frequency standards\n", "abstract": " In the Frequency and Time Standard Development Program of the TIMATION System, a new miniaturized rubidium vapor frequency standard has been tested and analyzed for possible use on the TIMATION 3A launch, as part of the Defense Navigation Satellite Development Program. The design and construction of a digital frequency control was required to remotely control this rubidium vapor frequency standard as well as the quartz oscillator in current use. This control must be capable of accepting commands from a satellite telemetry system, verify that the correct commands have been sent and control the frequency to the requirements of the system. Several modifications must be performed to the rubidium vapor frequency standard to allow it to be compatible with the digital frequency control. These include the addition of a varactor to voltage tune the coarse range of the flywheel oscillator, and a modification to supply the C field current externally. The digital frequency control for the rubidium vapor frequency standard has been successfully tested in prototype form.", "num_citations": "1\n", "authors": ["1779"]}