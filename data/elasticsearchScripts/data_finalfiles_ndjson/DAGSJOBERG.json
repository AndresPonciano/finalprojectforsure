{"title": "A survey of controlled experiments in software engineering\n", "abstract": " The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed\u00a0\u2026", "num_citations": "841\n", "authors": ["163"]}
{"title": "Variations in middle cerebral artery blood flow investigated with noninvasive transcranial blood velocity measurements\n", "abstract": " Observations on blood velocity in the middle cerebral artery using transcranial Doppler ultrasound and on the ipsilateral internal carotid artery flow volume were obtained during periods of transient, rapid blood flow variations in 7 patients. Five patients were investigated after carotid endarterectomy. A further 2 patients having staged carotid endarterectomy and open heart surgery were investigated during nonpulsatile cardiopulmonary bypass. The patient selection permitted the assumption that middle cerebral artery flow remained proportional to internal carotid artery flow. The integrated time-mean values from consecutive 5-second periods were computed. The arithmetic mean internal carotid artery flow varied from 167 to 399 ml/min in individual patients, with individual ranges between +/- 15% and +/- 35% of the mean flow. The mean middle cerebral artery blood velocity varied from 32 to 78 cm/sec. The relation\u00a0\u2026", "num_citations": "372\n", "authors": ["163"]}
{"title": "Are all code smells harmful? a study of god classes and brain classes in the evolution of three open source systems\n", "abstract": " Code smells are particular patterns in object-oriented systems that are perceived to lead to difficulties in the maintenance of such systems. It is held that to improve maintainability, code smells should be eliminated by refactoring. It is claimed that classes that are involved in certain code smells are liable to be changed more frequently and have more defects than other classes in the code. We investigated the extent to which this claim is true for God Classes and Brain Classes, with and without normalizing the effects with respect to the class size. We analyzed historical data from 7 to 10 years of the development of three open-source software systems. The results show that God and Brain Classes were changed more frequently and contained more defects than other kinds of class. However, when we normalized the measured effects with respect to size, then God and Brain Classes were less subject to change and had\u00a0\u2026", "num_citations": "222\n", "authors": ["163"]}
{"title": "Quantifying schema evolution\n", "abstract": " Achieving correct changes is the dominant activity in the application software industry. Modification of database schemata is one kind of change which may have severe consequences for database applications. The paper presents a method for measuring modifications to database schemata and their consequences by using a thesaurus tool. Measurements of the evolution of a large-scale database application currently running in several hospitals in the UK are presented and interpreted. The kind of measurements provided by this in-depth study is useful input to the design of change management tools.", "num_citations": "208\n", "authors": ["163"]}
{"title": "Effects of personality on pair programming\n", "abstract": " Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting the job performance of software professionals both individually and in teams. However, research suggests that other human-related factors such as motivation, general mental ability, expertise, and task complexity also affect the performance in general. This paper reports on a study of the impact of the Big Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals in three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part. The results show that: (1) Our data do not confirm a meta-analysis-based model of the impact of certain personality traits on performance and (2) personality traits, in general\u00a0\u2026", "num_citations": "187\n", "authors": ["163"]}
{"title": "Evaluating the effect of a delegated versus centralized control style on the maintainability of object-oriented software\n", "abstract": " A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a \"bad\" object-oriented design. We present a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate, and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. The results show that the most skilled\u00a0\u2026", "num_citations": "177\n", "authors": ["163"]}
{"title": "Quantifying the effect of using kanban versus scrum: A case study\n", "abstract": " Proponents of various processes and methods in the agile and lean communities have made many bold claims about usefulness, but those claims are rarely supported by empirical investigations. Data gathered from more than 12,000 work items collected over two years sheds light on Kanban versus Scrum.To acquire more knowledge about the performance of different agile or lean methods, scholars should conduct similar studies in different organizations in different application domains and with people of different cultures and competences.", "num_citations": "112\n", "authors": ["163"]}
{"title": "Towards an inspection technique for use case models\n", "abstract": " A use case model describes the functional requirements of a software system and is used as input to several activities in a software development project. The quality of the use case model therefore has an important impact on the quality of the resulting software product. Software inspection is regarded as one of the most efficient methods for verifying software documents. There are inspection techniques for most documents produced in a software development project, but no comprehensive inspection technique exists for use case models. This paper presents a taxonomy of typical defects in use case models and proposes a checklist-based inspection technique for detecting such defects. This inspection technique was evaluated in two studies with undergraduate students as subjects. The results from the evaluations indicate that inspections are useful for detecting defects in use case models and motivate further\u00a0\u2026", "num_citations": "111\n", "authors": ["163"]}
{"title": "Variability and reproducibility in software engineering: A study of four companies that developed the same system\n", "abstract": " The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, ldquomedium,rdquo\u00a0\u2026", "num_citations": "93\n", "authors": ["163"]}
{"title": "A web-based support environment for software engineering experiments\n", "abstract": " The software engineering communities frequently propose new software engineering technologies, such as new development techniques, programming languages and tools, without rigorous scientific evaluation. One way to evaluate software engineering technologies is through controlled experiments where the effects of the technology can be isolated from confounding factors, ie, establishing cause-effect relationships. For practical and financial reasons, however, such experiments are often quite unrealistic, typically involving students in a class-room environment solving small pen-and-paper tasks. A common criticism of the results of the experiments is their lack of external validity, ie, that the results are not valid outside the experimental conditions. To increase the external validity of the experimental results, the experiments need to be more realistic. The realism can be increased using professional developers as subjects who conduct larger experimental tasks in their normal work environment. However, the logistics involved in running such experiments are tremendous. More specifically, the experimental materials (eg, questionnaires, task descriptions, code and tools) must be distributed to each programmer, the progress of the experiment needs to be controlled and monitored, and the results of the experiment need to be collected and analyzed. To support this logistics for large-scale, controlled experiments, we have developed a web-based experiment support environment called SESE. This paper describes SESE, its development and the experiences from using it to conduct a large controlled experiment in industry.", "num_citations": "73\n", "authors": ["163"]}
{"title": "Questioning software maintenance metrics: a comparative case study\n", "abstract": " Context: Many metrics are used in software engineering research as surrogates for maintainability of software systems. Aim: Our aim was to investigate whether such metrics are consistent among themselves and the extent to which they predict maintenance effort at the entire system level. Method: The Maintainability Index, a set of structural measures, two code smells (Feature Envy and God Class) and size were applied to a set of four functionally equivalent systems. The metrics were compared with each other and with the outcome of a study in which six developers were hired to perform three maintenance tasks on the same systems. Results: The metrics were not mutually consistent. Only system size and low cohesion were strongly associated with increased maintenance effort. Conclusion: Apart from size, surrogate maintainability measures may not reflect future maintenance effort. Surrogates need to be\u00a0\u2026", "num_citations": "72\n", "authors": ["163"]}
{"title": "A study of development and maintenance in Norway: Assessing the efficiency of information systems support using functional maintenance\n", "abstract": " The large amount of work on information systems being taken up by maintenance activities has been one of the arguments of those speaking about a \u2018software crisis\u2019. We have investigated the applicability of this statement, and propose instead to look at the percentage of work being done on functional maintenance to assess the efficiency of the information systems support in an organisation. This paper presents the main results from a survey investigation performed in Norwegian organisations within the area of software development and maintenance. The results are based on responses from 53 Norwegian organisations. The investigation is compared with other investigations, both those performed in Norway where a similar investigation was conducted in 1994 and investigations performed in other countries. Similar to the investigation from 1994, the situation is better when looking at the situation from a\u00a0\u2026", "num_citations": "47\n", "authors": ["163"]}
{"title": "Investigating the role of use cases in the construction of class diagrams\n", "abstract": " Several approaches have been proposed for the transition from functional requirements to object-oriented design. In a use case-driven development process, the use cases are important input for the identification of classes and their methods. There is, however, no established, empirically validated technique for the transition from use cases to class diagrams. One recommended technique is to derive classes by analyzing the use cases. It has, nevertheless, been reported that this technique leads to problems, such as the developers missing requirements and mistaking requirements for design. An alternative technique is to identify classes from a textual requirements specification and subsequently apply the use case model to validate the resulting class diagram. This paper describes two controlled experiments conducted to investigate these two approaches to applying use case models in an object\u00a0\u2026", "num_citations": "43\n", "authors": ["163"]}
{"title": "Towards a framework for empirical assessment of changeability decay\n", "abstract": " Evolutionary development allows early and frequent adaptations to new or changed requirements. However, such unanticipated changes may invalidate design documentation and cause structural degradations of the software, which in turn may accelerate changeability decay. Our definition of changeability decay focuses on the increased effort required to implement changes. We have identified three approaches to the assessment of changeability decay: (1) structure measurement, (2) change complexity measurement, and (3) benchmarking. Our research aims to evaluate and compare these approaches in order to develop an empirical assessment framework. In this paper we propose a set of change complexity measures (2) and compare them with structural attribute measures (1) using detailed process and product data collected from a commercial object-oriented development project. The preliminary results\u00a0\u2026", "num_citations": "40\n", "authors": ["163"]}
{"title": "A longitudinal study of development and maintenance in Norway: Report from the 2003 investigation\n", "abstract": " The amount of work on application systems being taken up by maintenance activities (work done on an IT-system after being put in production) has been one of the arguments of those speaking about a \u2018software crisis\u2019. We have earlier investigated the applicability of this notion, and propose to rather look at the percentage of work being done on application portfolio upkeep (work made to keep up the functional coverage of the application system portfolio of the organization. This also includes the development of replacement systems), to assess the efficiency of the application systems support in an organisation. This paper presents the main results of a survey investigation performed in 2003 in 54 Norwegian organisations within this area. The amount of application portfolio upkeep is significantly higher than in a similar investigation conducted in 1993. The level of maintenance is smaller (although not significantly\u00a0\u2026", "num_citations": "29\n", "authors": ["163"]}
{"title": "Confronting the myth of rapid obsolescence in computing research\n", "abstract": " Computing research ages more slowly than research in other scientific disciplines, supporting the call for parity in funding.", "num_citations": "24\n", "authors": ["163"]}
{"title": "Thesaurus-based methodologies and tools for maintaining persistent application systems\n", "abstract": " The research presented in this thesis establishes thesauri as a viable foundation for models, methodologies and tools for change management. Most of the research has been undertaken in a persistent programming environment. Persistent language technology has enabled the construction of sophisticated and well-integrated change management tools; tools and applications reside in the same store. At the same time, the research has enhanced persistent programming environments with models, methodologies and tools that are crucial to the exploitation of persistent programming in construction and maintenance of long-lived, data-intensive application systems.", "num_citations": "24\n", "authors": ["163"]}
{"title": "The usability inspection performance of work-domain experts: An empirical study\n", "abstract": " It is a challenge for usability experts to perform usability inspections of interactive systems that are tailored to work-domains of which these experts have little knowledge. To counter this, usability inspections with work-domain experts have been explored, but little empirical research has been reported on these experts\u2019 performance as evaluators. The present study compared the performance of work-domain experts and usability experts with respect to validity and thoroughness. The work-domain experts were characterized by high computer experience and low system experience. The usability experts were recruited from different ICT companies. The usability inspection method applied was group-based expert walkthrough; a method particularly developed to support non-usability experts as evaluators. The criterion for performance comparison was established through user tests. Fifteen work-domain experts and 12\u00a0\u2026", "num_citations": "23\n", "authors": ["163"]}
{"title": "SESE an experiment support environment for evaluating software engineering technologies\n", "abstract": " The software engineering communities frequently propose new software engineering technologies, such as new development techniques, programming languages and tools, without rigorous scientific evaluation. One way to evaluate software engineering technologies is through controlled experiments where the effects of the technology can be isolated from confounding factors, ie, establishing cause-effect relationships. For practical and financial reasons, however, such experiments are often quite unrealistic, typically involving students in a class-room environment solving small penand-paper tasks. A common criticism of the results of the experiments is their lack of external validity, ie, that the results are not valid outside the experimental conditions. To increase the external validity of the experimental results, the experiments need to be more realistic. The realism can be increased using professional developers as subjects who conduct larger experimental tasks in their normal work environment. However, the logistics involved in running such experiments are tremendous. More specifically, the experimental materials (eg, questionnaires, task descriptions, code and tools) must be distributed to each programmer, the progress of the experiment needs to be controlled and monitored, and the results of the experiment need to be collected and analyzed. To support this logistics for large-scale, controlled experiments, we have developed a web-based experiment support environment called SESE. This paper describes SESE, its development and the experiences from using it to conduct a large controlled experiment in industry.", "num_citations": "21\n", "authors": ["163"]}
{"title": "Visualizing impacts of database schema changes-a controlled experiment\n", "abstract": " Research in schema evolution has been driven by the need for more effective software development and maintenance. Finding impacts of schema changes on the applications and presenting them in an appropriate way are particularly challenging. We have developed a tool that finds impacts of schema changes on applications in object-oriented systems. This tool displays components (packages, classes, interfaces, methods and fields) of a database application system as a graph. Components potentially affected by a change are indicated by changing the shape of the boxes representing those components. Two versions of the tool are available. One version identifies affected parts of applications at the granularity of packages, classes, and interfaces, whereas the other version identifies affected parts at the finer granularity of fields and methods. This paper presents the design and results of a controlled student\u00a0\u2026", "num_citations": "19\n", "authors": ["163"]}
{"title": "Marital break-up among couples raising families by artificial insemination by donor\n", "abstract": " The separation rate for Norwegian women who had given birth to children after artificial insemination by donor (AID, n\u00a0=\u00a0227) was compared with the separation rate among all Norwegian women that met certain demographic criteria for equality (controls, n\u00a0=\u00a06, 689). The criteria included identical year of maternal birth, similar age at marriage, and equally long period between marriage and birth of the first child. Mean duration of marriage at first delivery was 7.4 years, and mean age of the first child was 3.6 years at registration of the data. The incidence of marital break-up in the AID group was 1.8%, as compared with 3.0% to 3.7% in the control group. The difference is not significant (P\u00a0~\u00a00.15). The authors conclude that the separation rate in AID families does not differ significantly from the observed separation rate in a demographically matched population. The study highlights the importance of selecting a proper\u00a0\u2026", "num_citations": "17\n", "authors": ["163"]}
{"title": "Analysing persistent language applications\n", "abstract": " Most research into persistent programming has been directed towards the design and implementation of languages and object stores. There are few reports on the characteristics of systems exploiting such technology. This paper reports on a study of the source code of 20 applications consisting of more than 108,000 lines of persistent language code. The authors of the applications range from students to experienced programmers. The programs have been categorised and examined with respect to a persistent application model and the extent of inconsistencies relative to this model is presented. The results confirm the need for and give input to the design of programming methodologies and tools for persistent software engineering. Measurements also include the use of names, types, (polymorphic) procedures and persistent bindings. It is hoped that analysis of the measurements will be used as input to\u00a0\u2026", "num_citations": "15\n", "authors": ["163"]}
{"title": "Evaluating methods and technologies in software engineering with respect to developers' skill level\n", "abstract": " Background: It is trivial that the usefulness of a technology depends on the skill of the user. Several studies have reported an interaction between skill levels and different technologies, but the effect of skill is, for the most part, ignored in empirical, human-centric studies in software engineering. Aim: This paper investigates the usefulness of a technology as a function of skill. Method: An experiment that used students as subjects found recursive implementations to be easier to debug correctly than iterative implementations. We replicated the experiment by hiring 65 professional developers from nine companies in eight countries. In addition to the debugging tasks, performance on 17 other programming tasks was collected and analyzed using a measurement model that expressed the effect of treatment as a function of skill. Results: The hypotheses of the original study were confirmed only for the low-skilled subjects in\u00a0\u2026", "num_citations": "13\n", "authors": ["163"]}
{"title": "Applying use cases to design versus validate class diagrams-a controlled experiment using a professional modeling tool\n", "abstract": " Several processes have been proposed for the transition from functional requirements to an object-oriented design, but these processes have been subject to little empirical validation. A use case driven development process is often recommended when applying UML. Nevertheless, it has been reported that this process leads to problems, such as the developers missing some requirements and mistaking requirements for design. This paper describes a controlled experiment, width 53 students as subjects, conducted to investigate two alternative processes for applying a use case model in an object-oriented design process. One process was use case driven, while the other was a responsibility-driven process in which the use case model was applied as a means of validating the resulting class diagram. Half of the subjects used the modeling tool Tau UML Suite from Telelogic; the other half used pen and paper. The\u00a0\u2026", "num_citations": "13\n", "authors": ["163"]}
{"title": "Experiences from replicating a case study to investigate reproducibility of software development\n", "abstract": " Frequent changes in technology and continuously increasing demands from software customers means that software development is largely a creative process. Nevertheless, a fundamental question for software engineering in general, and for empirical software engineering in particular, is the reproducibility of software development: which aspects of a software development process and product are reproducible and which parts depend on the creativity of developers? A priori, it is not obvious that such separation into creative and reproducible aspects can be made, but if that were possible, we could focus software process, technologies, and empirical studies on the reproducible parts without restricting the creativity of developers. Therefore, this question is worthy of extensive investigation.The precise definition of reproducibility varies across disciplines, but it is often closely related to the definitions of repeatability. In an encyclopaedia of philosophy of science [Tetens 2004], reproducibility is described as the repeatability of the process of establishing a fact, or of the conditions under which the same fact can be observed. In natural science, reproducibility is often related closely to the repeatability of experimental conditions and results. See Anda et al.[2009] for a more comprehensive discussion of the concept of reproducibility in software engineering.", "num_citations": "11\n", "authors": ["163"]}
{"title": "How to recruit professionals as subjects in software engineering experiments\n", "abstract": " Controlled experiments are the classical scientific method for identifying cause-effect relationships, and are complementary to case studies and surveys as a means to empirically evaluate information systems development methods and practices. Most controlled experiments that evaluate software development methods and practices use students as subjects. Using students as subjects is convenient. However, a common criticism of these experiments is that it is difficult to generalize the results to industrial settings. Consequently, Simula Research Laboratory wanted to include professionals as subjects in the experiments. At present, more than 750 professional software developers from 46 software development organizations have participated in our different experiments. From this experience we have identified three important principles for research groups that want to include professional software developers as subjects in controlled experiments. First, practical constraints must be considered when defining the target population of software developers. Second, the participating organizations must be offered flexibility and value using a planned communication strategy, in order to ensure adequately sized representative samples of organizations and individuals. Third, to ensure long-term, relationships with the organizations, high professional and ethical standards must be employed. These findings complement existing knowledge on how to conduct experiments in our field.", "num_citations": "10\n", "authors": ["163"]}
{"title": "Managing change in persistent object systems\n", "abstract": " Persistent object systems are highly-valued technology because they offer an effective foundation for building very long-lived persistent application systems (PAS). The technology becomes more effective as it offers a more consistently integrated computational context.             For it to be feasible to design and construct a PAS it must be possible to incrementally add program and data to the existing collection. For a PAS to endure it must offer flexibility: a capacity to evolve and change. This paper examines the capacity of persistent object systems to accommodate incremental construction and change.             Established store based technologies can support incremental construction but methodologies are needed to deploy them effectively. Evolving data description is one motivation for inheritance but inheritance alone is not enough to support change management.             The case for supporting incremental\u00a0\u2026", "num_citations": "10\n", "authors": ["163"]}
{"title": "Exploiting persistence in build management\n", "abstract": " A challenging issue in the construction and maintenance of large application systems is how to determine which components need to be rebuilt after change, when and in which order. Rebuilding is typically recompilation and linking, but may also include update of derivable components such as cross\u2010reference databases and re\u2010creation of library indexes. Type definitions or schema, and data values in a file store, database or persistent store may also need to be rebuilt. The main purpose of this paper is to describe how persistent language technology can be exploited to enhance build management. In particular, the paper describes a method for transactional, incremental linking and the implementation of its support. To help implement this method, and to make it safer and more efficient to carry out rebuild activities in general, we have defined a set of automatically checkable constraints on the software. The build\u00a0\u2026", "num_citations": "9\n", "authors": ["163"]}
{"title": "An evaluation of applying use cases to construct design versus validate design\n", "abstract": " Use case models capture and describe the functional requirements of a software system. A use case driven development process, where a use case model is the principal basis for constructing an object-oriented design, is recommended when applying UML. There are, however, some problems with use case driven development processes and alternative ways of applying a use case model have been proposed. One alternative is to apply the use case model in a responsibility-driven process as a means to validate the design model. We wish to study how a use case model best can be applied in an object-oriented development process and have conducted a pilot experiment with 26 students as subjects to compare a use case driven process against a responsibility-driven process in which a use case model is applied to validate the design model. Each subject was given detailed guidelines on one of the two\u00a0\u2026", "num_citations": "8\n", "authors": ["163"]}
{"title": "Managing Change in Information Systems: Technological Challenges\n", "abstract": " Information systems and other computer-based systems must continuously undergo change in order to reflect change in their environments. The present technology used to implement such systems, including models, methods, tools and languages, does not have an inherent understanding of the nature of evolution. The rigidity of existing systems is a hindrance for user requested enhancements.Propagating changes correctly is a particular problem. It is common to find that necessary changes consequent on some other change have not been made, so that the system is inconsistent and will eventually fail to operate correctly. The paper discusses tools for system maintenance and focuses on the issue of automation. A tool that automatically generates and maintains all the information it needs is presented. To provide more information about the form and extent of the evolution in real-world systems, the same tool was instructed to collect change measurements. Information about the evolution of a large health management system was recorded over a period of 18 months. Methods for and problems of automatic change measurements collection are discussed.", "num_citations": "8\n", "authors": ["163"]}
{"title": "Building an integrated persistent application\n", "abstract": " The major motivation for database programming language (DBPL) research is to facilitate the construction and maintenance of large dataintensive applications. To fully benefit from DBPLs, supporting methodologies and tools are needed. This paper describes the construction of a multi-author, multi-level thesaurus application (TA). Some tools and methodologies were used in the TA construction, and requirements for other tools and methodologies are identified as the result of our experiences. Although built in a specific language (Napier88), the principles discovered apply to other DBPLs.               The TA comprises several loosely-integrated components constructed by different programmers. The components were themselves implemented using general purpose sub-components, including libraries. A principle result is that a realistic application could be constructed quickly in a DBPL. Rapid construction\u00a0\u2026", "num_citations": "8\n", "authors": ["163"]}
{"title": "Database concepts discussed in an object oriented perspective\n", "abstract": " The terminology in the area of databases and data models is inconsistent and inaccurate and thus often confusing. Some fundamental database concepts are described in this paper. The description of these concepts are based on general concepts related to the development of the object oriented languages SIMULA, DELTA and BETA. A database is defined as an information system providing information about a referent system (the modelled part of the world), and a data model is defined as \u201chaving an inherent structure and a set of tools and techniques used in the process of designing, constructing and manipulating model systems (in particular, databases)\u201d. The connection between databases, DBMS and data models and other concepts related to the development and use of databases are described in a process/structure hierarchy.", "num_citations": "7\n", "authors": ["163"]}
{"title": "Knowledge acquisition in software engineering requires sharing of data and artifacts\n", "abstract": " An important goal of empirical software engineering research is to cumulatively build up knowledge on the basis of our empirical studies, for example, in the form of theories and models (conceptual frameworks). Building useful bodies of knowledge will in general require the combined effort by several research groups over time. To achieve this goal, data, testbeds and artifacts should be shared in the community in an efficient way. There are basically two challenges: (1) How do we encourage researchers to use material provided by others? (2) How do we encourage researchers to make material available to others in an appropriate form? Making material accessible to others may require substantial effort by the creator. How should he or she benefit from such an effort, and how should the likelihood of misuse be reduced to a minimum? At the least, the requester should officially request permission to use the\u00a0\u2026", "num_citations": "6\n", "authors": ["163"]}
{"title": "Thesaurus-based software environments\n", "abstract": " Software environments support the process of constructing and maintaining application systems. This paper describes the idea of a thesaurus1 as a viable foundation for software environments. A thesaurus contains information about the names and identifiers in all the software written in all the languages of an application. Information about extensional data in a database or persistent store is also included. The comprehensiveness of the thesaurus is in contrast to most commercially available tools which focus either on the source code only (source code analysers) or on database-specific information (data dictionaries). A few data dictionary tools also include source code information, but relationships between names and identifiers in the software written in the various languages are not recorded automatically. All the contents of the thesaurus are automatically maintained. The whole application system is analysed, and the thesaurus updated, regularly at times specified by the user, for example daily at 02: 00. A full analysis and update can also be initiated at any time.Two thesaurus tools have been built. The HMS thesaurus tool was developed for a health management system (HMS) in an industrial (C, C++, X Window System and relational database) environment [13]. Another thesaurus tool was thereafter built in the context of the strongly typed, persistent programming language Napier88 [12]. The software environments that have been built around the thesauri focus on change management and include tools that display structures and dependencies and provide impact analysis. In the persistent case, automatic build management is supported\u00a0\u2026", "num_citations": "6\n", "authors": ["163"]}
{"title": "Software constraints for large application systems\n", "abstract": " As application systems live longer and grow in size and complexity, there is an ever increasing need for methods and tools that can support software builders in constructing maintainable, well-structured and consistent systems. This paper describes the notion of software constraints as an aid to developing such systems. Software constraints make rules and conventions commonly agreed to in a given programming environment explicit and automatically checkable. The potential usefulness of software constraints was investigated in both industrial and research environments. A framework for categorization of such constraints is defined. Constraints are proposed that are generally applicable and others that are tightly connected to and support a certain programming method. Tools for automatic checking are crucial if software constraints are to be used. An architecture for such tools and two realizations are\u00a0\u2026", "num_citations": "5\n", "authors": ["163"]}
{"title": "Documenting Theories Working Group Results\n", "abstract": " Attendees: Marcus Ciolkowski, Tore Dyb\u00e5, Frank Houdek, Andreas Jedlitschka, Barbara Kitchenham, Dietmar Pfahl, Dag Sj\u00f8berg (chair), Sira Vegas and Laurie Williams               There are many arguments in favour of theory use, such as structuring, conciseness, precision, parsimony, abstraction, generalisation, conceptualisation and communication [1,9,11]. Such arguments have been voiced in the software engineering community as well [2,3,8,10]. Theory provides explanations and understanding in terms of basic concepts and underlying mechanisms, which constitute an important counterpart to knowledge of passing trends and their manifestations. When developing better software engineering technology for long-lived industrial needs, building theory is a means to go beyond the mere observation of phenomena, and to try to understand why and how these phenomena occur.", "num_citations": "4\n", "authors": ["163"]}
{"title": "Empirical assessment of changeability decay in object-oriented software\n", "abstract": " Genera AS is a vendor of a CASE tool called Genova. The work described in this paper aims to define and evaluate an evolutionary development process (the Genova process) to complement and support the use of the Genova tool. As a starting point, we used a lightweight version of the Rational Unified Process in a development project for one of our customers. This process was instrumented to enable process improvement activities. An essential question is what to improve the defined or the actual process? Based on quantitative and qualitative data, we identified improvements related to the distribution of test effort throughout the life cycle. Furthermore, we gained useful experience on the management of evolutionary development projects. Our case study also provided insight for instrumentation of the process for collecting data related to the cost of changes.", "num_citations": "4\n", "authors": ["163"]}
{"title": "Evaluating usability aspects of pjama based on source code measurements\n", "abstract": " PJama is a system that provides orthogonal persistence defined by reachability with no changes to the Java [13] language. Introduction of persistence into the Java language is expected to give software productivity gains and reduce maintenance costs. A set of hypotheses that investigate these expectations have been defined and tested. The contribution of this paper is two-fold. First, it describes a tool that automatically measures the use of persistence in Java source code given a keywordfile specific to the technology being used. Second, we have tested a set of hypotheses applying this measurement technology. The results indicate that it is possible to have persistence with minimal changes to the source code in PJama. The results also indicate that the number of lines that explicitly use persistence does not grow with the size of the application. 1 Introduction Developing code to create, update and read persistent data is a major task in application building. It is frequently mentio...", "num_citations": "4\n", "authors": ["163"]}
{"title": "The relationship between software process, context and outcome\n", "abstract": " Most practitioners and researchers agree that when developing software, process affects product, and the usefulness of a process depends on the context. However, which processes are most useful for a specific company or project is generally unknown. When studying the relation between context, process and product, one challenge is that experiments often lack realism, which makes the transfer of results to industry difficult. In contrast, most of the important factors vary beyond the researcher\u2019s control in case studies, which makes it difficult to identify cause and effect relationships. This paper reports a study where realism was combined with control over certain context and process factors. Four companies developed the same system, and the price varied by a factor of six. Certain patterns of relationships were expected (expensive company, low cost, schedule overrun); others were unexpected (cheap\u00a0\u2026", "num_citations": "3\n", "authors": ["163"]}
{"title": "The persistent workshop\u2014a programming environment for Napier88\n", "abstract": " The Persistent Workshop is a programming environment to support the construction of application systems in Napier88, a persistent programming language. In addition to supporting programming activities, the Workshop provides a demonstration of the persistence technology together with an environment in which various experiments can be conducted. The basic structure of the Workshop is outlined, together with a short summary of the tools provided within the programming environment. Finally, the evaluation of the Workshop is briefly described.", "num_citations": "3\n", "authors": ["163"]}
{"title": "An empirical study of WIP in kanban teams\n", "abstract": " Background: Limiting the amount of Work-In-Progress (WIP) is considered a fundamental principle in Kanban software development. However, no published studies from real cases exist that indicate what an optimal WIP limit should be. Aims: The primary aim is to study the effect of WIP on the performance of a Kanban team. The secondary aim is to illustrate methodological challenges when attempting to identify an optimal or appropriate WIP limit. Method: A quantitative case study was conducted in a software company that provided information about more than 8,000 work items developed over four years by five teams. Relationships between WIP, lead time and productivity were analyzed. Results: WIP correlates with lead time; that is, lower WIP indicates shorter lead times, which is consistent with claims in the literature. However, WIP also correlates with productivity, which is inconsistent with the claim in the\u00a0\u2026", "num_citations": "2\n", "authors": ["163"]}
{"title": "The challenge of assessing and controlling complexity in a large portfolio of software systems\n", "abstract": " Controlling IT complexity in a large organization's portfolio of constantly evolving software systems is crucial in a business world with tighter economic constraints and an increased focus on business agility. Unfortunately, there are no commonly accepted metrics for software complexity, and the few metrics that exist have not been validated as good predictors of the impact on cost in a given context. Telenor Norway has therefore developed its own method to help assess the cost of IT complexity from a broad perspective. For all the systems affected by a given project, the architects and experts of each system subjectively estimate the change in the annual cost of developing and maintaining it as a result of the change in complexity brought about by that project. The method was subjectively evaluated and improved in six pilot studies. Following these experiences, it was decided to gradually introduce the method\u00a0\u2026", "num_citations": "2\n", "authors": ["163"]}
{"title": "The industry is our lab\u2014Organisation and Conduct of Empirical Studies in Software Engineering at Simula\n", "abstract": " Software systems form the foundation of modern information society and many of those systems are among the most complex things ever created by man. The quotation that follows is from the 1999 President\u2019s Information Technology Advisory Committee Report, but is as valid today as it was then:               Our ability to construct\u2026needed software systems and our ability to analyze and predict the performance of the enormously complex software systems that lie at the core of our economy are painfully inadequate. We are neither training enough professionals to supply the needed software, nor adequately improving the efficiency and quality of our construction methods.", "num_citations": "2\n", "authors": ["163"]}
{"title": "Construct Validity in Software Engineering\n", "abstract": " Empirical research aims to establish generalizable claims from data. Such claims involve concepts that often must be measured indirectly by using indicators. Construct validity is concerned with whether one can justifiably make claims at the conceptual level that are supported by results at the operational level. We report a quantitative analysis of the awareness of construct validity in the software engineering literature between 2000 and 2019 and a qualitative review of 83 articles about human-centric experiments published in five high-quality journals between 2015 and 2019. Over the two decades, the appearance in the literature of the term construct validity increased sevenfold. Some of the reviewed articles we reviewed employed various ways to ensure that the indicators span the concept in an unbiased manner. We also found articles that reuse formerly validated constructs. However, the articles disagree about how to define construct validity. Several interpret construct validity excessively by including threats to internal, external, or statistical conclusion validity. A few articles also include fundamental challenges of a study, such as cheating and misunderstandings of experiment material. The diversity of topics discussed makes us recommend a minimalist approach to construct validity. We propose seven guidelines to establish a common ground for addressing construct validity in software engineering.", "num_citations": "1\n", "authors": ["163"]}
{"title": "Survey on Risk Management in OTS-based Development in IT Industries in Several Countries\n", "abstract": " This report describes an initial version of the empirical design on risk management in COTS-based development in IT industry in several countries (Norway, Germany, and Italy). The initial work of this research was prompted by recent papers eg by Morisio et al. that seem to indicate new trends in using eg COTS components. We designed several hypotheses and did a prestudy in 16 projects in Norway from Oct. 2003 to Feb. 2004 by structured interview. The results of the prestudy showed that some of the hypotheses can be tested and some of them are not easy to test. The results of the open questions in the questionnaire also showed some new research questions and theses. We discovered that all the hypotheses used in the prestudy and the new theses found from the prestudy are relevant to risk-management in COTS-based development. We therefore change the research topic to risk management issues and\u00a0\u2026", "num_citations": "1\n", "authors": ["163"]}