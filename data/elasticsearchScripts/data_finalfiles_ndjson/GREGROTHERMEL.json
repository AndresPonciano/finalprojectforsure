{"title": "Prioritizing test cases for regression testing\n", "abstract": " Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1542\n", "authors": ["52"]}
{"title": "Supporting controlled experimentation with testing techniques: An infrastructure and its potential impact\n", "abstract": " Where the creation, understanding, and assessment of software testing and regression testing techniques are concerned, controlled experimentation is an indispensable research methodology. Obtaining the infrastructure necessary to support such experimentation, however, is difficult and expensive. As a result, progress in experimentation with testing techniques has been slow, and empirical data on the costs and effectiveness of techniques remains relatively scarce. To help address this problem, we have been designing and constructing infrastructure to support controlled experimentation with testing and regression testing techniques. This paper reports on the challenges faced by researchers experimenting with testing techniques, including those that inform the design of our infrastructure. The paper then describes the infrastructure that we are creating in response to these challenges, and that we are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1297\n", "authors": ["52"]}
{"title": "Test case prioritization: A family of empirical studies\n", "abstract": " To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1149\n", "authors": ["52"]}
{"title": "Analyzing regression test selection techniques\n", "abstract": " Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely affected by changes. Regression test selection techniques reuse tests from an existing test suite to test a modified program. Many regression test selection techniques have been proposed, however, it is difficult to compare and evaluate these techniques because they have different goals. This paper outlines the issues relevant to regression test selection techniques, and uses these issues as the basis for a framework within which to evaluate the techniques. The paper illustrates the application of the framework by using it to evaluate existing regression test selection techniques. The evaluation reveals the strengths and weaknesses of existing techniques, and highlights some problems that future work in this area should address.", "num_citations": "974\n", "authors": ["52"]}
{"title": "A safe, efficient regression test selection technique\n", "abstract": " Regression testing is an expensive but necessary maintenance activity performed on modified software to provide confidence that changes are correct and do not adversely affect other portions of the softwore. A regression test selection technique choses, from an existing test set, thests that are deemed necessary to validate modified software. We present a new technique for regression test selection. Our algorithms construct control flow graphs for a precedure or program and its modified version and use these graphs to select tests that execute changed code from the original test suite. We prove that, under certain conditions, the set of tests our technique selects includes every test from the original test suite that con expose faults in the modified procedfdure or program. Under these conditions our algorithms are safe. Moreover, although our algorithms may select some tests that cannot expose faults, they are at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "930\n", "authors": ["52"]}
{"title": "Test case prioritization: An empirical study\n", "abstract": " Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.", "num_citations": "758\n", "authors": ["52"]}
{"title": "An empirical study of regression test selection techniques\n", "abstract": " Regression testing is the process of validating modified software to detect whether new errors have been introduced into previously tested code and to provide confidence that modifications are correct. Since regression testing is an expensive process, researchers have proposed regression test selection techniques as a way to reduce some of this expense. These techniques attempt to reduce costs by selecting and running only a subset of the test cases in a program's existing test suite. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been reported in the literature. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing test cases, focusing on their\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "604\n", "authors": ["52"]}
{"title": "Prioritizing test cases for regression testing\n", "abstract": " Test case prioritization techniques schedule test cases in an order that increases their effectiveness in meeting some performance goal. One performance goal, rate of fault detection, is a measure of how quickly faults are detected within the testing process; an improved rate of fault detection can provide faster feedback on the system under test, and let software engineers begin locating and correcting faults earlier than might otherwise be possible. In previous work, we reported the results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions:(1) can prioritization techniques be effective when aimed at specific modified versions;(2) what tradeoffs exist between fine granularity and coarse granularity prioritization techniques;(3) can the incorporation of measures of fault proneness into prioritization techniques\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "534\n", "authors": ["52"]}
{"title": "Incorporating varying test costs and fault severities into test case prioritization\n", "abstract": " Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "467\n", "authors": ["52"]}
{"title": "Performing data flow testing on classes\n", "abstract": " The basic unit of testing in an object-oriented program is a class. Although there has been much recent research on testing of classes, most of this work has focused on black-box approaches. However, since black-box testing techniques may not provide sufficient code coverage, they should be augmented with code-based or white-box techniques. Dataflow testing is a code-based testing technique that uses the dataflow relations in a program to guide the selection of tests. Existing dataflow testing techniques can be applied both to individual methods in a class and to methods in a class that interact through messages, but these techniques do not consider the dataflow interactions that arise when users of a class invoke sequences of methods in an arbitrary order. We present a new approach to class testing that supports dataflow testing for dataflow interactions in a class. For individual methods in a class, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "419\n", "authors": ["52"]}
{"title": "An empirical study of the effects of minimization on the fault detection capabilities of test suites\n", "abstract": " Test suite minimization techniques attempt to reduce the cost of saving and reusing tests during software maintenance, by eliminating redundant tests from test suites. A potential drawback of these techniques is that in minimizing a test suite, they might reduce the ability of that test suite to reveal faults in the software. A study showed that minimization can reduce test suite size without significantly reducing the fault detection capabilities of test suites. To further investigate this issue, we performed an experiment in which we compared the costs and benefits of minimizing test suites of various sizes for several programs. In contrast to the previous study, our results reveal that the fault detection capabilities of test suites can be severely compromised by minimization.", "num_citations": "408\n", "authors": ["52"]}
{"title": "Whole program path-based dynamic impact analysis\n", "abstract": " Impact analysis, determining when a change in one part of a program affects other parts of the program, is time-consuming and problematic. Impact analysis is rarely used to predict the effects of a change, leaving maintainers to deal with consequences rather than working to a plan. Previous approaches to impact analysis involving analysis of call graphs, and static and dynamic slicing, exhibit several tradeoffs involving computational expense, precision, and safety, require access to source code, and require a relatively large amount of effort to re-apply as software evolves. This paper presents a new technique for impact analysis based on whole path profiling, that provides a different set of cost-benefits tradeoffs - a set which can potentially be beneficial for an important class of predictive impact analysis tasks. The paper presents the results of experiments that show that the technique can predict impact sets that are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "394\n", "authors": ["52"]}
{"title": "Statistical debugging: A hypothesis testing-based approach\n", "abstract": " Manual debugging is tedious, as well as costly. The high cost has motivated the development of fault localization techniques, which help developers search for fault locations. In this paper, we propose a new statistical method, called SOBER, which automatically localizes software faults without any prior knowledge of the program semantics. Unlike existing statistical approaches that select predicates correlated with program failures, SOBER models the predicate evaluation in both correct and incorrect executions and regards a predicate as fault-relevant if its evaluation pattern in incorrect executions significantly diverges from that in correct ones. Featuring a rationale similar to that of hypothesis testing, SOBER quantifies the fault relevance of each predicate in a principled way. We systematically evaluate SOBER under the same setting as previous studies. The result clearly demonstrates the effectiveness: SOBER\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "374\n", "authors": ["52"]}
{"title": "Leveraging user-session data to support web application testing\n", "abstract": " Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "331\n", "authors": ["52"]}
{"title": "On the use of mutation faults in empirical assessments of test case prioritization techniques\n", "abstract": " Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "328\n", "authors": ["52"]}
{"title": "Selecting a cost-effective test case prioritization technique\n", "abstract": " Regression testing is an expensive testing process used to validate modified software and detect whether new faults have been introduced into previously tested code. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One goal of prioritization is to increase a test suite's rate of fault detection. Previous empirical studies have shown that several prioritization techniques can significantly improve rate of fault detection, but these studies have also shown that the effectiveness of these techniques varies considerably across various attributes of the program, test suites, and modifications being considered. This variation makes it difficult for a practitioner to choose an appropriate prioritization technique for a given testing scenario. To address this problem, we analyze the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "316\n", "authors": ["52"]}
{"title": "Empirical studies of a safe regression test selection technique\n", "abstract": " Regression testing is an expensive testing procedure utilized to validate modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting a subset of a program's existing test suite. Safe regression test selection techniques select subsets that, under certain well-defined conditions, exclude no tests (from the original test suite) that if executed would reveal faults in the modified software. Many regression test selection techniques, including several safe techniques, have been proposed, but few have been subjected to empirical validation. This paper reports empirical studies on a particular safe regression test selection technique, in which the technique is compared to the alternative regression testing strategy of running all tests. The results indicate that safe regression test selection can be cost-effective, but that its costs and benefits vary widely based on a number of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "312\n", "authors": ["52"]}
{"title": "Improving web application testing with user session data\n", "abstract": " Web applications have become critical components of the global information infrastructure, and it is important that they be validated to ensure their reliability. Therefore, many techniques and tools for validating web applications have been created. Only a few of these techniques, however, have addressed problems of testing the functionality of web applications, and those that do have not fully considered the unique attributes of web applications. In this paper we explore the notion that user session data gathered as users operate web applications can be successfully employed in the testing of those applications, particularly as those applications evolve and experience different usage profiles. We report results of an experiment comparing new and existing test generation techniques for web applications, assessing both the adequacy of the generated tests and their ability to detect faults on a point-of-sale web\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "295\n", "authors": ["52"]}
{"title": "Empirical studies of test\u0393\u00c7\u00c9suite reduction\n", "abstract": " Test\u0393\u00c7\u00c9suite reduction techniques attempt to reduce the costs of saving and reusing test cases during software maintenance by eliminating redundant test cases from test suites. A potential drawback of these techniques is that reducing the size of a test suite might reduce its ability to reveal faults in the software. Previous studies have suggested that test\u0393\u00c7\u00c9suite reduction techniques can reduce test\u0393\u00c7\u00c9suite size without significantly reducing the fault\u0393\u00c7\u00c9detection capabilities of test suites. These studies, however, involved particular programs and types of test suites, and to begin to generalize their results, further work is needed. This paper reports on the design and execution of additional studies, examining the costs and benefits of test\u0393\u00c7\u00c9suite reduction, and the factors that influence these costs and benefits. In contrast to previous studies, results of these studies reveal that the fault\u0393\u00c7\u00c9detection capabilities of test suites can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "293\n", "authors": ["52"]}
{"title": "Techniques for improving regression testing in continuous integration development environments\n", "abstract": " In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "276\n", "authors": ["52"]}
{"title": "A methodology for testing spreadsheets\n", "abstract": " Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults; thus, we would like to provide at least some of the benefits of formal testing methodologies to the creators of spreadsheets. This article presents a testing methodology that adapts data flow adequacy criteria and coverage monitoring to the task of testing spreadsheets. To accommodate the evaluation model used with spreadsheets, and the interactive process by which they are created, our methodology is incremental. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing theory. We have  implemented our testing methodology in the context of the Forms/3 visual spreadsheet language. We report  on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "259\n", "authors": ["52"]}
{"title": "Software testing: a research travelogue (2000\u0393\u00c7\u00f42014)\n", "abstract": " Despite decades of work by researchers and practitioners on numerous software quality assurance techniques, testing remains one of the most widely practiced and studied approaches for assessing and improving software quality. Our goal, in this paper, is to provide an accounting of some of the most successful research performed in software testing since the year 2000, and to present what appear to be some of the most significant challenges and opportunities in this area. To be more inclusive in this effort, and to go beyond our own personal opinions and biases, we began by contacting over 50 of our colleagues who are active in the testing research area, and asked them what they believed were (1) the most significant contributions to software testing since 2000 and (2) the greatest open challenges and opportunities for future research in this area. While our colleagues\u0393\u00c7\u00d6 input (consisting of about 30 responses\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "239\n", "authors": ["52"]}
{"title": "Regression test selection for C++ software\n", "abstract": " Regression testing is an important but expensive software maintenance activity performed with the aim of providing confidence in modified software. Regression test selection techniques reduce the cost of regression testing by selecting test cases for a modified program from a previously existing test suite. Many researchers have addressed the regression test selection problem for procedural language software, but few have addressed the problem for object\u0393\u00c7\u00c9oriented software. This paper presents a regression test selection technique for use with object\u0393\u00c7\u00c9oriented software. The technique constructs graph representations for software, and uses these graphs to select test cases, from the original test suite, that execute code that has been changed for the new version of the software. The technique is strictly code based, and requires no assumptions about the approach used to specify or test the software initially. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "234\n", "authors": ["52"]}
{"title": "Configuration-aware regression testing: an empirical study of sampling and prioritization\n", "abstract": " Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Researchers have created techniques for testing configurable software, but to date, only a little research has addressed the problems of regression testing configurable systems as they evolve. Whereas problems such as selective retesting and test prioritization at the test case level have been extensively researched, these problems have rarely been considered at the configuration level. In this paper we address the problem of providing configuration-aware regression testing for evolving software systems. We use combinatorial interaction testing techniques to model and generate configuration samples for use in regression testing. We conduct an empirical study on a non-trivial evolving software system to measure the impact of configurations on testing effectiveness, and to compare the effectiveness of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "222\n", "authors": ["52"]}
{"title": "The EUSES spreadsheet corpus: a shared resource for supporting experimentation with spreadsheet dependability mechanisms\n", "abstract": " In recent years several tools and methodologies have been developed to improve the dependability of spreadsheets. However, there has been little evaluation of these dependability devices on spreadsheets in actual use by end users. To assist in the process of evaluating these methodologies, we have assembled a corpus of spreadsheets from a variety of sources. We have ensured that these spreadsheets are suitable for evaluating dependability devices in Microsoft Excel (the most commonly used commercial spreadsheet environment) and have measured a variety of feature of these spreadsheets to aid researchers in selecting subsets of the corpus appropriate to their needs.", "num_citations": "215\n", "authors": ["52"]}
{"title": "Selecting tests and identifying test coverage requirements for modified software\n", "abstract": " Regression testing is performed on modified software to provide confidence that changed and affected portions of the code behave correctly. We present an approach to regression testing that handles two important tasks: selecting tests from the existing test suite that should be rerun, and identifying portions of the code that must be covered by tests. Both tasks are performed by traversing graphs for the program and its modified version. We first apply our technique to single procedures and then show how our technique is applied at the interprocedural level. Our approach has several advantages over previous work. First, our test select ion technique is safe, selecting every test that may produce different output in the modified program. However, our selection technique chooses smaller test sets than other safe approaches. Second, our approach is the first safe approach to identify coverage requirements, and the first\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "206\n", "authors": ["52"]}
{"title": "An empirical investigation of program spectra\n", "abstract": " A variety of expensive software maintenance and testing tasks require a comparison of the behaviors of program versions. Program spectra have recently been proposed as a heuristic for use in performing such comparisons. To assess the potential usefulness of spectra in this context, we conducted an experiment that examined the relationship between program spectra and program behavior, and empirically compared several types of spectra. This paper reports the results of that experiment.", "num_citations": "198\n", "authors": ["52"]}
{"title": "The effects of time constraints on test case prioritization: A series of controlled experiments\n", "abstract": " Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "195\n", "authors": ["52"]}
{"title": "An empirical investigation of the relationship between spectra differences and regression faults\n", "abstract": " Many software maintenance and testing tasks involve comparing the behaviours of program versions. Program spectra have recently been proposed as a heuristic for use in performing such comparisons. To assess the potential usefulness of spectra in this context an experiment was conducted, examining the relationship between differences in program spectra and the exposure of regression faults (faults existing in a modified version of a program that were not present prior to modifications, or not revealed in previous testing), and empirically comparing several types of spectra. The results reveal that certain types of spectra differences correlate with high frequency\u0393\u00c7\u00f6at least in one direction\u0393\u00c7\u00f6with the exposure of regression faults. That is, when regression faults are revealed by particular inputs, spectra differences are likely also to be revealed by those inputs, though the reverse is not true. The results also suggest\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "195\n", "authors": ["52"]}
{"title": "Using component metacontent to support the regression testing of component-based software\n", "abstract": " Component based software technologies are viewed as essential for creating the software systems of the future. However, the use of externally-provided components has serious drawbacks for a wide range of software engineering activities, often because of a lack of information about the components. Previously (A. Orso et al., 2000), we proposed the use of component metacontents: additional data and methods provided with a component, to support software engineering tasks. The authors present two new metacontent based techniques that address the problem of regression test selection for component based applications: a code based approach and a specification based approach. First, we illustrate the two techniques. Then, we present a case study that applies the code based technique to a real component based system. On the system studied, on average, 26% of the overall testing effort was saved over\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "191\n", "authors": ["52"]}
{"title": "A safe, efficient algorithm for regression test selection\n", "abstract": " Regression testing is a necessary but costly maintenance activity aimed at demonstrating that code has not been adversely affected by changes. A selective approach to regression testing selects tests for a modified program from an existing test suite. A new technique for selective regression testing is presented. The proposed algorithm constructs control dependence graphs for program versions and uses these graphs to determine which tests from the existing test suite may exhibit changed behavior on the new version. Unlike most previous techniques for selective retest, the algorithm selects every test from the original test suite that might expose errors in the modified program, and does this without prior knowledge of program modifications. The algorithm handles all language constructs and program modifications and is easily automated.< >", "num_citations": "187\n", "authors": ["52"]}
{"title": "An empirical comparison of dynamic impact analysis algorithms\n", "abstract": " Impact analysis - determining the potential effects of changes on a software system - plays an important role in software engineering tasks such as maintenance, regression testing, and debugging. In previous work, two new dynamic impact analysis techniques, CoverageImpact and PathImpact, were presented. These techniques perform impact analysis based on data gathered about program behavior relative to specific inputs, such as inputs gathered from field data, operational profile data, or test-suite executions. Due to various characteristics of the algorithms they employ, CoverageImpact and PathImpact are expected to differ in terms of cost and precision; however, there have been no studies to date examining the extent to which such differences may emerge in practice. Since cost-precision tradeoffs may play an important role in technique selection and further research, we wished to examine these tradeoffs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "184\n", "authors": ["52"]}
{"title": "Bridging the gap between the total and additional test-case prioritization strategies\n", "abstract": " In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "172\n", "authors": ["52"]}
{"title": "Cost-cognizant test case prioritization\n", "abstract": " Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. Previous work has provided a metric, AP FD, for measuring rate of fault detection, and techniques for prioritizing test cases in order to improve AP F D. This metric and these techniques, however, assume that all test case and fault costs are uniform. In practice, test case and fault costs can vary, and in such cases the previous AP FD metric and techniques designed to improve AP FD can be unsatisfactory. This paper presents a new metric for assessing the rate of fault detection of prioritized test cases, AP FDc, that incorporates varying test case and fault costs. The paper also describes adjustments to previous prioritization techniques that allow them, too, to be \u0393\u00c7\u00a3cognizant\u0393\u00c7\u00a5 of these varying costs. These techniques enable practitioners to perform a new type of prioritization: cost-cognizant test case prioritization. Finally, the results of a formative case study are presented. This study was designed to investigate the cost-cognizant metric and techniques and how they compare to their non-cost-cognizant counterparts. The study\u0393\u00c7\u00d6s results provide insights regarding the use of cost-cognizant test case prioritization in a variety of real-world settings.", "num_citations": "162\n", "authors": ["52"]}
{"title": "A comparative study of coarse-and fine-grained safe regression test-selection techniques\n", "abstract": " Regression test-selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of some of these techniques have suggested that they can indeed benefit testers, but so far, few studies have empirically compared different techniques. In this paper, we present the results of a comparative empirical study of two safe regression test-selection techniques. The techniques we studied have been implemented as the tools DejaVu and TestTube; we compared these tools in terms of a cost model incorporating precision (ability to eliminate  unnecessary test cases), analysis cost, and test execution cost. Our results indicate, that in many instances, despite its relative lack of precision, TestTube can reduce the time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["52"]}
{"title": "WYSIWYT testing in the spreadsheet paradigm: An empirical evaluation\n", "abstract": " Is it possible to achieve some of the benefits of formal testing within the informal programming conventions of the spreadsheet paradigm? We have been working on an approach that attempts to do so via the development of a testing methodology for this paradigm. Our \"What You See Is What You Test\" (WYSIWYT) methodology supplements the convention by which spreadsheets provide automatic immediate visual feedback about values by providing automatic immediate visual feedback about \"testedness\". In previous work we described this methodology (G. Rothermal et al., 1998). We present empirical data about the methodology's effectiveness. Our results show that the use of the methodology was associated with significant improvement in testing effectiveness and efficiency, even with no training on the theory of testing or test adequacy that the model implements. These results may be due at least in part to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "152\n", "authors": ["52"]}
{"title": "A static approach to prioritizing junit test cases\n", "abstract": " Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "150\n", "authors": ["52"]}
{"title": "On test suite composition and cost-effective regression testing\n", "abstract": " Regression testing is an expensive testing process used to revalidate software as it evolves. Various methodologies for improving regression testing processes have been explored, but the cost-effectiveness of these methodologies has been shown to vary with characteristics of regression test suites. One such characteristic involves the way in which test inputs are composed into test cases within a test suite. This article reports the results of controlled experiments examining the effects of two factors in test suite composition---test suite granularity and test input grouping---on the costs and benefits of several regression-testing-related methodologies: retest-all, regression test selection, test suite reduction, and test case prioritization. These experiments consider the application of several specific techniques, from each of these methodologies, across ten releases each of two substantial software systems, using seven\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "147\n", "authors": ["52"]}
{"title": "Empirical studies of test case prioritization in a JUnit testing environment\n", "abstract": " Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "145\n", "authors": ["52"]}
{"title": "Predicting accurate and actionable static analysis warnings\n", "abstract": " Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings, and suggests that the models are competitive with alternative models built without screening.", "num_citations": "139\n", "authors": ["52"]}
{"title": "Selecting Regression Tests for Object-Oriented Software.\n", "abstract": " Regression testing is an important but expensive software maintenance activity aimed at providing condence in modi ed software. Selective retest methods reduce the cost of regression testing by selecting tests for a modi ed program from a previously existing test suite. Many researchers have addressed the selective retest problem for procedural-language software, but few have addressed the problem for object-oriented software. In this paper, we present a new technique for selective retest, that handles object-oriented software. Our algorithm constructs dependence graphs for classes and applications programs, and uses these graphs to determine which tests in an existing test suite can cause a modi ed class or program to produce di erent output than the original. Unlike previous selective retest techniques, our method applies to modi ed and derived classes, as well as to applications programs that use modi ed classes. Our technique is strictly code-based, and makes no assumptions about methods used to specify or test the software initially.", "num_citations": "138\n", "authors": ["52"]}
{"title": "Prioritizing JUnit test cases: An empirical assessment and cost-benefits analysis\n", "abstract": " Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites. In particular, Java and the JUnit testing framework are being used extensively to build software systems in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "137\n", "authors": ["52"]}
{"title": "Understanding the effects of changes on the cost\u0393\u00c7\u00c9effectiveness of regression testing techniques\n", "abstract": " Regression testing is an expensive testing process used to validate modified software. Regression test selection and test\u0393\u00c7\u00c9case prioritization can reduce the costs of regression testing by selecting a subset of test cases for execution, or scheduling test cases to meet testing objectives better. The cost\u0393\u00c7\u00c9effectiveness of these techniques can vary widely, however, and one cause of this variance is the type and magnitude of changes made in producing a new software version. Engineers unaware of the causes and effects of this variance can make poor choices in designing change integration processes, selecting inappropriate regression testing techniques, designing excessively expensive regression test suites and making unnecessarily costly changes. Engineers aware of causal factors can perform regression testing more cost\u0393\u00c7\u00c9effectively. This article reports the results of an embedded multiple case study investigating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "134\n", "authors": ["52"]}
{"title": "Test case prioritization\n", "abstract": " Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their e ectiveness in meeting some performance goal. Various goals are possible; one involves rate of fault detection| a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test, and let software engineers begin correcting faults earlier than might otherwise be possible. In this paper, we describe several techniques for prioritizing test cases, and report the results of empirical studies investigating the e ectiveness of these techniques for improving rate of fault detection. Our results suggest that several techniques can signi cantly improve rate of fault detection, and illustrate several tradeo s between the techniques.", "num_citations": "134\n", "authors": ["52"]}
{"title": "A framework for evaluating regression test selection techniques\n", "abstract": " Regression testing is a necessary but expensive activity aimed at showing that code has not been adversely affected by changes. A selective approach to regression testing attempts to reuse tests from an existing test suite to test a modified program. This paper outlines issues relevant to selective retest approaches, and presents a framework within which such approaches can be evaluated. This framework is then used to evaluate and compare existing selective retest algorithms. The evaluation reveals strengths and weaknesses of existing methods, and highlights problems that future work in this area should address.< >", "num_citations": "128\n", "authors": ["52"]}
{"title": "Interprocedural control dependence\n", "abstract": " Program-dependence information is useful for a variety of applications, such as software testing and maintenance tasks, and code optimization. Properly defined, control and data dependences can be used to identify semantic dependences. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependences: dependences that are identified by analyzing the interactions among procedures. Many techniques for computing interprocedural data dependences exist; however, virtually no attention has been paid to interprocedural control dependence. Analysis techniques that fail to account for interprocedural control dependences can suffer unnecessary imprecision and loss of safety. This article presents a definition of  interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "122\n", "authors": ["52"]}
{"title": "Modeling the cost-benefits tradeoffs for regression testing techniques\n", "abstract": " Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["52"]}
{"title": "Directed test suite augmentation: techniques and tradeoffs\n", "abstract": " Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. Our preliminary work suggests that several factors influence the cost and effectiveness of test suite augmentation techniques. These include the order in which affected elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of an empirical study examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our experiment show that the primary factor affecting augmentation is the test case generation algorithm utilized; this affects both cost and effectiveness. The manner in which existing and newly generated test cases are utilized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["52"]}
{"title": "An empirical study of regression test application frequency\n", "abstract": " Regression testing is an expensive process used to revalidate modified software. Regression test selection (RTS) techniques reduce the cost of regression testing by selecting a subset of a test suite. Many RTS techniques have been proposed, and studies have shown that they produce savings; other studies have shown that their cost\u0393\u00c7\u00c9effectiveness varies with characteristics of the workloads to which they are applied. It seems plausible, however, that another factor that impacts RTS techniques involves the process by which they are applied. In particular, issues such as the frequency with which regression testing is performed affect the techniques. Thus, in earlier work an experiment was conducted to assess the effects of test application frequency on the cost\u0393\u00c7\u00c9effectiveness of RTS techniques. The results exposed tradeoffs to consider when using these techniques over a series of releases. This work, however, was\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "110\n", "authors": ["52"]}
{"title": "The impact of test suite granularity on the cost-effectiveness of regression testing\n", "abstract": " Regression testing is an expensive testing process used to validate software following modifications. The cost-effectiveness of regression testing techniques varies with characteristics of test suites. One such characteristic, test suite granularity, involves the way in which test inputs are grouped into test cases within a test suite. Various cost-benefits tradeoffs have been attributed to choices of test suite granularity, but almost no research has formally examined these tradeoffs. To address this lack, we conducted several controlled experiments, examining the effects of test suite granularity on the costs and benefits of several regression testing methodologies across six releases of two non-trivial software systems. Our results expose essential tradeoffs to consider when designing test suites for use in regression testing evolving systems.", "num_citations": "109\n", "authors": ["52"]}
{"title": "A controlled experiment assessing test case prioritization techniques via mutation faults\n", "abstract": " Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "108\n", "authors": ["52"]}
{"title": "Empirical studies of a prediction model for regression test selection\n", "abstract": " Regression testing is an important activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that: chooses a subset of a test suite that was used to test the software before the modifications; then uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite. Rosenblum and Weyuker (1997) proposed coverage-based predictors for use in predicting the effectiveness of regression test selection strategies. Using the regression testing cost model of Leung and White (1989; 1990), Rosenblum and Weyuker demonstrated the applicability of these predictors by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "103\n", "authors": ["52"]}
{"title": "An empirical study of the effect of time constraints on the cost-benefits of regression testing\n", "abstract": " Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. Therefore, we conducted an experiment to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our results show that time constraints can indeed play a significant role in determining both the cost-effectiveness of prioritization, and the relative cost-benefit tradeoffs among techniques, with important implications for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["52"]}
{"title": "A unified test case prioritization approach\n", "abstract": " Test case prioritization techniques attempt to reorder test cases in a manner that increases the rate at which faults are detected during regression testing. Coverage-based test case prioritization techniques typically use one of two overall strategies: a total strategy or an additional strategy. These strategies prioritize test cases based on the total number of code (or code-related) elements covered per test case and the number of additional (not yet covered) code (or code-related) elements covered per test case, respectively. In this article, we present a unified test case prioritization approach that encompasses both the total and additional strategies. Our unified test case prioritization approach includes two models (basic and extended) by which a spectrum of test case prioritization techniques ranging from a purely total to a purely additional technique can be defined by specifying the value of a parameter referred to as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "101\n", "authors": ["52"]}
{"title": "The impact of software evolution on code coverage information\n", "abstract": " Many tools and techniques for addressing software maintenance problems rely on code coverage information. Often, this coverage information is gathered for a specific version of a software system, and then used to perform analyses on subsequent versions of that system without being recalculated. As a software system evolves, however, modifications to the software alter the software's behavior on particular inputs, and code coverage information gathered on earlier versions of a program may not accurately reflect the coverage that would be obtained on later versions. This discrepancy may affect the success of analyses dependent on code coverage information. Despite the importance of coverage information in various analyses, in our search of the literature we find no studies specifically examining the impact of software evolution on code coverage information. Therefore, we conducted empirical studies to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["52"]}
{"title": "Computation of interprocedural control dependence\n", "abstract": " Program dependence information is useful for a variety of software testing and maintenance tasks. Properly defined, control and data dependencies can be used to identify semantic dependencies. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependencies: dependencies that exist because of interactions among procedures. Many techniques for computing data and control dependencies exist; however, in our search of the literature we find only one attempt to define and compute interprocedural control dependencies. Unfortunately, that approach can omit important control dependencies, and incorrectly identifies control dependencies for a large class of programs. This paper presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence, an efficient\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "91\n", "authors": ["52"]}
{"title": "Efficient construction of program dependence graphs\n", "abstract": " We present a new technique for constructing a program dependence graph that contains a program's control flow, along with the usual control and data dependence information. Our algorithm constructs a program dependence graph while the program is being parsed. For programs containing only structured transfers of control, our algorithm does not require information provided by the control flow graph or post dominator tree and therefore obviates the construction of these auxiliary graphs. For programs containing explicit transfers of control, our algorithm adjusts the partial control dependence subgraph, constructed during the parse, to incorporate exact control dependence information. There are several advantages to our approach. For many programs, our algorithm may result in  substantial savings in time and memory since our construction of the program dependence graph does not require the auxiliary\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "91\n", "authors": ["52"]}
{"title": "An empirical study of regression testing techniques incorporating context and lifetime factors and improved cost-benefit models\n", "abstract": " Regression testing is an important but expensive activity, and a great deal of research on regression testing methodologies has been performed. In recent years, much of this research has emphasized empirical studies, including evaluations of the effectiveness and efficiency of regression testing techniques. To date, however, most studies have been limited in terms of their consideration of testing context and system lifetime, and have used cost-benefit models that omit important factors and render some types of comparisons between techniques impossible. These limitations can cause studies to improperly assess the costs and benefits of regression testing techniques in practical settings. In this paper, we provide improved cost-benefit models for use in assessing regression testing methodologies, that incorporate context and lifetime factors not considered in prior studies, and we use these models to compare\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["52"]}
{"title": "On-demand test suite reduction\n", "abstract": " Most test suite reduction techniques aim to select, from a given test suite, a minimal representative subset of test cases that retains the same code coverage as the suite. Empirical studies have shown, however, that test suites reduced in this manner may lose fault detection capability. Techniques have been proposed to retain certain redundant test cases in the reduced test suite so as to reduce the loss in fault-detection capability, but these still do concede some degree of loss. Thus, these techniques may be applicable only in cases where loose demands are placed on the upper limit of loss in fault-detection capability. In this work we present an on-demand test suite reduction approach, which attempts to select a representative subset satisfying the same test requirements as an initial test suite conceding at most l% loss in fault-detection capability for at least c% of the instances in which it is applied. Our technique\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["52"]}
{"title": "Testing homogeneous spreadsheet grids with the\" what you see is what you test\" methodology\n", "abstract": " Although there has been recent research into ways to design environments that enable end users to create their own programs, little attention has been given to helping these end users systematically test their programs. To help address this need in spreadsheet systems (the most widely used type of end-user programming language), we previously introduced a visual approach to systematically testing individual cells in spreadsheet systems. However, the previous approach did not scale well in the presence of largely homogeneous grids, which introduce problems somewhat analogous to the array-testing problems of imperative programs. We present two approaches to spreadsheet testing that explicitly support such grids. We present the algorithms, time complexities, and performance data comparing the two approaches. This is part of our continuing work to bring to end users at least some of the benefits of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["52"]}
{"title": "Aristotle: A system for research on and development of program analysis based tools\n", "abstract": " Aristotle provides program analysis information and supports the development of software engineering tools. Aristotle's front end consists of parsers that gather control-ow, local data-ow, and symbol table information for C and Java programs. Aristotle tools use the data provided by the front end to perform a variety of tasks, such as data-ow and control-dependence analysis, dataow testing, regression test selection, graph construction and graph viewing. Parsers and tools use database access routines to store information in, and retrieve it from, a data repository. Users can view analysis data textually or graphically. A user interface provides menu-driven access to tools; many tools can also be invoked directly from applications programs. Most of Aristotle's components function on single procedures and entire programs. We use Aristotle as a platform for developing and experimenting with program analysis, maintenance, and testing tools.", "num_citations": "88\n", "authors": ["52"]}
{"title": "Incremental dynamic impact analysis for evolving software systems\n", "abstract": " Impact analysis - determining the potential effects of changes on a software system - plays an important role in helping engineers revalidate modified software. In previous work we presented a new impact analysis technique. PathImpact, for performing dynamic impact analysis at the level of procedures, and we showed empirically that the technique can be cost-effective in comparison to prominent prior techniques. A drawback of that approach as presented, however, is that when attempting to apply the technique to a new version of a system as that system and its test suite evolves, the process of recomputing the data required by the technique for that version can be excessively expensive. In this paper, therefore, we present algorithms that allow the data needed by PathImpact to be collected incrementally. We present the results of a controlled experiment investigating the costs and benefits of this incremental\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "86\n", "authors": ["52"]}
{"title": "Infrastructure support for controlled experimentation with software testing and regression testing techniques\n", "abstract": " Where the creation, understanding, and assessment of software testing and regression testing techniques are concerned, controlled experimentation is an indispensable research methodology. Obtaining the infrastructure necessary to support such experimentation, however, is difficult and expensive. As a result, progress in experimentation with testing techniques has been slow, and empirical data on the costs and effectiveness of techniques remains relatively scarce. To help address this problem, we have been designing and constructing infrastructure to support controlled experimentation with testing and regression testing techniques. This paper reports on the challenges faced by researchers experimenting with testing techniques, including those that inform the design of our infrastructure. The paper then describes the infrastructure that we are creating in response to these challenges, and that we are now\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "83\n", "authors": ["52"]}
{"title": "Automated test case generation for spreadsheets\n", "abstract": " Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work, we presented a methodology that assists spreadsheet users in testing their spreadsheet formulas. Our empirical studies have shown that this methodology can help end-users test spreadsheets more adequately and efficiently; however, the process of generating test cases can still represent a significant impediment. To address this problem, we have been investigating how to automate test case generation for spreadsheets in ways that support incremental testing and provide immediate visual feedback. We have utilized two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these techniques, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["52"]}
{"title": "Regression model checking\n", "abstract": " Model checking is a promising technique for verifying program behavior and is increasingly finding usage in industry. To date, however, researchers have primarily considered model checking of single versions of programs. It is well understood that model checking can be very expensive for large, complex programs. Thus, simply reapplying model checking techniques on subsequent versions of programs as they evolve, in the limited time that is typically available for validating new releases, presents challenges. To address these challenges, we have developed a new technique for regression model checking (RMC), that applies model checking incrementally to new versions of systems. We report results of an empirical study examining the effectiveness of our technique; our results show that it is significantly faster than traditional model checking.", "num_citations": "78\n", "authors": ["52"]}
{"title": "Directed test suite augmentation\n", "abstract": " As software evolves, engineers use regression testing to evaluate its fitness for release. Such testing typically begins with existing test cases, and many techniques have been proposed for reusing these cost-effectively. After reusing test cases, however, it is also important to consider code or behavior that has not been exercised by existing test cases and generate new test cases to validate these. This process is known as test suite augmentation. In this paper we present a directed test suite augmentation technique, that utilizes results from reuse of existing test cases together with an incremental concolic testing algorithm to augment test suites so that they are coverage-adequate for a modified program. We present results of an empirical study examining the effectiveness of our approach.", "num_citations": "75\n", "authors": ["52"]}
{"title": "Slicing spreadsheets: An integrated methodology for spreadsheet testing and debugging\n", "abstract": " Spreadsheet languages, which include commercial spreadsheets and various research systems, have proven to be flexible tools in many domain specific settings. Research shows, however, that spreadsheets often contain faults. We would like to provide at least some of the benefits of formal testing and debugging methodologies to spreadsheet developers. This paper presents an integrated testing and debugging methodology for spreadsheets. To accommodate the modeless and incremental development, testing and debugging activities that occur during spreadsheet creation, our methodology is tightly integrated into the spreadsheet environment. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing and debugging theory, and that takes advantage of the immediate visual feedback that is characteristic of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["52"]}
{"title": "Understanding and measuring the sources of variation in the prioritization of regression test suites\n", "abstract": " Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["52"]}
{"title": "Efficient, effective regression testing using safe test selection techniques\n", "abstract": " Regression testing is an expensive procedure performed on modified software to establish confidence that the software performs correctly. Most research on regression testing addresses one or both of two problems: how to select regression tests from an existing test suite (the test selection problem), and how to determine portions of a modified program to retest (the coverage identification problem). Techniques for solving these problems are called selective retest techniques, because they reduce the cost of regression testing through selective reuse of tests, or selective retesting of program components.", "num_citations": "66\n", "authors": ["52"]}
{"title": "Using component metadata to regression test component\u0393\u00c7\u00c9based software\n", "abstract": " Increasingly, modern\u0393\u00c7\u00c9day software systems are being built by combining externally\u0393\u00c7\u00c9developed software components with application\u0393\u00c7\u00c9specific code. For such systems, existing program\u0393\u00c7\u00c9analysis\u0393\u00c7\u00c9based software engineering techniques may not directly apply, due to lack of information about components. To address this problem, the use of component metadata has been proposed. Component metadata are metadata and metamethods provided with components, that retrieve or calculate information about those components. In particular, two component\u0393\u00c7\u00c9metadata\u0393\u00c7\u00c9based approaches for regression test selection are described: one using code\u0393\u00c7\u00c9based component metadata and the other using specification\u0393\u00c7\u00c9based component metadata. The results of empirical studies that illustrate the potential of these techniques to provide savings in re\u0393\u00c7\u00c9testing effort are provided. Copyright \u252c\u2310 2006 John Wiley & Sons, Ltd.", "num_citations": "64\n", "authors": ["52"]}
{"title": "Integrating automated test generation into the WYSIWYT spreadsheet testing methodology\n", "abstract": " Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work we presented a methodology that helps spreadsheet users test their spreadsheet formulas. Our empirical studies have shown that end users can use this methodology to test spreadsheets more adequately and efficiently; however, the process of generating test cases can still present a significant impediment. To address this problem, we have been investigating how to incorporate automated test case generation into our testing methodology in ways that support incremental testing and provide immediate visual feedback. We have used two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["52"]}
{"title": "Separate computation of alias information for reuse\n", "abstract": " Interprocedural data flow information is useful for many software testing and analysis techniques, including data flow testing, regression testing, program slicing and impact analysis. For programs with aliases, these testing and analysis techniques can yield invalid results, unless the data flow information accounts for aliasing effects. Recent research provides algorithms for performing interprocedural data flow analysis in the presence of aliases; however, these algorithms are expensive, and achieve precise results only on complete programs. This paper presents an algorithm for performing alias analysis on incomplete programs that lets individual software components such as library routines, subroutines or subsystems be independently analyzed. The paper also presents an algorithm for reusing the results of this separate analysis when the individual software components are linked with calling modules. Our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["52"]}
{"title": "Scaling up a\" what you see is what you test\" methodology to spreadsheet grids\n", "abstract": " Although there has been considerable research into ways to design visual programming environments to improve the processes of creating new programs and of understanding existing ones, little attention has been given to helping users of these environments test their programs. This feature would be particularly important for systems aimed at end users, since testing is the primary device they use to determine whether their programs are correct. To help address this need, we introduce two visual approaches to testing large grids in spreadsheet systems. This work scales up a visual testing methodology we previously developed for individual cells. The approaches are tightly integrated into Forms/3, a visual spreadsheet language, and communication with the user happens solely through the use of checkbox devices and coloring mechanisms. The intent of this work is to bring to end users at least some of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["52"]}
{"title": "A scalable distributed concolic testing approach: An empirical evaluation\n", "abstract": " Although testing is a standard method for improving the quality of software, conventional testing methods often fail to detect faults. Concolic testing attempts to remedy this by automatically generating test cases to explore execution paths in a program under test, helping testers achieve greater coverage of program behavior in a more automated fashion. Concolic testing, however, consumes a significant amount of computing time to explore execution paths, which is an obstacle toward its practical application. To address this limitation, we have developed a scalable distributed concolic testing framework that utilizes large numbers of computing nodes to generate test cases in a scalable manner. In this paper, we present the results of an empirical study that shows that the proposed framework can achieve a several orders-of-magnitude increase in test case generation speed compared to the original concolic approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["52"]}
{"title": "Testing strategies for form-based visual programs\n", "abstract": " Form based visual programming languages, which include electronic spreadsheets and a variety of research systems, have had a substantial impact on end user computing. Research shows that form based visual programs often contain faults, and that their creators often have unwarranted confidence in the reliability of their programs. Despite this evidence, we find no discussion in the research literature of techniques for testing or assessing the reliability of form based visual programs. The paper addresses this lack. We describe differences between the form based and imperative programming paradigms, and discuss effects these differences have on strategies for testing form based programs. We then present several test adequacy criteria for form based programs, and illustrate their application. We show that an analogue to the traditional \"all-uses\" dataflow test adequacy criterion is well suited for code based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["52"]}
{"title": "Using sensitivity analysis to create simplified economic models for regression testing\n", "abstract": " Software engineering methodologies are subject to complex cost-benefit tradeoffs. Economic models can help practitioners and researchers assess methodologies relative to these tradeoffs. Effective economic models, however, can be established only through an iterative process of refinement involving analytical and empirical methods. Sensitivity analysis provides one such method. By identifying the factors that are most important to models, sensitivity analysis can help simplify those models; it can also identify factors that must be measured with care, leading to guidelines for better test strategy definition and application. In prior work we presented the first comprehensive economic model for the regression testing process, that captures both cost and benefit factors relevant to that process while supporting evaluation of these processes across entire system lifetimes. In this work we use sensitivity analysis to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["52"]}
{"title": "How well do professional developers test with code coverage visualizations? an empirical study\n", "abstract": " Despite years of availability of testing tools, professional software developers still seem to need better support to determine the effectiveness of their tests. Without improvements in this area, inadequate testing of software seems likely to remain a major problem. To address this problem, industry and researchers have proposed systems that visualize \"testedness\" for end-user and professional developers. Empirical studies of such systems for end-user programmers have begun to show success at helping end users write more effective tests. Encouraged by this research, we examined the effect that code coverage visualizations have on the effectiveness of test cases that professional software developers write. This paper presents the results of an empirical study conducted using code coverage visualizations found in a commercially available programming environment. Our results reveal how this kind of code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["52"]}
{"title": "Redefining prioritization: continuous prioritization for continuous integration\n", "abstract": " Continuous integration (CI) development environments allow software engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach\" continuously\" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["52"]}
{"title": "Using assertions to help end-user programmers create dependable web macros\n", "abstract": " Web macros give web browser users ways to\" program\" tedious tasks, allowing those tasks to be repeated more quickly and reliably than when performed by hand. Web macros face dependability problems of their own, however: changes in websites or failure on the part of end-user programmers to anticipate possible macro behaviors can cause macros to act incorrectly, often in ways that are difficult to detect. We would like to provide at least some of the benefits of software engineering methodologies to the creators of web macros. To do this we adapt assertions to web-macro programming scenarios. While assertions are well-known to professional software engineers, our web macro assertions are unique in their focus on website evolution, are generated automatically, and encode the expectations and assumptions of a rapidly growing group of users who often have limited formal programming expertise. We have\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["52"]}
{"title": "SimRT: An automated framework to support regression testing for data races\n", "abstract": " Concurrent programs are prone to various classes of difficult-to-detect faults, of which data races are particularly prevalent. Prior work has attempted to increase the cost-effectiveness of approaches for testing for data races by employing race detection techniques, but to date, no work has considered cost-effective approaches for re-testing for races as programs evolve. In this paper we present SimRT, an automated regression testing framework for use in detecting races introduced by code modifications. SimRT employs a regression test selection technique, focused on sets of program elements related to race detection, to reduce the number of test cases that must be run on a changed program to detect races that occur due to code modifications, and it employs a test case prioritization technique to improve the rate at which such races are detected. Our empirical study of SimRT reveals that it is more efficient and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["52"]}
{"title": "Waterfall: An incremental approach for repairing record-replay tests of web applications\n", "abstract": " Software engineers use record/replay tools to capture use case scenarios that can serve as regression tests for web applications. Such tests, however, can be brittle in the face of code changes. Thus, researchers have sought automated approaches for repairing broken record/replay tests. To date, such approaches have operated by directly analyzing differences between the releases of web applications. Often, however, intermediate versions or commits exist between releases, and these represent finer-grained sequences of changes by which new releases evolve. In this paper, we present WATERFALL, an incremental test repair approach that applies test repair techniques iteratively across a sequence of fine-grained versions of a web application. The results of an empirical study on seven web applications show that our approach is substantially more effective than a coarse-grained approach (209% overall\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["52"]}
{"title": "Understanding user understanding: determining correctness of generated program invariants\n", "abstract": " Recently, work has begun on automating the generation of test oracles, which are necessary to fully automate the testing process. One approach to such automation involves dynamic invariant generation which extracts invariants from program executions. To use such invariants as test oracles, however, it is necessary to distinguish correct from incorrect invariants, a process that currently requires human intervention. In this work we examine this process. In particular, we examine the ability of 30 users, across two empirical studies, to classify invariants generated from three Java programs. Our results indicate that users struggle to classify generated invariants: on average, they misclassify 9.1% to 31.7% of correct invariants and 26.1%-58.6% of incorrect invariants. These results contradict prior studies that suggest that classification by users is easy, and indicate that further work needs to be done to bridge the gap\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["52"]}
{"title": "Aristotle: A system for development of program analysis based tools\n", "abstract": " Aristotle provides program analysis information, and supports the development of software engineering tools. Aristotle's front end consists of parsers that gather control flow, local dataflow and symbol table information for procedural language programs. We implemented a parser for C by incorporating analysis routines into the GNU C parser; a C++ parser is being implemented using similar techniques. Aristotle tools use the data provided by the parsers to perform a variety of tasks, such as dataflow and control dependence analysis, dataflow testing, graph construction and graph viewing. Most of Aristotle's components function on single procedures and entire programs. Parsers and tools use database handler routines to store information in, and retrieve it from, a central database. A user interface provides interactive menu-driven access to tools, and users can view results textually or graphically. Many tools can also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["52"]}
{"title": "Why do record/replay tests of web applications break?\n", "abstract": " Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["52"]}
{"title": "Oracle-centric test case prioritization\n", "abstract": " Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover''\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["52"]}
{"title": "Factors affecting the use of genetic algorithms in test suite augmentation\n", "abstract": " Test suite augmentation techniques are used in regression testing to help engineers identify code elements affected by changes, and generate test cases to cover those elements. Researchers have created various approaches to identify affected code elements, but only recently have they considered integrating, with this task, approaches for generating test cases. In this paper we explore the use of genetic algorithms in test suite augmentation. We identify several factors that impact the effectiveness of this approach, and we present the results of a case study exploring the effects of one of these factors: the manner in which existing and newly generated test cases are utilized by the genetic algorithm. Our results reveal several ways in which this factor can influence augmentation results, and reveal open problems that researchers must address if they wish to create augmentation techniques that make use of genetic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["52"]}
{"title": "A hybrid directed test suite augmentation technique\n", "abstract": " Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. In previous work, we studied two approaches to augmentation, one using a concolic test case generation algorithm and one using a genetic test case generation algorithm. We found that these two approaches behaved quite differently in terms of their costs and their abilities to generate effective test cases for evolving programs. In this paper, we present a hybrid test suite augmentation technique that combines these two test case generation algorithms. We report the results of an empirical study that shows that this hybrid technique can be effective, but with varying degrees of costs, and we analyze our results further to provide suggestions for reducing costs.", "num_citations": "38\n", "authors": ["52"]}
{"title": "Test reuse in the spreadsheet paradigm\n", "abstract": " Spreadsheet languages are widely used by a variety of end users to perform many important tasks. Despite their perceived simplicity, spreadsheets often contain faults. Furthermore, users modify their spreadsheets frequently, which can render previously correct spreadsheets faulty. To address this problem, we previously introduced a visual approach by which users can systematically test their spreadsheets, see where new tests are required after changes, and request automated generation of potentially useful test inputs. To date, however, this approach has not taken advantage of previously developed test cases, which means that users of the approach cannot benefit, when re-testing following changes, from prior testing efforts. We have therefore been investigating ways to add support for test re-use into our spreadsheet testing methodology. In this paper we present a test re-use strategy for spreadsheets, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["52"]}
{"title": "A cost-effective random testing method for programs with non-numeric inputs\n", "abstract": " Random testing (RT) has been widely used in the testing of various software and hardware systems. Adaptive random testing (ART) is a family of random testing techniques that aim to enhance the failure-detection effectiveness of RT by spreading random test cases evenly throughout the input domain. ART has been empirically shown to be effective on software with numeric inputs. However, there are two aspects of ART that need to be addressed to render its adoption more widespread-applicability to programs with nonnumeric inputs, and the high computation overhead of many ART algorithms. We present a linear-order ART algorithm for software with non-numeric inputs. The key requirement for using ART with non-numeric inputs is an appropriate \u0393\u00c7\u00a3distance\u0393\u00c7\u00a5 measure. We use the concepts of categories and choices from category-partition testing to formulate such a measure. We investigate the failure-detection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["52"]}
{"title": "On the benefits of providing versioning support for end users: an empirical study\n", "abstract": " End users with little formal programming background are creating software in many different forms, including spreadsheets, web macros, and web mashups. Web mashups are particularly popular because they are relatively easy to create, and because many programming environments that support their creation are available. These programming environments, however, provide no support for tracking versions or provenance of mashups. We believe that versioning support can help end users create, understand, and debug mashups. To investigate this belief, we have added versioning support to a popular wire-oriented mashup environment, Yahoo! Pipes. Our enhanced environment, which we call \u0393\u00c7\u00a3Pipes Plumber,\u0393\u00c7\u00a5 automatically retains versions of pipes and provides an interface with which pipe programmers can browse histories of pipes and retrieve specific versions. We have conducted two studies of this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["52"]}
{"title": "Comparing model-based and dynamic event-extraction based GUI testing techniques: An empirical study\n", "abstract": " Graphical user interfaces are pervasive in modern software systems, and to ensure their quality it is important to test them. Two primary classes of automated GUI testing approaches, those based on static models and those based on dynamic event-extraction, present tradeoffs in cost and effectiveness. For example, static model-based GUI testing techniques can create test cases that contain nonexecutable events, whereas dynamic event-extraction based GUI testing techniques can create larger numbers of duplicate test cases. To better understand the effects of these tradeoffs, we created a GUI testing framework that facilitates fair comparison of different GUI testing techniques, and we conducted a controlled experiment comparing representative versions of static-model based and dynamic event-extraction based testing techniques on several GUI-based Java applications. Our study reveals several cost and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["52"]}
{"title": "Incorporating incremental validation and impact analysis into spreadsheet maintenance: An empirical study\n", "abstract": " Spreadsheets are among the most common form of software in use today. Unlike more traditional forms of software however, spreadsheets are created and maintained by end users with little or no programming experience. As a result, a high percentage of these \"programs\" contain errors. Unfortunately, software engineering research has for the most part ignored this problem. We have developed a methodology that is designed to aid end users in developing, testing, and maintaining spreadsheets. The methodology communicates testing information and information about the impact of cell changes to users in a manner that does not require an understanding of formal testing theory or the behind the scenes mechanisms. The paper presents the results of an empirical study that shows that, during maintenance, end users using our methodology were more accurate in making changes and did a significantly better job\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["52"]}
{"title": "Predator behavior in the wild web world of bugs: An information foraging theory perspective\n", "abstract": " Web active end users often coalesce web information using web mashups. Web contents, however, tend to evolve frequently, and along with the black box nature of visual languages this complicates the process of debugging mashups. While debugging, end users need to locate faults within the code and then find a way to correct them; this process requires them to seek information related to web page content and behavior. In this paper, using an information foraging theory lens, we qualitatively study the debugging behaviors of 16 web-active end users. Our results show that the stronger scents available within mashup programming environments can improve users' foraging success. Our results lead to a new model for debugging activities framed in terms of information foraging theory, and to a better understanding of ways in which end-user programming environments can be enhanced to better support debugging.", "num_citations": "32\n", "authors": ["52"]}
{"title": "Continuous test suite augmentation in software product lines\n", "abstract": " Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["52"]}
{"title": "Sofya: A flexible framework for development of dynamic program analyses for Java software\n", "abstract": " Dynamic analysis techniques are well established in the software engineering community as methods for validating, understanding, maintaining, and improving programs. Generally, this class of techniques requires developers to instrument programs to generate events that capture, or observe, relevant features of program execution. Streams of these events are then processed to achieve the goals of the dynamic analysis. The lack of high-level tools for defining program observations, automating their mapping to efficient low-level implementations, and supporting the flexible combination of different event-stream-based processing components hampers the development and evaluation of new dynamic analysis techniques. For example, mapping non-trivial program observations to existing low-level instrumentation facilities is a time-consuming and error-prone process that can easily result in poorly performing analyses.", "num_citations": "32\n", "authors": ["52"]}
{"title": "An empirical comparison of two safe regression test selection techniques\n", "abstract": " Regression test selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Safe regression test selection techniques guarantee (under specific conditions) that the selected subset will not omit faults that could have been revealed by the entire suite. Many regression test selection techniques have been described in the literature. Empirical studies of some of these techniques have shown that they can be beneficial, but only a few studies have empirically compared different techniques, and fewer still have considered safe techniques. In this paper, we report the results of a comparative empirical study of implementations of two safe regression test selection techniques: DejaVu and Pytia. Our results show that, despite differences in their approaches, and despite the theoretically greater ability of DejaVu to select smaller test suites than\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["52"]}
{"title": "An efficient, robust, and scalable approach for analyzing interacting android apps\n", "abstract": " When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-the-art approach, and that it can effectively and efficiently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["52"]}
{"title": "Putting your best tests forward\n", "abstract": " Test case prioritization orders tests so that they help you meet your testing goals earlier during regression testing. Prioritization techniques can, for example, order tests to achieve coverage at the fastest rate possible, exercise features in order of expected frequency of use, or reveal faults as early as possible. We focus on the last goal, which we describe as \"increasing a test suite's rate of fault detection\" or the speed with which the test suite reveals faults. A faster fault detection rate during regression testing provides earlier feedback on a system under test, supporting earlier strategic decisions about release schedules and letting engineers begin debugging sooner. Also, if testing time is limited or unexpectedly reduced, prioritization increases the chance that testing resources will have been spent as cost effectively as possible in the available time.", "num_citations": "31\n", "authors": ["52"]}
{"title": "Experimental program analysis: A new program analysis paradigm\n", "abstract": " Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that a new form of program analysis technique can be created by incorporating characteristics of experimentation into analyses. This paper reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We show how the use of an experimental program analysis paradigm can help researchers identify limitations of analysis techniques, improve existing experimental program analysis techniques, and create new experimental program analysis techniques.", "num_citations": "30\n", "authors": ["52"]}
{"title": "Dodona: automated oracle data set selection\n", "abstract": " Software complexity has increased the need for automated software testing. Most research on automating testing, however, has focused on creating test input data. While careful selection of input data is necessary to reach faulty states in a system under test, test oracles are needed to actually detect failures. In this work, we describe Dodona, a system that supports the generation of test oracles. Dodona ranks program variables based on the interactions and dependencies observed between them during program execution. Using this ranking, Dodona proposes a set of variables to be monitored, that can be used by engineers to construct assertion-based oracles. Our empirical study of Dodona reveals that it is more effective and efficient than the current state-of-the-art approach for generating oracle data sets, and can often yield oracles that are almost as effective as oracles hand-crafted by engineers without support.", "num_citations": "29\n", "authors": ["52"]}
{"title": "Syntax-directed construction of program dependence graphs\n", "abstract": " We present an algorithm that constructs program dependence graphs as a program is parsed. For programs that contain only structured transfers of control, our algorithm does not require explicit control ow or postdominator information to compute exact control dependencies. For programs that contain explicit transfers of control, our algorithm can determine whether these transfers of control are used in a structured way, and if so, compute control dependencies without explicit control ow or postdominator information. When transfers of control are ill-behaved, our algorithm adjusts the control dependence information computed during the parse, to obtain exact control dependencies. For many programs, our algorithm provides savings in time and memory, because it does not require prior computation of control ow or postdominator information. However, our algorithm also calculates control ow information during the parse, and incorporates this information into the program dependence graphs that it constructs; the resulting graphs have a wider range of applicability than graphs that do not contain this information.", "num_citations": "29\n", "authors": ["52"]}
{"title": "On the use of delta debugging to reduce recordings and facilitate debugging of web applications\n", "abstract": " Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging. Nevertheless, a recording of an event sequence may include many events that are not related to a failure, and this may render debugging more difficult. To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure. We present the results of three empirical studies that show that (1) recording reduction can achieve significant reductions in recording size and replay time on actual web applications obtained from developer forums,(2) reduced recordings do in fact help programmers locate faults significantly more efficiently as, and no less effectively than non-reduced recordings, and (3) recording reduction produces even greater reductions on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["52"]}
{"title": "SimTester: a controllable and observable testing framework for embedded systems\n", "abstract": " In software for embedded systems, the frequent use of interrupts for timing, sensing, and I/O processing can cause concurrency faults to occur due to interactions between applications, device drivers, and interrupt handlers. This type of fault is considered by many practitioners to be among the most difficult to detect, isolate, and correct, in part because it can be sensitive to execution interleavings and often occurs without leaving any observable incorrect output. As such, commonly used testing techniques that inspect program outputs to detect failures are often ineffective at detecting them. To test for these concurrency faults, test engineers need to be able to control interleavings so that they are deterministic. Furthermore, they also need to be able to observe faults as they occur instead of relying on observable incorrect outputs.", "num_citations": "27\n", "authors": ["52"]}
{"title": "Revealing the copy and paste habits of end users\n", "abstract": " Transferring data across applications is a common end user task, and copying and pasting via the clipboard lets users do so relatively easily. Using the clipboard, however, can also introduce inefficiencies and errors in user tasks. To help researchers and tool developers understand and address these problems, we studied how end users interact with the clipboard through cut, copy, and paste actions. This study was performed by logging clipboard interactions while end users performed everyday tasks. From the clip-board usage data, we have identified several usage patterns that describe how data is transferred within the desktop environment. Such patterns help us understand end user behavior and indicate areas in which clipboard support tools can be improved.", "num_citations": "27\n", "authors": ["52"]}
{"title": "Sofya: Supporting rapid development of dynamic program analyses for java\n", "abstract": " Dynamic analysis is an increasingly important means of supporting software validation and maintenance. To date, developers of dynamic analyses have used low-level instrumentation and debug interfaces to realize their analyses. Many dynamic analyses, however, share multiple common high-level requirements, e.g., capture of program data state as well as events, and efficient and accurate event capture in the presence of threading. We present SOFYA - an infra-structure designed to provide high-level, efficient, concurrency-aw are support for building analyses that reason about rich observations of program data and events. It provides a layered, modular architecture, which has been successfully used to rapidly develop and evaluate a variety of demanding dynamic program analyses. In this paper, we describe the SOFYA framework, the challenges it addresses, and survey several such analyses.", "num_citations": "27\n", "authors": ["52"]}
{"title": "A comparison of regression test selection techniques\n", "abstract": " Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely a ected by changes. A selective approach to regression testing reuses tests from an existing test suite to test a modi ed program. Many selective retest strategies have been proposed. However, it is di cult to compare and evaluate these strategies, because they are based on di erent philosophies. This paper outlines the issues relevant to selective retest strategies, and uses these issues as the basis for a framework within which to evaluate the strategies. We illustrate the application of this framework by using it to evaluate existing selective retest approaches. The evaluation reveals the strengths and weaknesses of existing approaches, and highlights the problems that future work in this area should address.", "num_citations": "27\n", "authors": ["52"]}
{"title": "Experience with regression test selection\n", "abstract": " Regression testing is a maintenance process that attempts to validate modi ed software, and ensure that no new errors are introduced into previously tested code. Regression testing is expensive; it can account for as much as one-half of the cost of software maintenance 19]. One approach to regression testing is selective retest, which addresses two problems:(1) the problem of selecting tests from an existing test suite, and (2) the problem of determining where additional tests may be required. We have developed a new selective retest technique that addresses the rst problem 26]. Our algorithms construct control ow graphs for a procedure or program and its modi ed version, and use these graphs to select tests from the original test suite that execute changed code. To investigate the application of our technique in practice, we implemented one of our algorithms as a tool called DejaVu. We conducted empirical studies with our technique, by applying DejaVu to various programs, modi ed versions, and test suites. Our empirical results support several conclusions about regression test selection. In particular, our results suggest that our technique can signi cantly reduce the cost of regression testing a modi ed program.", "num_citations": "26\n", "authors": ["52"]}
{"title": "Can fault\u0393\u00c7\u00c9exposure\u0393\u00c7\u00c9potential estimates improve the fault detection abilities of test suites?\n", "abstract": " Code\u0393\u00c7\u00c9coverage\u0393\u00c7\u00c9based test data adequacy criteria typically treat all coverable code elements (such as statements,  basic blocks or outcomes of decisions) as equal. In practice, however, the probability that a test case can expose a fault in a code element varies: some faults are more easily revealed than others. Thus, several researchers have suggested that if  one could estimate the probability that a fault in  a code element will cause a failure, one could use  this estimate to determine the number of executions of  a code element that are required to achieve a certain  level of confidence in that element's correctness. This estimate, in turn, could be used to improve the  fault\u0393\u00c7\u00c9detection effectiveness of test suites and help  testers distribute testing resources more effectively. This conjecture is intriguing; however, like many such  conjectures it has never been directly examined empirically. If empirical evidence were to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["52"]}
{"title": "Interactive fault localization techniques in a spreadsheet environment\n", "abstract": " End-user programmers develop more software than any other group of programmers, using software authoring devices such as multimedia simulation builders, e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been only a little research on finding ways to help these programmers with the dependability of the software they create. We have been working to address this problem in several ways, one of which includes supporting end-user debugging activities through interactive fault localization techniques. This paper investigates fault localization techniques in the spreadsheet domain, the most common type of end-user programming environment. We investigate a technique previously described in the research literature and two new techniques. We present the results of an empirical study to examine the impact of two individual factors on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["52"]}
{"title": "Hybrid directed test suite augmentation: An interleaving framework\n", "abstract": " Test suite augmentation techniques generate test cases to cover code missed by existing regression test suites. Various augmentation techniques have been proposed, utilizing several test case generation algorithms. Research has shown that different algorithms have different strengths, and that combining them into a single hybrid approach may be cost-effective. In this paper we present a framework for hybrid test suite augmentation that allows test case generation algorithms to be interleaved dynamically and that can easily incorporate new algorithms, interleaving strategies, and choices of other parameters that influence algorithm performance. We empirically study an implementation of this framework in which we use two test case generation algorithms and several algorithm interleavings. Our results show that specific instantiations of our framework can produce augmentation techniques that are more cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["52"]}
{"title": "An empirical comparison of the fault-detection capabilities of internal oracles\n", "abstract": " Modern computer systems are prone to various classes of runtime faults due to their reliance on features such as concurrency and peripheral devices such as sensors. Testing remains a common method for uncovering faults in these systems, but many runtime faults are difficult to detect using typical testing oracles that monitor only program output. In this work we empirically investigate the use of internal test oracles: oracles that detect faults by monitoring aspects of internal program and system states. We compare these internal oracles to each other and to output-based oracles for relative effectiveness and examine tradeoffs between oracles involving incorrect reports about faults (false positives and false negatives). Our results reveal several implications that test engineers and researchers should consider when testing for runtime faults.", "num_citations": "23\n", "authors": ["52"]}
{"title": "Web application characterization through directed requests\n", "abstract": " Web applications are increasingly prominent in society, serving a wide variety of user needs. Engineers seeking to enhance, test, and maintain these applications must be able to understand and characterize their interfaces. Third-party programmers (professional or end user) wishing to incorporate the data provided by such services into their own applications would also benefit from such characterization when the target site does not provide adequate programmatic interfaces. In this paper, therefore, we present methodologies for characterizing the interfaces to web applications through a form of dynamic analysis, in which directed requests are sent to the application, and responses are analyzed to draw inferences about its interface. We also provide mechanisms to increase the scalability of the approach. Finally, we evaluate the approach's performance on three well-known, non-trivial web applications.", "num_citations": "23\n", "authors": ["52"]}
{"title": "On the relative strengths of model-based and dynamic event extraction-based GUI testing techniques: An empirical study\n", "abstract": " Many software systems rely on graphical-user interfaces (GUIs) to support user interactions. The correctness of these GUIs affects the overall quality of the systems, and thus, it is important that GUIs be tested. To support such testing, GUI test case generation techniques based on graph models such as event flow graphs (EFGs) have been used to generate test cases in the form of sequences of events. Models, however, are abstractions of dynamic behavior and may not accurately reflect actual system behavior, thus, test case generation techniques based on models may create nonexecutable test cases and miss important events. Test case generation techniques based on dynamic event extraction-based approaches, in contrast, may suffer less from these effects. As a consequence, we expect that the two approaches will differ in terms of cost and effectiveness. In this paper, we report the results of an empirical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["52"]}
{"title": "Dynamic characterization of web application interfaces\n", "abstract": " Web applications are increasingly prominent in society, serving a wide variety of user needs. Engineers seeking to enhance, test, and maintain these applications and third-party programmers wishing to utlize these applications need to understand their interfaces. In this paper, therefore, we present methodologies for characterizing the interfaces of web applications through a form of dynamic analysis, in which directed requests are sent to the application, and responses are analyzed to draw inferences about its interface. We also provide mechanisms to increase the scalability of the approach. Finally, we evaluate the approach\u0393\u00c7\u00d6s performance on six non-trivial web applications.", "num_citations": "22\n", "authors": ["52"]}
{"title": "SimRacer: An automated framework to support testing for process-level races\n", "abstract": " Faults introduced by races are difficult to detect because they usually occur only under specific execution interleavings. Numerous program analysis and testing techniques have been proposed to detect races between threads. Little work, however, has addressed the problem of detecting and testing for process-level races, in which two processes access a shared resource without proper synchronization. In this paper, we present SIMRACER, a novel testing-based framework that allows engineers to effectively test for process-level races. SIMRACER first computes potential races based on runtime traces obtained by running existing tests on target processes, and then it controls process scheduling relative to the potential races so that real races can be created. We implemented SIMRACER on a commercial virtual platform that is widely used to support hardware/software co-design. We then evaluated its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["52"]}
{"title": "Debugging support for end user mashup programming\n", "abstract": " Programming for the web can be an intimidating task, particularly for non-professional (\" end-user\") programmers. Mashup programming environments attempt to remedy this by providing support for such programming. It is well known, however, that mashup programmers create applications that contain bugs. Furthermore, mashup programmers learn from examples and reuse other mashups, which causes bugs to propagate to other mashups. In this paper we classify the bugs that occur in a large corpus of Yahoo! Pipes mashups. We describe support we have implemented in the Yahoo! Pipes environment to provide automatic error detection techniques that help mashup programmers localize and correct these bugs. We present the results of a think-aloud study comparing the experiences of end-user mashup programmers using and not using our support. Our results show that our debugging enhancements do\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["52"]}
{"title": "The impact of concurrent coverage metrics on testing effectiveness\n", "abstract": " When testing multithreaded programs, the number of possible thread interactions makes exploring all interactions infeasible in practice. In response, researchers have developed concurrent coverage metrics for multithreaded programs. These metrics allow them to estimate how well they have exercised concurrent program behavior, just as branch and statement coverage metrics do for sequential program testing. However, unlike sequential coverage metrics, the effectiveness of concurrent coverage metrics in testing remains largely unexamined. In this paper, we explore the relationship between concurrent coverage and fault detection effectiveness by studying the application of eight concurrent coverage metrics in testing nine concurrent programs. Our results show that existing concurrent coverage metrics are often moderate to strong predictors of concurrent testing effectiveness, and are generally reasonable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["52"]}
{"title": "History repeats itself more easily when you log it: Versioning for mashups\n", "abstract": " Web mashup environments provide a way for users to combine data from web applications and services to create new content. Currently, these environments do not provide support for tracking the development histories of mashups. We have thus added configuration management support to the Yahoo! Pipes mashup environment. We describe this support, and provide results of an experiment studying the ability of programmers to create and debug mashups in its presence. Our results show that versioning support can help both groups of users do both tasks better.", "num_citations": "21\n", "authors": ["52"]}
{"title": "Using property-based oracles when testing embedded system applications\n", "abstract": " Embedded systems are becoming increasingly ubiquitous, controlling a wide variety of popular and safety-critical devices. Effective testing techniques could improve the dependability of these systems. In prior work we presented an approach for testing embedded systems, focusing on embedded system applications and the tasks that comprise them. In this work we focus on a second but equally important aspect of testing embedded systems, namely, the need to provide observability of system behavior sufficient to allow engineers to detect failures. We present several property-based oracles that can be instantiated in embedded systems through program analysis and instrumentation, and can detect failures for which simple output-based oracles are inadequate. An empirical study of our approach shows that it can be effective.", "num_citations": "21\n", "authors": ["52"]}
{"title": "Directed test suite augmentation: an empirical investigation\n", "abstract": " Test suite augmentation techniques are used in regression testing to identify code elements in a modified program that are not adequately tested and to generate test cases to cover those elements. A defining feature of test suite augmentation techniques is the potential for reusing existing regression test suites. Our preliminary work suggests that several factors influence the efficiency and effectiveness of augmentation techniques that perform such reuse. These include the order in which target code elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of two empirical studies examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our studies show that the primary factor affecting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["52"]}
{"title": "A coherent family of analyzable graphical representations for object-oriented software\n", "abstract": " Many software engineering tools and techniques rely on graphical representations of software, such as control ow graphs, program dependence graphs, or system dependence graphs. To apply these tools and techniques to object-oriented software, we cannot necessarily use existing graphical representations of procedural software. Representations of object-oriented software, like those for procedural software, must depict individual procedures (methods) and entire programs; however, they must also depict classes and their interactions, and account for the e ects of inheritance, polymorphism, and aggregation. These representations should facilitate the use of existing program analysis tools and techniques, rather than requiring us to create new techniques. A system for constructing and managing these representations should take advantage of the code reuse inherent in object-oriented software, by reusing representational components. In this paper, we describe a coherent family of graphical representations of object-oriented software that meet the foregoing quali cations, and we outline the architecture of an e cient, extensible system for constructing and managing those representations.", "num_citations": "20\n", "authors": ["52"]}
{"title": "Helping end-users\" engineer\" dependable Web applications\n", "abstract": " End-user programmers are increasingly relying on Web authoring environments to create Web applications. Although often consisting primarily of Web pages, such applications are increasingly going further, harnessing the content available on the Web through \"programs\" that query other Web applications for information to drive other tasks. Unfortunately, errors can be pervasive in Web applications, impacting their dependability. This paper reports the results of an exploratory study of end-user Web application developers, performed with the aim of exposing prevalent classes of errors. The results suggest that end-users struggle the most with the identification and manipulation of variables when structuring requests to obtain data from other Web sites. To address this problem, we present a family of techniques that help end user programmers perform this task, reducing possible sources of error. The techniques\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["52"]}
{"title": "Are concurrency coverage metrics effective for testing: a comprehensive empirical investigation\n", "abstract": " Testing multithreaded programs is inherently challenging, as programs can exhibit numerous thread interactions. To help engineers test these programs cost\u0393\u00c7\u00c9effectively, researchers have proposed concurrency coverage metrics. These metrics are intended to be used as predictors for testing effectiveness and provide targets for test generation. The effectiveness of these metrics, however, remains largely unexamined. In this work, we explore the impact of concurrency coverage metrics on testing effectiveness and examine the relationship between coverage, fault detection, and test suite size. We study eight existing concurrency coverage metrics and six new metrics formed by combining complementary metrics. Our results indicate that the metrics are moderate to strong predictors of testing effectiveness and effective at providing test generation targets. Nevertheless, metric effectiveness varies across programs, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["52"]}
{"title": "Versioning for mashups\u0393\u00c7\u00f4an exploratory study\n", "abstract": " End users with little software background are developing numerous software applications using devices such as spreadsheets, web mashups, and web macros. Web mashups are particularly popular because they are easy to create and there are large public repositories that store them and allow their reuse. Existing repositories, however, provide no functionality for tracking the development histories of mashups. We believe that versioning capabilities can help end users develop, understand, and reuse mashups. To investigate this belief, we created a versioning extension for Yahoo! Pipes \u0393\u00c7\u00f4 a popular mashup environment \u0393\u00c7\u00f4 and conducted an exploratory study of users utilizing the environment. Our results show that versioning information allows users to perform mashup creation tasks more correctly and in less time than users not having that information, while also improving the reusability of pipes.", "num_citations": "17\n", "authors": ["52"]}
{"title": "Test case prioritization based on information retrieval concepts\n", "abstract": " In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.", "num_citations": "16\n", "authors": ["52"]}
{"title": "An empirical evaluation of a testing and debugging methodology for Excel\n", "abstract": " Spreadsheets are one of the most commonly used types of programs in the world, and it is important that they be sufficiently dependable. To help end users who create spreadsheets do so more reliably, we have created a testing and debugging methodology and environment for use in spreadsheets, known as the WYSIWYT methodology. Our prior experiments with WYSIWYT show that users can utilize it to ensure that their spreadsheets are more dependable, but these experiments to date have considered only an unfamiliar prototype spreadsheet environment, and have not involved spreadsheet creation tasks. In this work we conducted a controlled experiment that addresses these limitations. The results of this study indicate that the use of WYSIWYT did not affect the correctness of spreadsheets created by users, but it did significantly reduce the amount of effort required to create them. Further, the subjects'\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["52"]}
{"title": "An integrated software engineering approach for end-user programmers\n", "abstract": " End-user programming has become the most common form of programming in use today. Despite this growth, there has been little investigation into the correctness of the programs end-users create. We have been investigating ways to address this problem via a holistic approach we call enduser software engineering. The concept is to bring support for aspects of software development that happen beyond the \u0393\u00c7\u00a3coding\u0393\u00c7\u00a5 stage\u0393\u00c7\u00f6such as testing and debugging\u0393\u00c7\u00f6together into the support that already exists for incremental, interactive programming by end-users. In this chapter, we present our progress on three aspects of end-user software engineering: systematic \u0393\u00c7\u00a3white box\u0393\u00c7\u00a5 testing assisted by automatic test generation, assertions in a form of postconditions that also serve as preconditions, and fault localization.We also present our strategy for motivating end-user programmers to make use of the end-user software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["52"]}
{"title": "Empirical studies of control dependence graph size for c programs\n", "abstract": " Many tools and techniques for performing software engineering tasks require control-dependence information, represented in the form of control-dependence graphs. Worst-case analysis of these graphs has shown that their size may be quadratic in the number of statements in the procedure that they represent. Despite this result, two empirical studies suggest that in practice, the relationship between control-dependence graph size and program size is linear. These studies, however, were performed on a relatively small number of Fortran procedures, all of which were derived from numerical methods programs. To further investigate control-dependence size, we implemented tools for constructing the two most popular types of control-dependence graphs, and ran our tools on over 3000 C functions extracted from a wide range of source programs. Our results support the earlier conclusions about control\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["52"]}
{"title": "Terminator: Better automated ui test case prioritization\n", "abstract": " Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process.", "num_citations": "15\n", "authors": ["52"]}
{"title": "Recovery of object oriented features from c++ binaries\n", "abstract": " Reverse engineering is the process of examining and probing a program to determine the original design. Over the past ten years researchers have produced a number of capabilities to explore, manipulate, analyze, summarize, hyperlink, synthesize, componentize, and visualize software artifacts. Many reverse engineering tools focus on non-object-oriented software binaries with the goal of transferring discovered information into the software engineers trying to reengineer or reuse it. In this paper, we present a method that recovers object-oriented features from stripped C++ binaries. We discover RTTI information, class hierarchies, member functions of classes, and member variables of classes. The information obtained can be used for reengineering legacy software, and for understanding the architecture of software systems. Our method works for stripped binaries, i.e., Without symbolic or relocation information\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["52"]}
{"title": "Simlatte: A framework to support testing for worst-case interrupt latencies in embedded software\n", "abstract": " Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["52"]}
{"title": "An approach to testing commercial embedded systems\n", "abstract": " A wide range of commercial consumer devices such as mobile phones and smart televisions rely on embedded systems software to provide their functionality. Testing is one of the most commonly used methods for validating this software, and improved testing approaches could increase these devices\u0393\u00c7\u00d6 dependability. In this article we present an approach for performing such testing. Our approach is composed of two techniques. The first technique involves the selection of test data; it utilizes test adequacy criteria that rely on dataflow analysis to distinguish points of interaction between specific layers in embedded systems and between individual software components within those layers, while also tracking interactions between tasks. The second technique involves the observation of failures: it utilizes a family of test oracles that rely on instrumentation to record various aspects of a system's execution behavior, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["52"]}
{"title": "Oracle-based regression test selection\n", "abstract": " Regression test selection (RTS) techniques attempt to reduce regression testing costs by selecting a subset of a software system's test cases for use in testing changes made to that system. In practice, RTS techniques may select inordinately large sets of test cases, particularly when applied to industrial systems such as those developed at ABB, where code changes may have far-reaching impact. In this paper, we present a new RTS technique that addresses this problem by focusing on specific classes of faults that can be detected by internal oracles - oracles (rules) that enforce constraints on system states during system execution. Our technique uses program chopping to identify code changes that are relevant to internal oracles, and selects test cases that cover these changes. We present the results of an empirical study that show that our technique is more effective and efficient than other RTS techniques, relative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["52"]}
{"title": "The effect of test suite type on regression test selection\n", "abstract": " Regression test selection (RTS) techniques reduce the cost of regression testing by running only test cases related to code modifications. RTS techniques have been extensively researched, and the effects of several context factors on techniques have been empirically studied, but no prior work has explored the effects that might arise due to differences in types of test suites. We believe such differences may matter, and thus, we designed an empirical study to investigate them. Specifically, we consider two types of test suites obtained with automated test case generation techniques-feedback-directed random techniques and search-based techniques-along with manually written test suites. We assess the effects of these test suite types on two RTS techniques: a \"fine-grained\" technique that selects test cases based on dependencies tracked at the method level and a \"coarse-grained\" technique that selects test cases\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["52"]}
{"title": "Semantic characterization of MapReduce workloads\n", "abstract": " MapReduce is a platform for analyzing large amounts of data on clusters of commodity machines. MapReduce is popular, in part thanks to its apparent simplicity. However, there are unstated requirements for the semantics of MapReduce applications that can affect their correctness and performance. MapReduce implementations do not check whether user code satisfies these requirements, leading to time-consuming debugging sessions, performance problems, and, worst of all, silently corrupt results. This paper makes these requirements explicit, framing them as semantic properties and assumed outcomes. It describes a black-box approach for testing for these properties, and uses the approach to characterize the semantics of 23 non-trivial MapReduce workloads. Surprisingly, we found that for most requirements, there is at least one workload that violates it. This means that MapReduce may be simple to use, but\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["52"]}
{"title": "Testing properties of dataflow program operators\n", "abstract": " Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["52"]}
{"title": "A Collaborative Investigation of Program-Analysis-Based Testing and Maintenance\n", "abstract": " Throughout their lifetimes, software systems undergo numerous changes. To make these changes, we have to understand these systems, identify and evaluate alternative modi cation strategies, implement the changes, and validate their correctness. In practice, the cost of these activities is enormous 1, 2, 5, 6].Consider the problem of validating modi ed software. One common way to do this is to rerun tests from existing test suites (called regression testing). Although valuable, this is often very expensive. For instance, some companies must release software products for users who speak di erent languages. Typically, they release an initial version and then localize it for speci c languages. Before releasing a localized version, they regression test it. Since localized versions must be available as soon as possible, regression testing time must be reduced. Another common practice is to run regression tests periodically (one company we know of regressions tests every third weekend for 24 to 36 hours per testing session). In this case, management might wish to test more frequently, but the time required is too great. Because these problems are important and expensive, many researchers have investigated ways to solve them. One approach is to use program-analysis techniques, which analyze source code to help developers perform speci c testing and maintenance tasks. Even though these techniques can partially or fully automate testing and maintenance, they're rarely used in practice. One reason for this is their current inability to scale to large systems. Our preliminary research suggests that scalable, feasible, and cost-e ective program\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["52"]}
{"title": "Representation and analysis of software\n", "abstract": " This paper presents some basic techniques for representation and analysis of software. We use the term program to refer to both procedures and monolithic programs.", "num_citations": "10\n", "authors": ["52"]}
{"title": "Jitana: A modern hybrid program analysis framework for android platforms\n", "abstract": " Security vetting of Android apps is often performed under tight time constraints (e.g., a few minutes). As such, vetting activities must be performed \u0393\u00c7\u00a3at speed\u0393\u00c7\u00a5, when an app is submitted for distribution or a device is analyzed for malware. Existing static and dynamic program analysis approaches are not feasible for use in security analysis tools because they require a much longer time to operate than security analysts can afford. There are two factors that limit the performance and efficiency of current analysis approaches. First, existing approaches analyze only one app at a time. Finding security vulnerabilities in collaborative environments such as Android, however, requires collaborating apps to be analyzed simultaneously. Thus, existing approaches are not adequate when applied in this context. Second, existing static program analysis approaches tend to operate in a \u0393\u00c7\u00a3closed world\u0393\u00c7\u00a5 fashion; therefore, they are not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["52"]}
{"title": "Automated refinement and augmentation of web service description files\n", "abstract": " Web Service Description Language (WSDL) is being increasingly used to specify web service interfaces. Specifications of this type, however, are often incomplete or imprecise. For example, cursory examination of the WSDL file for Amazon\u0393\u00c7\u00d6s E-Commerce Web Service reveals that it often uses a less specific type where a more specific type is applicable, or declares that elements could be missing where other documentation indicates that they are required. Further, specifications reflecting the temporal relationships between operations are completely missing, which is not surprising since they are not supported by the current WSDL standard. These problems in WSDL specifications can cause tools that use them to perform poorly or unreliably, and can mislead developers who rely on them. To address these problems, in this paper we present an automated methodology for collecting static and dynamic information about a web service, and using this information to suggest improvements to the WSDL file as well as providing complementary information about the behavior of the web service that cannot be captured by the WSDL. Additionally, we present the results of two case studies performed on commercial web services that show our methodology can find problems in WSDL files and suggest improvements.", "num_citations": "7\n", "authors": ["52"]}
{"title": "Experiments to assess the cost-benefits of test-suite reduction\n", "abstract": " Test-suite reduction techniques attempt to reduce the cost of saving and reusing test cases during software maintenance by eliminating redundant test cases from test suites. A potential drawback of these techniques is that in reducing a test suite they might reduce the ability of that test suite to reveal faults in the software. Previous studies suggested that test-suite reduction techniques can reduce test suite size without significantly reducing the fault-detection capabilities of test suites. To further investigate this issue we performed experiments in which we examined the costs and benefits of reducing test suites of various sizes for several programs and investigated factors that influence those costs and benefits. In contrast to the previous studies, our results reveal that the fault-detection capabilities of test suites can be severely compromised by test-suite reduction.", "num_citations": "7\n", "authors": ["52"]}
{"title": "Platinum: Reusing Constraint Solutions in Bounded Analysis of Relational Logic.\n", "abstract": " Alloy is a lightweight specification language based on relational logic, with an analysis engine that relies on SAT solvers to automate bounded verification of specifications. In spite of its strengths, the reliance of the Alloy Analyzer on computationally heavy solvers means that it can take a significant amount of time to verify software properties, even within limited bounds. This challenge is exacerbated by the ever-evolving nature of complex software systems. This paper presents PLATINUM, a technique for efficient analysis of evolving Alloy specifications, that recognizes opportunities for constraint reduction and reuse of previously identified constraint solutions. The insight behind PLATINUM is that formula constraints recur often during the analysis of a single specification and across its revisions, and constraint solutions can be reused over sequences of analyses performed on evolving specifications. Our empirical results show that PLAT-INUM substantially reduces (by 66.4% on average) the analysis time required on specifications extracted from real-world software systems.", "num_citations": "6\n", "authors": ["52"]}
{"title": "What happened to my application? Helping end users comprehend evolution through variation management\n", "abstract": " Context: Millions of end users are creating software applications. These end users typically do not have clear requirements in mind; instead, they debug their programs into existence and reuse their own or other persons\u0393\u00c7\u00d6 code. These behaviors often result in the creation of numerous variants of programs. Current end-user programming environments do not provide support for managing such variants.Objective: We wish to understand the variant creation behavior of end user programmers. Based on this understanding we wish to develop an automated system to help end user programmers efficiently manage variants.Method: We conducted an on-line survey to understand when and how end-user programmers create program variants and how they manage them. Our 124 survey respondents were recruited via email from among non-computer science majors who had taken at least one course in the computer\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["52"]}
{"title": "An automated analysis methodology to detect inconsistencies in web services with WSDL interfaces\n", "abstract": " Web Service Definition Language (WSDL) is being increasingly used to specify web service interfaces. Specifications of this type, however, are often incomplete or imprecise, which can create difficulties for client developers who rely on the WSDL files. To address this problem a semi\u0393\u00c7\u00c9automated methodology that probes a web service with semi\u0393\u00c7\u00c9automatically generated inputs and analyzes the resulting outputs is presented. The results of the analysis are compared to the original WSDL file and differences between the observed behavior of the service and the WSDL specifications are reported to the user. The methodology is applied in two case studies involving two popular commercial (Amazon and eBay) web services. The results show that the methodology can scale, and that it can uncover problems in the WSDL files that may impact a large number of clients. Copyright \u252c\u2310 2011 John Wiley & Sons, Ltd.", "num_citations": "6\n", "authors": ["52"]}
{"title": "Testing inter-layer and inter-task interactions in rtes applications\n", "abstract": " Real-time embedded systems (RTESs) are becoming increasingly ubiquitous, controlling a wide variety of popular and safety-critical devices. Effective testing techniques could improve the dependability of these systems. In this paper we present an approach for testing RTESs, intended specifically to help RTES application developers detect faults related to functional correctness. Our approach consists of two techniques that focus on exercising the interactions between system layers and between the multiple user tasks that enact application behaviors. We present results of an empirical study that shows that our techniques are effective at detecting faults.", "num_citations": "6\n", "authors": ["52"]}
{"title": "An ef perimental deter mination of su \u252c\u255d cient mutation operators\n", "abstract": " Mutation testing is a technique for unit testing software that, although powerful, is computationally expensive. The principal expense of mutation is that many variants of the test program, called mutants, must be repeatedly executed. This paper quanties the expense of mutation in terms of the number of mutants that are created, then proposes and evaluates a technique that reduces the number of mutants by an order of magnitude. Selective mutation reduces the cost of mutation testing by reducing the number of mutants. This paper reports experimental results that compare selective mutation testing with standard, or non-selective, mutation testing, and results that quantify the savings achieved by selective mutation testing. The results support the hypothesis that selective mutation is almost as strong as non-selective mutation; in experimental trials selective mutation provides almost the same coverage as non-selective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["52"]}
{"title": "Improving regression testing in continuous integration development environments (keynote)\n", "abstract": " In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. Merged code is then regression tested to help ensure that the codebase remains stable and that continuing engineering efforts can be performed more reliably. Continuous integration is advantageous because it can reduce the amount of code rework that is needed in later phases of development, and speed up overall development time. From a testing standpoint, however, continuous integration raises several challenges.", "num_citations": "5\n", "authors": ["52"]}
{"title": "An automated framework to support testing for process\u0393\u00c7\u00c9level race conditions\n", "abstract": " Race conditions are difficult to detect because they usually occur only under specific execution interleavings. Numerous program analysis and testing techniques have been proposed to detect race conditions between threads on single applications. However, most of these techniques neglect races that occur at the process level due to complex system event interactions. This article presents a framework, SIMEXPLORER, that allows engineers to effectively test for process\u0393\u00c7\u00c9level race conditions. SIMEXPLORER first uses dynamic analysis techniques to observe system execution, identify program locations of interest, and report faults related to oracles. Next, it uses virtualization to achieve the fine\u0393\u00c7\u00c9grained controllability needed to exercise event interleavings that are likely to expose races. We evaluated the effectiveness of SIMEXPLORER on 24 real\u0393\u00c7\u00c9world applications containing both known and unknown process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["52"]}
{"title": "A novel linear-order algorithm for adaptive random testing of programs with non-numeric inputs\n", "abstract": " Adaptive Random Testing (ART) is a family of testing techniques that aim to enhance the failuredetection effectiveness of random testing (RT) by spreading random test cases evenly throughout the input domain. ART has been empirically shown to be effective on software with numeric inputs. However, there are two aspects that need further improvement to make ART\u0393\u00c7\u00d6s adoption more widespread\u0393\u00c7\u00f6applicability to programs with non-numeric inputs, and the high computation overhead of many ART algorithms. We present a linear-order ART algorithm for software with non-numeric inputs. The key", "num_citations": "5\n", "authors": ["52"]}
{"title": "Software engineering for end-user programmers\n", "abstract": " There has been considerable work in empowering end users to be able to write their own programs, and as a result, end users are indeed doing so. In fact, the number of end-user programmers is expected to reach 55 million by 2005 in the US alone [2], writing programs using such devices as special-purpose scripting languages, multimedia and web authoring languages, and spreadsheets. Unfortunately, evidence from the spreadsheet paradigm, the most widely used of the end-user programming languages, abounds that end-user programmers are extremely prone to errors [15]. This problem is serious, because although some end users\u0393\u00c7\u00d6 programs are simply explorations and scratch pad calculations, others can be quite important to their personal or business livelihood, such as for calculating income taxes, ecommerce web pages, and financial forecasting.We would like to help reduce the error rate in the end-user programs that are important to the user. Although classical software engineering methodologies are not a panacea, there are several that are known to help reduce programming errors, and it would be useful to incorporate some of those successes in end-user programming. Toward this end, we have been working on a vision we call end-user software engineering, a holistic approach to the facets of software development in which end users engage. Its goal is to bring some of the gains from the software engineering community to end-user programming environments, without requiring training or even interest in traditional software engineering techniques.", "num_citations": "5\n", "authors": ["52"]}
{"title": "Does a visual \u0393\u00c7\u00a3testedness\u0393\u00c7\u00a5 methodology aid debugging\n", "abstract": " Spreadsheet languages are the most widely-used form of functional programming, and are used by a wide range of audiences from professional programmers to end users. We have been working to address problems that limit their usefulness, to understand how certain spreadsheet features promote their acceptance, and to make use of these results in other kinds of functional programming environments. One of the problems we would like to address\u0393\u00c7\u00f6in a way that is helpful to both programmers and end users\u0393\u00c7\u00f6is the high presence of logic errors in spreadsheet formulas. A central part of our strategy has been a program-analysis-based methodology to visually and incrementally guide the user into a systematic testing strategy. In this paper we describe an experiment on how this approach affects human productivity in debugging. Specifically, the experiment described in this paper explores the effect of the methodology\u0393\u00c7\u00d6s feedback about testedness on debugging effectiveness. The results show that testedness feedback significantly aids debugging accuracy and speed. However, we have found mixed evidence regarding effects on overconfidence.", "num_citations": "5\n", "authors": ["52"]}
{"title": "Energy efficiency\n", "abstract": " Encourage improvements in thermal efficiency in ALL new homes built in Huntingdonshire by ensuring they are built to the HIGHEST POSSIBLE LEVEL of the Code for Sustainable Homes & identify the most cost effective energy efficiency measures, likely to achieve the greatest reductions in carbon and facilitate their installation in existing households% of new dwellings built to levels of the code for sustainable homes Contributory projects Description Delivery year", "num_citations": "5\n", "authors": ["52"]}
{"title": "WYSIWYT testing in the spreadsheet paradigm\n", "abstract": " Is it possible to achieve some of the benefits of formal testing within the informal programming conventions of the spreadsheet paradigm? We have been working on an approach that attempts to do so via the development of a testing methodology for this paradigm. Our\" What You See Is What You Test\"(WYSIWYT) methodology supplements the convention by which spreadsheets provide automatic immediate visual feedback about values by providing automatic immediate visual feedback about\" testedness\". In previous work we described this methodology; in this paper, we present empirical data about the methodology's effectiveness. Our results show that the use of the methodology was associated with significant improvement in testing effectiveness and efficiency, even with no training on the theory of testing or test adequacy that the model implements. These results may be due at least in part to the fact that use of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["52"]}
{"title": "RETRACTED ARTICLE: Facilitating debugging of web applications through recording reduction\n", "abstract": " The Editors-in-Chief have retracted \u0393\u00c7\u00a3Facilitating debugging of web applications through recording reduction\u0393\u00c7\u00a5(https://doi. org/10.1007/s10664-017-9519-z) following an investigation by the University of Nebraska-Lincoln providing evidence of fabricated data used to evaluate the research results. Authors Ali Alakeel, Brian Burg, Gigon Bae, Gregg Rothermel agree to this retraction. Author Mouna Hammoudi has not responded to any correspondence from the editor or publisher about this retraction.", "num_citations": "4\n", "authors": ["52"]}
{"title": "Exploiting Domain-Specific Structures For End-User Programming Support Tools\n", "abstract": " In previous work we have tried to transfer ideas that have been successful in general-purpose programming languages and mainstream software engineering into the realm of spreadsheets, which is one important example of an end-user programming environment. More specifically, we have addressed the questions of how to employ the concepts of type checking, program generation and maintenance, and testing in spreadsheets. While the primary objective of our work has been to offer improvements for end-user productivity, we have tried to follow two particular principles to guide our research.(1) Keep the number of new concepts to be learned by end users at a minimum.(2) Exploit as much as possible information offered by the internal structure of spreadsheets. In this short paper we will illustrate our research approach with several examples.", "num_citations": "4\n", "authors": ["52"]}
{"title": "Distributed concolic algorithm of the SCORE framework\n", "abstract": " 2. Instrumentation. A target source program is statically instrumented with probes, which record symbolic path conditions from a concrete execution path when the target program is executed. For example, at each conditional branch, a probe is inserted to record the branch condition/symbolic path condition; then, the instrumented program is compiled into an executable binary file.", "num_citations": "3\n", "authors": ["52"]}
{"title": "Assessing the cost-benefits of using type inference algorithms to improve the representation of exceptional control flow in Java\n", "abstract": " Accurate representations of program control flow are important to the soundness and efficiency of program analysis and testing techniques. The Java programming language has introduced structured exception handling features that complicate the task of constructing safe and precise representations of the possible control flow in Java programs. Prior work has considered applying various type inference algorithms to exceptions, but has not yet investigated whether the use of higher cost algorithms is necessarily justified. It is important to understand and assess the tradeoffs associated with the use of more powerful yet costly algorithms, thus we conducted an empirical study to evaluate the relative performance of several such algorithms. We find that applying type inference to exceptions may improve representations of control flow, but that these improvements do not necessarily translate into benefits for practical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["52"]}
{"title": "End-user testing for the Lyee methodology using the screen transition paradigm and WYSIWYT\n", "abstract": " End-user specification of Lyee programs is one goal envisioned by the Lyee methodology. But with any software development effort comes the possibility of faults. Thus, providing end users a means to enter their own specifications is not enough; they must also be provided with the means to find faults in their specifications, in a manner that is appropriate not only for the end user's programming environment but also for his or her background. In this paper, we present an approach to solve this problem that marries two proven technologies for end users. One methodology for enabling end users to program is the screen transition paradigm. One useful visual testing methodology is \u0393\u00c7\u00ffWhat you see is what you test (WYSIWYT)\u0393\u00c7\u00d6. In this paper, we show that WYSIWYT test adequacy criteria can be used with the screen transition paradigm, and present a systematic translation from this paradigm to the formal model underlying\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["52"]}
{"title": "Prioritizing browser environments for web application test execution\n", "abstract": " When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["52"]}
{"title": "RRF: A race reproduction framework for use in debugging process-level races\n", "abstract": " Process-level races are endemic in modern systems. These races are difficult to debug because they are sensitive to execution events such as interrupts and scheduling. Unless a process interleaving that can result in the race can be found, it cannot be reproduced and cannot be corrected. In practice, however, the number of interleavings that can occur among processes in practice is large, and the patterns of interleavings can be complex. Thus, approaches for reproducing process-level races to date are often ineffective. In this paper, we present RRF, a race reproduction framework that can help software engineers reproduce reported process-level races, enabling them to potentially debug these races. RRF performs a hybrid analysis by leveraging existing static program analysis tools, dynamic kernel event reporting tools, and yield points to provide the observability and controllability needed to reproduce races\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["52"]}
{"title": "A scalable distributed concolic testing approach\n", "abstract": " Although testing is a standard method for improving the quality of software, conventional testing methods often fail to detect faults. Concolic testing attempts to remedy this by automatically generating test cases to explore execution paths in a program under test, helping testers achieve greater coverage of program behavior in a more automated fashion. Concolic testing, however, consumes a significant amount of computing time to explore execution paths, which is an obstacle toward its practical application. In this paper we describe a distributed concolic testing framework that utilizes large numbers of computing nodes to generate test cases in a scalable manner. We present the results of an empirical study that shows that the proposed framework can achieve a several orders-ofmagnitude increase in test case generation speed compared to the original concolic approach, and also demonstrates clear potential for scalability.", "num_citations": "2\n", "authors": ["52"]}
{"title": "Experimental program analysis\n", "abstract": " Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that certain program analysis techniques can be formulated as formal experiments. This article reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We also explore the applicability of experimental program analysis to three software engineering problems: program transformation, program debugging, and program understanding. Our findings indicate that experimental program analysis techniques can provide new and potentially improved\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["52"]}
{"title": "Position paper for EUSE 2007 at Dagstuhl\n", "abstract": " This brief position paper summarizes several facets of research underway by the Informal Learning in Software Development group at Pennsylvania State University. The focus of the work reported is on end user web development, with discussion of user needs and tools that might help to meet these needs.", "num_citations": "2\n", "authors": ["52"]}
{"title": "Helping End-User Programmers\n", "abstract": " While much of the software that people depend on is written by professional software engineers, increasingly, important applications are being created by non-professional (end-user) programmers. Using tools such as spreadsheet environments and web authoring tools, these programmers are creating software that is being used to support significant activities and inform decisions. Such software needs to work dependably and increase user productivity, but evidence shows that it frequently does not. For example, studies have shown that a large percentage of the spreadsheets created by end-users contain faults, and data suggests that time spent maintaining web macros may actually impede their users? overall efforts.", "num_citations": "2\n", "authors": ["52"]}
{"title": "An experimental determination of suficient mutation operators\n", "abstract": " Mutation testing is a technique for unit testing software that, although powerful, is computationallyexpensive. The principal expense of mutation is that many variants of the test program, called mutants, must be repeatedly executed. This paper quanties the expense of mutation in terms of the number of mutants that are created, then proposes and evaluates a technique that reduces the number of mutants by an order of magnitude. Selectivemutationreduces the cost ofmutationtesting byreducing the numberofmutants. This paper reports experimental results that compare selective mutation testing with standard, or non-selective, mutation testing, and results that quantify the savings achieved by selective mutation testing. The results support the hypothesis that selectivemutationis almostas strong as non-selective mutation; in experimental trials selective mutation provides almost the same coverage as non-selective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["52"]}
{"title": "A hybrid approach to testing for nonfunctional faults in embedded systems using genetic algorithms\n", "abstract": " Embedded systems are challenging to program correctly, because they use an interrupt\u0393\u00c7\u00c9driven programming paradigm and run in resource\u0393\u00c7\u00c9constrained environments. This leads to various classes of nonfunctional faults that can be detected only by customized verification techniques. These nonfunctional faults are specifically related to usage of resources such as time and memory. For example, the presence of interrupts can induce delays in interrupt servicing and in system execution time. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. As another example, stack overflows are caused when the combination of active methods and interrupt invocations on the stack grows too large, and these can lead to data loss and other significant device failures. To detect these types of nonfunctional faults, developers need to estimate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["52"]}
{"title": "Assessing the usefulness of type inference algorithms in representing Java control flow to support software maintenance tasks\n", "abstract": " A wide range of techniques for supporting software maintenance tasks rely on representations of program control flow. The accuracy of these representations can be important to the effectiveness and efficiency of these techniques. The Java programming language has introduced structured exception handling features that complicate the task of representing control flow. Previous work has attempted to address these complications by using type inference algorithms to analyze the control flow effects of exceptions, but to date, there has been no study of whether the use of these algorithms is justified. In this paper we report results of an empirical study addressing this issue. We find that type inference algorithms can lead to more accurate representations of control flow, but this improvement does not necessarily translate into benefits for maintenance techniques that use them. It follows that type inference algorithms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["52"]}
{"title": "A Methodology to Improve Dependability in Spreadsheets\n", "abstract": " Spreadsheets are one of the most commonly used end user programming environments. As such, there has been significant effort on the part of researchers and practitioners to develop methodologies and tools to improve the dependability of spreadsheets. Our work has focused on the development of the \u251c\u00e2\u252c\u00f3 \u251c\u00f3 \u0393\u00c7\u00dc\u252c\u00bc \u251c\u00e0 \u0393\u00c7\u00a3What You See Is What You Test\u251c\u00e2\u252c\u00f3 \u251c\u00f3 \u0393\u00c7\u00dc\u252c\u00bc \u251c\u00e9 (WYSIWYT) family of techniques. WYSIWYT is designed to be seamlessly integrated into a spreadsheet environment and the user\u251c\u00e2\u252c\u00f3 \u251c\u00f3 \u0393\u00c7\u00dc\u252c\u00bc \u251c\u00f3 \u0393\u00c7\u20a7\u252c\u00f3 s development processes. It uses visual devices that are integrated into the user\u251c\u00e2\u252c\u00f3 \u251c\u00f3 \u0393\u00c7\u00dc\u252c\u00bc \u251c\u00f3 \u0393\u00c7\u20a7\u252c\u00f3 s spreadsheet to guide the process of finding and fixing problems with the spreadsheet.", "num_citations": "1\n", "authors": ["52"]}
{"title": "Interdisciplinary Design Research for End-User Software Engineering\n", "abstract": " How does EUSE research build on empirical studies of programmers, and what kinds of empirical research might provide foundations for future EUSE research? My own work on interdisciplinary design draws comparisons across academic and professional boundaries, applying the results to the design of new technologies, and the critical assessment of technology.", "num_citations": "1\n", "authors": ["52"]}