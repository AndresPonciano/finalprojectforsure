{"title": "A global averaging method for dynamic time warping, with applications to clustering\n", "abstract": " Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging\u00a0\u2026", "num_citations": "731\n", "authors": ["1074"]}
{"title": "Satellite image time series analysis under time warping\n", "abstract": " Satellite Image Time Series are becoming increasingly available and will continue to do so in the coming years thanks to the launch of space missions which aim at providing a coverage of the Earth every few days with high spatial resolution. In the case of optical imagery, it will be possible to produce land use and cover change maps with detailed nomenclatures. However, due to meteorological phenomena, such as clouds, these time series will become irregular in terms of temporal sampling, and one will need to compare time series with different lengths. In this paper, we present an approach to image time series analysis which is able to deal with irregularly sampled series and which also allows the comparison of pairs of time series where each element of the pair has a different number of samples. We present the dynamic time warping from a theoretical point of view and illustrate its capabilities with two\u00a0\u2026", "num_citations": "284\n", "authors": ["1074"]}
{"title": "Spatio-temporal reasoning for the classification of satellite image time series\n", "abstract": " Satellite image time series (SITS) analysis is an important domain with various applications in land study. In the coming years, both high temporal and high spatial resolution SITS will become available. In the classical methodologies, SITS are studied by analyzing the radiometric evolution of the pixels with time. When dealing with high spatial resolution images, object-based approaches are generally used in order to exploit the spatial relationships of the data. However, these approaches require a segmentation step to provide contextual information about the pixels. Even if the segmentation of single images is widely studied, its generalization to series of images remains an open-issue. This article aims at providing both temporal and spatial analysis of SITS. We propose first segmenting each image of the series, and then using these segmentations in order to characterize each pixel of the data with a spatial\u00a0\u2026", "num_citations": "106\n", "authors": ["1074"]}
{"title": "Summarizing a set of time series by averaging: From Steiner sequence to compact multiple alignment\n", "abstract": " Summarizing a set of sequences is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. The definition of a consensus object is on the center of data analysis issues, since it crystallizes the underlying organization of the data.Dynamic Time Warping (DTW) is currently the most relevant similarity measure between sequences for a large panel of applications, since it makes it possible to capture temporal distortions. In this context, averaging a set of sequences is not a trivial task, since the average sequence has to be consistent with this similarity measure.The Steiner theory and several works in computational biology have pointed out the connection between multiple alignments and average sequences. Taking inspiration from these works, we introduce the notion of compact multiple alignment, which allows us to link these theories to the problem of summarizing\u00a0\u2026", "num_citations": "93\n", "authors": ["1074"]}
{"title": "Optimizing dynamic time warping\u2019s window width for time series data mining applications\n", "abstract": " Dynamic Time Warping (DTW) is a highly competitive distance measure for most time series data mining problems. Obtaining the best performance from DTW requires setting its only parameter, the maximum amount of warping (w). In the supervised case with ample data, w is typically set by cross-validation in the training stage. However, this method is likely to yield suboptimal results for small training sets. For the unsupervised case, learning via cross-validation is not possible because we do not have access to labeled data. Many practitioners have thus resorted to assuming that \u201cthe larger the better\u201d, and they use the largest value of w permitted by the computational resources. However, as we will show, in most circumstances, this is a na\u00efve approach that produces inferior clusterings. Moreover, the best warping window width is generally non-transferable between the two tasks, i.e., for a single dataset\u00a0\u2026", "num_citations": "47\n", "authors": ["1074"]}
{"title": "Efficient satellite image time series analysis under time warping\n", "abstract": " Earth observation satellites are now providing images with short revisit cycle and high spatial resolution. The amount of produced data requires new methods that will give a sound temporal analysis while being computationally efficient. Dynamic time warping has proved to be a very sound measure to capture similarities in radiometric evolutions. In this letter, we show that its nonlinear distortion behavior is compatible with the use of a spatiotemporal segmentation of the data cube that is formed by a satellite image time series (SITS). While dealing with spatial and temporal dimensions of SITS at the same time had already proven to be very challenging, this letter proves that, by taking advantage of the spatial and temporal connectivities, both the performance and the quality of the analysis can be improved. Our method is assessed on a SITS of 46 Formosat -2 images sensed in 2006, with an average cloud cover of\u00a0\u2026", "num_citations": "44\n", "authors": ["1074"]}
{"title": "Understanding concept drift\n", "abstract": " Concept drift is a major issue that greatly affects the accuracy and reliability of many real-world applications of machine learning. We argue that to tackle concept drift it is important to develop the capacity to describe and analyze it. We propose tools for this purpose, arguing for the importance of quantitative descriptions of drift in marginal distributions. We present quantitative drift analysis techniques along with methods for communicating their results. We demonstrate their effectiveness by application to three real-world learning tasks.", "num_citations": "35\n", "authors": ["1074"]}
{"title": "Analysing satellite image time series by means of pattern mining\n", "abstract": " Change detection in satellite image time series is an important domain with various applications in land study. Most previous works proposed to perform this detection by studying two images and analysing their differences. However, those methods do not exploit the whole set of images that is available today and they do not propose a description of the detected changes. We propose a sequential pattern mining approach for these image time series with two important features. First, our proposal allows for the analysis of all the images in the series and each image can be considered from multiple points of view. Second, our technique is specifically designed towards image time series where the changes are not the most frequent patterns that can be discovered. Our experiments show the relevance of our approach and the significance of our patterns.", "num_citations": "35\n", "authors": ["1074"]}
{"title": "Surgical motion analysis using discriminative interpretable patterns\n", "abstract": " ObjectiveThe analysis of surgical motion has received a growing interest with the development of devices allowing their automatic capture. In this context, the use of advanced surgical training systems makes an automated assessment of surgical trainee possible. Automatic and quantitative evaluation of surgical skills is a very important step in improving surgical patient care.Material and methodIn this paper, we present an approach for the discovery and ranking of discriminative and interpretable patterns of surgical practice from recordings of surgical motions. A pattern is defined as a series of actions or events in the kinematic data that together are distinctive of a specific gesture or skill level. Our approach is based on the decomposition of continuous kinematic data into a set of overlapping gestures represented by strings (bag of words) for which we compute comparative numerical statistic (tf-idf) enabling the\u00a0\u2026", "num_citations": "34\n", "authors": ["1074"]}
{"title": "Efficient parameter learning of Bayesian network classifiers\n", "abstract": " Recent advances have demonstrated substantial benefits from learning with both generative and discriminative parameters. On the one hand, generative approaches address the estimation of the parameters of the joint distribution\u2014, which for most network types is very computationally efficient (a notable exception to this are Markov networks) and on the other hand, discriminative approaches address the estimation of the parameters of the posterior distribution\u2014and, are more effective for classification, since they fit  directly. However, discriminative approaches are less computationally efficient as the normalization factor in the conditional log-likelihood precludes the derivation of closed-form estimation of parameters. This paper introduces a new discriminative parameter learning method for Bayesian network classifiers that combines in an elegant fashion parameters learned using both\u00a0\u2026", "num_citations": "27\n", "authors": ["1074"]}
{"title": "A multiple test correction for streams and cascades of statistical hypothesis tests\n", "abstract": " Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, ie~ rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be predetermined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even\u00a0\u2026", "num_citations": "24\n", "authors": ["1074"]}
{"title": "Judicious setting of Dynamic Time Warping's window width allows more accurate classification of time series\n", "abstract": " While the Dynamic Time Warping (DTW) - based Nearest-Neighbor Classification algorithm is regarded as a strong baseline for time series classification, in recent years there has been a plethora of algorithms that have claimed to be able to improve upon its accuracy in the general case. Many of these proposed ideas sacrifice the simplicity of implementation that DTW-based classifiers offer for rather modest gains. Nevertheless, there are clearly times when even a small improvement could make a large difference in an important medical or financial domain. In this work, we make an unexpected claim; an underappreciated \u201clow hanging fruit\u201d in optimizing DTW's performance can produce improvements that make it an even stronger baseline, closing most or all the improvement gap of the more sophisticated methods. We show that the method currently used to learn DTW's only parameter, the maximum amount of\u00a0\u2026", "num_citations": "22\n", "authors": ["1074"]}
{"title": "Automatic matching of surgeries to predict surgeons\u2019 next actions\n", "abstract": " ObjectiveMore than half a million surgeries are performed every day worldwide, which makes surgery one of the most important component of global health care. In this context, the objective of this paper is to introduce a new method for the prediction of the possible next task that a surgeon is going to perform during surgery.Material and MethodWe formulate the problem as finding the optimal registration of a partial sequence to a complete reference sequence of surgical activities. We propose an efficient algorithm to find the optimal partial alignment and a prediction system using maximum a posteriori probability estimation and filtering. We also introduce a weighting scheme allowing to improve the predictions by taking into account the relative similarity between the current surgery and a set of pre-recorded surgeries.ResultsOur method is evaluated on two types of neurosurgical procedures: lumbar disc herniation\u00a0\u2026", "num_citations": "20\n", "authors": ["1074"]}
{"title": "Discovering significant evolution patterns from satellite image time series\n", "abstract": " Satellite Image Time Series (SITS) provide us with precious information on land cover evolution. By studying these series of images we can both understand the changes of specific areas and discover global phenomena that spread over larger areas. Changes that can occur throughout the sensing time can spread over very long periods and may have different start time and end time depending on the location, which complicates the mining and the analysis of series of images. This work focuses on frequent sequential pattern mining (FSPM) methods, since this family of methods fits the above-mentioned issues. This family of methods consists of finding the most frequent evolution behaviors, and is actually able to extract long-term changes as well as short term ones, whenever the change may start and end. However, applying FSPM methods to SITS implies confronting two main challenges, related to the\u00a0\u2026", "num_citations": "20\n", "authors": ["1074"]}
{"title": "Discovering discriminative and interpretable patterns for surgical motion analysis\n", "abstract": " The analysis of surgical motion has received a growing interest with the development of devices allowing their automatic capture. In this context, the use of advanced surgical training systems make an automated assessment of surgical trainee possible. Automatic and quantitative evaluation of surgical skills is a very important step in improving surgical patient care. In this paper, we present a novel approach for the discovery and ranking of discriminative and interpretable patterns of surgical practice from recordings of surgical motions. A pattern is defined as a series of actions or events in the kinematic data that together are distinctive of a specific gesture or skill level. Our approach is based on the discretization of the continuous kinematic data into strings which are then processed to form bags of words. This step allows us to apply discriminative pattern mining technique based on the word occurrence\u00a0\u2026", "num_citations": "14\n", "authors": ["1074"]}
{"title": "Non-linear temporal scaling of surgical processes\n", "abstract": " ObjectiveSurgery is one of the riskiest and most important medical acts that is performed today. Understanding the ways in which surgeries are similar or different from each other is of major interest. Desires to improve patient outcomes and surgeon training, and to reduce the costs of surgery, all motivate a better understanding of surgical practices. To facilitate this, surgeons have started recording the activities that are performed during surgery. New methods have to be developed to be able to make the most of this extremely rich and complex data. The objective of this work is to enable the simultaneous comparison of a set of surgeries, in order to be able to extract high-level information about surgical practices.Materials and methodWe introduce non-linear temporal scaling (NLTS): a method that finds a multiple alignment of a set of surgeries. Experiments are carried out on a set of lumbar disc neurosurgeries. We\u00a0\u2026", "num_citations": "13\n", "authors": ["1074"]}
{"title": "Assessing the quality of temporal high-resolution classifications with low-resolution satellite image time series\n", "abstract": " Upcoming temporally and spatially high-resolution satellites such as Venus, SENTINEL-2, and Landsat Data Continuity Mission (LDCM) will provide very valuable data for land-cover and vegetation monitoring. However, owing to cloud cover and even to some rapid changes, a higher temporal resolution may be needed for some applications. In this work, we propose using the higher temporal resolution of satellites with mid to low spatial resolutions such as the upcoming PROBA-V. The aim of this work is to study how images provided by satellites with a lower spatial resolution but with a higher temporal one can be used to obtain information about the temporal classification derived from satellite image time series (SITS) provided by high-resolution satellites such as Venus, SENTINEL-2, or LDCM.We show that the low-spatial-resolution SITS can be used to inform about the stability and relevance of high-spatial\u00a0\u2026", "num_citations": "13\n", "authors": ["1074"]}
{"title": "Optimal sub-sequence matching for the automatic prediction of surgical tasks\n", "abstract": " Surgery is one of the riskiest and most important medical acts that is performed today. The desires to improve patient outcomes, surgeon training, and also to reduce the costs of surgery, have motivated surgeons to equip their Operating Rooms with sensors that describe the surgical intervention. The richness and complexity of the data that is collected calls for new machine learning methods to support pre-, peri- and post-surgery (before, during and after).                 This paper introduces a new method for the prediction of the next task that the surgeon is going to perform during the surgery (peri). Our method bases its prediction on the optimal matching of the current surgery to a set of pre-recorded surgeries.                 We assess our method on a set of neurosurgeries (lumbar disc herniation removal) and show that our method outperforms the state of the art by providing a prediction (of the next task that is\u00a0\u2026", "num_citations": "12\n", "authors": ["1074"]}
{"title": "Clustering of satellite image time series under time warping\n", "abstract": " Satellite Image Time Series are becoming increasingly available and will continue to do so in the coming years thanks to the launch of space missions which aim at providing a coverage of the Earth every few days with high spatial resolution. In the case of optical imagery, it will be possible to produce land use and cover change maps with detailed nomenclatures. However, due to meteorological phenomena, such as clouds, these time series will become irregular in terms of temporal sampling and one will need to compare irregularly sensed time series. In this paper, we present an approach to satellite image time series analysis which is able to both deal with irregularly sampled series and to capture distorted behaviors. We present the Dynamic Time Warping from a theoretical point of view and illustrate its abilities for satellite image time series clustering.", "num_citations": "12\n", "authors": ["1074"]}
{"title": "Surgical skills: Can learning curves be computed from recordings of surgical activities?\n", "abstract": " Purpose                 Surgery is one of the riskiest and most important medical acts that are performed today. The need to improve patient outcomes and surgeon training, and to reduce the costs of surgery, has motivated the equipment of operating rooms with sensors that record surgical interventions. The richness and complexity of the data that are collected call for new methods to support computer-assisted surgery. The aim of this paper is to support the monitoring of junior surgeons learning their surgical skill sets.                                               Methods                 Our method is fully automatic and takes as input a series of surgical interventions each represented by a low-level recording of all activities performed by the surgeon during the intervention (e.g., cut the skin with a scalpel). Our method produces a curve describing the process of standardization of the behavior of junior surgeons. Given the fact\u00a0\u2026", "num_citations": "11\n", "authors": ["1074"]}
{"title": "Finding discriminative and interpretable patterns in sequences of surgical activities\n", "abstract": " ObjectiveSurgery is one of the riskiest and most important medical acts that is performed today. Understanding the ways in which surgeries are similar or different from each other is of major interest to understand and analyze surgical behaviors. This article addresses the issue of identifying discriminative patterns of surgical practice from recordings of surgeries. These recordings are sequences of low-level surgical activities representing the actions performed by surgeons during surgeries.Materials and methodTo discover patterns that are specific to a group of surgeries, we use the vector space model (VSM) which is originally an algebraic model for representing text documents. We split long sequences of surgical activities into subsequences of consecutive activities. We then compute the relative frequencies of these subsequences using the tf*idf framework and we use the Cosine similarity to classify the\u00a0\u2026", "num_citations": "8\n", "authors": ["1074"]}
{"title": "Towards efficient satellite image time series analysis: Combination of dynamic time warping and quasi-flat zones\n", "abstract": " Satellite Image Time Series (SITS, for short) are useful resources for Earth monitoring. Upcoming satellites will provide a global coverage of the Earth's surface with a short revisit time (five days); a huge amount of data to analyze will be produced. In order to be able to analyze efficiently and accurately these images, new methods have to be designed. In this article, we propose to combine a spatio-temporal segmentation pre-processing method - quasi-flat zones, which have been recently extended to video analysis - and the distortion power of DTW to simplify the representation of the SITS, in order to reduce both the time and the memory consumption. Experiments carried out on a series of 46 images show that the memory consumption can be reduced by an order of magnitude without reducing the relevance of the analysis.", "num_citations": "7\n", "authors": ["1074"]}
{"title": "What can 100,000 books tell us about the international public library e-lending landscape?\n", "abstract": " Introduction: We investigated the relative availability of e-books to libraries for e-lending in five English-language countries, and analysed their licence terms and prices.Method: We created a unique dataset recording author, publisher, price and terms for 100,000 titles and 388,045 e-lending licences across Australia, New Zealand, Canada, the United States and United Kingdom via aggregator Overdrive. We developed new algorithms to estimate the original publication year for each title, and to match titles across jurisdictions.Analysis: We examined the relationships between title price, age, terms, jurisdiction, publisher and publisher type using various statistical analyses and machine learning.Results: Price and licence differences across countries are largely attributable to \u2018Big 5\u2019publishers. Prices are largely independent of title age (unless the title is in the public domain) or the rights libraries obtain in exchange. Licence terms are not affected by age either, meaning that the most restrictive terms are often applied to older, less demanded books.Conclusions: By setting terms independent of titles\u2019 value to libraries, publishers may discourage libraries from adding older and less-demanded books to their collections. We will test this hypothesis in a follow-up library survey.", "num_citations": "6\n", "authors": ["1074"]}
{"title": "A context-based approach for the classification of Satellite Image Time Series\n", "abstract": " Satellite Image Time Series (SITS) analysis is an important domain with various applications in land study. In the coming years, both high temporal and high spatial resolution SITS will be available. This article aims at providing both temporal and spatial analysis of SITS. We propose first segmenting each image of the series, and then using these segmentations in order to characterize each pixel of the data with a spatial dimension (i.e. with contextual information). Providing spatially characterized pixels, pixel-based temporal analysis can be performed. Experiments carried out with this methodology show the relevance of this approach and the significance of the resulting extracted patterns in the context of the analysis of SITS.", "num_citations": "6\n", "authors": ["1074"]}
{"title": "Deep Learning for an improved prediction of rainfall retrievals from commercial microwave links\n", "abstract": " Commercial microwave links (CMLs) have proven useful for providing rainfall information close to the ground surface. However, large uncertainties are associated with these retrievals, partly due to challenges in the type of data collection and processing. In particular, the most common case is when only minimum and maximum received signal levels (RSLs) over a given time interval (hereafter 15\u00a0min) are stored by mobile network operators. The average attenuation and the corresponding rainfall rate are then calculated based on a weighted average method using the minimum and maximum attenuation. In this study, an alternative to using a constant weighted average method is explored, based on a machine learning model trained to produce actual attenuation from minimum/maximum values. A rainfall retrieval deep learning model was designed based on a long short\u2010term memory (LSTM) model architecture\u00a0\u2026", "num_citations": "5\n", "authors": ["1074"]}
{"title": "Time Series Extrinsic Regression\n", "abstract": " This paper studies Time Series Extrinsic Regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting (TSF), relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.", "num_citations": "5\n", "authors": ["1074"]}
{"title": "Time series regression\n", "abstract": " This paper introduces Time Series Regression (TSR): a little-studied task of which the aim is to learn the relationship between a time series and a continuous target variable. In contrast to time series classification (TSC), which predicts a categorical class label, TSR predicts a numerical value. This task generalizes forecasting, relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and introduce this task, and benchmark possible solutions to tackling it on a novel archive of 19 TSR datasets which we have assembled. Our results show that the state-of-the-art TSC model Rocket, when adapted for regression, performs the best overall compared to other TSC models and state-of-the-art machine learning (ML) models such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much\u00a0\u2026", "num_citations": "5\n", "authors": ["1074"]}
{"title": "Experiments with learning graphical models on text\n", "abstract": " A rich variety of models are now in use for unsupervised modelling of text documents, and, in particular, a rich variety of graphical models exist, with and without latent variables. To date, there is inadequate understanding about the comparative performance of these, partly because they are subtly different, and they have been proposed and evaluated in different contexts. This paper reports on our experiments with a representative set of state of the art models: chordal graphs, matrix factorisation, and hierarchical latent tree models. For the chordal graphs, we use different scoring functions. For matrix factorisation models, we use different hierarchical priors, asymmetric priors on components. We use Boolean matrix factorisation rather than topic models, so we can do comparable evaluations. The experiments perform a number of evaluations: probability for each document, omni-directional prediction which\u00a0\u2026", "num_citations": "5\n", "authors": ["1074"]}
{"title": "Dynamic time warping: apports th\u00e9oriques pour l'analyse de donn\u00e9es temporelles: application \u00e0 la classification de s\u00e9ries temporelles d'images satellites\n", "abstract": " Les s\u00e9ries temporelles d\u2019images satellites (STIS) sont des donn\u00e9es cruciales pour l\u2019observation de la terre. Les s\u00e9ries temporelles actuelles sont soit des s\u00e9ries \u00e0 haute r\u00e9solution temporelle (Spot-V\u00e9g\u00e9tation, MODIS), soit des s\u00e9ries \u00e0 haute r\u00e9solution spatiale (Landsat). Dans les ann\u00e9es \u00e0 venir, les s\u00e9ries temporelles d\u2019images satellites \u00e0 hautes r\u00e9solutions spatiale et temporelle vont \u00eatre produites par le programme Sentinel de l\u2019ESA. Afin de traiter efficacement ces immenses quantit\u00e9s de donn\u00e9es qui vont \u00eatre produites (par exemple, Sentinel-2 couvrira la surface de la terre tous les cinq jours, avec des r\u00e9solutions spatiales allant de 10m \u00e0 60m et disposera de 13 bandes spectrales), de nouvelles m\u00e9thodes ont besoin d\u2019\u00eatre d\u00e9velopp\u00e9es. Cette th\u00e8se se focalise sur la comparaison des profils d\u2019\u00e9volution radiom\u00e9trique, et plus pr\u00e9cis\u00e9ment la mesure de similarit\u00e9 \u00abDynamic Time Warping\u00bb, qui constitue un outil permettant d\u2019exploiter la structuration temporelle des s\u00e9ries d\u2019images satellites.", "num_citations": "5\n", "authors": ["1074"]}
{"title": "A left-to-right algorithm for likelihood estimation in gamma-poisson factor analysis\n", "abstract": " Computing the probability of unseen documents is a natural evaluation task in\u00a0topic modeling. Previous work has addressed this problem for the well-known Latent Dirichlet Allocation (LDA) model. However, the same problem for a more general class of topic models, referred here to as Gamma-Poisson Factor Analysis (GaP-FA), remains unexplored, which hampers a fair comparison between models. Recent findings on the exact marginal likelihood of GaP-FA enable the derivation of a closed-form expression. In this paper, we show that its exact computation grows exponentially with the number of topics and non-zero words in a document, thus being only solvable for relatively small models and short documents. Experimentation in various corpus also indicates that existing methods in the literature are unlikely to accurately estimate this probability. With that in mind, we propose L2R, a left-to-right\u00a0\u2026", "num_citations": "4\n", "authors": ["1074"]}
{"title": "What happens when books enter the public domain?: Testing copyright's under use hypothesis across Australia, New Zealand, the United States and Canada\n", "abstract": " A key justification for copyright term extension has been that exclusive rights encourage publishers to make older works available (and that, without them, works will be 'underused'). We empirically test this hypothesis by investigating the availability of ebooks to public libraries across Australia, New Zealand, the United States and Canada. We find that titles are actually less available where they are under copyright, that competition apparently does not deter commercial publishers from investing in older works, and that the existence of exclusive rights is not enough to trigger investment in works with low commercial demand. Further, works are priced much higher when under copyright than when in the public domain. In sum, simply extending copyrights results in higher prices and worse access. We argue that nations should explore alternative ways of allocating copyrights to better achieve copyright's fundamental\u00a0\u2026", "num_citations": "3\n", "authors": ["1074"]}
{"title": "Automatic alignment of surgical videos using kinematic data\n", "abstract": " Over the past one hundred years, the classic teaching methodology of \u201csee one, do one, teach one\u201d has governed the surgical education systems worldwide. With the advent of Operation Room 2.0, recording video, kinematic and many other types of data during the surgery became an easy task, thus allowing artificial intelligence systems to be deployed and used in surgical and medical practice. Recently, surgical videos has been shown to provide a structure for peer coaching enabling novice trainees to learn from experienced surgeons by replaying those videos. However, the high inter-operator variability in surgical gesture duration and execution renders learning from comparing novice to expert surgical videos a very difficult task. In this paper, we propose a novel technique to align multiple videos based on the alignment of their corresponding kinematic multivariate time series data. By leveraging the\u00a0\u2026", "num_citations": "3\n", "authors": ["1074"]}
{"title": "Monitoring urban sprawl from satellite image time series\n", "abstract": " Satellite Image Time Series are becoming increasingly available and will continue to do so in the coming years thanks to the launch of space missions which aim at providing a coverage of the Earth every few days with high spatial resolution. In the case of optical imagery, it will be possible to produce land use and cover change maps with detailed nomenclatures. It has been shown that the Dynamic Time Warping (DTW) similarity measure makes it possible to compare radiometric time series with different lengths and sampling. This work aims at showing that DTW is also able to capture distorted phenomena sensed over a long satellite image time series. This article details the analysis of a satellite image time series sensed over 20 years; we show that DTW makes it possible to extract static phenomena, as well as distorted ones such as urbanized areas.", "num_citations": "3\n", "authors": ["1074"]}
{"title": "Introducing prior knowledge in temporal distances for Satellite Image Time Series analysis\n", "abstract": " Satellite Image Time Series are becoming increasingly available and will continue to do so in the coming years thanks to the launch of space missions which aim at providing a coverage of the Earth every few days with high spatial resolution. In the case of optical imagery, it will be possible to produce land use and cover change maps with detailed nomenclatures. It has been shown that the Dynamic Time Warping similarity measure is a consistent tool for the comparison of radiometric profiles of temporal evolution. Actually, it makes it possible to compare time series with both different lengths and different sampling. This property allows us to make the most of partially cloud-covered images, but also to transfer the knowledge learned on an agronomical year in order to classify the next year without using reference data. This article pursues this work on satellite image time series analysis and focuses on the\u00a0\u2026", "num_citations": "3\n", "authors": ["1074"]}
{"title": "Temporal domain adaptation under time warping\n", "abstract": " Satellite Image Time Series are becoming increasingly available and will continue to do so in the coming years thanks to the launch of space missions which aim at providing a coverage of the Earth every few days with high spatial resolution. In the case of optical imagery, it will be possible to produce land use and cover change maps with detailed nomenclatures. However, due to meteorological phenomena, such as clouds, these time series will become irregular in terms of temporal sampling and one will need to compare time series with different lengths. In this paper we present an approach to image time series analysis which is able to deal with irregularly sampled series and which also allows the comparison of pairs of time series where each element of the pair has a different number of samples. We present the Dynamic Time Warping from a theoretical point of view and illustrate its capabilities for domain\u00a0\u2026", "num_citations": "3\n", "authors": ["1074"]}
{"title": "Bayesian network classifiers using ensembles and smoothing\n", "abstract": " Bayesian network classifiers are, functionally, an interesting class of models, because they can be learnt out-of-core, i.e. without needing to hold the whole training data in main memory. The selective K-dependence Bayesian network classifier (SKDB) is state of the art in this class of models and has shown to rival random forest (RF) on problems with categorical data. In this paper, we introduce an ensembling technique for SKDB, called ensemble of SKDB (ESKDB). We show that ESKDB significantly outperforms RF on categorical and numerical data, as well as rivalling XGBoost. ESKDB combines three main components: (1) an effective strategy to vary the networks that is built by single classifiers (to make it an ensemble), (2) a stochastic discretization method which allows to both tackle numerical data as well as further increases the variance between different components of our ensemble and (3) a\u00a0\u2026", "num_citations": "2\n", "authors": ["1074"]}
{"title": "Seasonal averaged one-dependence estimators: a novel algorithm to address seasonal concept drift in high-dimensional stream classification\n", "abstract": " Stream classification methods classify a continuous stream of data as new labelled samples arrive. They often also have to deal with concept drift. This paper focuses on seasonal drift in stream classification, which can be found in many real-world application data sources. Traditional approaches of stream classification consider seasonal drift by including seasonal dummy/indicator variables or building separate models for each season. But these approaches have strong limitations in high-dimensional classification problems, or with complex seasonal patterns. This paper explores how to best handle seasonal drift in the specific context of news article categorization (or classification/tagging), where seasonal drift is overwhelmingly the main type of drift present in the data, and for which the data are high-dimensional. We introduce a novel classifier named Seasonal Averaged One-Dependence Estimators (SAODE\u00a0\u2026", "num_citations": "2\n", "authors": ["1074"]}
{"title": "Use of symbolic dynamic time warping in hierarchical clustering of urban fabric evolutions extracted from spatiotemporal topographic databases\n", "abstract": " This article introduces a new methodology dedicated to classify the evolutions of urban blocks extracted from spatio-temporal topographic databases where an urban block is defined as the smallest area that is surrounded by communication network (roads, railways,\u2026). To achieve that, an ascendant hierarchical clustering is applied to sequences of urban block states (ie, sequences of class labels to which the block belongs to at each date). The principal originality of this approach is to use a distance measure based on DTW (Dynamic Time Warping) which is able to apprehend temporal behaviors (mainly time lags in dates corresponding to a change of state) and which takes into account the semantic proximity between the different kinds of urban blocks. Several experiments have been carried out on areas in the city of Strasbourg (France). First results are relevant and highlight realistic urban dynamics.", "num_citations": "2\n", "authors": ["1074"]}
{"title": "Extraction de motifs d\u2019\u00e9volution dans les s\u00e9ries temporelles d\u2019images satellites\n", "abstract": " La d\u00e9tection de changements dans les s\u00e9ries temporelles d\u2019images satellites est un domaine important avec des applications vari\u00e9es en \u00e9tude de l\u2019occupation des sols. La plupart des m\u00e9thodes existantes effectuent cette d\u00e9tection en \u00e9tudiant les images deux \u00e0 deux et en consid\u00e9rant par exemple, leurs diff\u00e9rences. De fait, elles n\u2019exploitent pas la totalit\u00e9 des donn\u00e9es fournies par les images. De plus, elles ne proposent pas de caract\u00e9risation des changements observ\u00e9s. Cet article propose une approche d\u2019extraction de motifs d\u2019\u00e9volution de ces s\u00e9ries d\u2019images. Notre m\u00e9thode pr\u00e9sente deux propri\u00e9t\u00e9s importantes. Premi\u00e8rement, elle permet d\u2019analyser la totalit\u00e9 des donn\u00e9es fournies par les diff\u00e9rentes images, sans s\u00e9lection a priori. Deuxi\u00e8mement, cette m\u00e9thode a \u00e9t\u00e9 sp\u00e9cifiquement con\u00e7ue afin d\u2019extraire des motifs d\u2019\u00e9volutions non-majoritaires. Ce dernier point est particuli\u00e8rement utile pour l\u2019analyse de s\u00e9ries temporelles d\u2019images satellites car le comportement de non-\u00e9volution est souvent majoritaire. Nos exp\u00e9rimentations d\u00e9montrent la pertinence de notre approche pour l\u2019extraction de motifs d\u2019int\u00e9r\u00eat, et la signification des motifs d\u2019\u00e9volutions extraits.ABSTRACT. Change detection in satellite image time series is an important domain with various applications in land study. Most previous works proposed to perform this detection by studying two images and analysing their differences. However, those methods do not exploit the whole set of images that is available today and they do not propose a description of the detected changes. We propose a sequential pattern mining approach for these image time series with two important features\u00a0\u2026", "num_citations": "2\n", "authors": ["1074"]}
{"title": "Hierarchical Gradient Smoothing for Probability Estimation Trees\n", "abstract": " Decision trees are still seeing use in online, non-stationary and embedded contexts, as well as for interpretability. For applications like ranking and cost-sensitive classification, probability estimation trees (PETs) are used. These are built using smoothing or calibration techniques. Older smoothing techniques used counts local to a leaf node, but a few more recent techniques consider the broader context of a node when doing estimation. We apply a recent advanced smoothing method called Hierarchical Dirichlet Process (HDP) to PETs, and then propose a novel hierarchical smoothing approach called Hierarchical Gradient Smoothing (HGS) as an alternative. HGS smooths leaf nodes up to all the ancestors, instead of recursively smoothing to the parent used by HDP. HGS is made faster by efficiently optimizing the Leave-One-Out Cross-Validation (LOOCV) loss measure using gradient descent, instead of sampling\u00a0\u2026", "num_citations": "1\n", "authors": ["1074"]}
{"title": "Novel methods of incorporating time in longitudinal multivariate analysis reveals hidden associations with disease activity in systemic lupus erythematosus\n", "abstract": " Objective. Systemic lupus erythematosus (SLE) is a multisystem autoimmune disease. SLE is characterised by high inter-patient variability, including fluctuations over time, a factor which most biomarker studies omit from consideration. We investigated relationships between disease activity and biomarker expression in SLE, using novel methods to control for time-dependent variability, in a proof-of-concept study to evaluate whether doing so revealed additional information. Methods. We measured 4 serum biomarkers (MIF, CCL2, CCL19, CXCL10) and 13 routine clinical laboratory parameters, alongside disease activity measured by the SLE disease activity index-2k (SLEDAI-2k), collected longitudinally. We analysed these data with unsupervised learning methods via ensemble clustering, incorporating temporal relationships using dynamic time warping for distance metric calculation. Results. Data from 843 visits in 110 patients (median age 47, 83% female) demonstrated highly heterogeneous time-dependent relationships between disease activity and biomarkers. Unbiased magnitude-based hierarchical clustering of biomarker expression levels isolated a patient subset (n=9) with distinctively heterogeneous expression of the 17 biological parameters, and who had MIF, CCL2, CCL19 and CXCL10 levels that were higher and more strongly associated with disease activity, based on leave-one-out cross-validated regression analysis. In the remaining subgroup, a time-dependent regression model revealed significantly stronger predictive power of biomarkers for disease activity, compared to a time-agnostic regression model. Despite no\u00a0\u2026", "num_citations": "1\n", "authors": ["1074"]}
{"title": "Available\u2013But Not Accessible? Investigating Publisher E-Lending Licensing Practices\n", "abstract": " Introduction: We report our mixed-methods investigation of publishers\u2019 licensing practices, which affect the books public libraries can offer for e-lending.Method: We created unique datasets recording pricing, availability and licence terms for sampled titles offered by e-book aggregators to public libraries across Australia, New Zealand, Canada, the United States and United Kingdom. A third dataset records dates of availability for recent bestsellers. We conducted follow-up interviews with representatives of 5 e-book aggregators.Analysis: We quantitatively analysed availability, licence terms and price across all aggregators in Australia, snapshotting the competitive playing field in a single jurisdiction. We also compared availability and terms for the same titles from one aggregator across five jurisdictions, and measured how long it took for a sample of recent bestsellers to become available for e-lending. We used data from the aggregator interviews to explain the quantitative findings.Results: Contrary to aggregator expectations, we found considerable intra-jurisdictional price and licence differences. We also found numerous differences across jurisdictions.Conclusions: While availability was better than anticipated, licensing practices make it infeasible for libraries to purchase certain kinds of e-book (particularly older titles). Confidentiality requirements make it difficult for libraries to shop (and aggregators to compete) on price and terms.", "num_citations": "1\n", "authors": ["1074"]}
{"title": "Exact discovery of the most interesting sequential patterns under Leverage\n", "abstract": " This paper presents a framework for exact discovery of the most interesting sequential patterns. It combines (1) a novel definition of the expected support for a sequential pattern\u2013a concept on which most interestingness measures directly rely\u2013with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. We carry out experiments on both synthetic data with known patterns and realworld datasets; both experiments confirm the consistency and relevance of our approach.", "num_citations": "1\n", "authors": ["1074"]}
{"title": "Detecting land-cover modifications from multi-resolution satellite image time series\n", "abstract": " Frequent high-resolution images will be provided by new satellites such as Ven\u03bcs, SENTINEL-2 and Landsat Data Continuity Mission. Methods to handle this new type of data are currently developed (see [1] for an example). However, a more frequent observation of the surface of the Earth may be required for some applications. Moreover, the temporal resolution may be reduced by meteorological artifacts. In this work, we propose to take advantage of the higher temporal resolution of satellites with a lower spatial resolution to detect land-cover modification at a high spatial resolution. The proposed approach does not use any fusion step of the high- and low-resolution images. We show that the low spatial resolution satellite image time series (SITS) can be used in order to inform about the stability and relevance of the high spatial resolution classification. Experiments include a wide variety of resolution ratios and\u00a0\u2026", "num_citations": "1\n", "authors": ["1074"]}
{"title": "Description des alignements form\u00e9s par DTW\n", "abstract": " Ce document a pour but de d\u00e9crire des indices permettant de d\u00e9crire les alignements form\u00e9s par DTW entre deux s\u00e9quences. Il se focalisera surtout sur la description des alignements form\u00e9s par DTW entre une s\u00e9quence moyenne et une s\u00e9quence associ\u00e9e, mais le principe resterait identique si les indices \u00e9taient calcul\u00e9s entre deux s\u00e9quences.", "num_citations": "1\n", "authors": ["1074"]}