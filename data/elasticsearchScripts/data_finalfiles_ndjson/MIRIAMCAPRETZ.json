{"title": "Machine learning with big data: Challenges and approaches\n", "abstract": " The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other\u00a0\u2026", "num_citations": "472\n", "authors": ["355"]}
{"title": "Data management in cloud environments: NoSQL and NewSQL data stores\n", "abstract": " Advances in Web technology and the proliferation of mobile devices and sensors connected to the Internet have resulted in immense processing and storage requirements. Cloud computing has emerged as a paradigm that promises to meet these requirements. This work focuses on the storage aspect of cloud computing, specifically on data management in cloud environments. Traditional relational databases were designed in a different hardware and software era and are facing challenges in meeting the performance and scale requirements of Big Data. NoSQL and NewSQL data stores present themselves as alternatives that can handle huge volume of data. Because of the large number and diversity of existing NoSQL and NewSQL solutions, it is difficult to comprehend the domain and even more challenging to choose an appropriate solution for a specific task. Therefore, this paper reviews NoSQL and NewSQL\u00a0\u2026", "num_citations": "392\n", "authors": ["355"]}
{"title": "Challenges for mapreduce in big data\n", "abstract": " In the Big Data community, MapReduce has been seen as one of the key enabling approaches for meeting continuously increasing demands on computing resources imposed by massive data sets. The reason for this is the high scalability of the MapReduce paradigm which allows for massively parallel and distributed execution over a large number of computing nodes. This paper identifies MapReduce issues and challenges in handling Big Data with the objective of providing an overview of the field, facilitating better planning and management of Big Data projects, and identifying opportunities for future research in this field. The identified challenges are grouped into four main categories corresponding to Big Data tasks types: data storage (relational databases and NoSQL stores), Big Data analytics (machine learning and interactive analytics), online processing, and security and privacy. Moreover, current efforts\u00a0\u2026", "num_citations": "196\n", "authors": ["355"]}
{"title": "Mlaas: Machine learning as a service\n", "abstract": " The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.", "num_citations": "169\n", "authors": ["355"]}
{"title": "Energy forecasting for event venues: Big data and prediction accuracy\n", "abstract": " Advances in sensor technologies and the proliferation of smart meters have resulted in an explosion of energy-related data sets. These Big Data have created opportunities for development of new energy services and a promise of better energy management and conservation. Sensor-based energy forecasting has been researched in the context of office buildings, schools, and residential buildings. This paper investigates sensor-based forecasting in the context of event-organizing venues, which present an especially difficult scenario due to large variations in consumption caused by the hosted events. Moreover, the significance of the data set size, specifically the impact of temporal granularity, on energy prediction accuracy is explored. Two machine-learning approaches, neural networks (NN) and support vector regression (SVR), were considered together with three data granularities: daily, hourly, and 15\u00a0minutes\u00a0\u2026", "num_citations": "154\n", "authors": ["355"]}
{"title": "An ensemble learning framework for anomaly detection in building energy consumption\n", "abstract": " During building operation, a significant amount of energy is wasted due to equipment and human-related faults. To reduce waste, today's smart buildings monitor energy usage with the aim of identifying abnormal consumption behaviour and notifying the building manager to implement appropriate energy-saving procedures. To this end, this research proposes a new pattern-based anomaly classifier, the collective contextual anomaly detection using sliding window (CCAD-SW) framework. The CCAD-SW framework identifies anomalous consumption patterns using overlapping sliding windows. To enhance the anomaly detection capacity of the CCAD-SW, this research also proposes the ensemble anomaly detection (EAD) framework. The EAD is a generic framework that combines several anomaly detection classifiers using majority voting. To ensure diversity of anomaly classifiers, the EAD is implemented by\u00a0\u2026", "num_citations": "148\n", "authors": ["355"]}
{"title": "Contextual anomaly detection framework for big sensor data\n", "abstract": " The ability to detect and process anomalies for Big Data in real-time is a difficult task. The volume and velocity of the data within many systems makes it difficult for typical algorithms to scale and retain their real-time characteristics. The pervasiveness of data combined with the problem that many existing algorithms only consider the content of the data source; e.g. a sensor reading itself without concern for its context, leaves room for potential improvement. The proposed work defines a contextual anomaly detection framework. It is composed of two distinct steps: content detection and context detection. The content detector is used to determine anomalies in real-time, while possibly, and likely, identifying false positives. The context detector is used to prune the output of the content detector, identifying those anomalies which are considered both content and contextually anomalous. The context detector utilizes the concept of profiles, which are groups of similarly grouped data points generated by a multivariate clustering algorithm. The research has been evaluated against two real-world sensor datasets provided by a local company in Brampton, Canada. Additionally, the framework has been evaluated against the open-source Dodgers dataset, available at the UCI machine learning repository, and against the R statistical toolbox.", "num_citations": "107\n", "authors": ["355"]}
{"title": "Transfer learning with seasonal and trend adjustment for cross-building energy forecasting\n", "abstract": " Large scale smart meter deployments have resulted in popularization of sensor-based electricity forecasting which relies on historical sensor data to infer future energy consumption. Although those approaches have been very successful, they require significant quantities of historical data, often over extended periods of time, to train machine learning models and achieve accurate predictions. New buildings and buildings with newly installed meters have small historical datasets that are insufficient to create accurate predictions. Transfer learning methods have been proposed as a way to use cross-domain datasets to improve predictions. However, these methods do not consider the effects of seasonality within domains. Consequently, this paper proposes Hephaestus, a novel transfer learning method for cross-building energy forecasting based on time series multi-feature regression with seasonal and trend\u00a0\u2026", "num_citations": "84\n", "authors": ["355"]}
{"title": "A semantic big data platform for integrating heterogeneous wearable data in healthcare\n", "abstract": " Advances supported by emerging wearable technologies in healthcare promise patients a provision of high quality of care. Wearable computing systems represent one of the most thrust areas used to transform traditional healthcare systems into active systems able to continuously monitor and control the patients\u2019 health in order to manage their care at an early stage. However, their proliferation creates challenges related to data management and integration. The diversity and variety of wearable data related to healthcare, their huge volume and their distribution make data processing and analytics more difficult. In this paper, we propose a generic semantic big data architecture based on the \u201cKnowledge as a Service\u201d approach to cope with heterogeneity and scalability challenges. Our main contribution focuses on enriching the NIST Big Data model with semantics in order to smartly understand the collected\u00a0\u2026", "num_citations": "82\n", "authors": ["355"]}
{"title": "Myifogsim: A simulator for virtual machine migration in fog computing\n", "abstract": " Low latency in IT applications is an important aspect of improving the quality of the user's experience. Frequently, applications are run in a virtual machine in the cloud. Because cloud providers are datacentre facilities that are often distant from users, unacceptably high latencies are experienced in some applications. Fog computing can be seen as a cloud computing extension, namely cloudlets, located in access points at the edge of the network and hence able to provide lower latencies than the cloud. However, as mobile devices and applications become more popular, users' computing and data capacities should be maintained close to the user to keep latencies as low as possible. This paper discusses resource allocation in fog computing in the face of users' mobility and introduces MyiFogSim, an extension of iFogSim to support mobility through migration of virtual machines between cloudlets. Moreover, a\u00a0\u2026", "num_citations": "78\n", "authors": ["355"]}
{"title": "Contextual anomaly detection in big sensor data\n", "abstract": " Performing predictive modelling, such as anomaly detection, in Big Data is a difficult task. This problem is compounded as more and more sources of Big Data are generated from environmental sensors, logging applications, and the Internet of Things. Further, most current techniques for anomaly detection only consider the content of the data source, i.e. the data itself, without concern for the context of the data. As data becomes more complex it is increasingly important to bias anomaly detection techniques for the context, whether it is spatial, temporal, or semantic. The work proposed in this paper outlines a contextual anomaly detection technique for use in streaming sensor networks. The technique uses a well-defined content anomaly detection algorithm for real-time point anomaly detection. Additionally, we present a post-processing context-aware anomaly detection algorithm based on sensor profiles, which are\u00a0\u2026", "num_citations": "76\n", "authors": ["355"]}
{"title": "A dependency impact analysis model for web services evolution\n", "abstract": " As many software systems have been turned as Web services, the evolutionary changes of Web services are becoming an important issue. To understand the way in which the change affects the services, we must ascertain parts of the system that will be effected by the change and examine them for additional impacts. In this paper, we propose an impact analysis model based on service dependency. In particular, the service dependency graph model, service dependency and the relation matrix are examined. Based on the shift and calculation of the matrix, the dependency and impact of the service evolution can be analyzed and its quantity can be ascertained. Furthermore, we also represent an approach for service change annotation and for service evolution process. Overall, these works provide a foundation for the automatic management, control, and evaluation of service evolution.", "num_citations": "75\n", "authors": ["355"]}
{"title": "Online trust: Definition and principles\n", "abstract": " Trust is as significant a factor for successful online interactions as it is in offline communities. Trust is an important factor to predict the behaviour of an entity and as a criterion for an entity selection. Most trust studies focused on trust establishment without identifying and considering the main trust definition components and trust principles. This paper explores trust in the offline and the online world to extract important trust definition components and trust principles. The trust definition and principles are presented, which form a basis that should be followed to establish trust online.", "num_citations": "73\n", "authors": ["355"]}
{"title": "Component-based software development\n", "abstract": " Component-based software development (CBSD) strives to achieve a set of pre-built, standardized software components available to fit a specific architectural style for some application domain; the application is then assembled using these components. Component-based software reusability will be at the forefront of software development technology in the next few years. This paper describes a software life cycle that supports component-based development under an object-oriented framework. Development time versus software life cycle phases, which is an important assessment of the component-based development model put forward, is also mentioned.", "num_citations": "63\n", "authors": ["355"]}
{"title": "Knowledge as a service framework for disaster data management\n", "abstract": " Each year, a number of natural disasters strike across the globe, killing hundreds and causing billions of dollars in property and infrastructure damage. Minimizing the impact of disasters is imperative in today's society. As the capabilities of software and hardware evolve, so does the role of information and communication technology in disaster mitigation, preparation, response, and recovery. A large quantity of disaster-related data is available, including response plans, records of previous incidents, simulation data, social media data, and Web sites. However, current data management solutions offer few or no integration capabilities. Moreover, recent advances in cloud computing, big data, and NoSQL open the door for new solutions in disaster data management. In this paper, a Knowledge as a Service (KaaS) framework is proposed for disaster cloud data management (Disaster-CDM), with the objectives of 1\u00a0\u2026", "num_citations": "59\n", "authors": ["355"]}
{"title": "CEPSim: Modelling and simulation of Complex Event Processing systems in cloud environments\n", "abstract": " The emergence of Big Data has had profound impacts on how data are stored and processed. As technologies created to process continuous streams of data with low latency, Complex Event Processing (CEP) and Stream Processing (SP) have often been related to the Big Data velocity dimension and used in this context. Many modern CEP and SP systems leverage cloud environments to provide the low latency and scalability required by Big Data applications, yet validating these systems at the required scale is a research problem per se. Cloud computing simulators have been used as a tool to facilitate reproducible and repeatable experiments in clouds. Nevertheless, existing simulators are mostly based on simple application and simulation models that are not appropriate for CEP or for SP. This article presents CEPSim, a simulator for CEP and SP systems in cloud environments. CEPSim proposes a query\u00a0\u2026", "num_citations": "49\n", "authors": ["355"]}
{"title": "Storing massive Resource Description Framework (RDF) data: a survey\n", "abstract": " The Resource Description Framework (RDF) is a flexible model for representing information about resources on the Web. As a W3C (World Wide Web Consortium) Recommendation, RDF has rapidly gained popularity. With the widespread acceptance of RDF on the Web and in the enterprise, a huge amount of RDF data is being proliferated and becoming available. Efficient and scalable management of RDF data is therefore of increasing importance. RDF data management has attracted attention in the database and Semantic Web communities. Much work has been devoted to proposing different solutions to store RDF data efficiently. This paper focusses on using relational databases and NoSQL (for \u2018not only SQL (Structured Query Language)\u2019) databases to store massive RDF data. A full up-to-date overview of the current state of the art in RDF data storage is provided in the paper.", "num_citations": "47\n", "authors": ["355"]}
{"title": "Energy consumption prediction with big data: Balancing prediction accuracy and computational resources\n", "abstract": " In recent years, advances in sensor technologies and expansion of smart meters have resulted in massive growth of energy data sets. These Big Data have created new opportunities for energy prediction, but at the same time, they impose new challenges for traditional technologies. On the other hand, new approaches for handling and processing these Big Data have emerged, such as MapReduce, Spark, Storm, and Oxdata H2O. This paper explores how findings from machine learning with Big Data can benefit energy consumption prediction. An approach based on local learning with support vector regression (SVR) is presented. Although local learning itself is not a novel concept, it has great potential in the Big Data domain because it reduces computational complexity. The local SVR approach presented here is compared to traditional SVR and to deep neural networks with an H2O machine learning platform\u00a0\u2026", "num_citations": "42\n", "authors": ["355"]}
{"title": "Collective contextual anomaly detection framework for smart buildings\n", "abstract": " Buildings are responsible for a significant amount of total global energy consumption and as a result account for a substantial portion of overall carbon emissions. Moreover, buildings have a great potential for helping to meet energy efficiency targets. Hence, energy saving goals that target buildings can have a significant contribution in reducing environmental impact. Today's smart buildings achieve energy efficiency by monitoring energy usage with the aim of detecting and diagnosing abnormal energy consumption behaviour. This research proposes a generic collective contextual anomaly detection (CCAD) framework that uses sliding window approach and integrates historic sensor data along with generated and contextual features to train an autoencoder to recognize normal consumption patterns. Subsequently, by determining a threshold that optimizes sensitivity and specificity, the framework identifies\u00a0\u2026", "num_citations": "40\n", "authors": ["355"]}
{"title": "Integration of business process modeling and Web services: a survey\n", "abstract": " A significant challenge in business process automation involves bridging the gap between business process representations and Web service technologies that implement business activities. We are interested in business process representations such as Business Process Modeling Notation (BPMN) and Event-Driven Process Chains (EPCs). Web service technologies include protocols such as Simple Object Access Protocol (SOAP), architectures such as REpresentational State Transfer (RESTful), or semantic description languages and formalisms such as Web Ontology Language for Services (OWL-S) and Web Service Modeling Ontology (WSMO). This paper reviews previous work on the integration of business process representations and Web service technologies. It provides a perspective on the field by summarizing, organizing, and classifying the proposed approaches. Consequently, this study has\u00a0\u2026", "num_citations": "40\n", "authors": ["355"]}
{"title": "An approach for SDN traffic monitoring based on big data techniques\n", "abstract": " Software-defined networking overcomes the limitations of traditional networks by splitting the control plane from the data plane. The logic of the network is moved to a component called the controller that manages devices in the data plane. To implement this architecture, it has become the norm to use the OpenFlow (OF) protocol, which defines several counters maintained by network devices. These counters are the starting point for Traffic Engineering (TE) activities. TE monitors several network parameters, including network bandwidth utilization. A great challenge for TE is to collect and generate statistics about bandwidth utilization for monitoring and traffic analysis activities. This becomes even more challenging if fine-grained monitoring is required. Network management tasks such as network provisioning, capacity planning, load balancing, and anomaly detection can benefit from this fine-grained monitoring\u00a0\u2026", "num_citations": "39\n", "authors": ["355"]}
{"title": "A systematic review of convolutional neural network-based structural condition assessment techniques\n", "abstract": " With recent advances in non-contact sensing technology such as cameras, unmanned aerial and ground vehicles, the structural health monitoring (SHM) community has witnessed a prominent growth in deep learning-based condition assessment techniques of structural systems. These deep learning methods rely primarily on convolutional neural networks (CNNs). The CNN networks are trained using a large number of datasets for various types of damage and anomaly detection and post-disaster reconnaissance. The trained networks are then utilized to analyze newer data to detect the type and severity of the damage, enhancing the capabilities of non-contact sensors in developing autonomous SHM systems. In recent years, a broad range of CNN architectures has been developed by researchers to accommodate the extent of lighting and weather conditions, the quality of images, the amount of background and\u00a0\u2026", "num_citations": "34\n", "authors": ["355"]}
{"title": "Forecasting residential energy consumption: Single household perspective\n", "abstract": " With the development of smart electricity metering technologies, huge amounts of consumption data can be retrieved on a daily and hourly basis. Energy consumption forecasting facilitates electricity demand management and utilities load planning. Most studies have been focussed on commercial customers or residential building-level energy consumption, or have used behavioral and occupancy sensor data to characterize an individual household's electrical consumption. This study has analyzed energy consumption at single household level using smart meter data to improve residential energy services and gain insights into planning demand response programs. Electricity consumption for anonymous individual households has been predicted using a Support Vector Regression (SVR) modelling with both daily and hourly data granularity. The electricity usage data set for 2014 to 2016 was obtained from a\u00a0\u2026", "num_citations": "34\n", "authors": ["355"]}
{"title": "Metamodel for privacy policies within SOA\n", "abstract": " As service-oriented architecture (SOA) continues to grow as a viable approach to systems development, so too does the number of services available. The strength of services in an SOA environment to provide interoperability comes at the cost of reduced privacy, as more interactions between autonomous services require more information to be exchanged. In this paper we define a metamodel for privacy policy creation and comparison based on fair information practices introduced around the world to protect the privacy of individuals. We develop criteria for the comparison of the elements that compose the policies, creating hierarchical relationships between those elements that could not otherwise be directly compared. An example of two policies being compared is presented to demonstrate how this comparison can be done. We believe this definition of how to create and compare privacy policies forms a strong\u00a0\u2026", "num_citations": "31\n", "authors": ["355"]}
{"title": "A multi-agent framework for testing distributed systems\n", "abstract": " Software testing is a very expensive and time consuming process. It can account for up to 50% of the total cost of the software development. Distributed systems make software testing a daunting task. The research described in this paper investigates a novel multi-agent framework for testing 3-tier distributed systems. This paper describes the framework architecture as well as the communication mechanism among agents in the architecture. Web-based application is examined as a case study to validate the proposed framework. The framework is considered as a step forward to automate testing for distributed systems in order to enhance their reliability within an acceptable range of cost and time", "num_citations": "30\n", "authors": ["355"]}
{"title": "Intelligent security and access control framework for service-oriented architecture\n", "abstract": " One of the most significant difficulties with developing Service-Oriented Architecture (SOA) involves meeting its security challenges, since the responsibilities of SOA security are based on both the service providers and the consumers. In recent years, many solutions to these challenges have been implemented, such as the Web Services Security Standards, including WS-Security and WS-Policy. However, those standards are insufficient for the new generation of Web technologies, including Web 2.0 applications. In this research, we propose an intelligent SOA security framework by introducing its two most promising services: the Authentication and Security Service (NSS), and the Authorization Service (AS). The suggested autonomic and reusable services are constructed as an extension of WS-\u2217 security standards, with the addition of intelligent mining techniques, in order to improve performance and effectiveness\u00a0\u2026", "num_citations": "28\n", "authors": ["355"]}
{"title": "Service evolution patterns\n", "abstract": " Service evolution is the process of maintaining and evolving existing Web services to cater for new requirements and technological changes. In this paper, a service evolution model is proposed to analyze service dependencies, identify changes on services and estimate impact on consumers that will use new versions of these services. Based on the proposed service evolution model, four service evolution patterns are described: compatibility, transition, split-map, and merge-map. These proposed patterns provide reusable templates to encourage well-defined service evolution while minimizing issues that arise otherwise. They can be applied in the service evolution scenario where a single service is used by many, possibly unknown, consumers' applications. In such a scenario, providers evolve their services independently from consumers, which might cause unexpected errors and incur unpredicted impact on the\u00a0\u2026", "num_citations": "27\n", "authors": ["355"]}
{"title": "Software configuration management issues in the maintenance of existing systems\n", "abstract": " The application of the Software Configuration Management (SCM) discipline during the maintenance process of an existing poorly documented software system is vital to bring it under control. Incremental documentation, the activity of building up the software documentation whilst it is examined during the maintenance process, has a key role in such a process. The COMFORM (COnfiguration Management FORmalization for Maintenance) system provides a framework for aiding the maintenance process through the application of the SCM discipline. In this way reliable documentation of an existing software system is obtained incrementally whilst maintaining it. Our goal in the design of the COMFORM system is to define a method for the maintenance process through the use of forms representing each phase of a software maintenance model. This approach allows traceability among the software representations\u00a0\u2026", "num_citations": "27\n", "authors": ["355"]}
{"title": "Object-Oriented Software: Design.\n", "abstract": " This is a textbook for a course in object-oriented software engineering at advanced undergraduate and graduate levels, as well as for software engineers. It contains more than 120 exercises of diverse complexity. The book discusses fundamental concepts and terminology on object-oriented software development, assuming little background on software engineering, and emphasizes design and maintenance rather than programming. It also presents up-to-date and easily understood methodologies and puts forward a software life cycle model which explicitly encourages reusability during software development and maintenance.", "num_citations": "26\n", "authors": ["355"]}
{"title": "Privacy protection framework with defined policies for service-oriented architecture\n", "abstract": " Service-Oriented Architecture (SOA) is a computer systems design concept which aims to achieve reusability and integration in a distributed environment through the use of autonomous, loosely coupled, interoperable abstractions known as services. In order to interoperate, communication between services is very important due to their autonomous nature. This communication provides services with their functional strengths, but also creates the opportunity for the loss of privacy. In this paper, a Privacy Protection Framework for Service-Oriented Architecture (PPFSOA) is described. In this framework, a Privacy Service (PS) is used in combination with privacy policies to create privacy contracts that outline what can and cannot be done with a consumer\u2019s personally identifiable information (PII). The privacy policy consists of one-to-many privacy rules, with each rule created from a set of six privacy elements: collector, what, purpose, retention, recipient and trust. The PS acts as an intermediary between the service consumer and service provider, to establish an unbiased contract before the two parties begin sending PII. It is shown how many Privacy Services work together to form the privacy protection framework. An examination of what current approaches to protecting privacy in an SOA environment is also presented. Finally, the operations the PS must perform in order to fulfill its tasks are outlined.", "num_citations": "23\n", "authors": ["355"]}
{"title": "Trust-based service-oriented architecture\n", "abstract": " Service-Oriented Architecture (SOA) is an architectural style in building Web applications based on services. In SOA, the lack of trust between different parties affects the adoption of such architecture. Because trust is an important factor in successful online interactions, it is a major criterion for service selection. In the context of online services and SOA, the literature shows that the field of trust is not mature. The definitions of trust and its essential aspects do not reflect the true nature of trust online. This paper proposes a comprehensive trust-based SOA solution based on an identified trust definition and its principles for selecting services based on their trustworthiness. In particular, SOA is extended and a new component, the trust framework, which is responsible for the trust process, is added to the architecture. Consequently, its components are identified and built. The trust-based SOA is implemented through\u00a0\u2026", "num_citations": "21\n", "authors": ["355"]}
{"title": "Metrics suite for class complexity\n", "abstract": " Though software engineering seems to focus on the development of software, most of the funds are spent maintaining and testing the systems. The ability to predict effort required to test and maintain is valuable to an organization interested in allocating project resources accordingly. This paper reviews the metrics cyclomatic complexity and weighted methods per class. The flaws in weighted methods per class are examined and a suite of metrics to determine and justify class complexity is proposed. This paper also discusses the design of a software tool that can be used to extract the values for the proposed metrics suite. The benefits of the software tool are also discussed.", "num_citations": "21\n", "authors": ["355"]}
{"title": "Dependency and entropy based impact analysis for service-oriented system evolution\n", "abstract": " Service-oriented architecture (SOA) has become a widely accepted approach that provides a flexible IT infrastructure in order to deal with the increasing pace of business changes and global competition. However, the evolution of SOA challenges traditional research methodologies and motivates us to explore new approaches for analyzing and evaluating the effects of change. In order to identify the basic principles that analyze the SOA evolution, this work will utilize dependency analysis and information entropy. Dependency analysis is applied in order to derive information from service dependencies to measure the relative importance of the service in a service-oriented system. In addition, we combine information entropy with dependency analysis to measure the service and system entropy. Consequently, we provide a quantificational approach based on dependency analysis and information entropy to estimate\u00a0\u2026", "num_citations": "20\n", "authors": ["355"]}
{"title": "Federated critical infrastructure simulators: Towards ontologies for support of collaboration\n", "abstract": " Our society relies greatly on a variety of critical infrastructures (CI), such as power system networks, water distribution, oil and natural gas systems, telecommunication networks and others. Interdependency between those systems is high and may result in cascading failures spanning different infrastructures. Behavior of each CI can be observed and analyzed through the use of domain simulators, but this does not account for their interdependency. To explore CI interdependencies, domain simulators need to be integrated in a federation where they can collaborate. This paper explores three different simulators: the EPANET water distribution simulator, the PSCAD power system simulator and the I2Sim infrastructure interdependency simulator. Each simulator's modeling approach is explored and their similarities and differences between modeling approaches are determined. Core ontology for each simulation\u00a0\u2026", "num_citations": "20\n", "authors": ["355"]}
{"title": "Quality of security service for web services within SOA\n", "abstract": " Service-Oriented Architecture (SOA) is a paradigm for creating and encapsulating business processes in the form of loose-coupling, autonomous and abstracted services. Managing the non-functional requirements of SOA such as security, is an over arching problem due to the wide variety of ways the service consumer can access the services offered by the service provider and the equally varied restrictions the service provider can set for gaining access by the service consumer. In this work, we propose a metadata for quality of security service for SOA. The proposed metadata provides different levels to describe the available variations of the authentication, authorization and privacy features that are related to SOA security. A Web Service for Quality of Security Service (QoSS) is then constructed to encapsulate the suggested metadata in order to assist the service consumer and provider to achieve a QoSS\u00a0\u2026", "num_citations": "19\n", "authors": ["355"]}
{"title": "Use of data mining to enhance security for SOA\n", "abstract": " Service-oriented architecture (SOA) is an architectural paradigm for developing distributed applications so that their design is structured on loosely coupled services such as Web services. One of the most significant difficulties with developing SOA concerns its security challenges, since the responsibilities of SOA security are based on both the servers and the clients. In recent years, a lot of solutions have been implemented, such as the Web services security standards, including WS-Security and WS-SecurityPolicy. However, those standards are completely insufficient for the promising new generations of Web applications, such as Web 2.0 and its upgraded edition, Web 3.0. In this work, we are proposing an intelligent security service for SOA using data mining to predict the attacks that could arise with SOAP (Simple Object Access Protocol) messages. Moreover, this service will validate the new security policies\u00a0\u2026", "num_citations": "19\n", "authors": ["355"]}
{"title": "Towards a base ontology for privacy protection in service-oriented architecture\n", "abstract": " The service consumer's confidence in the protection of their privacy is an important factor for the success of electronic services (e-services). It may increase if the service provider offers a description of its data practices. This description can be compared to what the consumer defines as appropriate practices. To allow the exchange of privacy-related descriptions and automatically compare them, the parties involved in the interaction should be able to use a common vocabulary. The goal of this paper is to present a base privacy ontology for e-services and a privacy framework for service-oriented architecture (SOA). The ontology offers a base vocabulary that can be extended to create ontologies specific to a given service domain and operating environment. The framework uses ontologies so that it can support service selection considering the consumer's privacy requirements. It extends SOA with provider policies and\u00a0\u2026", "num_citations": "17\n", "authors": ["355"]}
{"title": "A model for negotiation among agents based on the transaction analysis theory\n", "abstract": " In the research work presented, interactions among the various types of agents in autonomous decentralized systems are based on the theory of transaction analysis (TA) of psychology. Human beings are considered one particular type of autonomous agent. Hence, the agent's structure as well as interactions among them are defined and performed much the same as human behavior and attitudes. Cooperation among agents is negotiated by exchanging strokes. After the cooperation among agents is established, each of them has a specific role to perform in order to reach a particular goal. The paper presents the definition of the inner structure of an agent along with the negotiation process occurred among them until the cooperation is established.< >", "num_citations": "17\n", "authors": ["355"]}
{"title": "A gamification framework for sensor data analytics\n", "abstract": " The Internet of Things (IoT) enables connected objects to capture, communicate, and collect information over the network through a multitude of sensors, setting the foundation for applications such as smart grids, smart cars, and smart cities. In this context, large scale analytics is needed to extract knowledge and value from the data produced by these sensors. The ability to perform analytics on these data, however, is highly limited by the difficulties of collecting labels. Indeed, the machine learning techniques used to perform analytics rely upon data labels to learn and to validate results. Historically, crowdsourcing platforms have been used to gather labels, yet they cannot be directly used in the IoT because of poor human readability of sensor data. To overcome these limitations, this paper proposes a framework for sensor data analytics which leverages the power of crowdsourcing through gamification to acquire\u00a0\u2026", "num_citations": "16\n", "authors": ["355"]}
{"title": "Cepsim: A simulator for cloud-based complex event processing\n", "abstract": " As one of the Vs defining Big Data, data velocity brings many new challenges to traditional data processing approaches. The adoption of cloud environments in complex event processing (CEP) systems is a recent architectural style that aims to overcome these challenges. Validating cloud-based CEP systems at the required Big Data scale, however, is often a laborious, error-prone, and expensive task. This article presents CEPSim, a new simulator that has been developed to facilitate this validation process. CEPSim extends CloudSim, an existing cloud simulator, with an application model based on directed acyclic graphs that is used to represent continuous CEP queries. Once defined, the queries can be simulated in different cloud environments under diverse load conditions. Moreover, CEPSim is also customizable with different operator placement and scheduling strategies. These features enable researchers\u00a0\u2026", "num_citations": "16\n", "authors": ["355"]}
{"title": "Trust bootstrapping services and service providers\n", "abstract": " Trust is an important factor for successful online communication. Trust has been used as a criterion for service selection. Most trust and reputation studies assume a system where trust and reputations already exist. However, it is important to initialize trust rates for new services, which have no rating history, the so-called trust bootstrapping process. Trust bootstrapping assists the requestors in their service selection decision. Trust bootstrapping is the initial step in trust building process. Trust bootstrapping is important for reliable interaction with services and service providers that are new to the system. This paper proposes an approach for trust bootstrapping services and service providers. The proposed solution follows the trust principles and addresses a number of trust challenges in the literature. Experiment study is conducted and the results are analysed.", "num_citations": "16\n", "authors": ["355"]}
{"title": "Security protocols in service-oriented architecture\n", "abstract": " In this paper, a comprehensive Quality of Security Service (QoSS) model for addressing security within a Service-Oriented Architecture (SOA) is proposed. We define a detailed SOA security model that supports and incorporates a number of networking security techniques and protocols. It utilizes symmetric keys, public keys and hash functions techniques, in order to provide different levels of QoSS agreements to satisfy the requirements of both the services providers and requesters. These levels are based on core networking security requirements such as Mutual Authentication, Session keys, Anonymity, and Perfect forward Secrecy. In addition, the proposed model forms a strong line of defense against Replay, Man-in-the-Middle, and Denial-of-Services attacks.", "num_citations": "16\n", "authors": ["355"]}
{"title": "Deep neural networks with confidence sampling for electrical anomaly detection\n", "abstract": " The increase in electrical metering has created tremendous quantities of data and, as a result, possibilities for deep insights into energy usage, better energy management, and new ways of energy conservation. As buildings are responsible for a significant portion of overall energy consumption, conservation efforts targeting buildings can provide tremendous effect on energy savings. Building energy monitoring enables identification of anomalous or unexpected behaviors which, when corrected, can lead to energy savings. Although the available data is large, the limited availability of labels makes anomaly detection difficult. This research proposes a deep semi-supervised convolutional neural network with confidence sampling for electrical anomaly detection. To achieve semi-supervised learning, two sub-networks are used: the first performs reconstruction and uses unlabelled data, while the second performs\u00a0\u2026", "num_citations": "14\n", "authors": ["355"]}
{"title": "A hybrid particle swarm optimisation-genetic algorithm applied to grid scheduling\n", "abstract": " Scheduling problems have been thoroughly explored by the research community, but they acquire challenging characteristics in grid computing systems. In this context, it is important to have a scheduling strategy that can make efficient use of the available grid resources. This article focuses on the application of the particle swarm optimisation (PSO) meta-heuristic to the scheduling of independent users' jobs on grids. It is shown that the PSO method can achieve satisfactory results in simple problem instances, yet it has a tendency to stagnate around local minima in high-dimensional problems. Therefore, this research also proposes a novel hybrid particle swarm optimisation-genetic algorithm (H_PSO) method that aims to increase swarm diversity when a stagnation condition is detected. This new method is evaluated and compared with other heuristics and PSO formulations; the comparison shows that H_PSO can\u00a0\u2026", "num_citations": "14\n", "authors": ["355"]}
{"title": "Trust metrics for services and service providers\n", "abstract": " Trust is as significant a factor for successful online interactions as it is in offline communities. Trust is an important factor that is used as a criterion for service selection. There is a need to know information about services and service providers to establish trust and identify their trustworthiness. Most trust studies focus on trust establishment for services without clearly identifying trust information for services and service providers. Services and service providers traverse many domains with different properties and requirements. Identifying a unified trust information (trust metrics) for such an open environment is a challenge. This paper proposes a unified trust metrics classification for services and service providers. The proposed trust metrics can be extended and used in an open environment or within specific domains to establish trust for services and service providers.", "num_citations": "14\n", "authors": ["355"]}
{"title": "Collaborative knowledge as a service applied to the disaster management domain\n", "abstract": " Cloud computing offers services which promise to meet continuously increasing computing demands by using a large number of networked resources. However, data heterogeneity remains a major hurdle for data interoperability and data integration. In this context, a knowledge as a service (KaaS) approach has been proposed with the aim of generating knowledge from heterogeneous data and making it available as a service. In this paper, a collaborative knowledge as a service (CKaaS) architecture is proposed, with the objective of satisfying consumer knowledge needs by integrating disparate cloud knowledge through collaboration among distributed KaaS entities. The NIST cloud computing reference architecture is extended by adding a KaaS layer that integrates diverse sources of data stored in a cloud environment. CKaaS implementation is domain-specific; therefore, this paper presents its application to the\u00a0\u2026", "num_citations": "13\n", "authors": ["355"]}
{"title": "From Glossaries to Ontologies: Disaster Management Domain\n", "abstract": " Our society\u2019s reliance on a variety of critical infrastructures (CI) presents significant challenges for disaster preparedness, response and recovery. Experts from different domains including police, paramedics, firefighters and various other CI teams are involved in the fast paced response to a disaster, increasing the risk of miscommunication. To ensure clear communication, as well as to facilitate CI software interoperability, a common disaster ontology is needed. We propose using the knowledge stored in domain glossaries, vocabularies and dictionaries for the creation of a lightweight disaster management domain ontology. Glossaries, vocabularies and dictionaries are semi structured representations of domain knowledge, where significant human effort has been invested in choosing relevant terms, determining their definitions, acronyms, synonyms and sometimes even relations. We use that knowledge built into semi formatted documents for ontology learning. In particular, we look at five glossaries/vocabularies from the disaster management domain and analyze their content similarity and structure. A lightweight disaster ontology is created exploiting the structure of the semi-structured source documents.", "num_citations": "13\n", "authors": ["355"]}
{"title": "Trust in web services\n", "abstract": " Trust is an important factor to predict the behaviour of a Web Service and as a criterion for Web Service selection. Although considerable research has been performed in the offline and online worlds, analysis of trust in the Web Services environment has been limited. Most trust studies in Web Services are focused on trust establishment without identifying and considering the main trust definition components and trust principles. Thus, this paper presents trust definition and trust principles based on the exploration of the trust literature in the offline and online worlds and Web Services. The trust definition and principles form a basis to establish trust in Web Services.", "num_citations": "13\n", "authors": ["355"]}
{"title": "Application maintenance using software agents\n", "abstract": " The benefits of software agents as a tool for helping in the maintenance process of a software application are shown. The goal of this research was to develop a group of intelligent agents that worked together to aid in software maintenance by automatically informing the appropriate individuals of any changes that were made to an open-source Internet software application. This type of application is suited for intelligent agents because the source code is accessed and modified by many users on the Internet, meaning that the application is under constant change. The methodology of completion for this research can be subdivided into four categories: interface agent algorithm development, implementation using Visual C++, multi-agent system development, and testing. The overall goal is accomplished using a network of four agents each having a specific task; one to monitor the code base (Monitor Agent), one to\u00a0\u2026", "num_citations": "13\n", "authors": ["355"]}
{"title": "Predicting energy demand peak using M5 model trees\n", "abstract": " Predicting energy demand peak is a key factor for reducing energy demand and electricity bills for commercial customers. Features influencing energy demand are many and complex, such as occupant behaviours and temperature. Feature selection can decrease prediction model complexity without sacrificing performance. In this paper, features were selected based on their multiple linear regression correlation coefficients. This paper discusses the capabilities of M5 model trees in energy demand prediction for commercial buildings. M5 model trees are similar to regression trees, however they are more suitable for continuous prediction problems. The M5 model tree prediction was developed based on a selected feature set including sensor energy demand readings, day of the week, season, humidity, and weather conditions (sunny, rain, etc.). The performance of the M5 model tree was evaluated by comparing it\u00a0\u2026", "num_citations": "12\n", "authors": ["355"]}
{"title": "A CLC chloride channel plays an essential role in copper homeostasis in Aspergillus nidulans at increased extracellular copper concentrations\n", "abstract": " A putative CLC voltage-gated anion channel gene from Aspergillus nidulans (AnCLCA) is characterised. The expression of the AnCLCA cDNA restored the iron-limited growth of the Saccharomyces cerevisiae CLC null mutant strain (gef1) suggesting that AnCLCA functions as a chloride channel. An AnCLCA conditional mutant was created and exhibited a strong and specific growth inhibition in the presence of extracellular copper concentrations >\u00a018 \u03bcM. This sensitivity was shown to be the result of a hyper-accumulation of copper by the conditional mutant, which generates superoxide to toxic levels inhibiting the growth. Further analysis revealed that copper dependent enzymes were disrupted in the AnCLCA conditional null mutant, specifically, a reduced activity of the copper\u2013zinc superoxide dismutase (CuZn\u2013SOD) and enhanced activity of the cytochrome oxidase (COX). These results suggest that AnCLCA\u00a0\u2026", "num_citations": "12\n", "authors": ["355"]}
{"title": "Application of agents and intelligent information technologies\n", "abstract": " Intelligent agent technology is emerging as one of the most important and rapidly advancing areas. Researchers are developing a number of agent-based applications and multi-agent systems in a variety of fields, such as: electronic commerce, supply chain management, resource allocation, intelligent manufacturing, mass customization, industrial control, information retrieval and filtering, collaborative work, mobile commerce, decision support, and computer games. Application of Agents and Intelligent Information Technologies presents an outstanding collection of the latest research associated with intelligent agents and information technologies. Application of Agents and Intelligent Information Technologies provides a comprehensive analysis of issues related to agent design, implementation, integration, deployment, evaluation, and business value. This book presents research results and application of agents and other intelligent information technologies in various domains. Application of Agents and Intelligent Information Technologies offers the intelligent information technologies that will potentially revolutionize the work environment as well as social computing.", "num_citations": "12\n", "authors": ["355"]}
{"title": "Ontology\u2013based representation of simulation models\n", "abstract": " Ontologies have been used in a variety of domains for multiple purposes such as establishing common terminology, organizing domain knowledge and describing domain in a machine-readable form. Moreover, ontologies are the foundation of the Semantic Web and often semantic integration is achieved using ontology. Even though simulation demonstrates a number of similar characteristics to Semantic Web or semantic integration, including heterogeneity in the simulation domain, representation and semantics, the application of ontology in the simulation domain is still in its infancy. This paper proposes an ontology-based representation of simulation models. The goal of this research is to facilitate comparison among simulation models, querying, making inferences and reuse of existing simulation models. Specifically, such models represented in the domain simulation engine environment serve as an information source for their representation as instances of an ontology. Therefore, the ontology-based representation is created from existing simulation models in their proprietary file formats, consequently eliminating the need to perform the simulation modeling directly in the ontology. The proposed approach is evaluated on a case study involving the I2Sim interdependency simulator.", "num_citations": "11\n", "authors": ["355"]}
{"title": "Personalities, cultures and software modeling: Questions, scenarios and research directions\n", "abstract": " Several evolutions in software engineering (SE) relate to the development of a reliable communication process among the project stakeholders. The models resulting from these improvements are the key instruments of communication in SE. There are studies that relate several problems in SE to user-engineers interactions during the modeling process. In addition to the usual challenges related to the technological issues of software modeling, new problems have appeared due to the presence of a multinational workforce. Specifically, people with distinct cultural identities must work together and, independently from their personal values and beliefs, they must develop common objectives. Consequently, this paper argues that the personality and cultural identity of project team members might be unconsciously affecting the SE process in a much greater way than is currently believed. Moreover, a research framework\u00a0\u2026", "num_citations": "11\n", "authors": ["355"]}
{"title": "COMFORM-a software maintenance method based on the software configuration management discipline\n", "abstract": " COMFORM (configuration management formalisation for maintenance), a method which provides guidelines and procedures for carrying out a variety of activities performed during the maintenance process, is discussed. The proposed method accommodates a change control framework around which the software configuration management discipline is applied. The aim is to exert control over an existing software system while simultaneously incrementally redocumenting it.<>", "num_citations": "11\n", "authors": ["355"]}
{"title": "Query analyzer and manager for complex event processing as a service\n", "abstract": " Complex Event Processing (CEP) is a set of tools and techniques that can be used to obtain insights from high-volume, high-velocity continuous streams of events. CEP-based systems have been adopted in many situations that require prompt establishment of system diagnostics and execution of reaction plans, such as in monitoring of complex systems. This article describes the Query Analyzer and Manager (QAM) module, a first effort toward the development of a CEP as a Service (CEPaaS) system. This module is responsible for analyzing user-defined CEP queries and for managing their execution in distributed cloud-based environments. Using a language-agnostic internal query representation, QAM has a modular design that enables its adoption by virtually any CEP system.", "num_citations": "10\n", "authors": ["355"]}
{"title": "uos: A resource rerouting middleware for ubiquitous games\n", "abstract": " Ubiquitous computing (ubicomp) relies on the computation distributed over the environment to simplify the tasks performed by its users. A smart space is an instance of a ubiquitous environment, composed of a dynamic and heterogeneous set of devices that interact to support the execution of distributed smart applications. In this context, mobile devices provide new resources when they join the environment, which disappear when they leave it. This introduces the challenge of self-adaptation, in which smart applications may either include new resources as they become available or replace them when they become unavailable. Ubiquitous games combine ubicomp and computer game technologies to enrich user's experience and fun. Such games may benefit from different input and output resources offered by mobile devices. To support the development and deployment of ubiquitous games, this work presents the\u00a0\u2026", "num_citations": "10\n", "authors": ["355"]}
{"title": "A distributed method for solving nonlinear equations applying the power load flow calculation\n", "abstract": " A new approach for distributed power load flow calculation using nonlinear equations is presented. This approach, which is similar to the Newton Raphson's simple method, uses an inverse Jacobian matrix of initial states for the iteration process. Moreover, nonlinear quadratic equations have been used as they are more appropriate for the distributed power load flow calculation. The paper describes and compares the new approach with the Newton Raphson method. It shows that such an approach is more suitable for distributed power load flow calculation as well as discussing some of its applications.", "num_citations": "10\n", "authors": ["355"]}
{"title": "ARCog: An Aerial Robotics Cognitive Architecture\n", "abstract": " Efficient algorithm integration is a key issue in aerial robotics. However, only a few integration solutions rely on a cognitive approach. Cognitive approaches break down complex problems into independent units that may deal with progressively lower-level data interfaces, all the way down to sensors and actuators. A cognitive architecture defines information flow among units to produce emergent intelligent behavior. Despite the improvements in autonomous decision-making, several key issues remain open. One of these issues is the selection, coordination, and decision-making related to the several specialized tasks required for fulfilling mission objectives. This work addresses decision-making for the cognitive unmanned-aerial-vehicle architecture coined as ARCog. The proposed architecture lays the groundwork for the development of a software platform aligned with the requirements of the state-of-the-art\u00a0\u2026", "num_citations": "9\n", "authors": ["355"]}
{"title": "Towards a unified trust framework for trust establishment and trust based service selection\n", "abstract": " Trust is an important factor in online communication. Trust is used as a criteria for service selection. The first step to build a trust solution is to define trust and trust principles. Most current trust-based service selection approaches do not define trust and they establish trust for services by ranking services or finding the services' compliance and reputation. In addition, most studies do not consider trust principles that underpin trust concepts and do not address trust challenges that should be resolved as a part of the trust solution. This paper proposes a framework to build trust for services and to select services based on trust criteria. The main purpose is to move towards a unified framework that includes the main trust components to have a complete trust solution based on trust definition and trust principles.", "num_citations": "9\n", "authors": ["355"]}
{"title": "Cepaas: Complex event processing as a service\n", "abstract": " Complex Event Processing (CEP) is a technology for performing continuous operations on fast and distributed streams of data. By using CEP, companies can obtain real-time insights, create competitive advantage, and, ultimately unlock the potential of Big Data. Nevertheless, despite this recent surge of interest, the CEP market is still dominated by solutions that are costly and inflexible or too low-level and hard to operate. To overcome these adoption barriers, this research proposes the creation of a CEP as a Service (CEPaaS) system to provide CEP functionalities to users together with the advantages of the Software as a Service (SaaS) model, such as no up-front investment and low maintenance cost. To ensure the success of such a system, however, many complex requirements must be satisfied, such as low latency processing, fault tolerance, and query execution isolation. To satisfy these requirements, this\u00a0\u2026", "num_citations": "8\n", "authors": ["355"]}
{"title": "Furthering the growth of cloud computing by providing privacy as a service\n", "abstract": " The evolution of Cloud Computing as a viable business solution for providing hardware and software has created many security concerns. Among these security concerns, privacy is often overlooked. If Cloud Computing is to continue its growth, this privacy concern will need to be addressed. In this work we discuss the current growth of Cloud Computing and the impact the public sector and privacy can have in furthering this growth. To begin to provide privacy protection for Cloud Computing, we introduce privacy constraints that outline privacy preferences. We propose the expansion of Cloud Service Level Agreements (SLAs) to include these privacy constraints as Quality of Service (QoS) levels. This privacy QoS must be agreed upon along with the rest of the QoS terms within the SLA by the Cloud consumer and provider. Finally, we introduce Privacy as a Service (PraaS) to monitor the agreement and\u00a0\u2026", "num_citations": "8\n", "authors": ["355"]}
{"title": "A Component-Based Software Process\n", "abstract": " A component is a self-contained piece of software that has clearly defined functionality, open interfaces, and offers a plug-and-play service. Component-based software development is at the forefront of new approaches to producing large software systems. Issues concerning an alternative software development model for component-based software engineering are considered. The paper also covers other general aspects to form a better assessment of the component-based development model put forward, for instance, the types of the most frequently reused components, and an estimate of the time spent on each phase of the proposed model.", "num_citations": "8\n", "authors": ["355"]}
{"title": "A software maintenance method based on the software configuration management discipline\n", "abstract": " Software maintenance has until recently been the neglected phase in the software engineering process, despite the fact that maintenance of existing software systems may account for over half of all efforts expended by a software organization. Research into software maintenance, compared to other phases of the software engineering process is rare. Moreover, it is widely accepted that current software maintenance methods and techniques are unable to cope with the complexity inherent in maintaining software systems. This thesis is concerned with the development of a method, named Configuration Management Formalization for Maintenance (COMFORM), designed for the maintenance of existing software systems. COMFORM provides guidelines and procedures for carrying out a variety of activities performed during software maintenance. It accommodates a change control framework, around which the Software Configuration Management discipline is applied. Redocumentation is another problem tackled by COMFORM, which gathers together the documentation necessary to improve the maintainability and quality of existing software systems. This is achieved by the use of forms representing the output of each phase of a proposed software maintenance model. The information obtained by filling in forms is formalized according to a data model, which provides a common basis for the representation of the method's functionality. Finally, a prototype of COMFORM has been implemented, so that the procedures and guidelines set up by the method can be enforced and followed by its users.", "num_citations": "8\n", "authors": ["355"]}
{"title": "Energy slices: benchmarking with time slicing\n", "abstract": " Benchmarking makes it possible to identify low-performing buildings, establishes a baseline for measuring performance improvements, enables setting of energy conservation targets, and encourages energy savings by creating a competitive environment. Statistical approaches evaluate building energy efficiency by comparing measured energy consumption to other similar buildings typically using annual measurements. However, it is important to consider different time periods in benchmarking because of differences in their consumption patterns. For example, an office can be efficient during the night, but inefficient during operating hours due to occupants\u2019 wasteful behavior. Moreover, benchmarking studies often use a single regression model for different building categories. Selecting the regression model based on actual data would ensure that the model fits the data well. Consequently, this paper\u00a0\u2026", "num_citations": "7\n", "authors": ["355"]}
{"title": "Evaluation of particle swarm optimization applied to grid scheduling\n", "abstract": " The problem of scheduling independent users' jobs to resources in Grid Computing systems is of paramount importance. This problem is known to be NP-hard, and many techniques have been proposed to solve it, such as heuristics, genetic algorithms (GA), and, more recently, particle swarm optimization (PSO). This article aims to use PSO to solve grid scheduling problems, and compare it with other techniques. It is shown that many often-overlooked implementation details can have a huge impact on the performance of the method. In addition, experiments also show that the PSO has a tendency to stagnate around local minima in high-dimensional input problems. Therefore, this work also proposes a novel hybrid PSO-GA method that aims to increase swarm diversity when a stagnation condition is detected. The method is evaluated and compared with other PSO formulations, the results show that the new method\u00a0\u2026", "num_citations": "7\n", "authors": ["355"]}
{"title": "Extension of Object-oriented metrics suite for software maintenance\n", "abstract": " Software developers require information to understand the characteristics of systems, such as complexity and maintainability. In order to further understand and determine characteristics of object-oriented (OO) systems, this paper describes research that identifies attributes that are valuable in determining the difficulty in implementing changes during maintenance, as well as the possible effects that such changes may produce. A set of metrics are proposed to quantify and measure these attributes. The proposed complexity metrics are used to determine the difficulty in implementing changes through the measurement of method complexity, method diversity, and complexity density. The paper establishes impact metrics to determine the potential effects of making changes to a class and dependence metrics that are used to measure the potential effects on a given class resulting from changes in other classes. The case study shows that the proposed metrics provide additional information not sufficiently provided by the related existing OO metrics. The metrics are also found to be useful in the investigation of large systems, correlating with project outcomes.", "num_citations": "7\n", "authors": ["355"]}
{"title": "An improvement of a different approach for medical image storage\n", "abstract": " Novel telemedicine and picture archiving and communication systems (PACS) are enabling research for unique data storage and retrieval mechanisms. This paper presents an improved proposal that considers the HDF5 paradigm as a high performance environment for the storage and retrieval of medical images via PACS. Specifically, the previous proposal has demonstrated that new approaches using alternative ways of storing and retrieving medical data are necessary for improving system performance. Experimental results comparing the approach with a conventional database management system shows a successful improvement of the proposal.", "num_citations": "7\n", "authors": ["355"]}
{"title": "An authorization model for Web Services within SOA\n", "abstract": " One of the most significant difficulties with developing service-oriented architecture (SOA) concerns its security challenges. In particular, the authorization task is especially demanding because of the diverse access requirements within the various SOA environments, such as the business world, the academic setting and the industry atmosphere. In this paper, we propose a 4-attribute vector authorization model constructed in the form of an authorization service (AS). The proposed model is founded on the attribute-role based access control and contains an intelligent mining engine that facilitates the management of several authorization roles and provides flexibility to dynamically infer and assign the roles for a wide range of users and objects.", "num_citations": "7\n", "authors": ["355"]}
{"title": "A Multi-Agent Framework for Testing 3-Tier Distributed Systems Architecture.\n", "abstract": " Lack of time and capital for software testing is an oft encountered problem in most organizations. The plethora of possible test cases for each software component makes software testing a very expensive and time consuming activity and can, in extreme cases, account for up to 50% of the total cost of the software development process. The advent of distributed systems has augmented the complexity of the testing process and has made it an even more daunting task than before. This paper presents an innovative multi-agent framework to ease some of the burdens associated with the testing of generic distributed systems. The architecture of the proposed MAS (Multi-Agent System) is presented along with detailed descriptions of the various models arrived at following the analysis and design phases of the proposed system. The proposed framework is considered as a step towards the automation of the testing process for distributed systems in order to enhance their reliability within an acceptable range of cost and time.", "num_citations": "7\n", "authors": ["355"]}
{"title": "A Multi-Agent Framework for Building an Automatic Operational Profile\n", "abstract": " Since the early 1970s, researchers have proposed several models to improve software reliability. Among these, the operational profile approach is one of the most common. Operational profiles are a quantification of usage patterns for a software application. The research described in this paper investigates a novel multi-agent framework for automatically creating an operational profile for generic distributed systems after their release into the market. The operational profile in this paper is extended to comprise seven different profiles. Also, the criticality of operations is defined using a new composed metrics in order to organise the testing process as well as to decrease the time and cost involved in this process. The proposed framework is considered as a step towards making distributed systems intelligent and self-managing.", "num_citations": "7\n", "authors": ["355"]}
{"title": "ADAM: A multi-agent system for autonomous database administration and maintenance\n", "abstract": " In today\u2019s world, databases and database systems have become an essential component of everyday life, so much so that a life without DBMSs has become inconceivable. This article focuses on relational database management systems in particular, and proposes a novel and innovative multi-agent system that would autonomously and rationally administer and maintain databases. The proposed multi-agent system tool, ADAM, is in the form of a self-administering wrapper around database systems, and it addresses and offers a solution to the problem of overburdened and expensive DBAs with the objective of making databases a cost-effective option for small/medium-sized organizations. An implementation of the agent-based system to proactively or reactively identify and resolve a small subset of DBA tasks is discussed, and the GAIA methodology is used to outline the detailed analysis and design of the same\u00a0\u2026", "num_citations": "7\n", "authors": ["355"]}
{"title": "Design of a multi-agent system for autonomous database administration\n", "abstract": " As more and more businesses and services find themselves online due to the Internet phenomenon, a plethora of information sources is now available. These trends have rendered databases an indispensable component of daily life and have increased their complexity significantly, which has, in turn, amplified the workload on database administrators (DBAs). This has given rise to an increasing need for self-managing and self-administering databases. We present the analysis and design of a novel and innovative solution to address the problem of overburdened and expensive DBAs. We propose a self-administering wrapper around database systems in the form of an intelligent multi-agent system tool that autonomously and rationally administers and maintains relational databases. A planned implementation of the agent-based system which proactively or reactively identifies and resolves a small sub-set of DBA\u00a0\u2026", "num_citations": "7\n", "authors": ["355"]}
{"title": "Ml4iot: A framework to orchestrate machine learning workflows on internet of things data\n", "abstract": " Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "Attributed Graph Rewriting for Complex Event Processing Self-Management\n", "abstract": " The use of Complex Event Processing (CEP) and Stream Processing (SP) systems to process high-volume, high-velocity Big Data has renewed interest in procedures for managing these systems. In particular, self-management and adaptation of runtime platforms have been common research themes, as most of these systems run under dynamic conditions. Nevertheless, the research landscape in this area is still young and fragmented. Most research is performed in the context of specific systems, and it is difficult to generalize the results obtained to other contexts. To enable generic and reusable CEP/SP system management procedures and self-management policies, this research introduces the Attributed Graph Rewriting for Complex Event Processing Management (AGeCEP) formalism. AGeCEP represents queries in a language- and technology-agnostic fashion using attributed graphs. Query reconfiguration\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "Knowledge as a service framework for collaborative data management in cloud environments-disaster domain\n", "abstract": " Decision-making in disaster management requires information gathering, sharing, and integration by means of collaboration on a global scale and across governments, industries, and communities. Large volume of heterogeneous data is available; however, current data management solutions offer few or no integration capabilities and limited potential for collaboration. Moreover, recent advances in NoSQL, cloud computing, and Big Data open the door for new solutions in disaster data management. This chapter presents a Knowledge as a Service (KaaS) framework for disaster cloud data management (Disaster-CDM), with the objectives of facilitating information gathering and sharing; storing large amounts of disaster-related data; and facilitating search and supporting interoperability and integration. In the Disaster-CDM approach NoSQL data stores provide storage reliability and scalability while service-oriented\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "Network and energy-aware resource selection model for opportunistic grids\n", "abstract": " Due to increasing hardware capacity, computing grids have been handling and processing more data. This has led to higher amount of energy being consumed by grids, hence the necessity for strategies to reduce their energy consumption. Scheduling is a process carried out to define in which node tasks will be executed in the grid. This process can significantly impact the global system performance, including energy consumption. This paper focuses on a scheduling model for opportunistic grids that considers network traffic, distance between input files and execution node as well as the execution node status. The model was tested in a simulated environment created using Green Cloud. The simulation results of this model compared to a usual approach show a total power consumption savings of 7.10%.", "num_citations": "6\n", "authors": ["355"]}
{"title": "ODEP-DPS: Ontology-driven engineering process for the collaborative development of semantic data providing services\n", "abstract": " ContextData services are services that handle operations involving the management of data. A problem with data services is that their interfaces are defined by their syntax alone. Consequently, Data Providing Services (DPSs) have been proposed to explicitly define semantics using ontologies for services that only retrieve data. However, the semantic annotations of DPSs are developed as afterthoughts to deployed data services.ObjectiveThe objective of this work is to present a DPS development process that considers all of a DPS\u2019s dimensions including its data acquisition logic, syntax and semantics thus addressing the issue of semantic annotations developed as afterthoughts. This shall decrease the cost of deploying and maintaining DPSs.MethodThis paper contributes a holistic and collaborative process \u2013 ODEP-DPS \u2013 for the development of DPSs. It is holistic as it considers both semantics and syntax from\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "A privacy manager for collaborative working environments\n", "abstract": " The ability to collaborate has long been key to the successful completion of tasks. With the availability of current networking and computing power, the creation of Collaborative Working Environments (CWEs) has allowed for this process to occur over virtually any geographic distance. While the strength of a CWE is its ability to allow for the exchange of information freely between collaborators, this opens the participants up to a possible loss of privacy. In this paper, the issue of protecting privacy while collaborating is discussed. To address the privacy concerns that are raised, a generic privacy ontology is presented, along with a Collaborative Privacy Manager (CPM). The generic privacy ontology allows for the creation of privacy policies at several layers of granularity. The architecture of the CPM, consisting of several levels and modules, is introduced. The functions this CPM will play to ensure privacy in collaborative\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "A multi-layered approach for the declarative development of data providing services\n", "abstract": " Data Providing Services (DPSs) have the sole purpose of retrieving data from existing sources according to their input parameters while also providing a semantic description of the data they provide using a parametrized view over a domain ontology. A layered model of viewing DPSs is proposed consisting of the data acquisition, syntactic and semantic layers. It is shown that by defining all three layers, a DPS may be generated and managed exclusively by its declarative definition. This will increase the agility and efficiency with which DPSs may be deployed and managed. As a development model, a set of reusable messages are created, these messages are to be semantically annotated using a view over the domain ontology and are syntactically represented such that they may be exported to XML Schema. These messages are used within the DPS definition where their views over the domain ontology are\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "Privacy protection mechanisms for web service technology\n", "abstract": " The successful use of Web service technology in areas such as healthcare and government depends on its support to privacy preservation. As there is currently no privacy standard for Web services, several solutions have recently been proposed in the literature to deal with privacy in Web services. However, there is no solution that provides a suitable mechanism to describe privacy properties in Web services. When loosely-coupled components are involved, such as in Web service environments, a rich description of components is needed to determine whether they can interact in a manner that preserves privacy. The goal of this paper is to present privacy protection mechanisms for Web services, which use policies defined in the Web Services Policy Framework (WS-Policy) and an ontology defined in the Web Ontology Language (OWL) in order to support Web service interactions with suitable privacy preservation\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "A context-aware authentication approach based on behavioral definitions\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "6\n", "authors": ["355"]}
{"title": "A fine-grained privacy structure for service-oriented architecture\n", "abstract": " In this paper we propose the creation and use of a privacy policy vocabulary. The elements of this policy will have criteria for comparison, creating hierarchical relationships between those elements that could not otherwise be directly compared. This policy vocabulary can be used in conjunction with the eXtensible Access Control Markup Language (XACML) to provide storage and enforcement.", "num_citations": "6\n", "authors": ["355"]}
{"title": "A policy driven approach for service-oriented business rule management\n", "abstract": " In modern business environments, business processes and rules frequently require changes according to intensive competition and a rapidly evolving technical infrastructure. However, traditional business logic embedded in enterprise applications is difficult to change to adapt to flexible business situations. On the other hand, Service Oriented Architecture (SOA) provides an effective solution for dynamic business process flexibility by composing Web Service components. In this paper, we propose a policy-based approach to transform and enforce high-level business policy into low-level service policy. In our approach, the policy hierarchy is structured as the group policy and the service policy for specific Web Services. The group policy describes the abstract, high-level business requirements for a small set of rules, while the low-level service policy can be created automatically via policy extension and implemented\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "An overview of the analysis and design of sigma: Supervisory intelligent multi-agent system architecture\n", "abstract": " SIGMA is a conceptual multiagent system architecture for supervisory systems that aims to improve the integration and coordination of heterogeneous tools and techniques. This paper presents an overview of the analysis and design of SIGMA according to the Gaia methodology and agent UML (AUML) diagrams", "num_citations": "6\n", "authors": ["355"]}
{"title": "Agent-oriented architecture for monitoring and diagnosis in supervisory systems\n", "abstract": " Failure diagnosis is a required mechanism in supervisory applications stemming from a system's necessity to deal with uncertain, dynamic and complex environments. Early detection of abnormal conditions prevents disastrous consequences, and the precise location of faults reduces downtime losses and maintenance costs. There is a myriad of solutions that deal with the diagnosis problem from different perspectives, but unfortunately, each solution usually does not benefit from the advantages of other approaches. This means that once one is committed with a particular solution, it is difficult to incorporate other tools and techniques. This paper presents a conceptual multi-agent system architecture for the improvement of monitoring and diagnosis capabilities in supervisory systems through the integration and coordination of heterogeneous tools and techniques. The analysis and design of this architecture, named\u00a0\u2026", "num_citations": "6\n", "authors": ["355"]}
{"title": "An ontology driven privacy framework for collaborative working environments\n", "abstract": " The ability to collaborate has always been vitally important to organisations. With the availability of current networking and computing power, the creation of collaborative working environments (CWEs) has allowed for this process to occur anytime over any geographical distance. While the strength of a CWE is its ability to allow for the exchange of information freely between collaborators, this opens the users up to a possible loss of privacy. In this work, the issue of protecting privacy while collaborating is discussed. To address the privacy concerns that are raised, a privacy framework is presented. This framework contains a generic privacy ontology that allows the framework to adapt to any domain, a reasoning engine that infers who has access to what information according to which privacy rules, and a collaborative privacy manager (CPM) which can make decisions and assist collaborating users with their privacy\u00a0\u2026", "num_citations": "5\n", "authors": ["355"]}
{"title": "Energy cost forecasting for event venues\n", "abstract": " Electricity price, consumption, and demand forecasting has been a topic of research interest for a long time. The proliferation of smart meters has created new opportunities in energy prediction. This paper investigates energy cost forecasting in the context of entertainment event-organizing venues, which poses significant difficulty due to fluctuations in energy demand and wholesale electricity prices. The objective is to predict the overall cost of energy consumed during an entertainment event. Predictions are carried out separately for each event category and feature selection is used to select the most effective combination of event attributes for each category. Three machine learning approaches are considered: k-nearest neighbor (KNN) regression, support vector regression (SVR) and neural networks (NN). These approaches are evaluated on a case study involving a large event venue in Southern Ontario. In\u00a0\u2026", "num_citations": "5\n", "authors": ["355"]}
{"title": "A generalized service replication process in distributed environments\n", "abstract": " Replication is one of the main techniques aiming to improve Web services\u2019(WS) quality of service (QoS) in distributed environments, including clouds and mobile devices. Service replication is a way of improving WS performance and availability by creating several copies or replicas of Web services which work in parallel or sequentially under defined circumstances. In this paper, a generalized replication process for distributed environments is discussed based on established replication studies. The generalized replication process consists of three main steps: sensing the environment characteristics, determining the replication strategy, and implementing the selected replication strategy. To demonstrate application of the generalized replication process, a case study in the telecommunication domain is presented. The adequacy of the selected replication strategy is demonstrated by comparing it to another replication strategy as well as to a non-replicated service. The authors believe that a generalized replication process will help service providers to enhance QoS and accordingly attract more customers", "num_citations": "5\n", "authors": ["355"]}
{"title": "A new matchmaking algorithm based on multi-level matching mechanism combined with fuzzy set\n", "abstract": " The lack of semantic parts, increasing the number of Web services in the Web, and syntactic-based search operation are the main problems of current Web service technologies, these factors make difficult for clients to find a required web service. This paper shows a matchmaking algorithm to discover Semantic Web Services that are satisfying client requirements. It depends on two factors that distinguish it from any conventional Web service discovery algorithm; the first one is using semantic matching technique to overcome shortcoming of keyword matching techniques, the second one is tying Quality of Service (QoS) metrics of Web Service (WS) with fuzzy words that are used in user\u2019s request. At least fifty percent average gain in search relevancy is obtained when our matchmaking algorithm is applied to WSs that are actually matching the chosen fuzzy semantic theme.", "num_citations": "5\n", "authors": ["355"]}
{"title": "A unit test approach for database schema evolution\n", "abstract": " ContextThe constant changes in today\u2019s business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.ObjectiveThis work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database.MethodThe approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively\u00a0\u2026", "num_citations": "5\n", "authors": ["355"]}
{"title": "Privacy and trust policies within SOA\n", "abstract": " Privacy for Service-Oriented Architecture (SOA) is required to gain the trust of those who would use the technology. Through the use of an independent privacy service (PS), the privacy policies of a service consumer and provider can be compared to create an agreed upon privacy contract. In this paper we further define a metamodel for privacy policy creation and comparison. A trust element is developed as an additional criterion for a privacy policy. We define the PS and what operations it must perform to accomplish its goals. We believe this PS combined with the presented metamodel provide a strong solution to providing privacy for SOA.", "num_citations": "5\n", "authors": ["355"]}
{"title": "A SOA-based collaborative environment for clinical trials on neglected diseases\n", "abstract": " The WebInVivo project aims at providing automated support for clinical research on neglected diseases. It includes mechanisms for (a) sharing and reusing clinical trial assets, such as protocols, protocol data, workflows and workflow metadata and (b) controlling the protocol life cycle, from modelling to execution. In this project, collaboration in the biomedical area will permeate three segments of Brazilian society: (a) research and development, (b) health agents and (c) population at risk. The knowledge gained from this study will be complied and published through social networks directed to these segments of Brazilian society.", "num_citations": "5\n", "authors": ["355"]}
{"title": "An implementation for merging images for version control\n", "abstract": " Software configuration management has become essential for developing software in an organization. It allows concurrent development and improves scalability and maintainability. Images are used extensively in web applications. However, images cannot be developed concurrently like source code because files are treated to be binary files. Moreover, when manual intervention is required during conflict merging, developers are not able to identify what portions of the image has conflict because the document is binary. This paper introduces an approach to how images can be developed concurrently. In addition, a tool was implemented to merge images and provide a method of visual conflict resolution.", "num_citations": "5\n", "authors": ["355"]}
{"title": "An efficient technique for extracting fuzzy rules from neural networks\n", "abstract": " Artificial neural networks (ANN) have the ability to model input-output relationships from processing raw data. This characteristic makes them invaluable in industry domains where such knowledge is scarce at best. In the recent decades, in order to overcome the black-box characteristic of ANNs, researchers have attempted to extract the knowledge embedded within ANNs in the form of rules that can be used in inference systems. This paper presents a new technique that is able to extract a small set of rules from a two-layer ANN. The extracted rules yield high classification accuracy when implemented within a fuzzy inference system. The technique targets industry domains that possess less complex problems for which no expert knowledge exists and for which a simpler solution is preferred to a complex one. The proposed technique is more efficient, simple, and applicable than most of the previously proposed techniques.", "num_citations": "5\n", "authors": ["355"]}
{"title": "An Agent Oriented Approach to Operational Profile Management\n", "abstract": " Software reliability, defined as the probability of a software system or application functioning without failure or errors over a defined period of time, has been an important area of research for over three decades. Several research efforts aimed at developing models to improve reliability are currently underway. One of the most popular approaches to software reliability adopted by some of these research efforts involves the use of operational profiles to predict how software applications will be used. Operational profiles are a quantification of usage patterns for a software application. The research presented in this paper investigates an innovative multiagent framework for automatic creation and management of operational profiles for generic distributed systems after their release into the market. The architecture of the proposed Operational Profile MAS (Multi-Agent System) is presented along with detailed descriptions of the various models arrived at following the analysis and design phases of the proposed system. The operational profile in this paper is extended to comprise seven different profiles. Further, the criticality of operations is defined using a new composed metrics in order to organize the testing process as well as to decrease the time and cost involved in this process. A prototype implementation of the proposed MAS is included as proof-of-concept and the framework is considered as a step towards making distributed systems intelligent and self-managing.", "num_citations": "5\n", "authors": ["355"]}
{"title": "Dynamic planning networks\n", "abstract": " We introduce Dynamic Planning Networks (DPN), a novel architecture for deep reinforcement learning, that combines model-based and model-free aspects for online planning. Our architecture learns to dynamically construct plans using a learned state-transition model by selecting and traversing between simulated states and actions to maximize information before acting. DPN learns to efficiently form plans by expanding a single action-conditional state transition at a time instead of exhaustively evaluating each action, reducing the number of state-transitions used during planning. We observe emergent planning patterns in our agent, including classical search methods such as breadth-first and depth-first search. DPN shows improved performance over existing baselines across multiple axes.", "num_citations": "4\n", "authors": ["355"]}
{"title": "Blockchain for Collaborative Businesses\n", "abstract": " Blockchain applications have continuously improved ever since its first debut on cryptocurrency. From then on, its uses have branched out from the financial realm, finding their way into numerous industries such as health, environmental, and governmental. Businesses are starting to take advantage of the intrinsic traits that made blockchain so notorious into their operations, such as security, integrity, and transparency. Blockchain\u2019s versatility allows companies to cooperate on a shielded environment with business partners safely. This paper details how permissioned blockchain networks can accommodate collaborative business models securely to provide thriving business alliances. Examples of cooperative business models and business relationship orientation are described here, as well as how they generate value when paired with permissioned blockchain networks - a more business-oriented variety\u00a0\u2026", "num_citations": "4\n", "authors": ["355"]}
{"title": "Assets Predictive Maintenance Using Convolutional Neural Networks\n", "abstract": " Predictive Maintenance (PdM) performs maintenance based on the asset's health status indicators. Sensors can measure an unusual pattern of these indicators, such as an increased motor's vibration level or higher energy consumption, and, in most cases, failures are preceded by an unusual pattern of these measurements. Convolutional Neural Network (CNN) is a Machine Learning technique capable of extracting data representation. This paper presents a CNN framework to tackle assets predictive maintenance problem and a method to transform 1-dimensional (1-D) data into an image-like representation (2-D). A data transformation step is very important to make the use of CNN feasible. To evaluate the proposed framework two datasets were obtained from fans, with distinct electrical pattern, from a building at Western University. The data was preprocessed, transformed in a image-like representation and fed\u00a0\u2026", "num_citations": "4\n", "authors": ["355"]}
{"title": "Forecasting Residential Energy Consumption Using Support Vector Regressions\n", "abstract": " With the development of smart electricity metering technologies, huge amount of consumption data can be retrieved on daily and hourly basis. Energy consumption forecasting facilitates electricity demand management and utilities load planning. Most of the prior researches are focused on commercial customers or residential building-level energy consumption, or use behavioral and occupancy sensor data to experiment on individual household\u2019s electrical consumption. This paper investigates fifteen anonymous individual household\u2019s electricity consumption forecasting using support vector regression (SVR) modelling approach, applied to both daily and hourly data granularity. The electricity usage dataset was collected from fifteen households by London Hydro, a local utility company, from 2014 to 2016. Exploratory data analysis (EDA) is adopted for data visualization and feature selection. Our analysis demonstrates that forecasting residential energy consumption by weather, calendar and Timeof-usage price is feasible and reliable with sufficient accuracy for some individual residential uses in either daily or hourly prediction.", "num_citations": "4\n", "authors": ["355"]}
{"title": "O papel estrat\u00e9gico da web sem\u00e2ntica no contexto do big data\n", "abstract": " A Web Sem\u00e2ntica apresenta um corpus te\u00f3rico e diversas tecnologias e aplica\u00e7\u00f5es que demonstram a sua consist\u00eancia, inclusive no que tange ao uso de seus conceitos e de suas tecnologias em outros escopos n\u00e3o se limitando unicamente a Web. Neste sentido, os projetos de Big Data podem tirar proveito da aplica\u00e7\u00e3o dos princ\u00edpios e dos desenvolvimentos realizados na \u00e1rea da Web Sem\u00e2ntica, para aperfei\u00e7oar os processos de an\u00e1lises de dados, em especial na inser\u00e7\u00e3o de caracter\u00edsticas sem\u00e2nticas para contextualiza\u00e7\u00e3o dos dados. Assim, esta pesquisa tem como objetivo analisar e discutir o potencial das tecnologias da Web Sem\u00e2ntica como meio de integra\u00e7\u00e3o e desenvolvimento de aplica\u00e7\u00f5es de Big Data. Utilizou-se uma metodologia qualitativa explorat\u00f3ria, onde buscou-se pontos de converg\u00eancia entre a Web Sem\u00e2ntica e Big Data. Foram identificados e discutidos tr\u00eas pontos principais: a aplica\u00e7\u00e3o do Linked Data enquanto fonte de dados para o Big Data; o uso de ontologias nas an\u00e1lises de dados; e o uso das tecnologias da Web Sem\u00e2ntica para promo\u00e7\u00e3o da interoperabilidade em cen\u00e1rios de Big Data. Neste sentido, foi poss\u00edvel identificar que a Web Sem\u00e2ntica, em especial no que permeia suas tecnologias e aplica\u00e7\u00f5es, pode auxiliar significativamente o desenvolvimento do Big Data, por fornecer um paradigma complementar dos aplicados majoritariamente nas an\u00e1lises de dados.", "num_citations": "4\n", "authors": ["355"]}
{"title": "An analysis of replication and retrieval of medical image data using a database management system and a distributed file system\n", "abstract": " This paper presents a research study consisting of a comparison analysis of replication and retrieval of medical image data which use the Digital Imaging and Communications in Medicine (DICOM) standard. These data are stored in a relational database management system (RDBMS) and in the Hierarchical Data Format (HDF) using a distributed file system as a data backend. The importance of this work was measured by verification of elapsed-time reduction for medical image data replication and retrieval in a real telemedicine environment.", "num_citations": "4\n", "authors": ["355"]}
{"title": "Support systems for Telehealth services: critical operational and ICT complementary assets for large-scale provisioning\n", "abstract": " Telehealth is considered one of the best strategies to support the increasing demand for health services. Unfortunately, most Telehealth initiatives fail before the operational phase or they do not reach the expected potential usage. Although several studies and pilot projects have demonstrated that there are several mature technologies available for supporting a wide range of clinical services over distance, the current state of large-scale Telehealth services is still inadequate for the needs of health professionals, policy makers, researchers, users and other stakeholders. Consequently, this paper evaluates the current operational barriers of large-scale Telehealth services provisioning, focusing specifically on the challenges of Information and Communications Technology. Some complementary assets, besides the core clinical and medical protocols, are required for performing technological innovations in large-scale\u00a0\u2026", "num_citations": "4\n", "authors": ["355"]}
{"title": "A service dependency model for multiple service version synchronization\n", "abstract": " As Web Services are used more frequently, their evolutionary changes become an increasingly prominent issue. Specifically, one challenge involving service maintenance entails identifying changes and synchronizing those changes with the dependent services. For instance, a service provider may support multiple versions of the same service, and these versions are dependent on common artifacts such as data types, operation functions, and security policies. Consequently, it is necessary to develop automatic or semi-automatic service version synchronization techniques in order to reduce the burden involved in the manual creation and maintenance of services. In this paper, we present a service dependency model for the synchronization of multiple service versions by relating the base service model with the dependent service versions to construct a service dependency graph and dependency matrices\u00a0\u2026", "num_citations": "4\n", "authors": ["355"]}
{"title": "Recovering brazilian indigenous cultural heritage using new information and communication technologies\n", "abstract": " The objective of this paper is to discuss the new demands imposed on museums, the possibilities to achieve them using new Information and Communication Technologies (ICTs) and to propose a platform to meet these requirements. The platform will provide services to the external public, the museum staff and researchers. The artifacts to be collected and interpreted belong to several Brazilian indigenous cultures.", "num_citations": "4\n", "authors": ["355"]}
{"title": "The object-oriented paradigm for software evolution\n", "abstract": " In this research work object-oriented concepts are applied to a software maintenance method named COMFORM (configuration management formalization for maintenance). COMFORM is composed of several phases to provide the necessary guidance to maintain existing software systems. One of the aims of the method is redocumentation by keeping the maintenance history and information related to the software modules being maintained. Redocumentation is obtained by filling in pre-defined forms which go hand in hand with the phases of the method. Each form is mapped by a class definition. The concepts of classes and inheritance are used along with software maintenance to help the system manager in the creation and administration of such forms. Version control of the predefined forms is carried out either by reusing common parts of these forms or by defining new subclasses from them. As a\u00a0\u2026", "num_citations": "4\n", "authors": ["355"]}
{"title": "O papel da web sem\u00e2ntica nos processos do big data\n", "abstract": " A Web Sem\u00e2ntica apresenta um corpus te\u00f3rico e diversas tecnologias e aplica\u00e7\u00f5es que demonstram a sua consist\u00eancia, inclusive no que tange ao uso de seus conceitos e de suas tecnologias em outros escopos n\u00e3o se limitando unicamente a Web. Neste sentido, os projetos de Big Data podem tirar proveito da aplica\u00e7\u00e3o dos princ\u00edpios e dos desenvolvimentos realizados na \u00e1rea da Web Sem\u00e2ntica, para aperfei\u00e7oar os processos de an\u00e1lises de dados, em especial na inser\u00e7\u00e3o de caracter\u00edsticas sem\u00e2nticas para contextualiza\u00e7\u00e3o dos dados. Assim, esta pesquisa tem como objetivo analisar e discutir o potencial das tecnologias da Web Sem\u00e2ntica como meio de integra\u00e7\u00e3o e desenvolvimento de aplica\u00e7\u00f5es de Big Data. Utilizou-se uma metodologia qualitativa explorat\u00f3ria, em que se buscaram pontos de converg\u00eancia entre a Web Sem\u00e2ntica e Big Data. Foram identificados e discutidos quatro pontos principais: a aplica\u00e7\u00e3o do Linked Data enquanto fonte de dados para o Big Data; o uso de ontologias nas an\u00e1lises de dados; o uso das tecnologias da Web Sem\u00e2ntica para promo\u00e7\u00e3o da interoperabilidade em cen\u00e1rios de Big Data; e o uso de machine learning para extrair dados automaticamente e convert\u00ea-los para os padr\u00f5es da Web Sem\u00e2ntica. Neste sentido, foi poss\u00edvel identificar que a Web Sem\u00e2ntica, em especial no que permeia suas tecnologias e aplica\u00e7\u00f5es, pode auxiliar significativamente o desenvolvimento do Big Data, por fornecer um paradigma complementar dos aplicados majoritariamente nas an\u00e1lises de dados.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Extracting traceability information from C# projects\n", "abstract": " The maintenance portion of the software lifecycle represents a major drain on most software companys resources. The transition from programmers to maintainers is high risk, since usually the maintainers have to learn the system from scratch before they can begin modifying it appropriately. This paper introduces a method for automatically extracting important traceability information from a C# software projects source code. Using this traceability information, maintainers (and programmers) are better able to evaluate the impacts their actions will have on the entire project.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Fine\u2010grained filtering to provide access control for data providing services within collaborative environments\n", "abstract": " A data providing service (DPS) in service\u2010oriented architecture is tasked only with the retrieval of data that are annotated over a domain ontology. One particular motivating application of DPSs is their use within collaborative environments. An important characteristic for the enterprises of such a collaborative environment is the ability to employ data sharing with one another. A major concern in this situation is the protection of each enterprise's privacy while still permitting data sharing. One potential solution is to provide filtered data through access control. This work describes how to implement access control through fine\u2010grained filtering of DPS response messages; it is accomplished using a filtering ontology and relations between the domain ontology of DPS and the proposed filtering ontology. Therefore, enterprises can write enterprise\u2010specific access control policies referencing a common filtering ontology\u00a0\u2026", "num_citations": "3\n", "authors": ["355"]}
{"title": "Fine-grained filtering of data providing Web Services with XACML\n", "abstract": " This work describes how a fine-grained filtering Web service is generated using an extended XACML architecture for Data Providing Web Services (DPWSs). A DPWS is a Web service tasked with the retrieval of data that is annotated using a domain ontology. This work extends XACML policies to reference filtering classes from a filtering ontology instead of directly referencing fine-grained XML elements. This paper then contributes a language to state relations between the filtering ontology and the DPWS's domain ontology. It also contributes an algorithm to interpret these relations to determine if a given XML element from a DPWS is a member of a filtering class. This algorithm is then used by the extended XACML architecture to infer how a DPWS shall be filtered. This approach provides a clear separation of concerns between a DPWS's data domain and the filtering domain.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Threats, countermeasures, and advances in applied information security\n", "abstract": " Organizations are increasingly relying on electronic information to conduct business, which has caused the amount of personal information to grow exponentially. Threats, Countermeasures, and Advances in Applied Information Security addresses the fact that managing information security program while effectively managing risks has never been so critical. This book contains 24 chapters on the most relevant and important issues and advances in applied information security management. The chapters are authored by leading researchers and practitioners in the field of information security from across the globe. The chapters represent emerging threats and countermeasures for effective management of information security at organizations.", "num_citations": "3\n", "authors": ["355"]}
{"title": "A reference ontology based approach for service oriented semantic interoperability\n", "abstract": " To establish effective information exchange among applications in a distributed environment, participants not only share their functions and service interfaces, but often exchange data models. This paper proposes the use of ontologies to represent data models thus allowing applications to locate and integrate these models. A reference ontology based approach for service oriented ontology management is introduced. Specifically, a domain-specific reference ontology for use in the evaluation of a practical case is developed. To validate and evaluate the approach, a prototype system is developed to provide ontology deploying, browsing and mapping operations based on a service-oriented system. Experiments provide promising results that are consistent with the original ideas of managing ontologies and optimizing ontology mappings to facilitate data interoperability in a distributed environment.", "num_citations": "3\n", "authors": ["355"]}
{"title": "An infrastructure for executing WS-BPEL workflows in a Cluster of Clusters\n", "abstract": " The use of heterogeneous distributed systems, such as Clusters of Clusters (CoC) and computational grids, enables the development of complex computational systems. However some of these systems require services to be easily composed and executed while maintaining the dependencies among them; both the services and the dependencies can be represented as workflows. Moreover, Web Services have been adopted in distributed systems, and WS-BPEL is the standard for Web Services composition. Accordingly, the objective of this work is to enable the execution of WS-BPEL workflows in a CoC. In order to achieve this execution, an extension to the WS-BPEL language is presented that includes the specification of Quality of Services (QoS), along with the required computational resources for service execution in CoCs. Additionally, this paper presents an infrastructure that enables the execution of such\u00a0\u2026", "num_citations": "3\n", "authors": ["355"]}
{"title": "Toward advance resource reservation in mobile grid configurations based on user-centric authentication\n", "abstract": " There has been growing interest in the use of mobile devices for providing access to the various applications, services and resources within grid computing environments. As is the case for most access to grid resources, the use of mobile devices requires user authentication. Given the limited amount of power available for mobile devices, the applications on these devices, including the authentication services used to access grid resources, must be designed to conserve power. In this paper, we present an authentication architecture utilizing a \u201clightweight\u201d user-centric authentication approach. Specifically, we aim to provide mobile users with access to advance resource reservation within grid environments, and, consequently, our approach incorporates this objective. The approach, however, applies to user authentication with any grid resource or service. In attempting to overcome the limitations of mobile devices, such as limited battery power, mobile users can utilize grid environments in a transparent and secure way.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Maintenance and monitoring of remote software using an agent platform\n", "abstract": " This paper discusses the maintenance needs of distributed devices, and proposes a platform for satisfying these needs while providing a homogeneous interface to maintainers. An agent-oriented paradigm is applied in developing this platform, and its use is defended. The platform is designed and implemented in a way that is flexible for deployment on a wide variety of embedded software devices, allowing maintainers to gather information and perform maintenance on these devices either locally or remotely.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Application of fuzzy logic for improved software project management estimations\n", "abstract": " Project management (PM) is the application of knowledge, skills, tools and techniques to project activities in order to meet project requirements. The success of any project relies heavily on the initial estimation of all project parameters. Fuzzy logic is a soft-computing technique used to effectively solve uncertainties due to imprecise inputs to generate linguistic or quantitative outputs. This paper investigates the application of fuzzy logic as a feasible technique for improved estimation accuracy to all the tasks within the project management knowledge areas to ensure higher software project success rates.", "num_citations": "3\n", "authors": ["355"]}
{"title": "Transfer Learning by Similarity Centred Architecture Evolution for Multiple Residential Load Forecasting\n", "abstract": " The development from traditional low voltage grids to smart systems has become extensive and adopted worldwide. Expanding the demand response program to cover the residential sector raises a wide range of challenges. Short term load forecasting for residential consumers in a neighbourhood could lead to a better understanding of low voltage consumption behaviour. Nevertheless, users with similar characteristics can present diversity in consumption patterns. Consequently, transfer learning methods have become a useful tool to tackle differences among residential time series. This paper proposes a method combining evolutionary algorithms for neural architecture search with transfer learning to perform short term load forecasting in a neighbourhood with multiple household load consumption. The approach centres its efforts on neural architecture search using evolutionary algorithms. The neural architecture evolution process retains the patterns of the centre-most house, and later the architecture weights are adjusted for each house in a multihouse set from a neighbourhood. In addition, a sensitivity analysis was conducted to ensure model performance. Experimental results on a large dataset containing hourly load consumption for ten houses in London, Ontario showed that the performance of the proposed approach performs better than the compared techniques. Moreover, the proposed method presents the average accuracy performance of 3.17 points higher than the state-of-the-art LSTM one shot method. View Full-Text", "num_citations": "2\n", "authors": ["355"]}
{"title": "Deep neural network for load forecasting centred on architecture evolution\n", "abstract": " Nowadays, electricity demand forecasting is critical for electric utility companies. Accurate residential load forecasting plays an essential role as an individual component for integrated areas such as neighborhood load consumption. Short-term load forecasting can help electric utility companies reduce waste because electric power is expensive to store. This paper proposes a novel method to evolve deep neural networks for time series forecasting applied to residential load forecasting. The approach centres its efforts on the neural network architecture during the evolution. Then, the model weights are adjusted using an evolutionary optimization technique to tune the model performance automatically. Experimental results on a large dataset containing hourly load consumption of a residence in London, Ontario shows that the performance of unadjusted weights architecture is comparable to other state-of-the-art\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "Heath-PRIOR: An Intelligent Ensemble Architecture to Identify Risk Cases in Healthcare\n", "abstract": " Smart city environments, when applied to healthcare, improve the quality of people\u2019s lives, enabling, for instance, disease prediction and treatment monitoring. In medical settings, case prioritization is of great importance, with beneficial outcomes both in terms of patient health and physicians\u2019 daily work. Recommender systems are an alternative to automatically integrate the data generated in such environments with predictive models and recommend actions, content, or services. The data produced by smart devices are accurate and reliable for predictive and decision-making contexts. This study main purpose is to assist patients and doctors in the early detection of disease or prediction of postoperative worsening through constant monitoring. To achieve this objective, this study proposes an architecture for recommender systems applied to healthcare, which can prioritize emergency cases. The architecture brings\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "A MapReduce Approach for Traffic Matrix Estimation in SDN\n", "abstract": " A traffic matrix (TM) is a source of critical traffic throughput information for traffic engineering activities and network management tasks such as traffic prediction, capacity planning, network provisioning, and anomaly detection. However, estimating TM poses several challenges for network engineers. One of the challenges is that traffic data statistics are constantly changing, and their aggregation for real-time monitoring becomes a difficult task. This paper presents a near real-time TM estimation approach for OpenFlow (OF) networks. It makes use of Big Data techniques based on MapReduce operations to tackle the aggregation problem. The proposed method uses traffic data statistics collected from OF switches through an SDN controller as input and aggregates these data in a Big Data streaming processing environment. This paper explores the benefits of the distributed MapReduce computing model to provide an\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "A Heterogeneous Edge-Fog Environment Supporting Digital Twins for Remote Inspections\n", "abstract": " The increase in the development of digital twins brings several advantages to inspection and maintenance, but also new challenges. Digital models capable of representing real equipment for full remote inspection demand the synchronization, integration, and fusion of several sensors and methodologies such as stereo vision, monocular Simultaneous Localization and Mapping (SLAM), laser and RGB-D camera readings, texture analysis, filters, thermal, and multi-spectral images. This multidimensional information makes it possible to have a full understanding of given equipment, enabling remote diagnosis. To solve this problem, the present work uses an edge-fog-cloud architecture running over a publisher-subscriber communication framework to optimize the computational costs and throughput. In this approach, each process is embedded in an edge node responsible for prepossessing a given amount of data that optimizes the trade-off of processing capabilities and throughput delays. All information is integrated with different levels of fog nodes and a cloud server to maximize performance. To demonstrate this proposal, a real-time 3D reconstruction problem using moving cameras is shown. In this scenario, a stereo and RDB-D cameras run over edge nodes, filtering, and prepossessing the initial data. Furthermore, the point cloud and image registration, odometry, and filtering run over fog clusters. A cloud server is responsible for texturing and processing the final results. This approach enables us to optimize the time lag between data acquisition and operator visualization, and it is easily scalable if new sensors and algorithms must be added\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "Evaluation of Cache for Bandwidth Optimization in ICN Through Software-Defined Networks\n", "abstract": " Traffic reduction in network segments through cache implementations has become a major research topic due to the exponential increase in data requests through the network. Even with high-speed connections, the conventional model still depends on point-to-point communication between two systems. Throughout the world, more connected devices are, accessing services and obtaining information. To support this activity, servers must have massive storage to support creation, retrieval, updated and deletion of large amounts of data. Therefore, in studies of Information Centric Networks (ICN), this model has been widely discussed as the new content distribution model for the Internet. To provide improved network management many approaches are using software-defined networks (SDN) to develop flexible content-based networks. This paper proposes to use cache replication for ICN through SDN to avoid\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "Collective Contextual Anomaly Detection for Building Energy Consumption\n", "abstract": " Commercial and residential buildings are responsible for a substantial portion of total global energy consumption and as a result make a significant contribution to global carbon emissions. Hence, energy-saving goals that target buildings can have a major impact in reducing environmental damage. During building operation, a significant amount of energy is wasted due to equipment and human-related faults. To reduce waste, today's smart buildings monitor energy usage with the aim of identifying abnormal consumption behaviour and notifying the building manager to implement appropriate energy-saving procedures. To this end, this research proposes the\\textit {ensemble anomaly detection}(EAD) framework. The EAD is a generic framework that combines several anomaly detection classifiers using majority voting. This anomaly detection classifiers are formed using existing machine learning algorithm. It is assumed that each anomaly classifier has equal weight. More importantly, to ensure diversity of anomaly classifiers, the EAD is implemented by combining pattern-based and prediction-based anomaly classifiers. For this reason, this research also proposes a new pattern-based anomaly classifier, the\\textit {collective contextual anomaly detection using sliding window}(CCAD-SW) framework. The CCAD-SW, which is also a machine leaning-based framework that identifies anomalous consumption patterns using overlapping sliding windows. The EAD framework combines the CCAD-SW, which is implemented using autoencoder, with two prediction-based anomaly classifiers that are implemented using the support vector regression and\u00a0\u2026", "num_citations": "2\n", "authors": ["355"]}
{"title": "An Iterative Association Rule Mining Framework to K-Anonymize a Dataset\n", "abstract": " Preserving and maintaining client privacy and anonymity is of utmost importance in any domain and specially so in healthcare, as loss of either of these can result in legal and ethical implications. Further, it is sometimes important to extract meaningful and useful information from existing data for research or management purposes. In this case it is necessary for the organization who manages the dataset to be certain that no attributes can identify individuals or group of individuals. This paper proposes an extendable and generalized framework to anonymize a dataset using an iterative association rule mining approach. The proposed framework also makes use of optional domain rules and filter rules to help customize the filtering process. The outcome of the proposed framework is a preprocessed dataset which can be used in further research with confidence that anonymity of individuals is conserved. Evaluation of this research will also be described in the form of a case study using a test dataset provided by the Lawson Health Research Institute in London, Ontario, Canada as a part of their Mental Health Engagement Network (MHEN) study.", "num_citations": "2\n", "authors": ["355"]}
{"title": "A user-centric authentication for advanced resource reservation in mobile grid environments\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "2\n", "authors": ["355"]}
{"title": "A process oriented semantic healthcare service composition\n", "abstract": " The service oriented computing paradigm is transforming traditional enterprise information from a closed, centralized control system into a dynamic information exchange with business process support. In particular, interoperability of healthcare information systems has become an important issue within the healthcare industry. In order to enable effective and efficient information exchange, we propose a process oriented service discovery approach to support a dynamic healthcare service composition that is to occur during design time. Specifically, our approach utilizes semantic profiles to specify the semantic descriptions of process operations using domain specific ontologies such as HL7. Consequently, our methodology provides substantial flexibility for semantic healthcare service compositions while reusing existing processes and services.", "num_citations": "2\n", "authors": ["355"]}
{"title": "OSTRA: A process framework for the transition to service-oriented architecture\n", "abstract": " Service-Oriented Architecture (SOA) is an emerging paradigm that entails building applications as a collection of flexible services. Although there is a considerable amount of research on the challenges involving the transition to SOA, it is still necessary to incorporate frameworks that provide guidance for companies adopting this architecture. Accordingly, this article presents a framework, which is entitled OSTRA (Opportunity-driven Service-oriented TRAnsition). OSTRA provides a realistic approach for the development of SOA by considering short-term and long-term term goals as well as by balancing planning and management. This work describes the three streams that OSTRA organizes throughout the SOA transition: SOA roadmap, SOA development and SOA governance. Moreover, it discusses the implementation of OSTRA in a real organization.", "num_citations": "2\n", "authors": ["355"]}
{"title": "Web technologies in a collaborative platform for clinical trials\n", "abstract": " As tecnologias de Internet mudaram o desenvolvimento de software. As mudan\u00e7as afetam uma gama completa de aplica\u00e7\u00f5es, assim como a forma em que os usu\u00e1rios interagem com os computadores. No campo da sa\u00fade, a pesquisa m\u00e9dica demanda muito investimento, esfor\u00e7o e informa\u00e7\u00f5es para a comercializa\u00e7\u00e3o segura de um f\u00e1rmaco novo. O projeto WebInVivo visa fornecer suporte automatizado \u00e0 pesquisa m\u00e9dica, alicer\u00e7ado em tecnologias de Internet. Ele inclui mecanismos para o compartilhamento e a reutiliza\u00e7\u00e3o de informa\u00e7\u00f5es de pesquisas m\u00e9dicas, tais como protocolos, dados de protocolos, fluxos de trabalho e metadados de fluxos de trabalho para o controle do ciclo de vida do protocolo, da modelagem \u00e0 execu\u00e7\u00e3o. Neste projeto, o conhecimento da \u00e1rea biom\u00e9dica permeia tr\u00eas segmentos da sociedade brasileira:(a) pesquisa e desenvolvimento,(b) agentes de sa\u00fade e (c) a popula\u00e7\u00e3o. Este conhecimento ser\u00e1 disponibilizado em redes sociais, para esses segmentos da sociedade brasileira.", "num_citations": "2\n", "authors": ["355"]}
{"title": "Tecnologias de Internet em uma plataforma de colabora\u00e7\u00e3o para a pesquisa m\u00e9dica\n", "abstract": " As tecnologias de Internet mudaram o desenvolvimento de software. As mudan\u00e7as afetam uma gama completa de aplica\u00e7\u00f5es, assim como a forma em que os usu\u00e1rios interagem com os computadores. No campo da sa\u00fade, a pesquisa m\u00e9dica demanda muito investimento, esfor\u00e7o e informa\u00e7\u00f5es para a comercializa\u00e7\u00e3o segura de um f\u00e1rmaco novo. O projeto WebInVivo visa fornecer suporte automatizado \u00e0 pesquisa m\u00e9dica, alicer\u00e7ado em tecnologias de Internet. Ele inclui mecanismos para o compartilhamento e a reutiliza\u00e7\u00e3o de informa\u00e7\u00f5es de pesquisas m\u00e9dicas, tais como protocolos, dados de protocolos, fluxos de trabalho e metadados de fluxos de trabalho para o controle do ciclo de vida do protocolo, da modelagem \u00e0 execu\u00e7\u00e3o. Neste projeto, o conhecimento da \u00e1rea biom\u00e9dica permeia tr\u00eas segmentos da sociedade brasileira: (a) pesquisa e desenvolvimento, (b) agentes de sa\u00fade e (c) a popula\u00e7\u00e3o. Este conhecimento ser\u00e1 disponibilizado em redes sociais, para esses segmentos da sociedade brasileira.", "num_citations": "2\n", "authors": ["355"]}
{"title": "C# traceability system.\n", "abstract": " Traceability information is a valuable asset that software development teams can leverage to minimisetheir risk during production and maintenance of software projects. When maintainersare added to a software project post-production, they have to learn the system from scratch and understand its dynamics before theycan begin making appropriate modifications to the source code. The system outlined inthis paper extracts traceability informationdirectly from the source code of C# projects, and presents it in such a way mat it can be easily used to understand the logic and validate changes to me system.", "num_citations": "2\n", "authors": ["355"]}
{"title": "Noisy Importance Sampling Actor-Critic: An Off-Policy Actor-Critic With Experience Replay\n", "abstract": " This paper presents Noisy Importance Sampling Actor-Critic (NISAC), a set of empirically validated modifications to the advantage actor-critic algorithm (A2C), allowing off-policy reinforcement learning and increased performance. NISAC uses additive action space noise, aggressive truncation of importance sample weights, and large batchsizes. We see that additive noise drastically changes how off-sample experience is weighted for policy updates. The modified algorithm achieves an increase in convergence speed and sample efficiency compared to both the on-policy actor-critic A2C and the importance weighted off-policy actor-critic algorithm. In comparison to state-of-the-art (SOTA) methods, such as actor-critic with experience replay (ACER), NISAC nears the performance on several of the tested environments while training 40% faster and being significantly easier to implement. The effectiveness of NISAC is\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "A blockchain approach to social responsibility\n", "abstract": " As blockchain technology matures, more sophisticated solutions arise regarding complex problems. Blockchain continues to spread towards various niches such as government, IoT, energy, and environmental industries. One often overlooked opportunity for blockchain is the social responsibility sector. Presented in this paper is a permissioned blockchain model that enables enterprises to come together and cooperate to optimize their environmental and societal impacts. This is made possible through a private or permissioned blockchain. Permissioned blockchains are blockchain networks where all the participants are known and trust relationships among them can be fostered more smoothly. An example of what a permissioned blockchain would look like is described in this paper as well as its implementation, achieved using Hyperledger Fabric, which is a business-oriented blockchain framework. This study\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "Blockchain-based federated identity and auditing\n", "abstract": " A federated identity is a single identity that enables users to access multiple services across a network of business parties. Such identities are subject to various threats and attacks and face diverse challenges including identity leaks, centralised management, auditing limitations, and long breach investigation processes. This paper proposes a framework aimed at automating and decentralising the generation and auditing of a robust and secured blockchain-based federated identity in a marketplace. Business parties participating in the marketplace form the nodes of a distributed blockchain network and participate in the creation of federated identities. Users of this network can access services provided by any one of the participating parties using a single federated identity. All transactions are fully audited in the blockchain, meaning that participating parties can monitor access to their service and users can trace the\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "2 architecture: contextual collaborative filtering\n", "abstract": " Recommender systems have dramatically changed the way we consume content. Internet applications rely on these systems to help users navigate among the ever-increasing number of choices available. However, most current systems ignore the fact that user preferences can change according to context, resulting in recommendations that do not fit user interests. This research addresses these issues by proposing the  architecture, which uses local learning techniques to embed contextual awareness into collaborative filtering models. The proposed architecture is demonstrated on two large-scale case studies involving over 130 million and over 7 million unique samples, respectively. Results show that contextual models trained with a small fraction of the data provided similar accuracy to collaborative filtering models trained with the complete dataset. Moreover, the impact of taking into account\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "A Experi\u00eancia do Usu\u00e1rio nos mecanismos de busca Knowledge Graph e o Knowledge Vault\n", "abstract": " Introdu\u00e7\u00e3o: Os mecanismos de busca s\u00e3o respons\u00e1veis por satisfazer a necessidade de informa\u00e7\u00e3o e recuperar de maneira eficaz e eficiente os documentos nos diferentes formatos. Nesse contexto, a disciplina de Experi\u00eancia de Usu\u00e1rio torna-se fundamental, na cria\u00e7\u00e3o de mecanismos que satisfa\u00e7am e contribuam na utiliza\u00e7\u00e3o de um sistema. Entretanto, necessita-se de pesquisas que analisem como algumas tecnologias de mecanismos de busca na Web est\u00e3o influenciando a Experi\u00eancia do Usu\u00e1rio. Objetivo: Discutir como as atuais tecnologias de informa\u00e7\u00e3o e comunica\u00e7\u00e3o podem influenciar na Experi\u00eancia de Usu\u00e1rio, tendo como enfoque os mecanismos de busca Knowledge Graph e Knowledge Vault, sob um olhar da Ci\u00eancia da Informa\u00e7\u00e3o. Metodologia: Qualitativa, de natureza explorat\u00f3rio e anal\u00edtica, em que se realizou o estudo das facetas da Experi\u00eancia de Usu\u00e1rio no \u00e2mbito das tecnologias Knowledge Graph e Knowledge Vault. Resultados: Relacionou-se em um quadro a Experi\u00eancia do Usu\u00e1rio com o Knowledge Graph e Knowledge Vault, utilizando as facetas da Experi\u00eancia do Usu\u00e1rio. Por meio deste quadro, identificou-se que as funcionalidades inseridas nesses sistemas v\u00eam contribuindo na apresenta\u00e7\u00e3o das respostas de buscas, que diferencia a experi\u00eancia do usu\u00e1rio neste sistema, quando comparado com a busca tradicional. Conclus\u00e3o: Por meio deste trabalho, verificou-se como os mecanismos de busca est\u00e3o evoluindo segundo a \u00f3tica da Experi\u00eancia do Usu\u00e1rio, onde as principais inova\u00e7\u00f5es referentes aos processos e busca se mostraram aderentes as necessidades apontadas pelos principais te\u00f3ricos da\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "Energy-aware resource selection on opportunistic grids\n", "abstract": " Energy consumption has been a constant concern for high-performance computing (HPC). Recently, this concern has gained attention from the research community, which is aiming to reduce its costs. The performance gain in such an environment is usually proportional to cost. Examples of such environments are computational grids, which are used in the academic and enterprise domains. On the other hand, one way of obtaining high-performance computing with low-cost investment is by using opportunistic grids, which have become a viable alternative to super-computers and dedicated clusters. This paper proposes an energy-aware resource-selection algorithm to reduce energy consumption in opportunistic grids. The proposed algorithm takes into consideration resource status as well as actions to be taken before allocation to calculate energy consumption. Experimental analysis conducted in this study\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "From Inception to Execution: Query Management for Complex Event Processing as a Service\n", "abstract": " Complex Event Processing (CEP) is a set of tools and techniques that can be used to obtain insights from high- volume, high-velocity continuous streams of events. CEP-based systems have been adopted in many situations that require prompt establishment of system diagnostics and execution of reaction plans, such as in monitoring of complex systems. This article describes the Query Analyzer and Manager (QAM) mod- ule, a first effort toward the development of a CEP as a Service (CEPaaS) system. This module is responsible for analyzing user-defined CEP queries and for managing their execution in distributed cloud-based environments. Using a language-agnostic internal query representation, QAM has a modular design that enables its adoption by virtually any CEP system.", "num_citations": "1\n", "authors": ["355"]}
{"title": "Web service semantic access control\n", "abstract": " Web services facilitate the sharing of resources between groups within a collaborative environment. Privacy is an important concern when sharing data. Access control can be used to protect privacy by limiting the access privileges to the underlying resources. This paper outlines a framework for applying access control to data providing Web services. The framework makes use of ontologies allowing for a clear description of the functionality offered by the services within the collaborative environment. The implementation of the framework is described using XACML as a basis for the access control. XACML is extended to function with the defined ontologies in order to provide fine-grained access control. Finally, a case study is worked through showing how the proposed framework could be applied within a healthcare environment.", "num_citations": "1\n", "authors": ["355"]}
{"title": "EEF-CAS: An Effort Estimation Framework with Customizable Attribute Selection\n", "abstract": " Existing estimation frameworks generally provide one-size-fits-all solutions that fail to produce accurate estimates in most environments. Research has shown that the accomplishment of accurate effort estimates is a long-term process that, above all, requires the extensive collection of effort estimation data by each organization. Collected data is generally characterized by a set of attributes that are believed to affect the development effort. The attributes that most affect development effort vary widely depending on the type of product being developed and the environment in which it is being developed. Thus, any new estimation framework must offer the flexibility of customizable attribute selection. Moreover, such attributes could provide the ability to incorporate empirical evidence and expert judgment into the effort estimation framework. Finally, because software is virtual and therefore intangible, the most important software metrics are notorious for being subjective according to the experience of the estimator. Consequently, a measurement and inference system that is robust to subjectivity and uncertainty must be in place. The Effort Estimation Framework with Customizable Attribute Selection (EEF-CAS) presented in this paper has been designed with the above requirements in mind. It is accompanied with four preparation process steps that allow for any organization implementing it to establish an estimation process. This estimation process facilitates data collection, framework customization to the organization\u2019s needs, its calibration with the organization\u2019s data, and the capability of continual improvement. The proposed framework described in\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "A Privacy Service for Comparison of Privacy and Trust Policies within SOA\n", "abstract": " Privacy for Service-Oriented Architecture (SOA) is required to gain the trust of those who would use the technology. Through the use of an independent Privacy Service (PS), the privacy policies of a service consumer and provider can be compared to create an agreed upon privacy contract. In this chapter, the authors further define a metamodel for privacy policy creation and comparison. A trust element is developed as an additional criterion for a privacy policy. The authors define the PS, outline what operations it must perform to accomplish its goals, and present how the PS operates in different scenarios. They believe the PS, combined with the enhanced metamodel, provides a strong solution for providing privacy in an SOA environment.", "num_citations": "1\n", "authors": ["355"]}
{"title": "Data providing web service-based integration framework for use in a health care context\n", "abstract": " Removing boundaries between health care sub-domains has recently received increasing attention in both research and practice. Termed \"silos\", traditional divisions in medicine are increasingly viewed as inefficient at a time when efficiency is essential. With a practical scenario as our basis, we review the use of a Data Providing Web Service (DPWS)-based framework to integrate data from multiple medical operations. A DPWS-based framework also addresses key software engineering requirements of our practical health care scenario such as the creation of a separation of concerns between the data source and the integration framework as well as the use of a domain ontology for describing the data that are accessible from the DPWSs. A separation of concerns gives full autonomy to each medical operation with regards to what information is exposed and with what controls (i.e. privacy and auditing). The\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "A service oriented ontology management framework in the automotive retail domain\n", "abstract": " In the automotive retail industry, many enterprises are transforming their systems to the Service Oriented Architecture (SOA) and mapping data models to achieve seamless information exchange. In this paper, we propose a Service Oriented Ontology Management Framework (SOOMF) that combines ontology with Web Services to improve the semantic interoperability of automotive retail applications. We will also introduce our reference ontology based approach for ontology mapping, STAR ontology, and the prototype system.", "num_citations": "1\n", "authors": ["355"]}
{"title": "Software integration through the use of a dynamic wrapper agent.\n", "abstract": " Software system services have become a pervasive part of oursociety. The various functions they perform have become an expectation. Many software applications still in use today are legacy systems which were developed decades ago in languages which are now considered obsolete. Yet, with proper maintenance and enhancements, these systems continue to perform their necessary functions effectively. These are not likely to be replaced any time soon, but presenta restriction as they provide little interoperability in today's open environment. The option of replacing these systems to keep pace with ever changing needs is daunting, costly and may even be unnecessary. Instead, incorporating the services of these systems into an open, cooperative, distributed environment will not only extend their life and services but will also make them available to future systems which have not yet been created. Agent\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}
{"title": "Software integration using a dynamic wrapper agent\n", "abstract": " Many application software packages still in use today are legacy systems which were developed decades ago in languages which are now considered obsolete. Yet, with proper maintenance and enhancements, these systems continue to perform their necessary functions effectively. These are not likely to be replaced any time soon, but present a restriction as they provide little interoperability in today\u2019s open environment. The option of replacing these systems to keep pace with ever changing needs is daunting, costly and may even be unnecessary. Instead, incorporating the services of these systems into an open, cooperative, distributed environment will not only extend their life and services but will also make them available to future systems which have not yet been created. Agent technology has been successfully used to provide legacy execution when embedded within the agent context. In order that legacy services become available to an extended clientele in the agent environment, a dynamic service providing connectivity to the legacy system is proposed in this paper. The focus of this solution is a wrapper agent that is able to create dynamic connections to various legacy applications on behalf of client agents as the need arises. It is proposed that the feasibility of the wrapper agent service depends on the agent\u2019s ability to internalize events and respond according to its goals and belief base.", "num_citations": "1\n", "authors": ["355"]}
{"title": "'fuzzy ProjectManager'\u2014FRAMEWORK FOR SOFTWARE PROJECT MANAGEMENT USING FUZZY LOGIC\n", "abstract": " Project Management is the application of knowledge, skills, tools and techniques to project activities in order to meet project requirements. The success of any project relies heavily on the initial estimation of all project parameters. The absence of reliable estimations leads to ineffective project planning, over- or under-commitment of resources and therefore an increased likelihood of a software project failure. Fuzzy Logic is a soft-computing technique used to effectively solve uncertainties due to imprecise inputs to generate linguistic or quantitative outputs. This paper presents a novel framework for project management incorporating fuzzy logic known as 'fuzzy ProjectManager'. Furthermore, this paper demonstrates the application of fuzzy logic as a feasible technique for improved estimation accuracy of all software project estimations to ensure higher software project success rates.", "num_citations": "1\n", "authors": ["355"]}
{"title": "Pawa: A program analysis tool for web based intranet applications\n", "abstract": " Web based intranet applications are increasingly becoming popular in organizations for in-house functions. These applications are different from Web sites as they offer substantially greater opportunities for users in terms of modifying the site status and they are different from traditional software applications due to their WWW platform. Unlike traditional Web applications they are smaller in size and closer to traditional software than Web sites. Usually such applications are developed in a short time and no documentation is available. Due to emerging technologies and organizational needs these applications need to be modified, tested and enhanced frequently. Existing tools are not helpful in analyzing the programs of such applications. The paper presents a tool for program analysis of Web based intranet applications. The information recovered by the tool helps in comprehending such applications and facilitates\u00a0\u2026", "num_citations": "1\n", "authors": ["355"]}