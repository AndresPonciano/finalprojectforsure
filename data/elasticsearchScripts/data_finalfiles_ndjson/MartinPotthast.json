{"title": "CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies\n", "abstract": " The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their  learning systems on the same data sets. In 2017, one of two tasks was devoted to learning dependency parsers for a large number of  languages, in a realworld setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.", "num_citations": "431\n", "authors": ["2097"]}
{"title": "A Stylometric Inquiry into Hyperpartisan and Fake News\n", "abstract": " This paper reports on a writing style analysis of hyperpartisan (i.e., extremely one-sided) news in connection to fake news. It presents a large corpus of 1,627 articles that were manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, 3 each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of which originated from hyperpartisan publishers. We propose and demonstrate a new way of assessing style similarity between text categories via Unmasking---a meta-learning approach originally devised for authorship verification---, revealing that the style of left-wing and right-wing news have a lot more in common than any of the two have with the mainstream. Furthermore, we show that hyperpartisan news can be discriminated well by its style from the mainstream (F1=0.78), as can be satire from both (F1=0.81). Unsurprisingly, style-based fake news detection does not live up to scratch (F1=0.46). Nevertheless, the former results are important to implement pre-screening for fake news detectors.", "num_citations": "423\n", "authors": ["2097"]}
{"title": "An evaluation framework for plagiarism detection\n", "abstract": " We present an evaluation framework for plagiarism detection. 1 The framework provides performance measures that address the specifics of plagiarism detection, and the PAN-PC-10 corpus, which contains 64 558 artificial and 4000 simulated plagiarism cases, the latter generated via Amazon\u2019s Mechanical Turk. We discuss the construction principles behind the measures and the corpus, and we compare the quality of our corpus to existing corpora. Our analysis gives empirical evidence that the construction of tailored training corpora for plagiarism detection can be automated, and hence be done on a large scale.", "num_citations": "361\n", "authors": ["2097"]}
{"title": "Overview of the 3rd international competition on plagiarism detection\n", "abstract": " [EN] This paper overviews eleven plagiarism detectors that have been developed and evaluated within PAN\u201911. We survey the detection approaches developed for the two sub-tasks \u201cexternal plagiarism detection\u201d and \u201cintrinsic plagiarism detection,\u201d and we report on their detailed evaluation based on the third revised edition of the PAN plagiarism corpus PAN-PC-11.", "num_citations": "304\n", "authors": ["2097"]}
{"title": "Overview of the 3rd Author Profiling Task at PAN 2015\n", "abstract": " We overview the framework and the results for the Author Profiling Shared Task organised at PAN 2015. This year\u2019s task aims at identifying age, gender, and personality traits of Twitter users. With the help of an online personality test1 a dataset was collected from Twitter with annotations about age, gender, and the Big Five personality traits in the four languages English, Spanish, Italian, and Dutch. The paper in hand presents the approaches of 22 participants.", "num_citations": "290\n", "authors": ["2097"]}
{"title": "Overview of the 5th international competition on plagiarism detection\n", "abstract": " This paper overviews 18 plagiarism detectors that have been evaluated within the fifth international competition on plagiarism detection at PAN 2013. We report on their performances for the two tasks source retrieval and text alignment of external plagiarism detection. Furthermore, we continue last year\u2019s initiative to invite software submissions instead of run submissions, and, re-evaluate this year\u2019s submissions on last year\u2019s evaluation corpora and vice versa, thus demonstrating the benefits of software submissions in terms of reproducibility.", "num_citations": "273\n", "authors": ["2097"]}
{"title": "A Wikipedia-based multilingual retrieval model\n", "abstract": " This paper introduces CL-ESA, a new multilingual retrieval model for the analysis of cross-language similarity. The retrieval model exploits the multilingual alignment of Wikipedia: given a document d written in language L we construct a concept vector d for d, where each dimension i in d quantifies the similarity of d with respect to a document  chosen from the \u201cL-subset\u201d of Wikipedia. Likewise, for a second document d\u2032 written in language L\u2032, , we construct a concept vector d\u2032, using from the L\u2032-subset of the Wikipedia the topic-aligned counterparts  of our previously chosen documents.               Since the two concept vectors d and d\u2032 are collection-relative representations of d and d\u2032 they are language-independent. I. e., their similarity can directly be computed with the cosine similarity measure, for instance.               We present results of an extensive analysis that demonstrates the power of\u00a0\u2026", "num_citations": "264\n", "authors": ["2097"]}
{"title": "Cross-language plagiarism detection\n", "abstract": " Cross-language plagiarism detection deals with the automatic identification and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the document that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual plagiarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are\u00a0\u2026", "num_citations": "261\n", "authors": ["2097"]}
{"title": "Overview of the 5th author profiling task at pan 2017: Gender and language variety identification in twitter\n", "abstract": " This overview presents the framework and the results of the Author Profiling task at PAN 2017. The objective of this year is to address gender and language variety identification. For this purpose a corpus from Twitter has been provided for four different languages: Arabic, English, Portuguese, and Spanish. Altogether, the approaches of 22 participants are evaluated.", "num_citations": "227\n", "authors": ["2097"]}
{"title": "Automatic vandalism detection in Wikipedia\n", "abstract": " We present results of a new approach to detect destructive article revisions, so-called vandalism, in Wikipedia. Vandalism detection is a one-class classification problem, where vandalism edits are the target to be identified among all revisions. Interestingly, vandalism detection has not been addressed in the Information Retrieval literature by now. In this paper we discuss the characteristics of vandalism as humans recognize it and develop features to render vandalism detection as a machine learning task. We compiled a large number of vandalism edits in a corpus, which allows for the comparison of existing and new detection approaches. Using logistic regression we achieve 83% precision at 77% recall with our model. Compared to the rule-based methods that are currently applied in Wikipedia, our approach increases the F-Measure performance by 49% while being faster at the same time.", "num_citations": "215\n", "authors": ["2097"]}
{"title": "Overview of the 4th author profiling task at PAN 2016: cross-genre evaluations\n", "abstract": " This overview presents the framework and the results of the Author Profiling task at PAN 2016. The objective was to predict age and gender from a cross-genre perspective. For this purpose a corpus from Twitter has been provided for training, and different corpora from social media, blogs, essays, and reviews have been provided for evaluation. Altogether, the approaches of 22 participants were evaluated.", "num_citations": "213\n", "authors": ["2097"]}
{"title": "Overview of the author identification task at PAN 2014\n", "abstract": " The author identification task at PAN-2014 focuses on author verification. Similar to PAN-2013 we are given a set of documents by the same author along with exactly one document of questioned authorship, and the task is to determine whether the known and the questioned documents are by the same author or not. In comparison to PAN-2013, a significantly larger corpus was built comprising hundreds of documents in four natural languages (Dutch, English, Greek, and Spanish) and four genres (essays, reviews, novels, opinion articles). In addition, more suitable performance measures are used focusing on the accuracy and the confidence of the predictions as well as the ability of the submitted methods to leave some problems unanswered in case there is great uncertainty. To this end, we adopt the c@ 1 measure, originally proposed for the question answering task. We received 13 software submissions that were evaluated in the TIRA framework. Analytical evaluation results are presented where one language-independent approach serves as a challenging baseline. Moreover, we continue the successful practice of the PAN labs to examine meta-models based on the combination of all submitted systems. Last but not least, we provide statistical significance tests to demonstrate the important differences between the submitted approaches.", "num_citations": "202\n", "authors": ["2097"]}
{"title": "TIRA Integrated Research Architecture\n", "abstract": " Data and software are immaterial. Scientists in computer science hence have the unique chance to let other scientists easily reproduce their findings. Similarly, and with the same ease, the organization of shared tasks, i.e., the collaborative search for new algorithms given a predefined problem, is possible. Experience shows that the potential of reproducibility is hardly tapped in either case. Based on this observation, and driven by the ambitious goal to find the best solutions for certain problems in our research field, we have been developing the TIRA Integrated Research Architecture. Within TIRA, the reproducibility requirement got top priority right from the start. This chapter introduces the platform, its design requirements, its workflows from both the participants\u2019 and the organizers\u2019 perspectives, alongside a report on user experience and usage scenarios.", "num_citations": "198\n", "authors": ["2097"]}
{"title": "Clickbait detection\n", "abstract": " This paper proposes a new model for the detection of clickbait, i.e., short messages that lure readers to click a link. Clickbait is primarily used by online content publishers to increase their readership, whereas its automatic detection will give readers a way of filtering their news stream. We contribute by compiling the first clickbait corpus of 2992 Twitter tweets, 767 of which are clickbait, and, by developing a clickbait model based on 215 features that enables a random forest classifier to achieve 0.79\u00a0ROC-AUC at 0.76\u00a0precision and 0.76\u00a0recall.", "num_citations": "196\n", "authors": ["2097"]}
{"title": "Overview of the 1st International Competition on Plagiarism Detection\n", "abstract": " The 1st International Competition on Plagiarism Detection, held in conjunction with the 3rd PAN workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse, brought together researchers from many disciplines around the exciting retrieval task of automatic plagiarism detection. The competition was divided into the subtasks external plagiarism detection and intrinsic plagiarism detection, which were tackled by 13 participating groups.An important by-product of the competition is an evaluation framework for plagiarism detection, which consists of a large-scale plagiarism corpus and detection quality measures. The framework may serve as a unified test environment to compare future plagiarism detection research. In this paper we describe the corpus design and the quality measures, survey the detection approaches developed by the participants, and compile the achieved performance results of the competitors.", "num_citations": "188\n", "authors": ["2097"]}
{"title": "Overview of the 2nd Author Profiling Task at PAN 2014\n", "abstract": " [EN] This overview presents the framework and the results for the Author Profiling task at PAN 2014. Objective of this year is the analysis of the adaptability of the detection approaches when given different genres. For this purpose a corpus with four different parts (subcorpora) has been compiled: social media, Twitter, blogs, and hotel reviews. The construction of the Twitter subcorpus happened in cooperation with RepLab in order to investigate also a reputational perspective. Altogether, the approaches of 10 participants are evaluated.", "num_citations": "166\n", "authors": ["2097"]}
{"title": "Strategies for retrieving plagiarized documents\n", "abstract": " For the identification of plagiarized passages in large document collections we present retrieval strategies which rely on stochastic sampling and chunk indexes. Using the entire Wikipedia corpus we compile n-gram indexes and compare them to a new kind of fingerprint index in a plagiarism analysis use case. Our index provides an analysis speed-up by factor 1.5 and is an order of magnitude smaller, while being equivalent in terms of precision and recall.", "num_citations": "138\n", "authors": ["2097"]}
{"title": "Overview of the 6th author profiling task at pan 2018: multimodal gender identification in twitter\n", "abstract": " This overview presents the framework and the results of the Author Profiling shared task at PAN 2018. The objective of this year\u2019s task is to address gender identification from a multimodal perspective, where not only texts but also images are given. For this purpose a corpus with Twitter data has been provided, covering the languages Arabic, English, and Spanish. Altogether, the approaches of 23 participants are evaluated.", "num_citations": "119\n", "authors": ["2097"]}
{"title": "Building an Argument Search Engine for the Web\n", "abstract": " Computational argumentation is expected to play a critical role in the future of web search. To make this happen, many search-related questions must be revisited, such as how people query for arguments, how to mine arguments from the web, or how to rank them. In this paper, we develop an argument search framework for studying these and further questions. The framework allows for the composition of approaches to acquiring, mining, assessing, indexing, querying, retrieving, ranking, and presenting arguments while relying on standard infrastructure and interfaces. Based on the framework, we build a prototype search engine, called args, that relies on an initial, freely accessible index of nearly 300k arguments crawled from reliable web resources. The framework and the argument search engine are intended as an environment for collaborative research on computational argumentation and its practical evaluation.", "num_citations": "119\n", "authors": ["2097"]}
{"title": "Semeval-2019 task 4: Hyperpartisan news detection\n", "abstract": " Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one is able to reliably compute this meta information, news articles may be automatically tagged, this way encouraging or discouraging readers to consume the text. It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art. We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision. The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run. The best team achieved an accuracy of 0.822 on a balanced sample (yes: no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.", "num_citations": "105\n", "authors": ["2097"]}
{"title": "Paraphrase Acquisition via Crowdsourcing and Machine Learning\n", "abstract": " To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to\u00a0\u2026", "num_citations": "84\n", "authors": ["2097"]}
{"title": "Crowdsourcing a wikipedia vandalism corpus\n", "abstract": " We report on the construction of the PAN Wikipedia vandalism corpus, PAN-WVC-10, using Amazon's Mechanical Turk. The corpus compiles 32452 edits on 28468 Wikipedia articles, among which 2391 vandalism edits have been identified. 753 human annotators cast a total of 193022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved level of agreement was analyzed in order to label an edit as\" regular\" or\" vandalism.\" The corpus is available free of charge.", "num_citations": "83\n", "authors": ["2097"]}
{"title": "Overview of the author identification task at PAN-2018: cross-domain authorship attribution and style change detection\n", "abstract": " Overview of the author identification task at PAN-2018: Cross-domain authorship attribution and style change detection \u041a\u041e\u0420\u0417\u0418\u041d\u0410 \u041f\u041e\u0418\u0421\u041a \u041d\u0410\u0412\u0418\u0413\u0410\u0422\u041e\u0420 \u0416\u0423\u0420\u041d\u0410\u041b\u042b \u041a\u041d\u0418\u0413\u0418 \u041f\u0410\u0422\u0415\u041d\u0422\u042b \u041f\u041e\u0418\u0421\u041a \u0410\u0412\u0422\u041e\u0420\u042b \u041e\u0420\u0413\u0410\u041d\u0418\u0417\u0410\u0426\u0418\u0418 \u041a\u041b\u042e\u0427\u0415\u0412\u042b\u0415 \u0421\u041b\u041e\u0412\u0410 \u0420\u0423\u0411\u0420\u0418\u041a\u0410\u0422\u041e\u0420 \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u0430\u044f \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0430 \u0421\u0415\u0421\u0421\u0418\u042f \u041a\u041e\u041d\u0422\u0410\u041a\u0422\u042b \u0418\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0418\u042f \u041e \u041f\u0423\u0411\u041b\u0418\u041a\u0410\u0426\u0418\u0418 eLIBRARY ID: 36215537 OVERVIEW OF THE AUTHOR IDENTIFICATION TASK AT PAN-2018: CROSS-DOMAIN AUTHORSHIP ATTRIBUTION AND STYLE CHANGE DETECTION KESTEMONT M. , DAELEMANS W. , TSCHUGGNALL M. , SPECHT G. , STAMATATOS E. , STEIN B. , POTTHAST M. 1 University of Antwerp 2 University of Innsbruck 3 University of the Aegean 4 Bauhaus-Universit\u00e4t Weimar 5 Leipzig University \u0422\u0438\u043f: \u0441\u0442\u0430\u0442\u044c\u044f \u0432 \u0441\u0431\u043e\u0440\u043d\u0438\u043a\u0435 \u0442\u0440\u0443\u0434\u043e\u0432 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0438 \u042f\u0437\u044b\u043a: \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u0413\u043e\u0434 \u0438\u0437\u0434\u0430\u043d\u0438\u044f: 2018 \u0418\u0421\u0422\u041e\u0427\u041d\u0418\u041a: CEUR WORKSHOP PROCEEDINGS 19. \u0421\u0435\u0440. \"CLEF 2018 - \u2026", "num_citations": "82\n", "authors": ["2097"]}
{"title": "Query segmentation revisited\n", "abstract": " We address the problem of query segmentation: given a keyword query, the task is to group the keywords into phrases, if possible. Previous approaches to the problem achieve reasonable segmentation performance but are tested only against a small corpus of manually segmented queries. In addition, many of the previous approaches are fairly intricate as they use expensive features and are difficult to be reimplemented.", "num_citations": "82\n", "authors": ["2097"]}
{"title": "Overview of PAN\u201917\n", "abstract": " The PAN\u00a02017 shared tasks on digital text forensics were held in conjunction with the annual CLEF conference. This paper gives a high-level overview of each of the three shared tasks organized this year, namely author identification, author profiling, and author obfuscation. For each task, we give a brief summary of the evaluation data, performance measures, and results obtained. Altogether, 29\u00a0participants submitted a total of 33\u00a0pieces of software for evaluation, whereas 4\u00a0participants submitted to more than one task. All submitted software has been deployed to the TIRA evaluation platform, where it remains hosted for reproducibility purposes.", "num_citations": "81\n", "authors": ["2097"]}
{"title": "Overview of PAN'17: author identification, author profiling, and author obfuscation\n", "abstract": " The PAN\u00a02017 shared tasks on digital text forensics were held in conjunction with the annual CLEF conference. This paper gives a high-level overview of each of the three shared tasks organized this year, namely author identification, author profiling, and author obfuscation. For each task, we give a brief summary of the evaluation data, performance measures, and results obtained. Altogether, 29\u00a0participants submitted a total of 33\u00a0pieces of software for evaluation, whereas 4\u00a0participants submitted to more than one task. All submitted software has been deployed to the TIRA evaluation platform, where it remains hosted for reproducibility purposes.", "num_citations": "81\n", "authors": ["2097"]}
{"title": "Webis: An ensemble for twitter sentiment detection\n", "abstract": " We reproduce four Twitter sentiment classification approaches that participated in previous SemEval editions with diverse feature sets. The reproduced approaches are combined in an ensemble, averaging the individual classifiers\u2019 confidence scores for the three classes (positive, neutral, negative) and deciding sentiment polarity based on these averages. The experimental evaluation on Sem-Eval data shows our re-implementations to slightly outperform their respective originals. Moreover, not too surprisingly, the ensemble of the reproduced approaches serves as a strong baseline in the current edition where it is top-ranked on the 2015 test set.", "num_citations": "81\n", "authors": ["2097"]}
{"title": "The suffix tree document model revisited\n", "abstract": " In text-based information retrieval, which is the predominant retrieval task at present, several document models have been proposed, such as boolean, probabilistic, or (extended) vector models [Baeza-Yates and Ribeiro-Neto 1999]. Interestingly, the suffix tree document model is usually not discussed in the literature on the subject though it comes along with a property that sets it apart from the other models: It encodes information about word order. The suffix tree document model owes much of its popularity from the Viv\u00edsimo search engine, which operationalizes on-the-fly categorization of Internet search results. While the classical document models can be considered as vectors of words, the suffix tree document model as well as the related similarity measures are graph-based. Both types of document models provide an efficient means to compute document similarities, and, according to various publications, both types of document models work well in practice. However, there is no comparison between both paradigms that explains the concepts of one in terms of the other, or that contrasts their advantages and disadvantages with respect to certain retrieval tasks. In this paper we start to tackle this gap by shading light on the following questions:(1) How does similarity computation work in the suffix tree document model?(2) Based on the insights of Question 1, is it possible to combine concepts of both document model types within classification or categorization tasks?(3) Which of the document model types is more powerful with respect to unsupervised document classification?", "num_citations": "77\n", "authors": ["2097"]}
{"title": "On Textual Analysis and Machine Learning for Cyberstalking Detection\n", "abstract": " Cyber security has become a\u00a0major concern for users and businesses alike. Cyberstalking and harassment have been identified as a\u00a0growing anti-social problem. Besides detecting cyberstalking and harassment, there is the need to gather digital evidence, often by the victim. To this end, we provide an overview of and discuss relevant technological means, in particular coming from text analytics as well as machine learning, that are capable to address the above challenges. We present a\u00a0framework for the detection of text-based cyberstalking and the role and challenges of some core techniques such as author identification, text classification and personalisation. We then discuss PAN, a\u00a0network and evaluation initiative that focusses on digital text forensics, in particular author identification.", "num_citations": "76\n", "authors": ["2097"]}
{"title": "Vandalism Detection in Wikidata\n", "abstract": " Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation. Its knowledge is increasingly used within Wikipedia itself and various other kinds of information systems, imposing high demands on its integrity. Wikidata can be edited by anyone and, unfortunately, it frequently gets vandalized, exposing all information systems using it to the risk of spreading vandalized and falsified information. In this paper, we present a new machine learning-based approach to detect vandalism in Wikidata. We propose a set of 47 features that exploit both content and context information, and we report on 4 classifiers of increasing effectiveness tailored to this learning task. Our approach is evaluated on the recently published Wikidata Vandalism Corpus WDVC-2015 and it achieves an area under curve value of the receiver operating characteristic, ROC-AUC, of 0.991. It significantly outperforms the state of the art\u00a0\u2026", "num_citations": "72\n", "authors": ["2097"]}
{"title": "Clustering by authorship within and across documents\n", "abstract": " The vast majority of previous studies in authorship attribution assume the existence of documents (or parts of documents) labeled by authorship to be used as training instances in either closed-set or open-set attribution. However, in several applications it is not easy or even possible to find such labeled data and it is necessary to build unsupervised attribution models that are able to estimate similarities/differences in personal style of authors. The shared tasks on author clustering and author diarization at PAN 2016 focus on such unsupervised authorship attribution problems. The former deals with single-author documents and aims at grouping documents by authorship and establishing authorship links between documents. The latter considers multi-author documents and attempts to segment a document into authorial components, a task strongly associated with intrinsic plagiarism detection. This paper presents an overview of the two tasks including evaluation datasets, measures, results, as well as a survey of a total of 10 submissions (8 for author clustering and 2 for author diarization).", "num_citations": "69\n", "authors": ["2097"]}
{"title": "Recent Trends in Digital Text Forensics and Its Evaluation\n", "abstract": " This paper outlines the concepts and achievements of our evaluation lab on digital text forensics, PAN\u00a013, which called for original research and development on plagiarism detection, author identification, and author profiling. We present a standardized evaluation framework for each of the three tasks and discuss the evaluation results of the altogether 58\u00a0submitted contributions. For the first time, instead of accepting the output of software runs, we collected the softwares themselves and run them on a computer cluster at our site. As evaluation and experimentation platform we use TIRA, which is being developed at the Webis Group in Weimar. TIRA can handle large-scale software submissions by means of virtualization, sandboxed execution, tailored unit testing, and staged submission. In addition to the achieved evaluation results, a major achievement of our lab is that we now have the largest collection of\u00a0\u2026", "num_citations": "66\n", "authors": ["2097"]}
{"title": "Overview of the PAN/CLEF 2015 Evaluation Lab\n", "abstract": " This paper presents an overview of the PAN/CLEF evaluation lab. During the last decade, PAN has been established as the main forum of text mining research focusing on the identification of personal traits of authors left behind in texts unintentionally. PAN 2015 comprises three tasks: plagiarism detection, author identification and author profiling studying important variations of these problems. In plagiarism detection, community-driven corpus construction is introduced as a new way of developing evaluation resources with diversity. In author identification, cross-topic and cross-genre author verification (where the texts of known and unknown authorship do not match in topic and/or genre) is introduced. A new corpus was built for this challenging, yet realistic, task covering four languages. In author profiling, in addition to usual author demographics, such as gender and age, five personality traits are\u00a0\u2026", "num_citations": "64\n", "authors": ["2097"]}
{"title": "Crowdsourcing a Large Corpus of Clickbait on Twitter\n", "abstract": " Clickbait has become a nuisance on social media. To address the urging task of clickbait detection, we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazon\u2019s Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017. Download: https://webis. de/data/webis-clickbait-17. html Challenge: https://clickbait-challenge. org", "num_citations": "63\n", "authors": ["2097"]}
{"title": "Overview of the author identification task at PAN-2017: style breach detection and author clustering\n", "abstract": " Several authorship analysis tasks require the decomposition of a multiauthored text into its authorial components. In this regard two basic prerequisites need to be addressed:(1) style breach detection, ie, the segmenting of a text into stylistically homogeneous parts, and (2) author clustering, ie, the grouping of paragraph-length texts by authorship. In the current edition of PAN we focus on these two unsupervised authorship analysis tasks and provide both benchmark data and an evaluation framework to compare different approaches. We received three submissions for the style breach detection task and six submissions for the author clustering task; we analyze the submissions with different baselines while highlighting their strengths and weaknesses.", "num_citations": "60\n", "authors": ["2097"]}
{"title": "ChatNoir: a search engine for the ClueWeb09 corpus\n", "abstract": " We present the ChatNoir search engine which indexes the entire English part of the ClueWeb09 corpus. Besides Carnegie Mellon's Indri system, ChatNoir is the second publicly available search engine for this corpus. It implements the classic BM25F information retrieval model including PageRank and spam likelihood. The search engine is scalable and returns the first results within three seconds, which is significantly faster than Indri. A convenient API allows for implementing reproducible experiments based on retrieving documents from the ClueWeb09 corpus. The search engine has successfully accomplished a load test involving 100,000 queries.", "num_citations": "60\n", "authors": ["2097"]}
{"title": "Author Obfuscation: Attacking the State of the Art in Authorship Verification.\n", "abstract": " We report on the first large-scale evaluation of author obfuscation approaches built to attack authorship verification approaches: the impact of 3 obfuscators on the performance of a total of 44 authorship verification approaches has been measured and analyzed. The best-performing obfuscator successfully impacts the decision-making process of the authorship verifiers on average in about 47% of the cases, causing them to misjudge a given pair of documents as having been written by \u201cdifferent authors\u201d when in fact they would have decided otherwise if one of them had not been automatically obfuscated. The evaluated obfuscators have been submitted to a shared task on author obfuscation that we organized at the PAN 2016 lab on digital text forensics. We contribute further by surveying the literature on author obfuscation, by collecting and organizing evaluation methodology for this domain, and by introducing performance measures tailored to measuring the impact of author obfuscation on authorship verification.", "num_citations": "58\n", "authors": ["2097"]}
{"title": "Crowdsourcing interaction logs to understand text reuse from the web\n", "abstract": " We report on the construction of the Webis text reuse corpus 2012 for advanced research on text reuse. The corpus compiles manually written documents obtained from a completely controlled, yet representative environment that emulates the web. Each of the 297 documents in the corpus is about one of the 150 topics used at the TREC Web Tracks 2009\u20132011, thus forming a strong connection with existing evaluation efforts. Writers, hired at the crowdsourcing platform oDesk, had to retrieve sources for a given topic and to reuse text from what they found. Part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents. This will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party\u2014a setting which has not been studied so far. In addition, the corpus provides an original resource for the evaluation of text reuse and plagiarism detectors, where currently only less realistic resources are employed.", "num_citations": "57\n", "authors": ["2097"]}
{"title": "Overview of PAN\u201916\n", "abstract": " This paper presents an overview of the PAN/CLEF evaluation lab. During the last decade, PAN has been established as the main forum of digital text forensic research. PAN 2016 comprises three shared tasks: (i) author identification, addressing author clustering and diarization (or intrinsic plagiarism detection); (ii) author profiling, addressing age and gender prediction from a cross-genre perspective; and (iii) author obfuscation, addressing author masking and obfuscation evaluation. In total, 35 teams participated in all three shared tasks of PAN 2016 and, following the practice of previous editions, software submissions were required and evaluated within the TIRA experimentation framework.", "num_citations": "52\n", "authors": ["2097"]}
{"title": "Who wrote the web? Revisiting influential author identification research applicable to information retrieval\n", "abstract": " In this paper, we revisit author identification research by conducting a new kind of large-scale reproducibility study: we select 15 of the most influential papers for author identification and recruit a group of students to reimplement them from scratch. Since no open source implementations have been released for the selected papers to date, our public release will have a significant impact on researchers entering the field. This way, we lay the groundwork for integrating author identification with information retrieval to eventually scale the former to the web. Furthermore, we assess the reproducibility of all reimplemented papers in detail, and conduct the first comparative evaluation of all approaches on three well-known corpora.", "num_citations": "51\n", "authors": ["2097"]}
{"title": "Overview of PAN 2018\n", "abstract": " PAN 2018 explores several authorship analysis tasks enabling a systematic comparison of competitive approaches and advancing research in digital text forensics. More specifically, this edition of PAN introduces a shared task in cross-domain authorship attribution, where texts of known and unknown authorship belong to distinct domains, and another task in style change detection that distinguishes between single-author and multi-author texts. In addition, a shared task in multimodal author profiling examines, for the first time, a combination of information from both texts and images posted by social media users to estimate their gender. Finally, the author obfuscation task studies how a text by a certain author can be paraphrased so that existing author identification tools are confused and cannot recognize the similarity with other texts of the same author. New corpora have been built to support these\u00a0\u2026", "num_citations": "50\n", "authors": ["2097"]}
{"title": "TL; DR: Mining Reddit to Learn Automatic Summarization\n", "abstract": " Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a \u201cTL; DR\u201d to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.", "num_citations": "44\n", "authors": ["2097"]}
{"title": "Twitter sentiment detection via ensemble classification using averaged confidence scores\n", "abstract": " We reproduce three classification approaches with diverse feature sets for the task of classifying the sentiment expressed in a given tweet as either positive, neutral, or negative. The reproduced approaches are also combined in an ensemble, averaging the individual classifiers\u2019 confidence scores for the three classes and deciding sentiment polarity based on these averages. Our experimental evaluation on SemEval\u00a0data shows our re-implementations to slightly outperform their respective originals. Moreover, in the SemEval Twitter sentiment detection tasks of 2013 and 2014, the ensemble of reproduced approaches would have been ranked in the top-5 among 50\u00a0participants. An error analysis shows that the ensemble classifier makes few severe misclassifications, such as identifying a positive sentiment in a negative tweet or vice versa. Instead, it tends to misclassify tweets as neutral that are not, which\u00a0\u2026", "num_citations": "42\n", "authors": ["2097"]}
{"title": "Towards optimum query segmentation: in doubt without\n", "abstract": " Query segmentation is the problem of identifying those keywords in a query, which together form compound concepts or phrases like\" new york times\". Such segments can help a search engine to better interpret a user's intents and to tailor the search results more appropriately. Our contributions to this problem are threefold.(1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations.(2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures.(3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to (partially) leave queries without any segmentation.", "num_citations": "41\n", "authors": ["2097"]}
{"title": "Data Acquisition for Argument Search: The args. me Corpus\n", "abstract": " Argument search is the study of search engine technology that can retrieve arguments for potentially controversial topics or claims upon user request. The design of an argument search engine is tied to its underlying argument acquisition paradigm. More specifically, the employed paradigm controls the trade-off between retrieval precision and recall and thus determines basic search characteristics: Compiling an exhaustive argument corpus offline benefits precision at the expense of recall, whereas retrieving arguments from the web on-the-fly benefits recall at the expense of precision. This paper presents the new corpus of our argument search engine args.me, which follows the former paradigm. We freely provide the corpus to the community. With 387\u00a0606\u00a0arguments it is one of the largest argument resources available so far. In a qualitative analysis, we compare the args.me corpus acquisition paradigm to\u00a0\u2026", "num_citations": "40\n", "authors": ["2097"]}
{"title": "Opinion summarization of web comments\n", "abstract": " All kinds of Web sites invite visitors to provide feedback on comment boards. Typically, submitted comments are published immediately on the same page, so that new visitors can get an idea of the opinions of previous visitors. Popular multimedia items, such as videos and images, frequently get up to thousands of comments, which is too much to be read in reasonable time. I.e., visitors read, if at all, only the newest comments and hence get an incomplete and possibly misleading picture of the overall opinion. To address this issue we introduce OPINIONCLOUD, a technology to summarize and visualize opinions that are expressed in the form of Web comments.", "num_citations": "40\n", "authors": ["2097"]}
{"title": "Overview of pan 2021: Authorship verification, profiling hate speech spreaders on twitter, and style change detection\n", "abstract": " The paper gives a brief overview of the three shared tasks organized at the PAN\u00a02021 lab on digital text forensics and stylometry hosted at the CLEF conference. The tasks include authorship verification across domains, author profiling for hate speech spreaders, and style change detection for multi-author documents. In part the tasks are new and in part they continue and advance past shared tasks, with the overall goal of advancing the state of the art, providing for an objective evaluation on newly developed benchmark datasets.", "num_citations": "39\n", "authors": ["2097"]}
{"title": "Evaluation-as-a-Service: Overview and Outlook\n", "abstract": " Evaluation in empirical computer science is essential to show progress and assess technologies developed. Several research domains such as information retrieval have long relied on systematic evaluation to measure progress: here, the Cranfield paradigm of creating shared test collections, defining search tasks, and collecting ground truth for these tasks has persisted up until now. In recent years, however, several new challenges have emerged that do not fit this paradigm very well: extremely large data sets, confidential data sets as found in the medical domain, and rapidly changing data sets as often encountered in industry. Also, crowdsourcing has changed the way that industry approaches problem-solving with companies now organizing challenges and handing out monetary awards to incentivize people to work on their challenges, particularly in the field of machine learning. This white paper is based on discussions at a workshop on Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets to participants and have them work on the data locally, but keeping the data central and allowing access via Application Programming Interfaces (API), Virtual Machines (VM) or other possibilities to ship executables. The objective of this white paper are to summarize and compare the current approaches and consolidate the experiences of these approaches to outline the next steps of EaaS, particularly towards sustainable research infrastructures. This white paper summarizes several existing approaches to EaaS and analyzes their usage scenarios and also the advantages and disadvantages. The many factors influencing\u00a0\u2026", "num_citations": "37\n", "authors": ["2097"]}
{"title": "New Issues in Near-duplicate Detection\n", "abstract": " Near-duplicate detection is the task of identifying documents with almost identical content. The respective algorithms are based on fingerprinting; they have attracted considerable attention due to their practical significance for Web retrieval systems, plagiarism analysis, corporate storage maintenance, or social collaboration and interaction in the World Wide Web.               Our paper presents both an integrative view as well as new aspects from the field of nearduplicate detection: (i) Principles and Taxonomy. Identification and discussion of the principles behind the known algorithms for near-duplicate detection, (ii) Corpus Linguistics. Presentation of a corpus that is specifically suited for the analysis and evaluation of near-duplicate detection algorithms. The corpus is public and may serve as a starting point for a standardized collection in this field. (iii) Analysis and Evaluation. Comparison of state-of-the-art\u00a0\u2026", "num_citations": "37\n", "authors": ["2097"]}
{"title": "Source Retrieval for Plagiarism Detection from Large Web Corpora: Recent Approaches.\n", "abstract": " This paper overviews the five source retrieval approaches that have been submitted to the seventh international competition on plagiarism detection at PAN 2015. We compare the performances of these five approaches to the 14 methods submitted in the two previous years (eight from PAN 2013 and six from PAN 2014). For the third year in a row, we invited software submissions instead of run submissions, such that cross-year evaluations are possible. This year\u2019s stand-alone source retrieval overview can thus to some extent also be used as a reference to the different ideas presented in the last three years\u2014the text alignment subtask will be depicted in another individual overview.", "num_citations": "36\n", "authors": ["2097"]}
{"title": "The clickbait challenge 2017: Towards a regression model for clickbait strength\n", "abstract": " Clickbait has grown to become a nuisance to social media users and social media operators alike. Malicious content publishers misuse social media to manipulate as many users as possible to visit their websites using clickbait messages. Machine learning technology may help to handle this problem, giving rise to automatic clickbait detection. To accelerate progress in this direction, we organized the Clickbait Challenge 2017, a shared task inviting the submission of clickbait detectors for a comparative evaluation. A total of 13 detectors have been submitted, achieving significant improvements over the previous state of the art in terms of detection performance. Also, many of the submitted approaches have been published open source, rendering them reproducible, and a good starting point for newcomers. While the 2017 challenge has passed, we maintain the evaluation system and answer to new registrations in support of the ongoing research on better clickbait detectors.", "num_citations": "35\n", "authors": ["2097"]}
{"title": "WSDM Cup 2017: Vandalism Detection and Triple Scoring\n", "abstract": " The WSDM Cup 2017 was a data mining challenge held in conjunction with the 10th International Conference on Web Search and Data Mining (WSDM). It addressed key challenges of knowledge bases today: quality assurance and entity search. For quality assurance, we tackle the task of vandalism detection, based on a dataset of more than 82 million user-contributed revisions of the Wikidata knowledge base, all of which annotated with regard to whether or not they are vandalism. For entity search, we tackle the task of triple scoring, using a dataset that comprises relevance scores for triples from type-like relations including occupation and country of citizenship, based on about 10,000 human relevance judgments. For reproducibility sake, participants were asked to submit their software on TIRA, a cloud-based evaluation platform, and they were incentivized to share their approaches open source.", "num_citations": "34\n", "authors": ["2097"]}
{"title": "The power of naive query segmentation\n", "abstract": " We address the problem of query segmentation: given a keyword query submitted to a search engine, the task is to group the keywords into phrases, if possible. Previous approaches to the problem achieve good segmentation performance on a gold standard but are fairly intricate. Our method is easy to implement and comes with a comparable accuracy.", "num_citations": "34\n", "authors": ["2097"]}
{"title": "Overview of the Style Change Detection Task at PAN 2019.\n", "abstract": " The goal of style change detection is to identify text positions within a multi-author document at which the author switches. Detecting these positions is a crucial part of processing multi-author documents for purposes of authorship identification. In this year\u2019s PAN style change detection task, we asked the participants to answer the following questions for a given document:(1) Given a document, was it written by multiple authors?(2) For each pair of consecutive paragraphs in a given document, is there a style change between these paragraphs? The task is performed and evaluated on two datasets compiled from an English Q&A platform, which differ in their topical breadth (ie, the number of different topics that are covered in the documents contained). The paper in hand introduces style change detection as a task and its underlying dataset, surveys the participants\u2019 submissions, and analyzes their performance.", "num_citations": "32\n", "authors": ["2097"]}
{"title": "Overview of the cross-domain authorship attribution task at {PAN} 2019\n", "abstract": " Authorship identification remains a highly topical research problem in computational text analysis, with many relevant applications in contemporary society and industry. In this edition of PAN, we focus on authorship attribution, where the task is to attribute an unknown text to a previously seen candidate author. Like in the previous edition we continue to work with fanfiction texts (in four Indo-European languages), written by non-professional authors in a crossdomain setting: the unknown texts belong to a different domain than the training material that is available for the candidate authors. An important novelty of this year\u2019s setup is the focus on open-set attribution, meaning that the test texts contain writing samples by previously unseen authors. For these, systems must consequently refrain from an attribution. We received altogether 12 submissions for this task, which we critically assess in this paper. We provide a detailed comparison of these approaches, including three generic baselines.", "num_citations": "32\n", "authors": ["2097"]}
{"title": "Information retrieval in the commentsphere\n", "abstract": " This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as comment-targeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and\u00a0\u2026", "num_citations": "32\n", "authors": ["2097"]}
{"title": "Overview of Touch\u00e9 2020: Argument Retrieval\n", "abstract": " This paper is a condensed report on Touch\u00e9: the first shared task on argument retrieval that was held at CLEF\u00a02020. With the goal to create a collaborative platform for research in argument retrieval, we run two tasks: (1)\u00a0supporting individuals in finding arguments on socially important topics and (2)\u00a0supporting individuals with arguments on everyday personal decisions.", "num_citations": "31\n", "authors": ["2097"]}
{"title": "Towards Vandalism Detection in Knowledge Bases: Corpus Construction and Analysis\n", "abstract": " We report on the construction of the Wikidata Vandalism Corpus WDVC-2015, the first corpus for vandalism in knowledge bases. Our corpus is based on the entire revision history of Wikidata, the knowledge base underlying Wikipedia. Among Wikidata's 24 million manual revisions, we have identified more than 100,000 cases of vandalism. An in-depth corpus analysis lays the groundwork for research and development on automatic vandalism detection in public knowledge bases. Our analysis shows that 58% of the vandalism revisions can be found in the textual portions of Wikidata, and the remainder in structural content, eg, subject-predicate-object triples. Moreover, we find that some vandals also target Wikidata content whose manipulation may impact content displayed on Wikipedia, revealing potential vulnerabilities. Given today's importance of knowledge bases for information systems, this shows that public\u00a0\u2026", "num_citations": "31\n", "authors": ["2097"]}
{"title": "Corpus and Evaluation Measures for Automatic Plagiarism Detection\n", "abstract": " Corpus and Evaluation Measures for Automatic Plagiarism Detection Page 1 Corpus and Evaluation Measures for Automatic Plagiarism Detection Alberto Barr\u00f3n-Cede\u00f1o1, Martin Potthast2, Paolo Rosso1, Benno Stein2, Andreas Eiselt2 1NLE Lab, Universidad Polit\u00e9cnica de Valencia, Spain {lbarron, prosso}@dsic.upv.es 2Webis, Bauhaus-Universit\u00e4t Weimar, Germany {martin.potthast, benno.stein, andreas.eiselt}@uni-weimar.de LREC 2010 May, 2010 Corpus & Evaluation Measures for Plagiarism Detection NLEL@UPV & Webis@BUW 1/25 Page 2 Outline Introduction PAN-PC-09 Plagiarism Corpus Evaluation Measures PAN Competition Final Remarks Corpus & Evaluation Measures for Plagiarism Detection NLEL@UPV & Webis@BUW 2/25 Page 3 Introduction Text reuse \u2022 The reuse (even after modification) of text. (from [Clough et al., 2002], [IEEE, 2008], and [Bierce, 1911]) Corpus & Evaluation Measures for \u2026", "num_citations": "31\n", "authors": ["2097"]}
{"title": "Overview of PAN 2019: bots and gender profiling, celebrity profiling, cross-domain authorship attribution and style change detection\n", "abstract": " We briefly report on the four shared tasks organized as part of the PAN\u00a02019 evaluation lab on digital text forensics and authorship analysis. Each task is introduced, motivated, and the results obtained are presented. Altogether, the four tasks attracted 373\u00a0registrations, yielding 72\u00a0successful submissions. This, and the fact that we continue to invite the submission of software rather than its run output using the TIRA experimentation platform, demarcates a good start into the second decade of PAN evaluations labs.", "num_citations": "29\n", "authors": ["2097"]}
{"title": "Argument Search: Assessing Argument Relevance\n", "abstract": " We report on the first user study on assessing argument relevance. Based on a search among more than 300,000 arguments, four standard retrieval models are compared on 40 topics for 20 controversial issues: every issue has one topic with a biased stance and another neutral one. Following TREC, the top results of the different models on a topic were pooled and relevance-judged by one assessor per topic. The assessors also judged the arguments' rhetorical, logical, and dialectical quality, the results of which were cross-referenced with the relevance judgments. Furthermore, the assessors were asked for their personal opinion, and whether it matched the predefined stance of a topic. Among other results, we find that Terrier's implementations of DirichletLM and DPH are on par, significantly outperforming TFIDF and BM25. The judgments of relevance and quality hardly correlate, giving rise to a more diverse set\u00a0\u2026", "num_citations": "29\n", "authors": ["2097"]}
{"title": "A Decade of Shared Tasks in Digital Text Forensics at PAN\n", "abstract": " Digital text forensics aims at examining the originality and credibility of information in electronic documents and, in this regard, to extract and analyze information about the authors of these documents. The research field has been substantially developed during the last decade. PAN is a series of shared tasks that started in 2009 and significantly contributed to attract the attention of the research community in well-defined digital text forensics tasks. Several benchmark datasets have been developed to assess the state-of-the-art performance in a wide range of tasks. In this paper, we present the evolution of both the examined tasks and the developed datasets during the last decade. We also briefly introduce the upcoming PAN\u00a02019 shared tasks.", "num_citations": "29\n", "authors": ["2097"]}
{"title": "Towards Data Submissions for Shared Tasks: First Experiences for the Task of Text Alignment\n", "abstract": " This paper reports on the organization of a new kind of shared task that outsources the creation of evaluation resources to its participants. We introduce the concept of data submissions for shared tasks, and we use our previous shared task on text alignment as a testbed. A total of eight evaluation datasets have been submitted by as many participating teams. To validate the submitted datasets, they have been manually peer-reviewed by the participants. Moreover, the submitted datasets have been fed to 31 text alignment approaches in order to learn about the datasets\u2019 difficulty. The text alignment implementations have been submitted to our shared task in previous years and since been kept operational on the evaluation-as-a-service platform TIRA.", "num_citations": "29\n", "authors": ["2097"]}
{"title": "Overview of the Cross-Domain Authorship Verification Task at PAN 2020.\n", "abstract": " Authorship identification remains a highly topical research problem in computational text analysis with many relevant applications in contemporary society and industry. For this edition of PAN, we focused on authorship verification, where the task is to assess whether a pair of documents has been authored by the same individual. Like in previous editions, we continued to work with (English-language) fanfiction, written by non-professional authors. As a novelty, we substantially increased the size of the provided dataset to enable more datahungry approaches. In total, thirteen systems (from ten participating teams) have been submitted, which are substantially more diverse than the submissions from previous years. We provide a detailed comparison of these approaches and two generic baselines. Our findings suggest that the increased scale of the training data boosts the state of the art in the field, but we also confirm the conventional issue that the field struggles with an overreliance on topic-related information.", "num_citations": "28\n", "authors": ["2097"]}
{"title": "Overview of the Author Obfuscation Task at PAN 2018: A New Approach to Measuring Safety.\n", "abstract": " In this paper, we evaluate seven author obfuscation approaches which are supposed to automatically mask an author\u2019s writing style in a given text to render automatic author identification impossible. The approaches are evaluated with regard to their safety, soundness, and sensibleness in terms of beating 44 author identification approaches, retaining the original meaning of the obfuscated text, and producing inconspicuous, human-readable obfuscations, respectively. Regarding the measurement of safety in particular, we introduce a set of new performance measures which are designed to render the performance of obfuscation approaches comparable as the numbers of author identification approaches and evaluation datasets increases, incorporating their respective performance and quality. Based on the new measures, we establish a world ranking of obfuscators.", "num_citations": "28\n", "authors": ["2097"]}
{"title": "Algorithms and corpora for Persian plagiarism detection\n", "abstract": " The task of plagiarism detection is to find passages of text-reuse in a suspicious document. This task is of increasing relevance, since scholars around the world take advantage of the fact that information about nearly any subject can be found on the World Wide Web by reusing existing text instead of writing their own. We organized the Persian PlagDet shared task at PAN 2016 in an effort to promote the comparative assessment of NLP techniques for plagiarism detection with a special focus on plagiarism that appears in a Persian text corpus. The goal of this shared task is to bring together researchers and practitioners around the exciting topic of plagiarism detection and text-reuse detection. We report on the outcome of the shared task, which divides into two subtasks: text alignment and corpus construction. In the first subtask, nine teams participated, whereas the best result achieved was a PlagDet score of\u00a0\u2026", "num_citations": "28\n", "authors": ["2097"]}
{"title": "Report on the Evaluation-as-a-Service (EaaS) Expert Workshop\n", "abstract": " In this report, we summarize the outcome of the \"Evaluation-as-a-Service\" workshop that was held on the 5th and 6th March 2015 in Sierre, Switzerland. The objective of the meeting was to bring together initiatives that use cloud infrastructures, virtual machines, APIs (Application Programming Interface) and related projects that provide evaluation of information retrieval or machine learning tools as a service.", "num_citations": "28\n", "authors": ["2097"]}
{"title": "Overview of the Author Obfuscation Task at PAN 2017: Safety Evaluation Revisited.\n", "abstract": " We report on the second large-scale evaluation of style obfuscation approaches in a shared task on author obfuscation, organized at the PAN 2017 lab on digital text forensics. Author obfuscation means to automatically paraphrase a given text such that state-of-the-art authorship verification approaches misjudge a given pair of documents as having been written by \u201cdifferent authors\u201d if in fact they would have decided otherwise without obfuscation. This year, two new obfuscators are compared to the participants from last year\u2019s task against a total of 44 authorship verification approaches. The best-performing obfuscator successfully impacts the decision-making process of the authorship verifiers significantly. However, as in the last year, the paraphrased texts are often not really human-readable anymore and have some changed context, indicating that there is still way to go to \u201cperfect\u201d automatic obfuscation that (1) tricks verification approaches,(2) keeps the meaning of the original, and (3) is, regarding its obfuscation, unsuspicious to a human eye.", "num_citations": "27\n", "authors": ["2097"]}
{"title": "Celebrity profiling\n", "abstract": " Celebrities are among the most prolific users of social media, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. Our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. We further establish the state of the art\u2019s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time.", "num_citations": "26\n", "authors": ["2097"]}
{"title": "Generalizing Unmasking for Short Texts\n", "abstract": " Authorship verification is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-of-the-art techniques that are optimized for texts of this length: it achieves accuracies of 75\u201380%, while also allowing for easy adjustment to forensic scenarios that require higher levels of confidence in the classification.", "num_citations": "26\n", "authors": ["2097"]}
{"title": "Measuring the descriptiveness of web comments\n", "abstract": " This paper investigates whether Web comments are of descriptive nature, that is, whether the combined text of a set of comments is similar in topic to the commented object. If so, comments may be used in place of the respective object in all kinds of cross-media retrieval tasks. Our experiments reveal that comments on textual objects are indeed descriptive: 10 comments suffice to expect a high similarity between the comments and the commented text; 100-500 comments suffice to replace the commented text in a ranking task, and to measure the contribution of the commenters beyond the commented text.", "num_citations": "26\n", "authors": ["2097"]}
{"title": "How Writers Search: Analyzing the Search and Writing Logs of Non-fictional Essays\n", "abstract": " Many writers of non-fictional texts engage intensively in exploratory web search scenarios during their background research on the essay topic. Though understanding such search behavior is necessary for the development of search engines that specifically support writing tasks, it has neither been systematically recorded nor analyzed. This paper contributes part of the missing research: We report on the outcomes of a large-scale corpus construction initiative to acquire detailed interaction logs of writers who were given a writing task on 150 pre-defined TREC topics. The corpus is freely available to foster research on exploratory search. Each essay is at least 5000 words long and comes with a chronological log of search queries, result clicks, web browsing trails, and fine-grained writing revisions that reflect the task completion status. To ensure reproducibility, a fully-fledged, static web search environment has been\u00a0\u2026", "num_citations": "24\n", "authors": ["2097"]}
{"title": "Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl\n", "abstract": " Elastic ChatNoir (Search:                     www.chatnoir.eu                                         Code:                     www.github.com/chatnoir-eu                                        ) is an Elasticsearch-based search engine offering a freely accessible search interface for the two ClueWeb corpora and the Common Crawl, together about 3\u00a0billion web pages. Running across 130\u00a0nodes, Elastic ChatNoir features subsecond response times comparable to commercial search engines. Unlike most commercial search engines, it also offers a powerful API that is available free of charge to IR\u00a0researchers. Elastic ChatNoir\u2019s main purpose is to serve as a baseline for reproducible IR\u00a0experiments and user studies for the coming years, empowering research at a scale not attainable to many labs beforehand, and to provide a platform for experimenting with new approaches to web search.", "num_citations": "23\n", "authors": ["2097"]}
{"title": "Wordgraph: Keyword-in-context visualization for netspeak's wildcard search\n", "abstract": " The WORDGRAPH helps writers in visually choosing phrases while writing a text. It checks for the commonness of phrases and allows for the retrieval of alternatives by means of wildcard queries. To support such queries, we implement a scalable retrieval engine, which returns high-quality results within milliseconds using a probabilistic retrieval strategy. The results are displayed as WORDGRAPH visualization or as a textual list. The graphical interface provides an effective means for interactive exploration of search results using filter techniques, query expansion, and navigation. Our observations indicate that, of three investigated retrieval tasks, the textual interface is sufficient for the phrase verification task, wherein both interfaces support context-sensitive word choice, and the WORDGRAPH best supports the exploration of a phrase's context or the underlying corpus. Our user study confirms these observations\u00a0\u2026", "num_citations": "22\n", "authors": ["2097"]}
{"title": "Technologies for Reusing Text from the Web\n", "abstract": " Texts from the web can be reused individually or in large quantities. The former is called text reuse and the latter language reuse. We first present a comprehensive overview of the different ways in which text and language is reused today, and how exactly information retrieval technologies can be applied in this respect. The remainder of the thesis then deals with specific retrieval tasks. In general, our contributions consist of models and algorithms, their evaluation, and for that purpose, large-scale corpus construction. The thesis divides into two parts. The first part introduces technologies for text reuse detection, and our contributions are as follows:(1) A unified view of projecting-based and embedding-based fingerprinting for near-duplicate detection and the first time evaluation of fingerprint algorithms on Wikipedia revision histories as a new, large-scale corpus of near-duplicates.(2) A new retrieval model for the quantification of cross-language text similarity, which gets by without parallel corpora. We have evaluated the model in comparison to other models on many different pairs of languages.(3) An evaluation framework for text reuse and particularly plagiarism detectors, which consists of tailored detection performance measures and a large-scale corpus of automatically generated and manually written plagiarism cases. The latter have been obtained via crowdv", "num_citations": "22\n", "authors": ["2097"]}
{"title": "Putting successor variety stemming to work\n", "abstract": " Stemming algorithms find canonical forms for inflected words, e. g. for declined nouns or conjugated verbs. Since such a unification of words with respect to gender, number, time, and case is a language-specific issue, stemming algorithms operationalize a set of linguistically motivated rules for the language in question. The most well-known rule-based algorithm for the English language is from [Porter (1980)].               The paper presents a statistical stemming approach which is based on the analysis of the distribution of word prefixes in a document collection, and which thus is widely language-independent. In particular, our approach tackles the problem of index construction for multi-lingual documents. Related work for statistical stemming either focuses on stemming quality (such as [Bachin et al. (2002) or Bordag (2005)]) or investigates runtime performance ([Mayfield and McNamee (2003)] for example\u00a0\u2026", "num_citations": "22\n", "authors": ["2097"]}
{"title": "Retrieving customary web language to assist writers\n", "abstract": " This paper introduces Netspeak, a Web service which assists writers in finding adequate expressions. To provide statistically relevant suggestions, the service indexes more than 1.8 billion n-grams, n\u2264 5, along with their occurrence frequencies on the Web. If in doubt about a wording, a user can specify a query that has wildcards inserted at those positions where she feels uncertain. Queries define patterns for which a ranked list of matching n-grams along with usage examples are retrieved. The ranking reflects the occurrence frequencies of the n-grams and informs about both absolute and relative usage. Given this choice of customary wordings, one can easily select the most appropriate. Especially second-language speakers can learn about style conventions and language usage. To guarantee response times within milliseconds we have developed an index that considers occurrence probabilities, allowing for a\u00a0\u2026", "num_citations": "21\n", "authors": ["2097"]}
{"title": "Bias Analysis and Mitigation in the Evaluation of Authorship Verification\n", "abstract": " The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art\u2014in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a \u201cBasic and Fairly Flawed\u201d(BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure.", "num_citations": "20\n", "authors": ["2097"]}
{"title": "Evaluation-as-a-service for the computational sciences: overview and outlook\n", "abstract": " Evaluation in empirical computer science is essential to show progress and assess technologies developed. Several research domains such as information retrieval have long relied on systematic evaluation to measure progress: here, the Cranfield paradigm of creating shared test collections, defining search tasks, and collecting ground truth for these tasks has persisted up until now. In recent years, however, several new challenges have emerged that do not fit this paradigm very well: extremely large data sets, confidential data sets as found in the medical domain, and rapidly changing data sets as often encountered in industry. Crowdsourcing has also changed the way in which industry approaches problem-solving with companies now organizing challenges and handing out monetary awards to incentivize people to work on their challenges, particularly in the field of machine learning. This article is based on\u00a0\u2026", "num_citations": "20\n", "authors": ["2097"]}
{"title": "Visual Assessment of Alleged Plagiarism Cases\n", "abstract": " We developed a visual analysis tool to support the verification, assessment, and presentation of alleged cases of plagiarism. The analysis of a suspicious document typically results in a compilation of categorized \u201cfinding spots\u201d. The categorization reveals the way in which the suspicious text fragment was created from the source, e.g. by obfuscation, translation, or by shake and paste. We provide a three\u2010level approach for exploring the finding spots in context. The overview shows the relationship of the entire suspicious document to the set of source documents. A glyph\u2010based view reveals the structural and textual differences and similarities of a set of finding spots and their corresponding source text fragments. For further analysis and editing of the finding spot's assessment, the actual text fragments can be embedded side\u2010by\u2010side in the diffline view. The different views are tied together by versatile navigation and\u00a0\u2026", "num_citations": "20\n", "authors": ["2097"]}
{"title": "Overview of the celebrity profiling task at PAN 2019\n", "abstract": " Celebrity profiling is author profiling applied to celebrities. The focus on celebrities has several advantages: Celebrities are prolific social media users supplying lots of writing samples, lots of personal details are public knowledge, and they try to build a consistent public persona either themselves or with the help of agents. In addition, a number of demographics apply only to this group of people. In this overview of the first shared task on celebrity profiling at PAN 2019, we survey and evaluate eight submitted models that try to predict the gender, the year of birth, the fame, and the occupation of 48,335 English-speaking celebrities based on text obtained from their Twitter timelines. Anticipating some key results we can report that the models work well for predicting binary gender or for distinguishing the most famous celebrities from the less famous ones. Also the occupations sports, politics, and performer are easily identified. The models work less well for the prediction of rare demographics such as non-binary gender and occupations that are not single-topic (eg, manager, science, and professional). Predicting the year of birth works best for the years between ca. 1980-2000 (ie, ages ca. 20-40), but less well for older celebrities, and not at all for younger ones.", "num_citations": "19\n", "authors": ["2097"]}
{"title": "Spatio-temporal analysis of reverted wikipedia edits\n", "abstract": " Little is known about what causes anti-social behavior online. The paper at hand analyzes vandalism and damage in Wikipedia with regard to the time it is conducted and the country it originates from. First, we identify vandalism and damaging edits via ex post facto evidence by mining Wikipedia\u2019s revert graph. Second, we geolocate the cohort of edits from anonymous Wikipedia editors using their associated IP addresses and edit times, showing the feasibility of reliable historic geolocation with respect to country and time zone, even under limited geolocation data. Third, we conduct the first spatio-temporal analysis of vandalism on Wikipedia. Our analysis reveals significant differences for vandalism activities during the day, and for different days of the week, seasons, countries of origin, as well as Wikipedia\u2019s languages. For the analyzed countries, the ratio is typically highest at non-summer workday mornings, with additional peaks after break times. We hence assume that Wikipedia vandalism is linked to labor, perhaps serving as relief from stress or boredom, whereas cultural differences have a large effect. Our results open up avenues for new research on collaborative writing at scale, and advanced technologies to identify and handle antisocial behavior in online communities.", "num_citations": "19\n", "authors": ["2097"]}
{"title": "Evaluating Humor Features on Web Comments\n", "abstract": " Research on automatic humor recognition has developed several features which discriminate funny text from ordinary text. The features have been demonstrated to work well when classifying the funniness of single sentences up to entire blogs. In this paper we focus on evaluating a set of the best humor features reported in the literature over a corpus retrieved from the Slashdot Web site. The corpus is categorized in a community-driven process according to the following tags: funny, informative, insightful, offtopic, flamebait, interesting and troll. These kinds of comments can be found on almost every large Web site; therefore, they impose a new challenge to humor retrieval since they come along with unique characteristics compared to other text types. If funny comments were retrieved accurately, they would be of a great entertainment value for the visitors of a given Web page. Our objective, thus, is to distinguish between an implicit funny comment from a not funny one. Our experiments are preliminary but nonetheless large-scale: 600,000 Web comments. We evaluate the classification accuracy of naive Bayes classifiers, decision trees, and support vector machines. The results suggested interesting findings.", "num_citations": "17\n", "authors": ["2097"]}
{"title": "Abstractive Snippet Generation\n", "abstract": " An abstractive snippet is an originally created piece of text to summarize a web page on a search engine results page. Compared to the conventional extractive snippets, which are generated by extracting phrases and sentences verbatim from a web page, abstractive snippets circumvent copyright issues; even more interesting is the fact that they open the door for personalization. Abstractive snippets have been evaluated as equally powerful in terms of user acceptance and expressiveness\u2014but the key question remains: Can abstractive snippets be automatically generated with sufficient quality?", "num_citations": "14\n", "authors": ["2097"]}
{"title": "Target Inference in Argument Conclusion Generation\n", "abstract": " In argumentation, people state premises to reason towards a conclusion. The conclusion conveys a stance towards some target, such as a concept or statement. Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We thus study the question to what extent an argument\u2019s conclusion can be reconstructed from its premises. In particular, we argue here that a decisive step is to infer a conclusion\u2019s target, and we hypothesize that this target is related to the premises\u2019 targets. We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines. According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.", "num_citations": "13\n", "authors": ["2097"]}
{"title": "Wikipedia in the pocket: indexing technology for near-duplicate detection and high similarity search\n", "abstract": " We develop and implement a new indexing technology which allows us to use complete (and possibly very large) documents as queries, while having a retrieval performance comparable to a standard term query. Our approach aims at retrieval tasks such as near duplicate detection and high similarity search. To demonstrate the performance of our technology we have compiled the search index\" Wikipedia in the Pocket\", which contains about 2 million English and German Wikipedia articles. 1 This index--along with a search interface--fits on a conventional CD (0.7 gigabyte). The ingredients of our indexing technology are similarity hashing and minimal perfect hashing.", "num_citations": "13\n", "authors": ["2097"]}
{"title": "Applying hash-based indexing in text-based information retrieval\n", "abstract": " Hash-based indexing is a powerful technology for similarity search in large document collections [13]. Central idea is the interpretation of hash collisions as similarity indication, provided that an appropriate hash function is given. In this paper we identify basic retrieval tasks which can benefit from this new technology, we relate them to well-known applications and discuss how hash-based indexing is applied. Moreover, we present two recently developed hash-based indexing approaches and compare the achieved performance improvements in real-world retrieval settings. This analysis, which has not been conducted in this or a similar form by now, shows the potential of tailored hash-based indexing methods.", "num_citations": "13\n", "authors": ["2097"]}
{"title": "Reproducible Web Corpora: Interactive Archiving with Automatic Quality Assessment\n", "abstract": " The evolution of web pages from static HTML pages toward dynamic pieces of software has rendered archiving them increasingly difficult. Nevertheless, an accurate, reproducible web archive is a necessity to ensure the reproducibility of web-based research. Archiving web pages reproducibly, however, is currently not part of best practices for web corpus construction. As a result, and despite the ongoing efforts of other stakeholders to archive the web, tools for the construction of reproducible web corpora are insufficient or ill-fitted. This article presents a new tool tailored to this purpose. It relies on emulating user interactions with a web page while recording all network traffic. The customizable user interactions can be replayed on demand, while requests sent by the archived page are served with the recorded responses. The tool facilitates reproducible user studies, user simulations, and evaluations of algorithms\u00a0\u2026", "num_citations": "12\n", "authors": ["2097"]}
{"title": "The NETSPEAK WORDGRAPH: Visualizing keywords in context\n", "abstract": " NETSPEAK helps writers in choosing words while writing a text. It checks for the commonness of phrases and allows for the retrieval of alternatives by means of wildcard queries. To support such queries, we implement a scalable retrieval engine, which returns high-quality results within milliseconds using a probabilistic retrieval strategy. The results are displayed as WORDGRAPH visualization or as a textual list. The graphical interface provides an effective means for interactive exploration of search results using filter techniques, query expansion and navigation. Our observations indicate that, of three investigated retrieval tasks, the textual interface is sufficient for the phrase verification task, wherein both views support context-sensitive word choice, and the WORDGRAPH best supports the exploration of a phrase's context or the underlying corpus. The preferred view for context-sensitive word choice seems to\u00a0\u2026", "num_citations": "12\n", "authors": ["2097"]}
{"title": "Modeling the usefulness of search results as measured by information use\n", "abstract": " The documents retrieved by a web search are useful if the information they contain contributes to some task or information need. To measure search result utility, studies have typically focused on perceived usefulness rather than on actual information use. We investigate the actual usefulness of search results\u2014as indicated by their use as sources in an extensive writing task\u2014and the factors that make a writer successful at retrieving useful sources. Our data comprise 150 essays written by 12 writers whose querying, clicking and writing activities were recorded. By tracking authors\u2019 text reuse behavior, we quantify the search results\u2019 contribution to the task more accurately than before. We model the overall utility of the search results retrieved throughout the writing process using path analysis, and compare a binary utility model (Reuse Events) to one that quantifies a degree of utility (Reuse Amount). The Reuse Events\u00a0\u2026", "num_citations": "11\n", "authors": ["2097"]}
{"title": "Source Retrieval for Web-Scale Text Reuse Detection\n", "abstract": " The first step of text reuse detection addresses the source retrieval problem: given a suspicious document, a set of candidate sources from which text might have been reused have to be retrieved by querying a search engine. Afterwards, in a second step, the retrieved candidates run through a text alignment with the suspicious document in order to identify reused passages. Obviously, any true source of text reuse that is not retrieved during the source retrieval step reduces the overall recall of a reuse detector. Hence, source retrieval is a recall-oriented task, a fact ignored even by experts: Only 3 of 20 teams participating in a respective task at PAN 2012-2016 managed to find more than half of the sources, the best one achieving a recall of only~ 0.59. We propose a new approach that reaches a recall of~ 0.89---a performance gain of~ 51%.", "num_citations": "11\n", "authors": ["2097"]}
{"title": "A Large-Scale Query Spelling Correction Corpus\n", "abstract": " We present a new large-scale collection of 54,772 queries with manually annotated spelling corrections. For 9,170 of the queries (16.74%), spelling variants that are different to the original query are proposed. With its size, our new corpus is an order of magnitude larger than other publicly available query spelling corpora. In addition to releasing the new large-scale corpus, we also provide an implementation of the winner of the Microsoft Speller Challenge from~ 2011 and compare it on the different publicly available corpora to spelling corrections mined from Google and Bing. This way, we also shed some light on the spelling correction performance of state-of-the-art commercial search systems.", "num_citations": "11\n", "authors": ["2097"]}
{"title": "CauseNet: Towards a Causality Graph Extracted from the Web\n", "abstract": " Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi-and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Exploring Argument Retrieval with Transformers.\n", "abstract": " We report on our recent efforts to employ transformer-based models as part of an information retrieval pipeline, using argument retrieval as a benchmark. Transformer models, both causal and bidirectional, are independently used to expand queries using generative approaches as well as to densely embed and retrieve arguments. In particular, we investigate three approaches:(1) query expansion using GPT-2,(2) query expansion using BERT, and orthogonal to these approaches,(3) embedding of documents using Google\u2019s BERT-like universal sentence encoder (USE) combined with a subsequent retrieval step based on a nearest-neighbor search in the embedding space. A comparative evaluation of our approaches at the Touch\u00e9 lab on argument retrieval places our query expansion based on GPT-2 first on the leaderboard with a retrieval performance of 0.808 nDCG@ 5, improving over the task baseline by 6.878%.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "GameStory Task at MediaEval 2018.\n", "abstract": " Game video streams are watched by millions, so that, meanwhile, one can make a living from broadcasting and commenting video games, whereas some have become professional e-sports athletes. E-sports leagues and tournaments have emerged worldwide, where players compete in controlled environments, streaming the matches online, and allowing the audience to discuss and criticize the gameplay. In the GameStory task, held for the second time at MediaEval, we foster research into this exciting domain. Our focus is on analyzing and summarizing video game streams. With the help of ZNIPE. tv, we compiled a high-quality dataset of a Counter-Strike: Global Offensive tournament alongside ground truth labels for two analysis tasks, forming a basis for summarization.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Towards Crowdsourcing Clickbait Labels for YouTube Videos.\n", "abstract": " Clickbait is increasingly used by publishers on social media platforms to spark their users\u2019 natural curiosity and to elicit clicks on their content. Every click earns them display advertisement revenue. Social media users who are tricked into clicking may experience a sense of disappointment or agitation, and social media operators have been observing growing amounts of clickbait on their platforms. As largest video-sharing platform on the web, YouTube, too, suffers from clickbait. Many users and YouTubers alike have complained about this development. In this paper, we lay the foundation for crowdsourcing the first YouTube clickbait corpus by (1) augmenting the YouTube 8M dataset with meta data to obtain a large-scale base population of videos, and by (2) studying the task design suitable to manual clickbait identification.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Overview of the Wikidata Vandalism Detection Task at WSDM Cup 2017\n", "abstract": " We report on the Wikidata vandalism detection task at the WSDM Cup 2017. The task received five submissions for which this paper describes their evaluation and a comparison to state of the art baselines. Unlike previous work, we recast Wikidata vandalism detection as an online learning problem, requiring participant software to predict vandalism in near real-time. The best-performing approach achieves a ROC-AUC of 0.947 at a PR-AUC of 0.458. In particular, this task was organized as a software submission task: to maximize reproducibility as well as to foster future research and development on this task, the participants were asked to submit their working software to the TIRA experimentation platform along with the source code for open source release.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Exploratory Search Missions for TREC Topics.\n", "abstract": " We report on the construction of a new query log corpus that consists of 150 exploratory search missions, each of which corresponds to one of the topics used at the TREC Web Tracks 2009\u20132011. Involved in the construction was a group of 12 professional writers, hired at the crowdsourcing platform oDesk, who were given the task to write essays of 5000 words length about these topics, thereby inducing genuine information needs. The writers used a ClueWeb09 search engine for their research to ensure reproducibility. Thousands of queries, clicks, and relevance judgments were recorded. This paper overviews the research that preceded our endeavors, details the corpus construction, gives quantitative and qualitative analyses of the data obtained, and provides original insights into the querying behavior of writers. With our work we contribute a missing building block in a relevant evaluation setting in order to allow for better answers to questions such as:\u201cWhat is the performance of today\u2019s search engines on exploratory search?\u201d and \u201cHow can it be improved?\u201d The corpus will be made publicly available.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Fourth international workshop on uncovering plagiarism, authorship, and social software misuse\n", "abstract": " The Fourth International Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN10) was held in conjunction with the 2010 Conference on Multilingual and Multimodal Information Access Evaluation (CLEF-10) in Padua, Italy. The workshop was organized as a competition covering two tasks: plagiarism detection and Wikipedia vandalism detection. This report gives a short overview of the plagiarism detection task. Detailed analyses of both tasks have been published as CLEF Notebook Papers [3, 6], which can be downloaded at www.webis.de/publications.", "num_citations": "10\n", "authors": ["2097"]}
{"title": "Efficient Pairwise Annotation of Argument Quality\n", "abstract": " We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. The model\u2019s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments. With up to 93% cost savings, our approach significantly outperforms existing annotation procedures. Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.", "num_citations": "9\n", "authors": ["2097"]}
{"title": "The Importance of Suppressing Domain Style in Authorship Analysis\n", "abstract": " The prerequisite of many approaches to authorship analysis is a representation of writing style. But despite decades of research, it still remains unclear to what extent commonly used and widely accepted representations like character trigram frequencies actually represent an author's writing style, in contrast to more domain-specific style components or even topic. We address this shortcoming for the first time in a novel experimental setup of fixed authors but swapped domains between training and testing. With this setup, we reveal that approaches using character trigram features are highly susceptible to favor domain information when applied without attention to domains, suffering drops of up to 55.4 percentage points in classification accuracy under domain swapping. We further propose a new remedy based on domain-adversarial learning and compare it to ones from the literature based on heuristic rules. Both can work well, reducing accuracy losses under domain swapping to 3.6% and 3.9%, respectively.", "num_citations": "9\n", "authors": ["2097"]}
{"title": "Heuristic Authorship Obfuscation\n", "abstract": " Authorship verification is the task of determining whether two texts were written by the same author. We deal with the adversary task, called authorship obfuscation: preventing verification by altering a to-be-obfuscated text. Our new obfuscation approach (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author\u2019s subconsciously encoded writing style in a sophisticated manner using heuristic search. To obfuscate, we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality. We analyze, quantify, and illustrate the rationale of this approach, define paraphrasing operators, derive obfuscation thresholds, and develop an effective obfuscation framework. Our authorship obfuscation approach defeats state-of-the-art verification approaches, including unmasking and compression models, while keeping text changes at a minimum.", "num_citations": "9\n", "authors": ["2097"]}
{"title": "A User Study on Snippet Generation: Text Reuse vs. Paraphrases\n", "abstract": " The snippets in the result list of a web search engine are built with sentences from the retrieved web pages that match the query. Reusing a web page's text for snippets has been considered fair use under the copyright laws of most jurisdictions. As of recent, notable exceptions from this arrangement include Germany and Spain, where news publishers are entitled to raise claims under a so-called ancillary copyright. A similar legislation is currently discussed at the European Commission. If this development gains momentum, the reuse of text for snippets will soon incur costs, which in turn will give rise to new solutions for generating truly original snippets. A key question in this regard is whether the users will accept any new approach for snippet generation, or whether they will prefer the current model of\" reuse snippets.\" The paper in hand gives a first answer. A crowdsourcing experiment along with a statistical\u00a0\u2026", "num_citations": "9\n", "authors": ["2097"]}
{"title": "A Plan for Ancillary Copyright: Original Snippets.\n", "abstract": " The snippets that web search engines generate for their result presentation are extracted from the retrieved web pages, reusing pieces of text that match a user\u2019s query. Copyright owners of the retrieved web pages are typically not asked for usage rights. This long-time practice now faces increasing backlash from news publishers, legal action, and even new legislation in Germany and Spain: the so-called ancillary copyright for news publishers. This copyright law restricts the fair use of intellectual property of news publishers, allowing them to raise claims for monetary compensation when their text is reused, even within snippets. If passed at the EU level, ancillary copyright could severely impact future information system development. This paper promotes a \u201ctechnological remedy\u201d, namely, to synthesize true original snippets without text reuse.", "num_citations": "9\n", "authors": ["2097"]}
{"title": "Syntax versus Semantics\n", "abstract": " < TEXT>< TITLE> CHRYSLER> DEAL LEAVES UNCERTAINTY FOR AMC WORKERS< AUTHOR> By Richard Walker, Reuters< DATELINE> DETROIT, March 11-< BODY> Chrysler Corp\u2019s 1.5 billion dlr bid to takeover American Motors Corp; AMO> should help bolster the small automaker\u2019s sales, but it leaves the future of its 19,000 employees in doubt, industry analysts say. It was\" business as usual\" yesterday at the American", "num_citations": "9\n", "authors": ["2097"]}
{"title": "Evolution of the PAN Lab on Digital Text Forensics\n", "abstract": " PAN is a networking initiative for digital text forensics, where researchers and practitioners study technologies for text analysis with regard to originality, authorship, and trustworthiness. The practical importance of such technologies is obvious for law enforcement, cyber-security , and marketing, yet the general public needs to be aware of their capabilities as well to make informed decisions about them. This is particularly true since almost all of these technologies are still in their infancy, and active research is required to push them forward. Hence PAN focuses on the evaluation of selected tasks from the digital text forensics in order to develop large-scale, standardized benchmarks, and to assess the state of the art. In this chapter we present the evolution of three shared tasks: plagiarism detection, author identification, and author profiling.", "num_citations": "8\n", "authors": ["2097"]}
{"title": "Predicting Retrieval Success Based on Information Use for Writing Tasks\n", "abstract": " This paper asks to what extent querying, clicking, and text editing behavior can predict the usefulness of the search results retrieved during essay writing. To render the usefulness of a search result directly observable for the first time in this context, we cast the writing task as \u201cessay writing with text reuse,\u201d where text reuse serves as usefulness indicator. Based on 150\u00a0essays written by 12\u00a0writers using a search engine to find sources for reuse, while their querying, clicking, reuse, and text editing activities were recorded, we build linear regression models for the two indicators (1)\u00a0number of words reused from clicked search results, and (2)\u00a0number of times text is pasted, covering\u00a069%\u00a0(90%)\u00a0of the variation. The three best predictors from both models cover 91\u201395%\u00a0of the explained variation. By demonstrating that straightforward models can predict retrieval success, our study constitutes a first step towards\u00a0\u2026", "num_citations": "8\n", "authors": ["2097"]}
{"title": "Netspeak\u2014Assisting Writers in Choosing Words\n", "abstract": " Netspeak is a Web service which helps writers in finding alternative expressions for what they want to say. It provides a large index of writing samples in the form of n-grams, n\u2009\u2264\u20095, along with an efficient means to retrieve them by the use of wildcard queries. When in doubt about a phrasing, a user can get additional evidence by retrieving samples that match a given context. The figure below shows the results for a query where a user is interested in the two most frequently written words between \u201clooks\u201d and \u201cme\u201d. The first two columns give an idea about the customariness of each result, and the user can select the one most appropriate for her sentence.", "num_citations": "8\n", "authors": ["2097"]}
{"title": "Cross-Language High Similarity Search: Why No Sub-linear Time Bound Can Be Expected\n", "abstract": " This paper contributes to an important variant of cross-language information retrieval, called cross-language high similarity search. Given a collection\u00a0D of documents and a query q in a language different from the language of\u00a0D, the task is to retrieve highly similar documents with respect to q. Use cases for this task include cross-language plagiarism detection and translation search.               The current line of research in cross-language high similarity search resorts to the comparison of q and the documents in D in a multilingual concept space\u2014which, however, requires a linear scan of D. Monolingual high similarity search can be tackled in sub-linear time, either by fingerprinting or by \u201cbrute force n-gram indexing\u201d, as it is done by Web search engines. We argue that neither fingerprinting nor brute force n-gram indexing can be applied to tackle cross-language high similarity search, and that a linear scan is\u00a0\u2026", "num_citations": "8\n", "authors": ["2097"]}
{"title": "Common Conversational Community Prototype: Scholarly Conversational Assistant\n", "abstract": " This paper discusses the potential for creating academic resources (tools, data, and evaluation approaches) to support research in conversational search, by focusing on realistic information needs and conversational interactions. Specifically, we propose to develop and operate a prototype conversational search system for scholarly activities. This Scholarly Conversational Assistant would serve as a useful tool, a means to create datasets, and a platform for running evaluation challenges by groups across the community. This article results from discussions of a working group at Dagstuhl Seminar 19461 on Conversational Search.", "num_citations": "7\n", "authors": ["2097"]}
{"title": "News Editorials: Towards Summarizing Long Argumentative Texts\n", "abstract": " The automatic summarization of argumentative texts has hardly been explored. This paper takes a further step in this direction, targeting news editorials, ie, opinionated articles with a well-defined argumentation structure. With Webis-EditorialSum-2020, we present a corpus of 1330 carefully curated summaries for 266 news editorials. We evaluate these summaries based on a tailored annotation scheme, where a high-quality summary is expected to be thesis-indicative, persuasive, reasonable, concise, and self-contained. Our corpus contains at least three high-quality summaries for about 90% of the editorials, rendering it a valuable resource for the development and evaluation of summarization technology for long argumentative texts. We further report details of both, an in-depth corpus analysis, and the evaluation of two extractive summarization models.", "num_citations": "6\n", "authors": ["2097"]}
{"title": "Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis\n", "abstract": " This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper.", "num_citations": "6\n", "authors": ["2097"]}
{"title": "Shaping the Information Nutrition Label.\n", "abstract": " We take up on the idea of a nutrition facts label for online documents: the Information Nutrition Label. Such a label has the potential to increase the readers\u2019 ability to make an informed decision before the \u201cconsumption\u201d of a news article or some other published online document. The basic ideas along with the dimensions (manifest, measurable text qualities, etc.) of such a label were proposed in [FGG+17]. The paper in hand focuses on the problem of an intuitive, unambiguous, and intelligible label presentation. For this purpose we (1) categorize the originally proposed information nutrition dimensions and (2) interpret them in terms of well-known physical quantities from which we belief that they are intuitively understandable for the general public. To give an impression of our ideas, a visual representation as well as the results of a preliminary crowd-sourcing study are presented.", "num_citations": "6\n", "authors": ["2097"]}
{"title": "Towards comment-based cross-media retrieval\n", "abstract": " This paper investigates whether Web comments can be exploited for cross-media retrieval. Comparing Web items such as texts, images, videos, music, products, or personal profiles can be done at various levels of detail; our focus is on topic similarity. We propose to compare user-supplied comments on Web items in lieu of the commented items themselves. If this approach is feasible, the task of extracting and mapping features between arbitrary pairs of item types can be circumvented, and well-known text retrieval models can be applied instead-given that comments are available. We report on results of a preliminary, but nonetheless large-scale experiment which shows that, if comments on textual items are compared with comments on video items, topically similar pairs achieve a sufficiently high cross-media similarity.", "num_citations": "6\n", "authors": ["2097"]}
{"title": "Opportunities and risks of disaster data from social media: a systematic review of incident information\n", "abstract": " Compiling and disseminating information about incidents and disasters are key to disaster management and relief. But due to inherent limitations of the acquisition process, the required information is often incomplete or missing altogether. To fill these gaps, citizen observations spread through social media are widely considered to be a promising source of relevant information, and many studies propose new methods to tap this resource. Yet, the overarching question of whether and under which circumstances social media can supply relevant information (both qualitatively and quantitatively) still remains unanswered. To shed some light on this question, we review 37\u00a0disaster and incident databases covering 27\u00a0incident types, compile a unified overview of the contained data and their collection processes, and identify the missing or incomplete information. The resulting data collection reveals six major use cases for social media analysis in incident data collection: (1)\u00a0 impact assessment and verification of model predictions, (2)\u00a0 narrative generation, (3)\u00a0 recruiting citizen volunteers, (4)\u00a0 supporting weakly institutionalized areas, (5)\u00a0 narrowing surveillance areas, and (6)\u00a0 reporting triggers for periodical surveillance. Furthermore, we discuss the benefits and shortcomings of using social media data for closing information gaps related to incidents and disasters.", "num_citations": "5\n", "authors": ["2097"]}
{"title": "Summarizing E-sports matches and tournaments: the example of counter-strike: global offensive\n", "abstract": " That video and computer games have reached the masses is a well known fact. Furthermore, game streaming and watching other people play video games is another phenomenon that has outgrown its small beginning by far, and game streams, be it live or recorded, are today viewed by millions. E-sports is the result of organized leagues and tournaments in which players can compete in controlled environments and viewers can experience the matches, discuss and criticize, just like in physical sports. However, as traditional sports, e-sports matches may be long and contain less interesting parts, introducing the challenge of producing well directed summaries and highlights. In this paper, we describe our efforts to approach the game streaming and e-sports phenomena from a multimedia research point of view. We focus on the challenge of summarizing matches from specific relevant game, Counter-Strike: Global\u00a0\u2026", "num_citations": "5\n", "authors": ["2097"]}
{"title": "Wikipedia Text Reuse: Within and Without\n", "abstract": " We study text reuse related to Wikipedia at scale by compiling the first corpus of text reuse cases within Wikipedia as well as without (i.e., reuse of Wikipedia text in a sample of the Common Crawl). To discover reuse beyond verbatim copy and paste, we employ state-of-the-art text reuse detection technology, scaling it for the first time to process the entire Wikipedia as part of a distributed retrieval pipeline. We further report on a pilot analysis of the 100\u00a0million reuse cases inside, and the 1.6\u00a0million reuse cases outside Wikipedia that we discovered. Text reuse inside Wikipedia gives rise to new tasks such as article template induction, fixing quality flaws, or complementing Wikipedia\u2019s ontology. Text reuse outside Wikipedia yields a tangible metric for the emerging field of quantifying Wikipedia\u2019s influence on the web. To foster future research into these tasks, and for reproducibility\u2019s sake, the Wikipedia text\u00a0\u2026", "num_citations": "5\n", "authors": ["2097"]}
{"title": "Syntax versus Semantics\n", "abstract": " This paper presents a robust method for the construction of collection-specific document models. These document models are variants of the well-known vector space model, which relies on a process of selecting, modifying, and weighting index terms with respect to a given document collection. We improve the step of index term selection by applying statistical methods for concept identification. This approach is particularly suited for post-retrieval categorization and retrieval tasks in closed collections, which is typical for intranet search.We compare our approach to \u201cenriched\u201d vector-space-based document models that employ knowledge of the underlying language in the form of external semantic concepts. Primary objective is to quantify the impact of a purely syntactic analysis in contrast to a semantic enrichment in the index construction step. As a by-product we provide an efficient and language-independent means for vector space model construction, whereas the resulting document models perform better than the standard vector space model.", "num_citations": "5\n", "authors": ["2097"]}
{"title": "Web Page Segmentation Revisited: Evaluation Framework and Dataset\n", "abstract": " Each web page can be segmented into semantically coherent units that fulfill specific purposes. Though the task of automatic web page segmentation was introduced two decades ago, along with several applications in web content analysis, its foundations are still lacking. Specifically, the developed evaluation methods and datasets presume a certain downstream task, which led to a variety of incompatible datasets and evaluation methods. To address this shortcoming, we contribute two resources:(1) An evaluation framework which can be adjusted to downstream tasks by measuring the segmentation similarity regarding visual, structural, and textual elements, and which includes measures for annotator agreement, segmentation quality, and an algorithm for segmentation fusion.(2) The Webis-WebSeg-20 dataset, comprising 42,450~ crowdsourced segmentations for 8,490~ web pages, outranging existing sources\u00a0\u2026", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Sampling Bias Due to Near-Duplicates in Learning to Rank\n", "abstract": " Learning to rank~(LTR) is the de facto standard for web search, improving upon classical retrieval models by exploiting (in) direct relevance feedback from user judgments, interaction logs, etc. We investigate for the first time the effect of a sampling bias on LTR~ models due to the potential presence of near-duplicate web pages in the training data, and how (in) consistent relevance feedback of duplicates influences an LTR~ model's decisions. To examine this bias, we construct a series of specialized LTR~ datasets based on the ClueWeb09 corpus with varying amounts of near-duplicates. We devise worst-case and average-case train/test splits that are evaluated on popular pointwise, pairwise, and listwise LTR~ models. Our experiments demonstrate that duplication causes overfitting and thus less effective models, making a strong case for the benefits of systematic deduplication before training and model evaluation.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Disaster Tweet Corpus 2020\n", "abstract": " This dataset consists of tweets collected during 48 disasters over 10 disaster types with human annotations denoting if a tweet is related to this disaster or not. This collection is intended as a benchmarking dataset for filtering algorithms.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Debiasing Vandalism Detection Models at Wikidata\n", "abstract": " Crowdsourced knowledge bases like Wikidata suffer from low-quality edits and vandalism, employing machine learning-based approaches to detect both kinds of damage. We reveal that state-of-the-art detection approaches discriminate anonymous and new users: benign edits from these users receive much higher vandalism scores than benign edits from older ones, causing newcomers to abandon the project prematurely. We address this problem for the first time by analyzing and measuring the sources of bias, and by developing a new vandalism detection model that avoids them. Our model FAIR-S reduces the bias ratio of the state-of-the-art vandalism detector WDVD from 310.7 to only 11.9 while maintaining high predictive performance at 0.963 ROC and 0.316 PR.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Towards Summarization for Social Media-Results of the TL; DR Challenge\n", "abstract": " In this paper, we report on the results of the TL; DR challenge, discussing an extensive manual evaluation of the expected properties of a good summary based on analyzing the comments provided by human annotators.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Team ORG@ GameStory Task 2018.\n", "abstract": " This paper describes the approach of the organizers\u2019 team for a submission to the GameStory task at MediaEval 2018. Goal of the task is to provide a summary of a match of Counter Strike: Global Offensive (CS: GO), a popular e-sports game, that boils down a long game to it\u2019s most important events and delivers a story on the progress of the match. Our approach was to provide match statistics and overlay them with events and highlights of the game. We focused on ends of critical rounds, ie the rounds where one team took the lead over the other one, and kill streaks, where one player eliminated a substantial number of other players in short time.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Task Proposal: The TL; DR Challenge\n", "abstract": " The TL; DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the web, which has been overlooked by summarization research so far. The challenge owes its name to the frequent practice of social media users to supplement long posts with a \u201cTL; DR\u201d\u2014for \u201ctoo long; didn\u2019t read\u201d\u2014followed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length. Posts featuring TL; DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from social media, opening the door to all kinds of generative models.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "WASP: Web Archiving and Search Personalized\n", "abstract": " Logging and re-finding the information we encounter every day while browsing the web is a non-trivial task that is, at best, inadequately supported by existing tools. It is time to take another step forward: we introduce WASP, a fully functional prototype of a personal web archive and search system, which is available open source and as an executable Docker image. Based on the experiences and insights gained while designing and using WASP, we outline how personal web archive and search systems can be implemented, discuss what technological and privacy-related challenges such systems face, and propose a setup to evaluate their effectiveness. As a key insight, we argue that the indexing and retrieval for a personal archive search can be strongly tailored towards a specific user and their behavior on the visited pages compared to regular web search.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Passphone: outsourcing phone-based web authentication while protecting user privacy\n", "abstract": " This work introduces Passphone, a new smartphone-based authentication scheme that outsources user verification to a trusted third party without sacrificing privacy: neither can the trusted third party learn the relation between users and service providers, nor can service providers learn those of their users to others. When employed as a second factor in conjunction with, for instance, passwords as a first factor, our scheme maximizes the deployability of two-factor authentication for service providers while maintaining user privacy. We conduct a twofold formal analysis of our scheme, the first regarding its general security, and the second regarding anonymity and unlinkability of its users. Moreover, we provide an automatic analysis using AVISPA, a comparative evaluation to existing schemes under Bonneau et al.\u2019s framework, and an evaluation of a prototypical implementation.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Using Web N-Grams to Help Second-Language Speakers\n", "abstract": " We demonstrate the NETSPEAK search engine which helps people with writing in a foreign language. 1 To this end the search engine implements wildcard query processing on top of an inverted index of Web n-grams. By indexing word n-grams extracted from the whole Web, our search engine allows writers to retrieve phrases matching almost any given context, while the occurrence frequencies of the retrieved n-grams indicate their commonness in everyday writing. To allow for retrieval in less than a second for most queries, we have devised an inverted index and a query processor tailored to n-gram retrieval.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Retrieval-Technologien f\u00fcr die Plagiaterkennung in Programmen\n", "abstract": " Plagiaterkennung in Programmen (Quellcode) funktioniert analog zu der in Texten: gegeben ist der Quellcode eines Programms dq sowie eine Kollektion D von Programmquellen. Die Retrieval-Aufgabe besteht darin, in dq alle Codeabschnitte zu identifizieren, die aus Dokumenten in D \u00fcbernommen wurden. Im vorliegenden Papier werden Parallelen und Unterschiede zwischen der Plagiaterkennung in Texten und der in Computerprogrammen aufgezeigt, ein neues Ma\u00df zum \u00c4hnlichkeitsvergleich kurzer Code-Abschnitte vorgestellt und erstmalig Fingerprinting als Technologie f\u00fcr effizientes Retrieval aus gro\u00dfen Codekollektionen (| D|\u2248 80.000) demonstriert. In den von uns durchgef\u00fchrten Experimenten werden kurze Codeabschnitte aus D, die eine hohe \u00c4hnlichkeit zu Abschnitten aus dq aufweisen, mit einer Precision von 0.45 bei einem Recall von 0.51 in konstanter Zeit gefunden.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Hashing-basierte Indizierungsverfahren im textbasierten Information-Retrieval\n", "abstract": " Hashing ist ein f\u00fcr die Indizierung und den \u00c4hnlichkeitsvergleich von Objekten bisher wenig beachteter Ansatz. Es ist bekannt, dass mittels Hashing in konstanter Suchzeit festgestellt werden kann, ob eine Menge von Objekten ein bestimmtes Objekt enth\u00e4lt. Die Kollision der Hashwerte zweier Objekte dient dabei als Indikator f\u00fcr ihre Gleichartigkeit. Locality-Sensitive-Hashing verallgemeinert dieses Prinzip. Hier dient die Kollision der Hashwerte zweier Objekte als Indikator f\u00fcr ihre \u00c4hnlichkeit. Damit ist die Extraktion aller zu einem bestimmten Objekt \u00e4hnlichen Objekte aus einer Menge in konstanter Zeit m\u00f6glich.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Hashing-basierte indizierung: Anwendungsszenarien, theorie und methoden\n", "abstract": " Hashing-basierte Indizierung ist eine m\u00e4chtige Technologie f\u00fcr die \u00c4hnlichkeitssuche in gro\u00dfen Dokumentkollektionen [Stein 2005]. Sie basiert auf der Idee, Hashkollisionen als \u00c4hnlichkeitsindikator aufzufassen\u2013vorausgesetzt, dass eine entsprechend konstruierte Hashfunktion vorliegt. In diesem Papier wird er\u00f6rtert, unter welchen Voraussetzungen grundlegende Retrieval-Aufgaben von dieser neuen Technologie profitieren k\u00f6nnen.Weiterhin werden zwei aktuelle, hashing-basierte Indizierungsans\u00e4tze pr\u00e4sentiert und die mit ihnen erzielbaren Verbesserungen bei der L\u00f6sung realer Retrieval-Aufgaben verglichen. Eine Analyse dieser Art ist neu; sie zeigt das enorme Potenzial ma\u00dfgeschneiderter hashing-basierter Indizierungsmethoden wie zum Beispiel dem Fuzzy-Fingerprinting.", "num_citations": "4\n", "authors": ["2097"]}
{"title": "Uncertainty-based Query Strategies for Active Learning with Transformers\n", "abstract": " Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (\"transformers\") became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively cancels out, or even outweighs aforementioned cost savings. In this paper, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation on five widely used text classification benchmarks, we show that considerable improvements of up to 14.4 percentage points in area under the learning curve are achieved, as well as a final accuracy close to the state of the art for all but one benchmark, using only between 0.4% and 15% of the training data.", "num_citations": "3\n", "authors": ["2097"]}
{"title": "Generating Informative Conclusions for Argumentative Texts\n", "abstract": " The purpose of an argumentative text is to support a certain conclusion. Yet, they are often omitted, expecting readers to infer them rather. While appropriate when reading an individual text, this rhetorical device limits accessibility when browsing many texts (e.g., on a search engine or on social media). In these scenarios, an explicit conclusion makes for a good candidate summary of an argumentative text. This is especially true if the conclusion is informative, emphasizing specific concepts from the text. With this paper we introduce the task of generating informative conclusions: First, Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of argumentative texts and their conclusions. Second, two paradigms for conclusion generation are investigated; one extractive, the other abstractive in nature. The latter exploits argumentative knowledge that augment the data via control codes and finetuning the BART model on several subsets of the corpus. Third, insights are provided into the suitability of our corpus for the task, the differences between the two generation paradigms, the trade-off between informativeness and conciseness, and the impact of encoding argumentative knowledge. The corpus, code, and the trained models are publicly available.", "num_citations": "3\n", "authors": ["2097"]}
{"title": "A Dataset for Content Error Detection in Web Archives\n", "abstract": " Archiving modern web pages is challenging, and a clear concept of possible errors is still missing. To further improve current web archiving technology, this paper introduces the concept of content errors, which refers to web pages whose archived versions have unexpected content different from their originals. This paper presents the first large scale analysis of a web crawl of 10.000 pages for content errors-the Webis Web Archive 2017. Using manual inspection and small annotation studies, we identified 5 different classes of content errors, and then annotated the entire crawl for these classes using crowdsourcing: error messages (4.5% of pages), pop-ups (3.9%), pages that largely consist of advertisements (1.1%), CAPTCHAs (0.8%), and loading indicators (0.5%). Combined, about 10% of pages are affected by content errors, which underlines the relevance of the problem.", "num_citations": "3\n", "authors": ["2097"]}
{"title": "Improving Cloze Test Performance of Language Learners Using Web N-Grams\n", "abstract": " We study the effectiveness of search engines for common usage, a new category of search engines that exploit n-gram frequencies on the web to measure the commonness of a formulation, and that allow their users to submit wildcard queries about formulation uncertainties often encountered in the process of writing. These search engines help to resolve questions on common prepositions following verbs, common synonyms in given contexts, and word order difficulties, to name only a few. Until now, however, it has never been shown that search engines for common usage have a positive impact on writing performance.Our contribution is a large-scale user study with 121 participants using the Netspeak search engine to shed light on this issue for the first time. Via carefully designed cloze tests we show that second language learners who have access to a search engine for common usage significantly and effectively improve their test performance as opposed to not using them.", "num_citations": "3\n", "authors": ["2097"]}
{"title": "CopyCat: Near-Duplicates within and between the ClueWeb and the Common Crawl\n", "abstract": " The amount of near-duplicates in web crawls like the ClueWeb or Common Crawl demands from their users either to develop a preprocessing pipeline for deduplication, which is costly both computationally and in person hours, or accepting the undesired effects that near-duplicates have on reliability and validity of experiments. We introduce ChatNoir-CopyCat-21, which simplifies deduplication significantly. It comes in two parts:(1) A compilation of near-duplicate documents within the ClueWeb09, the ClueWeb12, and two Common Crawl snapshots, as well as between selections of these crawls, and (2) a software library that implements the deduplication of arbitrary document sets. Our analysis shows that 14\u201352% of the documents within a crawl and around 0.7\u20132.5% between the crawls are near-duplicates. Two showcases demonstrate the application and usefulness of our resource.", "num_citations": "2\n", "authors": ["2097"]}
{"title": "On divergence-based author obfuscation: An attack on the state of the art in statistical authorship verification\n", "abstract": " Authorship verification is the task of determining whether two texts were written by the same author based on a writing style analysis. Author obfuscation is the adversarial task of preventing a successful verification by altering a text\u2019s style so that it does not resemble that of its original author anymore. This paper introduces new algorithms for both tasks and reports on a comprehensive evaluation to ascertain the merits of the state of the art in authorship verification to withstand obfuscation.After introducing a new generalization of the well-known unmasking algorithm for short texts, thus completing our collection of state-of-the-art algorithms for verification, we introduce an approach that (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author\u2019s writing style in a sophisticated manner using heuristic search. For obfuscation, we\u00a0\u2026", "num_citations": "2\n", "authors": ["2097"]}
{"title": "Heuristic Feature Selection for Clickbait Detection\n", "abstract": " We study feature selection as a means to optimize the baseline clickbait detector employed at the Clickbait Challenge 2017. The challenge's task is to score the \"clickbaitiness\" of a given Twitter tweet on a scale from 0 (no clickbait) to 1 (strong clickbait). Unlike most other approaches submitted to the challenge, the baseline approach is based on manual feature engineering and does not compete out of the box with many of the deep learning-based approaches. We show that scaling up feature selection efforts to heuristically identify better-performing feature subsets catapults the performance of the baseline classifier to second rank overall, beating 12 other competing approaches and improving over the baseline performance by 20%. This demonstrates that traditional classification approaches can still keep up with deep learning on this task.", "num_citations": "2\n", "authors": ["2097"]}
{"title": "The ChatNoir Ranking\n", "abstract": " After realizing that the proximity weight of the previous version was too high we had to change our ranking formula immediately. In consideration of the running PAN workshop and to allow our users to have more influence of our ranking formula we made just a slighty alteration. But extended our API to allow the users to tune the formula by their own. The default values (see table 1) we choosed for the parameters are 1, except for the proximity factor (which was 10 previously).", "num_citations": "2\n", "authors": ["2097"]}
{"title": "Key point analysis via contrastive learning and extractive argument summarization\n", "abstract": " Key point analysis is the task of extracting a set of concise and high-level statements from a given collection of arguments, representing the gist of these arguments. This paper presents our proposed approach to the Key Point Analysis shared task, collocated with the 8th Workshop on Argument Mining. The approach integrates two complementary components. One component employs contrastive learning via a siamese neural network for matching arguments to key points; the other is a graph-based extractive summarization model for generating key points. In both automatic and manual evaluation, our approach was ranked best among all submissions to the shared task.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Meta-Information in Conversational Search\n", "abstract": " The exchange of meta-information has always formed part of information behavior. In this article, we show that this rule also extends to conversational search. Information about the user\u2019s information need, their preferences, and the quality of search results are only some of the most salient examples of meta-information that are exchanged as a matter of course in a search conversation. To understand the importance of meta-information for conversational search, we revisit its definition and survey how meta-information has been taken into account in the past in information retrieval. Meta-information has gone by many names, about which a concise overview is provided. An in-depth analysis of the role of meta-information in search and conversation theories reveals that they provide significant support for the importance of meta-information in conversational search. We further identify conversational search datasets\u00a0\u2026", "num_citations": "1\n", "authors": ["2097"]}
{"title": "The Information Retrieval Anthology\n", "abstract": " We present the IR Anthology, a corpus of information retrieval publications accessible at IR. webis. de via a metadata browser and a full-text search engine. Following the example of the well-known ACL Anthology, the IR Anthology serves as a hub for scholars interested in information retrieval. Our search engine ChatNoir indexes the publications\u2019 full texts, enabling a focused search and linking users to the respective publisher\u2019s site for personal access.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "An Empirical Comparison of Web Page Segmentation Algorithms\n", "abstract": " Over the past two decades, several algorithms have been developed to segment a web page into semantically coherent units, a task with several applications in web content analysis. However, these algorithms have hardly been compared empirically and it thus remains unclear which of them\u2014or rather, which of their underlying paradigms\u2014performs best. To contribute to closing this gap, we report on the reproduction and comparative evaluation of five segmentation algorithms on a large, standardized benchmark dataset for web page segmentation: Three of the algorithms have been specifically developed for web pages and have been selected to represent paradigmatically different approaches to the task, whereas the other two approaches originate from the segmentation of photos and print documents, respectively. For a fair comparison, we tuned each algorithm\u2019s parameters, if applicable, to the dataset\u00a0\u2026", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28-April 1, 2021, Proceedings, Part I\n", "abstract": " This two-volume set LNCS 12656 and 12657 constitutes the refereed proceedings of the 43rd European Conference on IR Research, ECIR 2021, held virtually in March/April 2021, due to the COVID-19 pandemic. The 50 full papers presented together with 11 reproducibility papers, 39 short papers, 15 demonstration papers, 12 CLEF lab descriptions papers, 5 doctoral consortium papers, 5 workshop abstracts, and 8 tutorials abstracts were carefully reviewed and selected from 436 submissions. The accepted contributions cover the state of the art in IR: deep learning-based information retrieval techniques, use of entities and knowledge graphs, recommender systems, retrieval methods, information extraction, question answering, topic and prediction models, multimedia retrieval, and much more.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Same Side Stance Classification Using Contextualized Sentence Embeddings\n", "abstract": " The same side stance classification shared task surveyed approaches to decide whether two arguments have the same stance towards a particular topic. We show that embeddings derived from the transformer model BERT (Devlin et al., 2019) outperform traditional bagof-words and count-based word embeddings, yielding one of the two best-performing models on this task at the time of writing. In this paper, we detail our approach and further explore which of its hyperparameters influence the accuracy of our model with respect to the two task variants studied. We conclude that our model is good enough for the shared task but may need a more exhaustive inspection when exposed to a broader variety of data.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Web Archive Analytics\n", "abstract": " Web archive analytics is the exploitation of publicly accessible web pages and their evolution for research purposes\u2014to the extent organizationally possible for researchers. In order to better understand the complexity of this task, the first part of this paper puts the entirety of the world's captured, created, and replicated data (the \u201cGlobal Datasphere\u201d) in relation to other important data sets such as the public internet and its web pages, or what is preserved thereof by the Internet Archive. Recently, the Webis research group, a network of university chairs to which the authors belong, concluded an agreement with the Internet Archive to download a substantial part of its web archive for research purposes. The second part of the paper in hand describes our infrastructure for processing this data treasure: We will eventually host around 8 PB of web archive data from the Internet Archive and Common Crawl, with the goal of supplementing existing large scale web corpora and forming a non-biased subset of the 30 PB web archive at the Internet Archive.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Task Proposal: Abstractive Snippet Generation for Web Pages\n", "abstract": " We propose a shared task on abstractive snippet generation for web pages, a novel task of generating query-biased abstractive summaries for documents that are to be shown on a search results page. Conventional snippets are extractive in nature, which recently gave rise to copyright claims from news publishers as well as a new copyright legislation being passed in the European Union, limiting the fair use of web page contents for snippets. At the same time, abstractive summarization has matured considerably in recent years, potentially allowing for more personalization of snippets in the future. Taken together, these facts render further research into generating abstractive snippets both timely and promising.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Estimating Topic Difficulty Using Normalized Discounted Cumulated Gain\n", "abstract": " Information retrieval evaluation has to consider the varying\" difficulty\" between topics. Topic difficulty is often defined in terms of the aggregated effectiveness of a set of retrieval systems to satisfy a respective information need. Current approaches to estimate topic difficulty come with drawbacks such as being incomparable across different experimental settings. We introduce a new approach to estimate topic difficulty, which is based on the ratio of systems that achieve an NDCG score that is better than a baseline formed as random ranking of the pool of judged documents. We modify the NDCG measure to explicitly reflect a system's divergence from this hypothetical random ranker. In this way we achieve relative comparability of topic difficulty scores across experimental settings as well as stability to outlier systems? features lacking in previous difficulty estimations. We reevaluate the TREC 2012 Web Track's ad hoc\u00a0\u2026", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Towards Predicting the Subscription Status of Twitch. tv Users\n", "abstract": " We investigate whether the subscription status of active users of Twitch can be inferred from their activity patterns in the chats of streamers. To enable a diversity of solutions to this problem, this task was advertised as an ECML-PKDD discovery challenge 2020, called Chat Analytics for Twitch (ChAT). Four participants submitted their working prediction models, which were evaluated at our site. The winning approach achieved an F1 score of 0.343, outperforming the baseline by a significant margin. The most salient conclusion that can be drawn at this time is that interaction behavior plays a crucial role in solving this task, meriting further analysis into this direction.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Challenges for Multimedia Research in E-Sports Using Counter-Strike\n", "abstract": " That video and computer games have reached the masses is a well-known fact. However, game streaming and, therefore, watching other people play videogames has also outgrown its humble beginnings by far. Game streams, be it live or recorded, are viewed by millions. Many of the streams are broadcasting competitive multiplayer games. This is called e-sports and it is very similar to sports broadcasting. E-sports is organized in leagues and tournaments in which players can compete in controlled environments and viewers can experience the matches, discuss and criticize just like in physical sports. In this paper, we look into the challenges for computer science in general and multimedia research in particular. The multimedia research community has done a lot of work on video streaming, broadcasting and analyzing the audience, but has missed the opportunity to investigate e-sports in detail. We focus on one\u00a0\u2026", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Wikipedia Text Reuse: Within and Without\n", "abstract": " We study text reuse related to Wikipedia at scale by compiling the first corpus of text reuse cases within Wikipedia as well as without (i.e., reuse of Wikipedia text in a sample of the Common Crawl). To discover reuse beyond verbatim copy and paste, we employ state-of-the-art text reuse detection technology, scaling it for the first time to process the entire Wikipedia as part of a distributed retrieval pipeline. We further report on a pilot analysis of the 100 million reuse cases inside, and the 1.6 million reuse cases outside Wikipedia that we discovered. Text reuse inside Wikipedia gives rise to new tasks such as article template induction, fixing quality flaws due to inconsistencies arising from asynchronous editing of reused passages, or complementing Wikipedia's ontology. Text reuse outside Wikipedia yields a tangible metric for the emerging field of quantifying Wikipedia's influence on the web. To foster future research into these tasks, and for reproducibility's sake, the Wikipedia text reuse corpus and the retrieval pipeline are made freely available.", "num_citations": "1\n", "authors": ["2097"]}
{"title": "Algorithms and Corpora for Persian Plagiarism Detection\n", "abstract": " The task of plagiarism detection is to find passages of text-reuse in a suspicious document. This task is of increasing relevance, since scholars around the world take advantage of the fact that information about nearly any subject can be found on the World Wide Web by reusing existing text instead of writing their own. We organized the Persian PlagDet shared task at PAN 2016 in an effort to promote the comparative assessment of NLP techniques for plagiarism detection with a special focus on plagiarism that appears in a Persian text corpus. The goal of this shared task is to bring together researchers and practitioners around the exciting topic of plagiarism detection and text-reuse detection. We report on the outcome of the shared task, which divides into two subtasks: text alignment and corpus construction. In the first subtask, nine teams participated, whereas the best result achieved was a PlagDet score of 0.92. For the second subtask of corpus construction, five teams submitted a corpus, which were evaluated using the systems submitted for the first subtask. The results show that significant challenges remain in evaluating newly constructed corpora.", "num_citations": "1\n", "authors": ["2097"]}