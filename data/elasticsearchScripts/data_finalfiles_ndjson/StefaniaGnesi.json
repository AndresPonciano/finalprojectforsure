{"title": "Formal test-case generation for UML statecharts\n", "abstract": " The unified modelling language has been introduced as a notation for modelling and reasoning about large and complex systems, and their design, across a wide range of application domains. System modelling and analysis techniques, especially those based on formal methods, are more and more used for enhancing traditional system engineering techniques for improving system quality. In particular this holds for model-based formal test case derivation using formal conformance testing. The contribution of the present paper is to provide a solid mathematical basis for conformance testing and automatic test case generation for UML statecharts (UMLSCs). We propose a formal conformance-testing relation for input-enabled transition systems with transitions labelled by input/output-pairs (IOLTSs). IOLTSs provide a suitable semantic model for a behavioural subset of UMLSCs. We also provide an algorithm which\u00a0\u2026", "num_citations": "123\n", "authors": ["1467"]}
{"title": "A survey on services composition languages and models\n", "abstract": " Composition of services has received much interest to support business-to-business (B2B) or enterprise applications integration. On one side, the business world has developed a number of XML-based standards to formalize the specification of Web services, their composition and execution. On the other side, the Semantic Web community focuses on reasoning about web resources by explicitly declaring their preconditions and effects with terms precisely defined in ontologies. So far, both approaches have been developed rather independently from each other. In this paper the major languages, namely BPEL4WS, BPML, WSCI, WS-CDL and DAML-S, are compared with reference to the requirements identified and finally the trend of Services composition is discussed.", "num_citations": "116\n", "authors": ["1467"]}
{"title": "Model checking UML statechart diagrams using JACK\n", "abstract": " Statechart diagrams provide a graphical notation for describing dynamic aspects of system behaviour within the Unified Modelling Language (UML). In this paper, we present a branching-time model-checking approach to the automatic verification of the formal correctness of UML Statechart diagram specifications. We use a formal operational semantics for building a labelled transition system (automaton) which is then used as a model to be checked against correctness requirements expressed in Action-Based Temporal Logic (ACTL). Our reference verification environment is JACK, where automata are represented in a standard format, which facilitates the use of different tools for automatic verification.", "num_citations": "110\n", "authors": ["1467"]}
{"title": "The integration Project for the JACK Environment\n", "abstract": " JACK, standing for Just Another Concurrency Kit, is a new environment integrating a set of veri cation tools, supported by a graphical interface o ering facilities to use these tools separately or in combination.", "num_citations": "109\n", "authors": ["1467"]}
{"title": "A model-checking verification environment for mobile processes\n", "abstract": " This article presents a semantic-based environment for reasoning about the behavior of mobile systems. The verification environment, called HAL, exploits a novel automata-like model that allows finite-state verification of systems specified in the \u03c0-calculus. The HAL system is able to interface with several efficient toolkits (e.g. model-checkers) to determine whether or not certain properties hold for a given specification. We report experimental results on some case studies.", "num_citations": "96\n", "authors": ["1467"]}
{"title": "Ambiguity and tacit knowledge in requirements elicitation interviews\n", "abstract": " Interviews are the most common and effective means to perform requirements elicitation and support knowledge transfer between a customer and a requirements analyst. Ambiguity in communication is often perceived as a major obstacle for knowledge transfer, which could lead to unclear and incomplete requirements documents. In this paper, we analyze the role of ambiguity in requirements elicitation interviews, when requirements are still tacit ideas to be surfaced. To study the phenomenon, we performed a set of 34 customer\u2013analyst interviews. This experience was used as a baseline to define a framework to categorize ambiguity. The framework presents the notion of ambiguity as a class of four main sub-phenomena, namely unclarity, multiple understanding, incorrect disambiguation and correct disambiguation. We present examples of ambiguities from our interviews to illustrate the different\u00a0\u2026", "num_citations": "77\n", "authors": ["1467"]}
{"title": "A guidelines framework for understandable BPMN models\n", "abstract": " Business process modeling allows abstracting and reasoning on how work is structured within complex organizations. Business process models represent blueprints that can serve different purposes for a variety of stakeholders. For example, business analysts can use these models to better understand how the organization works; employees playing a role in the process can use them to learn the tasks that they are supposed to perform; software analysts/developers can refer to the models to understand the system-as-is before designing the system-to-be. Given the variety of stakeholders that need to interpret these models, and considering the pivotal function that models play within organizations, understandability becomes a fundamental quality that need to be taken into particular account by modelers. In this paper we provide a set of fifty guidelines that can help modelers to improve the understandability of their\u00a0\u2026", "num_citations": "71\n", "authors": ["1467"]}
{"title": "Dynamic programming as graph searching: An algebraic approach\n", "abstract": " Finding the solution of a dynamic programming problem m the form of polyadlc funcUonal equatmns is shown to be equivalent to searching a mmmaal cost path in an AND/OR graph with monotone cost functions The proof is given in an algebraic framework and is based on a commutaUvity result between solutton and mterpretauon of a symbohc system This approach Is simdar to the one used by some authors to prove the eqmvalence between the operaUonal and denotatmnal semantics of programming languages", "num_citations": "67\n", "authors": ["1467"]}
{"title": "Measuring and improving the completeness of natural language requirements\n", "abstract": " [Context and motivation] System requirements specifications are normally written in natural language. These documents are required to be complete with respect to the input documents of the requirements definition phase, such as preliminary specifications, transcripts of meetings with the customers, etc. In other terms, they shall include all the relevant concepts and all the relevant interactions among concepts expressed in the input documents. [Question/Problem] Means are required to measure and improve the completeness of the requirements with respect to the input documents. [Principal idea/results] To measure this completeness, we propose two metrics that take into account the relevant terms of the input documents, and the relevant relationships among terms. Furthermore, to improve the completeness, we present a natural language processing tool named Completeness Assistant for\u00a0\u2026", "num_citations": "59\n", "authors": ["1467"]}
{"title": "Modular semantics for a UML statechart diagrams kernel and its extension to multicharts and branching time model-checking\n", "abstract": " Statechart diagrams provide a graphical notation to model dynamic aspects of system behaviour within the unified modelling language (UML). In this paper, we present a formal operational semantics for a behavioural subset of UML statechart diagrams (UMLSDs) including a formal proof of their correctness with respect to major UML semantics requirements concerning behavioural issues. We show how the modularity of our semantics definition can be exploited to define extensions, in particular we show an extension to models composed of collections of communicating statechart diagrams, which we call multicharts. Finally we present all the conceptual issues related to building a tool for action based branching time model-checking, for the automatic verification of formal correctness of UML multicharts. The approach we propose preserves all the information necessary to report the results of model-checking in\u00a0\u2026", "num_citations": "56\n", "authors": ["1467"]}
{"title": "On the fly model checking of communicating UML State Machines\n", "abstract": " In this paper we present an``on the fly''model checker for the verification of the dynamic behavior of UML models seen as a set of communicating state machines. The logic supported by the tool is an extension of the action based branching time temporal logic \u00b5-ACTL and has the power of full \u00b5-calculus. Early results on the application of this model checker to a case study have been also reported.", "num_citations": "54\n", "authors": ["1467"]}
{"title": "Verifying mobile processes in the HAL environment\n", "abstract": " The HD Automata Laboratory (HAL) is an integrated tool set for the specification, verification and analysis of concurrent and distributed systems. A basic notion for the HAL environment is that of history-dependent automata (HD-automata)[11]. As ordinary automata, they are composed of states and of transitions between states. However, states and transitions of HD-automata are enriched with sets of local names. In particular, each transition can refer to the names associated to its source state but can also introduce new names, which can then appear in the destination state. Hence, names are not global and static entity but they are explicitly represented within states and transitions and can be dynamically created. HD-automata have shown to be appropriate to model systems whose behaviours are history dependent, ie, systems where the observable behaviour of a step of a computation may depend on what has\u00a0\u2026", "num_citations": "53\n", "authors": ["1467"]}
{"title": "A new quality model for natural language requirements specifications\n", "abstract": " This paper describes an extension to the natural language requirements specification quality model that is the basis for the QuARS (Quality Analyzer of Requirements Specification) tool. The extension takes into account ambiguities that were not handled before.", "num_citations": "52\n", "authors": ["1467"]}
{"title": "Modelling dynamic software architectures using typed graph grammars\n", "abstract": " Several recent research efforts have focused on the dynamic aspects of software architectures providing suitable models and techniques for handling the run-time modification of the structure of a system. A large number of heterogeneous proposals for addressing dynamic architectures at many different levels of abstraction have been provided, such as programmable, ad-hoc, self-healing and self-repairing among others. It is then important to have a clear picture of the relations among these proposals by formulating them into a uniform framework and contrasting the different verification aspects that can be reasonably addressed by each proposal. Our work is a contribution in this line. In particular, we map several notions of dynamicity into the same formal framework in order to distill the similarities and differences among them. As a result we explain different styles of architectural dynamisms in term of graph\u00a0\u2026", "num_citations": "49\n", "authors": ["1467"]}
{"title": "Graph-based design and analysis of dynamic software architectures\n", "abstract": " We illustrate two ways to address the specification, modelling and analysis of dynamic software architectures using: i) ordinary typed graph transformation techniques implemented in Alloy; ii) a process algebraic presentation of graph transformation implemented in Maude. The two approaches are compared by showing how different aspects can be tackled, including representation issues, modelling phases, property specification and analysis.", "num_citations": "48\n", "authors": ["1467"]}
{"title": "An automatic SPIN validation of a safety critical railway control system\n", "abstract": " This paper describes an experiment informal specification and validation performed in the context of an industrial joint project. The project involved an Italian company working in the field of railway engineering, Ansaldobreda Segnalamento Ferroviario, and the CNR Institutes IEI and CNUCE of Pisa, Within the project two formal models have been developed describing different aspects of a safety-critical system used in the management of medium-large railway networks. Validation of safety and liveness properties has been performed on both models. Safety properties have been checked primarily in presence of Byzantine faults as well as of silent faults embedded in the models themselves. Liveness properties have been more focused on a communication protocol used within the system. Properties have been specified by means of assertions or temporal logical formulae. We used PROMELA as specification\u00a0\u2026", "num_citations": "48\n", "authors": ["1467"]}
{"title": "A model checking verification environment for UML statecharts\n", "abstract": " In this paper we present the state/event-based temporal logic \u00b5UCTL which is a logic oriented towards a natural description of dynamic properties of UML models. This logic allows to specify the basic properties that a runtime system configuration should satisfy and to combine these basic predicates with logic and temporal operators which allow to take into consideration also the events performed by the system when evolving from one system configuration to another. Doubly Labelled Transition Systems are the semantic domain for \u00b5UCTL. The logic is supported by a prototypical verification environment under development at ISTI built around the\u201d on the fly\u201d UMC model checker.", "num_citations": "46\n", "authors": ["1467"]}
{"title": "Quality analysis of NL requirements: an industrial case study\n", "abstract": " Nowadays common practice indicates that the requirement engineering (RE) process critically influences the success of the system development life cycle. Several commercial tools allow to classify, archive and manage requirements and then to print out reports and requirement documents. QuARS (Quality Analyzer for Requirements Specifications) is an automatic analyzer of such requirement documents, developed by ISTI-CNR, that can be adopted to evaluate the document quality by linguistic point of view. This paper presents how a requirement management tool, an automatic document generator, and QuARS can be integrated to define an RE automation support. The case study investigates and highlights the efficacy and the role of such proposed support in the Siemens C.N.X. development process.", "num_citations": "45\n", "authors": ["1467"]}
{"title": "Pure: A dataset of public requirements documents\n", "abstract": " This paper presents PURE (PUblic REquirements dataset), a dataset of 79 publicly available natural language requirements documents collected from the Web. The dataset includes 34,268 sentences and can be used for natural language processing tasks that are typical in requirements engineering, such as model synthesis, abstraction identification and document structure assessment. It can be further annotated to work as a benchmark for other tasks, such as ambiguity detection, requirements categorisation and identification of equivalent re-quirements. In the paper, we present the dataset and we compare its language with generic English texts, showing the peculiarities of the requirements jargon, made of a restricted vocabulary of domain-specific acronyms and words, and long sentences. We also present the common XML format to which we have manually ported a subset of the documents, with the goal of\u00a0\u2026", "num_citations": "44\n", "authors": ["1467"]}
{"title": "Using collective intelligence to detect pragmatic ambiguities\n", "abstract": " This paper presents a novel approach for pragmatic ambiguity detection in natural language (NL) requirements specifications defined for a specific application domain. Starting from a requirements specification, we use a Web-search engine to retrieve a set of documents focused on the same domain of the specification. From these domain-related documents, we extract different knowledge graphs, which are employed to analyse each requirement sentence looking for potential ambiguities. To this end, an algorithm has been developed that takes the concepts expressed in the sentence and searches for corresponding \u201cconcept paths\u201d within each graph. The paths resulting from the traversal of each graph are compared and, if their overall similarity score is lower than a given threshold, the requirements specification sentence is considered ambiguous from the pragmatic point of view. A proof of concept is given\u00a0\u2026", "num_citations": "44\n", "authors": ["1467"]}
{"title": "Detecting domain-specific ambiguities: an NLP approach based on Wikipedia crawling and word embeddings\n", "abstract": " In the software process, unresolved natural language (NL) ambiguities in the early requirements phases may cause problems in later stages of development. Although methods exist to detect domain-independent ambiguities, ambiguities are also influenced by the domain-specific background of the stakeholders involved in the requirements process. In this paper, we aim to estimate the degree of ambiguity of typical computer science words (e.g., system, database, interface) when used in different application domains. To this end, we apply a natural language processing (NLP) approach based on Wikipedia crawling and word embeddings, a novel technique to represent the meaning of words through compact numerical vectors. Our preliminary experiments, performed on five different domains, show promising results. The approach allows an estimate of the variation of meaning of the computer science words when\u00a0\u2026", "num_citations": "41\n", "authors": ["1467"]}
{"title": "Formal methods for industrial critical systems: A survey of applications\n", "abstract": " Today, formal methods are widely recognized as an essential step in the design process of industrial safety-critical systems. In its more general definition, the term formal methods encompasses all notations having a precise mathematical semantics, together with their associated analysis methods, that allow description and reasoning about the behavior of a system in a formal manner. Growing out of more than a decade of award-winning collaborative work within the European Research Consortium for Informatics and Mathematics, Formal Methods for Industrial Critical Systems: A Survey of Applications presents a number of mainstream formal methods currently used for designing industrial critical systems, with a focus on model checking. The purpose of the book is threefold: to reduce the effort required to learn formal methods, which has been a major drawback for their industrial dissemination; to help designers to adopt the formal methods which are most appropriate for their systems; and to offer a panel of state-of-the-art techniques and tools for analyzing critical systems.", "num_citations": "40\n", "authors": ["1467"]}
{"title": "Natural Language Requirements Processing: A 4D Vision.\n", "abstract": " Software engineering is the process of transforming needs into computable artifacts in a controlled way. Requirements are the human facet of this process, where mental concepts gradually mutate into machine-executable instructions. Requirements are generally expressed with the most human of the communication codes, which is natural language (NL). NL is flexible, ductile, and adaptable when controlled by humans, but it becomes rigid when handled by a machine. Recent advances in natural language processing (NLP) techniques, and their experimental application in requirements, give us confidence that the future will bring a breakthrough disentanglement of this situation. We survey the current technologies, and envision the upcoming evolution according to four interwoven dimensions that are relevant in requirements engineering (RE), namely Discipline, Dynamism, Domain Knowledge and Data-sets (our 4Ds).", "num_citations": "39\n", "authors": ["1467"]}
{"title": "An automata based verification environment for mobile processes\n", "abstract": " A verification environment for the \u03c0-calculus is presented. The environment takes a direct advantage of a general theory which allows to associate ordinary finite state automata to a wide class of \u03c0-calculus agents, so that equivalent automata are associated to equivalent \u03c0-calculus agents. A key feature of the approach is the reuse of efficient algorithms and verification techniques which have been developed and implemented for ordinary automata.", "num_citations": "38\n", "authors": ["1467"]}
{"title": "Business process flexibility-a systematic literature review with a software systems perspective\n", "abstract": " Business Process flexibility supports organizations in changing their everyday work activities to remain competitive. Since much research has been done on this topic a better awareness on the current state of knowledge is needed. This paper reports the results of a systematic literature review to develop a map on Business Process flexibility with a special focus on software systems related aspects. It covers a spectrum of the state of the art from academic point of view. It includes 164 research works from the main computer science digital libraries. After an introduction into the topic the applied methodology is described. The output of the paper is in the form of schemes and reflections. Starting from the needs for Business Process flexibility, its impact on Business Process life-cycle is introduced. Successively instruments used to express and to support Business Process flexibility are presented together with\u00a0\u2026", "num_citations": "37\n", "authors": ["1467"]}
{"title": "Pragmatic ambiguity detection in natural language requirements\n", "abstract": " This paper presents an approach for pragmatic ambiguity detection in natural language requirements. Pragmatic ambiguities depend on the context of a requirement, which includes the background knowledge of the reader: different backgrounds can lead to different interpretations. The presented approach employs a graph-based modelling of the background knowledge of different readers, and uses a shortest-path search algorithm to model the pragmatic interpretation of a requirement. The comparison of different pragmatic interpretations is used to decide if a requirement is ambiguous or not. The paper also provides a case study on real-world requirements, where we have assessed the effectiveness of the approach.", "num_citations": "35\n", "authors": ["1467"]}
{"title": "An abstract, on the fly framework for the verification of service-oriented systems\n", "abstract": " In this chapter we present (some of) the design principles which have inspired the development of the CMC/UMC verification framework. The first of these is the need of an abstraction mechanism which allows to observe a model in terms of an abstract L                 2                 TS, therefore hiding all the unnecessary underlying details of the concrete computational model, while revealing only the details which might be important to understand the system behavior. The second of these is the need a Service-Oriented Logic (SocL ) which is an event and state based, branching-time, efficiently verifiable, parametric temporal logic, for the formal encoding of service-oriented properties. The third principle is the usefulness of an on-the-fly, bounded model-checking approach for an efficient, interactive analysis of service-oriented systems which starts from the early stages of the incremental system design.", "num_citations": "34\n", "authors": ["1467"]}
{"title": "Ambiguity as a resource to disclose tacit knowledge\n", "abstract": " Interviews are the most common and effective means to perform requirements elicitation and support knowledge transfer between a customer and a requirements analyst. Ambiguity in communication is often perceived as a major obstacle for knowledge transfer, which could lead to unclear and incomplete requirements documents. In this paper, we analyse the role of ambiguity in requirements elicitation interviews. To this end, we have performed a set of customer-analyst interviews to observe how ambiguity occurs during requirements elicitation. From this direct experience, we have observed that ambiguity is a multi-dimensional cognitive phenomenon with a dominant pragmatic facet, and we have defined a phenomenological framework to describe the different types of ambiguity in interviews. We have also discovered that, rather than an obstacle, the occurrence of an ambiguity is often a resource for discovering\u00a0\u2026", "num_citations": "29\n", "authors": ["1467"]}
{"title": "Statistical model checking of an energy-saving cyber-physical system in the railway domain\n", "abstract": " Studies devoted to reduce the energy consumption while guaranteeing acceptable reliability levels are nowadays gaining importance in a variety of application sectors. Analyses through formal models and tools help developers of energy supply strategies in properly trading between energy consumption and reliability. Generally, probabilistic phenomena are involved in those systems, and they can be modelled through stochastic formalisms. Validating these models is paramount, so to guarantee reliance on the analysis results they provide. In this paper, we uniformly address both evaluation and validation of energy consumption policies on a case study from the railway domain using formal techniques. In particular, we analyse a system of rail road switch heaters, which are used to keep the temperature of rail road switches above certain levels to assure their correct functioning. Strategies based on thresholds to\u00a0\u2026", "num_citations": "27\n", "authors": ["1467"]}
{"title": "Using clustering to improve the structure of natural language requirements documents\n", "abstract": " [Context and motivation] System requirements are normally provided in the form of natural language documents. Such documents need to be properly structured, in order to ease the overall uptake of the requirements by the readers of the document. A structure that allows a proper understanding of a requirements document shall satisfy two main quality attributes: (i) requirements relatedness: each requirement is conceptually connected with the requirements in the same section; (ii) sections independence: each section is conceptually separated from the others. [Question/Problem] Automatically identifying the parts of the document that lack requirements relatedness and sections independence may help improve the document structure. [Principal idea/results] To this end, we define a novel clustering algorithm named Sliding Head-Tail Component (S-HTC). The algorithm groups together similar\u00a0\u2026", "num_citations": "27\n", "authors": ["1467"]}
{"title": "A model-checking approach for service component architectures\n", "abstract": " We present a strategy for model-checking the correctness of service composition. We do so in the context of SRML, a formal modelling framework for service-oriented computing being defined within the SENSORIA project. We introduce a methodology for encoding patterns of typical service interaction with UML state machines and present a strategy for checking SRML specifications of service composition based on such patterns. For that purpose, we use the action-state branching time temporal logic UCTL and the model-checker UMC.", "num_citations": "27\n", "authors": ["1467"]}
{"title": "A stochastic extension of a behavioural subset of UML statechart diagrams\n", "abstract": " We present a stochastically timed extension of UML statechart diagrams. The extension is rather simple both from a notational point of view and from a semantics point of view. In particular we enrich a state/transition formal operational semantics we proposed in (Latella et al., 1999) with random clocks for expressing time values. We do this in an orthogonal way, which means that the enriched semantics preserves all the properties of the untimed one. We show, by means of a simple example, how the enriched notation and its semantics can be used for performing quantitative analysis of stochastic UML statechart diagram models.", "num_citations": "27\n", "authors": ["1467"]}
{"title": "On the Fly Verification of Network of Automata.\n", "abstract": " In this paper we present an\\on the fly model checker\" for the action based branching time temporal logic-ACTL. The model checker allows a logic formula to be evaluated directly on the network representing a concurrent system as a collection of synchronized agents working in parallel, without generating the global model of the system. It is possible in this way to verify interesting properties also on systems for which the state explosion problem makes other verification tools inapplicable.", "num_citations": "26\n", "authors": ["1467"]}
{"title": "An experience in using machine learning for short-term predictions in smart transportation systems\n", "abstract": " Bike-sharing systems (BSS) are a means of smart transportation with the benefit of a positive impact on urban mobility. To improve the satisfaction of a user of a BSS, it is useful to inform her/him on the status of the stations at run time, and indeed most of the current systems provide the information in terms of number of bicycles parked in each docking stations by means of services available via web. However, when the departure station is empty, the user could also be happy to know how the situation will evolve and, in particular, if a bike is going to arrive (and vice versa when the arrival station is full).To fulfill this expectation, we envisage services able to make a prediction and infer if there is in use a bike that could be, with high probability, returned at the station where she/he is waiting. The goal of this paper is hence to analyze the feasibility of these services. To this end, we put forward the idea of using Machine\u00a0\u2026", "num_citations": "25\n", "authors": ["1467"]}
{"title": "Research challenges in business process adaptability\n", "abstract": " Modern software systems are more and more deployed within moving and continuously changing contexts. It is not easy to consider all the possible contexts configurations/variances at priori, or it is quite cumbersome and error prone to list and program all this variability points at development time. For such a reason different research trends try to develop mechanisms to express, analyse and support the dynamic adaptation of a software system while it is running.", "num_citations": "23\n", "authors": ["1467"]}
{"title": "A stochastic model-based approach to analyse reliable energy-saving rail road switch heating systems\n", "abstract": " Rail road switch heaters are used to avoid the formation of snow and ice on top of rail road switches during the cold season, in order to guarantee their correct functioning.Effective management of the energy consumption of these devices is important to reduce the costs and minimise the environmental impact. While doing so, it is critical to guarantee the reliability of the system.In this work we analyse reliability and energy consumption indicators for a system of (remotely controlled) rail road switch heaters by developing and solving a stochastic model-based approach based on the Stochastic Activity Networks (SAN) formalism. An on-off policy is considered for heating the switches, with parametric thresholds of activation/deactivation of the heaters and considering different classes of priority.A case study has been developed inspired by a real rail road station, to practically demonstrate the application of the proposed\u00a0\u2026", "num_citations": "22\n", "authors": ["1467"]}
{"title": "A Model Checking Algorithm for \u03c0-Calculus Agents\n", "abstract": " This paper presents \u03c0-logic, an action-based logic for \u03c0-calculus. A model checker is built for this logic, following an automata-based approach. This is made possible by a result which allows finite state Labelled Transition Systems to be associated with a wide class of \u03c0-calculus agents by preserving a notion of bisimulation equivalence. The model checker was thus built reusing an efficient model checker for the action based logic Actl, after a sound translation from \u03c0-logic into Actl has been defined.", "num_citations": "22\n", "authors": ["1467"]}
{"title": "Specifying variability in service contracts\n", "abstract": " In Service Oriented Computing (SOC) contracts characterise the behavioural conformance of a composition of services and guarantee that the composition does not lead to spurious results. Variability features can enable services to adapt to customer requirements and to changes in the context in which they execute.", "num_citations": "21\n", "authors": ["1467"]}
{"title": "Towards an executable algebra for product lines\n", "abstract": " We propose the Controlled Language for Software Product Lines CL4SPL with the twofold aim of ensuring simplicity of use for product line engineers and safe translations to executable languages amenable for automated verification. We show an implementation of CL4SPL in Maude, a well-known rewrite engine, thus allowing formal analyses over product families specified with CL4SPL. We illustrate our approach with a toy family of coffee machines.", "num_citations": "21\n", "authors": ["1467"]}
{"title": "Identification of cross-domain ambiguity with language models\n", "abstract": " During requirements elicitation, different stakeholders with diverse backgrounds and skills need to effectively communicate to reach a shared understanding of the problem at hand. Linguistic ambiguity due to terminological discrepancies may occur between stakeholders that belong to different technical domains. If not properly addressed, ambiguity can create frustration and distrust during requirements elicitation meetings, and lead to problems at later stages of development. This paper presents a natural language processing approach to identify ambiguous terms between different domains. The approach is based on building domain-specific language models, one for each stakeholders' domain. Word embeddings from each language model are compared in order to measure the differences of use of a word, thus estimating its potential ambiguity across the domains of interest. The proposed strategy can be useful\u00a0\u2026", "num_citations": "16\n", "authors": ["1467"]}
{"title": "Ambiguity cues in requirements elicitation interviews\n", "abstract": " Customer-analyst interviews are considered among the most effective means to perform requirements elicitation. However, during these interviews, ambiguity can hamper communication between customer and requirements analyst. Ambiguity is particularly dangerous in those cases in which the analyst misunderstands some linguistic expression of the customer, with-out being aware of the misunderstanding. On the other hand, if the analyst is able to detect ambiguous situations, this has been shown to help him/her in disclosing tacit knowledge. Indeed, the occurrence of an ambiguity might reveal the presence of unexpressed, system-relevant knowledge that needs to be elicited. Therefore, for the requirements elicitation interview to succeed, it is important for the analyst not to overlook ambiguities. To support the ambiguity-awareness of the requirements analyst, this paper aims to provide a set of cues that can be\u00a0\u2026", "num_citations": "16\n", "authors": ["1467"]}
{"title": "My data, your data, our data: Managing privacy preferences in multiple subjects personal data\n", "abstract": " The evolution of mobile devices, the success of social networks, and the digitalization of business/personal services have resulted in a huge and continuous production of Personal Data (PD). The creation of a balanced ecosystem of PD, where data act as the fuel for novel application scenarios, may drive the shift toward a user-centric paradigm, in which constraints should be imposed on the data usage, to protect the individuals\u2019 privacy. The possibility for people to directly collect, manage and exploit PD introduces both technical and regulatory new issues in PD management. Uncertainty especially arises in the case of PD related to multiple subjects, e.g., containing identifiers referring to more than one person, each of which holds rights to control how these PD are treated. In this paper, we refer to this kind of valuable data as Multiple Subjects Personal Data (MSPD). The protection of MSPD in a user\u00a0\u2026", "num_citations": "16\n", "authors": ["1467"]}
{"title": "Web service composition approaches: From industrial standards to formal methods\n", "abstract": " Composition of web services is much studied to sup-port business-to-business and enterprise application integration in e-Commerce. Current web service composition approaches range from practical languages aspiring to become standards (like BPEL, WS-CDL, OWL-S and WSMO) to theoretical models (like automata, Petri nets and process algebras). In this paper we compare these approaches wrt a selected set of characteristics (like trust, security and performance) and we advocate the use of formal models, and their tool support, to increase one\u2019s confidence in web service compositions. This paper can assist web service composition designers and developers to deliver lasting solutions, in concordance with the technology\u2019s critical needs. I.", "num_citations": "16\n", "authors": ["1467"]}
{"title": "On testing UML statecharts\n", "abstract": " We present a formal framework for notions related to testing and model based test generation for a behavioural subset of UML Statecharts (UMLSCs). This framework builds, on one hand, upon formal testing and conformance theory that has originally been developed in the context of process algebras and Labeled Transition Systems (LTSs), and, on the other hand, upon our previous work on formal semantics for UMLSCs. The paper covers the development of proper extensional testing preorders and equivalence for UMLSCs. We present an algorithm for testing equivalence verification which is based on an intensional characterization of the testing relations. Testing equivalence verification is reduced to bisimulation equivalence verification. We also address the issue of conformance testing and present a formal conformance relation together with a test case generation algorithm which is proved sound and\u00a0\u2026", "num_citations": "15\n", "authors": ["1467"]}
{"title": "Model Checking Groupware Protocols.\n", "abstract": " We show how model checking can be used for the verification of protocols underlying groupware systems. To this aim, we present a case study of those protocols underlying the Clock toolkit [1, 2] that are responsible for its concurrency control and distributed notification aspects. We abstract from the original specification of these protocols given in [3] in order to obtain a less detailed specification (model) that nevertheless covers many issues of interest. We show that this model is very well amenable to model checking by addressing the formalisation and verification of a number of important issues for the correctness of groupware protocols in general, ie not limited to those underlying Clock. In particular, we address data consistency through distributed notification, view consistency, absence of (user) starvation, and key issues related to concurrency control. As a result, we contribute to the verification of Clock\u2019s underlying groupware protocols, which was attempted in [3] with very limited success.", "num_citations": "15\n", "authors": ["1467"]}
{"title": "Common mistakes of student analysts in requirements elicitation interviews\n", "abstract": " Context and Motivation: Customer-analyst interviews are among the most common techniques for eliciting requirements. However, students of computer science-related disciplines have little material and time for learning how to perform an effective interview. As a result, once out of the class, the effectiveness of analysts in interviewing highly depends on their experience. Question/problem: Since learning from failures is recognised as a wise strategy for professional improvement, this work aims at identifying communication mistakes of student requirements analysts. Principal idea/results: We conducted a case study involving 36 students to which we gave a typical introduction to requirements elicitation interviews. Then, we arranged and recorded 18 elicitation interviews involving the students. The interview recordings were analysed by interview experts. The experts produced a list of 9 main\u00a0\u2026", "num_citations": "14\n", "authors": ["1467"]}
{"title": "A formal specification and validation of a critical system in presence of byzantine errors\n", "abstract": " This paper describes an experience in formal specification and fault tolerant behavior validation of a railway critical system. The work, performed in the context of a real industrial project, had the following main targets: (a) to validate specific safety properties in the presence of byzantine system components or of some hardware temporary faults; (b) to design a formal model of a critical railway system at a right level of abstraction so that could be possible to verify certain safety properties and at the same time to use the model to simulate the system. For the model specification we used the Promela language, while the verification was performed using the Spin model checker. Safety properties were specified by means of both assertions and temporal logic formulae. To make the problem of validation tractable in the Spin environment, we used ad hoca bstraction techniques.", "num_citations": "14\n", "authors": ["1467"]}
{"title": "Improving the quality of business process descriptions of public administrations: Resources and research challenges\n", "abstract": " PurposeBusiness processes (BPs) of public administrations (PAs) are often described in the form of written procedures or operational manuals. These business process descriptions are expected to be properly understood and applied by civil servants, who have to provide legally compliant service provisions to the citizens. However, process descriptions in the PA are sometimes hard to read, ambiguous, or vague, leading to false interpretations or even incorrect execution of the processes. The purpose of this paper is to focus on improving the descriptions of BPs to be used in PAs.Design/methodology/approachTo this end, the authors present an in-depth domain analysis, including a literature review and interviews with PA stakeholders belonging to different realities. From this analysis, the authors identified a set of 52 typical defects of process descriptions.FindingsThe authors provide a set of guidelines and a\u00a0\u2026", "num_citations": "13\n", "authors": ["1467"]}
{"title": "FMCAT: supporting dynamic service-based product lines\n", "abstract": " We describe FMCAT, a toolkit for Featured Modal Contract Automata (FMCA). FMCAT supports the analysis of dynamic service product lines, ie, applications consisting of ensembles of interacting services organized as product lines. Services are modelled as FMCA, with features identifying obligations and requirements of services. Service requirements can be either permitted or necessary, whereas the latter are further partitioned according to their criticality. A notion of agreement among service contracts is used to characterise safety.", "num_citations": "11\n", "authors": ["1467"]}
{"title": "Model-based evaluation of energy saving systems\n", "abstract": " Nowadays, there is a great attention towards cautious usage of energy sources to be employed in disparate application domains, including critical infrastructures, to save both in financial terms and in environmental impact. This chapter focuses on stochastic model-based as a support to the analysis of energy saving systems, in combination with other non functional properties, such as reliability, safety and availability. We discuss general guidelines to build a model-based framework to analyse critical cyber-physical systems, where effective energy consumption is required, while assuring imposed levels of resilience. Also, an overview of the most commonly employed methodologies and tools for model-based analysis is provided, and extensive literature is indicated as pointers to relevant research activities performed on this attractive topic over the last decades. Finally, in order to corroborate the proposed\u00a0\u2026", "num_citations": "11\n", "authors": ["1467"]}
{"title": "SpyDer: a Security Model Checker\n", "abstract": " This paper presents SpyDer, a model checking environment for security protocols. In SpyDer a protocol is described as a term of a process algebra (called spy-calculus) consisting in a parallel composition of a finite number of communicating and finite-behaviored processes. Each process represents an instance of a protocol's role. The Dolev-Yao intruder is implicitly defined in the semantics of the calculus as an environment controlling all the communication events. Security properties are written as formulas of a linear-time temporal logic. Specifically the spy-calculus has a semantics based on labeled transition systems whose traces are the temporal model over which the satisfiability relation of the temporal logic is defined. The model checker algorithm is a depth first search that tests the satisfiability of a formula over all the traces, generated on-the fly, from a typed version of calculus. Here the use of types is finalized to obtain finite-state models, where the number of messages coming from the intruder is finite. Typed terms (ie typed versions of a protocol description) are obtained at run-time by providing each variable with a sum of basic types. We prove that an attack over a typed version always implies the presence of an attack over the untyped version and more interesting, that if there is an attack over a specification, a typing transformation catching the flaw exists. The main contribution of the paper lives in the flexibility of the framework proposed. In fact the modeling environment is quite general and a protocol is specified once for all as an untyped spy-calculus term admitting infinite-state semantics. Then different typed versions, with finite-state\u00a0\u2026", "num_citations": "11\n", "authors": ["1467"]}
{"title": "Assessing tools for defect detection in natural language requirements: Recall vs precision\n", "abstract": " This extended abstract discusses the tradeoffs between recall and precision in assessing tools for finding defects in natural language requirements specifications.", "num_citations": "10\n", "authors": ["1467"]}
{"title": "Towards a Dataset for Natural Language Requirements Processing.\n", "abstract": " [Context and motivation] The current breakthrough of natural language processing (NLP) techniques can provide the requirements engineering (RE) community with powerful tools that can help addressing specific tasks of natural language (NL) requirements analysis, such as traceability, ambiguity detection and requirements classification, to name a few.[Question/problem] However, modern NLP techniques are mainly statistical, and need large NL requirements datasets, to support appropriate training, test and validation of the techniques. The RE community has experimented with NLP since long time, but datasets were often proprietary, or limited to few software projects for which requirements were publicly available. Hence, replication of the experiments and generalization have always been an issue.[Principal idea/results] Our near future commitment is to provide a publicly available NL requirements dataset.[Contribution] To this end, we are collecting requirements documents from the Web, and we are representing them in a common XML format. In this paper, we present the current version of the dataset, together with our agenda concerning formatting, extension, and annotation of the dataset.", "num_citations": "10\n", "authors": ["1467"]}
{"title": "Collaborative requirements elicitation in a european research project\n", "abstract": " A relevant part of the research activities performed in European computer science institutions is funded through European Union (EU) projects. Eliciting and defining requirements for a software system in a distributed environment with heterogeneous stakeholders is generally challenging. In EU projects partners can have different objectives and views, needs are not sharply defined and communication is hampered both by differences in native spoken languages and by the physical distance of the stakeholders. This paper presents the definition and the results of applying an innovative requirements elicitation and refinement approach in the context of an EU financed project (Learn PAd). The approach combines the KJ-method and collaborative wiki-based requirement sessions to come to a set of consolidated requirements. The paper includes the lessons learnt from this experience, also it discusses both the\u00a0\u2026", "num_citations": "10\n", "authors": ["1467"]}
{"title": "Using a machine learning approach to implement and evaluate product line features\n", "abstract": " Bike-sharing systems are a means of smart transportation in urban environments with the benefit of a positive impact on urban mobility. In this paper we are interested in studying and modeling the behavior of features that permit the end user to access, with her/his web browser, the status of the Bike-Sharing system. In particular, we address features able to make a prediction on the system state. We propose to use a machine learning approach to analyze usage patterns and learn computational models of such features from logs of system usage. On the one hand, machine learning methodologies provide a powerful and general means to implement a wide choice of predictive features. On the other hand, trained machine learning models are provided with a measure of predictive performance that can be used as a metric to assess the cost-performance trade-off of the feature. This provides a principled way to assess the runtime behavior of different components before putting them into operation.", "num_citations": "10\n", "authors": ["1467"]}
{"title": "Specification-based testing of synchronous software\n", "abstract": " Test data generation and test execution are both time-consuming activities when done manually. Automated testing methods promise to save a great deal of human effort. This especially applies to reactive programs which have complex behaviors over time and which require long test sequences. In this article, we present Lutess, a testing environment for synchronous reactive software. Lutess produces automatically and dynamically test data with respect to some environment constraints of the program under test. Moreover, it allows to trace the test execution and spot the situations where the program violates its properties.Lutess offers several specification-based testing methods. They aim at simulating more realistic environment behaviors, producing relevant data to test thoroughly a given property or driving the program under test into interesting situations. To produce the test data, the methods use different types of guides: statistical distribution of the input generation, properties, or behavioral patterns. Lutess proved to be powerful and easy to use in industrial case studies. Lutess won the Best Tool Award of the First Feature Interaction Detection Contest. The tool is described hereafter from both practical and formal points of view.", "num_citations": "10\n", "authors": ["1467"]}
{"title": "Stochastic model-based analysis of energy consumption in a rail road switch heating system\n", "abstract": " Rail road switches enable trains to be guided from one track to another, and rail road switches heaters are used to avoid the formation of snow and ice during the cold season in order to guarantee their correct functioning. Managing the energy consumption of these devices is important in order to reduce the costs and minimise the environmental impact. While doing so, it is important to guarantee the reliability of the system.               In this work we analyse reliability and energy consumption indicators for a system of (remotely controlled) rail road switch heaters by developing and solving stochastic models based on the Stochastic Activity Networks (SAN) formalism. An on-off policy is considered for heating the switches, with parametric thresholds representing the temperatures activating/deactivating the heating. Initial investigations are carried on to understand the impact of different thresholds on the indicators\u00a0\u2026", "num_citations": "9\n", "authors": ["1467"]}
{"title": "Formal Specification and Verification of complex systems\n", "abstract": " The application of formal methods in the rigorous definition and analysis of the functionality and the behaviour of a system, promises the ability of showing that the system is correct. Given such a promise, that is already out since several years, it is astonishing to see how little formal methods are actually used in the safety critical system industry, though the use of formal methods is increasingly required by the international standards and guidelines for the development of complex systems.Industrial acceptance of formal methods is strictly related to the investment needed to introduce them, to the maturity of tool support available, and to the easiness of use of formal methods and tools. Nowadays, the industrial trend is directed to the adoption of formal verification techniques to validate the design, integrating them within the existing development process. Industries are more keen to accept formal verification techniques assessing the quality attributes of their products, obtained by a traditional life cycle, rather than a fully formal life cycle development, due to the lower training and innovation costs of the former. Several approaches to the application of formal methods in the development process have been proposed, differing for the degree of involvement of the method within it. Starting from rigorous specifications, formal methods can be used for the derivation of test cases, or as a validation technique aimed at proving that the specification satisfies the requirements, or as an auxiliary technique in the automated generation of code.", "num_citations": "9\n", "authors": ["1467"]}
{"title": "Requirements elicitation and refinement in collaborative research projects\n", "abstract": " European Union (EU) projects are means of the European Commission for funding research activities. Such projects address challenging research objectives by involving both academic and industrial partners, from several countries. Information and communication technologies\u2013related projects often undertake to deliver a software system prototype. In such a context, most of the typical issues of global requirements engineering may emerge. Partners can have different background and expertise, needs are not sharply defined, and communication is hampered by linguistic and cultural differences. If these issues are not carefully taken into account from the beginning, problems frequently emerge during project execution. This paper presents the experience of applying a customized elicitation and refinement approach in the context of the Learn PAd EU project, which involved about 50 people. The approach\u00a0\u2026", "num_citations": "8\n", "authors": ["1467"]}
{"title": "Tuning energy consumption strategies in the railway domain: a model-based approach\n", "abstract": " Cautious usage of energy resources is gaining great attention nowadays, both from environmental and economical point of view. Therefore, studies devoted to analyze and predict energy consumption in a variety of application sectors are becoming increasingly important, especially in combination with other non-functional properties, such as reliability, safety and availability.                 This paper focuses on energy consumption strategies in the railway sector, addressing in particular rail road switches through which trains are guided from one track to another. Given the criticality of their task, the temperature of these devices needs to be kept above certain levels to assure their correct functioning. By applying a stochastic model-based approach, we analyse a family of energy consumption strategies based on thresholds to trigger the activation/deactivation of energy supply. The goal is to offer an assessment\u00a0\u2026", "num_citations": "8\n", "authors": ["1467"]}
{"title": "Quality assessment strategy: Applying business process modelling understandability guidelines\n", "abstract": " The knowledge management in Public Administration is considered a challenging topic. The knowledge, in fact, is often scattered among different devices such as notes, information systems or it is actually based only on personal memories. This challenge the learning activities of civil servants. The project Learn PAd try to solve this issues by providing a holistic e-learning platform to collect knowledge in the graphical form of Business Process models and in the textual form of wiki pages. However, to guarantee that the knowledge collected in the Learn PAd platform is correct and understandable, a quality assessment strategy is required. In this Technical Report we present our contribution to the Learn PAd platform in terms of: the definition of a Quality Assessment Strategy, the collection and refinement of BP modelling understandability guidelines, the validation of the guidelines and their application on PA scenarios.", "num_citations": "8\n", "authors": ["1467"]}
{"title": "A clustering-based approach for discovering flaws in requirements specifications\n", "abstract": " In this paper, we present the application of a clustering algorithm to exploit lexical and syntactic relationships occurring between natural language requirements. Our experiments conducted on a real-world data set highlight a correlation between clustering outliers, ie, requirements that are marked as\" noisy\" by the clustering algorithm, and requirements presenting\" flaws\". Those flaws may refer to an incomplete explanation of the behavioral aspects, which the requirement is supposed to provide. Furthermore, flaws may also be caused by the usage of inconsistent terminology in the requirement specification. We evaluate the ability of our proposed algorithm to effectively discover such kind of flawed requirements. Evaluation is performed by measuring the accuracy of the algorithm in detecting a set of flaws in our testing data set, which have been previously manually-identified by a human assessor.", "num_citations": "8\n", "authors": ["1467"]}
{"title": "FME 2003: Formal Methods: International Symposium of Formal Methods Europe. Pisa Italy, September 8-14, 2003, Proceedings\n", "abstract": " ThisvolumecontainstheproceedingsofFM2003, the12thInternationalFormal Methods Europe Symposium which was held in Pisa, Italy on September 8\u201314, 2003. Formal Methods Europe (FME, www. fmeurope. org) is an independent-sociation which aims to stimulate the use of and research on formal methods for system development. FME conferences began with a VDM Europe symposium in 1987. Since then, the meetings have grown and have been held about once-ery 18 months. Throughout the years the symposia have been notably successful in bringing together researchers, tool developers, vendors, and users, both from academia and from industry. Unlike previous symposia in the series, FM 2003 was not given a speci? c theme. Rather, its main goal could be synthesized as \u201cwidening the scope.\u201d Indeed, the organizers aimed at enlarging the audience and impact of the symposium along several directions. Dropping the su? x \u2018E\u2019from the title of the conference re? ects the wish to welcome participation and contribution from every country; also, contributionsfromoutsidethetraditionalFormalMethodscommunitywere solicited. The recent innovation of including an Industrial Day as an important part of the symposium shows the strong commitment to involve industrial p-ple more and more within the Formal Methods community. Even the traditional and rather fuzzy borderline between \u201csoftware engineering formal methods\u201d and methods and formalisms exploited in di? erent? elds of engineering was so-what challenged.", "num_citations": "7\n", "authors": ["1467"]}
{"title": "A BRUTUS logic for the Spi-Calculus\n", "abstract": " A spi-calculus dialect and its BRUTUS logic is presented. The BRUTUS logic is a temporal rst order logic de ned within the BRUTUS model checker by Clarke, Jha and Marrero 10], and its use within spicalculus like languages can help in expressing security properties. The spi-calculus dialect is then provided with an operational semantics based on labeled transition systems, that are also the model on which the satis ability relation of BRUTUS logic formulas is de ned. In this way we intend to perform a rst step towards the de nition of a theoretical framework for model checking logic properties on spi-calculus.", "num_citations": "7\n", "authors": ["1467"]}
{"title": "An exercise in protocol verification\n", "abstract": " The word \u201cverification\u201d is used by various people in many different contexts, and with many different meanings. In the area of parallel and concurrent programming, it refers to activities as different as proof of equivalence between two programs, reachability analysis, the checking of logical properties of a program, or even assertion that a program passes a given test set, or generation of random traces by means of simulation. The verification activities we shall consider here are those directly associated with the analysis of a finite model of the behaviour of a system, namely the building and analysis of such a model, proof of equivalence, and model checking.", "num_citations": "7\n", "authors": ["1467"]}
{"title": "CMT and FDE: tools to bridge the gap between natural language documents and feature diagrams\n", "abstract": " A business subject who wishes to enter an established technological market is required to accurately analyse the features of the products of the different competitors. Such features are normally accessible through natural language (NL) brochures, or NL Web pages, which describe the products to potential customers. Building a feature model that hierarchically summarises the different features available in competing products can bring relevant benefits in market analysis. A company can easily visualise existing features, and reason about aspects that are not covered by the available solutions. However, designing a feature model starting from publicly available documents of existing products is a time consuming and error-prone task. In this paper, we present two tools, namely Commonality Mining Tool (CMT) and Feature Diagram Editor (FDE), which can jointly support the feature model definition process. CMT\u00a0\u2026", "num_citations": "6\n", "authors": ["1467"]}
{"title": "An approach to ambiguity analysis in safety-related standards\n", "abstract": " Standards for systems and software lifecycle processes have become rather popular in the last decade. Being expressed in natural language, their requirements, or clauses, are exposed to the risk of ambiguity, vagueness and subjectivity, even when safety of people and environment is the Standard's main concern. The paper addresses some issues of this problem and presents an experimental approach to the determination and evaluation of a set of properties of the clauses, which capture the notion of the quality of their expressions. The approach adopts a rather intuitive quality model for the English language and includes the use of a tool for sentence processing. Results of a descriptive analysis of some well-known, safety-related Standards for different software application domains are shown and discussed.", "num_citations": "6\n", "authors": ["1467"]}
{"title": "A uniform approach to security and fault-tolerance specification and analysis\n", "abstract": " The availability of techniques for dependability specification and analysis is essential for the design and the implementation of trustworthy software architectures. Today\u2019s software architectures are usually designed following the principle of component-based software engineering, they are open and networked, and dependable software architectures are required to be both secure and fault-tolerant. Traditional methods of dependability analysis of software architectures must evolve as well to keep on supporting the software engineering practice. This step is not straightforward. Methods and tools for the specification and analysis of fault-tolerance are usually independent from those available in security, while a unified approach would reinforce proving the overall systems\u2019 trustworthiness. This paper demonstrates that, in certain cases, a uniform approach between fault-tolerance and security is possible. We\u00a0\u2026", "num_citations": "6\n", "authors": ["1467"]}
{"title": "A New Quality Model for Natural Language Requirements Specification\n", "abstract": " Objective\u2022 There is a certainly a need to evaluate and improve the quality of any NL RS.", "num_citations": "6\n", "authors": ["1467"]}
{"title": "Applying generalized non deducibility on compositions (GNDC) approach in dependability\n", "abstract": " This paper presents a framework where dependable systems can be uniformly modeled and dependable properties analyzed within the Generalized Non Deducibility on Compositions (GNDC), a scheme that has been profitably used in definition and analysis of security properties. Precisely, our framework requires a systems to be modelled using a formal calculus, here the CCS process algebra, where both the failing behaviour of the system and the related fault-recovering procedures are also explicitly described. An environment able to inject any fault in the system is then defined as a separated component. The parallel composition between the system and the environment represents our scenario of analysis, where some fault tolerance property (eg, fail stop, safe and silent) are studied as instances of GNDC properties. By using different instances of GNDC we are able to argue about the availability of effective methodologies of analysis, and on the possibility of applying compositional techniques.", "num_citations": "6\n", "authors": ["1467"]}
{"title": "Verification on the Web of mobile systems\n", "abstract": " The vast majority of current available verification environments have been built by sticking to traditional architectural style centralized and without dealing with interoperability and dynamic reconfigurability. In this paper we present a verification toolkit whose design and implementation exploit the Web service architectural paradigm.", "num_citations": "6\n", "authors": ["1467"]}
{"title": "On quantitative assessment of reliability and energy consumption indicators in railway systems\n", "abstract": " Stochastic model-based approaches are widely used for obtaining quantitative non-functional indicators of the analysed systems, as for example reliability, performance and energy consumption. However, a critical issue with models is their validation, in order to justifiably put reliance on the analysis results they provide. In this paper, we address cross-validation on a case study from the railway domain, by modelling and evaluating it with different formalisms and tools. Stochastic Activity Networks models and Stochastic Hybrid Automata models of rail road switch heaters, developed for the purpose of evaluating energy consumption and reliability indicators, will be evaluated with Mobius and Uppaal SMC. We will compare the obtained results, to improve their trustworthiness and to provide insights on the design and analysis of energy-saving cyber-physical systems.", "num_citations": "5\n", "authors": ["1467"]}
{"title": "A formal security analysis of an OSA/Parlay authentication interface\n", "abstract": " We report on an experience in analyzing the security of the Trust and Security Management (TSM) protocol, an authentication procedure within the OSA/Parlay Application Program Interfaces (APIs) of the Open Service Access and Parlay Group. The experience has been conducted jointly by research institutes experienced in security and industry experts in telecommunication networking. OSA/Parlay APIs are designed to enable the creation of telecommunication applications outside the traditional network space and business model. Network operators consider the OSA/Parlay a promising architecture to stimulate the development of web service applications by third party providers, which may not necessarily be experts in telecommunication and security. The TSM protocol is executed by the gateways to OSA/Parlay networks; its role is to authenticate client applications trying to access the interfaces of some\u00a0\u2026", "num_citations": "5\n", "authors": ["1467"]}
{"title": "Towards Model Checking a Spi-Calculus Dialect\n", "abstract": " In this paper we present a model checking framework for a spi-calculus dialect which uses a linear tirne temporal logic for expressing security properties. We have provided our spi-calculus dialect, called SPID, with a semantics based on labeled transition systems (LTS), where the intruder is modeled in the Dolev-Yao style as an active environment controlling the communication and able to compose new messages by pairing, splitting, encryption or decryption. The LTS coming from protocols specification, usually infinite state because of the intruder's capability to generate an infinite amount of messages, are the odels over which the satisfiability of the logic formulae has been defined. As a logic we have used the one defined by Clarke, Jha and Marrero, for their BRUTUS model checker, We cail it BRUTUS logic, which is known to be suitable to express a wide class of security properties, such as secrecy, integrity\u00a0\u2026", "num_citations": "5\n", "authors": ["1467"]}
{"title": "A refinement approach to analyse critical cyber-physical systems\n", "abstract": " Cyber-Physical Systems (CPS) are characterised by digital components controlling physical equipment, and CPS are typically influenced by the surrounding environment conditions. Due to the stochastic continuous nature of the involved physical phenomena, for quantitative evaluation of non-functional properties (e.g. dependability, performance) stochastic hybrid model-based approaches are mainly used. In case of critical applications, it is also important to verify specific qualitative aspects (e.g. safety). Generally, stochastic hybrid approaches are not suitable to account for the co-existence of both qualitative and quantitative aspects. In this paper we address this issue by proposing a refinement approach for analysing stochastic hybrid systems starting from a verified discrete representation of their logic. Different formalisms are used and formally related. It is then possible to combine the quantitative\u00a0\u2026", "num_citations": "4\n", "authors": ["1467"]}
{"title": "Enhancing Models Correctness through Formal Verification: A Case Study from the Railway Domain.\n", "abstract": " Model-based approaches are widely used for analysing systems belonging to a variety of domains, including the transportation sector. A critical issue with models is their validation, in order to justifiably put reliance on the analysis results they provide (including non functional indicators such as reliability, performance and energy consumption). Typically, cross-validation is performed, eg through exercising modelling by different formalisms/tools or through forms of experimental analysis. In this paper, we address validation of a case study from the railway domain via formal techniques, specifically with automata-based models. Validation of interaction aspects of Stochastic Activity Networks models of rail road switch heaters, developed for the purpose of evaluating energy consumption and reliability indicators, is performed through a tool based on contract automata, a recently introduced formalism for verifying properties of communication-based applications.", "num_citations": "4\n", "authors": ["1467"]}
{"title": "The Sensoria Approach Applied to the Finance Case Study\n", "abstract": " This chapter provides an effective implementation of (part of) the Sensoria approach, specifically modelling and formal analysis of service-oriented software based on mathematically founded techniques. The \u2018Finance case study\u2019 is used as a test bed for demonstrating the feasibility and effectiveness of the use of the process calculus COWS and some of its related analysis techniques and tools. In particular, we report the results of an application of a temporal logic and its model checker for expressing and checking functional properties of services and a type system for guaranteeing confidentiality properties of services.", "num_citations": "4\n", "authors": ["1467"]}
{"title": "Model checking of embedded systems\n", "abstract": " The integration of different dependability techniques is an open research question. We address problems that arise when attempting to combine fault tolerance mechanisms with formal methods and formal verification tools in the development of an embedded system.In recent years, the wide spread deployment of embedded systems on which human activities depend has raised many concerns about safety issues. A combination of fault prevention, fault tolerance, fault removal and fault forecasting techniques are commonly used in order to achieve a high degree of dependability. However, there is no common agreement on a standard method to combine and integrate individual techniques. For example, industries with different backgrounds and application fields tend to adopt their own particular development trajectories when applying techniques aimed at enhancing dependability.", "num_citations": "4\n", "authors": ["1467"]}
{"title": "A\" Brutus\" model checking of a spi-calculus dialect\n", "abstract": " This paper proposes a preliminary framework in which protocols, expressed in a dialect of the spi-calculus, can be verified using model checking algorithms. In particular we define a formal semantics for a dialect of the spi-calculus based on labeled transition systems in such a way that the model checking environment developed by Clarke Marrero and Jha, can be re-used. Recently this environment has been extended with both a first order linear temporal logic, used to specify security properties, and partial order reduction techniques. These two new results make our approach interesting and effective for automatic verification.", "num_citations": "4\n", "authors": ["1467"]}
{"title": "Dependable dynamic routing for urban transport systems through integer linear programming\n", "abstract": " Highly automated transport systems play an important role in the transformation towards a digital society, and planning the optimal routes for a set of fleet vehicles has been proved useful for improving the delivered services. Traditionally, routes are planned beforehand. However, with the advent of autonomous urban transport systems (e.g. autonomous cars), possible obstructions of tracks due to traffic congestion or bad weather conditions need to be handled on the fly. In this paper we tackle the problem of dynamically computing routes of vehicles in urban lines in the presence of potential obstructions. The problem is formulated as an integer linear optimization problem. The proposed algorithm will assign routes to vehicles dynamically, considering the track segments that are no longer available and the positions of the vehicles in the urban area. The recomputed routes guarantee the minimal waiting time\u00a0\u2026", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Ensuring Action: Identifying Unclear Actor Specifications in Textual Business Process Descriptions.\n", "abstract": " In many organisations, business process (BP) descriptions are available in the form of written procedures, or operational manuals. These documents are expressed in informal natural language, which is inherently open to different interpretations. Hence, the content of these documents might be incorrectly interpreted by those who have to put the process into practice. It is therefore important to identify language defects in written BP descriptions, to ensure that BPs are properly carried out. Among the potential defects, one of the most relevant for BPs is the absence of clear actors in action-related sentences. Indeed, an unclear actor might lead to a missing responsibility, and, in turn, to activities that are never performed. This paper aims at identifying unclear actors in BP descriptions expressed in natural language. To this end, we define an algorithm named ABIDE, which leverages rule-based natural language processing (NLP) techniques. We evaluate the algorithm on a manually annotated data-set of 20 real-world BP descriptions (1,029 sentences). ABIDE achieves a recall of 87%, and a precision of 56%. We consider these results promising. Improvements of the algorithm are also discussed in the paper.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Modelling and validating an import/export shipping process\n", "abstract": " In recent years, business process management has become increasingly popular in many industrial contexts and application domains. This is mainly because it facilitates the modelling of process specifications and the development of an executable framework, while providing concise definitions and taxonomies. The data acquired during the business process execution phase can be used for quality analysis and to demonstrate compliance to specifications. We describe an experience in the real world context of the Livorno Port Authority.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Context transformations for goal models\n", "abstract": " This paper proposes a technique to support the requirements engineer in transforming existing models into new models to address the customer's needs. In particular, we identify a set of possible categories of context change that indicate in which direction the original model needs to evolve. Furthermore, we associate a transformation to each category, and we formalise it in terms of graph grammars. Our results are a generalisation of an experimental evaluation based on 10 models retrieved from the literature and 25 scenarios of context change. This work represents a step forward in the formalisation of requirements models since it provides the foundations of a tool to support the automatic transformation of models, and employs graph grammars to provide a formal layer to the approach.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "The Testing and Test Control Notation TTCN\u20103 and its Use\n", "abstract": " The testing and test control Notation TTCN\u20103 language was created due to the imperative necessity to have universally understood language syntax able to describe test behavior specifications. TTCN\u20103 enables systematic, specification\u2010based testing for various kinds of tests including, for example, conformance, interoperability, regression, robustness, performance, and scalability testing. A test system can communicate with a system under test (SUT) synchronously or asynchronously. In this chapter, a small test specification in TTCN\u20103 is given in order to give an insight into this technology. The semantics of TTCN\u20103 is defined by an operational semantics, which defines the meaning of TTCN\u20103 behaviors in an intuitive and unambiguous manner. The chapter also describes different case studies that demonstrate the application of the TTCN\u20103 to test various target systems\u00a0\u2026", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Introduction to the Sensoria Case Studies\n", "abstract": " The foundational research carried out in Sensoria has been steered by a number of case studies for ensuring applicability of Sensoria methods and meeting expectations of society and the economy. In this chapter, we introduce these case studies. Three of the case studies came from industrial applications in automotive, finance and telecommunication domains; one came from an academic application for distributed e-learning and course management. Having in mind the relevance that these areas have in society and the economy, the above case studies have been extensively used in Sensoria during the whole project.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Security analysis of Parlay/OSA framework\n", "abstract": " This paper analyzes the security of the Trust and Security Management (TSM) protocol, an authentication protocol which is part of the Parlay/OSA Application Program Interfaces (APIs). Architectures based on Parlay/OSA APIs allow third party service providers to develop new services that can access, in a controlled and secure way, to those network capabilities offered by the network operator. Role of the TSM protocol, run by network gateways, is to authenticate the client applications trying to access and use the network capabilities features offered. For this reason potential security flaws in its authentication strategy can bring to unauthorized use of network with evident damages to the operator and to the quality of services. This paper shows how a rigorous formal analysis of TSM underlines serious weaknesses in the model describing its authentication procedure. The paper relates about the design activity of the\u00a0\u2026", "num_citations": "3\n", "authors": ["1467"]}
{"title": "A Formal Specification and Verification of a Safety Critical Railway Control System\n", "abstract": " This paper describes an experiment in formal speci cation and validation performed in the context of an industrial joint project involving Ansaldobreda Segnalamento Ferroviario (ASF) and CNR Institutes-IEI and CNUCE-of Pisa. Within this project two formal models have been developed, describing di erent aspects of a wider safety-critical system for the management of medium-large railway networks. Validation of safety and liveness properties has been performed on both models. More speci cally safety properties have been checked also in presence of byzantine behavior as well as other kinds of faults embedded in the models themselves. Liveness properties have been more focused on a communication protocol used within the system. Properties have been speci ed by means of assertions or temporal logical formulae. We used Promela as speci cation language, while the veri cation was performed using Spin.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Formal verification of cryptographic protocols using history dependent automata\n", "abstract": " Cryptography has been introduced to insure an acceptable degree of security in presence of malicious intruders when critical information are transmitted via an insecure media, for example a computer wide network or a mobile telephone network.The security of a media does not only depend on the cryptographic system used but also on how it is used: in particular researchers have proposed the use of protocols to provide this security guaranteed. These protocols, called cryptographic protocols, consist of sequences of messages, many with encrypted parts whose usual objectives are:", "num_citations": "3\n", "authors": ["1467"]}
{"title": "A sound and complete axiom system for the logic actl\n", "abstract": " In this paper we present a sound and complete axiom system for the branching temporal logics ACTL. This logic has action-based modalities and is inter preted over Labelled Transition Systems, so it is suitable to specify and study the behavior of concurrent systems defined by process algebras and modelled on LTSs. ACTL is more expressive than Hennessy-Milner logic and can natu rally describe safety and liveness properties. The reason we give the axiomatisaton of ACTL is to complete the characterization of such a logic; moreover, the making of the axiom system was the first step for the development of a proof assistant based on it.", "num_citations": "3\n", "authors": ["1467"]}
{"title": "Research on NLP for RE at CNR-ISTI: a Report.\n", "abstract": " Abstract [Team Overview] The Formal Methods & Tools (FMT) group of CNR-ISTI focuses on the study and development of formal methods and tools to support software development processes.[Past Research] FMT started working on requirements formalisation through natural language processing (NLP) at the end of the nineties. This stream of research evolved into requirements analysis and defect detection by means of NLP, with a focus on ambiguity, and resulted in the development and application of the QuARS tool for requirements analysis. More recently, the group started working on the analysis of requirements elicitation interviews, in which ambiguity in spoken natural language and other communication defects are studied.[Research Plan] In the upcoming years, FMT will devote its effort to the diffusion of a dataset for requirements analysis, to the usage of NLP in product line engineering, and to research in NLP techniques applied to the analysis of interviews.", "num_citations": "2\n", "authors": ["1467"]}
{"title": "Are Standards an Ambiguity-Free Reference for Product Validation?\n", "abstract": " The increased use of standards as references for safety-critical applications is drawing the attention of researchers on the fact that the responsibility for the safety of standard-compliant systems may depend not only on developers and assessors, but also on the standards themselves. This paper is focused particularly on some quality aspects of standard clauses, i.e., the natural language statements that are expressed by the standards, and to which a standard-compliant process or product is required to adhere. Various railway standards are considered, and some linguistic issues, potentially leading to ambiguity of clause interpretation, are discovered with the aid of natural language processing (NLP) tools. Real cases of problems in clause interpretation, taken from industrial experience, are reported, to show the possible impact in products and processes that must be validated against such clauses, and to\u00a0\u2026", "num_citations": "2\n", "authors": ["1467"]}
{"title": "Modeling web applications by the multiple levels of integrity policy\n", "abstract": " We propose a formal method to validate the reliability of a web application, by modeling interactions among its constituent objects. Modeling exploits the recent \u201cMultiple Levels of Integrity\u201d mechanism which allows objects with dynamically changing reliability to cooperate within the application. The novelty of the method is the ability to describe systems where objects can modify their own integrity level, and react to such changes in other objects. The model is formalized with a process algebra, properties are expressed using the ACTL temporal logic, and can be verified by means of a model checker. Any instance of the above model inherits both the established properties and the proof techniques. To substantiate our proposal we consider several case-studies of web applications, showing how to express specific useful properties, and their validation schemata. Examples range from on-line travel agencies, inverted\u00a0\u2026", "num_citations": "2\n", "authors": ["1467"]}
{"title": "A temporal logic for UML statecharts diagrams\n", "abstract": " We present here the use UML statecharts for the design and the specication of the dynamic behavior of the airport system. A statechart diagram is dened for each class of the model, providing a complete operational description of the behavior of all the objects of the class. The full system is then represented bya set of class objects. The UML semantics?, 1, 9, 11] associates to each active object a state machine, and the possible system behaviours are dened by the possible evolutions of these communicating state machines. All the possible system evolutions can be formallyrepresented as a Doubly Labelled Transition Systems 3] in which the states represent the variuos system congurations and the edges the possible evolutions of a system conguration. The topology of the system is modelled by an\\atLoc\" attribute, associated to each class, which represents its locality. Mobility is realized by all the operations which\u00a0\u2026", "num_citations": "2\n", "authors": ["1467"]}
{"title": "UCTL: A Temporal Logic for UML Statecharts\n", "abstract": " In this paper we present the state/event-based temporal logic UCTL that makes possible the description of properties on UML model evolutions and assertions on explicit local state variables of UML state machines. This logic allows both to specify the basic properties that a state should satisfy, and to combine these basic predicates with advanced logic or temporal operators. Doubly Labelled Transition Systems are the semantic domain for UCTL where states are labelled by sets of propositions that hold in them and transitions by events performed. The logic we propose here is then applied to verify properties over the dynamic behaviour of a mobile system modelled as extended UML statecharts.", "num_citations": "2\n", "authors": ["1467"]}
{"title": "A formal specification and validation of a safety critical railway control system\n", "abstract": " This paper describes an important experiment in formal specification and validation, both performed in the context of an industrial project jointly performed by Ansaldobreda Segnalamento Ferroviario and CNR Institutes IEI and CNUCE of Pisa. Within this project we developed two formal models of a control system which is part of a wider safety-critical system for the management of medium-large railway networks. Each model describes different aspects of the system at a different level of abstraction. On these models we performed verification of both safety properties-in the hypothesis of Byzantine errors or in presence of some defined hardware faults--and liveness properties of a dependable communication protocols. The properties has been specified by means of assertions and temporal logical formulae. As a specification language we used Promela language while the verification was performed using the model checker Spin.", "num_citations": "2\n", "authors": ["1467"]}
{"title": "A logic-functional approach to the execution of CCS specifications modulo behavioural equivalences\n", "abstract": " This paper reports on a work that proposes a kernel for an execution environment for the operational semantics and the behavioural equivalences of CCS. The proposed execution environment distinguishes itself by being formal, by dealing with the behavioural equivalences as schemes of axioms, differently from other approaches based on automata, and by giving the possibility to define several strategies of verification in a modular and flexible way. The environment, obtained by techniques of logic-functional programming, treats basic CCS with bounded recursion. A particular strategy of verification is presented.", "num_citations": "2\n", "authors": ["1467"]}
{"title": "Checking business process modeling guidelines in apromore\n", "abstract": " We present the integration of BEBoP-understandaBility vErifier for Business Process models, into the Apromore open-source process analytics platform. Given a BPMN model the tool allows one to verify which understandability modeling guidelines such as layout conventions are violated by the model. Such guidelines are rules that a model designer should follow to guarantee that the designed model is easy to understand by relevant stakeholders. Given the variety of stakeholders that need to interpret these models, and considering the pivotal function that process models play within organizations, understandability becomes a fundamental quality requirement that needs to be taken into account by designers. The tool provides model designers with textual and graphical representations of which understandaiblity guidelines are violated. Designers can then decide to repair models in such a way to guarantee a higher degree of understandability.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Energy-saving buildings assessment through stochastic hybrid model-based evaluation\n", "abstract": " Optimizing the energy consumption of buildings is an increasingly important topic in ICT. Smart Buildings monitor and control their energy consumptions and safety-related aspect through networks of sensors and actuators connected to the Internet. Policies of energy consumption can be adopted to optimize the interactions among the involved nodes, while satisfying required safety and reliability levels. Stochastic hybrid formalisms have been proved a viable solution for evaluating the effectiveness of energy saving solutions for Smart Buildings, to support tuning of the most suitable one.The analysis and prediction of the energy consumption of ICT systems is nowadays an important research topic, both from environmental and economical point of view. Concerning dependability-critical application domains, as for example critical energyaware buildings, energy saving must be addressed in conjunction with other properties requested to the system, including reliability, safety and availability. Recently, Smart Buildings have been introduced to describe different solutions concerning energy optimization and facilities management of intelligent buildings [6, 8]. Examples of these solutions are the monitoring and control of energy consumptions and safety-related aspects through sensors connected to the Internet, and the deployment of renewable energy sources. Quoting [8]:", "num_citations": "1\n", "authors": ["1467"]}
{"title": "3rd fme workshop on formal methods in software engineering (Formalise 2015)\n", "abstract": " Despite their significant advantages, formal methods are not widely used in industrial software development. Following the successful workshops we organized at ICSE 2103 in San Francisco, and ICSE 2014 in Hyderabad, we organize a third edition of the FormaliSE workshop with the main goal to promote the integration between the formal methods and the software engineering communities.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Improvement and analysis of behavioural models with variability\n", "abstract": " Product Lines or Families represent a new paradigm widely used to describe company products with similar functionality and requirements in order to improve efficiency and productivity of a company. In this context many studies are focused on the research of the best behavioural model useful to describe a product family and to reason about properties of the family itself. In addition the model must allow to describe in a simple way different types of variability, needed to characterize several products of the family.One of the most important of these models is the Modal Transition System (MTS), an extension of a Labelled Transition System (LTS), which introduces two types of transitions useful to describe the necessary and allowed requirements. These models have been broadly studied and several its extensions have been described. These extensions follow different approaches which entail the introduction of more and more complex and expressive requirements. Furthermore MTS and its extensions define a concept of refinement which represents a step of design process, namely a step where some allowed requirements are discarded and other ones become necessary.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Can safety be obtained through good practices for requirement writing?\n", "abstract": " Software Requirements analysis of quality characteristics as completeness, consistency and unambiguity assume an important role in the safety-critical software. We consider then the contributions to Natural Language (NL) Software Requirements Analysis as good practices or recommendations for producing high level quality NL requirements. From a survey of different approaches and solutions, it is possible to draw an incremental list of Good Practices (GP) for writing NL software requirements, making them understandable to their users, typically from the linguistic point of view, thus reducing the efforts for the analysis. The industrial experiences of two research/service laboratories about requirements analysis are compared with the suggestions of the literature and the technology, and the results are shown.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "The Sensoria pattern-based approach applied to the finance case study\n", "abstract": " This deliverable provides an effective implementation of (part of) the SENSORIA approach, specifically modelling and formal analysis of service-oriented software based on mathematically founded techniques. The \u2018Finance case study\u2019is used as a test bed for demonstrating the feasibility and effectiveness of the use of the process calculus COWS and some of its related analysis techniques and tools. In particular, we report the results of an application of a temporal logic and its model checker for expressing and checking functional properties of services and a type system for guaranteeing confidentiality properties of services.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "A graph-based design framework for global computing systems\n", "abstract": " We present a framework for designing and analyzing Global Computing Systems using Dynamic Software Architectures. The framework, called TGGA, integrates typed graph grammars and the Alloy modeling language to specify Programmed Dynamic Software Architectures that represent systems that evolve their topology at runtime. We demonstrate the benefits of the framework by applying it to the study of an Automotive Software System.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "The industrialization of formal methods\n", "abstract": " This special section contains papers based on work presented at the 12th International Symposium on Formal Methods (FM 2003) held at the CNR Research Campus in Pisa on 8\u201313 September 2003. The papers all focus on tools and techniques for cost-effective application of formal methods on the industrial scale. The papers discuss a range of approaches, including the development of domain-specific and general-purpose techniques, and the construction of suites of cooperating but specialised tools. In spite of the diversity of approach, the papers share some themes, notably the levels of automation that are possible at appropriate levels of abstraction, and the incorporation of formal techniques within existing development paradigms. Although three papers cannot provide a comprehensive view, they do provide a useful \u201csnapshot\u201d of dominant concerns in industrial-strength formal methods today.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Preface: special issue on the fifth international workshop of the ERCIM working group on formal methods for industrial critical systems, Berlin, April 3-4, 2000-selected papers\n", "abstract": " Preface: special issue on the fifth international workshop of the ERCIM working group on formal methods for industrial critical systems, Berlin, April 3-4, 2000-selected papers: Science of Computer Programming: Vol 46, No 3 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Science of Computer Programming Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsScience of Computer ProgrammingVol. , No. Preface: special issue on the fifth international workshop of the ERCIM working group on formal methods for industrial critical systems, Berlin, April 3-4, 2000-selected papers article Preface: special issue on the fifth international workshop of the \u2026", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Introduction: Special Issue on the Fourth International Workshop of the ERCIM Working Group on Formal Methods for Industrial Critical Systems, Trento, July 11\u201312, 1999\u00a0\u2026\n", "abstract": " Introduction: Special Issue on the Fourth International Workshop of the ERCIM Working Group on Formal Methods for Industrial Critical Systems, Trento, July 11\u201312, 1999\u2014Selected Papers: Formal Methods in System Design: Vol 19, No 2 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Formal Methods in System Design Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsFormal Methods in System DesignVol. , No. Introduction: Special Issue on the Fourth International Workshop of the ERCIM Working Group on Formal Methods for Industrial Critical Systems, Trento, July 11\u201312, 1999\u2014Selected Papers article Introduction: Special Issue on the \u2026", "num_citations": "1\n", "authors": ["1467"]}
{"title": "A BRUTUS Logic for a Spi-Calculus Dialect\n", "abstract": " In the eld of process algebras, the spi-calculus, a modied version of the-calculus with encryption primitives, is indicated as an expressive speci cation language for cryptographic protocols. In spi-calculus basic security properties, such as secrecy and integrity can be formalized as may-testing equivalences which do not seem easily extendible to express other kinds of interesting properties such as, for example, anonymity. When, as a language for properties speci cation, temporal logics are used a more expressive power can be reached making possible to represent a wider class of properties. Recently, within the BRUTUS model checker, a rst order temporal logic has been de ned, making possible to express both basic and advanced properties, such as di erent kinds of authenticity and anonymity. In this work we de ne a spi-calculus dialect on which the BRUTUS logic can be interpreted with a double, in our opinion, potential advantage: to provide the spi-calculus like languages with a temporal logics as a exible medium of security properties expression, and to enlarge the BRUTUS model checker with a widely used speci cation language for cryptographic protocols.", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Special Issue on the First International workshop of the ERCIM Working Group on Formal Methods for Industrial Critical Systems, St. Hugh's College, Oxford, March 19, 1996\u00a0\u2026\n", "abstract": " Formal Methods have been advocated as a means for increasing the reliability of systems, especially those which are safety or business critical. In the last decade several theories have been developed which aim at coping with the problem of systems correctness by means of formal methodologies for the specification, design and verification of systems. These theories have been extended in order to deal with time, probability and stochastic aspects of behaviours. More recently, international standards for safety strongly recommend the use of such methodologies, especially for critical systems. Nevertheless, the use of formal methods in industry is still quite limited for several reasons. Among them, there is the notational difficulty of most of the formal methods available nowadays, which is often intrinsic in the mathematical nature of the methods themselves. Moreover, there is a relative shortage of real life examples\u00a0\u2026", "num_citations": "1\n", "authors": ["1467"]}
{"title": "Shortest path problems and tree grammars: An algebraic framework\n", "abstract": " Some interesting, recent results concern the equivalence between the operational and the denotational semantics of recursive programs [13, in particular nondeterministic programs [2 3. A recursive definition in the operational approach, is considered first as a recursive schema, ie a grammar which can generate a (possibly) infinite tree or a set o\u00a3 trees. Every tree is a symbolic expression built up in terms of the elementary operations available in the programming language. Such trees caq then be evaluated yielding a result. The same result can be obtained, in the denotationa! approach, by First evaluating the schema, obtaining the original program now considered as a system o\u00a3 equations in a suitable algebra, and then Finding the minimal Fixpoint o\u00a3 the system. The whole process can be completely described in simple algebraic terms i\u00a3 we consider also the recursive schema as a system of equations whose\u00a0\u2026", "num_citations": "1\n", "authors": ["1467"]}