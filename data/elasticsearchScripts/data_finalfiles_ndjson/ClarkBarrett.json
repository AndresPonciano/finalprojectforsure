{"title": "CVC Lite: A new implementation of the cooperating validity checker\n", "abstract": " We describe a tool called CVC Lite (CVCL), an automated theorem prover for formulas in a union of first-order theories. CVCL supports a set of theories which are useful in verification, including uninterpreted functions, arrays, records and tuples, and linear arithmetic. New features in CVCL (beyond those provided in similar previous systems) include a library API, more support for producing proofs, some heuristics for reasoning about quantifiers, and support for symbolic simulation primitives.", "num_citations": "384\n", "authors": ["1600"]}
{"title": "CVC: A cooperating validity checker\n", "abstract": " Decision procedures for decidable logics and logical theories have proven to be useful tools in verification. This paper describes the CVC (\u201cCooperating Validity Checker\u201d) decision procedure. CVC implements a framework for combining subsidiary decision procedures for certain logical theories into a decision procedure for the theories\u2019 union. Subsidiary decision procedures for theories of arrays, inductive datatypes, and linear real arithmetic are currently implemented. Other notable features of CVC are the incorporation of the high-performance Chaff solver for propositional reasoning, and the ability to produce independently checkable proofs for valid formulas.", "num_citations": "272\n", "authors": ["1600"]}
{"title": "Validity checking for combinations of theories with equality\n", "abstract": " An essential component in many verification methods is a fast decision procedure for validating logical expressions. This paper presents the algorithm used in the Stanford Validity Checker (SVC) which has been used to aid several realistic hardware verification efforts. The logic for this decision procedure includes Boolean and uninterpreted functions and linear arithmetic. We have also successfully incorporated other interpreted functions, such as array operations and linear inequalities. The primary techniques which allow a complete and efficient implementation are expression sharing, heuristic rewriting, and congruence closure with interpreted functions. We discuss these techniques and present the results of initial experiments in which SVC is used as a decision procedure in PVS, resulting in dramatic speed-ups.", "num_citations": "266\n", "authors": ["1600"]}
{"title": "Adversarial transformation networks: Learning to generate adversarial examples\n", "abstract": " Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "num_citations": "233\n", "authors": ["1600"]}
{"title": "Checking satisfiability of first-order formulas by incremental translation to SAT\n", "abstract": " In the past few years, general-purpose propositional satisfiability (SAT) solvers have improved dramatically in performance and have been used to tackle many new problems. It has also been shown that certain simple fragments of first-order logic can be decided efficiently by first translating the problem into an equivalent SAT problem and then using a fast SAT solver. In this paper, we describe an alternative but similar approach to using SAT in conjunction with a more expressive fragment of first-order logic. However, rather than translating the entire formula up front, the formula is incrementally translated during a search for the solution. As a result, only that portion of the translation that is actually relevant to the solution is obtained. We describe a number of obstacles that had to be overcome before developing an approach which was ultimately very effective, and give results on verification benchmarks using\u00a0\u2026", "num_citations": "214\n", "authors": ["1600"]}
{"title": "A decision procedure for an extensional theory of arrays\n", "abstract": " A decision procedure for a theory of arrays is of interest for applications in formal verification, program analysis and automated theorem proving. This paper presents a decision procedure for an extensional theory of arrays and proves it correct.", "num_citations": "198\n", "authors": ["1600"]}
{"title": "The marabou framework for verification and analysis of deep neural networks\n", "abstract": " Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network\u2019s properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the\u00a0\u2026", "num_citations": "189\n", "authors": ["1600"]}
{"title": "A decision procedure for bit-vector arithmetic\n", "abstract": " Bit-v ector theories with concatenation and extraction have been shown to be useful and important for hardware verification. We have implemented an extended theory which includes arithmetic. Although deciding equality in suc ha theory is NP-hard, our implementation is efficient for many practical examples. We believ e this to be the first such implementation which is efficient, automatic, and complete.", "num_citations": "174\n", "authors": ["1600"]}
{"title": "Algorithms for verifying deep neural networks\n", "abstract": " Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.", "num_citations": "151\n", "authors": ["1600"]}
{"title": "Deepsafe: A data-driven approach for assessing robustness of neural networks\n", "abstract": " Deep neural networks have achieved impressive results in many complex applications, including classification tasks for image and speech recognition, pattern analysis or perception in self-driving vehicles. However, it has been observed that even highly trained networks are very vulnerable to adversarial perturbations. Adding minimal changes to inputs that are correctly classified can lead to wrong predictions, raising serious security and safety concerns. Existing techniques for checking robustness against such perturbations only consider searching locally around a few individual inputs, providing limited guarantees. We propose DeepSafe, a novel approach for automatically assessing the overall robustness of a neural network. DeepSafe applies clustering over known labeled data and leverages off-the-shelf constraint solvers to automatically identify and check safe regions in which the network is robust\u00a0\u2026", "num_citations": "128\n", "authors": ["1600"]}
{"title": "SMT-COMP: Satisfiability modulo theories competition\n", "abstract": " Decision procedures for checking satisfiability of logical formulas are crucial for many verification applications (e.g.,\u00a0[2,6,3]). Of particular recent interest are solvers for Satisfiability Modulo Theories (SMT). SMT solvers decide logical satisfiability (or dually, validity) with respect to a background theory in classical first-order logic with equality. Background theories useful for verification are supported, like equality and uninterpreted functions (EUF), real or integer arithmetic, and theories of bitvectors and arrays. Input formulas are often syntactically restricted; for example, to be quantifier-free or to involve only difference constraints. Some solvers support a combination of theories, or quantifiers.", "num_citations": "123\n", "authors": ["1600"]}
{"title": "Provably minimally-distorted adversarial examples\n", "abstract": " The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.", "num_citations": "115\n", "authors": ["1600"]}
{"title": "Towards proving the adversarial robustness of deep neural networks\n", "abstract": " Autonomous vehicles are highly complex systems, required to function reliably in a wide variety of situations. Manually crafting software controllers for these vehicles is difficult, but there has been some success in using deep neural networks generated using machine-learning. However, deep neural networks are opaque to human engineers, rendering their correctness very difficult to prove manually; and existing automated techniques, which were not designed to operate on neural networks, fail to scale to large systems. This paper focuses on proving the adversarial robustness of deep neural networks, i.e. proving that small perturbations to a correctly-classified input to the network cannot cause it to be misclassified. We describe some of our recent and ongoing work on verifying the adversarial robustness of networks, and discuss some of the open questions we have encountered and how they might be addressed.", "num_citations": "101\n", "authors": ["1600"]}
{"title": "6 years of SMT-COMP\n", "abstract": " The annual Satisfiability Modulo Theories Competition (SMT-COMP) was initiated in 2005 in order to stimulate the advance of state-of-the-art techniques and tools developed by the Satisfiability Modulo Theories (SMT) community. This paper summarizes the first six editions of the competition. We present the evolution of the competition\u2019s organization and rules, show how the state of the art has improved over the course of the competition, and discuss the impact SMT-COMP has had on the SMT community and beyond. Additionally, we include an exhaustive list of all competitors, and present experimental results showing significant improvement in SMT solvers during these six years. Finally, we analyze to what extent the initial goals of the competition have been achieved, and sketch future directions for the competition.", "num_citations": "96\n", "authors": ["1600"]}
{"title": "TVOC: A translation validator for optimizing compilers\n", "abstract": " We describe a tool called TVOC, that uses the translation validation approach to check the validity of compiler optimizations: for a given source program, TVOC proves the equivalence of the source code and the target code produced by running the compiler. There are two phases to the verification process: the first phase verifies loop transformations using the proof rule permute; the second phase verifies structure-preserving optimizations using the proof rule Validate. Verification conditions are validated using the automatic theorem prover CVC Lite.", "num_citations": "87\n", "authors": ["1600"]}
{"title": "Cooperating theorem provers: A case study combining HOL-Light and CVC Lite\n", "abstract": " This paper is a case study in combining theorem provers. We define a derived rule in HOL-Light, CVC_PROVE, which calls CVC Lite and translates the resulting proof object back to HOL-Light. As a result, we obtain a highly trusted proof-checker for CVC Lite, while also fundamentally expanding the capabilities of HOL-Light.", "num_citations": "78\n", "authors": ["1600"]}
{"title": "A Generalization of Shostak's Method for Combining Decision Procedures\n", "abstract": " Consider the problem of determining whether a quantifier-free formula \u03d5 is satisfiable in some first-order theory T . Shostak#x2019;s algorithm decides this problem for a certain class of theories with both interpreted and uninterpreted function symbols. We present two new algorithms based on Shostak#x2019;s method. The first is a simple subset of Shostak's algorithm for the same class of theories but without uninterpreted function symbols. This simplified algorithm is easy to understand and prove correct, providing insight into how and why Shostak#x2019;s algorithm works. The simplified algorithm is then used as the foundation for a generalization of Shostak's method based on a variation of the Nelson- Oppen method for combining theories.", "num_citations": "70\n", "authors": ["1600"]}
{"title": "Handbook of satisfiability\n", "abstract": " Applications in artificial intelligence and formal methods for hardware and software development have greatly benefited from the recent advances in SAT. Often, however, applications in these fields require determining the satisfiability of formulas in more expressive logics such as first-order logic. Despite the great progress made in the last twenty years, general-purpose first-order theorem provers (such as provers based on the resolution calculus) are typically not able to solve such formulas directly. The main reason for this is that many applications require not general first-order satisfiability, but rather satisfiability with respect to some background theory, which fixes the interpretations of certain predicate and function symbols. For instance, applications using integer arithmetic are not interested in whether there exists a nonstandard interpretation of the symbols<,+, and 0 that makes the formula x< y\u2227\u00ac(x< y+ 0\u00a0\u2026", "num_citations": "69\n", "authors": ["1600"]}
{"title": "Translation and run-time validation of loop transformations\n", "abstract": " This paper presents new approaches to the validation of loop optimizations that compilers use to obtain the highest performance from modern architectures. Rather than verify the compiler, the approach of translation validationperforms a validation check after every run of the compiler, producing a formal proof that the produced target code is a correct implementation of the source code.               As part of an active and ongoing research project on translation validation, we have previously described approaches for validating optimizations that preserve the loop structure of the code and have presented a simulation-based general technique for validating such optimizations. In this paper, for more aggressive optimizations that alter the loop structure of the code\u2014such as distribution, fusion, tiling, and interchange\u2014we present a set of permutation ruleswhich establish that the transformed code satisfies all the\u00a0\u2026", "num_citations": "69\n", "authors": ["1600"]}
{"title": "Design and results of the first satisfiability modulo theories competition (SMT-COMP 2005)\n", "abstract": " The Satisfiability Modulo Theories Competition (SMT-COMP) is intended to spark further advances in the decision procedures field, especially for applications in hardware and software verification. Public competitions are a well-known means of stimulating advancement in automated reasoning. Evaluation of SMT solvers entered in SMT-COMP took place while CAV 2005 was meeting. Twelve solvers were entered; 1,352 benchmarks were collected in seven different divisions.", "num_citations": "58\n", "authors": ["1600"]}
{"title": "P4pktgen: Automated test case generation for p4 programs\n", "abstract": " With the rise of programmable network switches, network infrastructure is becoming more flexible and more capable than ever before. Programming languages such as P4 lower the barrier for changing the inner workings of network switches and offer a uniform experience across different devices. However, this programmability also brings the risk of introducing hard-to-catch bugs at a level that was previously covered by well-tested devices with a fixed set of capabilities. Subtle discrepancies between different implementations pose a risk of introducing bugs at a layer that is opaque to the user.", "num_citations": "54\n", "authors": ["1600"]}
{"title": "Into the loops: Practical issues in translation validation for optimizing compilers\n", "abstract": " Translation Validation is a technique for ensuring that the target code produced by a translator is a correct translation of the source code. Rather than verifying the translator itself, translation validation validates the correctness of each translation, generating a formal proof that it is indeed a correct. Recently, translation validation has been applied to prove the correctness of compilation in general, and optimizations in particular.Tvoc, a tool for the Translation Validation of Optimizing Compilers developed by the authors and their colleagues, successfully handles many optimizations employed by Intel's ORC compiler. Tvoc, however, is somewhat limited when dealing with loop reordering transformations. First, in the theory upon which it is based, separate proof rules are needed for different categories of loop reordering transformations. Second, Tvoc has difficulties dealing with combinations of optimizations that are\u00a0\u2026", "num_citations": "48\n", "authors": ["1600"]}
{"title": "Design and results of the 3rd annual satisfiability modulo theories competition (SMT-COMP 2007)\n", "abstract": " The Satisfiability Modulo Theories Competition (SMT-COMP) is an annual competition aimed at stimulating the advance of the state-of-the-art techniques and tools developed by the Satisfiability Modulo Theories (SMT) community. As with the first two editions, SMT-COMP 2007 was held as a satellite event of CAV 2007, held July 3-7, 2007. This paper gives an overview of the rules, competition format, benchmarks, participants and results of SMT-COMP 2007.", "num_citations": "44\n", "authors": ["1600"]}
{"title": "Checking validity of quantifier-free formulas in combinations of first-order theories\n", "abstract": " An essential component in many verification methods is a fast decision procedure for validating logical expressions. This thesis presents several advances in the theory and implementation of such decision procedures, developed as part of ongoing efforts to improve the Stanford Validity Checker. We begin with the general problem of combining satisfiability procedures for individual theories into a satisfiability procedure for the combined theory. Two known approaches, those of Shostak and Nelson and Oppen, are described. We show how to combine these two methods to obtain the generality of the Nelson-Oppen method while retaining the efficiency of the Shostak method. We then present a general framework for combining decision procedures which includes features for enhancing performance and flexibility. Finally, validity checking requires that a heuristic search be built on top of the core decision procedure\u00a0\u2026", "num_citations": "44\n", "authors": ["1600"]}
{"title": "The design and implementation of the model constructing satisfiability calculus\n", "abstract": " We present the design and implementation of the Model Constructing Satisfiability (MCSat) calculus. The MCSat calculus generalizes ideas found in CDCL-style propositional SAT solvers to SMT solvers, and provides a common framework where recent model-based procedures and techniques can be justified and combined. We describe how to incorporate support for linear real arithmetic and uninterpreted function symbols m the calculus. We report encouraging experimental results, where MCSat performs competitive with the state-of-the art SMT solvers without using pre-processing techniques and ad-hoc optimizations. The implementation is flexible, additional plugins can be easily added, and the code is freely available.", "num_citations": "42\n", "authors": ["1600"]}
{"title": "A framework for cooperating decision procedures\n", "abstract": " We present a flexible framework for cooperating decision procedures. We describe the properties needed to ensure correctness and show how it can be applied to implement an efficient version of Nelson and Oppen\u2019s algorithm for combining decision procedures. We also show how a Shostak style decision procedure can be implemented in the framework in such a way that it can be integrated with the Nelson\u2013Oppen method.", "num_citations": "42\n", "authors": ["1600"]}
{"title": "Verifying deep-RL-driven systems\n", "abstract": " Deep reinforcement learning (RL) has recently been successfully applied to networking contexts including routing, flow scheduling, congestion control, packet classification, cloud resource management, and video streaming. Deep-RL-driven systems automate decision making, and have been shown to outperform state-of-the-art handcrafted systems in important domains. However, the (typical) non-explainability of decisions induced by the deep learning machinery employed by these systems renders reasoning about crucial system properties, including correctness and security, extremely difficult. We show that despite the obscurity of decision making in these contexts, verifying that deep-RL-driven systems adhere to desired, designer-specified behavior, is achievable. To this end, we initiate the study of formal verification of deep RL and present Verily, a system for verifying deep-RL-based systems that leverages\u00a0\u2026", "num_citations": "38\n", "authors": ["1600"]}
{"title": "Polite theories revisited\n", "abstract": " The classic method of Nelson and Oppen for combining decision procedures requires the theories to be stably-infinite. Unfortunately, some important theories do not fall into this category (e.g. the theory of bit-vectors). To remedy this problem, previous work introduced the notion of polite theories. Polite theories can be combined with any other theory using an extension of the Nelson-Oppen approach. In this paper we revisit the notion of polite theories, fixing a subtle flaw in the original definition. We give a new combination theorem which specifies the degree to which politeness is preserved when combining polite theories. We also give conditions under which politeness is preserved when instantiating theories by identifying two sorts. These results lead to a more general variant of the theorem for combining multiple polite theories.", "num_citations": "35\n", "authors": ["1600"]}
{"title": "Automatic generation of invariants in processor verification\n", "abstract": " A central task in formal verification is the definition of invariants, which characterize the reachable states of the system. When a system is finitestate, invariants can be discovered automatically.             Our experience in verifying microprocessors using symbolic logic is that finding adequate invariants is extremely time-consuming. We present three techniques for automating the discovery of some of these invariants. All of them are essentially syntactic transformations on a logical formula derived from the state transition function. The goal is to eliminate quantifiers and extract small clauses implied by the larger formula.             We have implemented the method and exercised it on a description of the FLASH Protocol Processor (PP), a microprocessor designed at Stanford for handling communications in a multiprocessor. We had previously verified the PP by manually deriving invariants.             Although the method\u00a0\u2026", "num_citations": "33\n", "authors": ["1600"]}
{"title": "A structured approach to post-silicon validation and debug using symbolic quick error detection\n", "abstract": " During post-silicon validation and debug, manufactured integrated circuits (ICs) are tested in actual system environments to detect and fix design flaws (bugs). Existing post-silicon validation and debug techniques are mostly ad hoc and often involve manual steps. Such ad hoc approaches cannot scale with increasing IC complexity. We present Symbolic Quick Error Detection (Symbolic QED), a structured approach to post-silicon validation and debug. Symbolic QED combines the following steps in a coordinated fashion: 1. Quick Error Detection (QED) tests that quickly detect bugs with short error detection latencies and high coverage. 2. Formal analysis techniques to localize bugs and generate minimal-length bug traces upon detection of the corresponding bugs. We demonstrate the practicality and effectiveness of Symbolic QED using the OpenSPARC T2, a 500-million-transistor open-source multicore System-on\u00a0\u2026", "num_citations": "30\n", "authors": ["1600"]}
{"title": "Toward scalable verification for safety-critical deep networks\n", "abstract": " The increasing use of deep neural networks for safety-critical applications, such as autonomous driving and flight control, raises concerns about their safety and reliability. Formal verification can address these concerns by guaranteeing that a deep learning system operates as intended, but the state of the art is limited to small systems. In this work-in-progress report we give an overview of our work on mitigating this difficulty, by pursuing two complementary directions: devising scalable verification techniques, and identifying design choices that result in deep learning systems that are more amenable to verification.", "num_citations": "29\n", "authors": ["1600"]}
{"title": "Processor hardware security vulnerabilities and their detection by unique program execution checking\n", "abstract": " Recent discovery of security attacks in advanced processors, known as Spectre and Meltdown, has resulted in high public alertness about security of hardware. The root cause of these attacks is information leakage across covert channels that reveal secret data without any explicit information flow between the secret and the attacker. Many sources believe that such covert channels are intrinsic to highly advanced processor architectures based on speculation and out-of-order execution, suggesting that such security risks can be avoided by staying away from high-end processors. This paper, however, shows that the problem is of wider scope: we present new classes of covert channel attacks which are possible in average-complexity processors with in-order pipelining, as they are mainstream in applications ranging from Internet-of-Things to Autonomous Systems. We present a new approach as a foundation for\u00a0\u2026", "num_citations": "26\n", "authors": ["1600"]}
{"title": "Deciding local theory extensions via e-matching\n", "abstract": " Satisfiability Modulo Theories (SMT) solvers incorporate decision procedures for theories of data types that commonly occur in software. This makes them important tools for automating verification problems. A limitation frequently encountered is that verification problems are often not fully expressible in the theories supported natively by the solvers. Many solvers allow the specification of application-specific theories as quantified axioms, but their handling is incomplete outside of narrow special cases.                 In this work, we show how SMT solvers can be used to obtain complete decision procedures for local theory extensions, an important class of theories that are decidable using finite instantiation of axioms. We present an algorithm that uses E-matching to generate instances incrementally during the search, significantly reducing the number of generated instances compared to eager instantiation\u00a0\u2026", "num_citations": "26\n", "authors": ["1600"]}
{"title": "Proofs in satisfiability modulo theories\n", "abstract": " Satisfiability Modulo Theories (SMT) solvers4 check the satisfiability of firstorder formulas written in a language containing interpreted predicates and functions. These interpreted symbols are defined either by first-order axioms (eg the axioms of equality, or array axioms for operators read and write,...) or by a structure (eg the integer numbers equipped with constants, addition, equality, and inequalities). Theories frequently implemented within SMT solvers include the empty theory (aka the theory of uninterpreted symbols with equality), linear arithmetic on integers and/or reals, bit-vectors, and the theory of arrays. A very small example of an input formula for an SMT solver is a\u2264 b\u2227 b\u2264 a+ x\u2227 x= 0\u2227[f (a)= f (b)\u2228(q (a)\u2227\u00ac q (b+ x))].(1)The above formula uses atoms over a language of equality, linear arithmetic, and uninterpreted symbols (q and f) within some Boolean combination. The SMTLIB language (currently in version 2.0 [4]) is a standard concrete input language for SMT solvers. Figure 1 presents the above example formula in this format. SMT solvers were originally designed as decision procedures for decidable quantifier-free fragments, but many SMT solvers additionally tackle quantifiers, and some contain decision procedures for certain decidable quantified fragments (see eg [26]). For these solvers, refutational completeness for first-order logic (with equality but without further interpreted symbols) is an explicit goal. Also, some SMT solvers now deal with theories that are undecidable even in the quantifier-free case, for instance non-linear arithmetic on integers [13]. In some aspects, and also in their implementation, SMT solvers can be\u00a0\u2026", "num_citations": "26\n", "authors": ["1600"]}
{"title": "Verifying recurrent neural networks using invariant inference\n", "abstract": " Deep neural networks are revolutionizing the way complex systems are developed. However, these automatically-generated networks are opaque to humans, making it difficult to reason about them and guarantee their correctness. Here, we propose a novel approach for verifying properties of a widespread variant of neural networks, called recurrent neural networks. Recurrent neural networks play a key role in, e.g., speech recognition, and their verification is crucial for guaranteeing the reliability of many critical systems. Our approach is based on the inference of invariants, which allow us to reduce the complex problem of verifying recurrent networks into simpler, non-recurrent problems. Experiments with a proof-of-concept implementation of our approach demonstrate that it performs orders-of-magnitude better than the state of the art.", "num_citations": "21\n", "authors": ["1600"]}
{"title": "Partitioned memory models for program analysis\n", "abstract": " Scalability is a key challenge in static analysis. For imperative languages like C, the approach taken for modeling memory can play a significant role in scalability. In this paper, we explore a family of memory models called partitioned memory models which divide memory up based on the results of a points-to analysis. We review Steensgaard\u2019s original and field-sensitive points-to analyses as well as Data Structure Analysis (DSA), and introduce a new cell-based points-to analysis which more precisely handles heap data structures and type-unsafe operations like pointer arithmetic and pointer casting. We give experimental results on benchmarks from the software verification competition using the program verification framework in Cascade. We show that a partitioned memory model using our cell-based points-to analysis outperforms models using other analyses.", "num_citations": "20\n", "authors": ["1600"]}
{"title": "Design and results of the 4th annual satisfiability modulo theories competition (SMT-COMP 2008)\n", "abstract": " The Satisfiability Modulo Theories Competition (SMT-COMP) is an annual competition aimed at stimulating the advance of the state-of-the-art techniques and tools developed by the Satisfiability Modulo Theories (SMT) community. As with the first three editions, SMT-COMP 2008 was held as a satellite event of CAV 2008, held July 7\u201314, 2008. This report gives an overview of the rules, competition format, benchmarks, participants and results of SMT-COMP 2008.", "num_citations": "20\n", "authors": ["1600"]}
{"title": "Reuse of learned information to simplify functional verification of a digital circuit\n", "abstract": " A computer is programmed in accordance with the invention to automatically analyze a digital circuit, to check if the digital circuit can enter a target state starting from a start state, by reusing information learned during a another analysis, checking if the same digital circuit can enter the same or different target state from a different start state. Use of learned information in accordance with the invention simplifies the analysis of the digital circuit (eg by allowing skipping one or more analysis acts). The learned information may be stored in a database. Depending on the embodiment, the two or more analyses may check on operation of the digital circuit for the same or different numbers of cycles.", "num_citations": "20\n", "authors": ["1600"]}
{"title": "Parallelization techniques for verifying neural networks\n", "abstract": " Inspired by recent successes of parallel techniques for solving Boolean satisfiability, we investigate a set of strategies and heuristics to leverage parallelism and improve the scalability of neural network verification. We present a general description of the Split-and-Conquer partitioning algorithm, implemented within the Marabou framework, and discuss its parameters and heuristic choices. In particular, we explore two novel partitioning strategies, that partition the input space or the phases of the neuron activations, respectively. We introduce a branching heuristic and a direction heuristic that are based on the notion of polarity. We also introduce a highly parallelizable pre-processing algorithm for simplifying neural network verification problems. An extensive experimental evaluation shows the benefit of these techniques on both existing and new benchmarks. A preliminary experiment ultra-scaling our algorithm using a large distributed cloud-based platform also shows promising results.", "num_citations": "19\n", "authors": ["1600"]}
{"title": "Ground-truth adversarial examples\n", "abstract": " The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses. This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks. We propose to address this difficulty through formal verification techniques. We construct ground truths: adversarial examples with a provably-minimal distance from a given input point. We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement. We use this technique to assess recently suggested attack and defense techniques.", "num_citations": "18\n", "authors": ["1600"]}
{"title": "Proof translation and SMT-LIB benchmark certification: A preliminary report\n", "abstract": " Satisfiability Modulo Theories (SMT) solvers are large and complicated pieces of code. As a result, ensuring their correctness is challenging. In this paper, we discuss a technique for ensuring soundness by producing and checking proofs. We give details of our implementation using CVC3 and HOL Light and provide initial results from our effort to certify the SMT-LIB benchmarks.", "num_citations": "18\n", "authors": ["1600"]}
{"title": "G2SAT: Learning to generate sat formulas\n", "abstract": " The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem and is fundamental to computer science, with a wide array of applications in planning, verification, and theorem proving. Developing and evaluating practical SAT solvers relies on extensive empirical testing on a set of real-world benchmark formulas. However, the availability of such real-world SAT formulas is limited. While these benchmark formulas can be augmented with synthetically generated ones, existing approaches for doing so are heavily hand-crafted and fail to simultaneously capture a wide range of characteristics exhibited by real-world SAT instances. In this work, we present G2SAT, the first deep generative framework that learns to generate SAT formulas from a given set of input formulas. Our key insight is that SAT formulas can be transformed into latent bipartite graph representations which we model using a specialized\u00a0\u2026", "num_citations": "17\n", "authors": ["1600"]}
{"title": "A proof-producing boolean search engine\n", "abstract": " We present a proof-producing search engine for solving the Boolean satisfiability problem. We show how the proof-producing infrastructure can be used to track the dependency information needed to implement important optimizations found in modern SAT solvers. We also describe how the same search engine can be extended to work with decision procedures for quantifier-free firstorder logic. Initial results indicate that it is possible to extend a state-of-the-art SAT solver with proof production in a way that both preserves the algorithmic performance (eg the number of decisions to solve a problem) and does not incur unreasonable overhead for the proofs.", "num_citations": "17\n", "authors": ["1600"]}
{"title": "CoSA: Integrated verification for agile hardware design\n", "abstract": " Symbolic model-checking is a well-established technique used in hardware design to assess, and formally verify, functional correctness. However, most modern model-checkers encode the problem into propositional satisfiability (SAT) and do not leverage any additional information beyond the input design, which is typically provided in a hardware description language such as Verilog.In this paper, we present CoSA (CoreIR Symbolic Analyzer), a model-checking tool for CoreIR designs. CoreIR is a new intermediate representation for hardware. CoSA encodes model-checking queries into first-order formulas that can be solved by Satisfiability Modulo Theories (SMT) solvers. In particular, it natively supports encodings using the theories of bitvectors and arrays. CoSA is closely integrated with CoreIR and can thus leverage CoreIR-generated metadata in addition to user-provided lemmas to assist with formal\u00a0\u2026", "num_citations": "16\n", "authors": ["1600"]}
{"title": "Logic bug detection and localization using symbolic quick error detection\n", "abstract": " We present Symbolic Quick Error Detection (Symbolic QED), a structured approach for logic bug detection and localization which can be used both during pre-silicon design verification as well as post-silicon validation and debug. This new methodology leverages prior work on Quick Error Detection (QED) which has been demonstrated to drastically reduce the latency, in terms of the number of clock cycles, of error detection following the activation of a logic (or electrical) bug. QED works through software transformations, including redundant execution and control flow checking, of the applied tests. Symbolic QED combines these error-detecting QED transformations with bounded model checking-based formal analysis to generate minimal-length bug activation traces that detect and localize any logic bugs in the design. We demonstrate the practicality and effectiveness of Symbolic QED using the OpenSPARC T2, a\u00a0\u2026", "num_citations": "16\n", "authors": ["1600"]}
{"title": "Symbolic quick error detection using symbolic initial state for pre-silicon verification\n", "abstract": " Driven by the demand for highly customizable processor cores for IoT and related applications, there is a renewed interest in effective but low-cost techniques for verifying systems-on-chip (SoCs). This paper revisits the problem of processor verification and presents a radically different approach when compared to the state of the art. The proposed approach is highly automated and leverages recent progress in the field of post-silicon validation by the method of Quick Error Detection (QED) and Symbolic Quick Error Detection (SQED). In this paper, we modify SQED by incorporating a symbolic initial state in its BMC-based analysis and generalize the approach into the S 2 QED method. As a first advantage, S 2 QED can separate logic bugs from electrical bugs in QED-based postsilicon validation. Secondly, it also makes a strong contribution to pre-silicon verification by proving that the execution of each instruction is\u00a0\u2026", "num_citations": "16\n", "authors": ["1600"]}
{"title": "Theory-aided model checking of concurrent transition systems\n", "abstract": " We present a method for the automatic compositional verification of certain classes of concurrent programs. Our approach is based on the casting of the model checking problem into a theory of transition systems within CVC4, a DPLL(T) based SMT solver. Our transition system theory then cooperates with other theories supported by the solver (e.g., arithmetic, arrays), which can help accelerate the verification process. More specifically, our theory solver looks for known patterns within the input programs and uses them to generate lemmas in the languages of other theories. When applicable, these lemmas can often steer the search away from safe parts of the search space, reducing the number of states to be explored and expediting the model checking procedure. We demonstrate the potential of our technique on a number of broad classes of programs.", "num_citations": "14\n", "authors": ["1600"]}
{"title": "Combining SAT methods with non-clausal decision heuristics\n", "abstract": " A decision procedure for arbitrary first-order formulas can be viewed as combining a propositional search with a decision procedure for conjunctions of first-order literals, so Boolean SAT methods can be used for the propositional search in order to improve the performance of the overall decision procedure. We show how to combine some Boolean SAT methods with non-clausal heuristics developed for first-order decision procedures. The combination of methods leads to a smaller number of decisions than either method alone.", "num_citations": "14\n", "authors": ["1600"]}
{"title": "An SMT-based approach for verifying binarized neural networks\n", "abstract": " Deep learning has emerged as an effective approach for creating modern software systems, with neural networks often surpassing hand-crafted systems. Unfortunately, neural networks are known to suffer from various safety and security issues. Formal verification is a promising avenue for tackling this difficulty, by formally certifying that networks are correct. We propose an SMT-based technique for verifying Binarized Neural Networks - a popular kind of neural network, where some weights have been binarized in order to render the neural network more memory and energy efficient, and quicker to evaluate. One novelty of our technique is that it allows the verification of neural networks that include both binarized and non-binarized components. Neural network verification is computationally very difficult, and so we propose here various optimizations, integrated into our SMT procedure as deduction steps, as well as an approach for parallelizing verification queries. We implement our technique as an extension to the Marabou framework, and use it to evaluate the approach on popular binarized neural network architectures.", "num_citations": "12\n", "authors": ["1600"]}
{"title": "Simplifying neural networks using formal verification\n", "abstract": " Deep neural network (DNN) verification is an emerging field, with diverse verification engines quickly becoming available. Demonstrating the effectiveness of these engines on real-world DNNs is an important step towards their wider adoption. We present a tool that can leverage existing verification engines in performing a novel application: neural network simplification, through the reduction of the size of a DNN without harming its accuracy. We report on the work-flow of the simplification process, and demonstrate its potential significance and applicability on a family of real-world DNNs for aircraft collision avoidance, whose sizes we were able to reduce by as much as 10%.", "num_citations": "12\n", "authors": ["1600"]}
{"title": "Validating more loop optimizations\n", "abstract": " Translation validation is a technique for ensuring that a translator, such as a compiler, produces correct results. Because complete verification of the translator itself is often infeasible, translation validation advocates coupling the verification task with the translation task, so that each run of the translator produces verification conditions which, if valid, prove the correctness of the translation. In previous work, the translation validation approach was used to give a framework for proving the correctness of a variety of compiler optimizations, with a recent focus on loop transformations. However, some of these ideas were preliminary and had not been implemented. Additionally, there were examples of common loop transformations which could not be handled by our previous approaches.This paper addresses these issues. We introduce a new rule Reduce for loop reduction transformations, and we generalize our previous\u00a0\u2026", "num_citations": "12\n", "authors": ["1600"]}
{"title": "Sharing is caring: Combination of theories\n", "abstract": " One of the main shortcomings of the traditional methods for combining theories is the complexity of guessing the arrangement of the variables shared by the individual theories. This paper presents a reformulation of the Nelson-Oppen method that takes into account explicit equality propagation and can ignore pairs of shared variables that the theories do not care about. We show the correctness of the new approach and present care functions for the theory of uninterpreted functions and the theory of arrays. The effectiveness of the new method is illustrated by experimental results demonstrating a dramatic performance improvement on benchmarks combining arrays and bit-vectors.", "num_citations": "11\n", "authors": ["1600"]}
{"title": "Cascade 2.0\n", "abstract": " Cascade is a program static analysis tool developed at New York University. Cascade takes as input a program and a control file. The control file specifies one or more assertions to be checked together with restrictions on program behaviors. The tool generates verification conditions for the specified assertions and checks them using an SMT solver which either produces a proof or gives a concrete trace showing how an assertion can fail. Version 2.0 supports the majority of standard C features except for floating point. It can be used to verify both memory safety as well as user-defined assertions. In this paper, we describe the Cascade system including some of its distinguishing features such as its support for different memory models (trading off precision for scalability) and its ability to reason about linked data structures.", "num_citations": "10\n", "authors": ["1600"]}
{"title": "\u201cDecision Procedures: An Algorithmic Point of View,\u201d by Daniel Kroening and Ofer Strichman, Springer-Verlag, 2008\n", "abstract": " The topic of this book is decision procedures for first-order theories, a research area now typically referred to as Satisfiability Modulo Theories (SMT). 1 The book is important if for no other reason than because it is one of the first to capture the essential concepts of SMT in a book. 2 More importantly, most of the content is still relevant to those wishing to understand the area today. The book is structured as follows. After a high-level introductory chapter, and a chapter on propositional logic and Boolean satisfiability (SAT), the bulk of the content (chapters 3 through 8) deals with decision procedures for specific first-order theories: equality with uninterpreted functions (chapters 3 and 4), linear arithmetic (chapter 5), bit vectors (chapter 6), arrays (chapter 7), and pointer logic (chapter 8). Chapter 9 covers quantifiers (the other chapters focus on quantifier-free formulas); chapter 10 covers theory combination; and chapter 11\u00a0\u2026", "num_citations": "10\n", "authors": ["1600"]}
{"title": "Cascade: C assertion checker and deductive engine\n", "abstract": " We present a tool, called cascade, to check assertions in C programs as part of a multi-stage verification strategy. cascade takes as input a C program and a control file (the output of an earlier stage) that specifies one or more assertions to be checked together with (optionally) some restrictions on program behaviors. For each assertion, cascade produces either a concrete trace violating the assertion or a deduction (proof) that the assertion cannot be violated.", "num_citations": "10\n", "authors": ["1600"]}
{"title": "Gap-free Processor Verification by S2QED and Property Generation\n", "abstract": " The required manual effort and verification expertise are among the main hurdles for adopting formal verification in processor design flows. Developing a set of properties that fully covers all instruction behaviors is a laborious and challenging task. This paper proposes a highly automated and \"complete\" processor verification approach which requires considerably less manual effort and expertise compared to the state of the art.The proposed approach extends the S 2 QED approach to cover both single and multiple instruction bugs and ensures that a design is completely verified according to a well-defined criterion. This makes the approach robust against human errors. The properties are simple and can be automatically generated from an ISA model with small manual effort. Furthermore, unlike in conventional property checking, the verification engineer does not need to explicitly specify the processor's behavior\u00a0\u2026", "num_citations": "9\n", "authors": ["1600"]}
{"title": "Symbolic QED pre-silicon verification for automotive microcontroller cores: Industrial case study\n", "abstract": " We present an industrial case study that demonstrates the practicality and effectiveness of Symbolic Quick Error Detection (Symbolic QED) in detecting logic design flaws (logic bugs) during pre-silicon verification. Our study focuses on several microcontroller core designs (~1,800 flip-flops, ~70,000 logic gates) that have been extensively verified using an industrial verification flow and used for various commercial automotive products. The results of our study are as follows: 1. Symbolic QED detected all logic bugs in the designs that were detected by the industrial verification flow (which includes various flavors of simulation-based verification and formal verification). 2. Symbolic QED detected additional logic bugs that were not recorded as detected by the industrial verification flow. (These additional bugs were also perhaps detected by the industrial verification flow.)3.Symbolic QED enables significant design\u00a0\u2026", "num_citations": "9\n", "authors": ["1600"]}
{"title": "EMME: a formal tool for ECMAScript Memory Model Evaluation\n", "abstract": " Nearly all web-based interfaces are written in JavaScript. Given its prevalence, the support for high performance JavaScript code is crucial. The ECMA Technical Committee 39 (TC39) has recently extended the ECMAScript language (i.e., JavaScript) to support shared memory accesses between different threads. The extension is given in terms of a natural language memory model specification. In this paper we describe a formal approach for validating both the memory model and its implementations in various JavaScript engines. We first introduce a formal version of the memory model and report results on checking the model for consistency and other properties. We then introduce our tool, EMME, built on top of the Alloy analyzer, which leverages the model to generate all possible valid executions of a given JavaScript program. Finally, we report results using EMME together with small test programs to\u00a0\u2026", "num_citations": "9\n", "authors": ["1600"]}
{"title": "E-QED: electrical bug localization during post-silicon validation enabled by quick error detection and formal methods\n", "abstract": " During post-silicon validation, manufactured integrated circuits are extensively tested in actual system environments to detect design bugs. Bug localization involves identification of a bug trace (a sequence of inputs that activates and detects the bug) and a hardware design block where the bug is located. Existing bug localization practices during post-silicon validation are mostly manual and ad hoc, and, hence, extremely expensive and time consuming. This is particularly true for subtle electrical bugs caused by unexpected interactions between a design and its electrical state. We present E-QED, a new approach that automatically localizes electrical bugs during post-silicon validation. Our results on the OpenSPARC T2, an open-source 500-million-transistor multicore chip design, demonstrate the effectiveness and practicality of E-QED: starting with a failed post-silicon test, in a few hours (9\u00a0h on average\u00a0\u2026", "num_citations": "9\n", "authors": ["1600"]}
{"title": "Reluplex: a calculus for reasoning about deep neural networks\n", "abstract": " Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully\u00a0\u2026", "num_citations": "8\n", "authors": ["1600"]}
{"title": "Being careful about theory combination\n", "abstract": " One of the main shortcomings of traditional methods for combining theories is the complexity of guessing the arrangement of variables shared by the individual theories. This paper presents a reformulation of the Nelson-Oppen method that takes into account explicit equality propagation and can ignore pairs of shared variables that the theories do not care about. We show the correctness of the new approach and present care functions for the theory of uninterpreted functions and the theory of arrays. The effectiveness of the new method is illustrated by experimental results demonstrating a dramatic performance improvement on benchmarks combining arrays and bit-vectors.", "num_citations": "8\n", "authors": ["1600"]}
{"title": "Theory and algorithms for the generation and validation of speculative loop optimizations\n", "abstract": " Translation validation is a technique that verifies the results of every run of a translator such as a compiler, instead of the translator itself. Previous papers by the authors and others have described translation validation for compilers that perform loop optimizations (such as interchange, tiling, fusion, etc), using a proof rule that treats loop optimizations as permutations. In this paper we describe an improved permutation proof rule which considers the initial conditions and invariant conditions of the loop. This new proof rule not only improves the validation process for compile-time optimizations, it can also be used to ensure the correctness of speculative loop optimizations, the aggressive optimizations which are only correct under certain conditions that cannot be known at compile time. Based on the new permutation rule, with the help of an automatic theorem prover CVC Lite, an algorithm is proposed for validating\u00a0\u2026", "num_citations": "8\n", "authors": ["1600"]}
{"title": "Global optimization of objective functions represented by ReLU networks\n", "abstract": " Neural networks (NN) learn complex non-convex functions, making them desirable solutions in many contexts. Applying NNs to safety-critical tasks demands formal guarantees about their behavior. Recently, a myriad of verification solutions for NNs emerged using reachability, optimization, and search based techniques. Particularly interesting are adversarial examples, which reveal ways the network can fail. They are widely generated using incomplete methods, such as local optimization, which cannot guarantee optimality. We propose strategies to extend existing verifiers to provide provably optimal adversarial examples. Naive approaches combine bisection search with an off-the-shelf verifier, resulting in many expensive calls to the verifier. Instead, our proposed approach yields tightly integrated optimizers, achieving better runtime performance. We extend Marabou, an SMT-based verifier, and compare it with the bisection based approach and MIPVerify, an optimization based verifier.", "num_citations": "7\n", "authors": ["1600"]}
{"title": "Pono: A Flexible and Extensible SMT-Based Model Checker\n", "abstract": " Symbolic model checking is an important tool for finding bugs (or proving the absence of bugs) in modern system designs. Because of this, improving the ease of use, scalability, and performance of model checking tools and algorithms continues to be an important research direction. In service of this goal, we present Pono, an open-source SMT-based model checker. Pono is designed to be both a research platform for developing and improving model checking algorithms, as well as a performance-competitive tool that can be used for academic and industry verification applications. In addition to performance, Pono prioritizes transparency (developed as an open-source project on GitHub), flexibility (Pono can be adapted to a variety of tasks by exploiting its general SMT-based interface), and extensibility (it is easy to add new algorithms and new back-end solvers). In this paper, we describe the design of\u00a0\u2026", "num_citations": "6\n", "authors": ["1600"]}
{"title": "Counterexample-Guided Prophecy for Model Checking Modulo the Theory of Arrays\n", "abstract": " We develop a framework for model checking infinite-state systems by automatically augmenting them with auxiliary variables, enabling quantifier-free induction proofs for systems that would otherwise require quantified invariants. We combine this mechanism with a counterexample-guided abstraction refinement scheme for the theory of arrays. Our framework can thus, in many cases, reduce inductive reasoning with quantifiers and arrays to quantifier-free and array-free reasoning. We evaluate the approach on a wide set of benchmarks from the literature. The results show that our implementation often outperforms state-of-the-art tools, demonstrating its practical potential.", "num_citations": "6\n", "authors": ["1600"]}
{"title": "Run-Time Validation of Speculative Optimizations using CVC.\n", "abstract": " Translation validation is an approach for validating the output of optimizing compilers. Rather than verifying the compiler itself, translation validation mandates that every run of the compiler generate a formal proof that the produced target code is a correct implementation of the source code. Speculative loop optimizations are aggressive optimizations which are only correct under certain conditions which cannot be validated at compile time. We propose using an automatic theorem prover together with the translation validation framework to automatically generate run-time tests for such speculative optimizations. This run-time validation approach must not only detect the conditions under which an optimization generates incorrect code, but also provide a way to recover from the optimization without aborting the program or producing an incorrect result. In this paper, we apply the run-time validation technique to a class\u00a0\u2026", "num_citations": "6\n", "authors": ["1600"]}
{"title": "Creating an agile hardware design flow\n", "abstract": " Although an agile approach is standard for software design, how to properly adapt this method to hardware is still an open question. This work addresses this question while building a system on chip (SoC) with specialized accelerators. Rather than using a traditional waterfall design flow, which starts by studying the application to be accelerated, we begin by constructing a complete flow from an application expressed in a high-level domain-specific language (DSL), in our case Halide, to a generic coarse-grained reconfigurable array (CGRA). As our under-standing of the application grows, the CGRA design evolves, and we have developed a suite of tools that tune application code, the compiler, and the CGRA to increase the efficiency of the resulting implementation. To meet our continued need to update parts of the system while maintaining the end-to-end flow, we have created DSL-based hardware generators\u00a0\u2026", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Agile smt-based mapping for cgras with restricted routing networks\n", "abstract": " Coarse-grained reconfigurable architectures (CGRAs) are becoming popular accelerators for computationally intensive tasks. CGRAs offer the reconfigurability of an FPGA, but with larger configurable blocks which provide performance closer to ASICs. CGRAs can achieve very high compute density if the routing networks are restricted; however, mapping using traditional annealing-based approaches does not perform well for such architectures. This paper uses Satisfiability Modulo Theories (SMT) solvers to rapidly map designs onto arbitrary CGRA fabrics. This approach is sound, complete, and in many cases an order of magnitude faster than state-of-the-art constraint-based mapping techniques using integer linear programming (ILP). Additionally, we propose a functional duplication strategy that decreases pressure on the routing network from high-fanout operations, leading to significant performance\u00a0\u2026", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Integration and flight test of small UAS detect and avoid on a miniaturized avionics platform\n", "abstract": " Detect and avoid (DAA) all other aircraft is a critical component to enable small unmanned aircraft system (sUAS) beyond visual line of sight (BVLOS) operations. Derived from the version of Airborne Collision Avoidance System X (ACAS X) for large UAS (ACAS Xu), a new member of the ACAS X family for sUAS (ACAS sXu) is being developed by the Federal Aviation Administration's (FAA's) Traffic-Alert and Collision Avoidance System (TCAS) Program Office. ACAS sXu is intended to provide both collision avoidance (CA) and remain well clear (RWC) capabilities with both vertical and horizontal advisories for the remote pilot in command (RPIC) and/or automated response system onboard the aircraft. ACAS sXu is envisioned to utilize a standard logic to serve sUASs with different equipages and operating in different airspace domains. The standard ACAS sXu logic may be hosted either in the embedded\u00a0\u2026", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Cascade\n", "abstract": " Cascade is a static program analysis tool developed at New York University. It uses bounded model checking to generate verification conditions and checks them using an SMT solver which either produces a proof of correctness or gives a concrete trace showing how an assertion can fail. It supports the majority of standard C features except for floating point. A distinguishing feature of Cascade is that its analysis uses a memory model which divides up memory into several partitions based on alias information.", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Witness runs for counter machines\n", "abstract": " In this paper, we present recent results about the verification of counter machines by using decision procedures for Presburger arithmetic. We recall several known classes of counter machines for which the reachability sets are Presburger-definable as well as temporal logics with arithmetical constraints. We discuss issues related to flat counter machines, path schema enumeration, and the use of SMT solvers.", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Producing proofs from an arithmetic decision procedure in elliptical LF\n", "abstract": " Software that can produce independently checkable evidence for the correctness of its output has received recent attention for use in certifying compilers and proof-carrying code. CVC (Cooperating Validity Checker) is a proof-producing validity checker for a decidable fragment of first-order logic enriched with background theories. This paper describes how proofs of valid formulas are produced from the decision procedure for linear real arithmetic implemented in CVC. It is shown how extensions to LF which support proof rules schematic in an arity (\u201celliptical\u201d rules) are very convenient for this purpose.", "num_citations": "5\n", "authors": ["1600"]}
{"title": "Unlocking the Power of Formal Hardware Verification with CoSA and Symbolic QED\n", "abstract": " As designs grow in size and complexity, design verification becomes one of the most difficult and costly tasks facing design teams. Formal verification techniques offer great promise because of their ability to exhaustively explore design behaviors. However, formal techniques also have a reputation for being labor-intensive and limited to small blocks. Is there any hope for successful application of formal techniques at design scale? We answer this question affirmatively by digging deeper to understand what the real technological issues and opportunities are. First, we look at satisfiability solvers, the engines underlying formal techniques such as model checking. Given the recent innovations in satisfiability solving, we argue that there are many reasons to be optimistic that formal techniques will scale to designs of practical interest. We use our CoSA model checker as a demonstration platform to illustrate how\u00a0\u2026", "num_citations": "4\n", "authors": ["1600"]}
{"title": "DRAT-based Bit-Vector Proofs in CVC4\n", "abstract": " Many state-of-the-art Satisfiability Modulo Theories (SMT) solvers for the theory of fixed-size bit-vectors employ an approach called bit-blasting, where a given formula is translated into a Boolean satisfiability (SAT) problem and delegated to a SAT solver. Consequently, producing bit-vector proofs in an SMT solver requires incorporating SAT proofs into its proof infrastructure. In this paper, we describe three approaches for integrating DRAT proofs generated by an off-the-shelf SAT solver into the proof infrastructure of the SMT solver CVC4 and explore their strengths and weaknesses. We implemented all three approaches using CryptoMiniSat as the SAT back-end for its bit-blasting engine and evaluated performance in terms of proof-production and proof-checking.", "num_citations": "4\n", "authors": ["1600"]}
{"title": "Exploring and categorizing error spaces using BMC and SMT\n", "abstract": " We describe an abstract methodology for exploring and categorizing the space of error traces for a system using a procedure based on Satisfiability Modulo Theories and Bounded Model Checking. A key component required by the technique is a way to generalize an error trace into a category of error traces. We describe tools and techniques to support a human expert in this generalization task. Finally, we report on a case study in which the methodology is applied to a simple version of the Traffic Air and Collision Avoidance System.", "num_citations": "4\n", "authors": ["1600"]}
{"title": "Towards verification of neural networks for small unmanned aircraft collision avoidance\n", "abstract": " The ACAS X family of aircraft collision avoidance systems uses large numeric lookup tables to make decisions. Recent work used a deep neural network to approximate and compress a collision avoidance table, and simulations showed that the neural network performance was comparable to the original table. Consequently, neural network representations are being explored for use on small aircraft with limited storage capacity. However, the black-box nature of deep neural networks raises safety concerns because simulation results are not exhaustive. This work takes steps towards addressing these concerns by applying formal methods to analyze the behavior of collision avoidance neural networks both in isolation and in a closed-loop system. We evaluate our approach on a specific set of collision avoidance networks and show that even though the networks are not always locally robust, their closed-loop\u00a0\u2026", "num_citations": "3\n", "authors": ["1600"]}
{"title": "A-QED verification of hardware accelerators\n", "abstract": " We present A-QED (Accelerator-Quick Error Detection), a new approach for pre-silicon formal verification of stand-alone hardware accelerators. A-QED relies on bounded model checking -- however, it does not require extensive design-specific properties or a full formal design specification. While A- QED is effective for both RTL and high-level synthesis (HLS) design flows, it integrates seamlessly with HLS flows. Our A-QED results on several hardware accelerator designs demonstrate its practicality and effectiveness: 1. A-QED detected all bugs detected by conventional verification flow. 2. A-QED detected bugs that escaped conventional verification flow. 3. A-QED improved verification productivity dramatically, by 30X, in one of our case studies (1 person-day using A-QED vs. 30 person-days using conventional verification flow). 4. A-QED produced short counterexamples for easy debug (37X shorter on average vs\u00a0\u2026", "num_citations": "3\n", "authors": ["1600"]}
{"title": "Partial Order Reduction for Deep Bug Finding in Synchronous Hardware\n", "abstract": " Symbolic model checking has become an important part of the verification flow in industrial hardware design. However, its use is still limited due to scaling issues. One way to address this is to exploit the large amounts of symmetry present in many real world designs. In this paper, we adapt partial order reduction for bounded model checking of synchronous hardware and introduce a novel technique that makes partial order reduction practical in this new domain. These approaches are largely automatic, requiring only minimal manual effort. We evaluate our technique on open-source and commercial packet mover circuits\u2013designs containing FIFOs and arbiters.", "num_citations": "3\n", "authors": ["1600"]}
{"title": "Post-silicon validation and debug using symbolic quick error detection\n", "abstract": " Disclosed are improved methods and structures for verifying integrated circuits and in particular systems-on-a-chip constructed therefrom. We call methods and structures according to the present disclosure Symbolic Quick Error Detection or Symbolic QED, Illustrative characteristics of Symbolic QED include: 1) It is applicable to any System-on-Chip (SoC) design as long as it contains at least one programmable processor; 2) It is broadly applicable for logic bugs inside processor cores, accelerators, and uncore components; 3) It does not require failure reproduction; 4) It does not require human intervention during bug localization; 5) It does not require trace buffers, 6) It does not require assertions; and 7) It uses hardware structures called \u201cchange detectors\u201d which introduce only a small area overhead. Symbolic QED exhibits: 1) A systematic (and automated) approach to inserting \u201cchange detectors\u201d during a design\u00a0\u2026", "num_citations": "3\n", "authors": ["1600"]}
{"title": "lazybv2int at the SMT Competition 2020\n", "abstract": " Conclusion. This is a prototype experimental tool that is aimed to serve as a playground for arithmetic-based techniques for bit-vector solving. Incorporating such techniques in a fullfledged solver is left for future work, and is planned for when these techniques are better understood and evaluated using this tool.Acknowledgments. We would like to thank the CVC4 and MathSAT5 teams for allowing us to use their tools. In particular, we thank Alberto Griggio for clarifying the relevant aspects of the MathSAT5 license, and to Aina Niemetz and Mathias Preiner for helpful tips regarding benchmarking and evaluating bit-vector formulas. We also thank the competition organizers, Haniel Barbosa, Jochen Hoenicke, and Antti Hyvarinen for clarifying the status of this tool for the competition and willingly accepting it as a new participant.", "num_citations": "3\n", "authors": ["1600"]}
{"title": "Symbolic Quick Error Detection for Pre-Silicon and Post-Silicon Validation: Frequently Asked Questions\n", "abstract": " Reducing the error detection latency is critical for improving the design visibility while searching for design errors. This article uses a FAQ format to discuss the key points of the symbolic QED method that can be applied during both pre-silicon and post-silicon validation.", "num_citations": "3\n", "authors": ["1600"]}