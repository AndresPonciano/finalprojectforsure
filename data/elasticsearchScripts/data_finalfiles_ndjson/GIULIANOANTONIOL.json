{"title": "Recovering traceability links between code and documentation\n", "abstract": " Software system documentation is almost always expressed informally in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs, and related maintenance reports. We propose a method based on information retrieval to recover traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts and vice-versa. We apply both a probabilistic and a vector space information retrieval model in two case studies to trace C\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1226\n", "authors": ["55"]}
{"title": "An exploratory study of the impact of antipatterns on class change-and fault-proneness\n", "abstract": " Antipatterns are poor design choices that are conjectured to make object-oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. We detect 13 antipatterns in 54 releases of ArgoUML, Eclipse, Mylyn, and Rhino, and analyse (1) to what extent classes participating in antipatterns have higher odds to change or to be subject to fault-fixing than other classes, (2) to what extent these odds (if higher) are due to the sizes of the classes or to the presence of antipatterns, and (3) what kinds of changes affect classes participating in antipatterns. We show that, in almost all releases of the four systems, classes participating in antipatterns are more change-and fault-prone than others. We also show that size alone cannot explain the higher odds of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "368\n", "authors": ["55"]}
{"title": "An empirical study of the impact of two antipatterns, blob and spaghetti code, on program comprehension\n", "abstract": " Antipatterns are \"poor\" solutions to recurring design problems which are conjectured in the literature to make object-oriented systems harder to maintain. However, little quantitative evidence exists to support this conjecture. We performed an empirical study to investigate whether the occurrence of antipatterns does indeed affect the understandability of systems by developers during comprehension and maintenance tasks. We designed and conducted three experiments, with 24 subjects each, to collect data on the performance of developers on basic tasks related to program comprehension and assessed the impact of two antipatterns and of their combinations: Blob and Spaghetti Code. We measured the developers' performance with: (1) the NASA task load index for their effort, (2) the time that they spent performing their tasks, and, (3) their percentages of correct answers. Collected data show that the occurrence of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "317\n", "authors": ["55"]}
{"title": "Demima: A multilayered approach for design pattern identification\n", "abstract": " Design patterns are important in object-oriented programming because they offer design motifs, elegant solutions to recurrent design problems, which improve the quality of software systems. Design motifs facilitate system maintenance by helping to understand design and implementation. However, after implementation, design motifs are spread throughout the source code and are thus not directly available to maintainers. We present DeMIMA, an approach to identify semi-automatically micro-architectures that are similar to design motifs in source code and to ensure the traceability of these micro-architectures between implementation and design. DeMIMA consists of three layers: two layers to recover an abstract model of the source code, including binary class relationships, and a third layer to identify design patterns in the abstract model. We apply DeMIMA to five open-source systems and, on average, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "230\n", "authors": ["55"]}
{"title": "EEG data compression techniques\n", "abstract": " Electroencephalograph (EEG) and Holter EEG data compression techniques which allow perfect reconstruction of the recorded waveform from the compressed one are presented and discussed. Data compression permits one to achieve significant reduction in the space required to store signals and in transmission time. The Huffman coding technique in conjunction with derivative computation reaches high compression ratios (on average 49% on Holter and 58% on EEG signals) with low computational complexity. By exploiting this result a simple and fast encoder/decoder scheme capable of real-time performance on a PC was implemented. This simple technique is compared with other predictive transformations, vector quantization, discrete cosine transform (DCT), and repetition count compression methods. Finally, it is shown that the adoption of a collapsed Huffman tree for the encoding/decoding operations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "230\n", "authors": ["55"]}
{"title": "Design pattern recovery in object-oriented software\n", "abstract": " An approach to recover object oriented design patterns from design and code is presented. The pattern recovery process is based on a multi-stage filtering strategy to avoid combinatorial explosion on large software systems. To maintain independence from the language and the case tools adopted in developing software, both design and code are mapped into an intermediate representation. The multi-stage searching strategy allows to safely determine pattern candidates. To assess the effectiveness of the pattern recovery process a portable environment written in Java has been developed. Based on this environment, experimental results on public domain and industrial software were obtained and are discussed in the paper. Evidence is shown that, by exploiting information about method calls as a further constraint beyond the structural ones, the number of false positives is reduced.", "num_citations": "203\n", "authors": ["55"]}
{"title": "Cerberus: Tracing requirements to source code using information retrieval, dynamic analysis, and program analysis\n", "abstract": " The concern location problem is to identify the source code within a program related to the features, requirements, or other concerns of the program. This problem is central to program development and maintenance. We present a new technique called prune dependency analysis that can be combined with existing techniques to dramatically improve the accuracy of concern location. We developed CERBERUS, a potent hybrid technique for concern location that combines information retrieval, execution tracing, and prune dependency analysis. We used CERBERUS to trace the 360 requirements of RHINO, a 32,134 line Java program that implements the ECMAScript international standard. In our experiment, prune dependency analysis boosted the recall of information retrieval by 155% and execution tracing by 104%. Moreover, we show that our combined technique outperformed the other techniques when run\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "201\n", "authors": ["55"]}
{"title": "Analyzing cloning evolution in the linux kernel\n", "abstract": " Identifying code duplication in large multi-platform software systems is a challenging problem. This is due to a variety of reasons including the presence of high-level programming languages and structures interleaved with hardware-dependent low-level resources and assembler code, the use of GUI-based configuration scripts generating commands to compile the system, and the extremely high number of possible different configurations.This paper studies the extent and the evolution of code duplications in the Linux kernel. Linux is a large, multi-platform software system; it is based on the Open Source concept, and so there are no obstacles in discussing its implementation. In addition, it is decidedly too large to be examined manually: the current Linux kernel release (2.4.18) is about three million LOCs.Nineteen releases, from 2.4.0 to 2.4.18, were processed and analyzed, identifying code duplication among\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "186\n", "authors": ["55"]}
{"title": "Aura: a hybrid approach to identify framework evolution\n", "abstract": " Software frameworks and libraries are indispensable to today's software systems. As they evolve, it is often time-consuming for developers to keep their code up-to-date, so approaches have been proposed to facilitate this. Usually, these approaches cannot automatically identify change rules for one-replaced-by-many and many-replaced-by-one methods, and they trade off recall for higher precision using one or more experimentally-evaluated thresholds. We introduce AURA, a novel hybrid approach that combines call dependency and text similarity analyses to overcome these limitations. We implement it in a Java system and compare it on five frameworks with three previous approaches by Dagenais and Robillard, M. Kim et al., and Sch\u251c\u00f1fer et al. The comparison shows that, on average, the recall of AURA is 53.07% higher while its precision is similar, eg, 0.10% lower.", "num_citations": "165\n", "authors": ["55"]}
{"title": "Object oriented design pattern inference\n", "abstract": " When designing a new application, experienced software engineers usually try to employ solutions that proved successful in previous projects. Such reuse of code organizations is seldom made explicit. Nevertheless it represents important information about the system, that can be extremely valuable in the maintenance phase by documenting the design choices underlying the implementation. In addition, having it available, it can be reused whenever a similar problem is encountered. In this paper an approach is proposed to the inference of recurrent design patterns directly from the code or the design. No assumption is made on the availability of any pattern library and the concept analysis algorithm, adapted for this purpose, is able to infer the presence of class groups which instantiate a common, repeated pattern. In fact, concept analysis provides sets of objects sharing attributes, which, in the case of object\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "157\n", "authors": ["55"]}
{"title": "Automatic mutation test input data generation via ant colony\n", "abstract": " Fault-based testing is often advocated to overcome limitations ofother testing approaches; however it is also recognized as beingexpensive. On the other hand, evolutionary algorithms have beenproved suitable for reducing the cost of data generation in the contextof coverage based testing. In this paper, we propose a newevolutionary approach based on ant colony optimization for automatictest input data generation in the context of mutation testingto reduce the cost of such a test strategy. In our approach the antcolony optimization algorithm is enhanced by a probability densityestimation technique. We compare our proposal with otherevolutionary algorithms, eg, Genetic Algorithm. Our preliminaryresults on JAVA testbeds show that our approach performed significantlybetter than other alternatives.", "num_citations": "137\n", "authors": ["55"]}
{"title": "Using metrics to identify design patterns in object-oriented software\n", "abstract": " Object-oriented design patterns are an emergent technology: they are reusable micro-architectures, high level building blocks. This paper presents a conservative approach, based on a multi-stage reduction strategy using OO software metrics and structural properties to extract structural design patterns from OO design or code. Code and design are mapped into an intermediate representation, called Abstract Object Language, to maintain independence from the programming language and the adopted CASE tools. To assess the effectiveness of the pattern recovery process a portable environment written in Java, remotely accessible by means of any Web browser, has been developed. Based on this environment, experimental results obtained on public domain and industrial software are discussed in the paper.", "num_citations": "129\n", "authors": ["55"]}
{"title": "Feature identification: An epidemiological metaphor\n", "abstract": " Feature identification is a technique to identify the source code constructs activated when exercising one of the features of a program. We propose new statistical analyses of static and dynamic data to accurately identify features in large multithreaded object-oriented programs. We draw inspiration from epidemiology to improve previous approaches to feature identification and develop an epidemiological metaphor. We build our metaphor on our previous approach to feature identification, in which we use processor emulation, knowledge-based filtering, probabilistic ranking, and metamodeling. We carry out three case studies to assess the usefulness of our metaphor, using the \"save a bookmark\" feature of Web browsers as an illustration. In the first case study, we compare our approach with three previous approaches (a naive approach, a concept analysis-based approach, and our previous probabilistic approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "127\n", "authors": ["55"]}
{"title": "Object-oriented design patterns recovery\n", "abstract": " Object-Oriented (OO) design patterns are an emergent technology: they are reusable micro-architectures, high-level building blocks. A system which has been designed using well-known, documented and accepted design patterns is also likely to exhibit good properties such as modularity, separation of concerns and maintainability. While for forward engineering the benefits of using design patterns are clear, using reverse engineering technologies to discover instances of patterns in a software artifact (e.g., design or code) may help in several key areas, among which are program understanding, design-to-code traceability and quality assessment. This paper describes a conservative approach and experimental results, based on a multi-stage reduction strategy using OO software metrics and structural properties to extract structural design patterns from OO design or C++ code. To assess the effectiveness of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["55"]}
{"title": "An automatic approach to identify class evolution discontinuities\n", "abstract": " When a software system evolves, features are added, removed and changed. Moreover, refactoring activities are periodically performed to improve the software internal structure. A class may be replaced by another, two classes can be merged, or a class may be split in two others. As a consequence, it may not be possible to trace software features between a release and another. When studying software evolution, we should be able to trace a class lifetime even when it disappears because it is replaced by a similar one, split or merged. Such a capability is also essential to perform impact analysis. This work proposes an automatic approach, inspired on vector space information retrieval, to identify class evolution discontinuities and, therefore, cases of possible refactoring. The approach has been applied to identify refactorings performed over 40 releases of a Java open source domain name server. Almost all the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "122\n", "authors": ["55"]}
{"title": "Can better identifier splitting techniques help feature location?\n", "abstract": " The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "115\n", "authors": ["55"]}
{"title": "Trustrace: Mining software repositories to improve the accuracy of requirement traceability links\n", "abstract": " Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "113\n", "authors": ["55"]}
{"title": "Code siblings: Technical and legal implications of copying code between applications\n", "abstract": " Source code cloning does not happen within a single system only. It can also occur between one system and another. We use the term code sibling to refer to a code clone that evolves in a different system than the code from which it originates. Code siblings can only occur when the source code copyright owner allows it and when the conditions imposed by such license are not incompatible with the license of the destination system. In some situations copying of source code fragments are allowed - legally - in one direction, but not in the other. In this paper, we use clone detection, license mining and classification, and change history techniques to understand how code siblings - under different licenses - flow in one direction or the other between Linux and two BSD Unixes, FreeBSD and OpenBSD. Our results show that, in most cases, this migration appears to happen according to the terms of the license of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "110\n", "authors": ["55"]}
{"title": "Modeling clones evolution through time series\n", "abstract": " The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive model. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2. 0. 10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4 %. A higher prediction error\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "99\n", "authors": ["55"]}
{"title": "A novel approach to optimize clone refactoring activity\n", "abstract": " Software evolution and software quality are ever changing phenomena. As software evolves, evolution impacts software quality. On the other hand, software quality needs may drive software evolution strategies. This paper presents an approach to schedule quality improvement under constraints and priority. The general problem of scheduling quality improvement has been instantiated into the concrete problem of planning duplicated code removal in a geographical information system developed in C throughout the last 20 years. Priority and constraints arise from development team and from the adopted development process. The developer team long term goal is to get rid of duplicated code, improve software structure, decrease coupling, and improve cohesion. We present our problem formulation, the adopted approach, including a model of clone removal effort and preliminary results obtained on a real world\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["55"]}
{"title": "Quantitative founder-effect analysis of French Canadian families identifies specific loci contributing to metabolic phenotypes of hypertension\n", "abstract": " The Saguenay\u0393\u00c7\u00f4Lac St-Jean population of Quebec is relatively isolated and has genealogical records dating to the 17th-century French founders. In 120 extended families with at least one sib pair affected with early-onset hypertension and/or dyslipidemia, we analyzed the genetic determinants of hypertension and related cardiovascular and metabolic conditions. Variance-components linkage analysis revealed 46 loci after 100,000 permutations. The most prominent clusters of overlapping quantitative-trait loci were on chromosomes 1 and 3, a finding supported by principal-components and bivariate analyses. These genetic determinants were further tested by classifying families by use of LOD score density analysis for each measured phenotype at every 5 cM. Our study showed the founder effect over several generations and classes of living individuals. This quantitative genealogical approach supports the notion\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "95\n", "authors": ["55"]}
{"title": "Design\u0393\u00c7\u00c9code traceability for object\u0393\u00c7\u00c9oriented systems\n", "abstract": " Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However, few works have so far addressed the theme of tracing object oriented (OO) design into its implementation and evolving it. This paper presents an approach to checking the compliance of OO design with respect to source code and support its evolution. The process works on design artifacts expressed in the OMT (Object Modeling Technique) notation and accepts C++ source code. It recovers an \u0393\u00c7\u00a3as is\u0393\u00c7\u00a5 design from the code, compares the recovered design with the actual design and helps the user to deal with inconsistencies. The recovery process exploits the edit distance computation and the maximum match algorithm to determine traceability links between design and code. The output is a similarity measure associated to design\u0393\u00c7\u00c9code class pairs, which can be classified as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["55"]}
{"title": "Definition and experimental evaluation of function points for object-oriented systems\n", "abstract": " We present a method for estimating the size, and consequently effort and duration, of object oriented software development projects. Different estimates may be made in different phases of the development process, according to the available information. We define an adaptation of traditional function points, called Object Oriented Function Points, to enable the measurement of object oriented analysis and design specifications. Tools have been constructed to automate the counting method. The novel aspect of our method is its flexibility. An organisation can experiment with different counting policies, to find the most accurate predictors of size, effort, etc. in its environment. The method and preliminary results of its application in an industrial environment are presented and discussed.", "num_citations": "91\n", "authors": ["55"]}
{"title": "Detecting buffer overflow via automatic test input data generation\n", "abstract": " Buffer overflows cause serious problems in various categories of software systems. In critical systems, such as health-care, nuclear or aerospace software applications, a buffer overflow may cause severe threats to humans or severe economic losses. If they occur in network or security applications, they can be exploited to gain administrator privileges, perform system attacks, access unauthorized data, or misuse the system. This paper proposes a combination of genetic algorithms, linear programming, evolutionary testing, and static and dynamic information to detect buffer overflows. The newly proposed test input generation process avoids the need for human intervention to define and tune genetic algorithm weights and therefore it becomes completely automated. The process that guides the genetic search towards the detection of buffer overflow relies on a fitness function that takes into account static and dynamic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["55"]}
{"title": "Recovering code to documentation links in OO systems\n", "abstract": " Software system documentation is almost always expressed informally, in natural language and free text. Examples include requirement specifications, design documents, user manual pages, system development journals, error logs and related maintenance reports. We propose an approach to establish and maintain traceability links between the source code and free-text documents. A premise of our work is that programmers use meaningful names for program's items, such as functions, variables, types, classes and methods. We believe that the application domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts, and vice versa. In this paper, the approach is applied to software written in an object-oriented (OO) language, namely C++, to trace\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["55"]}
{"title": "An empirical study of the relationships between design pattern roles and class change proneness\n", "abstract": " Analyzing the change-proneness of design patterns and the kinds of changes occurring to classes playing role(s) in some design pattern(s) during software evolution poses the basis for guidelines to help developers who have to choose, apply or maintain design patterns. Building on previous work, this paper shifts the focus from design patterns as wholes to the finer-grain level of design pattern roles. The paper presents an empirical study to understand whether there are roles that are more change-prone than others and whether there are changes that are more likely to occur to certain roles. The study relies on data extracted from the source code repositories of three different systems (JHotDraw, Xerces, and Eclipse-JDT) and from 12 design patterns. Results obtained confirm the intuitive behavior about changeability of many roles in design motifs, but also warns about properly designing parts of the motif subject\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["55"]}
{"title": "An approach for reverse engineering of web-based applications\n", "abstract": " The new possibilities offered by WEB applications are pervasively and radically changing several areas. WEB applications, compared to WEB sites, offer substantially greater opportunities: a WEB application provides the WEB user with a means to modify the site status. WEB applications must cope with an extremely short development/evolution life cycle. Usually, they are implemented without producing any useful documentation for subsequent maintenance and evolution, thus compromising the desired high level of flexibility, maintainability, and adaptability that is de-facto necessary to compete and survive to market shakeout. This paper presents an approach inspired by the reverse engineering arena and a tool prototype supporting WEB application reverse engineering activities, to help maintain, comprehend and evolve WEB applications. The approach defines a set of abstract views, modeled using UML\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["55"]}
{"title": "A function point-like measure for object-oriented software\n", "abstract": " We present a method for estimating the size, and consequently effort and duration, of object oriented software development projects. Different estimates may be made in different phases of the development process, according to the available information. We define an adaptation of traditional function points, called \u0393\u00c7\u00a3Object Oriented Function Points\u0393\u00c7\u00a5, to enable the measurement of object oriented analysis and design specifications. Tools have been constructed to automate the counting method. The novel aspect of our method is its flexibility. An organization can experiment with different counting policies, to find the most accurate predictors of size, effort, etc. in its environment. The method and preliminary results of its application in an industrial environment are presented and discussed.", "num_citations": "82\n", "authors": ["55"]}
{"title": "Support vector machines for anti-pattern detection\n", "abstract": " Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and--or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a whole software system may be infeasible because of the required parsing time and of the subsequent needed manual validation. Detecting anti-patterns on subsets of a system could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["55"]}
{"title": "Assessing staffing needs for a software maintenance project through queuing simulation\n", "abstract": " We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["55"]}
{"title": "Linguistic antipatterns: What they are and how developers perceive them\n", "abstract": " Antipatterns are known as poor solutions to recurring problems. For example, Brown et al. and Fowler define practices concerning poor design or implementation solutions. However, we know that the source code lexicon is part of the factors that affect the psychological complexity of a program, i.e., factors that make a program difficult to understand and maintain by humans. The aim of this work is to identify recurring poor practices related to inconsistencies among the naming, documentation, and implementation of an entity\u0393\u00c7\u00f6called Linguistic Antipatterns (LAs)\u0393\u00c7\u00f6that may impair program understanding. To this end, we first mine examples of such inconsistencies in real open-source projects and abstract them into a catalog of 17 recurring LAs related to methods and attributes. Then, to understand the relevancy of LAs, we perform two empirical studies with developers\u0393\u00c7\u00f630 external (i.e., not familiar with the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["55"]}
{"title": "Recognizing words from source code identifiers using speech recognition techniques\n", "abstract": " The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Researchers have noticed that identifiers are one of the most important source of information about program entities and that the semantic of identifiers guide the cognitive process. Recognizing the words forming identifiers is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and-or when these words have been abbreviated or otherwise transformed. This paper proposes a technique inspired from speech recognition, i.e., dynamic time warping, to split identifiers into component words. The proposed technique has been applied to identifiers extracted from two different applications: JHotDraw and Lynx. Results compared to manually-built oracles and with Camel Case algorithm are encouraging. In fact, they show that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "75\n", "authors": ["55"]}
{"title": "Understanding web applications through dynamic analysis\n", "abstract": " The relevance and pervasiveness of Web applications as a vital part of modern enterprise systems has significantly increased in recent years. However, the lack of adequate documentation promotes the need for reverse engineering tools aiming at supporting Web application maintenance and evolution tasks. A nontrivial Web application is a complex artifact integrating technologies such as scripting languages, middleware, Web services, data warehouses and databases. The task to recover abstractions requires the adoption of dynamic analyses to complement the information gathered with static analyses. This paper presents an approach and a tool, named WANDA, that instruments Web applications and combines static and dynamic information to recover the as-is architecture and, in general, the UML documentation of the application itself. To this aim we propose an extension of the Conallen UML diagrams to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["55"]}
{"title": "Repent: Analyzing the nature of identifier renamings\n", "abstract": " Source code lexicon plays a paramount role in software quality: poor lexicon can lead to poor comprehensibility and even increase software fault-proneness. For this reason, renaming a program entity, i.e., altering the entity identifier, is an important activity during software evolution. Developers rename when they feel that the name of an entity is not (anymore) consistent with its functionality, or when such a name may be misleading. A survey that we performed with 71 developers suggests that 39 percent perform renaming from a few times per week to almost every day and that 92 percent of the participants consider that renaming is not straightforward. However, despite the cost that is associated with renaming, renamings are seldom if ever documented-for example, less than 1 percent of the renamings in the five programs that we studied. This explains why participants largely agree on the usefulness of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["55"]}
{"title": "Maintaining traceability links during object\u0393\u00c7\u00c9oriented software evolution\n", "abstract": " This paper presents a method to build and maintain traceability links and properties of a set of object\u0393\u00c7\u00c9oriented software releases. The method recovers an \u0393\u00c7\u00ffas is\u0393\u00c7\u00d6 design from C++ software releases, compares recovered designs at the class interface level, and helps the user to deal with inconsistencies by pointing out regions of code where differences are concentrated. The comparison step exploits edit distance and a maximum match algorithm. The method has been experimented with on two freely available C++ systems. Results as well as examples of applications to the visualization of the traceability information and to the estimation of the size of changes during maintenance are reported in the paper. Copyright \u252c\u2310 2001 John Wiley & Sons, Ltd.", "num_citations": "71\n", "authors": ["55"]}
{"title": "Playing with refactoring: Identifying extract class opportunities through game theory\n", "abstract": " In software engineering, developers must often find solutions to problems balancing competing goals, e.g., quality versus cost, time to market versus resources, or cohesion versus coupling. Finding a suitable balance between contrasting goals is often complex and recommendation systems are useful to support developers and managers in performing such a complex task. We believe that contrasting goals can be often dealt with game theory techniques. Indeed, game theory is successfully used in other fields, especially in economics, to mathematically propose solutions to strategic situation, in which an individual's success in making choices depends on the choices of others. To demonstrate the applicability of game theory to software engineering and to understand its pros and cons, we propose an approach based on game theory that recommend extract-class refactoring opportunities. A preliminary evaluation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["55"]}
{"title": "Improving network applications security: a new heuristic to generate stress testing data\n", "abstract": " Buffer overflows cause serious problems in different categories of software systems. For example, if present in network or security applications, they can be exploited to gain unauthorized grant or access to the system. In embedded systems, such as avionics or automotive systems, they can be the cause of serious accidents. This paper proposes to combine static analysis and program slicing with evolutionary testing, to detect buffer overflow threats. Static analysis identifies vulnerable statements, while slicing and data dependency analysis identify the relationship between these statements and program or function inputs, thus reducing the search space. To guide the search towards discovering buffer overflow in this work we define three multi-objective fitness functions and compare them on two open-source systems. These functions account for terms such as the statement coverage, the coverage of vulnerable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["55"]}
{"title": "A new family of software anti-patterns: Linguistic anti-patterns\n", "abstract": " Recent and past studies have shown that poor source code lexicon negatively affects software understand ability, maintainability, and, overall, quality. Besides a poor usage of lexicon and documentation, sometimes a software artifact description is misleading with respect to its implementation. Consequently, developers will spend more time and effort when understanding these software artifacts, or even make wrong assumptions when they use them. This paper introduces the definition of software linguistic antipatterns, and defines a family of them, i.e., those related to inconsistencies (i) between method signatures, documentation, and behavior and (ii) between attribute names, types, and comments. Whereas \"design\" antipatterns represent recurring, poor design choices, linguistic antipatterns represent recurring, poor naming and commenting choices. The paper provides a first catalogue of one family of linguistic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["55"]}
{"title": "Design evolution metrics for defect prediction in object oriented systems\n", "abstract": " Testing is the most widely adopted practice to ensure software quality. However, this activity is often a compromise between the available resources and software quality. In object-oriented development, testing effort should be focused on defective classes. Unfortunately, identifying those classes is a challenging and difficult activity on which many metrics, techniques, and models have been tried. In this paper, we investigate the usefulness of elementary design evolution metrics to identify defective classes. The metrics include the numbers of added, deleted, and modified attributes, methods, and relations. The metrics are used to recommend a ranked list of classes likely to contain defects for a system. They are compared to Chidamber and Kemerer\u0393\u00c7\u00d6s metrics on several versions of Rhino and of ArgoUML. Further comparison is conducted with the complexity metrics computed by Zimmermann et al. on several\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["55"]}
{"title": "Mining the lexicon used by programmers during sofware evolution\n", "abstract": " Identifiers represent an important source of information for programmers understanding and maintaining a system. Self-documenting identifiers reduce the time and effort necessary to obtain the level of understanding appropriate for the task at hand. While the role of the lexicon in program comprehension has long been recognized, only a few works have studied the quality and enhancement of the identifiers and no works have studied the evolution of the lexicon. In this paper, we characterize the evolution of program identifiers in terms of stability metrics and occurrences of renaming. We assess whether an evolution process similar to the one occurring for the program structure exists for identifiers. We report data and results about the evolution of three large systems, for which several releases are available. We have found evidence that the evolution of the lexicon is more limited and constrained than the evolution\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["55"]}
{"title": "Tracing object-oriented code into functional requirements\n", "abstract": " Software system documentation is almost always expressed informally, in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs and related maintenance reports. We propose an approach to establish and maintain traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high level concepts with program concepts, and vice-versa. The approach is applied to software written in an object oriented language, namely Java, to trace classes to functional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["55"]}
{"title": "An empirical study on the efficiency of graphical vs. textual representations in requirements comprehension\n", "abstract": " Graphical representations are used to visualise, specify, and document software artifacts in all stages of software development process. In contrast with text, graphical representations are presented in two-dimensional form, which seems easy to process. However, few empirical studies investigated the efficiency of graphical representations vs. textual ones in modelling and presenting software requirements. Therefore, in this paper, we report the results of an eye-tracking experiment involving 28 participants to study the impact of structured textual vs. graphical representations on subjects' efficiency while performing requirement comprehension tasks. We measure subjects' efficiency in terms of the percentage of correct answers (accuracy) and of the time and effort spend to perform the tasks. We observe no statistically-significant difference in term of accuracy. However, our subjects spent more time and effort while\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["55"]}
{"title": "Object-oriented function points: An empirical validation\n", "abstract": " We present an empirical validation of object-oriented size estimation models. In previous work we proposed object oriented function points (OOFP), an adaptation of the function points approach to object-oriented systems. In a small pilot study, we used the OOFP method to estimate lines of code (LOC). In this paper we extend the empirical validation of OOFP substantially, using a larger data set and comparing OOFP with alternative predictors of LOC. The aim of the paper is to gain an understanding of which factors contribute to accurate size prediction for OO software, and to position OOFP within that knowledge. A cross validation approach was adopted to build and evaluate linear models where the independent variable was either a traditional OO entity (classes, methods, association, inheritance, or a combination of them) or an OOFP-related measure. Using the full OOFP process, the best size predictor\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["55"]}
{"title": "Traceability recovery by modeling programmer behavior\n", "abstract": " When a system evolves, while the source code is changed, documentation and traceability links are barely ever updated: maintaining traceability information between software artifacts is a costly and tedious activity, which is frequently sacrificed during development and maintenance due to market pressure. The paper presents a new method to recovery traceability links between high level and low level artifacts. The method is based on the partial knowledge of a subset of tracability links. It can be fully automated and the human intervention is only required to confirm or confute recovered traceability links. The method has been applied to software written in Java, to trace classes onto functional requirements. Experimental results demonstrate the superiority of the novel method over previously published results on the same system.", "num_citations": "67\n", "authors": ["55"]}
{"title": "Tidier: an identifier splitting approach using speech recognition techniques\n", "abstract": " The software engineering literature reports empirical evidence on the relation between various characteristics of a software system and its quality. Among other factors, recent studies have shown that a proper choice of identifiers influences understandability and maintainability. Indeed, identifiers are developers' main source of information and guide their cognitive processes during program comprehension when high\u0393\u00c7\u00c9level documentation is scarce or outdated and when source code is not sufficiently commented. This paper proposes a novel approach to recognize words composing source code identifiers. The approach is based on an adaptation of Dynamic Time Warping used to recognize words in continuous speech. The approach overcomes the limitations of existing identifier\u0393\u00c7\u00c9splitting approaches when naming conventions (e.g., Camel Case) are not used or when identifiers contain abbreviations. We apply the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["55"]}
{"title": "A deformable grid-matching approach for microarray images\n", "abstract": " A fundamental step of microarray image analysis is the detection of the grid structure for the accurate location of each spot, representing the state of a given gene in a particular experimental condition. This step is known as gridding and belongs to the class of deformable grid matching problems which are well known in literature. Most of the available microarray gridding approaches require human intervention; for example, to specify landmarks, some points in the spot grid, or even to precisely locate individual spots. Automating this part of the process can allow high throughput analysis. This paper focuses on the development of a fully automated procedure for the problem of automatic microarray gridding. It is grounded on the Bayesian paradigm and on image analysis techniques. The procedure has two main steps. The first step, based on the Radon transform, is aimed at generating a grid hypothesis; the second\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["55"]}
{"title": "Language model representations for beam-search decoding\n", "abstract": " This paper presents an efficient way of representing a bigram language model for a beam-search based, continuous speech, large vocabulary HMM recognizer. The tree-based topology considered takes advantage of a factorization of the bigram probability derived from the bigram interpolation scheme, and of a tree organization of all the words that can follow a given one. Moreover, an optimization algorithm is used to considerably reduce the space requirements of the language model. Experimental results are provided for two 10,000-word dictation tasks: radiological reporting (perplexity 27) and newspaper dictation (perplexity 120). In the former domain 93% word accuracy is achieved with real-time response and 23 Mb process space. In the newspaper dictation domain, 88.1% word accuracy is achieved with 1.41 real-time response and 38 Mb process space. All recognition tests were performed on an HP-735\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["55"]}
{"title": "MC/DC automatic test input data generation\n", "abstract": " In regulated domain such as aerospace and in safety critical domains, software quality assurance is subject to strict regulation such as the RTCA DO-178B standard. Among other conditions, the DO-178B mandates for the satisfaction of the modified condition/decision coverage (MC/DC) testing criterion for software where failure condition may have catastrophic consequences. MC/DC is a white box testing criterion aiming at proving that all conditions involved in a predicate can influence the predicate value in the desired way. In this paper, we propose a novel fitness function inspired by chaining test data generation to efficiently generate test input data satisfying the MC/DC criterion. Preliminary results show the superiority of the novel fitness function that is able to avoid plateau leading to a behavior close to random test of traditional white box fitness functions.", "num_citations": "63\n", "authors": ["55"]}
{"title": "Stack overflow: a code laundering platform?\n", "abstract": " Developers use Question and Answer (Q&A) websites to exchange knowledge and expertise. Stack Overflow is a popular Q&A website where developers discuss coding problems and share code examples. Although all Stack Overflow posts are free to access, code examples on Stack Overflow are governed by the Creative Commons Attribute-ShareAlike 3.0 Unported license that developers should obey when reusing code from Stack Overflow or posting code to Stack Overflow. In this paper, we conduct a case study with 399 Android apps, to investigate whether developers respect license terms when reusing code from Stack Overflow posts (and the other way around). We found 232 code snippets in 62 Android apps from our dataset that were potentially reused from Stack Overflow, and 1,226 Stack Overflow posts containing code examples that are clones of code released in 68 Android apps, suggesting that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["55"]}
{"title": "Web site reengineering using RMM\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - Web Site Reengineering Using RMM CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa ] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556 \u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c0\u00fc\u00ab\u03c0\u00e9\u2561\u03c0\u00e2\u255d\u03c0\u00e2\u00f4\u03c0\u00e9\u2563\u03c0\u00fc\u00bd\u0398\u00fb\u00f3\u03c0\u00fc\u00d6\u03c0\u00e9\u00ef\u03c0\u00e9\u00f3\u03c0\u00e2\u2502\u03c0\u00e9\u2592\u03c0\u00e2\u255d\u03c0\u00e2\u00ea \u03c0\u00e9\u00c6\u03c3\u00ab\u0192\u00b5\u00fb\u255c\u03a3\u2555\u00a1\u03c0\u00fc\u00ba\u03c0\u00fc\u00d6\u2229\u255d\u00ea11/11(\u00b5\u2591\u2524)-12/23(\u00b5\u2591\u2524)\u2229\u255d\u00eb Web Site Reengineering Using RMM ANTONIOL G. \u03a6\u00f3\u00bd \u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 ANTONIOL G. \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 Proc. 2nd International Workshop on Web Site Evolution, 2000 Proc. 2nd International Workshop on Web Site Evolution, 2000, 2000 \u03a6\u00f3\u00bd \u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u03c3\u00ef\u00f2\u03c4\u00dc\u00e4\u03a6\u00ba\u00fa\u00b5\u20a7\u00c9\u03c0\u00fc\u00bd\u03c0\u00e9\u00ea\u03c0\u00e9\u00efWeb\u03c0\u00e9\u00f3\u03c0\u00e2\u00f9\u03c0\u00e2\u00ac\u03c0\u00e9\u2592\u03c0\u00e2\u255d\u03c0\u00e9\u2556\u03c0\u00e2\u00ba\u03c0\u00e2\u2502\u03c0\u00e2\u2557\u03c0\u00e2\u00f3\u03c0\u00e2\u00e7\u03c0\u00e2\u00bd\u00b5\u00e8\u255c\u03c3\u00e7\u2551\u00b5\u00f6\u00bb\u00b5\u00c5\u2524\u00b5\u00eb\u00ef\u00b5\u2502\u00f2 \u03c3\u00ab\u00eb\u0398\u00e2\u00bf \u0398\u2551\u2557\u0398\u00e7\u00ee , \u03c4\u00aa\u00c5\u03c4\u00f6\u2591 \u03c3\u00fc\u00d1\u03c3\u00f1\u00ac\u0398\u00e2\u00c4 , \u03c3\u00e1\u00c7 \u0398\u00a2\u00e0\u00b5\u2524\u00ef , \u03c4\u00f6\u2591\u03a3\u2551\u00f2 \u03c4\u00ba\u00c7\u00b5\u00bf\u2563 , \u00b5\u00e1\u2563\u03a6\u2556\u00bb\u0398\u00e8\u00ff \u03c3\u2524\u00e7 , \u03c3\u2591\u00c5\u0398\u00e7\u00c4 \u03c3\u2551\u2556\u03a3\u2555\u00c7 , \u03c3\u00f1\u00ba\u0398\u00e7\u00c4 \u03c4\u255b\u2310\u03c3\u00f1\u00bd \u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00e7\u00aa\u03c4\u00c9\u00e5\u03c3\u00a1\u00aa\u03a3\u255d\u00dc\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03a6\u00ac\u00ee 46(3), 683-693, 2005-03-15 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab22\u03a3\u2557\u2562 CiNii\u03c3\u00ea\u2310\u03c4\u00f6\u00bf\u03a6\u00c7\u00e0\u03c0\u00e9\u00f3\u03c0\u00e2\u2502\u03c0\u00e9\u2592\u03c0\u00e2\u255d\u03c0\u00e2\u00ea Tweet \u03c3\u00c9\u00e4\u03c4\u00bf\u00ab\u03c0\u00e9\u2502\u03c0\u00e2\u255d\u03c0\u00e2\u00eb NII\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7ID/\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["55"]}
{"title": "Language modelling for efficient beam-search\n", "abstract": " This paper considers the problems of estimating bigram language models and of efficiently representing them by a finite state network, which can be employed by an hidden Markov model based, beam-search, continuous speech recognizer.A review of the best known bigram estimation techniques is given together with a description of the original Stacked model. Language model comparisons in terms of perplexity are given for three text corpora with different data sparseness conditions, while speech recognition accuracy tests are presented for a 10,000-word real-time, speaker independent dictation task. The Stacked estimation method compares favorably with the others, by achieving about 93% of word accuracy. If better language model estimates can improve recognition accuracy, representations better suited to the search algorithm can improve its speed as well. Two static representations of language models are introduced: linear and tree-based. Results show that the latter organization is better exploited by the beam-search algorithm as it provides a 5 times faster response with same word accuracy. Finally, an off-line reduction algorithm is presented that cuts the space requirements of the tree-based topology to about 40%.", "num_citations": "61\n", "authors": ["55"]}
{"title": "Simulated annealing for improving software quality prediction\n", "abstract": " In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions. We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["55"]}
{"title": "Identifying design-code inconsistencies in object-oriented software: a case study\n", "abstract": " Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However few works have addressed the theme of tracing object oriented design into its software. This paper presents an approach to check the compliance of OO design with respect to source code. The process works on design artefacts expressed in OMT notation and accepts C++ source code. It recovers an \"as is\" design from the code, compares recovered design with the actual design and helps the user to deal with inconsistency by pointing out regions of code which do not match with design. The recovery process exploits regular expression and edit distance to bridge the gap between code and design. Results as well as consideration related to presentation issues are reported in the paper.", "num_citations": "60\n", "authors": ["55"]}
{"title": "Design-code traceability recovery: selecting the basic linkage properties\n", "abstract": " Traceability ensures that software artifacts of subsequent phases of the development cycle are consistent. Few works have so far addressed the problem of automatically recovering traceability links between object-oriented (OO) design and code entities. Such a recovery process is required whenever there is no explicit support of traceability from the development process. The recovered information can drive the evolution of the available design so that it corresponds to the code, thus providing a still useful and updated high-level view of the system. Automatic recovery of traceability links can be achieved by determining the similarity of paired elements from design and code. The choice of the properties involved in the similarity computation is crucial for the success of the recovery process. In fact, design and code objects are complex artifacts with several properties attached. The basic anchors of the recovered\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["55"]}
{"title": "Linear predictive coding and cepstrum coefficients for mining time variant information from software repositories\n", "abstract": " This paper presents an approach to recover time variant information from software repositories. It is widely accepted that software evolves due to factors such as defect removal, market opportunity or adding new features. Software evolution details are stored in software repositories which often contain the changes history. On the other hand there is a lack of approaches, technologies and methods to efficiently extract and represent time dependent information. Disciplines such as signal and image processing or speech recognition adopt frequency domain representations to mitigate differences of signals evolving in time. Inspired by time-frequency duality, this paper proposes the use of Linear Predictive Coding (LPC) and Cepstrum coefficients to model time varying software artifact histories. LPC or Cepstrum allow obtaining very compact representations with linear complexity. These representations can be used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["55"]}
{"title": "Would static analysis tools help developers with code reviews?\n", "abstract": " Code reviews have been conducted since decades in software projects, with the aim of improving code quality from many different points of view. During code reviews, developers are supported by checklists, coding standards and, possibly, by various kinds of static analysis tools. This paper investigates whether warnings highlighted by static analysis tools are taken care of during code reviews and, whether there are kinds of warnings that tend to be removed more than others. Results of a study conducted by mining the Gerrit repository of six Java open source projects indicate that the density of warnings only slightly vary after each review. The overall percentage of warnings removed during reviews is slightly higher than what previous studies found for the overall project evolution history. However, when looking (quantitatively and qualitatively) at specific categories of warnings, we found that during code reviews\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["55"]}
{"title": "Earmo: An energy-aware refactoring approach for mobile apps\n", "abstract": " The energy consumption of mobile apps is a trending topic and researchers are actively investigating the role of coding practices on energy consumption. Recent studies suggest that design choices can conflict with energy consumption. Therefore, it is important to take into account energy consumption when evolving the design of a mobile app. In this paper, we analyze the impact of eight type of anti-patterns on a testbed of 20 android apps extracted from F-Droid. We propose EARMO, a novel anti-pattern correction approach that accounts for energy consumption when refactoring mobile anti-patterns. We evaluate EARMO using three multiobjective search-based algorithms. The obtained results show that EARMO can generate refactoring recommendations in less than a minute, and remove a median of 84 percent of anti-patterns. Moreover, EARMO extended the battery life of a mobile phone by up to 29 minutes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["55"]}
{"title": "Effectiveness of fibrin glue in conjunction with collagen patches to reduce seroma formation after axillary lymphadenectomy for breast cancer\n", "abstract": " BackgroundAxillary lymphadenectomy remains an integral part of breast cancer treatment, yet seroma formation occurs in 15% to 85% of cases. Among methods employed to reduce seroma magnitude and duration, fibrin glue has been proposed in numerous studies, with controversial results.MethodsFifty patients underwent quadrantectomy or mastectomy with level I/II axillary lymphadenectomy; a suction drain was fitted in all patients. Fibrin glue spray and a collagen patch were applied to the axillary fossa in 25 patients; the other 25 patients were treated conventionally.ResultsSuction drainage was removed between postoperative days 3 and 4. Seroma magnitude and duration were significantly reduced (P = .004 and .02, respectively) and there were fewer evacuative punctures in patients receiving fibrin glue and collagen patches compared with the conventional treatment group.ConclusionsUse of fibrin glue\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["55"]}
{"title": "How developers' collaborations identified from different sources tell us about code changes\n", "abstract": " Written communications recorded through channels such as mailing lists or issue trackers, but also code co-changes, have been used to identify emerging collaborations in software projects. Also, such data has been used to identify the relation between developers' roles in communication networks and source code changes, or to identify mentors aiding newcomers to evolve the software project. However, results of such analyses may be different depending on the communication channel being mined. This paper investigates how collaboration links vary and complement each other when they are identified through data from three different kinds of communication channels, i.e., mailing lists, issue trackers, and IRC chat logs. Also, the study investigates how such links overlap with links mined from code changes, and how the use of different sources would influence (i) the identification of project mentors, and (ii) the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["55"]}
{"title": "Augmenting pattern-based architectural recovery with flow analysis: Mosaic-a case study\n", "abstract": " Understanding the overall organization of a software system, i.e. its software architecture, is often required during software maintenance: tools can help maintainers in managing the evolution of legacy systems, by showing them architectural information. The analysis of a medium-sized application using a pattern based architectural recovery environment is presented. The results obtained give useful information about the system architecture but also show some limitations of a purely pattern based approach. To overcome such limitations, architectural analysis algorithms have been augmented with information about control and data flow and the case study application has been re-analyzed. Complementing pattern matching with flow information has also allowed detection of architectural constructs when they are spread over different procedures in source code and to extract useful additional information through the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["55"]}
{"title": "Women and men\u0393\u00c7\u00f6different but equal: On the impact of identifier style on source code reading\n", "abstract": " Program comprehension is preliminary to any program evolution task. Researchers agree that identifiers play an important role in code reading and program understanding activities. Yet, to the best of our knowledge, only one work investigated the impact of gender on the memorability of identifiers and thus, ultimately, on program comprehension. This paper reports the results of an experiment involving 15 male subjects and nine female subjects to study the impact of gender on the subjects' visual effort, required time, as well as accuracy to recall Camel Case versus Underscore identifiers in source code reading. We observe no statistically-significant difference in term of accuracy, required time, and effort. However, our data supports the conjecture that male and female subjects follow different comprehension strategies: female subjects seem to carefully weight all options and spend more time to rule out wrong\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["55"]}
{"title": "Preventing seroma formation after axillary dissection for breast cancer: a randomized clinical trial\n", "abstract": " BackgroundSeroma formation after axillary dissection remains the most common early sequel to breast cancer surgery. Different surgical approaches have been performed to reduce seroma collection. Therefore, we aimed to assess the outcome of patients operated on using an ultrasound scalpel according to a standardized operative technique before accepting it as a routine procedure.MethodsA randomized controlled trial was designed to compare the outcome of patients undergoing breast surgery and axillary dissection using either standard scalpel blades, scissors, ligations, and electrocautery or the ultrasound scalpel only. Each arm of the trial consisted of 30 patients.ResultsA statistically significant benefit in terms of axillary and chest wall drainage volume, the number of axilla seromas, intraoperative bleeding, and hospitalization stay was recorded in the harmonic scalpel group. No significant differences\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["55"]}
{"title": "A language-independent software renovation framework\n", "abstract": " One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today\u0393\u00c7\u00d6s software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary.We propose a Software Renovation Framework (SRF) and a toolkit covering several aspects of software renovation, such as removing unused objects and code clones, and refactoring existing libraries into smaller more cohesive ones. Refactoring has been implemented in the SRF using a hybrid approach based on hierarchical clustering, on genetic algorithms and hill climbing, also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "45\n", "authors": ["55"]}
{"title": "Trust-based requirements traceability\n", "abstract": " Information retrieval (IR) approaches have proven useful in recovering traceability links between free text documentation and source code. IR-based traceability recovery approaches produce ranked lists of traceability links between pieces of documentation and source code. These traceability links are then pruned using various strategies and, finally, validated by human experts. In this paper we propose two contributions to improve the precision and recall of traceability links and, thus, reduces the required human experts' manual validation effort. First, we propose a novel approach, Trustrace, inspired by Web trust models to improve the precision and recall of traceability links: Trustrace uses any traceability recovery approach to obtain a set of traceability links, which rankings are then re-evaluated using a set of other traceability recovery approaches. Second, we propose a novel traceability recovery approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["55"]}
{"title": "Inference of object\u0393\u00c7\u00c9oriented design patterns\n", "abstract": " When designing a new application, experienced software engineers usually adopt solutions that have proven successful in previous projects. Such reuse of code organizations is seldom made explicit. Nevertheless, it represents important information, which can be extremely valuable in the maintenance phase by documenting the design choices underlying the implementation. In addition it can be reused whenever a similar problem is encountered. In this paper an approach for the inference of recurrent design patterns directly from the code is proposed. No assumption is made on the availability of any pattern library, and the concept analysis algorithm\u0393\u00c7\u00f6adapted for this purpose\u0393\u00c7\u00f6is able to infer the presence of class groups which instantiate a common, repeated pattern. In fact, concept analysis provides sets of objects sharing attributes, which\u0393\u00c7\u00f6in the case of object\u0393\u00c7\u00c9oriented design patterns\u0393\u00c7\u00f6become class\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["55"]}
{"title": "Taupe: Visualizing and analyzing eye-tracking data\n", "abstract": " Program comprehension is an essential part of any maintenance activity. It allows developers to build mental models of the program before undertaking any change. It has been studied by the research community for many years with the aim to devise models and tools to understand and ease this activity. Recently, researchers have introduced the use of eye-tracking devices to gather and analyze data about the developers\u0393\u00c7\u00d6 cognitive processes during program comprehension. However, eye-tracking devices are not completely reliable and, thus, recorded data sometimes must be processed, filtered, or corrected. Moreover, the analysis software tools packaged with eye-tracking devices are not open-source and do not always provide extension points to seamlessly integrate new sophisticated analyses. Consequently, we develop the Taupe software system to help researchers visualize, analyze, and edit the data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["55"]}
{"title": "Can lexicon bad smells improve fault prediction?\n", "abstract": " In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["55"]}
{"title": "Identification of behavioural and creational design motifs through dynamic analysis\n", "abstract": " Design patterns offer design motifs, solutions to object\u0393\u00c7\u00c9oriented design problems. Design motifs lead to well\u0393\u00c7\u00c9structured designs and thus are believed to ease software maintenance. However, after use, they are often \u0393\u00c7\u00fflost\u0393\u00c7\u00d6 and are consequently of little help during program comprehension and other maintenance activities. Therefore, several works proposed design pattern identification approaches to recover occurrences of the motifs. These approaches mainly used the structure and organization of classes as input. Consequently, they have a low precision when considering behavioural and creational motifs, which pertain to the assignment of responsibilities and the collaborations among objects at runtime. We propose MoDeC, an approach to describe behavioural and creational motifs as collaborations among objects in the form of scenario diagrams. We identify these motifs using dynamic analysis and constraint\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["55"]}
{"title": "Detecting groups of co-changing files in CVS repositories\n", "abstract": " Software systems continuously evolve. CVS record almost all of the changes the system parts undergo. Hence, CVS repositories contain a great deal of information about software artifact evolution. Software artifacts of a system can evolve following similar evolution patterns as well as very different ones. A peculiar kind of similarity in evolution is the one among two or more artifacts having changed almost at the same times for a certain number of changes. We name these co-changing artifacts. Co-changing artifacts are relevant because co-changes can be inducted by not trivial dependencies among system parts. In this paper, we propose a definition of co-changes suitable of practical application. We assess the challenges arising in detection of groups of co-changing software parts, and we present a robust approach, based on dynamic time warping, to detect groups of co-changing files in CVS repositories. We also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["55"]}
{"title": "Automated protection of php applications against SQL-injection attacks\n", "abstract": " Web sites may be static sites, programs, or databases, and very often a combination of the three integrating relational databases as a back-end. Web sites require care in configuration and programming to assure security, confidentiality, and trustworthiness of the published information. SQL-injection attacks exploit weak validation of textual input used to build database queries. Maliciously crafted input may threaten the confidentiality and the security policies of Web sites relying on a database to store and retrieve information. This paper presents an original approach that combines static analysis, dynamic analysis, and code re-engineering to automatically protect applications written in PHP from SQL-injection attacks. The paper also reports preliminary results of experiments performed on an old SQL-injection prone version of phpBB (version 2.0.0, 37193 LOC of PHP version 4.2.2 code). Results show that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["55"]}
{"title": "An empirical study on requirements traceability using eye-tracking\n", "abstract": " Requirements traceability (RT) links help developers to understand programs and ensure that their source code is consistent with its documentation. Creating RT links is a laborious and resource-consuming task. Information Retrieval (IR) techniques are useful to automatically recover traceability links. However, IR-based approaches typically have low accuracy (precision and recall) and, thus, creating RT links remains a human intensive process. We conjecture that understanding how developers verify RT links could help improve the accuracy of IR-based approaches to recover RT links. Consequently, we perform an empirical study consisting of two controlled experiments. First, we use an eye-tracking system to capture developers' eye movements while they verify RT links. We analyse the obtained data to identify and rank developers' preferred source code entities (SCEs), e.g., class names, method names\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["55"]}
{"title": "Concept location with genetic algorithms: A comparison of four distributed architectures\n", "abstract": " Genetic algorithms are attractive to solve many search-based software engineering problems because they allow the easy parallelization of computations, which improves scalability and reduces computation time. In this paper, we present our experience in applying different distributed architectures to parallelize a genetic algorithm used to solve the concept identification problem. We developed an approach to identify concepts in execution traces by finding cohesive and decoupled fragments of the traces. The approach relies on a genetic algorithm, on a textual analysis of source code using latent semantic indexing, and on trace compression techniques. The fitness function in our approach has a polynomial evaluation cost and is highly computationally intensive. A run of our approach on a trace of thousand methods may require several hours of computation on a standard PC. Consequently, we reduced\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["55"]}
{"title": "Serum vascular endothelial growth factor (VEGF) levels correlate with tumor VEGF and p53 overexpression in endocrine positive primary breast cancer\n", "abstract": " Vascular endothelial growth factor (VEGF) is a potent stimulator of angiogenesis, associated with unfavorable clinical characteristics in breast cancer. The aim of this study was to evaluate different angiogenic markers in endocrine-positive breast cancer patients. The authors analyzed serum and tumor samples from 71 patients with endocrine-positive operable primary breast cancer to determine the expression and the possible relationship between circulating serum VEGF levels, tumor VEGF expression, microvessel density (MVD), and other immunohistochemical parameters. Basal VEGF serum levels were significantly higher in breast cancer patients than in healthy controls. A significant correlation was observed between basal VEGF serum concentrations, microvessel density (p = 0.01) and p53 status (p = 0.004). Intratumoral VEGF expression was significantly associated with neoplastic embolization (p = 0.041\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["55"]}
{"title": "Trend analysis and issue prediction in large-scale open source systems\n", "abstract": " Effort to evolve and maintain a software system is likely to vary depending on the amount and frequency of change requests. This paper proposes to model change requests as time series and to rely on time series mathematical framework to analyze and model them. In particular, this paper focuses on the number of new change requests per KLOC and per unit of time. Time series can have a two-fold application: they can be used to forecast future values and to identify trends. Increasing trends can indicate an increase in customer requests for new features or a decrease in the software system quality. A decreasing trend can indicate application stability and maturity, but also a reduced popularity and adoption. The paper reports case studies over about five years for three large open source applications: Eclipse, Mozilla and JBoss. The case studies show the capability of time series to model change request density\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["55"]}
{"title": "A markov random field approach to microarray image gridding\n", "abstract": " This paper reports a novel approach for the problem of automatic gridding in microarray images. The solution is modeled as a Bayesian random field with a Gibbs prior possibly containing first order cliques (1-clique). On the contrary of previously published contributions, this paper does not assume second order cliques, instead it relies on a two step procedure to locate microarray spots. First a set of guide spots is used to interpolate a reference grid. The final grid is then produced by an a-posteriori maximization, which takes into account the reference rectangular grid, and local deformations. The algorithm is completely automatic and no human intervention is required, the only critical parameter being the range of the radius of the guide spots.", "num_citations": "39\n", "authors": ["55"]}
{"title": "Cyclomatic complexity\n", "abstract": " The cyclomatic complexity (CC) metric measures the number of linearly independent paths through a piece of code. Although Thomas McCabe developed CC for procedural languages, its popularity has endured throughout the object-oriented era. That said, CC is one of the most controversial metrics, shunned for the most part by academia for certain theoretical weaknesses and the belief that it's no more useful than a simple \u0393\u00c7\u00a3lines of code\u0393\u00c7\u00a5 metric. However, most metrics collection tools support its collection, and, paradoxically, industry uses it extensively. So, why is this the case? This question also leads to fundamental perennial questions about industry's exposure to academic opinion and whether academic research fails to take account of software development's daily practicalities. Maybe industry is simply looking for straightforward, widely understood metrics?", "num_citations": "38\n", "authors": ["55"]}
{"title": "Towards the integration of versioning systems, bug reports and source code meta-models\n", "abstract": " Versioning system repositories and bug tracking systems are valuable sources of information to study the evolution of large open source software systems. However, being conceived for specific purposes, i.e., to support the development or trigger maintenance activities, they do neither allow an easy information browsing nor support the study of software evolution. For example, queries such as locating and browsing the faultiest methods are not provided.This paper addresses such issues and proposes an approach and a framework to consistently merge information extracted from source code, versioning repositories and bug reports. Our information representation exploits the property concepts of the FAMIX information exchange meta-model, allowing to represent, browse, and query, at different level of abstractions, the concept of interest. This allows the user to navigate back and forth from versioning system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["55"]}
{"title": "On the use of developers\u0393\u00c7\u00d6 context for automatic refactoring of software anti-patterns\n", "abstract": " Anti-patterns are poor solutions to design problems that make software systems hard to understand and extend. Entities involved in anti-patterns are reported to be consistently related to high change and fault rates. Refactorings, which are behavior preserving changes are often performed to remove anti-patterns from software systems. Developers are advised to interleave refactoring activities with their regular coding tasks to remove anti-patterns, and consequently improve software design quality. However, because the number of anti-patterns in a software system can be very large, and their interactions can require a solution in a set of conflicting objectives, the process of manual refactoring can be overwhelming. To automate this process, previous works have modeled anti-patterns refactoring as a batch process where a program provides a solution for the total number of classes in a system, and the developer\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["55"]}
{"title": "An exploratory study of macro co-changes\n", "abstract": " The literature describes several approaches to identify the artefacts of programs that change together to reveal the (hidden) dependencies among these artefacts. These approaches analyse historical data, mined from version control systems, and report co-changing artefacts, which hint at the causes, consequences, and actors of the changes. We introduce the novel concepts of macro co-changes (MCC), i.e., of artefacts that co-change within a large time interval, and of dephase macro co-changes (DMCC), i.e., macro co-changes that always happen with the same shifts in time. We describe typical scenarios of MCC and DMCC and we use the Hamming distance to detect approximate occurrences of MCC and DMCC. We present our approach, Macocha, to identify these concepts in large programs. We apply Macocha and compare it in terms of precision and recall with UML Diff (file stability) and association rules\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["55"]}
{"title": "Application and user interface migration from basic to visual C++\n", "abstract": " An approach to reengineer BASIC PC legacy code into modern graphical systems is proposed. BASIC has historically been one of the first languages available on PCs. Based on it, small or medium size companies have developed systems that represent valuable company assets to be preserved. Our goal is the automatic migration from the BASIC character oriented user interface to a graphical environment which includes a GUI builder, and compiles event driven C/C++ code. For this purpose a conceptual representation in terms of abstract graphical objects and call-backs has been inferred from the original code, and a translator from BASIC to C has been developed. Moreover the GUI builder internal representation has been generated, so that the user interface can be interactively fine-tuned by the programmer. We present and discuss BASIC peculiarities, with preliminary results on code translation. For the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["55"]}
{"title": "Professional status and expertise for UML class diagram comprehension: An empirical study\n", "abstract": " Professional experience is one of the most important criteria for almost any job offer in software engineering. Professional experience refers both to professional status (practitioner vs. student) and expertise (expert vs. novice). We perform an experiment with 21 subjects including both practitioners and students, and experts and novices. We seek to understand the relation between the speed and accuracy of the subjects and their status and expertise in performing maintenance tasks on UML class diagrams. We also study the impact of the formulation of the maintenance task. We use an eye-tracking system to gather the fixations of the subjects when performing the task. We measure the subjects' comprehension using their accuracy, the time spent, the search effort, the overall effort, and the question comprehension effort. We found that (1) practitioners are more accurate than students while students spend around 35\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["55"]}
{"title": "Requirements traceability for object oriented systems by partitioning source code\n", "abstract": " Requirements trace ability ensures that source code is consistent with documentation and that all requirements have been implemented. During software evolution, features are added, removed, or modified, the code drifts away from its original requirements. Thus trace ability recovery approaches becomes necessary to re-establish the trace ability relations between requirements and source code. This paper presents an approach (Coparvo) complementary to existing trace ability recovery approaches for object-oriented programs. Coparvo reduces false positive links recovered by traditional trace ability recovery processes thus reducing the manual validation effort. Coparvo assumes that information extracted from different entities (i.e., class names, comments, class variables, or methods signatures) are different information sources, they may have different level of reliability in requirements trace ability and each\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["55"]}
{"title": "A feedback based quality assessment to support open source software evolution: the GRASS case study\n", "abstract": " Managing the software evolution for large open source software is a major challenge. Some factors that make software hard to maintain are geographically distributed development teams, frequent and rapid turnover of volunteers, absence of a formal means, and lack of documentation and explicit project planning. In this paper we propose remote and continuous analysis of open source software to monitor evolution using available resources such as CVS code repository, commitment log files and exchanged mail. Evolution monitoring relies on three principal services. The first service analyzes and monitors the increase in complexity and the decline in quality; the second supports distributed developers by sending them a feedback report after each contribution; the third allows developers to gain insight into the \"big picture\" of software by providing a dashboard of project evolution. Besides the description of provided\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["55"]}
{"title": "Improving Software Quality through Requirements Traceability Models\n", "abstract": " Requirement traceability plays an important part in producing quality software. Traceability ensures that design specifications are appropriately verified, and functional requirements are appropriately validated. Requirements traceability also plays a key role in fault avoidance and fault removal. The aim of the paper is to develop an efficient and dynamic requirement tractability model. This model will be more viable for use in small projects as well as large-scale projects.", "num_citations": "33\n", "authors": ["55"]}
{"title": "Improving bug location using binary class relationships\n", "abstract": " Bug location assists developers in locating culprit source code that must be modified to fix a bug. Done manually, it requires intensive search activities with unpredictable costs of effort and time. Information retrieval (IR) techniques have been proven useful to speedup bug location in object-oriented programs. IR techniques compute the textual similarities between a bug report and the source code to provide a list of potential culprit classes to developers. They rank the list of classes in descending order of the likelihood of the classes to be related to the bug report. However, due to the low textual similarity between source code and bug reports, IR techniques may put a culprit class at the end of a ranked list, which forces developers to manually verify all non-culprit classes before finding the actual culprit class. Thus, even with IR techniques, developers are not saved from manual effort. In this paper, we conjecture that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["55"]}
{"title": "System for building a language model network for speech recognition\n", "abstract": " A system for recognizing continuous speech, for example for automatic dictation applications, uses a bigramme language model organized as a network with finite probability states. The system also uses methods of estimating the probabilities associated with the bigrammes and of representing the model of the language in a tree-like probability network.", "num_citations": "32\n", "authors": ["55"]}
{"title": "Physical and conceptual identifier dispersion: Measures and relation to fault proneness\n", "abstract": " Poorly-chosen identifiers have been reported in the literature as misleading and increasing the program comprehension effort. Identifiers are composed of terms, which can be dictionary words, acronyms, contractions, or simple strings. We conjecture that the use of identical terms in different contexts may increase the risk of faults. We investigate our conjecture using a measure combining term entropy and term context coverage to study whether certain terms increase the odds ratios of methods to be fault-prone. Entropy measures the physical dispersion of terms in a program: the higher the entropy, the more scattered across the program the terms. Context coverage measures the conceptual dispersion of terms: the higher their context coverage, the more unrelated the methods using them. We compute term entropy and context coverage of terms extracted from identifiers in Rhino 1.4R3 and ArgoUML 0.16. We show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["55"]}
{"title": "Campora: a young genetic isolate in South Italy\n", "abstract": " Genetic isolates have been successfully used in the study of complex traits, mainly because due to their features, they allow a reduction in the complexity of the genetic models underlying the trait. The aim of the present study is to describe the population of Campora, a village in the South of Italy, highlighting its properties of a genetic isolate. Both historical evidence and multi-locus genetic data (genomic and mitochondrial DNA polymorphisms) have been taken into account in the analyses. The extension of linkage disequilibrium (LD) regions has been evaluated on autosomes and on a region of the X chromosome. We defined a study sample population on the basis of the genealogy and exogamy data. We found in this population a few different mitochondrial and Y chromosome haplotypes and we ascertained that, similarly to other isolated populations, in Campora LD extends over wider region compared to large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["55"]}
{"title": "Automatic unit test data generation using mixed-integer linear programming and execution trees\n", "abstract": " This paper presents an approach to automatic unit test data generation for branch coverage using mixed-integer linear programming, execution trees, and symbolic execution. This approach can be useful to both general testing and regression testing after software maintenance and reengineering activities. Several strategies, including original algorithms, to move towards practical test data generation have been investigated in this paper. Methods include: the analysis of minimum path-length partial execution trees for unconstrained arcs, thus increasing the generation performance and reducing the difficulties originated by infeasible paths the reduction of the difficulties originated by nonlinear path conditions by considering alternative linear paths the reduction of the number of test cases, which are needed to achieve the desired coverage, based on the concept of unconstrained arcs in a control flow graph the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["55"]}
{"title": "Recommending when design technical debt should be self-admitted\n", "abstract": " Previous research has shown how developers \"selfadmit\" technical debt introduced in the source code, commenting why such code represents a workaround or a temporary, incomplete solution. This paper investigates the extent to which previously self-admitted technical debt can be used to provide recommendations to developers when they write new source code, suggesting them when to \"self-admit\" design technical debt, or possibly when to improve the code being written. To achieve this goal, we have developed a machine learning approach named TEDIOUS (TEchnical Debt IdentificatiOn System), which leverages various kinds of method-level features as independent variables, including source code structural metrics, readability metrics and, last but not least, warnings raised by static analysis tools. We assessed TEDIOUS on data from nine open source projects for which there are available tagged self\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["55"]}
{"title": "An approach for search based testing of null pointer exceptions\n", "abstract": " Uncaught exceptions, and in particular null pointer exceptions (NPEs), constitute a major cause of crashes for software systems. Although tools for the static identification of potential NPEs exist, there is need for proper approaches able to identify system execution scenarios causing NPEs. This paper proposes a search-based test data generation approach aimed at automatically identify NPEs. The approach consists of two steps: (i) an inter-procedural data and control flow analysis - relying on existing technology - that identifies paths between input parameters and potential NPEs, and (ii) a genetic algorithm that evolves a population of test data with the aim of covering such paths. The algorithm is able to deal with complex inputs containing arbitrary data structures. The approach has been evaluated on to test class clusters from six Java open source systems, where NPE bugs have been artificially introduced\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["55"]}
{"title": "Investigating large software system evolution: the linux kernel\n", "abstract": " Large multi-platform, multi-million lines of codes software systems evolve to cope with new platform or to meet user ever changing needs. While there has been several studies focused on the similarity of code fragments or modules, few studies addressed the need to monitor the overall system evolution. Meanwhile, the decision to evolve or to re-factor a large software system needs to be supported by high level information, representing the system overall picture, abstracting from unnecessary details. This paper proposes to extend the concept of similarity of code fragments to quantify similarities at the release/system level. Similarities are captured by four software metrics representative of the commonalities and differences within and among software artifacts. To show the feasibility of characterizing large software system with the new metrics, 365 releases of the Linux kernel were analyzed. The metrics, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["55"]}
{"title": "A novel composite model approach to improve software quality prediction\n", "abstract": " Context:How can quality of software systems be predicted before deployment? In attempting to answer this question, prediction models are advocated in several studies. The performance of such models drops dramatically, with very low accuracy, when they are used in new software development environments or in new circumstances.ObjectiveThe main objective of this work is to circumvent the model generalizability problem. We propose a new approach that substitutes traditional ways of building prediction models which use historical data and machine learning techniques.MethodIn this paper, existing models are decision trees built to predict module fault-proneness within the NASA Critical Mission Software. A genetic algorithm is developed to combine and adapt expertise extracted from existing models in order to derive a \u0393\u00c7\u00a3composite\u0393\u00c7\u00a5 model that performs accurately in a given context of software development\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["55"]}
{"title": "Prereqir: Recovering pre-requirements via cluster analysis\n", "abstract": " High-level software artifacts, such as requirements, domain-specific requirements, and so on, are an important source of information that is often neglected during the reverse- and re-engineering processes. We posit that domain specific pre-requirements information (PRI) can be obtained by eliciting the stakeholderspsila understanding of generic systems or domains. We discuss the semi-automatic recovery of domain-specific PRI that can then be used during reverse- and re-engineering, for example, to recover traceability links or to assess the degree of obsolescence of a system with respect to competing systems and the clientspsila expectations. We present a method using partition around medoids and agglomerative clustering for obtaining, structuring, analyzing, and labeling textual PRI from a group of diverse stakeholders. We validate our method using PRI for the development of a generic Web browser\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["55"]}
{"title": "A queue theory-based approach to staff software maintenance centers\n", "abstract": " The Internet and WEB pervasivenesses are changing the landscape of several different areas, ranging from information gathering/managing and commerce to software development, maintenance and evolution. Software companies having a geographically distributed structure, or geographically distributed customers, are adopting information communication technologies to cooperate. Communication technologies and infrastructures allow the companies to create a virtual software factory. This paper proposes to adopt queue theory to deal with an economically relevant category of problems: the staffing, the process management and the service level evaluation of massive maintenance projects in a virtual software factory. Data from a massive corrective maintenance intervention were used to simulate and study different service center configurations, in particular, a monolithic configuration and a configuration\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["55"]}
{"title": "Adapting function points to object oriented information systems\n", "abstract": " The object oriented paradigm has become widely used to develop large information systems. This paper presents a method for estimating the size and effort of developing object oriented software. The approach is analogous to function points, and it is based on counting rules that pick up the elements in a static object model and combine them in order to produce a composite measure. Rules are proposed for counting \u0393\u00c7\u00a3Object Oriented Function Points\u0393\u00c7\u00a5 from an object model, and several questions are identified for empirical research.             A key aspect of this method is its flexibility. An organization can experiment with different counting policies, to find the most accurate predictors of size, effort, etc. in its environment.             \u0393\u00c7\u00a3Object Oriented Function Points\u0393\u00c7\u00a5 counting has been implemented in a Java tool, and results on size estimation obtained from a pilot project with an industrial partner are encouraging.", "num_citations": "27\n", "authors": ["55"]}
{"title": "An exploratory study of identifier renamings\n", "abstract": " Identifiers play an important role in source code understandability, maintainability, and fault-proneness. This paper reports a study of identifier renamings in software systems, studying how terms (identifier atomic components) change in source code identifiers. Specifically, the paper (i) proposes a term renaming taxonomy,(ii) presents an approximate lightweight code analysis approach to detect and classify term renamings automatically into the taxonomy dimensions, and (iii) reports an exploratory study of term renamings in two open-source systems, Eclipse-JDT and Tomcat. We thus report evidence that not only synonyms are involved in renamings but also (in a small fraction) more unexpected changes occur: surprisingly, we detected hypernym (a more abstract term, eg, size vs. length) and hyponym (a more concrete term, eg, restriction vs. rule) renamings, and antonym renamings (a term replaced with one\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["55"]}
{"title": "XOgastan: XML-oriented gcc AST analysis and transformations\n", "abstract": " Software maintenance, program analysis and transformation tools almost always rely on static source code analysis as the first and fundamental step to gather information. In the past, two different strategies have been adopted to develop tool suites. There are tools encompassing or implementing the source parse step, where the parser is internal to the toolkit, developed and maintained with it. A different approach builds tools on the top of external, already available, components such as compilers that output the abstract syntax tree, or make it available via an API. We present an approach and a tool, XOgastan, developed exploiting the gcc/g++ ability to save a representation of the intermediate abstract syntax tree into a file. XOgastan translates the gcc/g++ format into a graph exchange language representation, thus taking advantage of the high number of currently available XML tools for the subsequent analysis\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["55"]}
{"title": "Moving to smaller libraries via clustering and genetic algorithms\n", "abstract": " There may be several reasons to reduce a software system to its bare bone removing the extra fat introduced during development or evolution. Porting the software system on embedded devices or palmtops are just two examples. This paper presents an approach to re-factoring libraries with the aim of reducing the memory requirements of executables. The approach is organized in two steps. The first step defines an initial solution based on clustering methods, while the subsequent phase refines the initial solution via genetic algorithms. In particular, a novel genetic algorithm approach, considering the initial clusters as the starting population, adopting a knowledge-based mutation function and a multiobjective fitness function, is proposed. The approach has been applied to several medium and large-size open source software systems such as GRASS, KDE-QT Samba and MySQL, allowing one to effectively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["55"]}
{"title": "Modeling web maintenance centers through queue models\n", "abstract": " The Internet and the World Wide Web's pervasiveness are changing the landscape of several different areas, ranging from information gathering/managing and commerce to software development, maintenance and evolution. Traditional telephone-centric services, such as ordering of goods, maintenance/repair intervention requests and bug/defect reporting, are moving towards Web-centric solutions. This paper proposes the adoption of queuing theory to support the design, staffing, management and assessment of Web-centric service centers. Data from a mailing list archiving a mixture of corrective maintenance and information requests were used to mimic a service center. Queuing theory was adopted to model the relation between the number of servers and the performance level. Empirical evidence revealed that, by adding an express lane and a dispatcher service time, the variability is greatly reduced and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["55"]}
{"title": "Evolving object oriented design to improve code traceability\n", "abstract": " Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However, few works have so far addressed the theme of tracing object oriented design into its implementation and evolving it. The paper presents an approach to checking the compliance of OO design with respect to source code and support its evolution. The process works on design artifacts expressed in the OMT notation and accepts C++ source code. It recovers an \"as is\" design from the code, compares recovered design with the actual design and helps the user to deal with inconsistencies. The recovery process exploits the edit distance computation and the maximum match algorithm to determine traceability links between design and code. The output is a similarity measure associated to each matched class, plus a set of unmatched classes. A graphic display of the design with different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["55"]}
{"title": "Recovering the evolution stable part using an ecgm algorithm: Is there a tunnel in mozilla?\n", "abstract": " Analyzing the evolutionary history of the design of object-oriented software is an important and difficult task where matching algorithms play a fundamental role. In this paper, we investigate the applicability of an error-correcting graph matching (ECGM) algorithm to object-oriented software evolution. By means of a case study, we report evidence of ECGM applicability in studying the Mozilla class diagram evolution. We collected 144 Mozilla snapshots over the past six years, reverse-engineered class diagrams and recovered traceability links between subsequent class diagrams. Our algorithm allows us to identify evolving classes that maintain a stable structure of relations(associations, inheritances and aggregations) with other classes and thus likely constitute the backbone of Mozilla.", "num_citations": "25\n", "authors": ["55"]}
{"title": "Maintenance and testing effort modeled by linear and nonlinear dynamic systems\n", "abstract": " Maintenance and testing activities \u0393\u00c7\u00f6 conducted, respectively, on the release currently in use/to be delivered \u0393\u00c7\u00f6 absorb most of total lifetime cost of software development. Such economic relevance suggests investigating the maintenance and testing processes to find models allowing software engineers to better estimate, plan and manage costs and activities.Ecological systems in which predators and prey compete for surviving were investigated by applying suitable mathematical models. An analogy can be drawn between biological prey and software defects, and between predators and programmers. In fact, when programmers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find any new defect has an initial increase, followed by a decline, when almost all defects are removed, similar to prey and predator populations.This paper proposes to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["55"]}
{"title": "Dynamic model for maintenance and testing effort\n", "abstract": " The dynamic evolution of ecological systems in which predators and prey compete for survival has been investigated by applying suitable mathematical models. Dynamic systems theory provides a useful way to model interspecies competition and thus the evolution of predator and prey populations. This kind of mathematical framework has been shown to be well suited to describe evolution of economical systems as well, where instead of predators and prey there are consumers and resources. Maintenance and testing activities absorb the most relevant part of the total life-cycle cost of software. Such economic relevance strongly suggests to investigate the maintenance and testing processes in order to find new models allowing software engineers to better estimate, plan and manage costs and activities. We show how dynamic systems theory could be usefully applied to maintenance and testing context, namely to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["55"]}
{"title": "Towards understanding how developers spend their effort during maintenance activities\n", "abstract": " For many years, researchers and practitioners have strived to assess and improve the productivity of software development teams. One key step toward achieving this goal is the understanding of factors affecting the efficiency of developers performing development and maintenance activities. In this paper, we aim to understand how developers' spend their effort during maintenance activities and study the factors affecting developers' effort. By knowing how developers' spend their effort and which factors affect their effort, software organisations will be able to take the necessary steps to improve the efficiency of their developers, for example, by providing them with adequate program comprehension tools. For this preliminary study, we mine 2,408 developers' interaction histories and 3,395 patches from four open-source software projects (ECF, Mylyn, PDE, Eclipse Platform). We observe that usually, the complexity of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["55"]}
{"title": "Library miniaturization using static and dynamic information\n", "abstract": " Moving to smaller libraries can be considered as a relevant task when porting software systems to limited-resource devices (e.g., hand-helds). Library miniaturization will be particularly effective if based on both dynamic (keeping into account dependencies exploited during application execution in a given user profile) and static (keeping into account all possible dependencies) information. This paper presents distributed software architecture, based on Web services, to collect dynamic information at run-time, and an approach for miniaturization of libraries, exploiting both dynamic and static information with the aim of reducing the memory requirements of executables. New, smaller libraries are identified via hierarchical clustering and genetic algorithms; clustering produces a first initial solution, then optimized by multi-objective genetic algorithms. The approach has been applied to medium size open source software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["55"]}
{"title": "Keep it simple: Is deep learning good for linguistic smell detection?\n", "abstract": " Deep neural networks is a popular technique that has been applied successfully to domains such as image processing, sentiment analysis, speech recognition, and computational linguistic. Deep neural networks are machine learning algorithms that, in general, require a labeled set of positive and negative examples that are used to tune hyper-parameters and adjust model coefficients to learn a prediction function. Recently, deep neural networks have also been successfully applied to certain software engineering problem domains (e.g., bug prediction), however, results are shown to be outperformed by traditional machine learning approaches in other domains (e.g., recovering links between entries in a discussion forum). In this paper, we report our experience in building an automatic Linguistic Antipattern Detector (LAPD) using deep neural networks. We manually build and validate an oracle of around 1,700\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["55"]}
{"title": "Finding the best compromise between design quality and testing effort during refactoring\n", "abstract": " Anti-patterns are poor design choices that hinder code evolution, and understandability. Practitioners perform refactoring, that are semantic-preserving-code transformations, to correct anti-patterns and to improve design quality. However, manual refactoring is a consuming task and a heavy burden for developers who have to struggle to complete their coding tasks and maintain the design quality of the system at the same time. For that reason, researchers and practitioners have proposed several approaches to bring automated support to developers, with solutions that ranges from single anti-patterns correction, to multiobjective solutions. The latter approaches attempted to reduce refactoring effort, or to improve semantic similarity between classes and methods in addition to removing anti-patterns. To the best of our knowledge, none of the previous approaches have considered the impact of refactoring on another\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["55"]}
{"title": "An empirical study on the importance of source code entities for requirements traceability\n", "abstract": " Requirements Traceability (RT) links help developers during program comprehension and maintenance tasks. However, creating RT links is a laborious and resource-consuming task. Information Retrieval (IR) techniques are useful to automatically create traceability links. However, IR-based techniques typically have low accuracy (precision, recall, or both) and thus, creating RT links remains a human intensive process. We conjecture that understanding how developers verify RT links could help improve the accuracy of IR-based RT techniques to create RT links. Consequently, we perform an empirical study consisting of four case studies. First, we use an eye-tracking system to capture developers\u0393\u00c7\u00d6 eye movements while they verify RT links. We analyse the obtained data to identify and rank developers\u0393\u00c7\u00d6 preferred types of Source Code Entities (SCEs), e.g., domain vs. implementation-level source code terms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["55"]}
{"title": "Serum insulin-like growth factor 1 correlates with the risk of nodal metastasis in endocrine-positive breast cancer\n", "abstract": " Increased insulin-like growth factor (IGF) signalling has been observed in breast cancer, including endocrine-responsive cancers, and has been linked to disease progression and recurrence. In particular, IGF-1 has the ability to induce and promote lymphangiogenesis through the induction of vascular endothelial growth factor C (VEGFC). In the present study, we analyzed serum and tumour samples from 60 patients with endocrine-positive breast cancer to determine the expression and the possible relationship of circulating IGF-1, IGF binding protein 3 (IGFBP3), and VEGFC with the presence of lymphatic metastasis and other immunohistochemical parameters. The analysis revealed a clear and significant correlation between high basal levels of IGF-1, IGFBP3, and VEGFC and lymph node metastasis in endocrine-responsive breast cancer. In addition, expression of those molecules was significantly higher in breast cancer patients than in healthy control subjects. Those findings may enable more accurate prediction of prognosis in patients with breast cancer.", "num_citations": "23\n", "authors": ["55"]}
{"title": "Not all classes are created equal: toward a recommendation system for focusing testing\n", "abstract": " When evolving an object oriented system, one relevant question is the following: given a finite amount of resources, what are the most critical classes on which testers should focus their attention? In this paper, we propose a new way for identifying critical classes: classes often changed and playing a key role in the system. We rely on error correcting graph matching (ECGM) and random walks to associate each class with a pair of values representative of the frequency of changes and the class overall connectivity.", "num_citations": "23\n", "authors": ["55"]}
{"title": "Yaab (yet another ast browser): Using ocl to navigate asts\n", "abstract": " In the last decades several tools and environments defined and introduced languages for querying, navigating and transforming abstract syntax trees. These environments were meant to support software maintenance, reengineering and program comprehension activities. Instead of introducing a new language, this paper proposes to adopt the Object Constraint Language (OCL) to express queries over an object model representing the abstract syntax tree of the code to be analyzed. OCL is part of the UML lingua franca and thus several advantages can be readily obtained. Central to the idea is to shift the analysis paradigm from a tree-based to an object-oriented paradigm, and to provide a meta-model decoupling the query language from the target language. This paper presents the current status in implementing an OCL interpreter with the ability of querying an object model representing the abstract syntax tree\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["55"]}
{"title": "Getting the most from map data structures in Android\n", "abstract": " A map is a data structure that is commonly used to store data as key\u0393\u00c7\u00f4value pairs and retrieve data as keys, values, or key\u0393\u00c7\u00f4value pairs. Although Java offers different map implementation classes, Android SDK offers other implementations supposed to be more efficient than HashMap: ArrayMap and SparseArray variants (SparseArray, LongSparseArray, SparseIntArray, SparseLongArray, and SparseBooleanArray). Yet, the performance of these implementations in terms of CPU time, memory usage, and energy consumption is lacking in the official Android documentation; although saving CPU, memory, and energy is a major concern of users wanting to increase battery life. Consequently, we study the use of map implementations by Android developers in two ways. First, we perform an observational study of 5713 Android apps in GitHub. Second, we conduct a survey to assess developers\u0393\u00c7\u00d6 perspective on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["55"]}
{"title": "SCAN: an approach to label and relate execution trace segments\n", "abstract": " Program comprehension is a prerequisite to any maintenance and evolution task. In particular, when performing feature location, developers perform program comprehension by abstracting software features and identifying the links between high\u0393\u00c7\u00c9level abstractions (features) and program elements. We present Segment Concept AssigNer (SCAN), an approach to support developers in feature location. SCAN uses a search\u0393\u00c7\u00c9based approach to split execution traces into cohesive segments. Then, it labels the segments with relevant keywords and, finally, uses formal concept analysis to identify relations among segments. In a first study, we evaluate the performances of SCAN on six Java programs by 31 participants. We report an average precision of 69% and a recall of 63% when comparing the manual and automatic labels and a precision of 63% regarding the relations among segments identified by SCAN. After\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["55"]}
{"title": "Compiler hacking for source code analysis\n", "abstract": " Many activities related to software quality assessment and improvement, such as empirical model construction, data flow analysis, testing or reengineering, rely on static source code analysis as the first and fundamental step for gathering the necessary input information. In the past, two different strategies have been adopted to develop tool suites. There are tools encompassing or implementing the source parse step, where the parser is internal to the toolkit, and is developed and maintained with it. A different approach builds tools on the top of external already-available components such as compilers that output the program abstract syntax tree, or that make it available via an API.               This paper discusses techniques, issues and challenges linked to compiler patching or wrapping for analysis purposes. In particular, different approaches for accessing the compiler parsing information are compared, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["55"]}
{"title": "Variable precision reaching definitions analysis for software maintenance\n", "abstract": " A flow analyzer can be very helpful in the process of program understanding, by providing the programmer with different views of the code. As the documentation is often incomplete or inconsistent, it is extremely useful to extract the information a programmer may need directly from the code. Program understanding activities are interactive, thus program analysis tools may be asked for quick answers by the maintainer. Therefore the control on the trade-off between accuracy and efficiency should be given to the user. The paper presents an approach to interprocedural reaching definitions flow analysis based on three levels of precision depending on the sensitivity to the calling context and the control flow. A lower precision degree produces an overestimate of the data dependencies in a program. The result is anyhow conservative (all dependencies which hold are surely reported), and definitely faster than the more\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["55"]}
{"title": "Art: an architectural reverse engineering environment\n", "abstract": " When programmers perform maintenance tasks, program understanding is often required. One of the first activities in understanding a software system is identifying its subsystems and their relations, i.e., its software architecture. Since a large part of the effort is spent in creating a mental model of the system under study, tools can help maintainers in managing the evolution of legacy systems by showing them architectural information. This paper describes an environment for the architectural recovery of software systems called the architectural recovery tool (ART). The environment is based on a hierarchical architectural model that drives the application of a set of recognizers, each producing a different architectural view of a system or of some of its parts. Recognizers embody knowledge about architectural clich\u251c\u2310s and use flow analysis techniques to make their output more accurate. To test the accuracy and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["55"]}
{"title": "SPIRITuS: A simple information retrieval regression test selection approach\n", "abstract": " Context:Regression Test case Selection (RTS) approaches aim at selecting only those test cases of a test suite that exercise changed parts of the System Under Test (SUT) or parts affected by changes.Objective:We present SPIRITuS (SimPle Information Retrieval regressIon Test Selection approach). It uses method code coverage information and a Vector Space Model to select test cases to be run. In a nutshell, the extent of a lexical modification to a method is used to decide if a test case has to be selected. The main design goals of SPIRITuS are to be: (i) easy to adapt to different programming languages and (ii) tunable via an easy to understand\u252c\u00e1threshold.Method:To assess SPIRITuS, we conducted a large experiment on 389 faulty versions of 14 open-source programs implemented in Java. We were mainly interested in investigating the tradeoff between the number of selected test cases from the original test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "On the impact of sampling frequency on software energy measurements\n", "abstract": " Energy consumption is a major concern when developing and evolving mobile applications. The user wishes to access fast and powerful mobile applications, which is usually in contrast to optimized battery life and heat generation. The software engineering community have acknowledged the relevance of the problem and researchers are investigating ways to reduce energy consumption, for example by examining which library, device configuration, and applications parameters should be used to promote long battery life. We conjecture that these studies are at the border between hardware and software and we must be careful on how the energy consumption is measured and how the energy consumption is attributed to methods and libraries. To the best of our knowledge, no previous work investigates how much energy and power consumption is due to high frequency events missed when sampling at low frequencies such as 10 kHz and verified the error at the precision of method level. Low frequency sampling is a rough approximation that hinders the understanding of fine grain details: the real picture of energy consumption as well as the root causes are missed. This has profound implications on the choice of methods to evolve or components to replace. In this paper, we propose an approach for accurate measurements of the energy consumption of mobile applications. We apply the proposed approach to assess the energy consumption of 21 mobile, closed source, applications and four open source Android applications. We show that by sampling at 10 kHz one may expect a median error of 8%, however, such error may be as high as 50\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "Are PHP applications ready for Hack?\n", "abstract": " PHP is by far the most popular WEB scripting language, accounting for more than 80% of existing websites. PHP is dynamically typed, which means that variables take on the type of the objects that they are assigned, and may change type as execution proceeds. While some type changes are likely not harmful, others involving function calls and global variables may be more difficult to understand and the source of many bugs. Hack, a new PHP variant endorsed by Facebook, attempts to address this problem by adding static typing to PHP variables, which limits them to a single consistent type throughout execution. This paper defines an empirical taxonomy of PHP type changes along three dimensions: the complexity or burden imposed to understand the type change; whether or not the change is potentially harmful; and the actual types changed. We apply static and dynamic analyses to three widely used WEB\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "Using local similarity measures to efficiently address approximate graph matching\n", "abstract": " In this paper, we investigate heuristics for Approximate Graph Matching (AGM), in particular when it can be formulated as a Maximum Common Edge Subgraph (MCES) problem. First, we observe empirically that initializing a local search with a tiny subset of a known optimal solution always results in much better solutions than starting with an empty solution. The main challenge could then be to retrieve such small subsets for any problem instance. For this purpose, we propose several local similarity measures and evaluate their ability to predict node matches which could be used to start a local search. The resulting algorithm (SIM-T) is a classic tabu algorithm that is initialized by a greedy procedure relying mainly, in its earliest steps, on similarity measures.We conducted experiments on a large collection of random graphs of various orders (from 50 to 3000 nodes) and densities. Results obtained are mostly excellent\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "Detecting asynchrony and dephase change patterns by mining software repositories\n", "abstract": " Software maintenance accounts for the largest part of the costs of any program. During maintenance activities, developers implement changes (sometimes simultaneously) on artifacts in order to fix bugs and to implement new requirements. To reduce this part of the costs, previous work proposed approaches to identify the artifacts of programs that change together. These approaches analyze historical data, mined from version control systems, and report change patterns, which lead at the causes, consequences, and actors of the changes to source code files. They also introduce so\u0393\u00c7\u00c9called change patterns that describe some typical change dependencies among files. In this paper, we introduce two novel change patterns: the asynchrony change pattern, corresponding to macro co\u0393\u00c7\u00c9changes (MC), that is, of files that co\u0393\u00c7\u00c9change within a large time interval (change periods) and the dephase change pattern\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "Evolution and search based metrics to improve defects prediction\n", "abstract": " Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application.In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.", "num_citations": "20\n", "authors": ["55"]}
{"title": "Keynote paper: Search based software testing for software security: Breaking code to make it safer\n", "abstract": " Ensuring security of software and computerized systems is a pervasive problem plaguing companies and institutions and affecting many areas of modern life. Software vulnerability may jeopardize information confidentiality and cause software failure leading tocatastrophic threats to humans or severe economic losses. Size, complexity, extensibility, connectivity and the search for cheap systems make it very hard or even impossible to manually tackle vulnerability detection. Search based software testing attempts to solve two aspects of the cost - vulnerabilityproblem. First, it's cheaper because itis far less labor intensive when compared to traditional testing techniques. As a result, it can be used to more thoroughly test software and reduce the risk that a vulnerability slips into production code. Also, search based software testing can be specifically tailored to tackle the subset of well known security vulnerabilities\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["55"]}
{"title": "Insider and ousider threat-sensitive sql injection vulnerability analysis in php\n", "abstract": " In general, SQL-injection attacks rely on some weak validation of textual input used to build database queries. Maliciously crafted input may threaten the confidentiality and the security policies of Web sites relying on a database to store and retrieve information. Furthermore, insiders may introduce malicious code in a Web application, code that, when triggered by some specific input, for example, would violate security policies. This paper presents an original approach based on static analysis to automatically detect statements in PHP applications that may be vulnerable to SQL-injections triggered by either malicious input (outsider threats) or malicious code (insider threats). Original flow analysis equations, that propagate and combine security levels along an inter-procedural control flow graph (CFG), are presented. The computation of security levels presents linear execution time and memory complexity", "num_citations": "20\n", "authors": ["55"]}
{"title": "Robust speech understanding for robot telecontrol\n", "abstract": " This paper describes an Automatic Speech Understanding (ASU) system used in a human-robot interface for the remote control of a mobile robot. The intended application is that of an operator issuing telecontrol commands to one or more robots from a remote workstation. ASU is supposed to be performed with continuous speech, speaker independence, and quasi real time conditions. Training and testing of the system was based on speech data collected by means of Wizard of Oz simulations. Two kinds of robustness factors are introduced: the first is a recognition-errortolerant approach to semantic interpretation, the second is based on a technique for evaluating the reliability of the ASU system output with respect to the input utterance. Preliminary results are 93% correct semantic interpretations, and 96.5% correct detection of out-of-domain sentences at the cost of rejecting 6.7% correct in-domain sentences.", "num_citations": "20\n", "authors": ["55"]}
{"title": "On the detection of licenses violations in the android ecosystem\n", "abstract": " Mobile applications (apps) developers often reuse code from existing libraries and frameworks in order to reduce development costs. However, these libraries and frameworks are governed by licenses to which developers must comply. A failure to comply with a license is likely to result in penalties and fines. In this paper, we analyse the licenses of 857 mobile apps from the F-droid market with the aim to understand the types of licenses that are mostly used by developers of open-source mobile apps and how these licenses evolve over time. We also investigate licenses violations and the evolution of these violations over time. Results show that developers of open-source mobile apps mostly use GPL and Apache licenses. We found licenses violations in 17 out of 857 apps, and 7 apps still had violations in their latest release at the time of this study. We also observed that many files are not licensed in their first\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["55"]}
{"title": "Tris: A fast and accurate identifiers splitting and expansion algorithm\n", "abstract": " Understanding source code identifiers, by identifying words composing them, is a necessary step for many program comprehension, reverse engineering, or redocumentation tasks. To this aim, researchers have proposed several identifier splitting and expansion approaches such as Samurai, TIDIER and more recently GenTest. The ultimate goal of such approaches is to help disambiguating conceptual information encoded in compound (or abbreviated) identifiers. This paper presents TRIS, TRee-based Identifier Splitter, a two-phases approach to split and expand program identifiers. First, TRIS pre-compiles transformed dictionary words into a tree representation, associating a cost to each transformation. In a second phase, it maps the identifier splitting/expansion problem into a minimization problem, i.e., the search of the shortest path (optimal split/expansion) in a weighted graph. We apply TRIS to a sample of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["55"]}
{"title": "Identifying and locating interference issues in php applications: The case of wordpress\n", "abstract": " The large success of Content management Systems (CMS) such as WordPress is largely due to the rich ecosystem of themes and plugins developed around the CMS that allows users to easily build and customize complex Web applications featuring photo galleries, contact forms, and blog pages. However, the design of the CMS, the plugin-based architecture, and the implicit characteristics of the programming language used to develop them (often PHP), can cause interference or unwanted side effects between the resources declared and used by different plugins. This paper describes the problem of interference between plugins in CMS, specifically those developed using PHP, and outlines an approach combining static and dynamic analysis to detect and locate such interference. Results of a case study conducted over 10 WordPress plugins shows that the analysis can help to identify and locate plugin\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["55"]}
{"title": "Microarray image gridding with stochastic search based approaches\n", "abstract": " The paper reports a novel approach for the problem of automatic gridding in Microarray images. Such problem often requires human intervention; therefore, the development of automated procedures is a fundamental issue for large-scale functional genomic experiments involving many microarray images. Our method uses a two-step process. First a regular rectangular grid is superimposed on the image by interpolating a set of guide spots, this is done by solving a non-linear optimization process with a stochastic search producing the best interpolating grid parameterized by a six values vector. Second, the interpolating grid is adapted, with a Markov Chain Monte Carlo method, to local deformations. This is done by modeling the solution a Markov random field with a Gibbs prior possibly containing first order cliques (1-clique). The algorithm is completely automatic and no human intervention is required, it efficiently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["55"]}
{"title": "An experimental investigation on the effects of context on source code identifiers splitting and expansion\n", "abstract": " Recent and past studies indicate that source code lexicon plays an important role in program comprehension. Developers often compose source code identifiers with abbreviated words and acronyms, and do not always use consistent mechanisms and explicit separators when creating identifiers. Such choices and inconsistencies impede the work of developers that must understand identifiers by decomposing them into their component terms, and mapping them onto dictionary, application or domain words. When software documentation is scarce, outdated or simply not available, developers must therefore use the available contextual information to understand the source code. This paper aims at investigating how developers split and expand source code identifiers, and, specifically, the extent to which different kinds of contextual information could support such a task. In particular, we consider (i) an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["55"]}
{"title": "A study on the relation between antipatterns and the cost of class unit testing\n", "abstract": " Antipatterns are known as recurring, poor design choices, recent and past studies indicated that they negatively affect software systems in terms of understand ability and maintainability, also increasing change-and defect-proneness. For this reason, refactoring actions are often suggested. In this paper, we investigate a different side-effect of antipatterns, which is their effect on testability and on testing cost in particular. We consider as (upper bound) indicator of testing cost the number of test cases that satisfy the minimal data member usage matrix (MaDUM) criterion proposed by Bashir and Goel. A study-carried out on four Java programs, Ant 1.8.3, ArgoUML 0.20, Check Style 4.0, and JFreeChart 1.0.13-supports the evidence that, on the one hand, antipatterns unit testing requires, on average, a number of test cases substantially higher than unit testing for non-antipattern classes. On the other hand, antipattern\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["55"]}
{"title": "Moms: Multi-objective miniaturization of software\n", "abstract": " Smart phones, gaming consoles, and wireless routers are ubiquitous; the increasing diffusion of such devices with limited resources, together with society's unsatiated appetite for new applications, pushes companies to miniaturize their programs. Miniaturizing a program for a hand-held device is a time-consuming task often requiring complex decisions. Companies must accommodate conflicting constraints: customers' satisfaction with features may be in conflict with a device's limited storage, memory, or battery life. This paper proposes a process, MoMS, for the multi-objective miniaturization of software to help developers miniaturize programs while satisfying multiple conflicting constraints. It can be used to support the reverse engineering, next release problem, and porting of both software and product lines. The process directs the elicitation of customer pre-requirements, their mapping to program features, and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["55"]}
{"title": "Clustering support for inadequate test suite reduction\n", "abstract": " Regression testing is an important activity that can be expensive (e.g., for large test suites). Test suite reduction approaches speed up regression testing by removing redundant test cases. These approaches can be classified as adequate or inadequate. Adequate approaches reduce test suites so that they completely preserve the test requirements (e.g., code coverage) of the original test suites. Inadequate approaches produce reduced test suites that only partially preserve the test requirements. An inadequate approach is appealing when it leads to a greater reduction in test suite size at the expense of a small loss in fault-detection capability. We investigate a clustering-based approach for inadequate test suite reduction and compare it with well-known adequate approaches. Our investigation is founded on a public dataset and allows an exploration of trade-offs in test suite reduction. Results help a more informed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["55"]}
{"title": "Investigating the relation between lexical smells and change-and fault-proneness: an empirical study\n", "abstract": " Past and recent studies have shown that design smells which are poor solutions to recurrent design problems make object-oriented systems difficult to maintain, and that they negatively impact the class change- and fault-proneness. More recently, lexical smells have been introduced to capture recurring poor practices in the naming, documentation, and choice of identifiers during the implementation of an entity. Although recent studies show that developers perceive lexical smells as impairing program understanding, no study has actually evaluated the relationship between lexical smells and software quality as well as their interaction with design smells. In this paper, we detect 29 smells consisting of 13 design smells and 16 lexical smells in 30 releases of three projects: ANT, ArgoUML, and Hibernate. We analyze to what extent classes containing lexical smells have higher (or lower) odds to change or to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["55"]}
{"title": "Optimizing user experience in choosing android applications\n", "abstract": " In this paper, we present a recommendation system aimed at helping users and developers alike. We help users to choose optimal sets of applications belonging to different categories (eg. browsers, e-mails, cameras) while minimizing energy consumption, transmitted data, and maximizing application rating. We also help developers by showing the relative placement of their application's efficiency with respect to selected others. When the optimal set of applications is computed, it is leveraged to position a given application with respect to the optimal, median and worst application in its category (eg. browsers). Out of eight categories we selected 144 applications, manually defined typical execution scenarios, collected the relevant data, and computed the Pareto optimal front solving a multi-objective optimization problem. We report evidence that, on the one hand, ratings do not correlate with energy efficiency and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["55"]}
{"title": "Reverse engineering 4.7 million lines of code\n", "abstract": " The ITC\u0393\u00c7\u00c9Irst Reverse Engineering group was charged with analyzing a software application of approximately 4.7 million lines of C code. It was an old legacy system, maintained for a long time, on which several successive adaptive and corrective maintenance interventions had led to the degradation of the original structure. The company decided to re\u0393\u00c7\u00c9engineer the software instead of replacing it, because the complexity and costs of re\u0393\u00c7\u00c9implementing the application from scratch could not be afforded, and the associated risk could not be run. Several problems were encountered during re\u0393\u00c7\u00c9engineering, including identifying dependencies and detecting redundant functions that were not used anymore. To accomplish these goals, we adopted a conservative approach. Before performing any kind of analysis on the whole code, we carefully evaluated the expected costs. To this aim, a small but representative sample of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["55"]}
{"title": "Points-to analysis for program understanding\n", "abstract": " Program understanding activities are more difficult for programs written in languages (such as C) that heavily make use of pointers for data structure manipulation, because the programmer needs to build a mental model of the memory use and of the pointers to its locations. Pointers also pose additional problems to the tools supporting program understanding, since they introduce additional dependences that have to be accounted for. This paper extends the flow insensitive context insensitive points-to analysis (PTA) algorithm proposed by Steensgaard, to cover arbitrary combinations of pointer dereferences, array subscripts and field selections. It exhibits interesting properties, among which scalability resulting from the low complexity and good performances is one. The results of the analysis are valuable by themselves, as their graphical display represents the points-to links between locations. They are also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["55"]}
{"title": "Language model estimations and representations for real-time continuous speech recognition.\n", "abstract": " This paper compares different ways of estimating bigram language models and of representing them in a finite state network used by a beam-search based, continuous speech, and speaker independent HMM recognizer. Attention is focused on the n-gram interpolation scheme for which seven models are considered. Among them, the Stacked estimated linear interpolated model favourably compares with the best known ones. Further, two different static representations of the search space are investigated:\u0393\u00c7\u00a3linear\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3tree-based\u0393\u00c7\u00a5. Results show that the latter topology is better suited to the beam-search algorithm. Moreover, this representation can be reduced by a network optimization technique, which allows the dynamic size of the recognition process to be decreased by 60%. Extensive recognition experiments on a 10,000-word dictation task with four speakers are described in which an average word accuracy of 93% is achieved with real-time response.", "num_citations": "16\n", "authors": ["55"]}
{"title": "Benchmarks for traceability?\n", "abstract": " This position paper discusses the need for and the organization of a traceability benchmark. We establish the basic principles of organization of such a benchmark. We then observe the nature of traceability tasks in three areas of Software Engineering: independent verification and validation, software maintenance, and reverse engineering. Based on this, we derive the desiderata for a traceability benchmark addressing the needs of all three areas.", "num_citations": "15\n", "authors": ["55"]}
{"title": "Radiological reporting by speech recognition: the a. re. s. system.\n", "abstract": " Radiological reporting has already been identified as a field in which voice technologies can prove to be very useful. Recent progress in automatic speech recognition and in hardware and software technology makes it possible to build large-vocabulary, continuous speech, speaker-independent, real-time systems.In this paper a dictation system for radiology reporting, the A. Re. S. system, is presented. A. Re. S. is a \u0393\u00c7\u00a3software only\u0393\u00c7\u00a5 system which runs in real-time on an HP 715 workstation. It relies on an asynchronous and multi-process architecture in which speech decoding is performed by processes in pipeline.", "num_citations": "15\n", "authors": ["55"]}
{"title": "Methodological issues in a CMM Level 4 implementation\n", "abstract": " The Capability Maturity Model (CMM) developed by the Software Engineering Institute is an improvement paradigm. It provides a framework for assessing the maturity of software processes on a five\u0393\u00c7\u00c9level scale and guidelines that help improve software process and artifact quality. Moving toward CMM Level 4 and Level 5 is a very demanding task even for large software companies already accustomed to the CMM and ISO certifications. It requires, for example, quality monitoring, control, feedback, and process optimization. In fact, going beyond CMM Level 3 requires a radical change in the way projects are carried out and managed. It involves quantitative and statistical techniques to control software processes and quality, and it entails substantial changes in the way the organization approaches software life cycle activities. In this article, we describe the process changes, adaptation, integration, and tailoring, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["55"]}
{"title": "The impact of imperfect change rules on framework api evolution identification: an empirical study\n", "abstract": " Software frameworks keep evolving. It is often time-consuming for developers to keep their client code up-to-date. Not all frameworks have documentation about the upgrading process. Many approaches have been proposed to ease the impact of non-documented framework evolution on developers by identifying change rules between two releases of a framework, but these change rules are imperfect, i.e., not 100 % correct. To the best of our knowledge, there is no empirical study to show the usefulness of these imperfect change rules. Therefore, we design and conduct an experiment to evaluate their impact. In the experiment, the subjects must find the replacements of 21 missing methods in the new releases of three open-source frameworks with the help of (1) all-correct, (2) imperfect, and (3) no change rules. The statistical analysis results show that the precision of the replacements found by the subjects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["55"]}
{"title": "Microarray image addressing based on the radon transform\n", "abstract": " A fundamental step of microarray image analysis is the detection of the grid structure for the accurate localization of each spot, representing the state of a given gene in a particular experimental condition. This step is known as gridding or microarray addressing. Most of the available microarray gridding approaches require human intervention; for example, to specify landmarks, some points in the spot grid, or even to precisely locate individual spots. Automating this part of the process can allow high throughput analysis (Yang, Y, et al, 2002). This paper is aimed towards at the development fully automated procedures for the problem of automatic microarray gridding. Indeed, many of the automatic gridding approaches are based on two phases, the first aimed at the generation of an hypothesis consisting into a regular interpolating grid, whereas the second performs an adaptation of the hypothesis. Here we show that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["55"]}
{"title": "A distributed architecture for dynamic analyses on user-profile data\n", "abstract": " Combining static and dynamic information is highly relevant in many reverse engineering, program comprehension and maintenance tasks. To allow dynamic information reflecting software system usage scenarios, it should be collected during a long period of time, in a real user environment. This, however, poses several challenges. First and foremost, it is necessary to model the extraction of any relevant dynamic information from execution traces, thus avoiding to collect a large amount of unmanageable data. Second, we need a distributed architecture that allows to collect and compress such an information from geographically distributed users. We propose a probabilistic model for representing dynamic information, as well as a Web-service based distributed architecture for its collection and compression. The new architecture has been instantiated to collect inter-procedural program execution traces up to a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["55"]}
{"title": "Impact of function pointers on the call graph\n", "abstract": " Maintenance activities are made more difficult when pointers are heavily used in source code: the programmer needs to build a mental model of memory locations and of the way they are accessed by means of pointers, in order to comprehend the functionalities of the system. Although several points-to analysis algorithms have been proposed in literature to provide information about memory locations referenced by pointers, there are no quantitative evaluations of the impact of pointers on the overall program understanding activities. Program comprehension activities are usually supported by tools, providing suitable views of the source program. One of the most widely used code views is the call graph, a graph representing calls between functions in the given program. Unfortunately, when pointers, and especially function pointers, are heavily used in the code, the extracted call graph is highly inaccurate and thus\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["55"]}
{"title": "Experiencing real-life interactions with the experimental platform of MAIA\n", "abstract": " Over the years, automated vehicles have evolved from reliable yet rigidly constrained AGV to the fairly flexible ones of the HelpMate generation. If many problems related to the autonomous navigation of such vehicles have found solutions that appear to be adequate to many practical situations, yet their capacity of interacting with the users and the environment is still limited. In the present paper, the research work done at IRST in the framework of the Experimental Platform of MAIA is presented. In particular, a transport mission scenario (MAIA'94) is considered, in which autonomous navigation, speech recognition and planning skills represents three aspects of one and the same ability that the system can exhibit of establishing interactions with the external world in an reliable and autonomous fashion. IRST Technical Report 9406-27, June 1994. 1 Introduction Design and realization of artificial systems able to autonomously accomplish transport missions in relatively unstructured...", "num_citations": "13\n", "authors": ["55"]}
{"title": "A large-scale empirical study of code smells in JavaScript projects\n", "abstract": " JavaScript is a powerful scripting programming language that has gained a lot of attention this past decade. Initially used exclusively for client-side web development, it has evolved to become one of the most popular programming languages, with developers now using it for both client-side and server-side application development. Similar to applications written in other programming languages, JavaScript applications contain code smells, which are poor design choices that can negatively impact the quality of an application. In this paper, we perform a large-scale study of JavaScript code smells in server-side and client-side applications, with the aim to understand how they impact the fault-proneness of applications, and how they are evolved by the developers of the applications. We detect 12 types of code smells in 1807 releases of 15 popular JavaScript applications (i.e., express, grunt, bower, less.js\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["55"]}
{"title": "Efficient refactoring scheduling based on partial order reduction\n", "abstract": " Anti-patterns are poor solutions to design problems that make software systems hard to understand and to extend. Components involved in anti-patterns are reported to be consistently related to high changes and faults rates. Developers are advised to perform refactoring to remove anti-patterns, and consequently improve software design quality and reliability. However, since the number of anti-patterns in a system can be very large, the process of manual refactoring can be overwhelming. To assist a software engineer who has to perform this task, we propose a novel approach RePOR (Refactoring approach based on Partial Order Reduction). We perform a case study with five open source systems to assess the performance of RePOR against two well-known metaheuristics (Genetic Algorithm, and Ant Colony Optimization), one conflict-aware refactoring approach and, a new approach based on sampling (Sway\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["55"]}
{"title": "Sub-graph mining: identifying micro-architectures in evolving object-oriented software\n", "abstract": " Developers introduce novel and undocumented micro-architectures when performing evolution tasks on object-oriented applications. We are interested in understanding whether those organizations of classes and relations can bear, much like cataloged design and anti-patterns, potential harm or benefit to an object-oriented application. We present SGFinder, a sub-graph mining approach and tool based on an efficient enumeration technique to identify recurring micro-architectures in object-oriented class diagrams. Once SGFinder has detected instances of micro-architectures, we exploit these instances to identify their desirable properties, such as stability, or unwanted properties, such as change or fault proneness. We perform a feasibility study of our approach by applying SGFinder on the reverse-engineered class diagrams of several releases of two Java applications: ArgoUML and Rhino. We characterize and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["55"]}
{"title": "SQL-injection security evolution analysis in PHP\n", "abstract": " Web sites are often a mixture of static sites and programs that integrate relational databases as a back-end. Software that implements Web sites continuously evolve to meet ever-changing user needs. As a Web sites evolve, new versions of programs, interactions and functionalities are added and existing ones are removed or modified. Web sites require configuration and programming attention to assure security, confidentiality, and trustiness of the published information. During evolution of Web software, from one version to the next one, security flaws may be introduced, corrected, or ignored. This paper presents an investigation of the evolution of security vulnerabilities as detected by propagating and combining granted authorization levels along an inter-procedural control flow graph (CFG) together with required security levels for DB accesses with respect to SQL-injection attacks. The paper reports results about\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["55"]}
{"title": "Grand challenges of traceability: The next ten years\n", "abstract": " In 2007, the software and systems traceability community met at the first Natural Bridge symposium on the Grand Challenges of Traceability to establish and address research goals for achieving effective, trustworthy, and ubiquitous traceability. Ten years later, in 2017, the community came together to evaluate a decade of progress towards achieving these goals. These proceedings document some of that progress. They include a series of short position papers, representing current work in the community organized across four process axes of traceability practice. The sessions covered topics from Trace Strategizing, Trace Link Creation and Evolution, Trace Link Usage, real-world applications of Traceability, and Traceability Datasets and benchmarks. Two breakout groups focused on the importance of creating and sharing traceability datasets within the research community, and discussed challenges related to the adoption of tracing techniques in industrial practice. Members of the research community are engaged in many active, ongoing, and impactful research projects. Our hope is that ten years from now we will be able to look back at a productive decade of research and claim that we have achieved the overarching Grand Challenge of Traceability, which seeks for traceability to be always present, built into the engineering process, and for it to have \"effectively disappeared without a trace\". We hope that others will see the potential that traceability has for empowering software and systems engineers to develop higher-quality products at increasing levels of complexity and scale, and that they will join the active community of Software and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["55"]}
{"title": "A static measure of a subset of intra-procedural data flow testing coverage based on node coverage\n", "abstract": " In the past years, a number of research works, which have been mostly based on pre and post dominator analysis, have been presented about nding subsets of nodes and edges called unrestricted subsets\" such that their traversal during execution if feasible exercises respectively all feasible nodes and edges in a Control Flow Graph CFG.This paper presents an approach to statically measure a subset of intra-procedural data ow all uses\" coverage obtained by exercising an unrestricted subset\" of nodes during testing. This measure indicates the possible degree of data ow testing obtainable while using a weaker test coverage criteria.", "num_citations": "11\n", "authors": ["55"]}
{"title": "Points to analysis for program understanding\n", "abstract": " Real world programs (in languages like C) heavily make use of pointers. Program understanding activities are thus made more difficult, since pointers affect the memory locations that are referenced in a statement, and also the functions called by a statement, when function pointers are used. The programmer needs to build a mental model of the memory use and of the pointers to its locations, in order to comprehend the functionalities of the system. This paper presents an efficient flow insensitive context insensitive points-to analysis algorithm capable of dealing with the features of the C code. It is extremely promising with regard to scalability, because of the low complexity. The results are valuable by themselves, as their graphical display represents the points to links between locations. They are also integrated with other program understanding techniques like, e.g., call graph construction, slicing, plan recognition\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["55"]}
{"title": "Radiological reporting based on voice recognition\n", "abstract": " Speech recognition has proved to be a natural interaction modality and an effective technology for medical reporting, in particular in the speciality of radiology. High-volume text creation requirement and the complex structure of these texts make voice technologies useful. By employing speech, professionals in the field can generate reports and do so at a speed that approaches traditional dictation methods.             However, the integration of speech recognition in a user interface creates new problems: speech recognizers may introduce errors and moreover they should be adaptable to spoken language variations.             This paper describes a radiological reporting system and the related motivations for the use of the speech modality. A preliminary evaluation of the system has shown that, on average, although text recalling functions and keyword shortcuts are available, more than two thirds of a radiological\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["55"]}
{"title": "Comprehension of ads-supported and paid Android applications: are they different?\n", "abstract": " The Android market is a place where developers offer paid and-or free apps to users. Free apps can follow the freemium or the ads-business model. While the former offers less features and the user is charged for unlocking additional features, the latter includes ads to allow developers to get a revenue. Free apps are interesting to users because they can try them immediately without incurring a monetary cost. However, free apps often have limited features and-or contain ads when compared to their paid counterparts. Thus, users may eventually need to pay to get additional features and-or remove ads. While paid apps have clear market values, their ads-supported versions are not entirely free because ads have an impact on performance. The hidden costs of ads, and the recent possibility to form family groups in Google Play to share purchased apps, make it difficult for developers and users to balance between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["55"]}
{"title": "Studying software evolution of large object\u0393\u00c7\u00c9oriented software systems using an ETGM algorithm\n", "abstract": " Analyzing and understanding the evolution of large object\u0393\u00c7\u00c9oriented software systems is an important but difficult task in which matching algorithms play a fundamental role. An error\u0393\u00c7\u00c9tolerant graph matching (ETGM) algorithm can identify evolving classes that maintain a stable structure of relations (associations, inheritances, and aggregations) with other classes and thus likely constitute the backbone of the system. Therefore, to study the evolution of class diagrams, we first develop a novel ETGM algorithm, which improves the performance of our previous algorithm. Second, we describe the process of building an oracle to validate the results of our approach to solve the class diagram evolution problem. Third, we report for the new algorithm the impact of its parameters on the F\u0393\u00c7\u00c9measure summarizing precision (quantifying the exactness of the solution) and recall (quantifying the completeness of the solution). Finally\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["55"]}
{"title": "Factors impacting the inputs of traceability recovery approaches\n", "abstract": " In requirement engineering, researchers have proposed various tractability recovery approaches. To the best of our knowledge, all traceability recovery approaches have low precision and recall. Our main claim in this chapter is that there exist factors that impact the traceability approaches\u0393\u00c7\u00d6 inputs, in particular source document, target document, and experts\u0393\u00c7\u00d6 opinion, that cause low precision and recall. In this chapter, we pursue four objectives: first, to identify and document factors that impact traceability recovery approaches\u0393\u00c7\u00d6 inputs; second, to identify metrics/tools to measure/improve the quality of the inputs with respect to the identified factors, third, to provide precautions to control these factors, and, fourth, to empirically prove and quantify the effect of one of these factors\u0393\u00c7\u00f4expert\u0393\u00c7\u00d6s programming knowledge\u0393\u00c7\u00f4on the traceability recovery approaches\u0393\u00c7\u00d6 inputs. To achieve the first two objectives, we perform an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["55"]}
{"title": "Modeling maintenance effort by means of dynamic systems\n", "abstract": " The dynamic evolution of ecological systems in which predators and prey compete for survival has been investigated by applying suitable mathematical models. Dynamic systems theory provides a useful way to model interspecies competition and thus the evolution of predators and prey populations. This kind of mathematical framework has been shown to be well suited to describe evolution of economical systems as well, where instead of predators and prey there are consumers and resources. This paper suggests how dynamic systems could be usefully applied to the maintenance context, namely to model the dynamic evolution of the maintenance effort. When maintainers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find out any new defect has an initial increase, followed by a decline, in a similar way to prey and predator populations. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["55"]}
{"title": "Fragile base-class problem, problem?\n", "abstract": " The fragile base-class problem (FBCP) has been described in the literature as a consequence of \u0393\u00c7\u00a3misusing\u0393\u00c7\u00a5 inheritance and composition in object-oriented programming when (re)using frameworks. Many research works have focused on preventing the FBCP by proposing alternative mechanisms for reuse, but, to the best of our knowledge, there is no previous research work studying the prevalence and impact of the FBCP in real-world software systems. The goal of our work is thus twofold: (1) assess, in different systems, the prevalence of micro-architectures, called FBCS, that could lead to two aspects of the FBCP, (2) investigate the relation between the detected occurrences and the quality of the systems in terms of change and fault proneness, and (3) assess whether there exist bugs in these systems that are related to the FBCP. We therefore perform a quantitative and a qualitative study. Quantitatively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["55"]}
{"title": "Factors impacting the inputs of traceability recovery approaches\n", "abstract": " In requirement engineering, researchers have proposed various tractability recovery approaches. To the best of our knowledge, all traceability recovery approaches have low precision and recall. Our main claim in this chapter is that there exist factors that impact the traceability approaches\u0393\u00c7\u00d6 inputs, in particular source document, target document, and experts\u0393\u00c7\u00d6 opinion, that cause low precision and recall. In this chapter, we pursue four objectives: first, to identify and document factors that impact traceability recovery approaches\u0393\u00c7\u00d6 inputs; second, to identify metrics/tools to measure/improve the quality of the inputs with respect to the identified factors, third, to provide precautions to control these factors, and, fourth, to empirically prove and quantify the effect of one of these factors\u0393\u00c7\u00f6expert\u0393\u00c7\u00d6s programming knowledge\u0393\u00c7\u00f6on the traceability recovery approaches\u0393\u00c7\u00d6 inputs. To achieve the first two objectives, we perform an incremental literature review of traceability recovery approaches and identify and document three key inputs and the seven factors impacting these inputs, out of 12 identified factors. We analyse the reported results in literature for the identified factors to address our third objective. We conduct an empirical study to", "num_citations": "9\n", "authors": ["55"]}
{"title": "Towards the integration of cvs repositories, bug reporting and source code meta-models\n", "abstract": " Concurrent Versioning System (CVS) repositories and bug tracking systems are valuable sources of information to study the evolution of large open source software systems. However, being conceived for specific purposes, ie, to support the development or trigger maintenance activities, they do neither allow an easy information browsing nor support the study of software evolution. For example, queries such as locating and browsing the faultiest methods are not provided.This paper addresses such issues and proposes an approach and a framework to consistently merge information extracted from source code, CVS repositories and bug reports. Our information representation exploits the property concepts of the FAMIX information exchange meta-model, allowing to represent, browse, and query, at different level of abstractions, the concept of interest. This allows the user to navigate back and forth from CVS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["55"]}
{"title": "An approach for reverse engineering of web-based applications\n", "abstract": " The new possibilities offered by WEB applications are pervasively and radically changing several areas. WEB applications, compared to WEB sites, offer substantially greater opportunities: a WEB application provides the WEB user with a means to modify the site status. WEB applications must cope with an extremely short develop-ment/evolution life cycle. Usually, they are implemented without producing any useful documentation for subsequent maintenance and evolution, thus compromising the desired high level of flexibility, maintainability, and adaptability that is de-facto necessary to compete and survive to mar-ket shakeout. This paper presents an approach inspired by the reverse engineering arena and a tool prototype supporting WEB ap-plication reverse engineering activities, to help maintain, comprehend and evolve WEB applications. The approach defines a set of abstract views, modeled using UML dia\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["55"]}
{"title": "On the prevalence, impact, and evolution of SQL code smells in data-intensive systems\n", "abstract": " Code smells indicate software design problems that harm software quality. Data-intensive systems that frequently access databases often suffer from SQL code smells besides the traditional smells. While there have been extensive studies on traditional code smells, recently, there has been a growing interest in SQL code smells. In this paper, we conduct an empirical study to investigate the prevalence and evolution of SQL code smells in open-source, data-intensive systems. We collected 150 projects and examined both traditional and SQL code smells in these projects. Our investigation delivers several important findings. First, SQL code smells are indeed prevalent in data-intensive software systems. Second, SQL code smells have a weak co-occurrence with traditional code smells. Third, SQL code smells have a weaker association with bugs than that of traditional code smells. Fourth, SQL code smells are more\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["55"]}
{"title": "Optimizing threads schedule alignments to expose the interference bug pattern\n", "abstract": " Managing and controlling interference conditions in multi-threaded programs has been an issue of worry for application developers for a long time. Typically, when write events from two concurrent threads to the same shared variable are not properly protected, an occurrence of the interference bug pattern could be exposed. We propose a mathematical formulation and its resolution to maximize the possibility of exposing occurrences of the interference bug pattern. We formulate and solve the issue as an optimization problem that gives us (1) the optimal position to inject a delay in the execution flow of a thread and (2) the optimal duration for this delay to align at least two different write events in a multi-threaded program. To run the injected threads and calculate the thread execution times for validating the results, we use a virtual platform modelling a perfectly parallel system. All the effects due to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["55"]}
{"title": "Divide-by-zero exception raising via branch coverage\n", "abstract": " In this paper, we discuss how a search-based branch coverage approach can be used to design an effective test data generation approach, specifically targeting divide-by-zero exceptions. We first propose a novel testability transformation combining approach level and branch distance. We then use different search strategies, i.e. hill climbing, simulated annealing, and genetic algorithm, to evaluate the performance of the novel testability transformation on a small synthetic example as well as on methods known to throw divide-by-zero exceptions, extracted from real world systems, namely Eclipse and Android. Finally, we also describe how the test data generation for divide-by-zero exceptions can be formulated as a constraint programming problem and compare the resolution of this problem with a genetic algorithm in terms of execution time. We thus report evidence that genetic algorithm using our novel\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["55"]}
{"title": "A Google-inspired error-correcting graph matching algorithm\n", "abstract": " Graphs and graph algorithms are applied in many different areas including civil engineering, telecommunications, bio-informatics and software engineering. While exact graph matching is grounded on a consolidated theory and has well known results, approximate graph matching is still an open research subject. This paper presents an error tolerant approximated graph matching algorithm based on tabu search using the Google-like PageRank algorithm. We report preliminary results obtained on 2 graph data benchmarks. The first one is the TC-15 database [14], a graph data base at the University of Naples, Italy. These graphs are limited to exact matching. The second one is a novel data set of large graphs generated by randomly mutating TC-15 graphs in order to evaluate the performance of our algorithm. Such a mutation approach allows us to gain insight not only about time but also about matching accuracy.", "num_citations": "8\n", "authors": ["55"]}
{"title": "Variable\u0393\u00c7\u00c9precision reaching definitions analysis\n", "abstract": " Ascertaining the reaching definitions from the source code can give views of the linkages in that source code. These views can aid source code analyses, such as impact analysis and program slicing, and can assist in the reverse engineering and re\u0393\u00c7\u00c9engineering of large legacy systems. Maintainers like to do such activities interactively and value fast responses from program analysis tools. Therefore the control of the trade\u0393\u00c7\u00c9off between accuracy and efficiency should be given to the maintainer. Since some real world programs, especially in languages like C, make much use of pointers, and efficient points\u0393\u00c7\u00c9to analysis should be integrated within the computation of the data dependencies during the process of ascertaining the reaching definitions. This paper proposes three different approaches to the analysis of the reaching definitions based on different levels of precision, reflecting differences in their sensitivity to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["55"]}
{"title": "Estimating Size and E ort for Object Oriented Systems\n", "abstract": " This paper presents a method for estimating the development size and e ort of object oriented software. In an approach analogous to function points, counts of the elements in a static object model are combined to produce a composite measure. Rules are proposed for counting\\Object Oriented Function Points\" from an object model, and several questions are identi ed for empirical research.", "num_citations": "8\n", "authors": ["55"]}
{"title": "Enhancing a tabu algorithm for approximate graph matching by using similarity measures\n", "abstract": " In this paper, we investigate heuristics in order to solve the Approximated Matching Problem (AGM). We propose a tabu search algorithm which exploits a simple neighborhood but is initialized by a greedy procedure which uses a measure of similarity between the vertices of the two graphs. The algorithm is tested on a large collection of graphs of various sizes (from 300 vertices and up to 3000 vertices) and densities. Computing times range from less than 1 second up to a few minutes. The algorithm obtains consistently very good results, especially on labeled graphs. The results obtained by the tabu algorithm alone (without the greedy procedure) were very poor, illustrating the importance of using vertex similarity during the early steps of the search process.", "num_citations": "7\n", "authors": ["55"]}
{"title": "Error correcting graph matching application to software evolution\n", "abstract": " Graph representations and graph algorithms are widely adopted to model and resolve problems in many different areas from telecommunications, to bio-informatics, to civil and software engineering. Many software artefacts such as the class diagram can be thought of as graphs and thus, many software evolution problems can be reformulated as a graph matching problem.In this paper, we investigate the applicability of an error-correcting graph matching algorithm to object-oriented software evolution and report results, obtained on a small system - the Latazza application -, supporting applicability and usefulness of our proposal.", "num_citations": "7\n", "authors": ["55"]}
{"title": "3rd international workshop on traceability in emerging forms of software engineering (tefse 2005)\n", "abstract": " Establishing and maintaining traceability links and consistency between software artifacts produced or modified in the software life-cycle are costly and tedious activities that are crucial but frequently neglected in practice. Traceability between the free text documentation associated with the development and maintenance cycle of a software system and its source code are crucial in a number of tasks such as program comprehension, software maintenance, and software verification & validation. Finally, maintaining traceability links between subsequent releases of a software system is important for evaluating relative source code deltas, highlighting effort/code variation inconsistencies, and assessing the change history. The main theme of the workshop is focused on understanding and defining the foundations for consistency and change management of software systems within the scope of artifact-to-artifact (model-to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["55"]}
{"title": "Telephone transmission of 20-channel digital electroencephalogram using lossless data compression\n", "abstract": " Background The use of telecommunications for computer-assisted transmission of neurophysiological signals is a relatively new practice. With the development of digital technology, it is now possible to record electroencephalograms (EEGs) in digital form. Previous reports have demonstrated the possibility of real-time telephone transmission of a limited number of EEG channels.   Objectives To assess the effectiveness of specific data-compression software to improve the transmission of digital 20-channel EEG records over ordinary public telephone lines.   Methods A prototype system was built to transmit digital EEG signals from one computer to another using two 14.4-kbps modems and proprietary lossless data-compression software.   Results Forty compressed digital EEG records of 20 channels each were sent from different locations at variable distances using \"plain old telephone service\" (POTS). The mean\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["55"]}
{"title": "Techniques for robust recognition in restricted domains.\n", "abstract": " This paper describes an Automatic Speech Understanding (ASU) system used in a human-robot interface for the remote control of a mobile robot. The intended application is that of an operator issuing telecontrol commands to one or more robots from a remote workstation. ASU is supposed to be performed with spontaneous continuous speech and quasi real time conditions. Training and testing of the system was based on speech data collected by means of Wizard of Oz simulations. Two kinds of robustness factors are introduced: the first is a recognition error-tolerant approach to semantic interpretation, the second is based on a technique for evaluating the reliability of the ASU system output with respect to the input utterance. Preliminary results are 90.9% of correct semantic interpretations, and 89.1% of correct detection of out-of-domain sentences at the cost of rejecting 16.4% of correct in-domain sentences.", "num_citations": "7\n", "authors": ["55"]}
{"title": "Tools for Development, Test and Analysis of ASRs.\n", "abstract": " The aim of this work is to present a set of tools that can be helpful in the design, development and test of Automatic Speech Recognition (ASR) systems based on DSP boards. The architecture of the system presented allows the user to concentrate on ASR design problems rather than on the DSP board interfacing problem. This is obtained through the introduction of an intermediate module, called manager, that guides the communication between the acquisition module (DSP board) and a set of processes. Each process performs a specific task like data I/O from DSP board or to/from other devices, ASR testing, signal plotting, and so on. Finite State Networks (FSNs) was used to represent both the manager architecture and the languages accepted by various ASRs. In order to reduce both the time spent for defining the language and the time and space required by ASR algorithms, a package of routines was developed for defining, optimizing and transforming FSNs. The tools depicted above have been verifi...", "num_citations": "7\n", "authors": ["55"]}
{"title": "Adequate vs. inadequate test suite reduction approaches\n", "abstract": " Context: Regression testing is an important activity that allows ensuring the correct behavior of a system after changes. As the system grows, the time and resources to perform regression testing increase. Test Suite Reduction (TSR) approaches aim to speed up regression testing by removing obsolete or redundant test cases. These approaches can be classified as adequate or inadequate. Adequate TSR approaches reduce test suites and completely preserve test requirements (e.g., covered statements) of the original test suites. Inadequate TSR approaches do not preserve test requirements. The percentage of satisfied test requirements indicates the inadequacy level.Objective: We compare some state-of-the-art adequate and inadequate TSR approaches with respect to the size of reduced test suites and their fault-detection capability. We aim to increase our body of knowledge on TSR approaches by comparing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["55"]}
{"title": "Exact search-space size for the refactoring scheduling problem\n", "abstract": " Ouni et al. \u0393\u00c7\u00a3Maintainability defects detection and correction: a multi-objective approach\u0393\u00c7\u00a5 proposed a search-based approach for generating optimal refactoring sequences. They estimated the size of the search space for the refactoring scheduling problem using a formulation that is incorrect; the search space is estimated to be too much larger than it is. We provide in this paper the exact expression for computing the number of possible refactoring sequences of a software system. This could be useful for researchers and practitioners interested in developing new approaches to automate refactoring.", "num_citations": "6\n", "authors": ["55"]}
{"title": "Noise in Mylyn interaction traces and its impact on developers and recommendation systems\n", "abstract": " Interaction traces (ITs) are developers\u0393\u00c7\u00d6 logs collected while developers maintain or evolve software systems. Researchers use ITs to study developers\u0393\u00c7\u00d6 editing styles and recommend relevant program entities when developers perform changes on source code. However, when using ITs, they make assumptions that may not necessarily be true. This article assesses the extent to which researchers\u0393\u00c7\u00d6 assumptions are true and examines noise in ITs. It also investigates the impact of noise on previous studies. This article describes a quasi-experiment collecting both Mylyn ITs and video-screen captures while 15 participants performed four realistic software maintenance tasks. It assesses the noise in ITs by comparing Mylyn ITs and the ITs obtained from the video captures. It proposes an approach to correct noise and uses this approach to revisit previous studies. The collected data show that Mylyn ITs can miss, on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["55"]}
{"title": "Evaluating the use of design patterns during program comprehension\u0393\u00c7\u00f4experimental setting\n", "abstract": " In theory, there is no difference between theory and practice. But, in practice, there is.", "num_citations": "6\n", "authors": ["55"]}
{"title": "A. RE. S.: an interface for automatic reporting by speech\n", "abstract": " The project and the first prototype of an interface for dictating, recording and printing radiological reports is presented. The most important feature of this interface is multimodality. The radiologist may choose among speech, keyboard and mouse to generate a report. If he is busy with hands and eyes, he can dictate most of the report, when holding and analyzing the radiographs. He can type the text and select some actions from a menu or click some meaningful icons when he has his hands free. The choice of the input modality depends on the \u0393\u00c7\u00a3freedom\u0393\u00c7\u00a5 of the radiologist and the easiness and quickness of communication. The motivation for such a project, and the study of the impact of this system on the organisation of the radiologic department in terms of possible improvements on the reporting service, are also presented. The constraints on the radiologist-computer speech communication are analyzed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["55"]}
{"title": "Comparison and evaluation of clone detection tools\n", "abstract": " SOFTWARE ENGINEERING Page 1 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING A publication of the IEEE Computer Society SEPTEMBER 2007 VOLUME 33 NUMBER 9 IESEDJ (ISSN 0098-5589) PAPERS Comparison and Evaluation of Clone Detection Tools S. Bellon, R. Koschke, G. Antoniol, J. Krinke, and E. Merlo ............................................................................................... 577 An Extensible Metamodel for Program Analysis D. Strein, R. Lincke, J. Lundberg, and W. L\u251c\u2562we ................................................................................................................. 592 Finding Clones with Dup: Analysis of an Experiment BS Baker .......................................................................................................................................................................... 608 Systematic Testing of Model-Based Code Generators I. St\u251c\u255drmer, M. Conrad, H. D\u251c\u2562rr, and P. Pepper ................................................................................................................... 622 COMMENTS Comments on \"Data Mining \" \u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["55"]}
{"title": "On the use of similarity metrics for approximate graph matching\n", "abstract": " In this paper, we investigate heuristics for approximate graph matching, in particular its formulation as a Maximum Common Edge Subgraph problem. Our experiments suggest that a small percentage of accurate node matches is sufficient to get near optimal solutions using a simple hill-climbing. The real challenge could then be to somehow drag the search in this zone. For this purpose, we discuss the use of similarity measures. We present and assess the performance of two similarity measures. Very good results were obtained on labeled graphs.", "num_citations": "5\n", "authors": ["55"]}
{"title": "Code siblings: Phenotype evolution\n", "abstract": " Tyrell and Roy Batty are androids, ie, replicants of the same series. Rachel is a caring woman who does not know that she is a replicant while Roy is a slave who kills in an attempt to revert a failsafe system implanted in replicants to limit their life-span to four years. Both are clones of a same series, yet become very different individuals. Our position focuses on the code migration between different software systems and the subsequent evolution of code clones. A piece of code\u0393\u00c7\u00f6often an entire file or function\u0393\u00c7\u00f6can be copied from one system to another for many different reasons, including adding features already implemented in the other system, the need to fix a bug relying on a known and robust implementation, or the migration of a developer from one project to another. Our goal is to detect clones across systems, study the", "num_citations": "5\n", "authors": ["55"]}
{"title": "Appendix A: Glossary of Traceability Terms (v1. 0)\n", "abstract": " Automated traceability\u0393\u00c7\u00f4The potential for automated tracing. Automated tracing\u0393\u00c7\u00f4When traceability is established via automated techniques, methods and tools. Currently, it is the decision as to among which artifacts to create and maintain trace links that is automated. Backward traceability\u0393\u00c7\u00f4The potential for backward tracing. Backward tracing\u0393\u00c7\u00f4In software and systems engineering contexts, the term is commonly used when the tracing follows antecedent steps in a developmental path, which is not necessarily a chronological path, such as backward from code through design to requirements. Note that the trace links themselves could be used in either a primary or reverse trace link direction, dependent upon the specification of the participating traces.This glossary is reproduced material from Center of Excellence for Software Traceability Technical Report# CoEST-2011-001, with permission. An up to date version of this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["55"]}
{"title": "Change impact analysis: an earthquake metaphor\n", "abstract": " Impact analysis is crucial to make decisions among different alternative implementations and to anticipate future maintenance tasks. Several approaches were proposed to identify software artefacts being affected by a change. However, to the best of our knowledge, none of these approaches have been used to study the scope of changes in a program. Yet, this information would help developers assess their change efforts and perform more adequate changes. Thus, we present a metaphor inspired by seismology and propose a mapping between the concepts of seismology and software evolution. We show the applicability and usefulness of our metaphor using Rhino and Xerces-J.", "num_citations": "4\n", "authors": ["55"]}
{"title": "A novel process and its implementation for the multi-objective miniaturization of software\n", "abstract": " Smart phones, gaming consoles, wireless routers are ubiquitous; the increasing diffusion of such devices with limited resources, together with society\u0393\u00c7\u00d6s unsatiated appetite for new applications, pushes companies to miniaturize their programs. Miniaturizing a program for a hand-held device is a time-consuming task often requiring complex decisions. Companies must accommodate conflicting constraints: customers\u0393\u00c7\u00d6 satisfaction may be in conflict with a device\u0393\u00c7\u00d6s limited storage and memory. This paper proposes a process, MoMS, for the multi-objective miniaturization of software to help developers miniaturize programs while satisfying multiple conflicting constraints. The process directs: the elicitation of customer pre-requirements, their mapping to program features, and the selection of the features to port. We present two case studies based on Pooka, an email client, and SIP Communicator, an instant messenger, to demonstrate that MoMS supports miniaturization and helps reduce effort by 77%, on average, over a manual approach.", "num_citations": "4\n", "authors": ["55"]}
{"title": "Classification of digital terrain models through fuzzy clustering: An application\n", "abstract": " Experts classifications of spatial data are strongly affected by subjectivity and rigidity of rules. They do not take into account, in a quantitative way, the overlap of classes and as a major consequence, their classifications are often not reproducibles. To overcome this subjectivity, exploratory techniques can suggest a coherent set of rules that will produce suitable polythetic and overlapping classes. The aim of this paper is to validate the unsupervised method of fuzzy clustering applied to classification of raster spatial data.", "num_citations": "4\n", "authors": ["55"]}
{"title": "The mobile robot of MAIA: actions and interactions in a real life scenario\n", "abstract": " Over the years, automated vehicles have evolved from reliable yet rigidly constrained AGVs to the fairly flexible ones of the Help Mate generation. It is interesting to observe that while such systems exhibit quite respectful autonomous navigation capabilities, their capacity of interacting with the users and the environment is still limited.               In this paper, the research work done at IRST in the framework of the Experimental Platform of MAIA is presented. In particular, a transport mission scenario (MAIA \u0393\u00c7\u00d694) is considered, in which autonomous navigation, speech recognition and planning represent three aspects of the same capacity that the system has of interacting with the external world in a reliable and autonomous fashion.", "num_citations": "4\n", "authors": ["55"]}
{"title": "Late shoulder-arm morbidity using ultrasound scalpel in axillary dissection for breast cancer: a retrospective analysis\n", "abstract": " BackgroundWe aimed to assess whether the use of the harmonic scalpel (HS) in axillary dissection would reduce long-term shoulder-arm morbidity compared to traditional instruments (TIs).Materials and methodsA retrospective analysis on 180 patients who underwent standard axillary dissection for breast cancer between 2007 and 2015 was carried out. All patients were evaluated for postoperative pain, impairment of shoulder-arm mobility, seroma formation in axilla, frozen shoulder, and lymphedema.ResultsHS procedure on average was 50% shorter compared to the TI technique. HS reduced by 4.5 times the risk of axillary seroma. TIs were associated with 4 times higher risk of developing a painful frozen shoulder.ConclusionsUse of the HS was associated with reduced costs and a positive long-term effect on shoulder-arm morbidity. Axillary seromas are not the only reason of later postoperative shoulder-arm\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["55"]}
{"title": "Requiem for software evolution research: a few steps toward the creative age\n", "abstract": " Nowadays almost every company depends on software technologies to function, the challenge is that the technologies and software applications are constantly changing and adapting to the needs of users. This process of change is risky, since unplanned and undisciplined changes in any software system of realistic size risk degrading the quality of the software and producing unexpected side effects. The need for disciplined, intelligent, cost-effective software change and evolution is an urgent technological challenge in the software engineering field.", "num_citations": "3\n", "authors": ["55"]}
{"title": "Design pattern detection for reverse engineering\n", "abstract": " The main goal of the workshop is to address the issues related to design patterns identification for design recovery focusing on the role of the reverse engineering in identifying the sub-elements of the design patterns that can improve their detection.", "num_citations": "3\n", "authors": ["55"]}
{"title": "Investigating Java type analyses for the receiver-classes testing criterion\n", "abstract": " This paper investigates the precision of three linear-complexity type analyses for Java software: Class Hierarchy Analysis (CHA), Rapid Type Analysis (RTA) and Variable Type Analysis (VTA). Precision is measured relative to class targets. Class targets results are useful in the context of the receiver-classes criterion, which is an object-oriented testing strategy that aims to exercise every possible class binding of the receiver object reference at each dynamic call site. In this context, using a more precise analysis decreases the number of infeasible bindings to cover, thus it reduces the time spent on conceiving test data sets. This paper also introduces two novel variations to VTA, called the iteration and intersection variants. We present experimental results about the precision of CHA, RTA and VTA on a set of 17 Java programs, corresponding to a total of 600 kLOC of source code. Results show that, on average, RTA\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["55"]}
{"title": "Recovery of traceability links in software artifacts and systems\n", "abstract": " Untitled Page 1 Page 2 Page 3 Page 4 Page 5 Page 6 Page 7 Page 8 Page 9 Page 10 Page 11 Page 12 Page 13 Page 14 Page 15 Page 16 Page 17 Page 18 Page 19 Page 20 Page 21 Page 22 Page 23 Page 24 Page 25 Page 26 Page 27 Page 28 Page 29 Page 30 Page 31 Page 32 Page 33 Page 34 Page 35 Page 36 Page 37 Page 38 Page 39 Page 40 Page 41 Page 42 Page 43 Page 44 Page 45 Page 46 Page 47 Page 48 Page 49 Page 50 Page 51 Page 52 Page 53 Page 54 Page 55 Page 56 Page 57 Page 58 Page 59 Page 60 Page 61 Page 62 Page 63 Page 64 Page 65 Page 66 Page 67 Page 68 Page 69 Page 70 Page 71 Page 72 Page 73 Page 74 Page 75 Page 76 Page 77 Page 78 Page 79 Page 80 Page 81 Page 82 Page 83 Page 84 Page 85 Page 86 Page 87 Page 88 Page 89 Page 90 Page 91 Page 92 Page 93 Page 94 Page 95 Page 96 Page 97 Page 98 Page 99 Page 100 Page 101 Page \u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["55"]}
{"title": "GASSER: Genetic Algorithm for teSt Suite Reduction\n", "abstract": " Background. Regression testing is a practice that ensures a System Under Test (SUT) still works as expected after changes. The simplest regression testing approach is Retest-all, which consists of re-executing the entire Test Suite (TS) on the new version of the SUT. When SUT and its TS grow in size, applying Retest-all could be expensive. Test Suite Reduction (TSR) approaches would allow overcoming the above-mentioned issues by reducing TSs while preserving their fault-detection capability.Aim. In this paper, we introduce GASSER (Genetic Algorithm for teSt SuitE Reduction), a new approach for TSR based on a multi-objective evolutionary algorithm, namely NSGA-II.Method. GASSER reduces TSs by maximizing statement coverage and diversity of test cases, and by minimizing the size of the reduced TSs.Results. The preliminary study shows that GASSER reduces more the TS size with a small effect on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["55"]}
{"title": "Guest editorial: reverse engineering\n", "abstract": " Reverse engineering aims at obtaining high-level representations of software systems from existing low-level artifacts, such as binaries, source code, execution traces or historical information. Reverse engineering methods and technologies play an important role in many software engineering tasks and quite often are the only way to get an understanding of large and complex software systems. Given for example the task to understand certain aspects of the test code in large software systems, such as Eclipse. How would you figure out the answers to the following questions:\u0393\u00c7\u00f4How are the tests in Eclipse organized and where is the test code located?\u0393\u00c7\u00f4What is tested by a unit test, test plug-in, and a whole test suite\u0393\u00c7\u00f6what is not?\u0393\u00c7\u00f4What was the reason for running these tests?\u0393\u00c7\u00f4What influences the test execution environment?", "num_citations": "2\n", "authors": ["55"]}
{"title": "Insider threat resistant SQL-injection prevention in PHP\n", "abstract": " Web sites are either static sites, programs, or databases. Very often they are a mixture of these three aspects integrating relational databases as a back-end. Web sites require configuration and programming attention to assure security, confidentiality, and trustiness of the published information. SQL-injection attacks rely on some weak validation of textual input used to build database queries. Maliciously crafted input may threaten the confidentiality and the security policies of Web sites relying on a database to store and retrieve information. Furthermore, insiders may introduce malicious code in a Web application, code that, when triggered by some specific input, for example, would violate security policies. This paper presents an original approach that combines static analysis, dynamic analysis, and code reengineering to automatically protect applications written in PHP from both malicious input (outsider threats) and malicious code (insider threats) that carry SQLinjection attacks. The paper also reports preliminary results about experiments performed on an old SQL-injection prone version of phpBB (version 2.0.0, 37193 LOC of PHP version 4.2.2 code). Results show that our approach successfully improved phpBB-2.0.0 resistance to SQLinjection attacks.", "num_citations": "2\n", "authors": ["55"]}
{"title": "A language-independent framework for software miniaturization\n", "abstract": " One of the undesired effects of software evolution is the proliferation of unused components, or components unlikely to be used by a given subset of the applications. As a consequence, the size of binaries and libraries tends to grow. One of the major trends of today\u0393\u00c7\u00d6s software market is the porting of applications on hand-held devices or, in general, on devices having a limited amount of resources available. Several forms of refactoring and, in particular, the miniaturization of libraries and applications, are therefore necessary.We propose a framework and a toolkit covering several aspects of software miniaturization, such as removing unused objects and code clones, as well as creating small, cohese libraries refactoring the existing ones. The last step has been implemented using a hybrid approach based on hierarchical clustering, genetic algorithms and hill climbing, also incorporating the developer\u0393\u00c7\u00d6s knowledge. Most of the framework activities are language independent (relying on object module analysis) thus they do not require any kind of source code parsing and are applicable to software systems developed with", "num_citations": "2\n", "authors": ["55"]}
{"title": "Flow analysis to detect blocked statements\n", "abstract": " In the context of software quality assessment, the paper proposes two new kinds of data which can be extracted from source code. The first, definitely blocked statements, can never be executed because preceding code prevents the execution of the program. The other data, called possibly blocked statements, may be blocked by blocking code. The paper presents original flow equations to compute definitely and possibly blocked statements in source code. The experimental context is described and results are shown and discussed. Suggestions for further research are also presented.", "num_citations": "2\n", "authors": ["55"]}
{"title": "Language models comparison in a robot telecontrol application\n", "abstract": " Stochastic Language Models (LMs) are key for achieving good performance in speech recognition systems. This is confirmed by the numerous LMs that have been proposed recently in the literature. This work compares three different LMs within the robot telecontrol speech understanding system developed at IRST.", "num_citations": "2\n", "authors": ["55"]}
{"title": "Robust and reliable speech understanding in restricted domains\n", "abstract": " This paper describes the components of an Automatic Speech Understanding (ASU) system developed at IRST within the framework of the MAIA 1 project. The scenario is that of a remote human operator which is supposed to sit in front of a workstation, assisting and managing the activities of a mobile robot (see [3] for more details). In particular, in this work interactions on the basis of so called behaviours [3] of the robots are considered. A corpus of 1024 sentences, uttered by 49 speakers, was collected for this task during 1992 by means of Wizard of Oz simulations. Briefly, these sentences contain both navigation commands, eg \u0393\u00c7\u00a3go on until you see a corridor on your right side\u0393\u00c7\u00a5, or requests of visual information, eg \u0393\u00c7\u00a3show me your position on the map\u0393\u00c7\u00a5. Several utterances present spontaneous speech phenomena, like pauses, false starts and hesitations. In fact, in this application ASU is supposed to be performed in conditions of spontaneous speech, speaker independence, real time conditions, and high reliability-ie as few mis-interpreted commands as possible. For this purpose, robustness was improved by adopting a class-based stochastic language model, a pattern-matching approach to semantic interpretation, and a technique for evaluating the reliability of the ASR system output with respect to the input utterance.", "num_citations": "2\n", "authors": ["55"]}
{"title": "Finite State Network, Isolated Word, Real Time Automatic Speech Recognizer based on DSP32C.\n", "abstract": " This work describes a Finite State Network (FSN), isolated word, speaker dependent, real time Automatic Speech Recognition (ASR) system. FSNs are used to represent grammars belonging to regular language class [Aho et al., 1974]. It is well known that regular languages are powerful enough to generate useful subsets of the natural language. On the other hand, they can be easily integrated into an ASR system [Bahl et al., 1983][Rabiner and Juang, Jan 1988] embedding grammar constraints to reduce the search space, hence the time spent in recognition, and increase the recognition rate [Rabiner and Levinson, 1981][Kai-Fu Lee, April 1988]. For isolated word, speaker dependent ASR systems, a template matching recognition algorithm based on Dynamic Time Warping (DTW)[Velichko and Zagoruyko, 1970][Vintsyuk, 1968][Silverman and Morgan, July 1990] ensures a high recognition rate and quick development. A single AT&T LSI DSP32C board plugged into an ISA bus has been used as co...", "num_citations": "2\n", "authors": ["55"]}
{"title": "RePOR: Mimicking humans on refactoring tasks. Are we there yet?\n", "abstract": " Refactoring is a maintenance activity that aims to improve design quality while preserving the behavior of a system. Several (semi)automated approaches have been proposed to support developers in this maintenance activity, based on the correction of anti-patterns, which are \u0393\u00c7\u00a3poor\u0393\u00c7\u00a5 solutions to recurring design problems. However, little quantitative evidence exists about the impact of automatically refactored code on program comprehension, and in which context automated refactoring can be as effective as manual refactoring. Leveraging RePOR, an automated refactoring approach based on partial order reduction techniques, we performed an empirical study to investigate whether automated refactoring code structure affects the understandability of systems during comprehension tasks. (1) We surveyed 80 developers, asking them to identify from a set of 20 refactoring changes if they were generated by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "Documentation of Machine Learning Software\n", "abstract": " Machine Learning software documentation is different from most of the documentations that were studied in software engineering research. Often, the users of these documentations are not software experts. The increasing interest in using data science and in particular, machine learning in different fields attracted scientists and engineers with various levels of knowledge about programming and software engineering. Our ultimate goal is automated generation and adaptation of machine learning software documents for users with different levels of expertise. We are interested in understanding the nature and triggers of the problems and the impact of the users' levels of expertise in the process of documentation evolution. We will investigate the Stack Overflow Q&As and classify the documentation related Q/As within the machine learning domain to understand the types and triggers of the problems as well as the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "Preface to the special issue on program comprehension\n", "abstract": " Give up? It is a (ham-handed) attempt to use Newton\u0393\u00c7\u00d6s method to find a root of the polynomial g, to within accuracy acc. Without the right background, or even a helpful comment, it may take a while to comprehend what the function does, albeit", "num_citations": "1\n", "authors": ["55"]}
{"title": "An ica approach to unsupervised change detection in multispectral images\n", "abstract": " Detecting regions of change in multiple images of the same scene taken at different times is of widespread interest due to a large number of applications in diverse disciplines, including remote sensing, surveillance, medical diagnosis and treatment, civil infrastructure, and underwater sensing.             The paper proposes a data dependent change detection approach based on textural features extracted by the Independent Component Analysis (ICA) model. The properties of ICA allow to create energy features for computing multispectral and multitemporal difference images to be classified. Our experiments on remote sensing images show that the proposed method can efficiently and effectively classify temporal discontinuities corresponding to changed areas over the observed scenes.", "num_citations": "1\n", "authors": ["55"]}
{"title": "RESOLUTION OF COMPLEX TRAIT OF HYPERTENSION BY LINKAGE, SYSTEMATIC ALLELIC CONTRIBUTION ANALYSIS AND ANCESTRAL DATA IN FRENCH-CANADIAN FAMILIES: 1B. 3\n", "abstract": " Genetic determinants of hypertension were studied in 118 large families with at least one sibpair affected with hypertension/dyslipidemia from a relatively isolated population of the Saguenay region of Quebec, Canada, constituting 1,568 sibpairs who were submitted to anthropomet-ric assessment and among whom 269 individuals underwent extensive cardiovascular phenotyping. Hypertension below 55 years was present in 59% of subjects. Analysis of components of total variance suggested the presence of 2 QTLs for BMI, waist, subscapular and suprailiac circumference traits comprising from 40 to 60% of the total variance with residual variance attributable to environment. Several clusters of QTLs were detected on chromosomes (chr) 1, 3, 16 and 19 A major cluster on chr 1 was significant for obesity, insulin levels, apolipoproteins A and B and suggestive for DBP Chr 3 was strongly associated with DBP at rest\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "OR-37: Dynamic linkage of the angiotensin II type I receptor gene locus in essential hypertension: A study of hypertensive families of french-canadian origin\n", "abstract": " Objective:The goal of the present study was to evaluate the involvement of the angiotensin II type I receptor (AT1R) gene in the development of essential hypertension as well as cardiac hypertrophy in hypertensive families of French-Canadian origin.Design and Methods:Twenty-four-hour ambulatory blood pressure, 9 echocardiography and 15 impedance cardiography parameters were collected during a posture test in 186 sib-pairs originating from 114 hypertensive families. The posture test, defined as 60 min being in a supine position, 10 min standing and 30 min sitting, was followed by a mathematical stress test. The families were selected from a geographically-isolated population of French-Canadian origin on the basis of having at least two siblings affected by early onset (<55 years) hypertension and dyslipidemia. The AT1R gene locus was investigated using a microsatellite and two single nucleotide\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "Fast flow analysis to compute fuzzy estimates of risk levels\n", "abstract": " In the context of software quality assessment, this paper proposes original flow analyses which propagate numerical estimates of blocking risks along an inter-procedural control flow graph (CFG) and which combine these estimates along the different CFG paths using fuzzy logic operations. Two specialized analyses can be further defined in terms of definite and possible flow analysis. The definite analysis computes the minimum blocking risk levels that statements may encounter on every path, while the possible analysis computes the highest blocking risk levels encountered by statements on at least one path. This paper presents original flow equations to compute the definite and possible blocking risk levels for statements in source code. The described fix-point algorithm presents a linear execution time and memory complexity and it is also fast in practice. The experimental context used to validate the presented\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "A system for recognizing continuous speech\n", "abstract": " The present invention relates in general to voice recognition systems, particularly systems for recognizing continuous speech and, more specifically, relates to the estimation of the language model and to its representation in a voice-recognition system.", "num_citations": "1\n", "authors": ["55"]}
{"title": "Complexity and Feasibility Issues in Object Oriented Clone Detection\n", "abstract": " Large multi-platform software systems are likely to encompass a variety of programming languages, coding styles, idioms and hardware-dependent code. Analyzing multi-platform source code, however, is a challenging task. Assembler code is often mixed with high-level Object Oriented (OO) or procedural programming languages. Furthermore, scripting languages, configuration files, and hardware specific resources are typically used. Often, systems were originally conceived as a single platform application, with a limited number of functionalities and of supported devices. Then, they evolved by adding new functionalities and were ported on new product families. In other words, new devices and target platforms were added. When writing a device driver or porting an existing application to a new processor, developers may decide to copy an entire working subsystem and then modify the code to cope with the new hardware. This approach is believed to increase the chances that the developers\u0393\u00c7\u00d6 work will have little unplanned effect on the original piece of code they have just copied. However, this evolving practice promotes the appearing of duplicated code snippets, also called clones. There have been many publications proposing various ways of identifying similar code fragments and components in a software system [14, 9, 6, 2, 10, 12, 3]. However, those publications essentially focused on procedural programming languages such as C and only few contribution addressed the problem of detecting duplicated code in OO systems [8, 1]. Moreover, algorithms presented and published to detect duplications in procedural systems exhibit linear or\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["55"]}
{"title": "Comparison of Automatic Speech Recognition Systems based on New High Speed RISC Workstations\n", "abstract": " In the last few years the computational power of general-purpose machines has dramatically increased; furthermore high performance workstations quite often o er multimedial capabilities. Therefore it is now possible to implement automatic Speech Recognizers (SRs) without dedicated hardware devices such as DSP boards. In this paper performance and costs for di erent con gurations of some SRs based on workstations, DSP boards, and mixed architectures (workstations equipped with DSP boards), developed at IRST, are presented and discussed. Although the response time depends both on the speech modalities,(isolated words or continuous speech), and the hardware platform on which the SR runs, nevertheless, for isolated words, a 486 machine can give very interesting results, above all considering the overall system price. The conclusions are that the use of Digital Signal Processor boards (DSP) as computational engines for Automatic Speech Recognition (ASR) systems will probably be no longer convenient, whilst they can still be used to implement sophisticated recognition systems or single chip SRs.", "num_citations": "1\n", "authors": ["55"]}
{"title": "ASR System Comparison on New High Performance Workstations\n", "abstract": " In the last few years the computational power of general-purpose machines has dramatically increased; furthermore high performance workstations quite often o er multimedial capabilities. Therefore it is now possible to implement automatic Speech Recognizers (SRs) without dedicated hardware devices such as DSP boards. In this paper performance and costs for di erent con gurations of some SRs based on workstations and mixed architectures (workstations equipped with DSP boards), developed at IRST, are presented and discussed. Although the response time depends both on the speech modalities,(isolated words or continuous speech), and the hardware platform on which the SR runs, nevertheless, for isolated words, considering the overall system price, a 486 machine can give very interesting results.", "num_citations": "1\n", "authors": ["55"]}