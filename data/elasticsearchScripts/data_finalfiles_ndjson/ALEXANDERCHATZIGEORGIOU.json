{"title": "Design pattern detection using similarity scoring\n", "abstract": " The identification of design patterns as part of the reengineering process can convey important information to the designer. However, existing pattern detection methodologies generally have problems in dealing with one or more of the following issues: identification of modified pattern versions, search space explosion for large systems and extensibility to novel patterns. In this paper, a design pattern detection methodology is proposed that is based on similarity scoring between graph vertices. Due to the nature of the underlying graph algorithm, this approach has the ability to also recognize patterns that are modified from their standard representation. Moreover, the approach exploits the fact that patterns reside in one or more inheritance hierarchies, reducing the size of the graphs to which the algorithm is applied. Finally, the algorithm does not rely on any pattern-specific heuristic, facilitating the extension to novel\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "564\n", "authors": ["63"]}
{"title": "Identification of move method refactoring opportunities\n", "abstract": " Placement of attributes/methods within classes in an object-oriented system is usually guided by conceptual criteria and aided by appropriate metrics. Moving state and behavior between classes can help reduce coupling and increase cohesion, but it is nontrivial to identify where such refactorings should be applied. In this paper, we propose a methodology for the identification of Move Method refactoring opportunities that constitute a way for solving many common feature envy bad smells. An algorithm that employs the notion of distance between system entities (attributes/methods) and classes extracts a list of behavior-preserving refactorings based on the examination of a set of preconditions. In practice, a software system may exhibit such problems in many different places. Therefore, our approach measures the effect of all refactoring suggestions based on a novel entity placement metric that quantifies how well\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "372\n", "authors": ["63"]}
{"title": "Identification of extract method refactoring opportunities for the decomposition of methods\n", "abstract": " The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing dependences is proposed that exclude refactoring opportunities corresponding to slices whose\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "185\n", "authors": ["63"]}
{"title": "The financial aspect of managing technical debt: A systematic literature review\n", "abstract": " ContextTechnical debt is a software engineering metaphor, referring to the eventual financial consequences of trade-offs between shrinking product time to market and poorly specifying, or implementing a software product, throughout all development phases. Based on its inter-disciplinary nature, i.e. software engineering and economics, research on managing technical debt should be balanced between software engineering and economic theories.ObjectiveThe aim of this study is to analyze research efforts on technical debt, by focusing on their financial aspect. Specifically, the analysis is carried out with respect to: (a) how financial aspects are defined in the context of technical debt and (b) how they relate to the underlying software engineering concepts.MethodIn order to achieve the abovementioned goals, we employed a standard method for SLRs and applied it on studies retrieved from seven general-scope\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "168\n", "authors": ["63"]}
{"title": "Investigating the evolution of bad smells in object-oriented code\n", "abstract": " Software design problems are known and perceived under many different terms such as bad smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and antipatterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial bad smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human intervention\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["63"]}
{"title": "Predicting the probability of change in object-oriented systems\n", "abstract": " Of all merits of the object-oriented paradigm, flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches. However, it is rather difficult to quantify this aspect of quality: this paper describes a probabilistic approach to estimate the change proneness of an object-oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. The extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible \"saturation\" level beyond which any attempt to improve the design without major refactoring is impossible. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "147\n", "authors": ["63"]}
{"title": "Energy consumption estimation in embedded systems\n", "abstract": " This paper presents an energy consumption modeling technique for embedded systems based on a microcontroller. The software tasks that run on the embedded system are profiled, and their characteristics are analyzed. The type of executed assembly instructions, as well as the number of accesses to the memory and the analog-to-digital converter, is the required information for the derivation of the proposed model. An appropriate instrumentation setup has been developed for measuring and modeling the energy consumption in the corresponding digital circuits.", "num_citations": "117\n", "authors": ["63"]}
{"title": "Architectural risk analysis of software systems based on security patterns\n", "abstract": " The importance of software security has been profound, since most attacks to software systems are based on vulnerabilities caused by poorly designed and developed software. Furthermore, the enforcement of security in software systems at the design phase can reduce the high cost and effort associated with the introduction of security during implementation. For this purpose, security patterns that offer security at the architectural level have been proposed in analogy to the well-known design patterns. The main goal of this paper is to perform risk analysis of software systems based on the security patterns that they contain. The first step is to determine to what extent specific security patterns shield from known attacks. This information is fed to a mathematical model based on the fuzzy-set theory and fuzzy fault trees in order to compute the risk for each category of attacks. The whole process has been automated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "112\n", "authors": ["63"]}
{"title": "Evaluation of object-oriented design patterns in game development\n", "abstract": " The use of object-oriented design patterns in game development is being evaluated in this paper. Games\u0393\u00c7\u00d6 quick evolution, demands great flexibility, code reusability and low maintenance costs. Games usually differentiate between versions, in respect of changes of the same type (additional animation, terrains etc). Consequently, the application of design patterns in them can be beneficial regarding maintainability. In order to investigate the benefits of using design patterns, a qualitative and a quantitative evaluation of open source projects is being performed. For the quantitative evaluation, the projects are being analyzed by reverse engineering techniques and software metrics are calculated.", "num_citations": "99\n", "authors": ["63"]}
{"title": "A modeling technique for CMOS gates\n", "abstract": " In this paper, a modeling technique for CMOS gates, based on the reduction of each gate to an equivalent inverter, is presented. The proposed method can be incorporated in existing timing simulators in order to improve their accuracy. The conducting and parasitic behavior of parallel and serially connected transistors is accurately analyzed and an equivalent transistor is extracted for each case, taking into account the actual operating conditions of each device in the structure. The proposed model incorporates short-channel effects, the influence of body effect and is developed for nonzero transition time inputs. The exact time point when the gate starts conducting is efficiently calculated improving significantly the accuracy of the method. A mapping algorithm for reducing every possible input pattern of a gate to an equivalent signal is introduced and the \"weight\" of each transistor position in the gate structure is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["63"]}
{"title": "Identification of extract method refactoring opportunities\n", "abstract": " Extract method has been recognized as one of the most important refactorings, since it decomposes large methods and can be used in combination with other refactorings for fixing a variety of design problems. However, existing tools and methodologies support extraction of methods based on a set of statements selected by the user in the original method. The goal of the proposed methodology is to automatically identify extract method refactoring opportunities and present them as suggestions to the designer of an object-oriented system. The suggested refactorings adhere to three principles: the extracted code should contain the complete computation of a given variable declared in the original method, the behavior of the program should be preserved after the application of the refactoring, and the extracted code should not be excessively duplicated in the original method. The proposed approach is based on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["63"]}
{"title": "The effect of GoF design patterns on stability: a case study\n", "abstract": " Stability refers to a software system's resistance to the \u0393\u00c7\u00a3ripple effect\u0393\u00c7\u00a5, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected \u0393\u00c7\u00a3shielding\u0393\u00c7\u00a5 of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["63"]}
{"title": "Decomposing object-oriented class modules using an agglomerative clustering technique\n", "abstract": " Software can be considered a live entity, as it undergoes many alterations throughout its lifecycle. Furthermore, developers do not usually retain a good design in favor of adding new features, comply with requirements or meet deadlines. For these reasons, code can become rather complex and difficult to understand. More particularly in object-oriented systems, classes may become very large and less cohesive. In order to identify such problematic cases, existing approaches have proposed the use of cohesion metrics. However, while metrics can identify classes with low cohesion, they cannot identify new or independent concepts. Moreover, these methods require a lot of human interpretation to identify the respective design flaws. In this paper, we propose a class decomposition method using an agglomerative clustering algorithm based on the Jaccard distance between class members. Our methodology is able to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["63"]}
{"title": "Investigating the evolution of code smells in object-oriented systems\n", "abstract": " Software design problems are known and perceived under many different terms, such as code smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and anti-patterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial code smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["63"]}
{"title": "Identifying, categorizing and mitigating threats to validity in software engineering secondary studies\n", "abstract": " ContextSecondary studies are vulnerable to threats to validity. Although, mitigating these threats is crucial for the credibility of these studies, we currently lack a systematic approach to identify, categorize and mitigate threats to validity for secondary studies.ObjectiveIn this paper, we review the corpus of secondary studies, with the aim to identify: (a) the trend of reporting threats to validity, (b) the most common threats to validity and corresponding mitigation actions, and (c) possible categories in which threats to validity can be classified.MethodTo achieve this goal we employ the tertiary study research method that is used for synthesizing knowledge from existing secondary studies. In particular, we collected data from more than 100 studies, published until December 2016 in top quality software engineering venues (both journals and conference).ResultsOur results suggest that in recent years, secondary studies are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["63"]}
{"title": "Ranking refactoring suggestions based on historical volatility\n", "abstract": " The widespread acceptance of refactorings as a simple yet effective approach to improve the design of object-oriented systems, has stimulated an effort to develop semi-automatic tools for detecting design flaws, with simultaneous suggestions for their removal. However, even in medium-sized projects the number of detected occurrences can be so large that the refactoring process becomes intractable for the designer. It is reasonable to expect that some of the suggested refactorings will have a significant effect on the improvement of maintainability while others might be less important. This implies that the suggested solutions can be ranked according to one or more criteria. In this paper we propose the exploitation of past source code versions in order to rank refactoring suggestions according to the number, proximity and extent of changes related with the corresponding code smells. The underlying philosophy is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["63"]}
{"title": "A qualitative analysis of software security patterns\n", "abstract": " Software security, which has attracted the interest of the industrial and research community during the last years, aims at preventing security problems by building software without the so-called security holes. One way to achieve this goal is to apply specific patterns in software architecture. In the same way that the well-known design patterns for building well-structured software have been defined, a new kind of patterns called security patterns have emerged. These patterns enable us to incorporate a level of security already at the design phase of a software system. There exists no strict set of rules that can be followed in order to develop secure software. However, a number of guidelines have already appeared in the literature. Furthermore, the key problems in building secure software and major threat categories for a software system have been identified. An attempt to evaluate known security patterns based on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["63"]}
{"title": "A qualitative evaluation of security patterns\n", "abstract": " Software Security has received a lot of attention during the last years. It aims at preventing security problems by building software without the so-called security holes. One of the ways to do this is to apply specific patterns in software architecture. In the same way that the well-known design patterns for building well-structured software have been used, a new kind of patterns, called security patterns have emerged. The way to build secure software is still vague, but guidelines for this have already appeared in the literature. Furthermore, the key problems in building secure software have been mentioned. Finally, threat categories for a software system have been identified. Based on these facts, it would be useful to evaluate known security patterns based on how well they follow each guideline, how they encounter with possible problems in building secure software and for which of the threat categories they do\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["63"]}
{"title": "The evolution of technical debt in the apache ecosystem\n", "abstract": " Software systems must evolve over time or become increasingly irrelevant says one of Lehman\u0393\u00c7\u00d6s laws of software evolution. Many studies have been presented in the literature that investigate the evolution of software systems but few have focused on the evolution of technical debt. In this paper we study sixty-six Java open-source software projects from the Apache ecosystem focusing on the evolution of technical debt. We analyze the evolution of these systems over the past five years at the temporal granularity level of weekly snapshots. We calculate the trends of the technical debt time series but we also investigate the lower-level constituent components of this technical debt. We aggregate some of the information to the ecosystem level.                 Our findings show that the technical debt together with source code metrics increase for the majority of the systems. However, technical debt normalized to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["63"]}
{"title": "Evaluating performance and power of object-oriented vs. procedural programming in embedded processors\n", "abstract": " The development of high-performance and low power portable devices relies on both the underlying hardware architecture and technology as well as on the application software that executes on embedded processor cores. It has been extensively pointed out that the increasing complexity and decreasing time-to-market of embedded software can only be confronted by the use of modular and reusable code, which forces software designers to use objected oriented programming languages such as C++. However, the object-oriented approach is known to introduce a significant performance penalty compared to classical procedural programming. In this paper, the object oriented programming style is evaluated in terms of both performance and power for embedded applications. A set of benchmark kernels is compiled and executed on an embedded processor simulator, while the results are fed to instruction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["63"]}
{"title": "An empirical study on students\u0393\u00c7\u00d6 ability to comprehend design patterns\n", "abstract": " Design patterns have become a widely acknowledged software engineering practice and therefore have been incorporated in the curricula of most computer science departments. This paper presents an observational study on students\u0393\u00c7\u00d6 ability to understand and apply design patterns. Within the context of a postgraduate software engineering course, students had to deliver two versions of a software system; one without and one with design patterns. The former served as a poorly designed system suffering from architectural problems, while the latter served as an improved system where design problems had been solved by appropriate patterns. The experiment allowed the quantitative evaluation of students\u0393\u00c7\u00d6 preference to patterns. Moreover, it was possible to assess students\u0393\u00c7\u00d6 ability in relating design problems with patterns and interpreting the impact of patterns on software metrics. The overall goal was to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["63"]}
{"title": "A mapping study on design-time quality attributes and metrics\n", "abstract": " Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["63"]}
{"title": "Studying the evolution of PHP web applications\n", "abstract": " ContextSoftware evolution analysis can reveal important information concerning maintenance practices. Most of the studies which analyze software evolution focus on desktop applications written in compiled languages, such as Java and C. However, a vast amount of the web content today is powered by web applications written in PHP and thus the evolution of software systems written in such a scripting language deserves a distinct analysis.ObjectiveThe aim of this study is to analyze the evolution of open-source PHP projects in an attempt to investigate whether Lehman's laws of software evolution are confirmed in practice for web applications.MethodData (changes and metrics) have been collected for successive versions of 30 PHP projects while statistical tests (primarily trend tests) have been employed to evaluate the validity of each law on the examined web applications.ResultsWe found that Laws: I\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["63"]}
{"title": "Identification of refused bequest code smells\n", "abstract": " Accumulated technical debt can be alleviated by means of refactoring application aiming at architectural improvement. A prerequisite for wide scale refactoring application is the automated identification of the corresponding refactoring opportunities, or code smells. One of the major architectural problems that has received limited attention is the so called 'Refused Bequest' which refers to inappropriate use of inheritance in object-oriented systems. This code smell occurs when subclasses do not take advantage of the inherited behavior, implying that replacement by delegation should be used instead. In this paper we propose a technique for the identification of Refused Bequest code smells whose major novelty lies in the intentional introduction of errors in the inherited methods. The essence of inheritance is evaluated by exercising the system's functionality through the corresponding unit tests in order to reveal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["63"]}
{"title": "Software metrics fluctuation: a property for assisting the metric selection process\n", "abstract": " ContextSoftware quality attributes are assessed by employing appropriate metrics. However, the choice of such metrics is not always obvious and is further complicated by the multitude of available metrics. To assist metrics selection, several properties have been proposed. However, although metrics are often used to assess successive software versions, there is no property that assesses their ability to capture structural changes along evolution.ObjectiveWe introduce a property, Software Metric Fluctuation (SMF), which quantifies the degree to which a metric score varies, due to changes occurring between successive system's versions. Regarding SMF, metrics can be characterized as sensitive (changes induce high variation on the metric score) or stable (changes induce low variation on the metric score).MethodSMF property has been evaluated by: (a) a case study on 20 OSS projects to assess the ability of SMF\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["63"]}
{"title": "How do developers fix issues and pay back technical debt in the apache ecosystem?\n", "abstract": " During software evolution technical debt (TD) follows a constant ebb and flow, being incurred and paid back, sometimes in the same day and sometimes ten years later. There have been several studies in the literature investigating how technical debt in source code accumulates during time and the consequences of this accumulation for software maintenance. However, to the best of our knowledge there are no large scale studies that focus on the types of issues that are fixed and the amount of TD that is paid back during software evolution. In this paper we present the results of a case study, in which we analyzed the evolution of fifty-seven Java open-source software projects by the Apache Software Foundation at the temporal granularity level of weekly snapshots. In particular, we focus on the amount of technical debt that is paid back and the types of issues that are fixed. The findings reveal that a small subset of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["63"]}
{"title": "The perception of technical debt in the embedded systems domain: an industrial case study\n", "abstract": " Technical Debt Management (TDM) has drawn the attention of software industries during the last years, including embedded systems. However, we currently lack an overview of how practitioners from this application domain perceive technical debt. To this end, we conducted a multiple case study in the embedded systems industry, to investigate: (a) the expected life-time of components that have TD, (b) the most frequently occurring types of TD in them, and (c) the significance of TD against run-time quality attributes. The case study was performed on seven embedded systems industries (telecommunications, printing, smart manufacturing, sensors, etc.) from five countries (Greece, Netherlands, Sweden, Austria, and Finland). The results of the case study suggest that: (a) maintainability is more seriously considered when the expected lifetime of components is larger than ten years, (b) the most frequent types of debt\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["63"]}
{"title": "Energy consumption analysis of design patterns\n", "abstract": " The importance of low power consumption is widely acknowledged due to the increasing use of portable devices, which require minimizing the consumption of energy. Energy dissipation is heavily dependent on the software used in the system. Applying design patterns in object-oriented designs is a common practice nowadays. In this paper we analyze six design patterns and explore the effect of them on energy consumption and performance.", "num_citations": "35\n", "authors": ["63"]}
{"title": "Modeling CMOS gates driving RC interconnect loads\n", "abstract": " The problem of estimating the performance of CMOS gates driving RC interconnect loads is addressed in this paper. The widely accepted /spl pi/-model is used for the representation of an interconnect line that is driven by an inverter. The output waveform and the propagation delay of the inverter are analytically calculated taking into account the coupling capacitance between input and output and the effect of the short-circuit current. In addition, short-circuit power dissipation is accurately estimated. Once the voltage waveform at both the beginning and the end of an interconnect line are obtained, a simple method is employed in order to calculate the voltage waveform at each point of the line.", "num_citations": "35\n", "authors": ["63"]}
{"title": "Identifying extract method refactoring opportunities based on functional relevance\n", "abstract": " `Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["63"]}
{"title": "Forecasting java software evolution trends employing network models\n", "abstract": " The evolution of networks representing systems in various domains, including social networks, has been extensively studied enabling the development of growth models which govern their behavior over time. The architecture of software systems can also be naturally represented in the form of networks, whose properties change as software evolves. In this paper we attempt to model several aspects of graphs representing object-oriented software systems as they evolve over a number of versions. The goal is to develop a prediction model by considering global phenomena such as preferential attachment, past evolutionary trends such as the tendency of classes to create fewer relations as they age, as well as domain knowledge in terms of principles that have to be followed in object-oriented design. The derived models can provide insight into the future trends of software and potentially form the basis for eliciting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["63"]}
{"title": "Evaluating object-oriented designs with link analysis\n", "abstract": " The hyperlink induced topic search algorithm, which is a method of link analysis, primarily developed for retrieving information from the Web, is extended in this paper, in order to evaluate one aspect of quality in an object-oriented model. Considering the number of discrete messages exchanged between classes, it is possible to identify \"God\" classes in the system, elements which imply a poorly designed model. The principal eigenvectors of matrices derived from the adjacency matrix of a modified class diagram, are used to identify and quantify heavily loaded portions of an object-oriented design that deviate from the principle of distributed responsibilities. The non-principal eigenvectors are also employed in order to identify possible reusable components in the system. The methodology can be easily automated as illustrated by a Java program that has been developed for this purpose.", "num_citations": "28\n", "authors": ["63"]}
{"title": "Introducing a ripple effect measure: a theoretical and empirical validation\n", "abstract": " Context: Change impact analysis investigates the negative consequence of system changes, i.e., the propagation of changes to other parts of the system (also known as the ripple effect). Identifying modules of the system that will be affected by the ripple effect is an important activity, before and after the application of any change. Goal: However, in the literature, there is only a limited set of studies that investigate the probability of a random change occurring in one class, to propagate to another. In this paper we discuss and evaluate the Ripple Effect Measure (in short REM), a metric that can be used to assess the aforementioned probability. Method: To evaluate the capacity of REM as an assessor of the prob-ability of a class to change due to the ripple effect, we: (a) mathematically validate it against established metric properties (e.g., non-negativity, monotonicity, etc.), proposed by Briand et al., and (b) empirically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["63"]}
{"title": "Facilitating software extension with design patterns and Aspect-Oriented Programming\n", "abstract": " Software products, especially large applications, need to continuously evolve, in order to adapt to the changing environment and updated requirements. With both the producer and the customer unwilling to replace the existing application with a completely new one, adoption of design constructs and techniques which facilitate the application extension is a major design issue. In the current work we investigate the behavior of an object-oriented software application at a specific extension scenario, following three implementation alternatives with regards to a certain design problem relevant to the extension. The first alternative follows a simplistic solution, the second makes use of a design pattern and the third applies Aspect-Oriented Programming techniques to implement the same pattern. An assessment of the three alternatives is attempted, both on a qualitative and a quantitative level, by identifying the additional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["63"]}
{"title": "Mathematical assessment of object-oriented design quality\n", "abstract": " A method of link analysis employed for retrieving information from the Web is extended in order to evaluate one aspect of quality in an object-oriented model. The principal eigenvectors of matrices derived from the adjacency matrix of a modified class diagram are used to identify and quantity heavily loaded portions of an object-oriented design that deviate from the principle of distributed responsibilities.", "num_citations": "25\n", "authors": ["63"]}
{"title": "Ten years of JDeodorant: Lessons learned from the hunt for smells\n", "abstract": " Deodorants are different from perfumes, because they are applied directly on body and by killing bacteria they reduce odours and offer a refreshing fragrance. That was our goal when we first thought about \"bad smells\" in code: to develop techniques for effectively identifying and removing (i.e., deodorizing) code smells from object-oriented software. JDeodorant encompasses a number of techniques for suggesting and automatically applying refactoring opportunities on Java source code, in a way that requires limited effort on behalf of the developer. In contrast to other approaches that rely on generic strategies that can be adapted to various smells, JDeodorant adopts ad-hoc strategies for each smell considering the particular characteristics of the underlying design or code problem. In this retrospective paper, we discuss the impact of JDeodorant over the last ten years and a number of tools and techniques that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["63"]}
{"title": "Reusability of open source software across domains: A case study\n", "abstract": " Exploiting the enormous amount of open source software (OSS) as a vehicle for reuse is a promising opportunity for software engineers. However, this task is far from trivial, since such projects are sometimes not easy to understand and adapt to target systems, whereas at the same time the reusable assets are not obvious to identify. In this study, we assess open source software projects, with respect to their reusability, i.e., the easiness to adapt them in a new system. By taking into account that domain-specific reuse is more beneficial than domain-agnostic; we focus this study on identifying the application domains that contain the most reusable software projects. To achieve this goal, we compared the reusability of approximately 600 OSS projects from ten application domains through a case study. The results of the study suggested that in every aspect of reusability, there are different dominant application domains\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["63"]}
{"title": "A financial approach for managing interest in technical debt\n", "abstract": " Technical debt (TD) is a metaphor that is used by both technical and management stakeholders to acknowledge and discuss issues related to compromised design-time qualities. Until now, despite the inherent relevance of technical debt to economics, the TD community has not sufficiently exploited economic methods/models. In this paper we present a framework for managing interest in technical debt, founded on top of Liquidity Preference, a well-known economics theory. To tailor this theory to fit the TD context, we exploit the synthesized knowledge as presented in two recent studies. Specifically, in our framework, we discuss aspects related to technical debt interest, such as: types of TD interest, TD interest characteristics, and a proposed TD interest theory. Finally, to boost the amount of empirical studies in TD research, we propose several tentative research designs that could be used for exploring the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["63"]}
{"title": "Maintenance patterns of large-scale PHP web applications\n", "abstract": " Scripting languages such as PHP have been criticized as inadequate for supporting maintenance of large-scale software projects. In this paper we attempt to provide insight into the way that five large and well-known PHP applications evolved over time. Several aspects of their history are examined including the amount of unused code, the removal of functions, the use of libraries, the stability of their interfaces, the migration to object-orientation and the evolution of complexity. The results suggest that these systems undergo systematic maintenance which is driven by targeted design decisions and evolution is by no means hindered by the underlying programming language.", "num_citations": "22\n", "authors": ["63"]}
{"title": "Energy complexity of software in embedded systems\n", "abstract": " The importance of low power consumption is widely acknowledged due to the increasing use of portable devices, which require minimizing the consumption of energy. The energy in a computational system depends heavily on the software being executed, since it determines the activity in the underlying circuitry. In this paper we introduce the notion of energy complexity of an algorithm for estimating the required energy consumption. As test vehicle we employ matrix multiplication algorithms and from the results it can be observed that energy complexity in combination with computational complexity, provides an accurate estimation for the energy consumed in the system.", "num_citations": "22\n", "authors": ["63"]}
{"title": "Energy metric for software systems\n", "abstract": " Acknowledging the intense requirement for low power operation in most portable computing systems, this paper introduces the notion of energy efficient software design and proposes metrics, for evaluating software systems in terms of their energy consumption. Considering the sources of power consumption in every digital circuit, and the fact that power is primarily dependent on the executing software, appropriate energy measures are derived, which can be extracted from the flowgraph of a program. The proposed measures are computed by applying rules common to the existing hierarchical measures of other internal software attributes, and form the basis for the definition of a software energy metric. This metric can be used in order to determine the level of energy consumption of any software system more efficiently than existing assembly-parsing techniques, with only a limited penalty in accuracy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["63"]}
{"title": "Efficient management of inspections in software development projects\n", "abstract": " During the last two decades a universal agreement has been established on the fact that software inspections play a fundamental role in improving software quality. The number of software organizations that have incorporated formal reviews in their development process is constantly increasing and the belief that efficient inspections can not only detect defects but also reduce cycle time and lower costs is spreading. However, despite the importance of the inspections in a software development project, scheduling of inspections has not been given the necessary attention so far. As a result, inspections tend to accumulate towards internal project deadlines, possibly leading to excess overtime costs, quality degradation and difficulties in meeting milestones. In this paper, data from a major telecommunications software project is analyzed in an effort to illustrate the problems that can arise from inefficient planning of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["63"]}
{"title": "A method for assessing class change proneness\n", "abstract": " Change proneness is a quality characteristic of software artifacts that represents their probability to change in the future due to:(a) evolving requirements,(b) bug fixing, or (c) ripple effects. In the literature, change proneness has been associated with many negative consequences along software evolution. For example, artifacts that are change-prone tend to produce more defects, and accumulate more technical debt. Therefore, identifying and monitoring modules of the system that are change-prone is of paramount importance. Assessing change proneness requires information from two sources:(a) the history of changes in the artifact as a proxy of how frequently the artifact itself is changing, and (b) the source code structure that affects the probability of a change being propagated among artifacts. In this paper, we propose a method for assessing the change proneness of classes based on the two aforementioned\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["63"]}
{"title": "Multilayer feed forward models in groundwater level forecasting using meteorological data in public management\n", "abstract": " Managing the groundwater resources is very vital for human life. This research proposes a methodology for predicting the groundwater levels which can be very valuable in water resources management. This study investigates the application of multilayer feed forward network models for forecasting the groundwater values in the region of Montgomery country in Pennsylvania. Multiple training algorithms and network structures were investigated to develop the best model in order to forecast the groundwater levels. Several multilayer feed forward models were created in order to be tested for their performance by changing the network topology parameters so as to find the optimal prediction model. The forecasting models were developed by applying different structures regarding the number of the neurons in every hidden layer and the number of the hidden network layers. The final results have shown a very good\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["63"]}
{"title": "Analytical estimation of propagation delay and short\u0393\u00c7\u00c9circuit power dissipation in CMOS gates\n", "abstract": " An efficient analytical method for calculating the propagation delay and the short\u0393\u00c7\u00c9circuit power dissipation of CMOS gates is introduced in this paper. Key factors that determine the operation of a gate, such as the different modes of operation of serially connected transistors, the starting point of conduction, the parasitic behaviour of the short\u0393\u00c7\u00c9circuiting block of a gate and the behaviour of parallel transistor structures are analysed and properly modelled. The analysis is performed taking into account second\u0393\u00c7\u00c9order effects of short\u0393\u00c7\u00c9channel devices and for non\u0393\u00c7\u00c9zero transition time inputs. Analytical expressions for the output waveform, the propagation delay and the short\u0393\u00c7\u00c9circuit power dissipation are obtained by solving the differential equations that govern the operation of the gate. The calculated results are in excellent agreement with SPICE simulations. Copyright \u252c\u2310 1999 John Wiley & Sons, Ltd.", "num_citations": "18\n", "authors": ["63"]}
{"title": "Collapsing the transistor chain to an effective single equivalent transistor\n", "abstract": " The most common practice to model the transistor chain, as it appears in CMOS gates, is to collapse it to a single equivalent transistor. This method is analyzed and improvements are presented in this paper. Inherent shortcomings are removed and an effective transistor width is calculated taking into account the operating conditions of the structure, resulting in very good agreement with SPICE simulations. The actual time point when the chain starts conducting which influences significantly the accuracy of the model is also extracted. Finally, an algorithm to collapse every possible input pattern to a single input is presented.", "num_citations": "18\n", "authors": ["63"]}
{"title": "Quantitative evaluation of systems with security patterns using a fuzzy approach\n", "abstract": " The importance of Software Security has been evident, since it has been shown that most attacks to software systems are based on vulnerabilities caused by software poorly designed and developed. Furthermore, it has been discovered that it is desirable to embed security already at design phase. Therefore, patterns aiming at enhancing the security of a software system, called security patterns, have been suggested. The main target of this paper is to propose a mathematical model, based on fuzzy set theory, in order to quantify the security characteristics of systems using security patterns. In order to achieve this we first determine experimentally to what extent specific security patterns enhance several security aspects of systems. To determine this, we have developed two systems, one without security patterns and one containing them and have experimentally determined the level of the higher robustness to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["63"]}
{"title": "Probabilistic evaluation of object-oriented systems\n", "abstract": " The goal of this study is the development of a probabilistic model for the evaluation of flexibility of an object-oriented design. In particular, the model estimates the probability that a certain class of the system gets affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. Useful conclusions can be drawn from this model regarding the comparative evaluation of two or more object-oriented systems or even the assessment of several generations of the same system, in order to determine whether or not good design principles have been applied. The proposed model has been implemented in a Java program that can automatically analyze the class diagram of a given system.", "num_citations": "16\n", "authors": ["63"]}
{"title": "Performance and power evaluation of C++ object-oriented programming in embedded processors\n", "abstract": " The development of high-performance and lower power portable devices relies on both the underlying hardware architecture and technology as well as on the application software that executes on embedded processor cores. One way to confront the increasing complexity and decreasing time-to-market of embedded software is by means of modular and reusable code, forcing software designers to use objected oriented programming languages such as C++ [6]. However, the object-oriented approach is known to introduce a significant performance penalty compared to classical procedural programming. In this paper, the object oriented programming style is evaluated in terms of both performance and power for embedded applications. Profiling results indicate that C++ programs apart from being slower than their corresponding C versions, consume significantly more energy. Further analysis shows that this is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["63"]}
{"title": "Delay and power estimation for a CMOS inverter driving RC interconnect loads\n", "abstract": " The resistive-capacitive behavior of long interconnects which are driven by CMOS gates is analyzed in this paper. The analysis is based on the /spl pi/-model of an RC load and is developed for submicron devices. Accurate and analytical expressions for the output voltage waveform, the propagation delay and the short circuit power dissipation are derived by solving the system of differential equations which describe the behavior of the circuit. The effect of the coupling capacitance between input and output and that of short circuit current are also incorporated in the proposed model. The calculated propagation delay and short circuit power dissipation are in very good agreement with SPICE simulations.", "num_citations": "16\n", "authors": ["63"]}
{"title": "Interrelations between software quality metrics, performance and energy consumption in embedded applications\n", "abstract": " Source code refactorings and transformations are extensively used by embedded system developers to improve the quality of applications, often supported by various open source and proprietary tools. They either aim at improving the design time quality such as the maintainability and reusability of software artifacts, or the runtime quality such as performance and energy efficiency. However, an inherent trade-off between design-and run-time qualities is often present posing challenges to embedded software development. This work is a first step towards the investigation of the impact of transformations for improving the performance and the energy efficiency on software quality metrics and the impact of refactorings for increasing the design time quality on the execution time, the memory and the energy consumption. Based on a set of embedded applications from widely used benchmark suites and typical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["63"]}
{"title": "Who is producing more technical debt? A personalized assessment of TD principal\n", "abstract": " Technical debt (TD) impedes software projects by reducing the velocity of development teams during software evolution. Although TD is usually assessed on either the entire system or on individual software artifacts, it is the actual craftsmanship of developers that causes the accumulation of TD. In the light of extremely high maintenance costs, efficient software project management cannot occur without recognizing the relation between developer characteristics and the tendency to evoke violations that lead to TD. In this paper, we investigate three research questions related to the distribution of TD among the developers of a software project, the types of violations caused by each developer and the relation between developers' maturity and the tendency to accumulate TD. The study has been performed on four widely employed PHP open-source projects. All developers' personal characteristics have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["63"]}
{"title": "Energy issues in software design of embedded systems\n", "abstract": " The increasing use of programmable processor cores in embedded systems which are mainly used in portable devices, creates an intense need for low power operation. Although power has been primarily addressed at the circuit/technology level it has become obvious that power consumption is heavily dependent on the executing software. The goal of this paper is to illustrate the impact of several software decisions on the energy consumption of the underlying hardware and the need to consider power in software design.", "num_citations": "15\n", "authors": ["63"]}
{"title": "What can violations of good practices tell about the relationship between GoF patterns and run-time quality attributes?\n", "abstract": " ContextGoF patterns have been extensively studied with respect to the benefit they provide as problem-solving, communication and quality improvement mechanisms. The latter has been mostly investigated through empirical studies, but some aspects of quality (esp. run-time ones) are still under-investigated.ObjectiveIn this paper, we study if the presence of patterns enforces the conformance to good coding practices. To achieve this goal, we explore the relationship between the presence of GoF design patterns and violations of good practices related to source code correctness, performance and security, via static analysis.MethodSpecifically, we exploit static analysis so as to investigate whether the number of violations of good coding practices identified on classes is related to: (a) their participation in pattern occurrences, (b) the pattern category, (c) the pattern in which they participate, and (d) their role within the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["63"]}
{"title": "Technical debt principal assessment through structural metrics\n", "abstract": " One of the first steps towards the effective Technical Debt (TD) management is the quantification and continuous monitoring of the TD principal. In the current state-ofresearch and practice the most common ways to assess TD principal are the use of: (a) structural proxies-i.e., most commonly through quality metrics; and (b) monetized proxies-i.e., most commonly through the use of the SQALE (Software Quality Assessment based on Lifecycle Expectations) method. Although both approaches have merit, they seem to rely on different viewpoints of TD and their levels of agreement have not been evaluated so far. Therefore, in this paper, we empirically explore this relation by analyzing data obtained from 20 open source software projects and build a regression model that establishes a relationship between them. The results of the study suggest that a model of seven structural metrics, quantifying different aspects of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["63"]}
{"title": "Analytical exploration of power efficient data-reuse transformations on multimedia applications\n", "abstract": " Power savings that can be achieved by data-reuse decisions targeting at a custom memory hierarchy for multimedia applications executing on embedded cores are examined in this paper. Exploiting the temporal locality of memory accesses in data-intensive applications a set of data-reuse transformations on a typical motion estimation algorithm is determined. The aim is to reduce data related power consumption by moving background memory accesses to smaller foreground memories, which are less power costly. The impact of these transformations on power, performance and area is evaluated both for application specific circuits and general purpose processors. The number of data and instruction memory accesses is analytically calculated, enabling a fast exploration of the design space by varying algorithmic parameters.", "num_citations": "14\n", "authors": ["63"]}
{"title": "Modelling the operation of pass transistor and CPL gates\n", "abstract": " Pass transistor logic and complementary pass-transistor logic (CPL) are becoming increasingly important in the design of a specific class of digital integrated circuits owing to their speed and power efficiency as compared with conventional CMOS logic. In this paper, a simple and very accurate technique for the timing analysis of gates that involve pass transistor logic is presented. This investigation offers for the first time the possibility of simulating pass transistor and CPL gates by partitioning the behaviour of complex structures into well defined subcircuits whose interaction is studied separately. Using the proposed analysis, which is validated by results for two submicron technologies, most pass-transistor logic styles can be modelled efficiently. Consequently, a significant speed advantage can be gained compared with simulation tools that employ numerical methods such as SPICE.", "num_citations": "13\n", "authors": ["63"]}
{"title": "Technical debt quantification through metrics: an industrial validation\n", "abstract": " Technical Debt is a software engineering metaphor that refers to the intentional or unintentional situation in which a software industry, produces a software at a lower quality, to achieve business goals (eg, shorten time to market). Nevertheless, similarly to financial debt, technical debt does not come without negative consequences. The accumulation of technical debt leads to additional maintenance. The technical debt metaphor is built around three major notions: principal, interest, and interest probability. The quantification of these notions is the first step towards the efficient management of technical debt, in the sense that \u0393\u00c7\u00a3you cannot control what you cannot measure\u0393\u00c7\u00a5. In this paper, we employ an established method for quantifying technical debt, namely FITTED, to measure the technical debt of an industrial software product, and contrast it to the perception of the software engineers. The main contribution of this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["63"]}
{"title": "Methods and Tools for TD Estimation and Forecasting: A State-of-the-art Survey\n", "abstract": " Technical debt (TD), a metaphor inspired by the financial debt of economic theory, indicates quality compromises that can yield short-term benefits in the software development process, but may negatively affect the long-term quality of software products. Numerous techniques, methods, and tools have been proposed over the years for estimating and managing TD, providing a variety of options to the developers and project managers of software applications. However, apart from managing TD, predicting its future value is equally important since this knowledge is expected to facilitate decision-making tasks regarding software implementation and maintenance, such as incurring or paying off TD instances. To this end, the purpose of the present study is to (i) summarize the work that has been conducted until today in the field of TD estimation and forecasting, and (ii) to identify existing open issues that have not been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["63"]}
{"title": "Assessing code smell interest probability: a case study\n", "abstract": " An important parameter in deciding to eliminate technical debt (TD) is the probability of a module to generate interest along software evolution. In this study, we explore code smells, which according to practitioners are the most commonly occurring type of TD in industry, by assessing the associated interest probability. As a proxy of smell interest probability we use the frequency of smell occurrences and the change proneness of the modules in which they are identified. To achieve this goal we present a case study on 47,751 methods extracted from two well-known open source projects. The results of the case study suggest that:(a) modules in which\" code smells\" are concentrated are more change-prone than smell-free modules,(b) there are specific types of\" code smells\" that are concentrated in the most change-prone modules, and (c) interest probability of code clones seems to be higher than the other two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["63"]}
{"title": "Non-functional requirements that influence gaming experience: A survey on gamers satisfaction factors\n", "abstract": " Requirements engineering is an extremely crucial phase in the software development lifecycle, because mishaps in this stage are usually expensive to fix in later development phases. In the domain of computer games, requirements engineering is a heavily studied research field (39.3% of published papers are dealing with requirements [1]), since it is considered substantially different from traditional software requirements engineering (see [1] and [14]). The main point of differentiation is that almost all computer games share a common key-driver as requirement, ie user satisfaction. In this paper, we investigate the most important user satisfaction factors from computer games, though a survey on regular gamers. The results of the study suggest that, user satisfaction factors are not uniform across different types of games (game genres), but are heavily dependent on them. Therefore, this study underlines the most\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["63"]}
{"title": "Combining metrics for software evolution assessment by means of Data Envelopment Analysis\n", "abstract": " Research and practice in software engineering have led to an extensive set of metrics for the evaluation of almost every aspect of software development. One of the major challenges for any quality model is the combination of metrics, which are complementary to each other. In this paper, we propose the use of Data Envelopment Analysis (DEA), a non\u0393\u00c7\u00c9parametric technique employed in economics, as a means of providing a unified view of selected design metrics. The proposed application of DEA aims at assessing the overall trend of quality during the evolution of software systems, by considering releases of a project as units to be ranked. An important benefit derived from the use of DEA is the ability to \u0393\u00c7\u00a3normalize\u0393\u00c7\u00a5 the evaluation over the size characteristics of the examined systems, which is vital when comparing projects of different scale. Results are presented for successive versions of two open\u0393\u00c7\u00c9source, one\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["63"]}
{"title": "Efficient output waveform evaluation of a CMOS inverter based on short\u0393\u00c7\u00c9circuit current prediction\n", "abstract": " A novel approach for obtaining the output waveform, the propagation delay and the short\u0393\u00c7\u00c9circuit power dissipation of a CMOS inverter is introduced. The output voltage is calculated by solving the circuit differential equation only for the conducting transistor while the effect of the short\u0393\u00c7\u00c9circuit current is considered as an additional charge, which has to be discharged through the conducting transistor causing a shift to the output waveform. The short\u0393\u00c7\u00c9circuit current as well as the corresponding discharging current are accurately predicted as functions of the required time shift of the output waveform. A program has been developed that implements the proposed method and the results prove that a significant speed improvement can be gained with a minor penalty in accuracy. Copyright \u252c\u2310 2002 John Wiley & Sons, Ltd.", "num_citations": "12\n", "authors": ["63"]}
{"title": "Student perceptions on the benefits and shortcomings of distributed pair programming assignments\n", "abstract": " Pair Programming (PP) has been extensively used for enhancing the learning of programming. Specifically, PP is considered to: make the learning of programming more pleasant, promote collaboration and communication between the members of pairs, encourage the sharing of knowledge and skills, and even improve code quality. More recently, systems have appeared that support Distributed Pair programming (DPP). DPP is considered to maintain all the benefits of PP and in addition to allow for the distributed collaboration of pairs from anywhere and at any time. However, DPP might impose limitations as well, such as the requirement from students to configure their systems and ensure a good Internet connection. In order to draw safer conclusions on the benefits and shortcomings of DPP and maximize its effects on the learning of programming, it is necessary to investigate its impact under real world situations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["63"]}
{"title": "Trends in object-oriented software evolution: Investigating network properties\n", "abstract": " The rise of social networks and the accompanying interest to study their evolution has stimulated a number of research efforts to analyze their growth patterns by means of network analysis. The inherent graph-like structure of object-oriented systems calls for the application of the corresponding methods and tools to analyze software evolution. In this paper we investigate network properties of two open-source systems and observe interesting phenomena regarding their growth. Relating the observed evolutionary trends to principles and laws of software design enables a highlevel assessment of tendencies in the underlying design quality.", "num_citations": "11\n", "authors": ["63"]}
{"title": "Energy estimation with systemC: a programmer's perspective\n", "abstract": " A modification to the SystemC library to enable power estimation of digital systems built upon a set of primitive logic gates is proposed. Acknowledging both the intense requirement for low power systems as well as the increasing use of SystemC as a modeling methodology, an approach for obtaining the dynamic power consumption of SystemC modules is presented. In order to correctly handle glitches during energy estimation, a simulation approach based on guarded evaluation is used. Emphasis is given to the fact that extensions to SystemC can be performed in a simple manner broadening the design and analysis possibilities of circuit designers. Even computer science students, with limited background on digital electronics, can easily grasp the concept of energy consumption and implement enhancements to SystemC, justifying its use as a common modeling platform between HW and SW designers.", "num_citations": "11\n", "authors": ["63"]}
{"title": "Modeling the transistor chain operation in CMOS gates for short channel devices\n", "abstract": " A detailed analysis of the transistor chain operation in CMOS gates is introduced. The chain is modeled by a transistor pair, according to the operating conditions of the structure. The system of differential equations for the derived chain model is solved and analytical expressions which accurately describe the temporal evolution of the output voltage are extracted. For the first time, a fully mathematical analysis without simplified step inputs and linear approximations of the output waveform, and without resistors replacing transistors, is presented. The width of the equivalent transistor that replaces all nonsaturated devices is efficiently calculated, eliminating previous inconsistencies in chain currents. A mapping algorithm for all possible input patterns to a scheme that can be handled analytically is also derived. The final results for the calculated response and the propagation delay of this structure are in excellent\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["63"]}
{"title": "Monitoring technical debt in an industrial setting\n", "abstract": " Context: Technical Debt (TD) quantification has been studied in the literature and is supported by various tools; however, there is no common ground on what information shall be presented to stakeholders. Similarly to other quality monitoring processes, it is desirable to provide several views of quality through a dashboard, in which metrics concerning the phenomenon of interest are displayed.Objective: The aim of this study is to investigate the indicators that shall be presented in such a dashboard, so as to:(a) be meaningful for industrial stakeholders,(b) present all necessary information, and (c) be simple enough so that stakeholders can use them.Method: We explore TD Management (TDM) activities (ie, measurement, prioritization, repayment) and choose the main concepts that need to be visualized, based on existing literature and toolsupport. Next, we perform a survey with 60 software engineers (ie, architects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["63"]}
{"title": "Factors affecting students\u0393\u00c7\u00d6 performance in distributed pair programming\n", "abstract": " Pair Programming has been shown to increase productivity and code quality not only in professional software development but also in the context of programming education. The provision of broadband Internet access gave rise to Distributed Pair Programming (DPP) enabling two programmers to collaborate remotely. To gain insight into the benefits of DPP, we performed an empirical study on an object-oriented programming course where 62 students carried out assignments through a DPP platform. The goal of the study is to investigate, in the context of DPP, whether prior programming skills (assessed at the level of student, his or her partner and pair) and pair compatibility are related to student performance. To further examine the effect of DPP on learning outcomes, we have studied whether a pair\u0393\u00c7\u00d6s performance on DPP assignments is related to the students\u0393\u00c7\u00d6 grade. The findings indicate that the student\u0393\u00c7\u00d6s actual\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["63"]}
{"title": "The relation between technical debt and corrective maintenance in PHP web applications\n", "abstract": " ContextTechnical Debt Management (TDM) refers to activities that are performed to prevent the accumulation of Technical Debt (TD) in software. The state-of-research on TDM lacks empirical evidence on the relationship between the amount of TD in a software module and the interest that it accumulates. Considering the fact that in the last years, a large portion of software applications are deployed in the web, we focus this study on PHP applications.ObjectiveAlthough the relation between debt amount and interest is well-defined in traditional economics (i.e., interest is proportional to the amount of debt), this relation has not yet been explored in the context of TD. To this end, the aim of this study is to investigate the relation between the amount of TD and the interest that has to be paid during corrective maintenance.MethodTo explore this relation, we performed a case study on 10 open source PHP projects. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["63"]}
{"title": "A spatiotemporal Data Envelopment Analysis (ST DEA) approach: the need to assess evolving units\n", "abstract": " One of the major challenges in measuring efficiency in terms of resources and outcomes is the assessment of the evolution of units over time. Although Data Envelopment Analysis (DEA) has been applied for time series datasets, DEA models, by construction, form the reference set for inefficient units (lambda values) based on their distance from the efficient frontier, that is, in a spatial manner. However, when dealing with temporal datasets, the proximity in time between units should also be taken into account, since it reflects the structural resemblance among time periods of a unit that evolves. In this paper, we propose a two-stage spatiotemporal DEA (S-T DEA) approach, which captures both the spatial and temporal dimension through a multi-objective programming model. In the first stage, DEA is solved iteratively extracting for each unit only previous DMUs as peers in its reference set. In the second stage\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["63"]}
{"title": "Benchmarking library and application software with Data Envelopment Analysis\n", "abstract": " Library software is generally believed to be well-structured and follows certain design guidelines due to the need of continuous evolution and stability of the respective APIs. We perform an empirical study to investigate whether the design of open-source library software is actually superior to that of application software. By analyzing certain design principles and heuristics that are considered important for API design, we extract a set of software metrics that are expected to reflect the improved nature of libraries. An initial comparison by conventional statistical analysis confirms the overall belief that products of different software size scale should not be compared by simply examining metric values in isolation. In this paper, we propose the use of Data Envelopment Analysis (DEA), borrowed from production economics, as a means of measuring and benchmarking the quality of different object-oriented software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["63"]}
{"title": "Entropy as a measure of object-oriented design quality\n", "abstract": " In this paper, object-oriented designs are approached from an information theoretic point of view and entropy is proposed as a design quality metric. One of the primary aims of object-orientation is the flexibility and the ease in extending a system\u0393\u00c7\u00d6s functionality, with limited alterations to existing modules. This feature is evaluated defining an appropriate probability space according to the number of unary associations enabling the definition of an entropy metric. The entropy of the next generation of an object-oriented system with enhanced functionality remains close to its previous level in case the added functionality affects a limited number of existing classes; on the other hand, a poorly designed system increases entropy drastically. In this way, not only a given system is evaluated but it is also possible to assess the degradation of a system and its \u0393\u00c7\u00a3distance\u0393\u00c7\u00a5 from the original design.", "num_citations": "10\n", "authors": ["63"]}
{"title": "Collapsing the CMOS transistor chain to an effective single equivalent transistor\n", "abstract": " A method for collapsing the transistor chain of CMOS gates to a single equivalent transistor is introduced. The width of the equivalent transistor is calculated taking into account the operating conditions of each transistor in the structure, resulting in very good agreement with SPICE simulations. Second-order effects such as carrier velocity saturation in submicron devices, body effect and coupling capacitance are considered and ramp inputs are used. The actual time point when the chain starts conducting which influences significantly the accuracy of the model is also extracted. Finally, an algorithm to collapse every possible input pattern to a single input is introduced.", "num_citations": "10\n", "authors": ["63"]}
{"title": "Technical debt forecasting: an empirical study on open-source repositories\n", "abstract": " Technical debt (TD) is commonly used to indicate additional costs caused by quality compromises that can yield short-term benefits in the software development process, but may negatively affect the long-term quality of software products. Predicting the future value of TD could facilitate decision-making tasks regarding software maintenance and assist developers and project managers in taking proactive actions regarding TD repayment. However, no notable contributions exist in the field of TD forecasting, indicating that it is a scarcely investigated field. To this end, in the present paper, we empirically evaluate the ability of machine learning (ML) methods to model and predict TD evolution. More specifically, an extensive study is conducted, based on a dataset that we constructed by obtaining weekly snapshots of fifteen open source software projects over three years and using two popular static analysis tools to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["63"]}
{"title": "The developer's dilemma: factors affecting the decision to repay code debt\n", "abstract": " The set of concepts collectively known as Technical Debt (TD) assume that software liabilities set up a context that can make a future change more costly or impossible; and therefore repaying the debt should be pursued. However, software developers often disagree with an automatically generated list of improvement suggestions, which they consider not fitting or important for their own code. To shed light into the reasons that drive developers to adopt or reject refactoring opportunities (ie TD repayment), we have performed an empirical study on the potential factors that affect the developers' decision to agree with the removal of a specific TD liability. The study has been addressed to the developers of four well-known open-source applications. To increase the response rate, a personalized assessment has first been sent to each developer, summarizing his/her own contribution to the TD of the corresponding project\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["63"]}
{"title": "Artificial intelligence and regression analysis in predicting ground water levels in public administration\n", "abstract": " Water level prediction of ground water can be considered as a very important tool in water resources management. This research implements artificial neural network models in order to build the optimal forecasting models for predicting the water levels and regression analysis in order to evaluate the prediction accuracy. In this research, an Artificial Neural Network Perceptron is applied in order to construct forecasting models for predicting water levels of ground water. The developed models are then compared each other in order to find the optimal one according the best performance that will lead to the most accurate predictions. Several topologies were tested in order to discover the best forecasting model. The different predictive models were constructed by implementing different number of the nodes in the hidden layers, also by testing different number of the hidden layers. The results showed an increased\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["63"]}
{"title": "Predicting environmental data in public management by using artificial intelligence\n", "abstract": " This paper focuses on building neural network models in order to predict environmental information that is crucial in environmental decision making in public management and planning. The application of artificial intelligence in various fields have been highly increased the last decades with the development of new neural network learning techniques and tools in constructing neural network models. In this research, the application of Feedforward Neural Network Models for predicting environmental data is implemented. Environmental data can play an important role in public administration and also in environmental management and planning by promoting sustainable decision making strategies. The levels of pollution factors were chosen to be predicted, since pollution factors are associated with public health problems which is of high importance. Several artificial neural network models were constructing by using different architectures regarding the number of the neurons in hidden layers, the number of the hidden layers and the input and output neurons in order to build the optimal model that would predict efficiently the environmental data. Multilayer Feedforward Perceptron was used in this research as it is the most suitable for time series forecasting according to many researchers. The forecasting model can be valuable for public administration, since it can be used as a tool for a more efficient environmental management and also in adopting proactive measures and policies.", "num_citations": "9\n", "authors": ["63"]}
{"title": "A comparative study on the effectiveness of patterns in software libraries and standalone applications\n", "abstract": " The existence of design pattern instances is often regarded as an indication of elaborate software design, since patterns have been reported in many studies as techniques that improve software quality properties. Driven by the widespread belief that software libraries excel in terms of design quality compared to standalone applications, this study investigates first whether this claim is confirmed and second whether the improved quality can be attributed to the use of patterns. In particular we examine: (a) whether libraries exhibit improved design quality in terms of metrics compared to standalone applications, (b) the intensity of use of design patterns in the two software categories and (c) whether there is any correlation of design patterns usage and design quality at system level. The results of the study suggest that, some of the quality properties are improved in library software although no significant difference in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["63"]}
{"title": "SEAgle: Effortless Software Evolution Analysis\n", "abstract": " The analysis of software evolution by means of mining public repositories has been established as one of the dominant approaches for empirical studies in software engineering. However, even the investigation of the simplest research question demands a mazy process involving installation and configuration of tools, climbing their learning curve and tedious collection of desired information. Acknowledging the need for effortless querying of remote repositories we introduce a Web-based 'one-click approach' to perform software evolution analysis of Git projects.", "num_citations": "9\n", "authors": ["63"]}
{"title": "BuCo Reporter: Mining Software and Bug Repositories.\n", "abstract": " Version Control and Bug Tracking Systems are essential tools in contemporary software development methods and are widely employed by development teams for systematic source code revision tracking and effective bug management. By combining information provided from both tools, a maintainer could shed light to various qualitative and quantitative characteristics of software projects. BuCo Reporter is a Java application that mines source code and bug repositories and by combining these kinds of information provides useful reports that describe project history. BuCo also calculates several bug and source code metrics. Its novelty lies in its modular structure which allows for effortless extensibility. Moreover the framework that is provided is very easy to install, use and modify without requiring background knowledge.", "num_citations": "9\n", "authors": ["63"]}
{"title": "Software engineering practices for scientific software development: A systematic mapping study\n", "abstract": " Background:The development of scientific software applications is far from trivial, due to the constant increase in the necessary complexity of these applications, their increasing size, and their need for intensive maintenance and reuse.Aim:To this end, developers of scientific software (who usually lack a formal computer science background) need to use appropriate software engineering (SE) practices. This paper describes the results of a systematic mapping study on the use of SE for scientific application development and their impact on software quality.Method:To achieve this goal we have performed a systematic mapping study on 359 papers. We first describe a catalog of SE practices used in scientific software development. Then, we discuss the quality attributes of interest that drive the application of these practices, as well as tentative side-effects of applying the practices on qualities.Results:The main findings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["63"]}
{"title": "Reusing code from stackoverflow: the effect on technical debt\n", "abstract": " Software reuse is a well-established software engineering process that aims at improving development productivity. Although reuse can be performed in a systematic way (e.g., through product lines), in practice, reuse is performed in many cases opportunistically, i.e., copying small code chunks either from the web or in-house developed projects. Knowledge sharing communities and especially StackOverflow constitute the primary source of code-related information for amateur and professional software developers. Despite the obvious benefit of increased productivity, reuse can have a mixed effect on the quality of the resulting code depending on the properties of the reused solutions. An efficient concept for capturing a wide-range of internal software qualities is the metaphor of Technical Debt which expresses the impact of shortcuts in software development on its maintenance costs. In this paper, we present the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["63"]}
{"title": "Exploring the relationship between software modularity and technical debt\n", "abstract": " Modularity is one of the key principles of software design. In order for a software system to be modular, it should be organized into modules that are highly coherent internally, whereas at the same time as independent from other modules as possible. In this paper we explore coupling and cohesion metrics at the software package level-i.e., one of most basic levels of software functional decomposition in object-oriented (OO) systems, with the aim of investigating their relation to the technical debt of each package. Current state-of-the-art tools in TD measurement are working on the source code level, and the extent to which they can unveil limitations at the architecture level (e.g., violations of the modularity principle), has not been explored so far. To achieve this goal, we conducted a case study on 1,200 packages retrieved from 20 well-known open source software projects. The results of the study suggested that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["63"]}
{"title": "Memory hierarchy exploration for low power architectures in embedded multimedia applications\n", "abstract": " Multimedia applications are characterized by an increased number of data transfer and storage operations due to real time requirements. Appropriate transformations can be applied at the algorithmic level to improve crucial implementation characteristics. The effect of the data-reuse transformations on power consumption, area and performance of multimedia applications realized on embedded cores is examined. As demonstrators, widely applicable video processing algorithmic kernels, namely the row-column decomposition DCT and its fast implementation found in MPEG-X, are used. Experimental results prove that significant improvements in power consumption can be achieved without performance degradation by the application of data-reuse transformations in combination with the use of a custom memory hierarchy.", "num_citations": "8\n", "authors": ["63"]}
{"title": "Temporal discounting in technical debt: how do software practitioners discount the future?\n", "abstract": " Technical Debt management decisions always imply a trade-off among outcomes at different points in time. In such intertemporal choices, distant outcomes are often valued lower than close ones, a phenomenon known as temporal discounting. Technical Debt research largely develops prescriptive approaches for how software engineers should make such decisions. Few have studied how they actually make them. This leaves open central questions about how software practitioners make decisions. This paper investigates how software practitioners discount uncertain future outcomes and whether they exhibit temporal discounting. We adopt experimental methods from intertemporal choice, an active area of research. We administered an online questionnaire to 33 developers from two companies in which we presented choices between developing a feature and making a longer-term investment in architecture. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["63"]}
{"title": "REACT-A Process for Improving Open-Source Software Reuse\n", "abstract": " Software reuse is a popular practice, which is constantly gaining ground among practitioners. The main reason for this is the potential that it provides for reducing development effort and increasing the end-product quality. At the same time, Open-Source Software (OSS) repositories are nowadays flourishing and can facilitate the reuse process, through the provision of a variety of software artifacts. However, up-to-date OSS reuse processes have mostly been opportunistic, leading to not fully capitalizing existing reuse potentials. In this study we propose a process (namely REACT) for improving planned OSS reuse practices, i.e., we define the activities that a software engineer can perform to reuse OSS artifacts. To illustrate the applicability of REACT, we provide an example, in which a mobile application is developed based upon the reuse of OSS artifacts. To validate the proposed process we compared the effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["63"]}
{"title": "Placement of entities in object-oriented systems by means of a single-objective genetic algorithm\n", "abstract": " Behavior and state allocation in object-oriented systems is a rather non-trivial task that is hard to master and automate since it is guided by conceptual criteria and therefore relies on human expertise. Since attributes and methods can be placed in the classes of a system in uncountable different ways, the task can be regarded as a search space exploration problem. In this paper we present our experience from treating this issue by a genetic algorithm, which in contrast to previous approaches, is aiming at single-objective optimization. The fitness function is based on a novel metric which ensures that optimization improves both coupling and cohesion. The approach has been implemented as an Eclipse plugin allowing the effortless experimentation on any system. The evaluation results indicate that the problem is suitable for single-objective genetic algorithms and that an optimal or near-optimal solution can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["63"]}
{"title": "Developing an environment for embedded software energy estimation\n", "abstract": " The paper presents the results of a novel method for the instruction-level energy consumption measurement and the corresponding modeling approach for embedded microprocessors. According to the proposed method the base and inter-instruction energy costs of the ARM7TDMI embedded processor as well as the energy cost due to different values in the instruction parameters are modeled. These models can be used in the estimation of the energy consumed by the processor to execute real software programs. A software tool has been developed to automate energy estimation.", "num_citations": "7\n", "authors": ["63"]}
{"title": "CODE reuse in practice: Benefiting or harming technical debt\n", "abstract": " During the last years the TD community is striving to offer methods and tools for reducing the amount of TD, but also understand the underlying concepts. One popular practice that still has not been investigated in the context of TD, is software reuse. The aim of this paper is to investigate the relation between white-box code reuse and TD principal and interest. In particular, we target at unveiling if the reuse of code can lead to software with better levels of TD. To achieve this goal, we performed a case study on approximately 400 OSS systems, comprised of 897 thousand classes, and compare the levels of TD for reused and natively-written classes. The results of the study suggest that reused code usually has less TD interest; however, the amount of principal in them is higher. A synthesized view of the aforementioned results suggest that software engineers shall opt to reuse code when necessary, since apart from the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["63"]}
{"title": "Applying the Single Responsibility Principle in Industry: Modularity Benefits and Trade-offs\n", "abstract": " Refactoring is a prevalent technique that can be applied for improving software structural quality. Refactorings can be applied at different levels of granularity to resolve'bad smells' that can be identified in various artifacts (eg, methods, classes, packages). A fundamental software engineering principle that can be applied at various levels of granularity is the Single Responsibility Principle (SRP), whose violation leads to the creation of lengthy, complex and non-cohesive artifacts; incurring smells like Long Method, God Class, and Large Package. Such artifacts, apart from being large in size tend to implement more than one functionalities, leading to decreased cohesion, and increased coupling. In this paper, we study the effect of applying refactorings that lead to conformance to the SRP, at all three levels of granularity to identify possible differences between them. To study these differences, we performed an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["63"]}
{"title": "Evaluating Power Efficient Data-Reuse Decisions for Embedded Multimedia Applications: An Analytical Approach\n", "abstract": " Power consumption of multimedia applications executing on embedded cores is heavily dependent on data transfers between system memory and processing units. The purpose of this paper is to extend an existing power optimizing methodology based on data-reuse decisions, in order to determine the optimal solution in a rapid and reliable way. An analytical approach is proposed by extracting expressions for the number of accesses to each memory layer. Moreover, the design space is further reduced since these analytical expressions are calculated only for a subset of all transformations. The results concerning the power efficiency of data-reuse transformations are in agreement to those in previous studies. However, the exploration time of the design space is significantly reduced. The proposed methodology is also applied to the case of multiple parallel processing cores, proving that the relative effect of each\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["63"]}
{"title": "Output waveform evaluation of basic pass transistor structure\n", "abstract": " Pass transistor logic is a promising alternative to conventional CMOS logic for low-power high-performance applications due to the decreased node capacitance and reduced transistor count it offers. However, the lack of supporting design automation tools has hindered the widespread application of pass transistors. In this paper, a simple and robust modeling technique for the timing analysis of the basic pass transistor structure is presented. The proposed methodology is based on the actual phenomena that govern the operation of the pass transistor and enables fast timing simulation of circuits that employ pass transistors as controlled switches without significant loss of accuracy, compared to SPICE simulation.", "num_citations": "6\n", "authors": ["63"]}
{"title": "Jcaliper: search-based technical debt management\n", "abstract": " Technical Debt (TD) reflects problems in software maintainability along evolution. TD principal is defined as the effort required for refactoring an existing system to an ideal one (aka optimal) that suffers from no maintainability problems. One of the open problems in the TD community is that ideal versions of systems do not exist, and there are no methods in the literature for approaching them, even theoretically. To alleviate this problem, in this paper we propose an efficient TD management strategy, by applying Search-Based Software Engineering techniques. In particular, we focus on one specific aspect of TD, namely inefficient software modularity, by properly assigning behavior and state to classes through search space exploration. At the same time, in the context of TD, we:(a) investigate the use of local search algorithms to obtain a near-optimum solution and propose TD repayment actions (ie, refactorings), and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["63"]}
{"title": "Structural quality metrics as indicators of the long method bad smell: An empirical study\n", "abstract": " Empirical evidence has pointed out that Extract Method refactorings are among the most commonly applied refactorings by software developers. The identification of Long Method code smells and the ranking of the associated refactoring opportunities is largely based on the use of metrics, primarily with measures of cohesion, size and coupling. Despite the relevance of these proper-ties to the presence of large, complex and non-cohesive pieces of code, the empirical validation of these metrics has exhibited relatively low accuracy (max precision: 66%) regarding their predictive power for long methods or extract method opportunities. In this work we perform an empirical validation of the ability of cohesion, coupling and size metrics to predict the existence and the intensity of long method occurrences. According to the statistical analysis, the existence and the intensity of the Long Method smell can be effectively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["63"]}
{"title": "Assessing Change Proneness at the Architecture Level: An Empirical Validation\n", "abstract": " Change proneness is a characteristic of software artifacts that represents their probability to change in future. Change proneness can be assessed at different levels of granularity, ranging from classes to modules. Although change proneness can be successfully assessed at the source code level (i.e., methods and classes), it remains rather unexplored for architectures. Additionally, the methods that have been introduced at the source code level are not directly transferable to the architecture level. In this paper, we propose and empirically validate a method for assessing the change proneness of architectural modules. Assessing change proneness at the level of architectural modules requires information from two sources: (a) the history of changes in the module, as a proxy of how frequently the module itself undergoes changes; and (b) the dependencies with other modules that affect the probability of a change\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["63"]}
{"title": "Monitoring an OOP course through assignments in a distributed pair programming system\n", "abstract": " Distributed Pair Programming (DPP) is widely known to promote collaboration and knowledge sharing among novice programmers, while it engages them in carrying out programming assignments. Moreover, DPP is a means of experiencing agile software development techniques that are considered important in the software market. In this paper, we share some experiences on using the DPP system of SCEPPSys for carrying out assignments in an undergraduate Object Oriented Programming (OOP) course. Specifically, we focus on the information recorded during problem solving and the statistics reported by the system and how this information can be utilized for monitoring both the course regarding the fulfillment of its goals and the programming habits and progress of students. Some proposals are made towards extending the possibilities of SCEPPSys for generating automatically more sophisticated reports that would support instructors in more successfully  monitoring a course and students. The ultimate goal of such an enhanced monitoring is to improve students\u0393\u00c7\u00d6 software quality.", "num_citations": "5\n", "authors": ["63"]}
{"title": "A Taxonomy of Evaluation Approaches in Software Engineering\n", "abstract": " As in any academic discipline, the evaluation of proposed methodologies and techniques is of vital importance for assessing the validity of novel ideas or findings in Software Engineering. Over the years, a large number of evaluation approaches have been employed, some of them drawn from other domains and other particularly developed for the needs of software engineering related research. In this paper we present the results of a survey of evaluation techniques that have been utilized in research papers that appeared in three leading software engineering journal and propose a taxonomy of evaluation approaches which might be helpful towards the organization of knowledge regarding the different strategies for the validation of research outcomes. The applicability of the proposed taxonomy has been evaluated by classifying the articles retrieved from ICSE'2012.", "num_citations": "5\n", "authors": ["63"]}
{"title": "SEANets: Software evolution analysis with networks\n", "abstract": " Evolving software systems can be systematically studied by treating them as networks and employing ideas and techniques from the field of Social Network Analysis. SEANets is an Eclipse plugin that allows the analysis of multiple software versions and the extraction and visualization of network properties required to investigate evolutionary trends of the underlying system.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Moving from requirements to design confronting security issues: A case study\n", "abstract": " Since the emergence of software security as a research area, it has been evident that security should be incorporated as early as possible in the software lifecycle. The advantage is that large gains can be achieved in terms of cost and effort compared to the introduction of security as an afterthought. The earliest possible phase to consider possible attacks is during requirements specification. A widely accepted approach to consider security in the requirements is the employment of misuse cases. In this paper we examine a case study to automatically generate a class diagram, based on the use and misuse cases present in the requirements. Particularly, we extend a natural language processing approach to move beyond a general domain model and produce a detailed class diagram. Moreover, security patterns are introduced in appropriate places of the design to confront the documented attacks and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["63"]}
{"title": "A novel approach to automated design pattern detection\n", "abstract": " The importance of the use of Design Patterns in order to build reusable and wellstructured software has been eminent since these patterns have been formalized. Thus, it became desirable to be able to detect which design patterns are present in a software system. Knowing this information it is possible to make an evaluation on different aspects of the system. Though, it is a very difficult task for a software engineer to pinpoint all the Design Patterns present in a system, without any assistance. Addressing this need, techniques for automated design pattern detection have appeared in the literature. Some are based on reverseengineering of already existing code while others can work already at design level by analyzing UML diagrams. Though, complexity is one of the characteristics of all the methods proposed until now. Furthermore, all of these techniques work only for a limited number of the GoF patterns. Our aim in this paper is to elaborate on a simple approach for automatic detection of design patterns that works by analyzing UML class diagrams. Our method can achieve the automated detection of all GoF patterns that do not require any code specific information to recognize them.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Teaching queuing systems modeling using UML\n", "abstract": " In this paper we suggest a new approach in the way that a network modeling and simulation or a queuing theory course can be developed. Beyond the mathematical model that sometimes is too difficult for students to understand, this paper introduces the use of Unified Modeling Language as the mean to teach modeling of discrete event systems such as queues and networks. The basic course scheduling and teaching material are presented too.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Timing analysis of pass transistor and CPL gates\n", "abstract": " Pass Transistor Logic and Complementary Pass-transistor Logic are becoming increasingly important in the design of a specific class of digital integrated circuits due to their speed and power efficiency compared to conventional CMOS logic. A simple and accurate technique for the timing analysis of gates that involve pass transistor logic is introduced. This investigation offers for the first time the possibility to simulate PTL/CPL gates by partitioning the behavior of complex structures into well defined subcircuits whose interaction is separately studied.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Estimating starting point of conduction of CMOS gates\n", "abstract": " To model effectively the output waveform and propagation delay of a CMOS gate, knowledge of the time point at which it starts to conduct is essential. An efficient method for calculating analytically this time point taking into account the structure of the gate and the input waveform, is introduced. Such a method can easily be integrated into a timing analysis system.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Input mapping algorithm for modelling of CMOS circuits\n", "abstract": " An algorithm for mapping every possible input pattern of a complementary metal oxide semiconductor (CMOS) gate to an equivalent set of normalised inputs (inputs which have the same starting point and transition time) is presented. Such an algorithm is required in order to perform analytical modelling of CMOS gates, and the results obtained are very accurate compared to SPICE simulations.", "num_citations": "5\n", "authors": ["63"]}
{"title": "Exploring the relation between technical debt principal and interest: An empirical approach\n", "abstract": " ContextThe cornerstones of technical debt (TD) are two concepts borrowed from economics: principal and interest. Although in economics the two terms are related, in TD there is no study on this direction so as to validate the strength of the metaphor.ObjectiveWe study the relation between Principal and Interest, and subsequently dig further into the \u0393\u00c7\u00ffingredients\u0393\u00c7\u00d6 of each concept (since they are multi-faceted). In particular, we investigate if artifacts with similar levels of TD Principal exhibit a similar amount of TD Interest, and vice-versa.MethodTo achieve this goal, we performed an empirical study, analyzing the dataset using the Mantel test. Through the Mantel test, we examined the relation between TD Principal and Interest, and identified aspects that are able to denote proximity of artifacts, with respect to TD. Next, through Linear Mixed Effects (LME) modelling we studied the generalizability of the results.ResultsThe\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "A preliminary Study of Knowledge Sharing related to Covid-19 Pandemic in Stack Overflow\n", "abstract": " The Covid-19 outbreak has changed to an unprecedented extent almost every aspect of human activity. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open \u0393\u00c7\u00ffquestion & answer\u0393\u00c7\u00d6 communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. Through the study of 464 Stack Overflow questions posted in February and March 2020 and leveraging the power of text mining, we attempt to shed light into the interest of developers in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "The sdk4ed platform for embedded software quality improvement-preliminary overview\n", "abstract": " Maintaining high level of quality with respect to important quality attributes is critical for the success of modern software applications. Hence, appropriate tooling is required to help developers and project managers monitor and optimize software quality throughout the overall Software Development Lifecycle (SDLC). Moreover, embedded software engineers and developers need support to manage complex interdependencies and inherent trade-offs between design and run-time qualities. To this end, in an attempt to address these issues, we are developing the SDK4ED Platform as part of the ongoing EU-funded SDK4ED project, a software quality system that enables the monitoring and optimization of software quality, with emphasis on embedded software. The purpose of this technical paper is to provide an overview of the SDK4ED Platform and present the main novel functionalities that have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "Guidelines for Managing Threats to Validity of Secondary Studies in Software Engineering\n", "abstract": " Secondary studies review and compile data retrieved from primary studies and are vulnerable to factors that threaten their validity as any other research method. Considering that secondary studies are often used to support the evidence-based paradigm, it is crucial to properly manage their threats, i.e., identify, categorize, mitigate, and report them. In this chapter, we build upon the outcomes of a systematic review of secondary studies in software engineering, which has identified (a) the most common threats to validity and corresponding mitigation actions and (b) the categories in which threats to validity can be classified, so as to guide the authors of future secondary studies in managing the threats to validity of their work. To achieve this goal, we describe (a) a classification schema for reporting threats to validity and possible mitigation actions and (b) a checklist, which authors of secondary studies can use for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "Automatic identification of assumptions from the Hibernate developer mailing list\n", "abstract": " During the software development life cycle, assumptions are an important type of software development knowledge that can be extracted from textual artifacts. Analyzing assumptions can help to, for example, comprehend software design and further facilitate software maintenance. Manual identification of assumptions by stakeholders is rather time-consuming, especially when analyzing a large dataset of textual artifacts. To address this problem, one promising way is to use automatic techniques for assumption identification. In this study, we conducted an experiment to evaluate the performance of existing machine learning classification algorithms for automatic assumption identification, through a dataset extracted from the Hibernate developer mailing list. The dataset is composed of 400 \"Assumption\" sentences and 400 \"Non-Assumption\" sentences. Seven classifiers using different machine learning algorithms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "Temporal discounting in software engineering: a replication study\n", "abstract": " Background: Many decisions made in Software Engineering practices are intertemporal choices: trade-offs in time between closer options with potential short-term benefit and future options with potential long-term benefit. However, how software professionals make intertemporal decisions is not well understood. Aim: This paper investigates how shifting time frames influence preferences in software projects in relation to purposefully selected background factors. Method: We investigate temporal discounting by replicating a questionnaire-based observational study. The replication uses a changed-population and -experimenter design to increase the internal and external validity of the original results. Results: The results of this study confirm the occurrence of temporal discounting in samples of both professional and student participants from different countries and demonstrate strong variance in discounting between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["63"]}
{"title": "Investigating the effect of evolution and refactorings on feature scattering\n", "abstract": " The implementation of a functional requirement is often distributed across several modules posing difficulties to software maintenance. In this paper, we attempt to quantify the extent of feature scattering and study its evolution with the passage of software versions. To this end, we trace the classes and methods involved in the implementation of a feature, apply formal approaches for studying variations across versions, measure whether feature implementation is uniformly distributed and visualize the reuse among features. Moreover, we investigate the impact of refactoring application on feature scattering in order to assess the circumstances under which a refactoring might improve the distribution of methods implementing a feature. The proposed techniques are exemplified for various features on several versions of four open-source projects.", "num_citations": "4\n", "authors": ["63"]}
{"title": "An assessment of design patterns' influence on a java-based e-commerce application\n", "abstract": " Design patterns, acting as recurring solutions to common problems, offer significant benefits such as avoiding unnecessary complexity, and promoting code reuse, maintainability and extensibility. This paper describes how four not technology-specific or language-specific design patterns (Front Controller, Model View Controller, Transfer Object and Service to Worker) can be applied to one typical e-commerce application developed using Java EE platform. The first goal is to evaluate the improvement of design properties after the implementation of each design pattern using software metrics. Another goal is to assess the influence of design patterns on the maintainability of the e-commerce application under study by examining the evolution of software metrics when performing certain extensions. The results indicate that the application of patterns positively influences design properties such as coupling, complexity and messaging implying a possible improvement in high-level quality attributes such as flexibility, extensibility and reusability.", "num_citations": "4\n", "authors": ["63"]}
{"title": "Lessons learned from an open-source University project\n", "abstract": " Open-source software development has become a widespread trend within the software engineering community and has begun to attract the attention of other disciplines as well. To help students understand the practices used within the open-source movement and to initiate an effort for logging all aspects of the software development process, our Laboratory has set up an open-source project targeting undergraduate and postgraduate computer science students. The main aim of the project is to systematically record architecture and code related information as well as\" soft\" issues related to the individuals that take part in the development. The paper discusses results and conclusions drawn from this project.", "num_citations": "4\n", "authors": ["63"]}
{"title": "Simulation software for a network modelling lab\n", "abstract": " We introduce a software application that simulates the most common types of queues. It also compares the simulation results with the values that the mathematical model predicts for specific types of queues. Thus, the software we have developed has two academic aspects. The first one applies to its ability to simulate most of the queuing processes that take place in computer networks. Therefore someone who studies network modelling or computer performance analysis may use it to derive some practical results in a very convenient way. The second aspect of the software refers to the part of the application that simulates specific types of queue and it can be used as a guide for teaching network modelling. We believe that it is very easy for someone to understand the aspects of queuing theory even though he/she is not familiar with it.", "num_citations": "4\n", "authors": ["63"]}
{"title": "Single transistor primitive for timing and power modelling of CMOS gates\n", "abstract": " An accurate and efficient method for modelling CMOS gates by a single equivalent transistor is introduced in this paper. The output waveform of a CMOS inverter is obtained by solving the circuit differential equation considering only the conducting transistor of the inverter. The effect of the short-circuiting transistor is incorporated as a differentiation of the width of the conducting transistor. The proposed model is the simplest primitive that can be used in order to obtain the propagation delay and short-circuit power dissipation of CMOS gates. Consequently, it can offer significant speed improvement to existing dynamic timing and power simulators while maintaining a sufficient level of accuracy.", "num_citations": "4\n", "authors": ["63"]}
{"title": "Can Clean New Code reduce Technical Debt Density\n", "abstract": " While technical debt grows in absolute numbers as software systems evolve over time, the density of technical debt (technical debt divided by lines of code) is reduced in some cases. This can be explained by either the application of refactorings or the development of new artifacts with limited Technical Debt. In this paper we explore the second explanation, by investigating the relation between the amount of Technical Debt in new code and the evolution of Technical Debt in the system. To this end, we compare the Technical Debt Density of new code with existing code, and we investigate which of the three major types of code changes (additions, deletions and modifications) is primarily responsible for changes in the evolution of Technical Debt density. Furthermore, we study whether there is a relation between code quality practices and the \u0393\u00c7\u00a3cleanness\u0393\u00c7\u00a5 of new code. To obtain the required data, we have performed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "On the temporality of introducing code technical debt\n", "abstract": " Code Technical Debt (TD) is intentionally or unintentionally created when developers introduce inefficiencies in the codebase. This can be attributed to various reasons such as heavy work-load, tight delivery schedule, unawareness of good practices, etc. To shed light into the context that leads to technical debt accumulation, in this paper we investigate: (a) the temporality of code technical debt introduction in new methods, i.e., whether the introduction of technical debt is stable across the lifespan of the project, or if its evolution presents spikes; and (b) the relation of technical debt introduction and the development team\u0393\u00c7\u00d6s workload in a given period. To answer these questions, we perform a case study on twenty-seven Apache projects, and inspect the number of Technical Debt Items introduced in 6-month sliding temporal windows. The results of the study suggest that: (a) overall, the number of Technical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "Investigating trade-offs between portability, performance and maintainability in exascale systems\n", "abstract": " Due to the rapid advancements in the hardware architectures of High-Performance Computing infrastructures, new challenges have arisen in the development of scientific software applications. In particular, software that runs on Exascale machines, needs to be highly portable, highly parallelizable and at the same time maintainable, since software for HPC evolves constantly over time. By taking into account that an overall optimization of all the aforementioned qualities is not realistic, in this study, we explore the possible trade-offs, when optimizing the run-time qualities of the software (i.e., performance and portability) through state-of-practice techniques in Exascale software development, in expense of code maintainability, as expressed by technical debt. To achieve this goal, we have performed a case study, in which the effect of run-time optimizations on technical debt has been measured. The results suggest that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "Exploring the frequency and change proneness of dynamic feature pattern instances in PHP applications\n", "abstract": " Although numerous technologies are available for developing web applications, PHP holds the lions' share of web content today. PHP offers several features that enable developers to easily produce dynamically extendible code, forming an entire ecosystem of standard as well as more \u0393\u00c7\u00ffexotic\u0393\u00c7\u00d6 opportunities that can be exploited. One reason that drives developers to rely on the dynamic features of a scripting language is to enable effortless functionality extensions. The aim of this work is twofold: initially, we (a) provide an overview of all possible dynamically extendible code patterns (i.e., either through method invocation, or object instantiation) and (b) investigate their frequency by mining the code base of ten milestone PHP projects to identify the subset of patterns that developers actually use. Next, in order to investigate whether the expected flexibility of these patterns stands in practice, we examine if code chunks\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "EXA2PRO programming environment: Architecture and Applications\n", "abstract": " The EXA2PRO programming environment will integrate a set of tools and methodologies that will allow to systematically address many exascale computing challenges, including performance, performance portability, programmability, abstraction and reusability, fault tolerance and technical debt. The EXA2PRO tool-chain will enable the efficient deployment of applications in exascale computing systems, by integrating high-level software abstractions that offer performance portability and efficient exploitation of exascale systems' heterogeneity, tools for efficient memory management, optimizations based on trade-offs between various metrics and fault-tolerance support. Hence, by addressing various aspects of productivity challenges, EXA2PRO is expected to have significant impact in the transition to exascale computing, as well as impact from the perspective of applications. The evaluation will be based on 4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "Applying Levenberg Marquardt Algorithmin Feedforward Neural Network Models for Predicting Crime in Public Management\n", "abstract": " Artificial intelligence applications have been tremendously increased in various science fields, the last decades, with the development of new machine learning techniques and algorithms and of new artificial neural network tools for developing neural network models. In this research, the application of Levenberg Marquardt algorithm in Feedforward Neural Network Models for predicting crime urban data is implemented. The Levenberg Marquardt algorithm is a combination of the steepest descent algorithm and the Gauss-Newton algorithm which is used for solving non-linear least-squares problems. The algorithm combines the minimization advantages of the steepest descent method with the quadratic model of Gauss-Newton method in order to increase the speed of the overall process of finding the minimum of a function. The Levenberg Marquardt algorithm was utilized as it is considered as the most suitable for time series predictions, among several training algorithms. Urban crime forecasting can play a significant role in urban planning and public management by facilitating decision making and the adoption of the most adequate proactive strategies in crime prevention and in public safety management planning.", "num_citations": "3\n", "authors": ["63"]}
{"title": "An application of data envelopment analysis to software quality assessment\n", "abstract": " Data Envelopment Analysis (DEA) is a non-parametric technique which involves the use of linear programming methods to measure the efficiency of a homogenous set of units. These units are known as Decision Making Units (DMUs) and defined by multiple input and output data. Efficiencies are measured relative to a piece-wise surface (efficient frontier) which envelops the data, thus justifying the name of the technique. Although DEA has been mostly used in production economics, its application in the context of software quality evaluation seems to be a promising approach. This study provides an application of DEA to assess the evolution of two open-source software projects in terms of selected metric values for successive versions of each project. What is really interesting in DEA is that a single efficiency score is calculated for each version despite the often convoluted overall picture of the metric values\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["63"]}
{"title": "Evaluating the Effect of Data-Reuse Transformations on Processor Power Consumption\n", "abstract": " Processor power savings that can be obtained by the application of data-reuse transformations on multimedia applications are discussed in this paper. Data Transfer and Storage Exploration methodologies primarily aim at memory related power reduction by moving data accesses to smaller memories, which are less power costly. However, it is shown that the applied code transformations have also a significant effect on the processor power consumption. Physical measurements of the average current drawn by instruction sequences provide a way to evaluate the processor power consumption of alternative code implementations. Simulation results prove that code transformations based on memory hierarchy exploration can have a significantly larger impact on power than existing software energy optimizing methodologies.", "num_citations": "3\n", "authors": ["63"]}
{"title": "A clustering approach towards cross-project technical debt forecasting\n", "abstract": " Technical debt (TD) describes quality compromises that can yield short-term benefits but may negatively affect the quality of software products in the long run. A wide range of tools and techniques have been introduced over the years in order for the developers to be able to determine and manage TD. However, being able to also predict its future evolution is of equal importance to avoid its accumulation, and, in turn, the unlikely event of making the project unmaintainable. Although recent research endeavors have showcased the feasibility of building accurate project-specific TD forecasting models, there is a gap in the field regarding cross-project TD forecasting. Cross-project TD forecasting is of practical importance, since it would enable the application of pre-existing forecasting models on previously unknown software projects, especially new projects that do not exhibit sufficient commit history to enable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "Architectural decision-making as a financial investment: An industrial case study\n", "abstract": " ContextMaking architectural decisions is a crucial task but also very difficult, considering the scope of the decisions and their impact on quality attributes. To make matters worse, architectural decisions need to combine both technical and business factors, which are very dissimilar by nature.ObjectivesWe provide a cost-benefit approach and supporting tooling that treats architectural decisions as financial investments by: (a) combining both technical and business factors; and (b) transforming the involved factors into currency, allowing their uniform aggregation. Apart from illustrating the method, we validate both the proposed approach and the tool, in terms of fitness for purpose, usability, and potential limitations.MethodTo validate the approach, we have performed a case study in a software development company, in the domain of low-energy embedded systems. We employed triangulation in the data collection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "Evaluating the agreement among technical debt measurement tools: building an empirical benchmark of technical debt liabilities\n", "abstract": " Software teams are often asked to deliver new features within strict deadlines leading developers to deliberately or inadvertently serve \u0393\u00c7\u00a3not quite right code\u0393\u00c7\u00a5 compromising software quality and maintainability. This non-ideal state of software is efficiently captured by the Technical Debt (TD) metaphor, which reflects the additional effort that has to be spent to maintain software. Although several tools are available for assessing TD, each tool essentially checks software against a particular ruleset. The use of different rulesets can often be beneficial as it leads to the identification of a wider set of problems; however, for the common usage scenario where developers or researchers rely on a single tool, diverse estimates of TD and the identification of different mitigation actions limits the credibility and applicability of the findings. The objective of this study is two-fold: First, we evaluate the degree of agreement among\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "Complexity Clustering of BPMN Models: Initial Experiments with the K-means Algorithm\n", "abstract": " This paper introduces a method to assess the complexity of process models by utilizing a cluster analysis technique. The presented method aims to facilitate multi-criteria decision making and process objective management, through the combination of specific quality indicators. This is achieved by leveraging established complexity metrics from literature, and combining three complementary ones (i.e. NOAJS, CFC and CNC) to a single weighted measure, offering an integrated scheme for evaluating complexity. K-means clustering algorithm is implemented on 87 eligible models, out of a repository of 1000 models, and classifies them to corollary clusters that correspond to complexity levels. By assigning weighted impact on specific complexity metrics -an action that leads to the production of threshold values- cluster centroids can fluctuate, thus produce customized model categorizations. The paper demonstrates\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "A metric suite for evaluating interactive scenarios in video games: An empirical validation\n", "abstract": " Game development is one of the fastest growing industries. Since games' success is mostly related to users' enjoyment, one of the cornerstones of their quality assessment is the evaluation from the user perspective. According to literature, game scenario constitutes a key-factor that leads to users' enjoyment. Despite their importance, scenarios are currently evaluated through heuristics in a subjective way. The aim of this paper is to develop an objective model (ie, a set of quality attributes and metrics) for evaluating game scenarios with respect to users' satisfaction. The proposed model can be applied to flow charts and character models (ie, common game scenario representation mechanisms). To achieve this goal, we:(a) gathered game scenario characteristics that are related to users' satisfaction,(b) proposed several metrics for quantifying these characteristics, and (c) performed a case study on three interactive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "REI: An integrated measure for software reusability\n", "abstract": " To capitalize upon the benefits of software reuse, an efficient selection among candidate reusable assets should be performed in terms of functional fitness and adaptability. The reusability of assets is usually measured through reusability indices. However, these do not capture all facets of reusability, such as structural characteristics, external quality attributes, and documentation. In this paper, we propose a reusability index (REI) as a synthesis of various software metrics and evaluate its ability to quantify reuse, based on IEEE Standard on Software Metrics Validity. The proposed index is compared with existing ones through a case study on 80 reusable open\u0393\u00c7\u00c9source assets. To illustrate the applicability of the proposed index, we performed a pilot study, where real\u0393\u00c7\u00c9world reuse decisions have been compared with decisions imposed by the use of metrics (including REI). The results of the study suggest that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "Artificial intelligence in atmospheric PM10 forecasting in public and environmental management\n", "abstract": " Artificial intelligence has invaded in various fields the last years with the development of new technologies and techniques and for constructing artificial neural network models. In this research, the application of Artificial Neural Network models is implemented for building predictive models in order to forecast the concentrations of atmospheric Particulate Matter PM10 in urban environment. In order to construct the optimal artificial neural network predictive model several topologies were investigated. Several researchers have studied the association of air pollution with human health problems and specifically with respiratory diseases. Also, several studies have investigated the negative effects of PM10 pollution on human health. Various diseases are associated with air pollution concentrations of various factors such as Particulate Matter PM10. The findings of the proposed methodology can provide predictions of Particulate Matter PM10 that can play an important role in urban and public management since they can be used in order to help the authorities at adopting proactive measures for preventing further air pollution and environmental degradation and also for protecting public health.", "num_citations": "2\n", "authors": ["63"]}
{"title": "Blending an Android development course with software engineering concepts\n", "abstract": " The tremendous popularity of mobile computing and Android in particular has attracted millions of developers who see opportunities for building their own start-ups. As a consequence Computer Science students express an increasing interest into the related technology of Java development for Android applications. Android projects are complex by nature and relatively large software products while their development calls for the application of established software engineering practices and tools. However, most software engineering courses focus on \u0393\u00c7\u00ffconventional\u0393\u00c7\u00d6 software development for desktop or web applications. In this paper we report on the design, implementation and assessment of a novel short course aiming at bridging the gap between software engineering and Android development. The goal is to demonstrate the need for applying software engineering principles on Android development as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "A case study on the availability of open-source components for game development\n", "abstract": " Nowadays the amount of source code that is freely available inside open-source software repositories offers great reuse opportunities to software developers. Therefore, it is expected that the implementation of several requirements can be facilitated by reusing open source software components. In this paper, we focus on the reuse opportunities that can be offered in one specific application domain, i.e., game development. In particular, we performed an embedded multiple case study on approximately 110 open-source games, exploiting a large-scale repository of OSS components, and investigated: (a) which game genres can benefit from open source reuse, and (b) what types of requirements can the available open-source components map to. The results of the case study suggest that: (a) game genres with complex game logic, e.g., First Person Shooter, Strategy, Role-Playing, and Sport games offer the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["63"]}
{"title": "Assessing the evolution of quality in java libraries\n", "abstract": " Libraries are increasingly employed in software practice to speed up the development process by reusing available and tested components. Software systems, that are available as libraries, are expected to be well-designed, because they have to adhere to specific principles, in order to accommodate the needs of multiple clients in a robust and stable way. Considering that most software libraries are continuously upgraded, in this paper we investigate the evolution of their quality over time. In particular, we perform a systematic case study to assess whether quality, in terms of three software metrics (CBO, LCOM, WMC), exhibits clear trends during the history of twenty analyzed libraries. The findings indicate that the examined software libraries can be considered as stable software projects in terms of quality, in the sense that in contrast to the general belief about software aging, their quality does not degrade over time.", "num_citations": "2\n", "authors": ["63"]}
{"title": "Evaluation of a queuing theory and systems modeling course based on UML\n", "abstract": " This paper presents how a new teaching method in the way that a queuing theory and systems modeling or simulation course can be done, was evaluated by the teachers and the students that attended the course and answered a questionnaire. This course is based on the use of Unified Modeling Language (UML) as the mean to teach modeling of discrete event systems such as queues and networks and not on Mathematics that sometimes is too difficult for students to understand", "num_citations": "2\n", "authors": ["63"]}
{"title": "Power exploration of parallel embedded architectures implementing data-reuse transformations\n", "abstract": " Efficient use of data-reuse transformations combined with a custom memory hierarchy that exploits the temporal locality of data related memory accesses can have a significant impact on system power consumption, especially in data dominated applications e.g. multimedia processing. In this paper the effect of data-reuse decisions on power consumption, area and performance of multimedia applications implemented on uni- and dual-processor embedded cores is explored. By this work it is clarified that conclusions for the transformations effect on multi-processor architectures can be extracted by the corresponding effect on the uniprocessor architecture. In this way the exploration space can be significantly reduced. A motion estimation algorithm, namely the two-dimensional logarithmic search, and a discrete cosine transform (DCT) algorithm are used as demonstrator applications.", "num_citations": "2\n", "authors": ["63"]}
{"title": "CMOS gate modeling based on equivalent inverter\n", "abstract": " A method for modeling complex CMOS gates by the reduction of each gate to an effective equivalent inverter is introduced. The conducting and parasitic behavior of parallel and serially connected transistors is accurately analyzed and an equivalent transistor is extracted for each case, taking into account the actual operating conditions of each device in the structure. The accuracy of the method is validated by the results for two submicron technologies and its efficiency as a technique that can improve existing timing simulators is demonstrated.", "num_citations": "2\n", "authors": ["63"]}
{"title": "RepoSkillMiner: identifying software expertise from GitHub repositories using natural language processing\n", "abstract": " A GitHub profile is becoming an essential part of a developer's resume enabling HR departments to extract someone's expertise, through automated analysis of his/her contribution to open-source projects. At the same time, having clear insights on the technologies used in a project can be very beneficial for resource allocation and project maintainability planning. In the literature, one can identify various approaches for identifying expertise on programming languages, based on the projects that developer contributed to. In this paper, we move one step further and introduce an approach (ac-companied by a tool) to identify low-level expertise on particular software frameworks and technologies apart, relying solely on GitHub data, using the GitHub API and Natural Language Processing (NLP)-using the Microsoft Language Understanding Intelligent Service (LUIS). In particular, we developed an NLP model in LUIS for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["63"]}
{"title": "Applying Machine Learning in Technical Debt Management: Future Opportunities and Challenges\n", "abstract": " Technical Debt Management (TDM) is a fast-growing field that in the last years has attracted the attention of both academia and industry. TDM is a complex process, in the sense that it relies on multiple and heterogeneous data sources (e.g., source code, feature requests, bugs, developers\u0393\u00c7\u00d6 activity, etc.), which cannot be straightforwardly synthesized; leading the community to using mostly qualitative empirical methods. However, empirical studies that involve expert judgement are inherently biased, compared to automated or semi-automated approaches. To overcome this limitation, the broader (not TDM) software engineering community has started to employ machine learning (ML) technologies. Our goal is to investigate the opportunity of applying ML technologies for TDM, through a Systematic Literature Review (SLR) on the application of ML to software engineering problems (since ML applications on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["63"]}
{"title": "Final Energy Consumption Forecasting by Applying Artificial Intelligence Models\n", "abstract": " The application of artificial neural networks has been increased in many scientific sectors the last years, with the development of new machine learning techniques and methodologies. In this research, neural networks are applied in order to build and compare neural network forecasting models for predicting the final energy consumption. Predicting the energy consumption can be very significant in public management at improving the energy management and also at designing the optimal energy planning strategies. The final energy consumption covers the energy consumption in sectors such as industry, households, transport, commerce and public management. Several architectures were examined in order to construct the optimal neural network forecasting model. The results have shown a very good prediction accuracy according to the mean squared error. The proposed methodology can provide more\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["63"]}
{"title": "Integrating traceability within the ide to prevent requirements documentation debt\n", "abstract": " Documentation issues in software projects have been recently classified as a type of technical debt (TD), a concept that express-es how shortcuts during software development result in additional maintenance and evolution effort. The specific type of TD is termed documentation debt, and is among the most prevalent ones in practice and research. In this study we propose a tool-based approach for preventing documentation TD during requirements engineering, by: (a) integrating requirements specifications into the IDE, and (b) enabling the real-time creation of traces between requirements and code. To this end, we collaborated with a small/medium software company and conducted a qualitative case study to: (a) analyze the current process and identify existing TD types, (b) collect the requirements and implement a tool that aims at preventing the accumulation of documentation TD, and (c) investigate whether the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["63"]}
{"title": "Public e-governance and city marketing based on indicators monitoring information system in urban environment\n", "abstract": " Public management in urban environment has to take into consideration many factors in order to adopt the best management and planning practices. Also, city marketing is considered as very essential for improving the city\u0393\u00c7\u00d6s image in the world especially when a city wants to attract tourists according to its natural resources and its cultural monuments. In this study, an indicators system is proposed regarding public management and city marketing based on several factors that affect managerial issues and also the city\u0393\u00c7\u00d6s image to the tourists. The interpretation of the values of these indicators can help public managers to construct the adequate strategies in order to deal with various matters that affect the quality of the citizens\u0393\u00c7\u00d6 life. The proposed indicators system includes indicators regarding city marketing for measuring also the competitiveness in tourism. E-government can provide the appropriate framework for the public participation in the indicators based information system. The indicators information system aims at providing enough information to the public administration in order to build decision models and tactics that improve decision making processes in public e-governance and also in place marketing.", "num_citations": "1\n", "authors": ["63"]}
{"title": "Evolution of method invocation and object instantiation patterns in a PHP ecosystem\n", "abstract": " PHP is one of the most frequently used scripting languages for server-side programming, since approximately 75% of successful web applications have been developed with PHP. The main benefits of PHP are its low learning curve and the rich variety of dynamic features that it offers. These benefits have contributed towards the development of a large community of programmers around PHP, which in turn created a vast ecosystem of applications and frameworks. In this study we have empirically investigated ten famous PHP frameworks/applications and over 240 MLOC in order to explore their internal structure. More specifically, we present some demographics on method invocation and object instantiation patterns, empowered by the dynamic nature of the PHP language. To present the results we employ statistical methods inspired by ecology. In particular, we explore the diversity and dominance of these patterns\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["63"]}
{"title": "A Study On The Accumulation Of Technical Debt On Framework-based Web Applications\n", "abstract": " This paper presents the results of an observational study to investigate the advantages of using widely used software development frameworks for Java EE applications. Also, it presents the accumulation of Technical Debt and the evolution of the quality code metrics, when the software is developed using frameworks. Considering that web applications hold the lion\u0393\u00c7\u00d6s share of today\u0393\u00c7\u00d6s IT industry, this study focuses on two widely popular Java EE frameworks, namely Spring Web MVC Framework and Apache Struts 2. In particular, we have developed one system over four versions in both frameworks while Technical Debt and quality code metrics have been monitored. The findings indicate that software developed based on frameworks is relatively free of Technical Debt. Moreover, we have not noticed any significant differences between the two frameworks in terms of Technical Debt. In general, one could claim that framework-based development can potentiality lead to high quality and maintainable systems, if the framework is properly used.", "num_citations": "1\n", "authors": ["63"]}
{"title": "Implementation and evaluation of a queuing systems modelling course using Unified Modelling Language (UML)\n", "abstract": " In this paper, we present a new approach in the way that a network modelling and simulation or a queuing theory course can be done. Beyond the mathematical model that sometimes is too difficult for students to understand, this paper introduces the use of Unified Modelling Language (UML) as the mean to teach modelling of discrete event systems such as queues and networks. The basic course scheduling and teaching material are presented too. It is also shown how this method was evaluated by the teachers and the students that attended the course and answered a questionnaire.", "num_citations": "1\n", "authors": ["63"]}
{"title": "Code improvement: implementing design patterns to Java EE applications\n", "abstract": " Design patterns, acting actually as recurring solutions to common problems, offer significant benefits such as avoiding unnecessary complexity, and promoting code reuse, maintainability and extensibility. This paper describes how four not technology-specific or language-specific design patterns (Front Controller, Model View Controller, Transfer Object and Service to Worker) can be implemented to Java EE applications. It also calculates the code improvement after the implementation of each design pattern using software metrics. The improvement of the quality of the code is considered by measuring the decrease of complexity, coupling or/and response size.", "num_citations": "1\n", "authors": ["63"]}
{"title": "\u0393\u00c7\u00ffCircuit-level low-power design\n", "abstract": " It is in the nature of digital design that any logic function can be implemented by different transistor structures. These alternatives present different characteristics in terms of speed and power dissipation, depending on the capacitance at the internal nodes, the fan-in requirements, the operating conditions and the structure itself. Speed and low power dissipation are in the most cases contradictory factors but there are also exceptions, where design for high speed results in low power circuits. Circuit designers have to be aware of this diversity of structures and the available possibilities that can be exploited to optimally correlate high speed and low power dissipation.", "num_citations": "1\n", "authors": ["63"]}
{"title": "Reducing power consumption in memories\n", "abstract": " Memory circuits which form an integral part of every digital system, consume often significant power compared to processing units, especially in embedded systems for data-intensive applications. Power reduction techniques for SRAM and DRAM memories, address three key issues that affect energy consumption: static current, supply voltage and charging/discharging capacitance. Past efforts have resulted in continuous power reduction for each memory generation; however, future research has to deal with tradeoffs arising from performance degradation for lower supply voltage and standby power dissipation due to subthreshold leakage currents.", "num_citations": "1\n", "authors": ["63"]}
{"title": "Analysis of the transistor chain operation in CMOS gates for short channel devices\n", "abstract": " A detailed analysis of the transistor chain operation in CMOS gates is presented. The chain is diminished to a transistor pair taking into account the actual operating conditions of the structure. The output waveform is obtained analytically, without linear approximations of the output voltage and for ramp inputs. The /spl alpha/-power transistor current model which takes into account second order effects of submicron devices is used, while previous inconsistencies in the chain currents are eliminated by introducing a drain-to-source voltage modulation factor. The exact time when the chain starts conducting is efficiently calculated removing a major source of errors. The calculated output waveform results according to the proposed model are in excellent agreement with SPICE simulations.", "num_citations": "1\n", "authors": ["63"]}