{"title": "Software cost estimation methods: A review 1\n", "abstract": " Project planning is one of the most important activities in software projects. Poor planning often leads to project faults and dramatic outcomes for the project team. If cost and effort are determined pessimistic in software projects, suitable occasions can be missed; whereas optimistic predictions can be caused to some resource losing. Nowadays software project managers should be aware of the increasing of project failures. The main reason for this problem is imprecision of the estimation. In this paper, several existing methods for software cost estimation are illustrated and their aspects will be discussed. Comparing the features of the methods could be applied for clustering based on abilities; it is also useful for selecting the special method for each project. The history of using each estimation model leads to have a good choice for an especial software project. In this paper an example of estimation is also presented in an actual software project. Keywords-Cost Estimation; project fail; Cocomo; Accuracy I.", "num_citations": "174\n", "authors": ["2185"]}
{"title": "Quality of service approaches in cloud computing: A systematic mapping study\n", "abstract": " Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area.Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions.Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type.Result: The majority of the articles are of the validation research\u00a0\u2026", "num_citations": "151\n", "authors": ["2185"]}
{"title": "Electromagnetic field optimization: a physics-inspired metaheuristic optimization algorithm\n", "abstract": " This paper presents a physics-inspired metaheuristic optimization algorithm, known as Electromagnetic Field Optimization (EFO). The proposed algorithm is inspired by the behavior of electromagnets with different polarities and takes advantage of a nature-inspired ratio, known as the golden ratio. In EFO, a possible solution is an electromagnetic particle made of electromagnets, and the number of electromagnets is determined by the number of variables of the optimization problem. EFO is a population-based algorithm in which the population is divided into three fields (positive, negative, and neutral); attraction\u2013repulsion forces among electromagnets of these three fields lead particles toward global minima. The golden ratio determines the ratio between attraction and repulsion forces to help particles converge quickly and effectively. The experimental results on 30 high dimensional CEC 2014 benchmarks reflect\u00a0\u2026", "num_citations": "148\n", "authors": ["2185"]}
{"title": "Test case prioritization approaches in regression testing: A systematic literature review\n", "abstract": " ContextSoftware quality can be assured by going through software testing process. However, software testing phase is an expensive process as it consumes a longer time. By scheduling test cases execution order through a prioritization approach, software testing efficiency can be improved especially during regression testing.ObjectiveIt is a notable step to be taken in constructing important software testing environment so that a system's commercial value can increase. The main idea of this review is to examine and classify the current test case prioritization approaches based on the articulated research questions.MethodSet of search keywords with appropriate repositories were utilized to extract most important studies that fulfill all the criteria defined and classified under journal, conference paper, symposiums and workshops categories. 69 primary studies were nominated from the review strategy.ResultsThere\u00a0\u2026", "num_citations": "121\n", "authors": ["2185"]}
{"title": "Aspect-oriented model-driven code generation: A systematic mapping study\n", "abstract": " ContextModel-driven code generation is being increasingly applied to enhance software development from perspectives of maintainability, extensibility and reusability. However, aspect-oriented code generation from models is an area that is currently underdeveloped.ObjectiveIn this study we provide a survey of existing research on aspect-oriented modeling and code generation to discover current work and identify needs for future research.MethodA systematic mapping study was performed to find relevant studies. Classification schemes have been defined and the 65 selected primary studies have been classified on the basis of research focus, contribution type and research type.ResultsThe papers of solution proposal research type are in a majority. All together aspect-oriented modeling appears being the most focused area divided into modeling notations and process (36%) and model composition and\u00a0\u2026", "num_citations": "70\n", "authors": ["2185"]}
{"title": "PHandler: an expert system for a scalable software requirements prioritization process\n", "abstract": " Software requirements engineering is a critical discipline in the software development life cycle. The major problem in software development is the selection and prioritization of the requirements in order to develop a system of high quality. This research analyzes the issues associated with existing software requirement prioritization techniques. One of the major issues in software requirement prioritization is that the existing techniques handle only toy projects or software projects with very few requirements. The current techniques are not suitable for the prioritization of a large number of requirements in projects where requirements may grow to the hundreds or even thousands. The research paper proposes an expert system, called the Priority Handler (PHandler), for requirement prioritization. PHandler is based on the value-based intelligent requirement prioritization technique, neural network and analytical\u00a0\u2026", "num_citations": "44\n", "authors": ["2185"]}
{"title": "Agile development in the cloud computing environment: A systematic review\n", "abstract": " Background: Agile software development is based on a set of values and principles. The twelve principles are inferred from agile values. Agile principles are composition of evolutionary requirement, simple design, continuous delivery, self-organizing team and face-to-face communication. Due to changing market demand, agile methodology faces problems such as scalability, more effort and cost required in setting up hardware and software infrastructure, availability of skilled resource and ability to build application from multiple locations. Twelve (12) principles may be practiced more appropriately with the support of cloud computing. This merger of agile and cloud computing may provide infrastructure optimization and automation benefits to agile practitioners.Objective: This Systematic Literature Review (SLR) identifies the techniques employed in cloud computing environment that are useful for agile development\u00a0\u2026", "num_citations": "39\n", "authors": ["2185"]}
{"title": "Non-functional requirements elicitation guideline for agile methods\n", "abstract": " One of the essential activities in software development is elicitation of requirement. Majority of the studies has pointed out that less attention is given to the NonFunctional Requirement (NFR). The negligence of NFR elicitation is due to lack of knowledge of the user and developer about NFR. Our study presents elicitation guidelines for NFRs in agile methods. This guideline will helps developers as well as users in agile methods. A case study is conducted on the group of master students for eliciting NFR with the help of elicitation guidelines. In addition, the initial results were obtained by extracting NFRs from eProcurement document that carries requirements of major European Union projects. The result of the case study is positive and encouraging for the new developers and users having less awareness about NFRs. Furthermore, the study describes the role of cloud computing in agile methods, especially in elicitation activity", "num_citations": "26\n", "authors": ["2185"]}
{"title": "StakeMeter: Value-Based stakeholder identification and quantification framework for value-based software systems\n", "abstract": " Value-based requirements engineering plays a vital role in the development of value-based software (VBS). Stakeholders are the key players in the requirements engineering process, and the selection of critical stakeholders for the VBS systems is highly desirable. Based on the stakeholder requirements, the innovative or value-based idea is realized. The quality of the VBS system is associated with the concrete set of valuable requirements, and the valuable requirements can only be obtained if all the relevant valuable stakeholders participate in the requirements elicitation phase. The existing value-based approaches focus on the design of the VBS systems. However, the focus on the valuable stakeholders and requirements is inadequate. The current stakeholder identification and quantification (SIQ) approaches are neither state-of-the-art nor systematic for the VBS systems. The existing approaches are time-consuming, complex and inconsistent which makes the initiation process difficult. Moreover, the main motivation of this research is that the existing SIQ approaches do not provide the low level implementation details for SIQ initiation and stakeholder metrics for quantification. Hence, keeping in view the existing SIQ problems, this research contributes in the form of a new SIQ framework called \u2018StakeMeter\u2019. The StakeMeter framework is verified and validated through case studies. The proposed framework provides low-level implementation guidelines, attributes, metrics, quantification criteria and application procedure as compared to the other methods. The proposed framework solves the issues of stakeholder quantification or prioritization\u00a0\u2026", "num_citations": "23\n", "authors": ["2185"]}
{"title": "A framework for agile development in cloud computing environment\n", "abstract": " Distributed agile software development faces difficulties for instance lack of visibility across development and delivery processes, complex and disjointed development processes, inability to capitalize on business opportunities, lack of communication agility between disconnected owners, development teams, and users or clients. However these difficulties are solved with the help of cloud computing services. This study proposes a framework to provide a skeletal or structural environment for distributed agile software development in cloud computing environment. The framework guide towards the best tooling to deliver a consistent, automated, governed, and unified agile software development process with reduced technical debt, and minimized project backlog. In addition to this, the study highlights the benefits of cloud computing in agile software development.", "num_citations": "22\n", "authors": ["2185"]}
{"title": "Modular weightless neural network architecture for intelligent navigation\n", "abstract": " The standard multi layer perceptron neural network (MLPNN) type has various drawbacks, one of which is training requires repeated presentation of training data, which often results in very long learning time. An alternative type of network, almost unique, is the Weightless Neural Network (WNNs) this is also called n-tuple networks or RAM based networks. In contrast to the weighted neural models, there are several one-shot learning algorithms for WNNs where training takes only one epoch. This paper describes WNNs for recognizes and classifies the environment in mobile robot using a simple microprocessor system. We use a look-up table to minimize the execution time, and that output stored into the robot RAM memory and becomes the current controller that drives the robot. This functionality is demonstrated on a mobile robot using a simple, 8 bit microcontroller with 512 bytes of RAM. The WNNs approach is code efficient only 500 bytes of source code, works well, and the robot was able to successfully recognize the obstacle in real time.", "num_citations": "22\n", "authors": ["2185"]}
{"title": "A component-oriented programming for embedded mobile robot software\n", "abstract": " Applying software reuse to many Embedded Real-Time (ERT) systems poses significant challenges to industrial software processes due to the resource-constrained and real-time requirements of the systems. Autonomous Mobile Robot (AMR) system is a class of ERT systems, hence, inherits the challenge of applying software reuse in general ERT systems. Furthermore, software reuse in AMR systems is challenged by the diversities in terms of robot physical size and shape, environmental interaction and implementation platform. Thus, it is envisioned that component-based software engineering will be the suitable way to promote software reuse in AMR systems with consideration to general requirements to be self-contained, platform-independent and real-time predictable. A framework for component-oriented programming for AMR software development using PECOS component model is proposed in this paper\u00a0\u2026", "num_citations": "22\n", "authors": ["2185"]}
{"title": "Model driven web engineering: A systematic mapping study\n", "abstract": " Background: Model Driven Web Engineering (MDWE) is the application of the model driven paradigm to the domain of web software development, where it is particularly helpful because of the continuous evolution of Web technologies and platforms. Objective: In this paper, we prepare a survey of primary studies on MDWE to explore current work and identify needs for future research. Method: Systematic mapping study uses for finding the most relevant studies and classification. In this study, we found 289 papers and a classification scheme divided them depending on their research focus, contribution type and research type. Results: The papers of solution proposal (20%) research type are majority. The most focused areas of MDWE appear to be: Web Applicability (31%), Molding and Notation (19%), and Services and Oriented (18%). The majority of contributions are methods (33%). Moreover, this shows MDWE as a wide, new, and active area to publications. Conclusions: Whilst additional analysis is warranted within the MDWE scope, in literature, composition mechanisms have been thoroughly discoursed. Furthermore, we have witnessed that the recurrent recommendation for Validation Research, Solution Proposal and Philosophical Papers has been done through earlier analysis.", "num_citations": "19\n", "authors": ["2185"]}
{"title": "Stakeholder management in value-based software development: systematic review\n", "abstract": " In the value-based software (VBS) development, an innovative idea is realised in order to gain an economic leverage. The VBS systems deal with financial streams and this thing make them different from the conventional systems. The success of a VBS system is associated with a valuable set of requirements. The valuable requirements can only be gathered from success critical stakeholders. To select a set of success critical stakeholders, different stakeholders identification and quantification (SIQ) approaches are presented by the researchers. The current approaches cannot be adopted as a standard as different methods and processes are adopted in different approaches. In this study, the aim is to find out the reported evidence based attributes or characteristics of the stakeholders and their usage context in terms of their application in different domains, stakeholders' quantification metrics, the reported stakeholder\u00a0\u2026", "num_citations": "18\n", "authors": ["2185"]}
{"title": "Enhancement of UWE navigation model: Homepage development case study\n", "abstract": " UML-based Web Engineering (UWE) is a web engineering methodology which provides a systematic approach to development of web applications. It provides a UML extension, defining modeling elements considering the extension mechanisms that are offered by UML. Navigation model is presented as a very critical characteristic in all the hypermedia and web methodologies; also homepage one of the most important pages on the web sites, because it is the key to showing the quality of websites. The UWE navigation model defined as a new proposal meaning still now need it to improve, the problem is elements of the UWE navigation model until now cannot support modules of the homepage especially new modules as Tab, Flash News, Main Menu, Frame, Multi Data and Application Icons, meaning it has weaknesses, in this paper we enhance the UWE navigation model through defining six elements by using an extension mechanism to fully support homepage development.", "num_citations": "16\n", "authors": ["2185"]}
{"title": "SOFTWARE QUALITY ENHANCEMENT FOR VALUE BASED SYSTEMS THROUGH STAKEHOLDERS QUANTIFICATION.\n", "abstract": " Software quality assurance plays an important role to check the overall quality of the software product especially when a product is a value based system. The valuable software product or product line is tested under strict circumstances to meet the minimum constraints of software quality. This paper focuses on stakeholders, requirements engineering, different testing techniques being applied in software professional environment, issues and current trends to resolve the requirement problems for continuous software quality improvement. This paper presents the criticality of stakeholders, requirements and software testing techniques for software professionals in terms of quality assurance. A model is proposed in order to achieve a high quality value based software application. There is the dire need to integrate stakeholders, requirements and testing in order to evaluate the performance and quality of a value based system. A systematic stakeholder analysis framework does not exist, and there is the need of a systematic framework that may be adopted as a standard. This research also focuses on a systematic stakeholder\u2019s identification and quantification framework.", "num_citations": "16\n", "authors": ["2185"]}
{"title": "Enhancements of PECOS embedded real-time component model for autonomous mobile robot application\n", "abstract": " Social media allows customers to easily create their own community. Using social identity theory, we are able to explain important characteristics of these communities. These characteristics can be both beneficial and harmful to a company, depends on a situation. We are particularly interested in a crisis situation when company's decisions are critical to its reputation and even its survival. Our case study is the project of the Japanese company that planned to organize the concert of the music band with an aim to promote its brand and increase its customers. We had been cooperating with this company to promote the concert to fan community in Twitter. As we deploy our communication strategies to the community, we encountered several problems. These problems are discussed based on social identity theory and are concluded as practices that are open for future development and investigation. We propose that an\u00a0\u2026", "num_citations": "16\n", "authors": ["2185"]}
{"title": "Test case prioritization using firefly algorithm for software testing\n", "abstract": " Software testing is a vital and complex part of the software development life cycle. Optimization of software testing is still a major challenge, as prioritization of test cases remains unsatisfactory in terms of Average Percentage of Faults Detected (APFD) and time execution performance. This is attributed to a large search space to find an optimal ordering of test cases. In this paper, we have proposed an approach to prioritize test cases optimally using Firefly Algorithm. To optimize the ordering of test cases, we applied Firefly Algorithm with fitness function defined using a similarity distance model. Experiments were carried on three benchmark programs with test suites extracted from Software-artifact Infrastructure Repository (SIR). Our Test Case Prioritization (TCP) technique using Firefly Algorithm with similarity distance model demonstrated better if not equal in terms of APFD and time execution performance compared\u00a0\u2026", "num_citations": "15\n", "authors": ["2185"]}
{"title": "Introducing computer programming to secondary school students using mobile robots\n", "abstract": " This paper reports results of an action research conducted in an outreach program aimed at introducing secondary school students to the exciting world of computer programming. The key challenges were to introduce programming and problem solving session that can be done in a very short time (within 1 to 2 hours) and in a fun way. This was achieved by using a physical mobile robot called RoboKar to engage students with the problem solving activities involving programming. From the entry and exit surveys, and facilitators' observations, it can be concluded that the robot platform and the outreach program were able to introduce and spark interest in learning programming among the school students.", "num_citations": "14\n", "authors": ["2185"]}
{"title": "Systematic reviews in requirements engineering: A systematic review\n", "abstract": " Requirements engineering (RE) in software development life cycle is considered as a main pillar of success of a software system. RE process is used to find out the key requirements in order to develop a system as per needs of the stakeholders. The two focal points in RE process are stakeholders and requirements. Systematic literature review (SLR) is the mean of collecting evidence-based data about the key areas of a research topic and its related issues. In this paper, a study of existing SLRs on RE is conducted in order to find out the key intricacies dealt in the domain of RE. The main purpose of this study is to aggregate the knowledge in the domain of software RE as elicited in different SLRs about different RE problems.", "num_citations": "14\n", "authors": ["2185"]}
{"title": "A Comparative Evaluation of State-of-the-Art Cloud Migration Optimization Approaches\n", "abstract": " Cloud computing has become more attractive for consumers to migrate their applications to the cloud environment. However, because of huge cloud environments, application customers and providers face the problem of how to assess and make decisions to choose appropriate service providers for migrating their applications to the cloud. Many approaches have investigated how to address this problem. In this paper we classify these approaches into non-evolutionary cloud migration optimization approaches and evolutionary cloud migration optimization approaches. Criteria including cost, QoS, elasticity and degree of migration optimization have been used to compare the approaches. Analysis of the results of comparative evaluations shows that a Multi-Objectives optimization approach provides a better solution to support decision making to migrate an application to the cloud environment based on the\u00a0\u2026", "num_citations": "14\n", "authors": ["2185"]}
{"title": "A survey of design model for quality analysis: From a performance and reliability perspective\n", "abstract": " The use of a model for the analysis of the software quality attributes during the design phase has been gaining more attention in recent years. These models, which are peripheral in system design, are the center of quality analysis. The system design is the central focus in representing the structure and behavior of the system. Therefore, the goal of the software architecture performance and reliability analysis is to discover possible quality problems that may violate the quality requirements, which have been stated in the design. The use of an intermediate model to correlate the performance and reliability specification from the UML model and which is then transformed into an analysis model could facilitate the analysis process. This paper provides a survey of the existing intermediate metamodels from a performance and reliability perspective, through the evaluation and discussion on the similarities and differences focusing on the aspects of general concepts, modelling and analysis. The purpose of the discussion is to offer guidelines on which intermediate metamodel is appropriate for the use of quality analysis at design time as well as outline the possible space for improvement by making classifications and comparisons studies.", "num_citations": "13\n", "authors": ["2185"]}
{"title": "Early-life cycle reuse approach for component-based software of autonomous mobile robot system\n", "abstract": " Applying software reuse to many embedded realtime systems, such as autonomous mobile robot system poses significant challenges to industrial software processes due to the resource-constrained and realtime requirements of the systems. An approach for early life-cycle systematic reuse for component-based software engineering (ELCRA) of autonomous mobile robot software is developed. The approach allows reuse at the early stage of software development process by integrating analysis patterns, component model, and component-oriented programming framework. The results of applying the approach in developing software for real robots show that the strategies and processes proposed in the approach can fulfill requirements for self-contained, platform-independent and real-time predictable mobile robot.", "num_citations": "13\n", "authors": ["2185"]}
{"title": "Analyzing interaction flow modeling language in web development lifecycle\n", "abstract": " (OMG) adopted a new standard method named Interaction Flow Modeling Language (IFML) for web engineering domain. IFML is designed to express the content, user interaction, and control behavior of the front end of applications. There are number lacks in web engineering methods, because each of them is defined to particular specifications, one of which is the open issue of supporting the whole lifecycle in process development. In this paper, we analyze IFML models in the process development lifecycle to show capability of the method used in the process development. We then make a comparison between IFML and other methods in lifecycle phases. Finally, we add IFML to the web engineering lifecycle\u2019s map. It is anticipated that the result of this paper will be to become a guide for developers for using IFML in the development of new applications.", "num_citations": "12\n", "authors": ["2185"]}
{"title": "A comparative survey of aspect-oriented code generation approaches\n", "abstract": " Model-driven code generation is being increasingly applied to developing software systems as a result of its recognition as an instrument to enhance the produced software. At the same time, aspect-oriented programming languages have come to the mainstream of software development due to their distinctive features to provide better modularization and separation of concerns. As a consequence of this prevalence and recognition of its impact on improving several software quality factors, different approaches have been proposed in literature to generate aspect-oriented model-driven code. This paper provides a comparative review of some existing approaches and discusses important issues and directions in this particular area. The results of this survey indicate aspect-oriented model-driven code generation being a rather immature area. Majority of approaches address structure diagrams only, a fact that limits\u00a0\u2026", "num_citations": "12\n", "authors": ["2185"]}
{"title": "Software Reuse for Mobile Robot Applications Through Analysis Patterns.\n", "abstract": " Software analysis pattern is an approach of software reuse which provides a way to reuse expertise that can be used across domains at early level of development. Developing software for a mobile robot system involves multi-disciplines expert knowledge which includes embedded systems, real-time software issues, control theories and artificial intelligence aspects. This paper focuses on analysis patterns as a means to facilitate mobile robot software knowledge reuse by capturing conceptual models in those domains in order to allow reuse across applications. The use of software analysis patterns as a means to facilitate Autonomous Mobile Robots (AMR) software knowledge reuse through component-based software engineering is proposed. The software analysis patterns for AMR were obtained through a pattern mining process, and documented using a standard catalogue template. These analysis patterns are categorized according to hybrid deliberate layered architecture of robot software: Reactive layer, supervisor layer and deliberative layer. Particularly, the analysis patterns in the reactive layer are highlighted and presented. The deployment of the analysis patterns are illustrated and discussed using an AMR software case study. To verify the existence of the pattern in AMR systems, pattern-based reverse engineering was performed on two existing AMR systems. The reuse potential of these patterns is evaluated by measuring the reusability of components in the analysis patterns.", "num_citations": "12\n", "authors": ["2185"]}
{"title": "Struktur data & algoritma menggunakan C++\n", "abstract": " Buku Struktur Data dan Algoritma Menggunakan C++ ditulis bagi memenuhi keperluan silibus pelajar jurusan Sains Komputer. Buku ini memberi penekanan terhadap struktur data dan keberkesanan algoritma dalam penyelesaian masalah dengan menggunakan bahasa pengaturcaraan C++. Kandungan buku ini juga merangkumi prinsip kejuruteraan perisian dan pengaturcaraan berorientasikan objek. Buku ini sesuai digunakan sebagai bahan pengajaran dan pembelajaran oleh pensyarah, pelajar mahupun orang awam yang ingin mendalami ilmu mengenai Struktur Data dan Algoritma dengan menggunakan Bahasa Pengaturcaraan C++.", "num_citations": "12\n", "authors": ["2185"]}
{"title": "Ensembling artificial bee colony with analogy-based estimation to improve software development effort prediction\n", "abstract": " Analogy-Based Estimation (ABE) is one of the promising estimation models used for predicting the software development effort. Researchers proposed different variants of the ABE model, but still, the most suitable procedure could not be produced for accurate estimation. In this study, an artificial Bee colony guided Analogy-Based Estimation (BABE) model is proposed which ensembles Artificial Bee Colony (ABC) with ABE for accurate estimation. ABC produces different weights, out of which the most appropriate is infused in the similarity function of ABE during the stage of model training, which are later used in the testing stage for evaluation. There are six real datasets utilized for simulating the model procedure. Five of these datasets are taken from the PROMISE repository. The predictive performance is improved for BABE over the existing ones. The most significant of its performance is found on the International\u00a0\u2026", "num_citations": "10\n", "authors": ["2185"]}
{"title": "Integration of pecos into marmot for embedded real time software component-based development\n", "abstract": " In ERT (embedded real time) software development, software functionality is not the only focus but multi-constraints extra-functionality requirement such as timing, resources constraint, statically predictable, safety-critical, processing power and memory is also important. Besides that, ERT software development involves multi-disciplinary knowledge that includes software, mechanical and electronic engineering fields. To meet these two challenges ERT software development must be able to support the multi-constraint and multi-disciplinary challenges. From this perspective, CBD (component-based development) appears to be one of the appropriate approaches to design the ERT software due to the ability of domain experts to interactively compose and adapt sophisticated ERT software which the decrease of development time and improvement of software quality. Existing component infrastructures used in ERT\u00a0\u2026", "num_citations": "10\n", "authors": ["2185"]}
{"title": "Extraction of non-functional requirement using semantic similarity distance\n", "abstract": " Functional and non-functional requirements are important equally in software development. Usually, the requirements are expressed in natural languages. The functional and non-functional requirements are written inter-mixed in software requirement document. The extraction of requirement from the software requirement document is a challenging task. Most of the recent studies adopted a supervised learning approach for the extraction of non-functional requirements. However, there is a drawback of supervised learning such as training of model and retrain if the domain changed. The proposed approach manipulates the textual semantic of functional requirements to identify the non-functional requirements. The semantic similarity is calculated based on co-occurrence of patterns in large human knowledge repositories of Wikipedia. This study finds the similarity distance between the popular indicator keywords and\u00a0\u2026", "num_citations": "9\n", "authors": ["2185"]}
{"title": "A comparison of navigation model between UWE and WebML: Homepage development case study\n", "abstract": " Web Engineering is a systematic approach for the development of web applications. In this area navigation model is defined as an important model in all the hypermedia and web engineering methods. Also homepage is one of the most important pages on the websites, because it is considered to be the key element that demonstrates the quality of websites. The web engineering methods still have weaknesses that are difficult for designers to design homepage's content perfectly. In this paper we compare navigation model between UML-Based Web Engineering (UWE) and Web Modeling Language (WebML). The purpose of this paper is to find a quality in these methods for the homepage's development, through designing navigation model in UWE and WebML, and comparing the elements of the navigation model. The result of this paper helps the designers to develop the homepages in order to solve of the weakness's navigation model.", "num_citations": "9\n", "authors": ["2185"]}
{"title": "Development test case prioritization technique in regression testing based on hybrid criteria\n", "abstract": " Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.", "num_citations": "9\n", "authors": ["2185"]}
{"title": "Metamodels evaluation of web engineering methodologies to develop web applications\n", "abstract": " The purpose of a metamodel in web engineering methodologies is for the platform independent analysis as well as the design of the content, navigation, and presentation issues of web applications. In the previous years, numbers of methodologies for the development of web applications were proposed, and most of them defined their notation for creating metamodels. The increasing expansion and complexity of web applications are a new challenge for web software developers. This paper presents a comparison study between metamodel of the three methodologies which are; UML-Based Web Engineering (UWE), Web Modeling Language (WebML) and Object Oriented Hypermedia (OOH). The aim is to show the capability of the methodologies to address the challenges in developing the web applications. The evaluation results presented in this paper help the designer in providing initial knowledge of the strengths and weaknesses of the three methodologies for developing web applications.", "num_citations": "9\n", "authors": ["2185"]}
{"title": "A Bi-metric and Fuzzy c-means Based Intelligent Stakeholder Quantification System for Value-based Software.\n", "abstract": " Requirements engineering (RE) deals with the software requirements elicitation, validation and verification processes and implementation of elicited requirements. The RE process, for value-based software (VBS) development, is one of the highly complex and non-linear processes which mainly depends on the key stakeholders. VBS systems are extremely different from traditional systems due to their association with financial streams. There is a need of value-based RE practices. Stakeholders\u2019 analysis plays a vital role in the selection of critical stakeholders for RE phase in VBS development. Different stakeholders\u2019 identification and quantification (SIQ) approaches are presented by researchers in order to identify and quantify the stakeholders. The existing approaches are not uniform in terms of process activities and different stakeholders\u2019 attributes. Nonuniformity results in higher time consumption and higher complexity. The existing SIQ approaches are not suitable for current value-based RE practices. Hence, this research presents an intelligent decision support system for the VBS SIQP using stakeholder metrics and fuzzy logic. The data is finally divided into three clusters in order to select the highly prioritized stakeholders for VBS systems.", "num_citations": "9\n", "authors": ["2185"]}
{"title": "Environmental recognition using RAM-network based type-2 fuzzy neural for navigation of mobile robot\n", "abstract": " Reactive autonomous mobile robot navigating in real time environment is one of the most important requirements. Most of the systems have some common drawbacks such as, large computation, expensive equipment, hard implementation, and the complexity of the system. The work presented in this paper deals with a type-2 fuzzy-neural controller using RAM-based network to make navigation decisions. The proposed architecture can be implemented easily with low cost range sensor and low cost microprocessor. To minimize the execution time, we used a look-up table and that output stored into the robot RAM memory and becomes the current controller that drives the robot. This functionality is demonstrated on a mobile robot using a simple, 8 bit microcontroller with 512 bytes of RAM. The experiment results show that source code is efficient, works well, and the robot was able to successfully avoid obstacle in\u00a0\u2026", "num_citations": "9\n", "authors": ["2185"]}
{"title": "Multiple black hole inspired meta-heuristic searching optimization for combinatorial testing\n", "abstract": " Combinatorial searching-based software testing (CSST) is a challenging optimization procedure. The achievement of optimal solutions involves a careful formulation of the optimization problem and the selection of an appropriate approach. Meta-heuristic searching procedures have proven to be effective for solving CSST issues. Black hole (BH) optimization is among the more recently developed meta-heuristic searching algorithms. While this approach has been observed to be an effective alternative to particle swarm optimization, its operation is based on only one swarm. To date, no efforts have been made to modify this approach to accommodate multiple swarms. This study proposes a new variant of BH that involves a combination of multiple swarms. The BH optimizer is modified from continuous searching to binary searching and subsequently applied for solving CSST. The evaluation is based on a modified\u00a0\u2026", "num_citations": "8\n", "authors": ["2185"]}
{"title": "Extensibility interaction flow modeling language metamodels to develop new web application concerns\n", "abstract": " Web engineering is a systematic approach to develop web applications, and numerous web engineering methods have been proposed. These methods were extended through defining new models by using different mechanisms to capture the web application concepts. Due to the complexity rising of web applications, the web engineering methods cannot provide web solutions anymore. Even though Interaction Flow Modeling Language (IFML) is recently proposed as a new method for developing web applications, it has limitations. Therefore these methods need to be improved. In this paper, we present the ability of IFML extensibility to support new concerns from web applications. Moreover, we extend IFML through UML mechanisms to support new concerns from the context to the user interface. The new IFML solves the lack of context web application through defining a new model and becomes a new direction to develop concerns modern web applications.", "num_citations": "8\n", "authors": ["2185"]}
{"title": "Technique for early reliability prediction of software components using behaviour models\n", "abstract": " Behaviour models are the most commonly used input for predicting the reliability of a software system at the early design stage. A component behaviour model reveals the structure and behaviour of the component during the execution of system-level functionalities. There are various challenges related to component reliability prediction at the early design stage based on behaviour models. For example, most of the current reliability techniques do not provide fine-grained sequential behaviour models of individual components and fail to consider the loop entry and exit points in the reliability computation. Moreover, some of the current techniques do not tackle the problem of operational data unavailability and the lack of analysis results that can be valuable for software architects at the early design stage. This paper proposes a reliability prediction technique that, pragmatically, synthesizes system behaviour in the form of a state machine, given a set of scenarios and corresponding constraints as input. The state machine is utilized as a base for generating the component-relevant operational data. The state machine is also used as a source for identifying the nodes and edges of a component probabilistic dependency graph (CPDG). Based on the CPDG, a stack-based algorithm is used to compute the reliability. The proposed technique is evaluated by a comparison with existing techniques and the application of sensitivity analysis to a robotic wheelchair system as a case study. The results indicate that the proposed technique is more relevant at the early design stage compared to existing works, and can provide a more realistic and meaningful\u00a0\u2026", "num_citations": "8\n", "authors": ["2185"]}
{"title": "Understanding the impact of web layout and emotional changes towards navigation behaviour among visually impaired users\n", "abstract": " Facebook is the largest social network and it\u2019s widely used by everyone. Facebook consists of webpages designed in modular layouts. Assistive technologies such as screen readers not able interpret the content in complex layout to visually impaired (VI) users. This causes lots of troubles to VI users. In order to understand the impact of layout to VI users\u2019 navigation behavior, we conducted a comparison study of how VI users navigate in Facebook desktop and mobile version. Besides, this article examines the relationships between users\u2019 emotions and their navigation behavior in complex layout. The result shows that VI users prefer Facebook mobile version compares to desktop version. The study revealed that there is a strong relationship between users\u2019 emotions and their navigation behavior. The understanding of VI users\u2019 navigation behavior and their emotions changes can help web designers to bridge the gap between the user and system.", "num_citations": "8\n", "authors": ["2185"]}
{"title": "A quantitative assessment of aspect design notations with respect to reusability and maintainability of models\n", "abstract": " Aspect-oriented design notations improve the reusability and maintainability of models by means of separating crosscutting concerns from the core functionality of the system. However, as different design notations propose dissimilar ways of modeling and composing the crosscutting and core concerns, the capabilities of aspect design notations with regards to producing highly reusable and maintainable models may also vary. In this paper, we apply a selected set of aspect modeling notations to a modeling scenario and evaluate them with respect to reusability and maintainability of the obtained models. For this purpose, we have used different metrics related to well-known software engineering attributes that affect the reusability and maintainability of produced design. The results reveal that the Reusable Aspect Models performs well against majority of the employed metrics.", "num_citations": "8\n", "authors": ["2185"]}
{"title": "Using Na\u00efve Bayes and bayesian network for prediction of potential problematic cases in tuberculosis\n", "abstract": " Both Data Mining techniques and Machine Learning algorithms are tools that can be used to provide beneficial support in constructing models that could effectively assist medical practitioners in making comprehensive decisions regarding potential problematic cases in Tuberculosis (TB). This study introduces two machine learning techniques which are Na\u00efve Bayes inductive learning technique and the state of the art Bayesian Networks. These two techniques can be used towards constructing a model that can be used for predicting potential problematic cases in Tuberculosis. To construct a model, this study made have use of data collected from an Epidemiology laboratory. The volume of data was collated and divided into two data sets which are the training dataset and the investigation dataset. The model constructed by this study has shown a high predictive capability strength compared to other models presented on similar studies.", "num_citations": "8\n", "authors": ["2185"]}
{"title": "An automated approach for identification of non-functional requirements using Word2Vec model\n", "abstract": " Non-Functional Requirements (NFR) are embedded in functional requirements in requirements specification document. Identification of NFR from the requirement document is a challenging task. Ignorance of NFR identification in early stages of development increase cost and ultimately cause the failure of the system. The aim of this approach is to help the analyst and designers in architect and design of the system by identifying NFR from the requirements document. Several supervised learningbased solutions were reported in the literature. However, for accurate identification of NFR, a significant number of pre-categorized requirements are needed to train supervised text classifiers and system analysts perform the categorization process manually. This study proposed an automated semantic similarity based approach which does not needs pre-categorized requirements for identification of NFR from requirements documents. The approach uses an application of Word2Vec model and popular", "num_citations": "7\n", "authors": ["2185"]}
{"title": "Adaptation of Project-Oriented Problem-Based Framework for Teaching Computer Programming\n", "abstract": " Emerging technologies in education in these recent years have constantly impacted teaching and learning. One of those emerging technologies for teaching nowadays is using robotics. In the field of engineering and technology, programming is one of the most vital and challenging skills to learn. Programming can be tough for novices especially for those who have not acquired any basic programming skills. Robots have been used in the past few years to support the teaching of programming. Researchers have found that students are more interested in programming through robots utilization in teaching compared to traditional teaching methods. Therefore, this study has proposed a way for teaching and learning of programming to be more effective through adapting an existing Project-Oriented Problem-Based Learning (POPBL) framework. Most research works in the past have only implemented the teaching of\u00a0\u2026", "num_citations": "7\n", "authors": ["2185"]}
{"title": "Inferring approximated models for integration testing of component-based software\n", "abstract": " Recently, component-based software has earned widespread notice and acceptance as a method that facilitates the development of today's large, complex and very critical systems by integrating prefabricated small pieces of software called components or COTS. Components integration becomes an essential stage in the component-based software development Lifecycle. Testing components after integration is an important activity. Due to the unavailability of source code of integrated components and due to the lack of component information, integration testing becomes more difficult and very complex task. Furthermore, the formal models of integrated components are not always available. Therefore, we need to extract/learn them from test observations. This paper proposed an improvement on the existing learning algorithm to infer a model of integrated components in a more efficient way in order to bring down\u00a0\u2026", "num_citations": "7\n", "authors": ["2185"]}
{"title": "Sequential strategy for software process measurement that uses Statistical Process Control\n", "abstract": " Effective measurement for successful Statistical Process Control (SPC) implementations across any level of Capability Maturity Model (CMM) is not a very straightforward activity. Based on CMM standard, SPC is only applicable to its higher maturity levels so as to tackle quantitative process management issues like out-of-control process and to point out further areas of process improvements. This study reveals that; SPC can be used at CMM level 2 to repeatedly control process but with some difficulties. The difficulty of accurate process measurement for successful SPC implementation is addressed by proposing a Sequential Strategy for Process Measurement (SSPM). This strategy can support software organization's process measurement activities for process control and management.", "num_citations": "7\n", "authors": ["2185"]}
{"title": "Web services composition with redundancy consideration\n", "abstract": " Web services composition is a process of creating a new Web service from a set of available Web services. Semi-dynamic Web services composition in Service Oriented Architecture (SOA) consists of designing composition plan by service provider, and automation of Web service discovery and selection at runtime. This research paper describes redundant-free Web services composition in order to reduce execution time and overall cost of the composition process and also improve efficiency of the composition method. To fulfill the needs, this paper uses a linked list data structure called Composition List (CL) to select the ideal services. In addition, a hash table called Available Output Parameters (AOP) is used to find redundant Web services and reject them from Web services composition process.", "num_citations": "7\n", "authors": ["2185"]}
{"title": "Internet of Thing and smart city: State of the art and future trends\n", "abstract": " Fast growing of cities, urban places, and population presents major challenges in our daily lives. Finding proper solution from different perspectives of cities became a concern by both researchers and industries concern. Smart city (SC) is the answer to overcome this issue. Internet of thing (IoT) is a crucial part of SC which has a tremendous impact on all the cities\u2019 sectors such as governance, health care, education, environment, and transportation. This paper provides a state of the art on Internet of thing (IoT)-based smart city (SC) from platform, architecture, application domain, and technology perspectives, also it concludes the relations between different technologies used in developing smart cities. Challenges and future trends of SC based on IoT will be discussed. As this research conducts, the SC solution based on IoT still confronts many challenges and problems which required further researches.", "num_citations": "6\n", "authors": ["2185"]}
{"title": "Similarity distance measure and prioritization algorithm for test case prioritization in software product line testing\n", "abstract": " To achieve the goal of creating products for a specific market segment, implementation of Software Product Line (SPL) is required to fulfill specific needs of customers by managing a set of common features and exploiting the variabilities between the products. Testing product-by-product is not feasible in SPL due to the combinatorial explosion of product number, thus, Test Case Prioritization (TCP) is needed to select a few test cases which could yield high number of faults. Among the most promising TCP techniques is similarity-based TCP technique which consists of similarity distance measure and prioritization algorithm. The goal of this paper is to propose an enhanced string distance and prioritization algorithm which could reorder the test cases resulting to higher rate of fault detection. Comparative study has been done between different string distance measures and prioritization algorithms to select the best techniques for similarity-based test case prioritization. Identified enhancements have been implemented to both techniques for a better adoption of prioritizing SPL test cases. Experiment has been done in order to identify the effectiveness of enhancements done for combination of both techniques. Result shows the effectiveness of the combination where it achieved highest average fault detection rate, attained fastest execution time for highest number of test cases and accomplished 41.25% average rate of fault detection. The result proves that the combination of both techniques improve SPL testing effectiveness compared to other existing techniques.", "num_citations": "6\n", "authors": ["2185"]}
{"title": "Big data survey in healthcare and a proposal for intelligent data diagnosis framework\n", "abstract": " Healthcare is one of the core areas in medical domain. In healthcare the data exist in various forms like respiration data, blood pressure readings, prescriptions and others. The data may help in decision-making for different initiatives in order to provide better healthcare services. However, in order to make this possible there is a need to diagnose the data in a professional way. Currently, there is a lack of a system or way which may help in decision-making in big data analysis in the form of phases. In this research, a framework has been proposed to diagnose the healthcare data for efficient data analysis.", "num_citations": "6\n", "authors": ["2185"]}
{"title": "Non functional requirements (NFRs) traceability metamodel for agile development\n", "abstract": " Agile methodologies are well known for early and frequent releases. Besides, these methodologies also handle requirement changes well without causing delays. However, it has been noticed that the functional requirements changes can affect the non-functional requirements (NFRs) such as security and performance. It is also possible that the agile team is not even aware of these effects causing dysfunctional system. This issue could be addressed by offering traceability mechanism that helps to trace the effect of functional requirement changes on the non-functional requirements. Unfortunately, a few researchers have conducted studies regarding this issue. Thus, this study attempts to present a Traceability Process Model (TPM) to tackle the issue of tracing NFR especially security and performance. However, to materialize a full scale TPM, a metamodel is necessary. Therefore in this paper, we present a metamodel by integrating two existing metamodels. Then we validate the newly built metamodel with precision and recall methods. Lastly, we also develop a traceability tool that is based on the proposed metamodel.", "num_citations": "6\n", "authors": ["2185"]}
{"title": "Residual useful life estimation based on stable distribution feature extraction and SVM classifier\n", "abstract": " The This paper deals with a data-driven diagnostic and prognostic method based on Stable distribution feature extraction and SVM Classier. The prognostic process of the proposed method is made in two steps. In the first step, which is performed online, the monitoring data provided by sensors are processed to extract features based on stable distribution, which are then used to learn SVM classifier that capture the time evolution of the degradation and therefore of the systems health state. In the second step, performed on-line, the learned models are exploited to do failure prognostic by estimating the assets current health state, its remaining useful life. The experiments on the recently published database taken from Pronostia of FEMTO, Prognostic data repository: Bearing data set, clearly show the superiority of the proposed approach compared to well establish method in literature.", "num_citations": "6\n", "authors": ["2185"]}
{"title": "Model-based methodology for implementing marte in embedded real-time software\n", "abstract": " This paper presents an integration of a model-based methodology for embedded real-time software with MARTE. However, although has being introduced as a new profile to overcome problem in previous profile, a sound and systematic methodology is necessary to tackle complexity problem that arise. The objective of this paper is to propose an integration of profile and method for satisfying embedded real-time software requirements and helping engineers to model their system, enhancing the structure and behavior modeling. For that, this paper describes a proposed methodology for the integration process, involving two elements: a profile and a method. The integration result will support to solve complexity whereby the profile is used to solve the lack of specific modeling language notation for embedded real-time system and the method can provide a systematical software process. The proposed integration\u00a0\u2026", "num_citations": "6\n", "authors": ["2185"]}
{"title": "An experiment of different similarity measures on test case prioritization for software product lines\n", "abstract": " Software product line (SPL) engineering paradigm is commonly used to manage variability and commonalities of business applications to satisfy a specific need or goal of a particular market. However, due to time and space complexity, combinatorial interaction testing (CIT) has been suggested to reduce the size of test suites. Although CIT is known as a promising approach to overcome these problems, there are still issues such as combinatorial explosion of features, which drains budget allocated for testing. Therefore, test case prioritization (TCP) is preferred to gain a better result in terms of producing an efficient detection of faults. Among prioritization techniques used in regression testing is similarity-based test case prioritization. Similarity-based test case prioritization rearranges test cases through calculation of distance between test cases using similarity measures. Result from the use of similarity measures in test case prioritization contributes to a much better testing process. This paper provides a comparison of selected similarity measures to investigate the feasibility and suitability of similarity measures to be used in SPL through experimentation. Jaccard, Hamming, Jaro-Winkler, Cosine similarity, Counting, and Sorensein distances have been chosen as similarity measures in this study. The result showed JaroWinkler as the best similarity measure with an 84.96% Average Percentage of Faults Detected (APFD) value across eight feature models. The study offers insights on similarity measures in SPL context. Further, the paper concludes with suggestions on room for improvement, which could be achieved through experimentation and\u00a0\u2026", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Comparison between Web Engineering Methods to Develop Multi Web Applications.\n", "abstract": " Web applications act as a source of strength for business growth over the internet and have become the backbone of the e-commerce. With the evolution of these type of applications, numerous features have been added to overcome the issues of complexities of the processes development web engineering methods. These methods are intended for specific purposes, and there has been some lack of the development of web applications, especially concerning the new types of web applications. The problem is that web engineering methods cannot support new features of web applications, furthermore no single method to support all types of web applications. In an attempt to solve this problem, a significant amount of work has been undertaken to improve or extend these methods to develop new types of web applications. In this paper, we make a comparison between web engineering methods (including IFML) to support the features of modern web applications. The result of our comparison will show the capability of the methods to support modern and multi web applications, and help the developers in the process development web applications.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Model-Based Testing for Software Product Line: A Systematic Literature Review\n", "abstract": " Testing is an crucial part within quality assurance process for the software product line engineering. In recent years, there are many software product line testing approaches have been constructed, and various of surveys have been carried on them. However, very few of them on model based testing. We carry out a Systematic Literature Review (SLR) to indicate the strategies and the models used in software product line testing. This paper also covers on the problems that had been solved using the model based testing. These results will help other researchers to find right technique. Thus, this paper identify the model based testing strategies exist, the models exist and problems that had been solved by using model based in testing in software product line.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "A Survey of Agile Transition Models\n", "abstract": " Nowadays, since business environment is highly dynamic, software necessities are continuously being improved in order to meet the needs of modern industrialized world. Therefore, IT organizations seek for a quick way of software delivery and for adapting to the necessary technological changes. From this ideal viewpoint, traditional plan-driven developments lag behind to overcome these conflicts. The purpose of this chapter is to present the existing models and frameworks which guide organizations to adopt agile methods. This may help organizations to follow professionals' suggestions during their migration from traditional systems to agile development.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Analyzing Modern Web Applications to Recognize Features-based Web Engineering Methods\n", "abstract": " A number of Web applications are involved in the advancement from Web 1.0 to Web 4.0. These applications act as strength for the business growth over the internet. With the development of these Web applications, several features have been introduced to overcome the issues of complexities and development processes being utilized in Web engineering. In this paper, we analyze four modern Web applications namely: Rich Internet Applications (RIA), Ubiquitous Web Applications (UWA), Semantic Web Applications (SWA), and Intelligent Web Application (IWA) by inspecting the history and complexity features of these applications. Experimental analysis reveals the effectiveness of developing new Web applications specifically, Intelligent Web Application in support of improvement towards Web engineering methods.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Scalable scenario specifications to synthesize component-centric behaviour models\n", "abstract": " Several scenario description languages and associated behaviour synthesis processes have been developed. The goal of these is to synthesize behaviour models from system requirement specifications, in order to enable the early identification of weak design spots or code generation. To date, however, most of the scenario languages are poorly scaled with regards to system sizes. This is because scalability requires more expressive constructs that can help when writing a scenario specification in a concise and compact manner, thereby resulting in a reduced number of scenarios. Furthermore, due to a lack of expressiveness in scenario languages, synthesis algorithms may need to rely on global behaviour models to determine inter-scenario dependencies. The global model is an additional factor that limits the approaches\u2019 scalability. The reason is that the construction of a global model becomes harder as the system specification increases. To tackle these issues, within this article is proposed an expressive scenario description language that provides a concise and compact approach to scenario description, and defines inter-scenario dependencies semantically. A new algorithm that can address the additional constructs of the language was defined, in order to synthesize componentcentric behaviour models. The applicability of this work has been demonstrated through both an illustrative example and a real-world case study. The evaluation indicates that the proposed scenario description language is more scalable than existing languages.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Modeling and calculation of scenarios reliability in component-based software systems\n", "abstract": " System scenarios that derived from system design specification play an important role in the reliability engineering of component-based software systems. A great deal of research effort has been devoted to predict the reliability of a system at the design-time, only few papers address the challenges of modeling and calculation of scenarios reliability. This paper identifies various challenges related to scenario modeling at the early design stages based on software architecture specification. It discusses the truncation and modeling methods of scenarios, the related computation cost, and their effect on the approach scalability, which has been missed by most of the current approaches. The paper proposes a modeling and calculation approach that, pragmatically, models and divides scenarios by a scenario specification language and finite state machine. It utilizes mathematical formulas for the calculations. The\u00a0\u2026", "num_citations": "5\n", "authors": ["2185"]}
{"title": "An approach for representing domain requirements and domain architecture in software product line\n", "abstract": " Software Product Line (SPL) core assets development is an effective approach in software reuse in which core assets can be shared among the members of the product line with an explicit treatment of variability. Among the artefacts of core asset are architecture, reusable software components, domain models, requirements statements, documentation and specifications, performance models, schedules, budgets, test plans, test cases, work plans, and process descriptions. Variability in its own right is the central concept in SPL which is not being catered by conventional method of reuse. Consequently, it is important for variability to be identified and to be represented early at requirements phase. The importance of identifying requirements variability earlier at requirements level is also known as systematic reuse by researchers (Frakes and Isoda 1994; Muthig 2002). Variability at requirements levels also initiates the existence of the variability at architecture thus further highlight the inadequacy of considering variability solely at architectural level. Therefore, considering on variability at architecture and its implementation level is not enough where the understanding of variability at the requirements level is also required (Yu, Akhihebbal et al. 1998; Moon 2005; Kircher, Schwanninger et al. 2006).Nonetheless, there are challenges on relating variability at both abstraction levels where mapping of user requirements with the core assets for the adaptation process and derivation of core assets based on user requirements is a complex task (Matinlassi 2004; Dhungana 2006). This task is made difficult due to the dependencies among variants in\u00a0\u2026", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Requirements identification and representation in software product line\n", "abstract": " Software Product Line (SPL) core assets development is an effective approach in software reuse in which core assets can be shared among the members of the product line with an explicit treatment of variability. Considering reuse early in development phase can promise the increase in reuse and understanding reuse at a later development phase. This paper concentrates on identifying and representing variability at requirement level. The proposed approach differs slightly from other approaches where identification of the variant requirements is done with commonality and variability analysis and using two important requirements model, feature and use case model. Relation between both requirements model is through matrix table. This paper illustrates the application of this approach using the library systems product line example.", "num_citations": "5\n", "authors": ["2185"]}
{"title": "Elicitation of nonfunctional requirements in agile development using cloud computing environment\n", "abstract": " Nonfunctional requirements get less attention because functional requirements are considered more important in the domain of agile software methodologies. This is due to the lack of mature requirement elicitation methodologies and the nature of the software agile software development process. The less attention caused few solutions in the domain which lead to software project failure. Cloud computing helps to practice twelve (12) agile principles including nonfunctional requirement elicitation. This study proposed a semi-automated methodology which will help analyst and developers in eliciting nonfunctional requirements in agile development and cloud computing environment. The methodology used an NLP based automatic NFR extraction approach to fast the NFR elicitation process. The methodology is evaluated by applying on eProcurement dataset. The results are improved by 8.77% and 1.76% in terms\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Framework for agile development using cloud computing: a survey\n", "abstract": " Agile methods are based on frequent delivery of software, improved customer satisfaction, closed interaction with the clients and accommodation of requirement change at any stage of development. However, several challenges exist such as scalability, transparency, face-to-face communication, smooth control of development, and ability to build applications from distributed locations. A framework proposed for Agile Development using Cloud Computing (ADCC) in the earlier study is evaluated in the current study. The Malaysia Research and Education Network cloud is utilized to implement the framework. An industrial survey is conducted to evaluate the framework. The results of the industrial survey show that the ADCC framework has a positive impact on the performance of agile methods and overcomes some challenges found in distributed agile development. Furthermore, the survey verifies the\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Non-functional requirement traceability process model for agile software development\n", "abstract": " Agile methodologies have been appreciated for the fast delivery of software. They are criticized for poor handling of Non-Functional Requirements (NFRs) such as security and performance and difficulty in tracing the changes caused by updates in NFR that are also associated with Functional Requirements (FRs). This paper presents a novel approach named Traceability process model of Agile Software Development for Tracing NFR change impact (TANC). In order to validate TANC\u2019s compatibility with most of Agile process models, we present a logical model that synchronizes TANC with the two of enhanced models: secure feature-driven development (SFDD) and secured scrum (SScrum). Then, we conducted a case study on TANC using a tool support called Sagile. In terms of adaptability with agile process model, the logical model could be depicted in SFDD and the case study proved that TANC is carried out successfully in SFDD.", "num_citations": "4\n", "authors": ["2185"]}
{"title": "IMPROVING SOFTWARE RELIABILITY GROWTH MODEL SELECTION RANKING USING PARTICLE SWARM OPTIMIZATION.\n", "abstract": " Reliability of software always related to software failures and a number of software reliability growth models (SRGMs) have been proposed past few decades to predict software reliability. Different characteristics of SRGM leading to the study and practices of SRGM selection for different domains. Appropriate model must be chosen for suitable domain in order to predict the occurrence of the software failures accurately then help to estimate the overall cost of the project and delivery time. In this paper, particle swarm optimization (PSO) method is used to optimize a parameter estimation and distance based approach (DBA) is used to produce SRGM model selection ranking. The study concluded that the use of PSO for optimizing the SRGM's parameter has provided more accurate reliability prediction and improved model selection rankings. The model selection ranking methodology can facilitate a software developer\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Multi attribute architecture design decision for core asset derivation\n", "abstract": " Software Product Line (SPL) is an effective approach in software reuse in which core assets can be shared among the members of the product line with an explicit treatment of variability. Core assets, which are developed for reuse in domain engineering, are selected for product specific derivation in application engineering. Decision making support during product derivation is crucial to assist in making multiple decisions during product specific derivation. Multiple decisions are to be resolved at the architectural level as well as the detailed design level, address the need for assisting the decision making process during core asset derivation. Architectural level decision making is based on imprecise, uncertain and subjective nature of stakeholder for making architectural selection based on non-functional requirements (NFR). Furthermore, detail design level involves the selection of suitable features which have the rationale behind each decision. The rationale for the selection, if not documented properly, will also result in loss of tacit knowledge. Therefore, a multi-attribute architecture design decision technique is proposed to overcome the above mentioned problem. The technique combines Fuzzy Analytical Hierarchy Process (FAHP) with lightweight architecture design decision documentation to support the decision making during core asset derivation. We demonstrate our approach using the case study of Autonomous Mobile Robot (AMR). The case study implementation shows showed that the proposed technique supports software engineer in the process of decision making at the architecture and detail design levels.", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Model-driven estimation approach for system reliability using integrated tasks and resources\n", "abstract": " The increasing complexity of software systems in embedded systems or industrial business domains has led to the importance of reliability analysis for current systems. Reliability analysis has become a crucial part of the system development life cycle, and a new approach is needed to enable an early analysis for reliability estimation, especially for the system under design. However, the existing approach neglects the correlation between system resource and system task for estimating system reliability. This subsequently restricts accuracy of the estimation as well as causing difficulties in identifying critical resources and tasks during the design phase. This paper proposes a model-driven system reliability estimation using a scenario-based approach to estimate system reliability and identify its critical resources and system tasks during the design phase. This model is based on the PerFAM model, which can\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "A study on code peer review process monitoring using statistical process control\n", "abstract": " Software products are like two sides of a coin. We cannot achieve one without another. Today, in our software industries, monitoring software process is very challenging. Many problems of software process monitoring are hampering the quality of our software products. In this paper, we plan to address the problem of process instability and causes of process anomalies. These problems can be addressed using one of the powerful statistical techniques known as statistical process control (SPC). Also, control chart would be used in our study as it has been proved to be one of the suitable tools of SPC in monitoring process stability. As we know, the more defects we found during SDP, the less quality of the software product. Therefore, this study considers defect density as the metric to be use due to its significance in determining product quality.", "num_citations": "4\n", "authors": ["2185"]}
{"title": "A formal semantic for scenario-based model using algebraic semantics framework for MOF\n", "abstract": " Model-driven development uses models to represent system artifacts to improve the visibility of the system towards the real world. The development of models is underlying in the meta-object facility (MOF) standard in order to define the building concepts of metamodel and models. However, these concepts, especially within MOF standard, are not yet formally define which will be difficult to check the consistency between metamodel and models. Therefore, defining a formal semantic for MOF-based metamodel is essential for discovering the meaning of the model and to ensure a structural and behavioral conformance between metamodel and its model. In this paper, we define a formal semantic for a scenario-based model called Performability Failure Behavior Awareness Metamodel (PerFAM) by applying algebraic semantics for MOF framework which provides a formal stage: metamodel, model and model conformance. For this purpose, a formal consistency checking can be applied as to ensure the accuracy of the produced model towards its metamodel.", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Comparative evaluation of performance assessment and modeling method for software architecture\n", "abstract": " Conducting performance assessment during the early phases of system development enhances early design decisions of system design. The generated performance models from system design will help in identifying the potential performance problems in the system design based on the result of the performance assessment. In recent years, several methods for performance assessment and modeling have been proposed. This paper presents the comparative evaluation of performance assessment and modeling methods to discover the strengths and weaknesses of the existing methods based on a set of criteria which includes process and modeling elements that was developed with the purpose to represent a specific process to assess the performance attributes with the help of modeling concepts. The four selected methods were evaluated based on these criterions and the results will hopefully guidance\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "MARMOT and PECOS hybrid approach for embedded real time software development\n", "abstract": " Embedded real time (ERT) software development involves multi-disciplinary knowledge such as software, mechanical and electrical engineering. Besides that, multi-constraints such as timing, resources constraint, statically predictable, safety-critical and memory are also important in ERT software development. From this perspective, component based development (CBD) appears to be one of the appropriate approaches to design the ERT software due to the ability of domain experts to interactively compose and adapt sophisticated ERT software. Besides that, it can decrease the development time and improve the software quality. Therefore, existing component model such as MARMOT is introduced to master in multidisciplinary knowledge and PECOS is good support for multi-constraints extra-functionality requirement. However, MARMOT and PECOS still have their own weaknesses. MARMOT cannot support software component at detail design while PECOS does not provide component modeling. Due to the strength and the weaknesses of the MARMOT method and PECOS component model, this paper propose to hybrid the two approaches to produce a better approach for CBD of ERT software.", "num_citations": "4\n", "authors": ["2185"]}
{"title": "A Framework for Component-Based Reuse for Autonomous Mobile Robot Software\n", "abstract": " Applying software reuse to Embedded Real-Time (ERT) systems poses significant challenges to industrial software processes due to the resourceconstrained and real-time requirements of the systems. Autonomous Mobile Robot (AMR) system is a class of ERT systems, hence, inherits the challenge of applying software reuse in general ERT systems. Furthermore, software reuse in AMR systems is challenged by the diversities in terms of robot physical size and shape, environmental interaction and implementation platform. Thus, it is foresee that component-based reuse will be the suitable way to promote software reuse in AMR systems with consideration to three AMR general requirements to be self-contained, platform-independence and real-time predictable. In this thesis, a framework for component-based reuse of AMR software has been developed to enable a systematic reuse through component-based software engineering. The aim of the framework is to outline the strategies for software reuse in software development of AMR applications. The developed framework consists of four main elements: AMR component-based analysis patterns, a modified component model, a componentbased timing analysis approach, and a component-oriented programming framework. The results of implementing the framework in developing software for real AMR show that the strategies and processes proposed in the framework can fulfill the three AMR general requirements. To quantify the effectiveness of the reuse approach in the developed framework, the component reusability and the amount of reuse were measured using software metrics. The\u00a0\u2026", "num_citations": "4\n", "authors": ["2185"]}
{"title": "Agile Software Development Using Cloud Computing: A Case Study\n", "abstract": " Agile software development is successful due to self-organizing teams, adaptive planning, a cooperative environment with respect to communication with clients and team members, small development cycles, continuous design improvements, continuous delivery and feedback of clients. Cloud computing helps to reduce cost, enables scalability and enhances communication through its services. A generic framework with the conjunction of Agile Development and Cloud Computing (ADCC) proposed in an earlier study is evaluated in this study. The Malaysia Research and education network (MyRen) cloud is utilized to implement the framework. A case study is conducted to evaluate the framework. Before conducting the case study, the participants are educated on the ADCC framework. The results of the case study show that the performance of agile methods is improved with the usage of the ADCC framework. The\u00a0\u2026", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Enhancing interaction flow modeling language metamodels for designing features of rich internet applications\n", "abstract": " Rich Internet Applications (RIAs) became to standard of interactive web applications on the internet fastly. It is a complex application with a rich user interface that distributed the data between client and server also allowing an asynchronous communication between them, but web engineering methods are not able to design and implement these features impeccably. The recent web engineering method is Interaction Flow Modeling Language (IFML) which adopted by Object Management Group (OMG). It has many features for developing interactions in web application compared with other web engineering methods but also has limitation for designing RIA features. In this paper, we enhance IFML method through extension the metamodels by using UML extension mechanism, in which, we define new IFML metamodel and some new elements to cope RIA features designing in data distribution between client and server. The results show that this enhancement enables IFML to develop the new types of web applications efficiently.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Real-Time campus university bus tracking mobile application\n", "abstract": " Real-Time Campus University Bus Tracking Mobile Application is a mobile application to help campus members detect the current location of the bus in real-time. Real-Time Campus University Bus Tracking Mobile Application is a hybrid mobile application. However, for this development, it is developed for Android user only. It can show updated estimation time arrival and the number of persons inside the bus. This project using two devices embedded inside the bus, which are GPS Tracker device and IoT people counter device. All devices will transmit the data into cloud database which is Firebase. Real-Time Campus University Bus Tracking Mobile Application is developed as a platform for user to receive the data transmitted from database. Other than that, Student will know the time arrival of the bus and the current quantity of people inside the bus to lead them avoid wasted time knowing that they wait for the bus\u00a0\u2026", "num_citations": "3\n", "authors": ["2185"]}
{"title": "A new adaptive model for web engineering methods to develop modern web applications\n", "abstract": " With the evolution of modern web applications, several web engineering methods proposed to develop web applications. The modern web applications are; Rich Internet Application (RIA), Semantic Web Application (SWA), Ubiquitous Web Applications (UWA), and Intelligent Web Applications (IWA), with each of them having new features. The problem is that current web engineering methods cannot support new features of modern web applications. However, some of them extended for new concern of web applications but have limited, meaning these methods have a lack of adaptability to support features from modern web applications. In an attempt to solve this gap, we have defined a new adaptive model for the web engineering methods that can support the new features of modern web applications. This model very efficient in the process development and will be to increase the usability of the methods.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Coverage-based approach for model-based testing in Software Product Line\n", "abstract": " Rapid Quality assurance is an important element in software testing in order to produce high quality products in Software Product Line (SPL). One of the testing techniques that can enhance product quality is Model-Based Testing (MBT). Due to MBT effectiveness in terms of reuse and potential to be adapted, this technique has become an efficient approach that is capable to handle SPL requirements. In this paper, the authors present an approach to manage variability and requirements by using Feature Model (FM) and MBT. This paper focuses on modelling the integration towards enhancing product quality and reducing testing effort. Further, the authors considered coverage criteria, including pairwise coverage, all-state coverage, and all-transition coverage, in order to improve the quality of products. For modelling purposes, the authors constructed a mapping model based on variability in FM and behaviour from statecharts. The proposed approach was validated using mobile phone SPL case study.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Malaysia MOOC: Improving low student retention with predictive analytics\n", "abstract": " Massive Open Online Courses MOOCs have become more acceptable as a learning program globally, including Malaysia. One main issue that has been discussed since the implementation of MOOCs is the issue of low student retention or high dropout rates from the course. Various factors have been found to play a role in this issue including the interaction factor. Previous studies have experimented with various strategies to monitor student retention and apply intervention programs to improve the situation. The strategies include the usage of machine learning and data mining techniques in analysing students\u2019 online interactions to predict student retention rates. The implementation of these strategies produced promising result. However, in Malaysia, these strategies are not really implemented yet. Therefore, this paper discusses the issue of student retention in MOOCs, explores possible intervention plans using data mining and its suitability with the current platforms used for MOOCs. The proposed method includes predictive analytics that involves classification analysis. This paper suggests that the method can be applied to the current platform and complement intervention programs for the issue of low retention or high dropouts with several improvements.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "A comparison on similarity distances and prioritization techniques for early fault detection rate\n", "abstract": " Nowadays, the Software Product Line (SPL) had replaced the conventional product development system. Many researches have been carried out to ensure the SPL usage prune the benefits toward the recent technologies. However, there are still some problems exist within the concept itself, such as variability and commonality. Due to its variability, exhaustive testing is not possible. Various solutions have been proposed to lessen this problem. One of them is prioritization technique, in which it is used to arrange back the test cases to achieve a specific performance goal. In this paper, the early fault detection is selected as the performance goal. Similarity function is used within our prioritization approach. Five different types of prioritization techniques are used in the experiment. The experiment results indicate that the greed-aided-clustering ordered sequence (GOS) shows the highest rate of early fault detection.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Evaluation of Software Product Line Test Case Prioritization Technique\n", "abstract": " Software product line (SPL) engineering paradigm is commonly used to handle commonalities and variabilities of business applications to satisfy the specific needs or goal of a particular market. However, due to time and space complexities, testing all products is not feasible, and SPL testing is proven to be difficult due to a combinatorial explosion of the number of products to be considered. Combinatorial interaction testing (CIT) is suggested to reduce the size of test suites to overcome budget limitations and deadlines. CIT is conducted to fulfill certain quality attributes. This method can be further improvised through the prioritization of list configuration generated from CIT to gain better results in terms of efficiency and scalability, However, to the best of our knowledge, not much research has been done to evaluate existing Test Case Prioritization (TCP) techniques in SPL. This paper provides a survey of existing works on test case prioritization technique. This study provides classification and compares the best technique, trends, gaps and proposed frameworks based on the literature. The evaluation and discussion are using Normative Information Model-based Systems Analysis and Design (NIMSAD) on aspects that include context, content, and validation. The discussion highlights the lack of technique for scalability issue in SPL with most of the work is on academia setting but not on industrial practices.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Combining web engineering methods to cover lifecycle\n", "abstract": " Web applications have rapidly evolved in the last decade, whilst web engineering methods have been lacking in the process development Web applications. One of the issues in web engineering methods is that no single web engineering method provides adequate coverage for the whole life cycle, because the web engineering methods are divided into three phases, which are; requirements, analysis/design, and implementation. Therefore, each method designed to special concern. It is obvious that we need to design a new method to cover the whole lifecycle to solve this issue. In this paper, we propose a framework for the new web engineering method through a combination of three methods comprising: Navigational Development Techniques (NDT) method for requirements phase; UML-Based Web Engineering (UWE) for analysis/design phase; and Interaction Flow Modeling Language (IFML) for the implementation phase. NDT and UWE are the most representative methods to develop web applications; while IFML is the newest method that focused on design and implementation. Our framework for the new method can support a whole lifecycle. Moreover, this method is more usable from developers.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "RISK BASED DECISION SUPPORT SYSTEM FOR STAKEHOLDER QUANTIFICATION FOR VALUE BASED SOFTWARE SYSTEMS.\n", "abstract": " Stakeholder identification and quantification (SIQ) is one of the core processes in software requirements engineering (RE). The significance of stakeholders becomes more vital when a software project is a valuebased software (VBS) and the value-based requirements engineering (VBRE) is involved in it. VBS systems are developed based on business concepts in order to gain market leverage in terms of financial or economic benefits. Different SIQ approaches are presented in literature. However, most of the approaches are partially effective and SIQ process is still immature. The techniques are presented and applied under different conditions in order to monitor the performance of the approach. The presented techniques are vague and difficult to initiate. In this research, a decision support system is presented for stakeholders\u2019 quantification. The proposed system predicts the risk associated with the stakeholders using backpropagation neural network (BPNN).", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Ontology matching: In search of challenges ahead\n", "abstract": " This paper presents key features and challenges for the development of the next generation ontology matcher. Matching elements of two data instances plays an important role in e-business, multilingual data instances, biomedical and open data cloud. This paper elaborates technologies, tools, algorithms and methods used by recent ontology matchers. Ontology matching has become a Meta Research field where research topics such as developing advanced reasoner, algorithms for optimum matching, working on Meta matchers and improving results are major open research areas. In order to find the future direction towards the development of optimum matchers we illustrated a list of future challenges, key features, and their importance. This paper does not propose solution or framework for an optimum matcher. Reader will get help in deciding next ontology matching techniques in his domain of ontology matching research.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Multi-criteria architecture style selection for precision farming software product lines using fuzzy AHP\n", "abstract": " Precision Farming (PF) system is an alternative and innovative approach to improve the quality and production of crop yields. However, due to heterogeneity and user demands, PF system complexity has become higher. As such, software complexity has always been an issue in software development, especially for larger systems with innovative functionalities. One solution by which to reduce the problem of software complexity is by incorporating software reuse. Software Product Line (SPL) is a strategic reuse approach, which targets common artefacts for its product line while having a variability management mechanism to cater for variability in individual applications. This research proposes an integrated approach of SPL with architecture style selection and componentbased design for the precision farming domain. The focus of this paper is to highlight the process of architecture style selection in the proposed approach, which involves a multi-criteria design decision. The selection process uses a fuzzy analytic hierarchy process (fuzzy AHP) in order to select the best architectural style, which can fulfil most of the sought-after criteria for precision farming product line application.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Aspect-Oriented Code Generation for Integration of Aspect Orientation and Model-Driven Engineering\n", "abstract": " Software development can be improved from many perspectives by combining aspect orientation and model-driven engineering techniques. At a higher level, they can be integrated in two different ways:(1) by handling specifics of aspect orientation at modeling level and later generating object-oriented code, or (2) by transforming an aspect model directly into aspect-oriented code. The latter approach has been shown to have advantages over the former. Consequently, different aspect-oriented code generation approaches have appeared in literature. This paper comparatively evaluates some of those existing approaches, which are well-published and can be used in integration with model-driven engineering process. The results of this study indicate that in order to provide this integration, existing aspect code generation need to be improved from various perspectives. Moreover, these results also provide some insight into the prerequisites for a valuable integration of these approaches into a model-driven engineering process.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "An implementation of embedded real time system framework in service oriented architecture\n", "abstract": " Currently, the complexity of embedded software is increased, hence, more efficient design approaches are demanded. Although component based design is well-defined for developing Embedded Real Time (ERT) systems, the design and implementation of ERT component software is slow and complex. Distributed ERT systems can reduce the complexity of a component and increase its reliability and re-usability as well. Currently, Service Oriented Architecture (SOA) is an excellent technology for the implementation of distributed software. Some platforms are introduced to implement the components in SOA concept such as Service Component Architecture (SCA) and OSGI (Open Services Gateway Initiative). SCA provides a hierarchical component composition, distributed configurations and an interconnection with various means to design and combine services. However, SCA is unable to discover and reference\u00a0\u2026", "num_citations": "3\n", "authors": ["2185"]}
{"title": "A code generator for Component Oriented Programming framework\n", "abstract": " A component-based Embedded Real Time (ERT) system can improve the system development through enhancing the ability of experts in developing sophisticated ERT software. In order to generate codes, ERT code generator system frameworks have generally two parts: a graphical semi-formal-model representation to define the components, and an optimal code generator that generates codes for a resource limited microcontroller. The Unified Modelling Language(UML) semi-formal-model system does not deal with minute details of the code generation (e.g. IF, THEN, ..). Currently, code generators use a state diagram semi-formal-model to provide optimal codes. However, manual coding has not been quit entirely since current state diagrams are not capable of providing details in code generation. The Component Oriented Programming framework (COP) is an ERT framework which targets on the code\u00a0\u2026", "num_citations": "3\n", "authors": ["2185"]}
{"title": "A Component-Oriented Programming Framework for Developing Embedded Mobile Robot Software using PECOS Model\n", "abstract": " A practical framework for component-based software engineering of embedded real-time systems, particularly for autonomous mobile robot embedded software development using PECOS component model is proposed The main features of this framework are:(1) use graphical representation for components definition and composition;(2) target C language for optimal code generation with small micro-controller; and (3) does not requires run-time support except for real-time kernel. Real-time implementation indicates that, the PECOS component model together with the proposed framework is suitable for resource constrained embedded systems.", "num_citations": "3\n", "authors": ["2185"]}
{"title": "Formal Specification of a Wall-Climbing Robot Using Z \u00e2\u20ac\u201cA Case Study of Small-Scale Embedded Hard Real-Time System\n", "abstract": " Aktiviti menguji sama ada sistem masa nyata memenuhi spesifikasi masa dan keserempakan adalah sangat penting. Salah satu bidang penyelidikan dalam bidang keboleh-percayaan perisian ialah teknik formal yang cuba untuk membuktikan kesahihan sesuatu atur cara dengan spesifikasinya. Oleh kerana masa dan keserampakan merupakan aspek yang penting dalam sistem masa nyata, keperluan untuk menggunakan teknik formal sebagai teknik untuk mengesahkan aspek masa dan keserempakan ini adalah amat tinggi. Kertas kerja ini mengkaji proses membina spesifikasi formal untuk sistem masa nyata berskala kecil dengan menggunakan teknik Z. Spesifikasi formal yang dibangunkan di dalam kertas kerja ini diharap dapat membantu proses penganalisisan fasa reka bentuk di awal proses pembangunan sistem. Kertas kerja ini juga diharap dapat menjadi rujukan kepada projek\u00e2\u20ac\u201cprojek teknik formal yang akan datang terutamanya projek yang berkaitan dengan sistem masa nyata berskala kecil. Kata kunci: Kebolehpercayaan perisian; spesifikasi formal; Z; masa nyata; sistem berskala kecil. The task of checking whether a real\u00e2\u20ac\u201ctime system satisfies its timing and concurrency specifications is extremely important. One major area of research addressing software reliability aspect is called formal method, which attempts to prove the correctness of programs with respect to system specifications. Since, timing and concurrency properties can very important in the operation of real\u00e2\u20ac\u201ctime systems, there is a need for applying formal methods to verify timing properties. This paper investigates the process of building a formal specification of a\u00a0\u2026", "num_citations": "3\n", "authors": ["2185"]}
{"title": "\u0623\u062b\u0631 \u062a\u0642\u0644\u0628\u0627\u062a \u0633\u0639\u0631 \u0627\u0644\u0635\u0631\u0641 (\u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a) \u0639\u0644\u0649 \u062a\u062f\u0641\u0642\u0627\u062a \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631 \u0627\u0644\u0623\u062c\u0646\u0628\u064a \u0627\u0644\u0645\u0628\u0627\u0634\u0631 \u0641\u064a \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u200e\n", "abstract": " \u062a\u0646\u0627\u0648\u0644 \u0647\u0630\u0627 \u0627\u0644\u0628\u062d\u062b \u0645\u0648\u0636\u0648\u0639 \u0623\u062b\u0631 \u062a\u0642\u0644\u0628\u0627\u062a \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0639\u0644\u0649 \u062c\u0630\u0628 \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631 \u0627\u0644\u0623\u062c\u0646\u0628\u064a \u0627\u0644\u0645\u0628\u0627\u0634\u0631 \u0641\u064a \u0627\u0644\u0633\u0648\u062f\u0627\u0646 \u060c \u0647\u062f\u0641 \u0625\u0644\u0649 \u0642\u064a\u0627\u0633 \u0623\u062b\u0631 \u062a\u0642\u0644\u0628\u0627\u062a \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0639\u0644\u0649 \u062c\u0630\u0628 \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631\u0627\u062a \u0627\u0644\u0623\u062c\u0646\u0628\u064a\u0629 \u0627\u0644\u0645\u0628\u0627\u0634\u0631 \u0641\u064a \u0627\u0644\u0633\u0648\u062f\u0627\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0641\u062a\u0631\u0629 \u0645\u0646 2000 \u0625\u0644\u0649 2018\u0645 \u060c \u062a\u0645\u062b\u0644\u062a \u0645\u0634\u0643\u0644\u0629 \u0627\u0644\u0628\u062d\u062b \u0641\u064a \u0625\u0644\u0649 \u0623\u064a \u0645\u062f\u0649 \u0623\u062b\u0631\u062a \u062a\u0642\u0644\u0628\u0627\u062a \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0639\u0644\u0649 \u062c\u0630\u0628 \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631\u0627\u062a \u0627\u0644\u0623\u062c\u0646\u0628\u064a\u0629 \u0627\u0644\u0645\u0628\u0627\u0634\u0631\u0629 \u060c \u0648\u0627\u0633\u062a\u062e\u062f\u0645 \u0627\u0644\u0628\u062d\u062b \u0627\u0644\u0645\u0646\u0647\u062c \u0627\u0644\u0648\u0635\u0641\u064a \u0627\u0644\u062a\u062d\u0644\u064a\u0644\u064a \u0627\u0644\u0642\u064a\u0627\u0633\u064a \u060c \u0648\u0643\u0627\u0646\u062a \u0641\u0631\u0636\u064a\u0629 \u0627\u0644\u0628\u062d\u062b \u0648\u062c\u0648\u062f \u0639\u0644\u0627\u0642\u0629 \u0637\u0631\u062f\u064a\u0629 \u0628\u064a\u0646 \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0648\u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631\u0627\u062a \u0627\u0644\u0623\u062c\u0646\u0628\u064a\u0629 \u0627\u0644\u0645\u0628\u0627\u0634\u0631\u0629 \u060c \u0648\u062a\u0648\u0635\u0644 \u0627\u0644\u0628\u062d\u062b \u0625\u0644\u0649 \u0639\u062f\u0629 \u0646\u062a\u0627\u0626\u062c \u0645\u0646 \u0623\u0647\u0645\u0647\u0627 \u0623\u0646 \u0647\u0646\u0627\u0644\u0643 \u0639\u0644\u0627\u0642\u0629 \u0637\u0631\u062f\u064a\u0629 \u0628\u064a\u0646 \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0648\u062d\u062c\u0645 \u062a\u062f\u0641\u0642\u0627\u062a \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631 \u0627\u0644\u0623\u062c\u0646\u0628\u064a \u0641\u064a \u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u062e\u0644\u0627\u0644 \u0627\u0644\u0641\u062a\u0631\u0629 (2000-2018) \u060c \u0645\u0645\u0627 \u064a\u0639\u0646\u064a \u0645\u0646 \u062c\u0647\u0629 \u0623\u062e\u0631\u0649 \u0648\u062c\u0648\u062f \u0639\u0644\u0627\u0642\u0629 \u0639\u0643\u0633\u064a\u0629 \u0628\u064a\u0646 \u0633\u0639\u0631 \u0635\u0631\u0641 \u0627\u0644\u062f\u0648\u0644\u0627\u0631 \u0627\u0644\u0623\u0645\u0631\u064a\u0643\u064a \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u062c\u0646\u064a\u0647 \u0627\u0644\u0633\u0648\u062f\u0627\u0646\u064a \u0648\u0645\u0639\u062f\u0644 \u0627\u0644\u0646\u0645\u0648 \u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a \u0641\u064a \u0627\u0644\u0633\u0648\u062f\u0627\u0646 \u060c \u0643\u0645\u0627 \u062a\u0648\u0635\u0644 \u0627\u0644\u0628\u062d\u062b \u0625\u0644\u0649 \u0639\u062f\u0629 \u062a\u0648\u0635\u064a\u0627\u062a \u0623\u0647\u0645\u0647\u0627 \u0636\u0631\u0648\u0631\u0629 \u0627\u0644\u0627\u0647\u062a\u0645\u0627\u0645 \u0628\u0633\u0639\u0631 \u0627\u0644\u0635\u0631\u0641 \u0648\u0625\u0633\u062a\u0642\u0631\u0627\u0631\u0647 \u0648\u062c\u0639\u0644\u0647 \u0623\u0643\u062b\u0631 \u0645\u0631\u0648\u0646\u0629 \u0644\u0645\u0627 \u0644\u0647 \u0645\u0646 \u062f\u0648\u0631 \u0643\u0628\u064a\u0631 \u0641\u064a \u062a\u062d\u0642\u064a\u0642 \u0627\u0644\u0627\u0633\u062a\u0642\u0631\u0627\u0631 \u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a \u0648\u062a\u062d\u0642\u064a\u0642 \u0627\u0644\u062a\u0648\u0627\u0632\u0646\u0627\u062a \u0627\u0644\u062f\u0627\u062e\u0644\u064a\u0629.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Feature engineering for predicting MOOC performance\n", "abstract": " Increasing data recorded in massive open online course (MOOC) requires more automated analysis. The analysis, which includes making student's prediction requires better strategy to produce good features and reduces prediction error. This paper presents the process of feature engineering for predicting MOOC student's performance utilizing deep feature synthesis (DFS) method. The experiment produces features which all the top features selected using principal component analysis (PCA) are the features that are generated from method. In terms of prediction comparing both based features and generated features, the result shows better accuracy for generated features proposed using k-nearest neighbours technique which shows the method potential to be used for future prediction model.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Case study on non-functional requirement change impact traceability for Agile software development\n", "abstract": " Currently, it is crucial to develop a complex software on time. Agile software development methodologies provide methods to develop a system in term of time and cost-saving but it has been criticized for software quality management. In this paper, a case study is used to find out the need of NFR change impact traceability approach in most of Agile software methodology. This case study was conducted in an undergraduate course that trained the students on how to develop software using Agile process model. This case study has been conducted for 4 months in an undergraduate-level course, Application Development. The samples of this case study are among Year 3 undergraduate students. The case study shows the lack of traceability techniques in the existing Agile process model (SFDD-Secured Feature Driven Development) that result to non-awareness of NFR change impact during development. Based on the case study mentioned the main objective of the case study conducted in survey is to empirically test the theoretical constructs and the hypothesized relationships of the research issues that concern on the lack of change impact management towards NFR in Agile Software Methodology. TANC (Traceability for Agile Non-Functional Requirement Change Impact) model offered techniques in tracing change impact during the agile development process. Therefore, the result of the case study, a traceability process model needs to design in order to tackle the NFR change impact issues in Agile software development.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Derivation of Test Cases for Model-based Testing of Software Product Line with Hybrid Heuristic Approach\n", "abstract": " In Model-based testing (MBT) for Software Product Lines (SPLs), many algorithms have been proposed for test case generation. The test case is generated based on a test model which aims to achieve optimization. The heuristic search algorithm is one of the techniques that can be used to traverse the test model with a good quality of solutions. This paper describes our experience in using three types of search algorithm, which are Floyd\u2019s Warshall, Branch and Bound algorithm and Best First Search (FWA-BBA-BFS) which were integrated and hybridized in order to fully explore the test model. In this paper, this algorithm is validated based on test case results measured according to coverage criteria, generation time and size of test suite. Based on the experimental results, it is established that our proposed algorithm can generate test cases with reasonable cover-age, minimal execution time and appropriate\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Requirements prioritization aspect s quantification for value-based software developments\n", "abstract": " Requirements prioritization considered as an important activity in requirements engineering, helps in decision making for software development. Requirements prioritization is performed to select the candidate requirements for different software releases. Different prioritization techniques are available in literature to facilitate experts for requirements prioritization in the industry. It is also evident from the literature that different requirements prioritization aspects are considered to support the process of prioritization. Consideration of these aspects is more worth-fuller in the domain of value-based software engineering where the success of the system depends on the success of software. These aspects are reviewed from literature and summarized accordingly to improve the process of requirements prioritization. Moreover, these aspects are classified and quantified as technical aspects and business aspects based on relevance towards prioritization process. Further efforts are made to validate these aspects and their logical grouping from industry experts through survey. Most of the existing techniques in general and value-based software development in specific missing consideration of these aspects. This research contribution is an effort to highlight and summarize identification and quantification of possible aspects to be a part of requirements prioritization process.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Enhanced educational robotics feature model in software product line\n", "abstract": " Educational robotics (ER) has been increasingly used as a pedagogical tool to attract students in Science, Technology, Engineering and Mathematic subjects for engaging students with technology. A more efficient approach is needed to ensure less time and effort invested for ER development for different educational context level. Software Product Line (SPL) is an approach which allows systematic reuse in developing family of products where similar applications will be analysed in terms of its features commonality and variability. Feature model (FM) is the most popular model used to represent common and variable features in SPL To enable the success of SPL reuse in ER, FM must be able to represent the most important elements in ER, the learning elements. Currently there is only one approach, the Generative Learning Object (GLO) which represents FM with learning elements. Nevertheless, GLO has its own\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Increasing usability for web engineering methods\n", "abstract": " Usability is considered to be one of the most important quality factors for products, and accordingly, there are a number of MDWE methods existing which can develop web applications. With the rapid evolution of web applications, existing MDWE methods has a number of weaknesses was appeared in the process development web applications; one of the issues is usability problem. There are many factors to increase usability of MDWE methods such as adaptability, lifecycle, user interface and so on. In this paper, we define a new framework to increase usability for MDWE methods through two important factors that are adaptability and lifecycle. Increased usability will ultimately lead to increased quality of the methods. Furthermore, increased quality of web engineering solutions would subsequently lead to improved usability for websites and web applications.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "A weakly hard real-time tasks on global scheduling of multiprocessor systems\n", "abstract": " Real-time tasks can be classified into three categories, based on the \u201cseriousness\u201d of deadline misses - hard, soft and weakly hard real-time tasks. The consequences of a deadline miss of a hard real-time task cannot be accepted whereas soft real-time tasks tolerate \u201csome\u201d deadline misses. While, in a weakly hard real-time task, the distribution of its met and missed deadlines is stated and specified precisely. Due to the complexity and significantly increased functionality in system computation, attention has been given to multiprocessor scheduling, comprised of several processors. Due to the fact that in multiprocessor, there have more than one processor, algorithms which can cater higher computational complexity for task allocation and for task migration are highly required. Thus, the sufficient and efficient scheduling algorithm supported by accurate schedulability analysis technique is presented to provide weakly\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "A survey of software reliability growth model selection methods for improving reliability prediction accuracy\n", "abstract": " Reliability is a software quality characteristic that refer to the probability a system will work correctly over a period of time. Reliability prediction is important as it can be used to plan deployment, maintenance and test activities. This study assesses the efficiency of several techniques in software reliability model (SRM) selection and aims to find out the possible enhancement to improve software reliability accuracy. The result of the survey shows most of the SRM selection technique does not optimizes the model parameter.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "A weakly hard scheduling approach of partitioned scheduling on multiprocessor systems\n", "abstract": " Real-time systems or tasks can be classified into three categories, based on the \u00e2\u20ac \u0153seriousness\u00e2\u20ac of deadline misses \u00e2\u20ac\u201chard, soft and weakly hard real-time tasks. The consequences of a deadline miss of a hard real-time task can be prohibitively expensive because all the tasks must meet their deadlines whereas soft real-time tasks tolerate \u00e2\u20ac \u0153some\u00e2\u20ac deadline misses. Meanwhile, in a weakly hard real-time task, the distribution of its met and missed deadlines is stated and specified precisely. \u00c2 As real-time application systems increasingly come to be implemented upon multiprocessor environments, thus, this study applies multiprocessor scheduling approach for verification of weakly hard real-time tasks and to guaranteeing the timing requirements of the tasks. In fact, within the multiprocessor, the task allocation problem seem even harder than in uniprocessor case; thus, in order to cater that problem, the sufficient and efficient scheduling algorithm supported by accurate schedulability analysis technique is present to provide weakly hard real-time guarantees. In this paper, a weakly hard scheduling approach has been proposed and schedulability analysis of proposed approach consists of the partitioned multiprocessor scheduling techniques with solutions for the bin-packing problem, called R-BOUND-MP-NFRNS (R-BOUND-MP with next-fit-ring noscaling) combining with the exact analysis, named hyperperiod analysis and deadline models; weakly hard constraints and \u00c2\u00b5-pattern under static priority scheduling. Then, Matlab simulation tool is used in order to validate the result of analysis. From the evaluation results, it can be proven that\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Deriving behavioural models of component-based software systems from requirements specifications\n", "abstract": " Software behavioural models that derive from early requirements specifications such as use-case scenarios and properties have proven useful in early analysis and checking of the design correctness of individual components or whole system. However, deriving of these models becomes harder as a system specification grows. Expressive scenario description language that able to compact and concise scenario specifications is one of the solutions can enhance the ability of modeling large requirements specifications. Deriving behaviour models from compacted scenario specifications is a challenge related to this solution. This paper, introduce expressive scenario language and outline a process to derive system behavioural models from scenarios of this language in form of Labelled Transition Systems (LTS). In addition, the paper also covers some discussion of how these derived models can help analysis of\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Clustering Test Cases in Web Application Regression Testing using Self-Organizing Maps\n", "abstract": " In the software testing domain, different techniques and approaches are used to support the process of regression testing in an effective way. Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to detect the faults in a reasonable amount of time. User-session is a unique feature of web applications that is useful in the process of regression testing as they comprise precious information about the application state before and after any change that is made to the software code. The main challenge is the effectiveness of average percentage fault detection rate and time constraint in the existing techniques. Thus, in this research the priority is given to the test cases, clustered according to some criteria. Using self-organizing map for clustering helps to obtain a higher fault detection rate in a relatively small time span.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "A COMPARATIVE EVALUATION OF STATE-OF-THE-ART INTEGRATION TESTING TECHNIQUES OF COMPONENT-BASED SOFTWARE.\n", "abstract": " In the last few years, component-based software has gained widespread notice and acceptance as a method that facilitates the development of existing large, complex, and very critical systems by integrating prefabricated small pieces of software called components. Component integration becomes an essential stage in the component-based software development Lifecycle. Therefore, testing components after integration is an important activity. Due to the unavailability of source code of integrated components and due to the lack of component information or documentations, integration testing becomes more difficult and very complex task. In the literature, different techniques have been proposed with the purpose of facilitating the integration testing of component-based software. In this paper, we study, classify, and evaluate some of the existing integration testing techniques and make a comparison in order to help\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Integrated MARTE-based Model for Designing Component-Based Embedded Real-Time Software\n", "abstract": " Recently, modeling and implementation of Embedded Real Time System (ERTS) are unavoidably becoming more complicated to develop and be reused because of the increasingly complex design. The complexity is due to the functionality increment factor in accordance with users\u2019 needs and demands, resulting in the growing scale of the developed systems. The current development approach based on Object-Oriented (OO) does not match the current requirements of the system. The OO approach has numerous flaws, thus, Component-Based Software Engineering (CBSE) has been appointed to resolve those problems. However, the current CBSE approach also has some drawbacks such as lack of ERTS standardized modeling and specific development methodology. The problems concerning the established ERTS development methodologies, Methods for Component-Based Real-Time Object-Oriented\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Service-oriented Analysis and Design Approach for Distributed Embedded Real-time Systems\n", "abstract": " Distributed Embedded Real-time Systems (DERTS) are computing systems that are integrated inside real-world objects. Their unique characteristics and the wide spread demand with increasing functionality make their development different and complex as compared to enterprise software. Furthermore, companies need reusable design of DERTS to achieve reduced time-to-market and cost. Therefore, the designers of DERTS are always looking for the latest state of the art software engineering methods and techniques for analysis and design of DERTS. Previously, the Service-Oriented Computing (SOC) has been used for DERTS development, but it is mostly used in an ad hoc manner and without any focus on analysis and design. To address these issues, this thesis presents a systematic analysis and design approach for service-oriented DERTS development, aimed at reducing complexity and increasing reusability of DERTS design. The existing service-oriented concepts and methods were extended but the focus was on specific DERTS characteristics such as resource constraints, device considerations and real-time properties. The key findings of this study are service analysis and modelling mechanisms and serviceoriented process for DERTS development. The service analysis mechanism includes service layer architecture and service identification guideline for DERTS. Secondly, the service modelling mechanism includes service meta-model, profile and levels of abstraction models. Finally, the service-oriented process defines the analysis and design phases for DERTS development. The applicability of the proposed approach is\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Web Application Regression Testing: A Session Based Test Case Prioritization Approach\n", "abstract": " In a software testing domain, different techniques and approaches are used to support the process of regression testing in an effective way. The main approaches are test suite minimization, test case prioritization and test case selection. Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. User-sessions and cookies are unique features of web applications that are useful in regression testing because they have precious information about the application state before and after any change that is made to the software code. The main challenge is the effectiveness of average percentage fault detection rate and time constraint in the existing techniques. Thus, in this research the priority is given to clustering test cases that are performed based on some criteria related to http requests which collected from the database of server side. To verify the new technique some fault will be seeded in subject application then applying the prioritization criteria on test cases for comparing the effectiveness of average percentage fault detection rate and time with existing techniques.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "An Effectiveness Test Case Prioritization Technique for Web Application Testing\n", "abstract": " Regression tests are executed when some changes are made in the existing application in order to check the negative impact of the changes in the rest of the system or on the expected behavior of other parts of the software. There are two primary options for test suites used during regression testing, first generate test suites that fulfill a certain criterion, or user session based test suites. User-sessions and cookies are unique features of web applications that are useful in regression testing. The main challenge is the effectiveness of average percentage fault detection rate and time constraint in the existing techniques. Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. Thus, in this research the priority is given to test cases that are performed based on some criteria related to log file which collected from the database of server side. In this technique some fault will be seeded in subject application then applying the prioritization criteria on test cases to obtain the effectiveness of average percentage fault detection rate.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "An intermediate metamodel for failure-based behavior of performance and reliability\n", "abstract": " Software quality analysis is an important part in getting better software systems. Performing software quality analysis during design time enhances design decisions. In order to assist design decisions and the quality analysis, the design model, which is annotated with quality information, must be transformed into analysis model to execute software analysis part. To achieve this purpose, a main idea is to define model transformation that takes some input from design model and transformed into analysis model. However, both model inherits heterogeneous notation and semantic that could be difficult to perform direct model transformation. To solve this shortcoming, the intermediate metamodel, which is based on failure behavior, is defined as to capture the essential quality information particularly for performance and reliability and be able to transform into a multiple analysis model. In this paper, the intermediate\u00a0\u2026", "num_citations": "2\n", "authors": ["2185"]}
{"title": "An approach to reusable software for mobile robot applications through analysis patterns\n", "abstract": " The use of software analysis patterns as a means to facilitate Autonomous mobile robots (AMR) software knowledge reuse through component-based software engineering is proposed. The software analysis patterns for AMR were obtained through a pattern mining process, and documented using a standard catalogue template. These analysis patterns are categorized according to hybrid deliberate layered architecture of robot software: reactive layer, supervisor layer and deliberative layer. In this paper, the analysis patterns in the reactive layer are highlighted and presented. The deployment of the analysis patterns are illustrated and discussed using an AMR software case study. The reuse potential of these patterns is evaluated by measuring the reusability of components in the analysis patterns.", "num_citations": "2\n", "authors": ["2185"]}
{"title": "Systematic Literature Review on Global Software Development Risks in Agile Methodology\n", "abstract": " Background: The word \u201cGlobal Software Development\u201d can be described as the development of software, with development teams spread across different geographical locations. Problem statement: The issues arise when there are gaps in information, workflows or processes, policies and others in the world. Objective: This paper aims to build an understanding of the risk in Global Software Development. Then, to identify category risk in Global Software Development and how Agile can reduce or mitigate their challenges. Method: This review paper using the standard systematic literature review method by Kitchehamm by reviewing and analyzing the relevant state-of -art techniques and approaches in the journal libraries based on the research questions. Results: The findings show that communication in Agile Global Software development is the main risk challenge. Contribution: The contributions of this paper may\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Features and Behaviours Mapping In Model-based Testing in Software Product Line\n", "abstract": " Testing in Software Product Line (SPL) is very hard task due to the high degree of variability that existed in products. Nowadays, many testing approaches have concern on reusability of technique. Feature Model (FM) is used to represent variability and commonality of products in SPL. The components of FM that represented as symbols caused the needs of mapping with other models to represent their semantics. In this paper, there are concise definitions that relates with mapping approaches between FM and behaviour model. Model definitions presented in our algorithms is used for automation mapping process based on traceability link created. The advanced query mechanism proposed to automate the process of mapping between models. Based on the experimental result, it shows that our proposed algorithm can help tester in automate searches for accurate mapping of features and requirements.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Adaptive Learning in Computing Education: A Systematic Mapping Study\n", "abstract": " Adaptive learning has become popular among researches in the recent decade. It is believed in giving good benefits which promotes effective ways or methods to improve youth performance in education either for computing field or any non-computing fields. This study focuses on investigating the contribution of adaptive learning in computing education including the adaptation environment involved. This systematic mapping study was undertaken to analyse all relevant and related studies. A set of three research questions were defined in which 68 primary studies, dated from 2012-2020, were analysed and evaluated. The mapping shows the trends and the taxonomy of the contribution adaptive learning in computing education. However, the result also can be applied for non-computing education field. Most of the studied areas are on providing adaptive learning environment and investigating the suitability of\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Intelligent Web Applications as Future Generation of Web Applications\n", "abstract": " During the recent years World Wide Web very fast increased a fundamental part in our everyday life. In commerce, personal relationship, the effect of the universal network has wholly changed the way people interact with each other and with machines. The problem is after rising the Artificial Intelligence to presenting human feelings, everything changed including web applications. In this paper, we describe the intelligent web applications as present and future of web applications, moreover we highlight the special features and their roles in increasing intelligence of web applications as well as impact this application in the process development of the web systems. The result of this paper leds the developers to create smart and modern web applications.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Missing Data Imputation Techniques for Software Effort Estimation: A Study of Recent Issues and Challenges\n", "abstract": " Software effort estimation is one the critical aspects of software engineering. It revolves around predicting the required efforts needed to complete a software task. However, any estimation technique or model relies on an input data in which it defines and predicts future values. Missing data and values within such data is a common occurrence in the software development industry and thus it leads to inaccurate predictions or misleading results. Thus, Missing Data is an important aspect of effort estimation models that is required to be addressed. However, Missing Data is not without its gaps and issues. This review aims at elaborating the recent issues and gaps that exist within the missing data and software effort estimation field. This may allow future researchers to get a better grasp and understanding of the inner workings of Missing Data and the methods through which these challenges can be addressed.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Role of Interaction Flow Modeling Language (IFML) in the Development of Ubiquitous Web Applications (UWAs)\n", "abstract": " During the revolution of webs, several modern web applications appeared. After developing electronic devices, Ubiquitous web applications (UWA) becomes an important part of web applications. UWAs are a new type of web applications which are accessed in various contexts, meaning through different devices, by users with various interests, at any time from any place around the globe. The problem is UWA features made challenge during the process development of web engineering methods. Interaction Flow Modeling Language (IFML) is the recent modeling language has many concepts for developing interactions in web applications compared with other web engineering methods. In this paper, we present the role of IFML method to develop UWA features through analyzing IFML and designing a case study. The result of this paper becomes a guide for developers and increase the usability of IFML.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "An Approach for Mapping the Aspect State Models to Aspect-Oriented Code\n", "abstract": " Model-driven code generation allows rapid generation of precise code thus reducing the development effort and the delivery time. Consequently, model-driven code generation has been a topic of interest in varying perspectives. While model-driven code generation has been explored well in many domains, its full potential has not been exploited in the context of aspect-oriented software development. The existing approaches have mainly focused on code generation from class diagrams only. Code generation from class diagrams is straightforward as majority of the constructs involved in these diagrams are directly mapped to those in programming languages. However, code generated using class diagrams is limited to skeletons of classes and methods only and does not contain behavior. In this study, we use the state diagrams and propose a mapping of its constructs to AspectJ language. We use the Reusable\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Towards Formulating Dynamic Model for Predicting Defects in System Testing using Metrics in Prior Phases\n", "abstract": " Many studies have been carried out in formulating software defect prediction but it is of limited knowledge that those studies emphasized on predicting defects in system testing phase. This study specifically focuses on establishing a prediction model for system testing defects by exploiting metrics prior to system testing under V-model development. The initiative helps independent testing team to prevent as many defects as possible from escaping to production environment. The proposed model analyzes development-related and testing-related metrics collected from requirement, design and construction phases in determining which of those could significantly predict defects at the start of system testing. By applying statistical analysis to those metrics, this model able to formulate one generalized mathematical equation for predicting defects in system testing. The model applies 95% prediction interval to ensure the accuracy of the prediction.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Particle Swarm Optimization for Test Case Prioritization Using String Distance\n", "abstract": " Test case prioritization (TCP) is to maximize the fault detection rate, which enumerates the speed of fault detection through the testing phase. The uses of the previous executed test cases information, is one of common approach for TCP. The information such as test cases inputs can be useful to distinguish each test case and resulting into prioritization process by using some algorithm. Current research validate that the performance of such prioritization algorithm degrades as the number test cases increase. In this paper, a particle swarm optimization was proposed to prioritize TCP based on string distances. This experiment was evaluated using real-world TSL benchmark dataset and the proposed work was compared to nearest neighbor (NN) and random ordering. The result of the experiment, the proposed mPSO have two best APFD values for two dataset which is 98.26 for grep and 97.00 for gzip, while NN have\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "A NEW FRAMEWORK FOR USABILITY EVALUATION WEB ENGINEERING METHODS.\n", "abstract": " Usability is an important attribute for measuring a quality of web engineering methods. Also, many usability evaluation methods proposed. The problem is web engineering methods can not complete web development lifecycle especially evaluation phase and the developers sometimes do not meet users' expectations regarding usability. For expressing the actual feedback from developers directly, we have defined a new framework for usability evaluation of web engineering methods. The new framework consists of six phases, in these phases we defined web engineering process development, web application features, usability attributes, usability evaluation methods, and usability testing. In the usability testing, we use several ways for returning user's feedback such as usability testing by expert and group developers, open-ended and close-ended questionnaires, and interviews. This framework is too strong for\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Evaluation of Computer Programming Teaching Tools for Secondary Schools Students\n", "abstract": " This paper describes the result obtained from the research related to programming teaching tools for secondary school students. The study aimed to identify the suitable tools for delivering the introductory programming to the school students. Survey, line following case study and experiment were used in the evaluation of this research. The experiment was conducted using Snap, Scratch and RoboKar to compare which tool is most effective in teaching programming for a group of secondary schools students. The performance of the students in solving line following problem were recorded and analyzed in order to evaluate each tool used. From the analysis result, text based programming approach such as RoboKar tool is identified as suitable approach in teaching sequence control structure for 13 to 17 years old students. Besides, it is also suitable for teaching selection control structure for 13 to 15 years old and\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Systematic reviews and mapping studies on software requirements: encyclopedia of objectives and issues for newbies\n", "abstract": " The domain of requirements engineering (RE) is intercepted by the interest of a wider community in the software industry. The role of RE is inevitable in the success and failure of software projects. RE is a multi-dimensional area and it is comprised of stakeholder analysis, requirements elicitation, and requirements prioritization mainly. In RE, the valuable requirements are explored from a critical set of stakeholders and these requirements are prioritized in order to design a system of high assurance. Several systematic literature reviews (SLR) are written related to RE domain which deliver a comprehensive knowledge in the domain of RE. Hence, currently no SLR exists that is able to provide an aggregate knowledge about RE intricacies. In this research, the different systematic reviews were investigated in order to aggregate the generated knowledge in different sub-domains of RE. This research highlights the objectives, sub-objectives of the existing SLRs and also presents information about RE issues in different perspectives. Initially, 214 studies were identified and only 39 studies were included in this SLR for research and analysis purposes.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Correction: technique for early reliability prediction of software components using behaviour models\n", "abstract": " Copyright:\u00a9 2016 Ali et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Test case prioritization with textual comparison metrics\n", "abstract": " Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Strategy for Scalable Scenarios Modeling and Calculation in Early Software Reliability Engineering\n", "abstract": " System scenarios derived from requirements specification play an important role in the early software reliability engineering. A great deal of research effort has been devoted to predict reliability of a system at early design stages. The existing approaches are unable to handle scalability and calculation of scenarios reliability for large systems. This paper proposes modeling of scenarios in a scalable way by using a scenario language that describes system scenarios in a compact and concise manner which can results in a reduced number of scenarios. Furthermore, it proposes a calculation strategy to achieve better traceability of scenarios, and avoid computational complexity. The scenarios are pragmatically modeled and translated to finite state machines, where each state machine represents the behaviour of component instance within the scenario. The probability of failure of each component exhibited in the scenario is calculated separately based on the finite state machines. Finally, the reliability of the whole scenario is calculated based on the components\u00e2\u20ac\u2122 behaviour models and their failure information using modified mathematical formula. In this paper, an example related to a case study of an automated railcar system is used to verify and validate the proposed strategy for scalability of system modeling.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "A schedulability analysis for weakly hard real-time tasks in partitioning scheduling on multiprocessor systems\n", "abstract": " The importance of missing a deadline is one of the classifications of a real-time tasks or systems. A weakly hard real-time system is a new generation for a real-time system in which some degree of missed or losses deadlines are tolerated occasionally but the missing of deadlines has to be stated precisely. For a traditional real-time system, timing requirements in hard real-time are very restrictive, must meet all the tasks deadlines. Meanwhile, in soft real-time timing requirements, the requirements are too relaxed because there are no guarantees can be given for the deadline, whether it is met or missed. As a systems demand complex and significantly increased functionality, multiprocessor scheduling has been given attention and taken into consideration. In fact, within the multiprocessor, the predictability problems seem even harder. Thus, in order to deal with the problem, the partitioned multiprocessor scheduling\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Modeling of broadband light source for optical network applications using fiber non-linear effect\n", "abstract": " Vision towards establishing an all fiber configuration is the motivation behind this work. With the increasing need for high capacity communication systems, Broadband sources have become a necessity. Broadband optical sources are an integral part of multichannel high speed fiber optical communication networks based on all-optical WDM and OCDM. FWM (Four-Wave Mixing) effects and SC (Super Continuum) phenomenon in fibers are used in the design of broadband optical sources. The spectral slicing of the broadband spectra has been proposed in literature as a simple technique to create multi-wavelength optical sources for wavelength division multiplexing applications. The objective of this work is to develop an accurate model for simulating FWM and SC based broadband optical spectra and compare their performances. The modeling work is carried out using SIMULINK in MATLAB 7.10. 0 (R2010a).", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Integration of mobile and web application: an implementation of diabetic management system\n", "abstract": " Rapid development and high mobile devices penetration around the globe has impacted wireless and flexibility usage that promotes usability of technology at anytime and anywhere. Wireless technology has turned out to be the major medium in communication. One of the best-known examples of wireless technology is the mobile phone that can help to create solutions faster and easier than earlier days. However, designing and implementing these types of information systems can be highly challenging and difficult as user interaction with such technologies will be continuous and pervasive. Therefore, we propose a solution on how to deliver and share information in an ubiquitous environment. A prototype of diabetes monitoring system that support both mobile and web platform were implemented and evaluated. An evaluation based on expert judgment technique was used to validate the prototype. This study has shown that integration with mobile and web to access information on the internet can be a paradigm to promote ubiquitous information access and sharing.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Reusable Aspect Models versus Theme/UML: the Aspect-Oriented Code Generation Perspective\n", "abstract": " The integration of aspect oriented modeling approaches with model-driven engineering process achieved through their direct transformation to aspect-oriented code is expected to enhance the software development from many perspectives. This study aims to assess the existing UML-based aspect-oriented modeling techniques from the perspective of their suitability with regards to integration into model-driven engineering process through aspect-oriented code generation. For this purpose, we selected two mature aspectoriented modeling approaches, Reusable Aspect Models and Theme/UML, and proceeded to evaluate them in a detailed way from the specific perspectives of design and its mapping to the implementation code. The in-depth comparison reveals some points equally shared by both approaches, and identifies some areas where one has advantage over the other. The study concludes that the Reusable Aspect Models approach may be seen as a preferred approach to handling the task of integration using aspect-oriented code generation.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Integration Islamic Banking System Based on Service Oriented Architecture and Enterprise Service Bus\n", "abstract": " Integration is one of the most important parts for the complex system like Islamic Banking System (IBS). Most of the Islamic Banking System have becomes in two different parts of financial and deposit part that made IBS to becomes more complex for integration than other type of banking system. Despite to the current technologies ability to integrate different application together, it also makes integrating IBS more complicated due to the poor reusability and loosely coupling in the present technologies and approaches like traditional Enterprise Application Integration (EAI) or Point to Point Web Services (P2PWS). This paper present the concept of Service Oriented Architecture (SOA) based application integration, by proposing an application integration framework for IBS using Enterprise Service Bus (ESB) and Business Process Execution Language (BPEL). The outcome of this paper demonstrates that\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Aspect-oriented modeling approaches and aspect code generation\n", "abstract": " The integration of aspect oriented modeling approaches with model-driven engineering process achieved through their direct transformation into aspect-oriented code can enhance the software development from many perspectives. However, since no aspect modeling technique has been adopted as the standard so far, it becomes imperative to compare all existing techniques on the basis of some appropriate criteria. This study aims to assess existing UML-based aspect-oriented modeling techniques from the perspective of their suitability with regards to integration into model-driven engineering process through aspect-oriented code generation. For this purpose, an evaluation framework has been defined and employed to evaluate 14 well-published, UML-based aspect-oriented modeling approaches. Results of this comparison show that a majority of these approaches works well to handle the basic modeling tasks. However, in the context of their integration into model-driven engineering process, these approaches need to be improved from various perspectives.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "A systematic map of integration of aspect orientation and model-driven engineering\n", "abstract": " An integration of aspect orientation and model-driven engineering is expected to enhance software development from many perspectives. Different approaches for this integration have already appeared in literature. In general, all such approaches use aspect-oriented model as the primary artifact and apply different techniques to obtain an executable from it. In this study we have provided a survey of existing research in this context by conducting a systematic mapping study. Classification schemes were defined and 38 selected primary studies were classified on the basis of research focus, contribution type and research type. Results show that solutions proposals are in a majority and current research has mainly focused on using model weavers for the integration (81% versus 19% code generation approaches). The majority of contributions are methods.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Considerations in Software Quality Enhancement for Value Based Systems\n", "abstract": " Software quality assurance plays an important role to check the overall quality of the software product especially when a product is a value based system. So to meet this quality the valuable software product or product line is tested under strict circumstances to meet the minimum constraints of software quality. This paper focuses on stakeholders, requirements engineering, different testing techniques being applied in software professional environment, the issues and current trends to resolve the requirements problems for continuous software quality improvement. This paper presents the criticality of stakeholders, requirements and software testing techniques for software professionals in terms of quality assurance. A model is proposed in order to achieve a high quality value based software application. There is the dire need to integrate stakeholders, requirements and testing in order to evaluate the performance and\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Meta-model Validation of Integrated MARTE and Component-Based Methodology Component Model for Embedded Real-Time Software\n", "abstract": " A validation process for integrated model-based methodology for component-based embedded real-time software with a profile is presented in this paper. Unified Modeling Language for Modeling and Analysis Real-Time and Embedded System, as a newly developed profile has been introduced to overcome problems in previous profiles. Nevertheless, a sound and systematic methodology is needed in order to tackle complexity problems that arose. The objective of this paper is to validate the integrated profile and a selected component-based methodology component model for satisfying embedded real-time software requirements, thus helping engineers to model their system, enhancing the structure and component modeling. For that, this paper described a component model meta-model validation process using quality matching for the integration process, involving a profile and a methodology\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "The specifications of the weakly hard real-time systems: a review\n", "abstract": " A real-time system is one in which the temporal aspects of its behaviour are part of their specification. The problem with traditional real-time specification is no guarantees on when and how many deadlines may be missed can be given to the tasks because their specification is focused on met all the deadlines and cannot missed it, otherwise the tasks is totally failed. Thus, the weakly hard specification solve this problem with define a gradation on how the deadlines can be missed while still guaranteeing the tasks meets their deadlines. In this paper, a review has been made on the three specifications of real-time systems which is losses or missed of the deadlines can be permitted occasionally. Three criteria used in the evaluation are the process model, temporal specifications and predictability. These three criteria were chosen because the tasks in real-time systems are usually periodic in nature, have\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "Mapping architectural concepts to SysML profile for product line architecture modeling\n", "abstract": " Line Architecture. To model the Product Line Architecture (PLA), the most important elements are the explicit treatment of its commonality and variability representation. This paper concentrates on the use of Architecture Description Language (ADL) and its integration with object oriented modeling, for the representation of architecture in order to model PLA architecture construct and variability construct effectively. Consequently, the possibility of integration which involves the mapping between the xADL and SysML a UML2 based profile extension to enable the profile to be incorporated to an existing UML commercial tool was investigated. The result of the mapping is proposed extension to SysML profile. The profile is then applied to a case study of Autonomous Mobile Robot Product Line. Based on the case study evaluation, the profile has shown a significant improvement to the existing SysML for modelling PLA.", "num_citations": "1\n", "authors": ["2185"]}
{"title": "A Review on Current Component Technologies for Embedded Real Time System\n", "abstract": " Nowadays, software development becomes more and more complex especially for embedded systems. Traditionally, software development addressed challenges of increasing complexity and dependence on external software by focusing on one system at a time and on delivery deadlines and budgets, while ignoring the evolutionary needs of the system. In software development embedded real time (ERT) software functionality is not the only focus but extra-functionality such as timing, resource constraint etc is also an important focus. To meet these challenges, ERT software development must be able to cope with complexity, to adapt quickly to changes and can support extrafunctionality. From this perspective Component based development (CBD) appears to be the appropriate approach. In this paper we reviewed the current component technologies for embedded real time system. They are Koala, PECOS, .NET\u00a0\u2026", "num_citations": "1\n", "authors": ["2185"]}
{"title": "A Temporal Modeling Approach for Schedulability Analysis in Component-Based Development\n", "abstract": " (AMR) system requires on-board computation and the system is typically constrained by limited processing power and memories. However, the software needs to fulfill its timing and application requirements, despite the constraints. Thus, the capability of real-time scheduling analysis to predict the AMR performance against the timing requirements is very important. The aims of this paper are to propose an approach which enables temporal modeling and to demonstrate prediction of AMR realtime performance based on the temporal models in component-based software development. This high-level prediction can avoid timing error in the field and costly late rework at the implementation phases. An experiment on AMR case-study was designed, to demonstrate the approach and to validate the predicted results against the real performance of the AMR software.", "num_citations": "1\n", "authors": ["2185"]}